[{"id": "2012.00091", "submitter": "Barbara Ilse Mahler", "authors": "Barbara I. Mahler", "title": "Contagion Dynamics for Manifold Learning", "comments": "33 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.AT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Contagion maps exploit activation times in threshold contagions to assign\nvectors in high-dimensional Euclidean space to the nodes of a network. A point\ncloud that is the image of a contagion map reflects both the structure\nunderlying the network and the spreading behaviour of the contagion on it.\nIntuitively, such a point cloud exhibits features of the network's underlying\nstructure if the contagion spreads along that structure, an observation which\nsuggests contagion maps as a viable manifold-learning technique. We test\ncontagion maps as a manifold-learning tool on a number of different real-world\nand synthetic data sets, and we compare their performance to that of Isomap,\none of the most well-known manifold-learning algorithms. We find that, under\ncertain conditions, contagion maps are able to reliably detect underlying\nmanifold structure in noisy data, while Isomap fails due to noise-induced\nerror. This consolidates contagion maps as a technique for manifold learning.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 20:58:21 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Mahler", "Barbara I.", ""]]}, {"id": "2012.00110", "submitter": "Andrew Miller", "authors": "Jeffrey Chan, Andrew C. Miller, Emily B. Fox", "title": "Representing and Denoising Wearable ECG Recordings", "comments": "ML for Mobile Health Workshop, NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern wearable devices are embedded with a range of noninvasive biomarker\nsensors that hold promise for improving detection and treatment of disease. One\nsuch sensor is the single-lead electrocardiogram (ECG) which measures\nelectrical signals in the heart. The benefits of the sheer volume of ECG\nmeasurements with rich longitudinal structure made possible by wearables come\nat the price of potentially noisier measurements compared to clinical ECGs,\ne.g., due to movement. In this work, we develop a statistical model to simulate\na structured noise process in ECGs derived from a wearable sensor, design a\nbeat-to-beat representation that is conducive for analyzing variation, and\ndevise a factor analysis-based method to denoise the ECG. We study synthetic\ndata generated using a realistic ECG simulator and a structured noise model. At\nvarying levels of signal-to-noise, we quantitatively measure an upper bound on\nperformance and compare estimates from linear and non-linear models. Finally,\nwe apply our method to a set of ECGs collected by wearables in a mobile health\nstudy.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 21:33:11 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Chan", "Jeffrey", ""], ["Miller", "Andrew C.", ""], ["Fox", "Emily B.", ""]]}, {"id": "2012.00113", "submitter": "Michail Tsagris", "authors": "Michail Tsagris", "title": "The FEDHC Bayesian network learning algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new hybrid Bayesian network learning algorithm, termed Forward Early\nDropping Hill Climbing (FEDHC), devised to work with either continuous or\ncategorical variables. FEDHC consists of a skeleton identification phase and a\nsubsequent scoring phase that assigns the (causal) directions. Further, the\npaper manifests that the only implementation of MMHC in the statistical\nsoftware \\textit{R}, is prohibitively expensive and a new implementation is\noffered. In addition, specifically for the case of continuous data, a robust to\noutliers version of FEDHC, that can be adopted by other BN learning algorithms\nas well is proposed. The FEDHC is tested via Monte Carlo simulations that\ndistinctly show it is computationally efficient, and produces Bayesian networks\nof similar to, or of higher accuracy than MMHC and PCHC. Specifically, FEDHC\nyields more accurate Bayesian networks than PCHC with continuous data but less\naccurate with categorical data. Finally, an application of FEDHC, PCHC and MMHC\nalgorithms to real data, from the field of economics, is demonstrated using the\nstatistical software \\textit{R}.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 21:36:25 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 13:08:47 GMT"}, {"version": "v3", "created": "Thu, 27 May 2021 19:38:08 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Tsagris", "Michail", ""]]}, {"id": "2012.00123", "submitter": "Yujia Xie", "authors": "Yujia Xie, Yixiu Mao, Simiao Zuo, Hongteng Xu, Xiaojing Ye, Tuo Zhao,\n  Hongyuan Zha", "title": "A Hypergradient Approach to Robust Regression without Correspondence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider a variant of regression problem, where the correspondence between\ninput and output data is not available. Such shuffled data is commonly observed\nin many real world problems. Taking flow cytometry as an example, the measuring\ninstruments may not be able to maintain the correspondence between the samples\nand the measurements. Due to the combinatorial nature of the problem, most\nexisting methods are only applicable when the sample size is small, and limited\nto linear regression models. To overcome such bottlenecks, we propose a new\ncomputational framework -- ROBOT -- for the shuffled regression problem, which\nis applicable to large data and complex nonlinear models. Specifically, we\nreformulate the regression without correspondence as a continuous optimization\nproblem. Then by exploiting the interaction between the regression model and\nthe data correspondence, we develop a hypergradient approach based on\ndifferentiable programming techniques. Such a hypergradient approach\nessentially views the data correspondence as an operator of the regression, and\ntherefore allows us to find a better descent direction for the model parameter\nby differentiating through the data correspondence. ROBOT can be further\nextended to the inexact correspondence setting, where there may not be an exact\nalignment between the input and output data. Thorough numerical experiments\nshow that ROBOT achieves better performance than existing methods in both\nlinear and nonlinear regression tasks, including real-world applications such\nas flow cytometry and multi-object tracking.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 21:47:38 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 16:47:12 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Xie", "Yujia", ""], ["Mao", "Yixiu", ""], ["Zuo", "Simiao", ""], ["Xu", "Hongteng", ""], ["Ye", "Xiaojing", ""], ["Zhao", "Tuo", ""], ["Zha", "Hongyuan", ""]]}, {"id": "2012.00152", "submitter": "Pedro Domingos", "authors": "Pedro Domingos", "title": "Every Model Learned by Gradient Descent Is Approximately a Kernel\n  Machine", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning's successes are often attributed to its ability to\nautomatically discover new representations of the data, rather than relying on\nhandcrafted features like other learning methods. We show, however, that deep\nnetworks learned by the standard gradient descent algorithm are in fact\nmathematically approximately equivalent to kernel machines, a learning method\nthat simply memorizes the data and uses it directly for prediction via a\nsimilarity function (the kernel). This greatly enhances the interpretability of\ndeep network weights, by elucidating that they are effectively a superposition\nof the training examples. The network architecture incorporates knowledge of\nthe target function into the kernel. This improved understanding should lead to\nbetter learning algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 23:02:47 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Domingos", "Pedro", ""]]}, {"id": "2012.00168", "submitter": "Satya Narayan Shukla", "authors": "Satya Narayan Shukla, Benjamin M. Marlin", "title": "A Survey on Principles, Models and Methods for Learning from Irregularly\n  Sampled Time Series", "comments": "Presented at NeurIPS 2020 Workshop: ML Retrospectives, Surveys &\n  Meta-Analyses (ML-RSA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Irregularly sampled time series data arise naturally in many application\ndomains including biology, ecology, climate science, astronomy, and health.\nSuch data represent fundamental challenges to many classical models from\nmachine learning and statistics due to the presence of non-uniform intervals\nbetween observations. However, there has been significant progress within the\nmachine learning community over the last decade on developing specialized\nmodels and architectures for learning from irregularly sampled univariate and\nmultivariate time series data. In this survey, we first describe several axes\nalong which approaches to learning from irregularly sampled time series differ\nincluding what data representations they are based on, what modeling primitives\nthey leverage to deal with the fundamental problem of irregular sampling, and\nwhat inference tasks they are designed to perform. We then survey the recent\nliterature organized primarily along the axis of modeling primitives. We\ndescribe approaches based on temporal discretization, interpolation,\nrecurrence, attention and structural invariance. We discuss similarities and\ndifferences between approaches and highlight primary strengths and weaknesses.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 23:41:47 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 20:59:24 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Shukla", "Satya Narayan", ""], ["Marlin", "Benjamin M.", ""]]}, {"id": "2012.00188", "submitter": "Alexander Soen", "authors": "Alexander Soen, Hisham Husain, Richard Nock", "title": "Data Preprocessing to Mitigate Bias with Boosted Fair Mollifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In a recent paper, Celis et al. (2020) introduced a new approach to fairness\nthat corrects the data distribution itself. The approach is computationally\nappealing, but its approximation guarantees with respect to the target\ndistribution can be quite loose as they need to rely on a (typically limited)\nnumber of constraints on data-based aggregated statistics; also resulting in a\nfairness guarantee which can be data dependent.\n  Our paper makes use of a mathematical object recently introduced in privacy\n-- mollifiers of distributions -- and a popular approach to machine learning --\nboosting -- to get an approach in the same lineage as Celis et al. but without\nthe same impediments, including in particular, better guarantees in terms of\naccuracy and finer guarantees in terms of fairness. The approach involves\nlearning the sufficient statistics of an exponential family. When the training\ndata is tabular, the sufficient statistics can be defined by decision trees\nwhose interpretability can provide clues on the source of (un)fairness.\nExperiments display the quality of the results for simulated and real-world\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 00:49:17 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 01:09:20 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Soen", "Alexander", ""], ["Husain", "Hisham", ""], ["Nock", "Richard", ""]]}, {"id": "2012.00314", "submitter": "Sanae Amani", "authors": "Sanae Amani, Christos Thrampoulidis", "title": "Decentralized Multi-Agent Linear Bandits with Safety Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study decentralized stochastic linear bandits, where a network of $N$\nagents acts cooperatively to efficiently solve a linear bandit-optimization\nproblem over a $d$-dimensional space. For this problem, we propose DLUCB: a\nfully decentralized algorithm that minimizes the cumulative regret over the\nentire network. At each round of the algorithm each agent chooses its actions\nfollowing an upper confidence bound (UCB) strategy and agents share information\nwith their immediate neighbors through a carefully designed consensus procedure\nthat repeats over cycles. Our analysis adjusts the duration of these\ncommunication cycles ensuring near-optimal regret performance\n$\\mathcal{O}(d\\log{NT}\\sqrt{NT})$ at a communication rate of\n$\\mathcal{O}(dN^2)$ per round. The structure of the network affects the regret\nperformance via a small additive term - coined the regret of delay - that\ndepends on the spectral gap of the underlying graph. Notably, our results apply\nto arbitrary network topologies without a requirement for a dedicated agent\nacting as a server. In consideration of situations with high communication\ncost, we propose RC-DLUCB: a modification of DLUCB with rare communication\namong agents. The new algorithm trades off regret performance for a\nsignificantly reduced total communication cost of $\\mathcal{O}(d^3N^{2.5})$\nover all $T$ rounds. Finally, we show that our ideas extend naturally to the\nemerging, albeit more challenging, setting of safe bandits. For the recently\nstudied problem of linear bandits with unknown linear safety constraints, we\npropose the first safe decentralized algorithm. Our study contributes towards\napplying bandit techniques in safety-critical distributed systems that\nrepeatedly deal with unknown stochastic environments. We present numerical\nsimulations for various network topologies that corroborate our theoretical\nfindings.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 07:33:00 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Amani", "Sanae", ""], ["Thrampoulidis", "Christos", ""]]}, {"id": "2012.00348", "submitter": "Song-Kyoo Amang Kim Ph.D.", "authors": "Song-Kyoo Kim, Chan Yeob Yeun, Paul D. Yoo, Nai-Wei Lo, Ernesto\n  Damiani", "title": "Deep Learning-Based Arrhythmia Detection Using RR-Interval Framed\n  Electrocardiograms", "comments": "This paper is considered to be submitted to an international journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning applied to electrocardiogram (ECG) data can be used to achieve\npersonal authentication in biometric security applications, but it has not been\nwidely used to diagnose cardiovascular disorders. We developed a deep learning\nmodel for the detection of arrhythmia in which time-sliced ECG data\nrepresenting the distance between successive R-peaks are used as the input for\na convolutional neural network (CNN). The main objective is developing the\ncompact deep learning based detect system which minimally uses the dataset but\ndelivers the confident accuracy rate of the Arrhythmia detection. This compact\nsystem can be implemented in wearable devices or real-time monitoring equipment\nbecause the feature extraction step is not required for complex ECG waveforms,\nonly the R-peak data is needed. The results of both tests indicated that the\nCompact Arrhythmia Detection System (CADS) matched the performance of\nconventional systems for the detection of arrhythmia in two consecutive test\nruns. All features of the CADS are fully implemented and publicly available in\nMATLAB.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 09:10:24 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Kim", "Song-Kyoo", ""], ["Yeun", "Chan Yeob", ""], ["Yoo", "Paul D.", ""], ["Lo", "Nai-Wei", ""], ["Damiani", "Ernesto", ""]]}, {"id": "2012.00370", "submitter": "Martin Huber", "authors": "Hugo Bodory, Martin Huber, Luk\\'a\\v{s} Laff\\'ers", "title": "Evaluating (weighted) dynamic treatment effects by double machine\n  learning", "comments": "arXiv admin note: text overlap with arXiv:2002.12710", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider evaluating the causal effects of dynamic treatments, i.e. of\nmultiple treatment sequences in various periods, based on double machine\nlearning to control for observed, time-varying covariates in a data-driven way\nunder a selection-on-observables assumption. To this end, we make use of\nso-called Neyman-orthogonal score functions, which imply the robustness of\ntreatment effect estimation to moderate (local) misspecifications of the\ndynamic outcome and treatment models. This robustness property permits\napproximating outcome and treatment models by double machine learning even\nunder high dimensional covariates and is combined with data splitting to\nprevent overfitting. In addition to effect estimation for the total population,\nwe consider weighted estimation that permits assessing dynamic treatment\neffects in specific subgroups, e.g. among those treated in the first treatment\nperiod. We demonstrate that the estimators are asymptotically normal and\n$\\sqrt{n}$-consistent under specific regularity conditions and investigate\ntheir finite sample properties in a simulation study. Finally, we apply the\nmethods to the Job Corps study in order to assess different sequences of\ntraining programs under a large set of covariates.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 09:55:40 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 19:05:21 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2021 13:28:53 GMT"}, {"version": "v4", "created": "Tue, 16 Feb 2021 16:05:40 GMT"}, {"version": "v5", "created": "Sat, 19 Jun 2021 14:56:54 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Bodory", "Hugo", ""], ["Huber", "Martin", ""], ["Laff\u00e9rs", "Luk\u00e1\u0161", ""]]}, {"id": "2012.00428", "submitter": "Jure Brence", "authors": "Jure Brence and Ljup\\v{c}o Todorovski and Sa\\v{s}o D\\v{z}eroski", "title": "Probabilistic Grammars for Equation Discovery", "comments": "Submitted to Knowledge-Based Systems, Elsevier. 28 pages + 13 pages\n  appendix. 7 figures", "journal-ref": null, "doi": "10.1016/j.knosys.2021.107077", "report-no": null, "categories": "cs.LG cs.FL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Equation discovery, also known as symbolic regression, is a type of automated\nmodeling that discovers scientific laws, expressed in the form of equations,\nfrom observed data and expert knowledge. Deterministic grammars, such as\ncontext-free grammars, have been used to limit the search spaces in equation\ndiscovery by providing hard constraints that specify which equations to\nconsider and which not. In this paper, we propose the use of probabilistic\ncontext-free grammars in equation discovery. Such grammars encode soft\nconstraints, specifying a prior probability distribution on the space of\npossible equations. We show that probabilistic grammars can be used to\nelegantly and flexibly formulate the parsimony principle, that favors simpler\nequations, through probabilities attached to the rules in the grammars. We\ndemonstrate that the use of probabilistic, rather than deterministic grammars,\nin the context of a Monte-Carlo algorithm for grammar-based equation discovery,\nleads to more efficient equation discovery. Finally, by specifying prior\nprobability distributions over equation spaces, the foundations are laid for\nBayesian approaches to equation discovery.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 11:59:19 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 10:53:27 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Brence", "Jure", ""], ["Todorovski", "Ljup\u010do", ""], ["D\u017eeroski", "Sa\u0161o", ""]]}, {"id": "2012.00459", "submitter": "Cheng Zhang", "authors": "Cheng Zhang", "title": "Improved Variational Bayesian Phylogenetic Inference with Normalizing\n  Flows", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Variational Bayesian phylogenetic inference (VBPI) provides a promising\ngeneral variational framework for efficient estimation of phylogenetic\nposteriors. However, the current diagonal Lognormal branch length approximation\nwould significantly restrict the quality of the approximating distributions. In\nthis paper, we propose a new type of VBPI, VBPI-NF, as a first step to empower\nphylogenetic posterior estimation with deep learning techniques. By handling\nthe non-Euclidean branch length space of phylogenetic models with carefully\ndesigned permutation equivariant transformations, VBPI-NF uses normalizing\nflows to provide a rich family of flexible branch length distributions that\ngeneralize across different tree topologies. We show that VBPI-NF significantly\nimproves upon the vanilla VBPI on a benchmark of challenging real data Bayesian\nphylogenetic inference problems. Further investigation also reveals that the\nstructured parameterization in those permutation equivariant transformations\ncan provide additional amortization benefit.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 13:10:00 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Zhang", "Cheng", ""]]}, {"id": "2012.00481", "submitter": "Stan Z Li", "authors": "Stan Z. Li, Lirong Wu and Zelin Zang", "title": "Consistent Representation Learning for High Dimensional Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High dimensional data analysis for exploration and discovery includes three\nfundamental tasks: dimensionality reduction, clustering, and visualization.\nWhen the three associated tasks are done separately, as is often the case thus\nfar, inconsistencies can occur among the tasks in terms of data geometry and\nothers. This can lead to confusing or misleading data interpretation. In this\npaper, we propose a novel neural network-based method, called Consistent\nRepresentation Learning (CRL), to accomplish the three associated tasks\nend-to-end and improve the consistencies. The CRL network consists of two\nnonlinear dimensionality reduction (NLDR) transformations: (1) one from the\ninput data space to the latent feature space for clustering, and (2) the other\nfrom the clustering space to the final 2D or 3D space for visualization.\nImportantly, the two NLDR transformations are performed to best satisfy local\ngeometry preserving (LGP) constraints across the spaces or network layers, to\nimprove data consistencies along with the processing flow. Also, we propose a\nnovel metric, clustering-visualization inconsistency (CVI), for evaluating the\ninconsistencies. Extensive comparative results show that the proposed CRL\nneural network method outperforms the popular t-SNE and UMAP-based and other\ncontemporary clustering and visualization algorithms in terms of evaluation\nmetrics and visualization.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 13:39:50 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Li", "Stan Z.", ""], ["Wu", "Lirong", ""], ["Zang", "Zelin", ""]]}, {"id": "2012.00493", "submitter": "Iana Sereda", "authors": "Iana Sereda, Sergey Alekseev, Aleksandra Koneva, Alexey Khorkin,\n  Grigory Osipov", "title": "Problems of representation of electrocardiograms in convolutional neural\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Using electrocardiograms as an example, we demonstrate the characteristic\nproblems that arise when modeling one-dimensional signals containing inaccurate\nrepeating pattern by means of standard convolutional networks. We show that\nthese problems are systemic in nature. They are due to how convolutional\nnetworks work with composite objects, parts of which are not fixed rigidly, but\nhave significant mobility. We also demonstrate some counterintuitive effects\nrelated to generalization in deep networks.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 14:02:06 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Sereda", "Iana", ""], ["Alekseev", "Sergey", ""], ["Koneva", "Aleksandra", ""], ["Khorkin", "Alexey", ""], ["Osipov", "Grigory", ""]]}, {"id": "2012.00499", "submitter": "Fabian Hinder", "authors": "Fabian Hinder, Jonathan Jakob, Barbara Hammer", "title": "Analysis of Drifting Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of concept drift refers to the phenomenon that the distribution,\nwhich is underlying the observed data, changes over time. We are interested in\nan identification of those features, that are most relevant for the observed\ndrift. We distinguish between drift inducing features, for which the observed\nfeature drift cannot be explained by any other feature, and faithfully drifting\nfeatures, which correlate with the present drift of other features. This notion\ngives rise to minimal subsets of the feature space, which are able to\ncharacterize the observed drift as a whole. We relate this problem to the\nproblems of feature selection and feature relevance learning, which allows us\nto derive a detection algorithm. We demonstrate its usefulness on different\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 14:09:19 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Hinder", "Fabian", ""], ["Jakob", "Jonathan", ""], ["Hammer", "Barbara", ""]]}, {"id": "2012.00513", "submitter": "S{\\o}ren B. Vilsen", "authors": "S{\\o}ren B. Vilsen, Torben Tvedebrink, and Poul Svante Eriksen", "title": "DNA mixture deconvolution using an evolutionary algorithm with multiple\n  populations, hill-climbing, and guided mutation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  DNA samples crime cases analysed in forensic genetics, frequently contain DNA\nfrom multiple contributors. These occur as convolutions of the DNA profiles of\nthe individual contributors to the DNA sample. Thus, in cases where one or more\nof the contributors were unknown, an objective of interest would be the\nseparation, often called deconvolution, of these unknown profiles. In order to\nobtain deconvolutions of the unknown DNA profiles, we introduced a multiple\npopulation evolutionary algorithm (MEA). We allowed the mutation operator of\nthe MEA to utilise that the fitness is based on a probabilistic model and guide\nit by using the deviations between the observed and the expected value for\nevery element of the encoded individual. This guided mutation operator (GM) was\ndesigned such that the larger the deviation the higher probability of mutation.\nFurthermore, the GM was inhomogeneous in time, decreasing to a specified lower\nbound as the number of iterations increased. We analysed 102 two-person DNA\nmixture samples in varying mixture proportions. The samples were quantified\nusing two different DNA prep. kits: (1) Illumina ForenSeq Panel B (30 samples),\nand (2) Applied Biosystems Precision ID Globalfiler NGS STR panel (72 samples).\nThe DNA mixtures were deconvoluted by the MEA and compared to the true DNA\nprofiles of the sample. We analysed three scenarios where we assumed: (1) the\nDNA profile of the major contributor was unknown, (2) DNA profile of the minor\nwas unknown, and (3) both DNA profiles were unknown. Furthermore, we conducted\na series of sensitivity experiments on the ForenSeq panel by varying the\nsub-population size, comparing a completely random homogeneous mutation\noperator to the guided operator with varying mutation decay rates, and allowing\nfor hill-climbing of the parent population.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 14:23:55 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Vilsen", "S\u00f8ren B.", ""], ["Tvedebrink", "Torben", ""], ["Eriksen", "Poul Svante", ""]]}, {"id": "2012.00560", "submitter": "Zahra Atashgahi", "authors": "Zahra Atashgahi, Ghada Sokar, Tim van der Lee, Elena Mocanu, Decebal\n  Constantin Mocanu, Raymond Veldhuis, Mykola Pechenizkiy", "title": "Quick and Robust Feature Selection: the Strength of Energy-efficient\n  Sparse Training for Autoencoders", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Major complications arise from the recent increase in the amount of\nhigh-dimensional data, including high computational costs and memory\nrequirements. Feature selection, which identifies the most relevant and\ninformative attributes of a dataset, has been introduced as a solution to this\nproblem. Most of the existing feature selection methods are computationally\ninefficient; inefficient algorithms lead to high energy consumption, which is\nnot desirable for devices with limited computational and energy resources. In\nthis paper, a novel and flexible method for unsupervised feature selection is\nproposed. This method, named QuickSelection, introduces the strength of the\nneuron in sparse neural networks as a criterion to measure the feature\nimportance. This criterion, blended with sparsely connected denoising\nautoencoders trained with the sparse evolutionary training procedure, derives\nthe importance of all input features simultaneously. We implement\nQuickSelection in a purely sparse manner as opposed to the typical approach of\nusing a binary mask over connections to simulate sparsity. It results in a\nconsiderable speed increase and memory reduction. When tested on several\nbenchmark datasets, including five low-dimensional and three high-dimensional\ndatasets, the proposed method is able to achieve the best trade-off of\nclassification and clustering accuracy, running time, and maximum memory usage,\namong widely used approaches for feature selection. Besides, our proposed\nmethod requires the least amount of energy among the state-of-the-art\nautoencoder-based feature selection methods.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 15:05:15 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Atashgahi", "Zahra", ""], ["Sokar", "Ghada", ""], ["van der Lee", "Tim", ""], ["Mocanu", "Elena", ""], ["Mocanu", "Decebal Constantin", ""], ["Veldhuis", "Raymond", ""], ["Pechenizkiy", "Mykola", ""]]}, {"id": "2012.00632", "submitter": "Felix Sattler", "authors": "Felix Sattler and Arturo Marban and Roman Rischke and Wojciech Samek", "title": "Communication-Efficient Federated Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Communication constraints are one of the major challenges preventing the\nwide-spread adoption of Federated Learning systems. Recently, Federated\nDistillation (FD), a new algorithmic paradigm for Federated Learning with\nfundamentally different communication properties, emerged. FD methods leverage\nensemble distillation techniques and exchange model outputs, presented as soft\nlabels on an unlabeled public data set, between the central server and the\nparticipating clients. While for conventional Federated Learning algorithms,\nlike Federated Averaging (FA), communication scales with the size of the\njointly trained model, in FD communication scales with the distillation data\nset size, resulting in advantageous communication properties, especially when\nlarge models are trained. In this work, we investigate FD from the perspective\nof communication efficiency by analyzing the effects of active\ndistillation-data curation, soft-label quantization and delta-coding\ntechniques. Based on the insights gathered from this analysis, we present\nCompressed Federated Distillation (CFD), an efficient Federated Distillation\nmethod. Extensive experiments on Federated image classification and language\nmodeling problems demonstrate that our method can reduce the amount of\ncommunication necessary to achieve fixed performance targets by more than two\norders of magnitude, when compared to FD and by more than four orders of\nmagnitude when compared with FA.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 16:57:25 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Sattler", "Felix", ""], ["Marban", "Arturo", ""], ["Rischke", "Roman", ""], ["Samek", "Wojciech", ""]]}, {"id": "2012.00634", "submitter": "Maren Hackenberg", "authors": "Maren Hackenberg, Philipp Harms, Thorsten Schmidt, Harald Binder", "title": "Deep dynamic modeling with just two time points: Can we still allow for\n  individual trajectories?", "comments": "18 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Longitudinal biomedical data are often characterized by a sparse time grid\nand individual-specific development patterns. Specifically, in epidemiological\ncohort studies and clinical registries we are facing the question of what can\nbe learned from the data in an early phase of the study, when only a baseline\ncharacterization and one follow-up measurement are available. Inspired by\nrecent advances that allow to combine deep learning with dynamic modeling, we\ninvestigate whether such approaches can be useful for uncovering complex\nstructure, in particular for an extreme small data setting with only two\nobservations time points for each individual. Irregular spacing in time could\nthen be used to gain more information on individual dynamics by leveraging\nsimilarity of individuals. We provide a brief overview of how variational\nautoencoders (VAEs), as a deep learning approach, can be linked to ordinary\ndifferential equations (ODEs) for dynamic modeling, and then specifically\ninvestigate the feasibility of such an approach that infers individual-specific\nlatent trajectories by including regularity assumptions and individuals'\nsimilarity. We also provide a description of this deep learning approach as a\nfiltering task to give a statistical perspective. Using simulated data, we show\nto what extent the approach can recover individual trajectories from ODE\nsystems with two and four unknown parameters and infer groups of individuals\nwith similar trajectories, and where it breaks down. The results show that such\ndynamic deep learning approaches can be useful even in extreme small data\nsettings, but need to be carefully adapted.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 16:58:02 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Hackenberg", "Maren", ""], ["Harms", "Philipp", ""], ["Schmidt", "Thorsten", ""], ["Binder", "Harald", ""]]}, {"id": "2012.00649", "submitter": "Yang Zhao", "authors": "Yang Zhao, Changyou Chen", "title": "Unpaired Image-to-Image Translation via Latent Energy Transport", "comments": "CVPR2021. Code: https://github.com/YangNaruto/latent-energy-transport", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-to-image translation aims to preserve source contents while translating\nto discriminative target styles between two visual domains. Most works apply\nadversarial learning in the ambient image space, which could be computationally\nexpensive and challenging to train. In this paper, we propose to deploy an\nenergy-based model (EBM) in the latent space of a pretrained autoencoder for\nthis task. The pretrained autoencoder serves as both a latent code extractor\nand an image reconstruction worker. Our model, LETIT, is based on the\nassumption that two domains share the same latent space, where latent\nrepresentation is implicitly decomposed as a content code and a domain-specific\nstyle code. Instead of explicitly extracting the two codes and applying\nadaptive instance normalization to combine them, our latent EBM can implicitly\nlearn to transport the source style code to the target style code while\npreserving the content code, an advantage over existing image translation\nmethods. This simplified solution is also more efficient in the one-sided\nunpaired image translation setting. Qualitative and quantitative comparisons\ndemonstrate superior translation quality and faithfulness for content\npreservation. Our model is the first to be applicable to\n1024$\\times$1024-resolution unpaired image translation to the best of our\nknowledge.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 17:18:58 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 19:33:19 GMT"}, {"version": "v3", "created": "Sun, 23 May 2021 19:54:38 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Zhao", "Yang", ""], ["Chen", "Changyou", ""]]}, {"id": "2012.00708", "submitter": "G\\'abor Melis", "authors": "G\\'abor Melis, Andr\\'as Gy\\\"orgy, Phil Blunsom", "title": "Mutual Information Constraints for Monte-Carlo Objectives", "comments": "32 pages, 29 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common failure mode of density models trained as variational autoencoders\nis to model the data without relying on their latent variables, rendering these\nvariables useless. Two contributing factors, the underspecification of the\nmodel and the looseness of the variational lower bound, have been studied\nseparately in the literature. We weave these two strands of research together,\nspecifically the tighter bounds of Monte-Carlo objectives and constraints on\nthe mutual information between the observable and the latent variables.\nEstimating the mutual information as the average Kullback-Leibler divergence\nbetween the easily available variational posterior $q(z|x)$ and the prior does\nnot work with Monte-Carlo objectives because $q(z|x)$ is no longer a direct\napproximation to the model's true posterior $p(z|x)$. Hence, we construct\nestimators of the Kullback-Leibler divergence of the true posterior from the\nprior by recycling samples used in the objective, with which we train models of\ncontinuous and discrete latents at much improved rate-distortion and no\nposterior collapse. While alleviated, the tradeoff between modelling the data\nand using the latents still remains, and we urge for evaluating inference\nmethods across a range of mutual information values.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 18:14:08 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Melis", "G\u00e1bor", ""], ["Gy\u00f6rgy", "Andr\u00e1s", ""], ["Blunsom", "Phil", ""]]}, {"id": "2012.00714", "submitter": "Jingyan Wang", "authors": "Jingyan Wang, Ivan Stelmakh, Yuting Wei, Nihar B. Shah", "title": "Debiasing Evaluations That are Biased by Evaluations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common to evaluate a set of items by soliciting people to rate them.\nFor example, universities ask students to rate the teaching quality of their\ninstructors, and conference organizers ask authors of submissions to evaluate\nthe quality of the reviews. However, in these applications, students often give\na higher rating to a course if they receive higher grades in a course, and\nauthors often give a higher rating to the reviews if their papers are accepted\nto the conference. In this work, we call these external factors the \"outcome\"\nexperienced by people, and consider the problem of mitigating these\noutcome-induced biases in the given ratings when some information about the\noutcome is available. We formulate the information about the outcome as a known\npartial ordering on the bias. We propose a debiasing method by solving a\nregularized optimization problem under this ordering constraint, and also\nprovide a carefully designed cross-validation method that adaptively chooses\nthe appropriate amount of regularization. We provide theoretical guarantees on\nthe performance of our algorithm, as well as experimental evaluations.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 18:20:43 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Wang", "Jingyan", ""], ["Stelmakh", "Ivan", ""], ["Wei", "Yuting", ""], ["Shah", "Nihar B.", ""]]}, {"id": "2012.00729", "submitter": "Mike Ludkovski", "authors": "Mike Ludkovski", "title": "mlOSP: Towards a Unified Implementation of Regression Monte Carlo\n  Algorithms", "comments": "Package repository is at http://github.com/mludkov/mlOSP", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce mlOSP, a computational template for Machine Learning for Optimal\nStopping Problems. The template is implemented in the R statistical environment\nand publicly available via a GitHub repository. mlOSP presents a unified\nnumerical implementation of Regression Monte Carlo (RMC) approaches to optimal\nstopping, providing a state-of-the-art, open-source, reproducible and\ntransparent platform. Highlighting its modular nature, we present multiple\nnovel variants of RMC algorithms, especially in terms of constructing\nsimulation designs for training the regressors, as well as in terms of machine\nlearning regression modules. At the same time, mlOSP nests most of the existing\nRMC schemes, allowing for a consistent and verifiable benchmarking of extant\nalgorithms. The article contains extensive R code snippets and figures, and\nserves the dual role of presenting new RMC features and as a vignette to the\nunderlying software package.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 18:41:02 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Ludkovski", "Mike", ""]]}, {"id": "2012.00745", "submitter": "Martin Huber", "authors": "Michela Bia, Martin Huber, Luk\\'a\\v{s} Laff\\'ers", "title": "Double machine learning for sample selection models", "comments": "arXiv admin note: text overlap with arXiv:2012.00370,\n  arXiv:2002.12710", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper considers the evaluation of discretely distributed treatments when\noutcomes are only observed for a subpopulation due to sample selection or\noutcome attrition. For identification, we combine a selection-on-observables\nassumption for treatment assignment with either selection-on-observables or\ninstrumental variable assumptions concerning the outcome attrition/sample\nselection process. We also consider dynamic confounding, meaning that\ncovariates that jointly affect sample selection and the outcome may (at least\npartly) be influenced by the treatment. To control in a data-driven way for a\npotentially high dimensional set of pre- and/or post-treatment covariates, we\nadapt the double machine learning framework for treatment evaluation to sample\nselection problems. We make use of (a) Neyman-orthogonal, doubly robust, and\nefficient score functions, which imply the robustness of treatment effect\nestimation to moderate regularization biases in the machine learning-based\nestimation of the outcome, treatment, or sample selection models and (b) sample\nsplitting (or cross-fitting) to prevent overfitting bias. We demonstrate that\nthe proposed estimators are asymptotically normal and root-n consistent under\nspecific regularity conditions concerning the machine learners and investigate\ntheir finite sample properties in a simulation study. We also apply our\nproposed methodology to the Job Corps data for evaluating the effect of\ntraining on hourly wages which are only observed conditional on employment. The\nestimator is available in the causalweight package for the statistical software\nR.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 19:40:21 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 12:44:10 GMT"}, {"version": "v3", "created": "Sat, 1 May 2021 09:12:02 GMT"}, {"version": "v4", "created": "Sat, 19 Jun 2021 14:53:34 GMT"}, {"version": "v5", "created": "Thu, 15 Jul 2021 15:55:40 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Bia", "Michela", ""], ["Huber", "Martin", ""], ["Laff\u00e9rs", "Luk\u00e1\u0161", ""]]}, {"id": "2012.00752", "submitter": "Yuchen Wang", "authors": "Yuchen Wang, Matthieu Chan Chee, Ziyad Edher, Minh Duc Hoang, Shion\n  Fujimori, Sornnujah Kathirgamanathan, Jesse Bettencourt", "title": "Forecasting Black Sigatoka Infection Risks with Latent Neural ODEs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Black Sigatoka disease severely decreases global banana production, and\nclimate change aggravates the problem by altering fungal species distributions.\nDue to the heavy financial burden of managing this infectious disease, farmers\nin developing countries face significant banana crop losses. Though scientists\nhave produced mathematical models of infectious diseases, adapting these models\nto incorporate climate effects is difficult. We present MR. NODE (Multiple\npredictoR Neural ODE), a neural network that models the dynamics of black\nSigatoka infection learnt directly from data via Neural Ordinary Differential\nEquations. Our method encodes external predictor factors into the latent space\nin addition to the variable that we infer, and it can also predict the\ninfection risk at an arbitrary point in time. Empirically, we demonstrate on\nhistorical climate data that our method has superior generalization performance\non time points up to one month in the future and unseen irregularities. We\nbelieve that our method can be a useful tool to control the spread of black\nSigatoka.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 21:59:53 GMT"}, {"version": "v2", "created": "Sun, 10 Jan 2021 06:21:44 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Wang", "Yuchen", ""], ["Chee", "Matthieu Chan", ""], ["Edher", "Ziyad", ""], ["Hoang", "Minh Duc", ""], ["Fujimori", "Shion", ""], ["Kathirgamanathan", "Sornnujah", ""], ["Bettencourt", "Jesse", ""]]}, {"id": "2012.00780", "submitter": "Abdul Fatir Ansari", "authors": "Abdul Fatir Ansari, Ming Liang Ang, Harold Soh", "title": "Refining Deep Generative Models via Discriminator Gradient Flow", "comments": "ICLR 2021 Camera Ready; Code available at\n  https://github.com/clear-nus/DGflow; Updated Related Work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative modeling has seen impressive advances in recent years, to the\npoint where it is now commonplace to see simulated samples (e.g., images) that\nclosely resemble real-world data. However, generation quality is generally\ninconsistent for any given model and can vary dramatically between samples. We\nintroduce Discriminator Gradient flow (DGflow), a new technique that improves\ngenerated samples via the gradient flow of entropy-regularized f-divergences\nbetween the real and the generated data distributions. The gradient flow takes\nthe form of a non-linear Fokker-Plank equation, which can be easily simulated\nby sampling from the equivalent McKean-Vlasov process. By refining inferior\nsamples, our technique avoids wasteful sample rejection used by previous\nmethods (DRS & MH-GAN). Compared to existing works that focus on specific GAN\nvariants, we show our refinement approach can be applied to GANs with\nvector-valued critics and even other deep generative models such as VAEs and\nNormalizing Flows. Empirical results on multiple synthetic, image, and text\ndatasets demonstrate that DGflow leads to significant improvement in the\nquality of generated samples for a variety of generative models, outperforming\nthe state-of-the-art Discriminator Optimal Transport (DOT) and Discriminator\nDriven Latent Sampling (DDLS) methods.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 19:10:15 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 09:08:55 GMT"}, {"version": "v3", "created": "Wed, 17 Mar 2021 19:33:49 GMT"}, {"version": "v4", "created": "Sat, 5 Jun 2021 04:45:44 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Ansari", "Abdul Fatir", ""], ["Ang", "Ming Liang", ""], ["Soh", "Harold", ""]]}, {"id": "2012.00805", "submitter": "Prasenjit Karmakar", "authors": "Prasenjit Karmakar", "title": "Stochastic Approximation with Markov Noise: Analysis and applications in\n  reinforcement learning", "comments": "124 pages, PhD thesis, IIS, Bangalore (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.DS math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present for the first time an asymptotic convergence analysis of two\ntime-scale stochastic approximation driven by \"controlled\" Markov noise. In\nparticular, the faster and slower recursions have non-additive controlled\nMarkov noise components in addition to martingale difference noise. We analyze\nthe asymptotic behavior of our framework by relating it to limiting\ndifferential inclusions in both time scales that are defined in terms of the\nergodic occupation measures associated with the controlled Markov processes.\nUsing a special case of our results, we present a solution to the off-policy\nconvergence problem for temporal-difference learning with linear function\napproximation. We compile several aspects of the dynamics of stochastic\napproximation algorithms with Markov iterate-dependent noise when the iterates\nare not known to be stable beforehand. We achieve the same by extending the\nlock-in probability (i.e. the probability of convergence to a specific\nattractor of the limiting o.d.e. given that the iterates are in its domain of\nattraction after a sufficiently large number of iterations (say) n_0) framework\nto such recursions. We use these results to prove almost sure convergence of\nthe iterates to the specified attractor when the iterates satisfy an\n\"asymptotic tightness\" condition. This, in turn, is shown to be useful in\nanalyzing the tracking ability of general \"adaptive\" algorithms. Finally, we\nobtain the first informative error bounds on function approximation for the\npolicy evaluation algorithm proposed by Basu et al. when the aim is to find the\nrisk-sensitive cost represented using exponential utility. We show that this\nhappens due to the absence of difference term in the earlier bound which is\nalways present in all our bounds when the state space is large.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 03:59:21 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Karmakar", "Prasenjit", ""]]}, {"id": "2012.00807", "submitter": "Matthias L\\\"offler", "authors": "Geoffrey Chinot, Matthias L\\\"offler and Sara van de Geer", "title": "On the robustness of minimum-norm interpolators", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.NA math.IT math.NA stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article develops a general theory for minimum-norm interpolated\nestimators in linear models in the presence of additive, potentially\nadversarial, errors. In particular, no conditions on the errors are imposed. A\nquantitative bound for the prediction error is given, relating it to the\nRademacher complexity of the covariates, the norm of the minimum norm\ninterpolator of the errors and the shape of the subdifferential around the true\nparameter. The general theory is illustrated with several examples: the sparse\nlinear model with minimum $\\ell_1$-norm or group Lasso penalty interpolation,\nthe low rank trace regression model with nuclear norm minimization, and minimum\nEuclidean norm interpolation in the linear model. In case of sparsity or\nlow-rank inducing norms, minimum norm interpolation yields a prediction error\nof the order of the average noise level, provided that the overparameterization\nis at least a logarithmic factor larger than the number of samples. Lower\nbounds that show near optimality of the results complement the analysis.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 20:03:20 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 14:37:55 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Chinot", "Geoffrey", ""], ["L\u00f6ffler", "Matthias", ""], ["van de Geer", "Sara", ""]]}, {"id": "2012.00898", "submitter": "Zhe Liu", "authors": "Zhe Liu, Fuchun Peng", "title": "Federated Marginal Personalization for ASR Rescoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce federated marginal personalization (FMP), a novel method for\ncontinuously updating personalized neural network language models (NNLMs) on\nprivate devices using federated learning (FL). Instead of fine-tuning the\nparameters of NNLMs on personal data, FMP regularly estimates global and\npersonalized marginal distributions of words, and adjusts the probabilities\nfrom NNLMs by an adaptation factor that is specific to each word. Our presented\napproach can overcome the limitations of federated fine-tuning and efficiently\nlearn personalized NNLMs on devices. We study the application of FMP on\nsecond-pass ASR rescoring tasks. Experiments on two speech evaluation datasets\nshow modest word error rate (WER) reductions. We also demonstrate that FMP\ncould offer reasonable privacy with only a negligible cost in speech\nrecognition accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 23:54:41 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Liu", "Zhe", ""], ["Peng", "Fuchun", ""]]}, {"id": "2012.01012", "submitter": "J. Emmanuel Johnson", "authors": "J. Emmanuel Johnson, Valero Laparra, Gustau Camps-Valls, Raul\n  Santos-Rodr\\'iguez, Jes\\'us Malo", "title": "Information Theory in Density Destructors", "comments": "Accepted at the Workshop on Invertible Neural Nets and Normalizing\n  Flows, ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Density destructors are differentiable and invertible transforms that map\nmultivariate PDFs of arbitrary structure (low entropy) into non-structured PDFs\n(maximum entropy). Multivariate Gaussianization and multivariate equalization\nare specific examples of this family, which break down the complexity of the\noriginal PDF through a set of elementary transforms that progressively remove\nthe structure of the data. We demonstrate how this property of density\ndestructive flows is connected to classical information theory, and how density\ndestructors can be used to get more accurate estimates of information theoretic\nquantities. Experiments with total correlation and mutual information\ninmultivariate sets illustrate the ability of density destructors compared to\ncompeting methods. These results suggest that information theoretic measures\nmay be an alternative optimization criteria when learning density destructive\nflows.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 08:04:53 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Johnson", "J. Emmanuel", ""], ["Laparra", "Valero", ""], ["Camps-Valls", "Gustau", ""], ["Santos-Rodr\u00edguez", "Raul", ""], ["Malo", "Jes\u00fas", ""]]}, {"id": "2012.01064", "submitter": "Stephane Gaiffas Pr", "authors": "Ibrahim Merad and Yiyang Yu and Emmanuel Bacry and St\\'ephane\n  Ga\\\"iffas", "title": "About contrastive unsupervised representation learning for\n  classification and its convergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive representation learning has been recently proved to be very\nefficient for self-supervised training. These methods have been successfully\nused to train encoders which perform comparably to supervised training on\ndownstream classification tasks. A few works have started to build a\ntheoretical framework around contrastive learning in which guarantees for its\nperformance can be proven. We provide extensions of these results to training\nwith multiple negative samples and for multiway classification. Furthermore, we\nprovide convergence guarantees for the minimization of the contrastive training\nerror with gradient descent of an overparametrized deep neural encoder, and\nprovide some numerical experiments that complement our theoretical findings\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 10:08:57 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Merad", "Ibrahim", ""], ["Yu", "Yiyang", ""], ["Bacry", "Emmanuel", ""], ["Ga\u00efffas", "St\u00e9phane", ""]]}, {"id": "2012.01088", "submitter": "Rohit Kannan", "authors": "Rohit Kannan, G\\\"uzin Bayraksan, James R. Luedtke", "title": "Residuals-based distributionally robust optimization with covariate\n  information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider data-driven approaches that integrate a machine learning\nprediction model within distributionally robust optimization (DRO) given\nlimited joint observations of uncertain parameters and covariates. Our\nframework is flexible in the sense that it can accommodate a variety of\nlearning setups and DRO ambiguity sets. We investigate the asymptotic and\nfinite sample properties of solutions obtained using Wasserstein, sample robust\noptimization, and phi-divergence-based ambiguity sets within our DRO\nformulations, and explore cross-validation approaches for sizing these\nambiguity sets. Through numerical experiments, we validate our theoretical\nresults, study the effectiveness of our approaches for sizing ambiguity sets,\nand illustrate the benefits of our DRO formulations in the limited data regime\neven when the prediction model is misspecified.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 11:21:34 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Kannan", "Rohit", ""], ["Bayraksan", "G\u00fczin", ""], ["Luedtke", "James R.", ""]]}, {"id": "2012.01089", "submitter": "Andr\\'es Hoyos-Idrobo", "authors": "Andr\\'es Hoyos-Idrobo", "title": "Aligning Hyperbolic Representations: an Optimal Transport-based approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Hyperbolic-spaces are better suited to represent data with underlying\nhierarchical relationships, e.g., tree-like data. However, it is often\nnecessary to incorporate, through alignment, different but related\nrepresentations meaningfully. This aligning is an important class of machine\nlearning problems, with applications as ontology matching and cross-lingual\nalignment. Optimal transport (OT)-based approaches are a natural choice to\ntackle the alignment problem as they aim to find a transformation of the source\ndataset to match a target dataset, subject to some distribution constraints.\nThis work proposes a novel approach based on OT of embeddings on the Poincar\\'e\nmodel of hyperbolic spaces. Our method relies on the gyrobarycenter mapping on\nM\\\"obius gyrovector spaces. As a result of this formalism, we derive extensions\nto some existing Euclidean methods of OT-based domain adaptation to their\nhyperbolic counterparts. Empirically, we show that both Euclidean and\nhyperbolic methods have similar performances in the context of retrieval.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 11:22:19 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Hoyos-Idrobo", "Andr\u00e9s", ""]]}, {"id": "2012.01185", "submitter": "Alois Pichler", "authors": "Kipngeno Benard Kirui, Georg Ch. Pflug, Alois Pichler", "title": "New Algorithms And Fast Implementations To Approximate Stochastic\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present new algorithms and fast implementations to find efficient\napproximations for modelling stochastic processes. For many numerical\ncomputations it is essential to develop finite approximations for stochastic\nprocesses. While the goal is always to find a finite model, which represents a\ngiven knowledge about the real data process as accurate as possible, the ways\nof estimating the discrete approximating model may be quite different: (i) if\nthe stochastic model is known as a solution of a stochastic differential\nequation, e.g., one may generate the scenario tree directly from the specified\nmodel; (ii) if a simulation algorithm is available, which allows simulating\ntrajectories from all conditional distributions, a scenario tree can be\ngenerated by stochastic approximation; (iii) if only some observed trajectories\nof the scenario process are available, the construction of the approximating\nprocess can be based on non-parametric conditional density estimates.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 06:14:16 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Kirui", "Kipngeno Benard", ""], ["Pflug", "Georg Ch.", ""], ["Pichler", "Alois", ""]]}, {"id": "2012.01194", "submitter": "Ariel Neufeld", "authors": "Christian Beck, Sebastian Becker, Patrick Cheridito, Arnulf Jentzen,\n  Ariel Neufeld", "title": "Deep learning based numerical approximation algorithms for stochastic\n  partial differential equations and high-dimensional nonlinear filtering\n  problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we introduce and study a deep learning based approximation\nalgorithm for solutions of stochastic partial differential equations (SPDEs).\nIn the proposed approximation algorithm we employ a deep neural network for\nevery realization of the driving noise process of the SPDE to approximate the\nsolution process of the SPDE under consideration. We test the performance of\nthe proposed approximation algorithm in the case of stochastic heat equations\nwith additive noise, stochastic heat equations with multiplicative noise,\nstochastic Black--Scholes equations with multiplicative noise, and Zakai\nequations from nonlinear filtering. In each of these SPDEs the proposed\napproximation algorithm produces accurate results with short run times in up to\n50 space dimensions.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 13:25:35 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Beck", "Christian", ""], ["Becker", "Sebastian", ""], ["Cheridito", "Patrick", ""], ["Jentzen", "Arnulf", ""], ["Neufeld", "Ariel", ""]]}, {"id": "2012.01205", "submitter": "Angelos Chatzimparmpas", "authors": "Angelos Chatzimparmpas, Rafael M. Martins, Kostiantyn Kucher, Andreas\n  Kerren", "title": "VisEvol: Visual Analytics to Support Hyperparameter Search through\n  Evolutionary Optimization", "comments": "This manuscript is accepted for publication in a special issue of\n  Computer Graphics Forum (CGF)", "journal-ref": "Computer Graphics Forum 2021, 40(3), 201-214", "doi": "10.1111/cgf.14300", "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the training phase of machine learning (ML) models, it is usually\nnecessary to configure several hyperparameters. This process is computationally\nintensive and requires an extensive search to infer the best hyperparameter set\nfor the given problem. The challenge is exacerbated by the fact that most ML\nmodels are complex internally, and training involves trial-and-error processes\nthat could remarkably affect the predictive result. Moreover, each\nhyperparameter of an ML algorithm is potentially intertwined with the others,\nand changing it might result in unforeseeable impacts on the remaining\nhyperparameters. Evolutionary optimization is a promising method to try and\naddress those issues. According to this method, performant models are stored,\nwhile the remainder are improved through crossover and mutation processes\ninspired by genetic algorithms. We present VisEvol, a visual analytics tool\nthat supports interactive exploration of hyperparameters and intervention in\nthis evolutionary procedure. In summary, our proposed tool helps the user to\ngenerate new models through evolution and eventually explore powerful\nhyperparameter combinations in diverse regions of the extensive hyperparameter\nspace. The outcome is a voting ensemble (with equal rights) that boosts the\nfinal predictive performance. The utility and applicability of VisEvol are\ndemonstrated with two use cases and interviews with ML experts who evaluated\nthe effectiveness of the tool.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 13:43:37 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2021 18:42:07 GMT"}, {"version": "v3", "created": "Sat, 27 Mar 2021 04:37:57 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Chatzimparmpas", "Angelos", ""], ["Martins", "Rafael M.", ""], ["Kucher", "Kostiantyn", ""], ["Kerren", "Andreas", ""]]}, {"id": "2012.01293", "submitter": "William Knauth", "authors": "William Knauth", "title": "The Self-Simplifying Machine: Exploiting the Structure of Piecewise\n  Linear Neural Networks to Create Interpretable Models", "comments": "38 pages, 32 figures, appendices A, B", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, it is more important than ever before for users to have trust in the\nmodels they use. As Machine Learning models fall under increased regulatory\nscrutiny and begin to see more applications in high-stakes situations, it\nbecomes critical to explain our models. Piecewise Linear Neural Networks (PLNN)\nwith the ReLU activation function have quickly become extremely popular models\ndue to many appealing properties; however, they still present many challenges\nin the areas of robustness and interpretation. To this end, we introduce novel\nmethodology toward simplification and increased interpretability of Piecewise\nLinear Neural Networks for classification tasks. Our methods include the use of\na trained, deep network to produce a well-performing, single-hidden-layer\nnetwork without further stochastic training, in addition to an algorithm to\nreduce flat networks to a smaller, more interpretable size with minimal loss in\nperformance. On these methods, we conduct preliminary studies of model\nperformance, as well as a case study on Wells Fargo's Home Lending dataset,\ntogether with visual model interpretation.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 16:02:14 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Knauth", "William", ""]]}, {"id": "2012.01349", "submitter": "Abhinav Prakash", "authors": "Abhinav Prakash, Rui Tuo and Yu Ding", "title": "The temporal overfitting problem with applications in wind power curve\n  modeling", "comments": "30 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with a nonparametric regression problem in which the\nindependence assumption of the input variables and the residuals is no longer\nvalid. Using existing model selection methods, like cross validation, the\npresence of temporal autocorrelation in the input variables and the error terms\nleads to model overfitting. This phenomenon is referred to as temporal\noverfitting, which causes loss of performance while predicting responses for a\ntime domain different from the training time domain. We propose a new method to\ntackle the temporal overfitting problem. Our nonparametric model is partitioned\ninto two parts -- a time-invariant component and a time-varying component, each\nof which is modeled through a Gaussian process regression. The key in our\ninference is a thinning-based strategy, an idea borrowed from Markov chain\nMonte Carlo sampling, to estimate the two components, respectively. Our\nspecific application in this paper targets the power curve modeling in wind\nenergy. In our numerical studies, we compare extensively our proposed method\nwith both existing power curve models and available ideas for handling temporal\noverfitting. Our approach yields significant improvement in prediction both in\nand outside the time domain covered by the training data.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 17:39:57 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Prakash", "Abhinav", ""], ["Tuo", "Rui", ""], ["Ding", "Yu", ""]]}, {"id": "2012.01474", "submitter": "Stefan Vlaski", "authors": "Stefan Vlaski, Elsa Rizk, Ali H. Sayed", "title": "Second-Order Guarantees in Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA eess.SP math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is a useful framework for centralized learning from\ndistributed data under practical considerations of heterogeneity, asynchrony,\nand privacy. Federated architectures are frequently deployed in deep learning\nsettings, which generally give rise to non-convex optimization problems.\nNevertheless, most existing analysis are either limited to convex loss\nfunctions, or only establish first-order stationarity, despite the fact that\nsaddle-points, which are first-order stationary, are known to pose bottlenecks\nin deep learning. We draw on recent results on the second-order optimality of\nstochastic gradient algorithms in centralized and decentralized settings, and\nestablish second-order guarantees for a class of federated learning algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 19:30:08 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Vlaski", "Stefan", ""], ["Rizk", "Elsa", ""], ["Sayed", "Ali H.", ""]]}, {"id": "2012.01536", "submitter": "Ian Covert", "authors": "Ian Covert, Su-In Lee", "title": "Improving KernelSHAP: Practical Shapley Value Estimation via Linear\n  Regression", "comments": "AISTATS 2021 Camera Ready (fixed supplement)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Shapley value concept from cooperative game theory has become a popular\ntechnique for interpreting ML models, but efficiently estimating these values\nremains challenging, particularly in the model-agnostic setting. Here, we\nrevisit the idea of estimating Shapley values via linear regression to\nunderstand and improve upon this approach. By analyzing the original KernelSHAP\nalongside a newly proposed unbiased version, we develop techniques to detect\nits convergence and calculate uncertainty estimates. We also find that the\noriginal version incurs a negligible increase in bias in exchange for\nsignificantly lower variance, and we propose a variance reduction technique\nthat further accelerates the convergence of both estimators. Finally, we\ndevelop a version of KernelSHAP for stochastic cooperative games that yields\nfast new estimators for two global explanation methods.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 21:20:47 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 23:33:41 GMT"}, {"version": "v3", "created": "Fri, 23 Apr 2021 01:33:31 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Covert", "Ian", ""], ["Lee", "Su-In", ""]]}, {"id": "2012.01559", "submitter": "Hongyu Guo", "authors": "Hongyu Guo", "title": "Regularization via Adaptive Pairwise Label Smoothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Label Smoothing (LS) is an effective regularizer to improve the\ngeneralization of state-of-the-art deep models. For each training sample the LS\nstrategy smooths the one-hot encoded training signal by distributing its\ndistribution mass over the non ground-truth classes, aiming to penalize the\nnetworks from generating overconfident output distributions. This paper\nintroduces a novel label smoothing technique called Pairwise Label Smoothing\n(PLS). The PLS takes a pair of samples as input. Smoothing with a pair of\nground-truth labels enables the PLS to preserve the relative distance between\nthe two truth labels while further soften that between the truth labels and the\nother targets, resulting in models producing much less confident predictions\nthan the LS strategy. Also, unlike current LS methods, which typically require\nto find a global smoothing distribution mass through cross-validation search,\nPLS automatically learns the distribution mass for each input pair during\ntraining. We empirically show that PLS significantly outperforms LS and the\nbaseline models, achieving up to 30% of relative classification error\nreduction. We also visually show that when achieving such accuracy gains the\nPLS tends to produce very low winning softmax scores.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 22:08:10 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Guo", "Hongyu", ""]]}, {"id": "2012.01598", "submitter": "Bjorn Lutjens", "authors": "Salva R\\\"uhling Cachay, Emma Erickson, Arthur Fender C. Bucker, Ernest\n  Pokropek, Willa Potosnak, Salomey Osei, Bj\\\"orn L\\\"utjens", "title": "Graph Neural Networks for Improved El Ni\\~no Forecasting", "comments": "Presented at the NeurIPS 2020 Workshop on Tackling Climate Change\n  with Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.ao-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based models have recently outperformed state-of-the-art\nseasonal forecasting models, such as for predicting El Ni\\~no-Southern\nOscillation (ENSO). However, current deep learning models are based on\nconvolutional neural networks which are difficult to interpret and can fail to\nmodel large-scale atmospheric patterns called teleconnections. Hence, we\npropose the application of spatiotemporal Graph Neural Networks (GNN) to\nforecast ENSO at long lead times, finer granularity and improved predictive\nskill than current state-of-the-art methods. The explicit modeling of\ninformation flow via edges may also allow for more interpretable forecasts.\nPreliminary results are promising and outperform state-of-the art systems for\nprojections 1 and 3 months ahead.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 23:40:53 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 19:13:37 GMT"}, {"version": "v3", "created": "Fri, 12 Feb 2021 11:38:16 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Cachay", "Salva R\u00fchling", ""], ["Erickson", "Emma", ""], ["Bucker", "Arthur Fender C.", ""], ["Pokropek", "Ernest", ""], ["Potosnak", "Willa", ""], ["Osei", "Salomey", ""], ["L\u00fctjens", "Bj\u00f6rn", ""]]}, {"id": "2012.01643", "submitter": "Yanfei Kang", "authors": "Yanfei Kang, Wei Cao, Fotios Petropoulos, Feng Li", "title": "Forecast with Forecasts: Diversity Matters", "comments": "23 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Forecast combination has been widely applied in the last few decades to\nimprove forecast accuracy. In recent years, the idea of using time series\nfeatures to construct forecast combination model has flourished in the\nforecasting area. Although this idea has been proved to be beneficial in\nseveral forecast competitions such as the M3 and M4 competitions, it may not be\npractical in many situations. For example, the task of selecting appropriate\nfeatures to build forecasting models can be a big challenge for many\nresearchers. Even if there is one acceptable way to define the features,\nexisting features are estimated based on the historical patterns, which are\ndoomed to change in the future, or infeasible in the case of limited historical\ndata. In this work, we suggest a change of focus from the historical data to\nthe produced forecasts to extract features. We calculate the diversity of a\npool of models based on the corresponding forecasts as a decisive feature and\nuse meta-learning to construct diversity-based forecast combination models. A\nrich set of time series are used to evaluate the performance of the proposed\nmethod. Experimental results show that our diversity-based forecast combination\nframework not only simplifies the modelling process but also achieves superior\nforecasting performance.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 02:14:02 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 12:09:43 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Kang", "Yanfei", ""], ["Cao", "Wei", ""], ["Petropoulos", "Fotios", ""], ["Li", "Feng", ""]]}, {"id": "2012.01668", "submitter": "Yuantong Li", "authors": "Yuantong Li, Chi-hua Wang, Guang Cheng", "title": "Online Forgetting Process for Linear Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the EU's \"Right To Be Forgotten\" regulation, we initiate a study\nof statistical data deletion problems where users' data are accessible only for\na limited period of time. This setting is formulated as an online supervised\nlearning task with \\textit{constant memory limit}. We propose a deletion-aware\nalgorithm \\texttt{FIFD-OLS} for the low dimensional case, and witness a\ncatastrophic rank swinging phenomenon due to the data deletion operation, which\nleads to statistical inefficiency. As a remedy, we propose the\n\\texttt{FIFD-Adaptive Ridge} algorithm with a novel online regularization\nscheme, that effectively offsets the uncertainty from deletion. In theory, we\nprovide the cumulative regret upper bound for both online forgetting\nalgorithms. In the experiment, we showed \\texttt{FIFD-Adaptive Ridge}\noutperforms the ridge regression algorithm with fixed regularization level, and\nhopefully sheds some light on more complex statistical models.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 02:57:53 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Li", "Yuantong", ""], ["Wang", "Chi-hua", ""], ["Cheng", "Guang", ""]]}, {"id": "2012.01696", "submitter": "Yuji Roh", "authors": "Yuji Roh, Kangwook Lee, Steven Euijong Whang, Changho Suh", "title": "FairBatch: Batch Selection for Model Fairness", "comments": "In Proceedings of the 9th International Conference on Learning\n  Representations (ICLR), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a fair machine learning model is essential to prevent demographic\ndisparity. Existing techniques for improving model fairness require broad\nchanges in either data preprocessing or model training, rendering themselves\ndifficult-to-adopt for potentially already complex machine learning systems. We\naddress this problem via the lens of bilevel optimization. While keeping the\nstandard training algorithm as an inner optimizer, we incorporate an outer\noptimizer so as to equip the inner problem with an additional functionality:\nAdaptively selecting minibatch sizes for the purpose of improving model\nfairness. Our batch selection algorithm, which we call FairBatch, implements\nthis optimization and supports prominent fairness measures: equal opportunity,\nequalized odds, and demographic parity. FairBatch comes with a significant\nimplementation benefit -- it does not require any modification to data\npreprocessing or model training. For instance, a single-line change of PyTorch\ncode for replacing batch selection part of model training suffices to employ\nFairBatch. Our experiments conducted both on synthetic and benchmark real data\ndemonstrate that FairBatch can provide such functionalities while achieving\ncomparable (or even greater) performances against the state of the arts.\nFurthermore, FairBatch can readily improve fairness of any pre-trained model\nsimply via fine-tuning. It is also compatible with existing batch selection\ntechniques intended for different purposes, such as faster convergence, thus\ngracefully achieving multiple purposes.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 04:36:04 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 14:55:19 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Roh", "Yuji", ""], ["Lee", "Kangwook", ""], ["Whang", "Steven Euijong", ""], ["Suh", "Changho", ""]]}, {"id": "2012.01705", "submitter": "Kush Bhatia", "authors": "Kush Bhatia, Karthik Sridharan", "title": "Online learning with dynamics: A minimax perspective", "comments": "Published at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of online learning with dynamics, where a learner\ninteracts with a stateful environment over multiple rounds. In each round of\nthe interaction, the learner selects a policy to deploy and incurs a cost that\ndepends on both the chosen policy and current state of the world. The\nstate-evolution dynamics and the costs are allowed to be time-varying, in a\npossibly adversarial way. In this setting, we study the problem of minimizing\npolicy regret and provide non-constructive upper bounds on the minimax rate for\nthe problem.\n  Our main results provide sufficient conditions for online learnability for\nthis setup with corresponding rates. The rates are characterized by 1) a\ncomplexity term capturing the expressiveness of the underlying policy class\nunder the dynamics of state change, and 2) a dynamics stability term measuring\nthe deviation of the instantaneous loss from a certain counterfactual loss.\nFurther, we provide matching lower bounds which show that both the complexity\nterms are indeed necessary.\n  Our approach provides a unifying analysis that recovers regret bounds for\nseveral well studied problems including online learning with memory, online\ncontrol of linear quadratic regulators, online Markov decision processes, and\ntracking adversarial targets. In addition, we show how our tools help obtain\ntight regret bounds for a new problems (with non-linear dynamics and non-convex\nlosses) for which such bounds were not known prior to our work.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 05:06:08 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Bhatia", "Kush", ""], ["Sridharan", "Karthik", ""]]}, {"id": "2012.01744", "submitter": "Antoine Dedieu", "authors": "Antoine Dedieu, Miguel L\\'azaro-Gredilla, Dileep George", "title": "Sample-Efficient L0-L2 Constrained Structure Learning of Sparse Ising\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning the underlying graph of a sparse Ising\nmodel with $p$ nodes from $n$ i.i.d. samples. The most recent and best\nperforming approaches combine an empirical loss (the logistic regression loss\nor the interaction screening loss) with a regularizer (an L1 penalty or an L1\nconstraint). This results in a convex problem that can be solved separately for\neach node of the graph. In this work, we leverage the cardinality constraint L0\nnorm, which is known to properly induce sparsity, and further combine it with\nan L2 norm to better model the non-zero coefficients. We show that our proposed\nestimators achieve an improved sample complexity, both (a) theoretically, by\nreaching new state-of-the-art upper bounds for recovery guarantees, and (b)\nempirically, by showing sharper phase transitions between poor and full\nrecovery for graph topologies studied in the literature, when compared to their\nL1-based state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 07:52:20 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 23:08:24 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 22:34:14 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Dedieu", "Antoine", ""], ["L\u00e1zaro-Gredilla", "Miguel", ""], ["George", "Dileep", ""]]}, {"id": "2012.01780", "submitter": "Quanquan Gu", "authors": "Pan Xu and Zheng Wen and Handong Zhao and Quanquan Gu", "title": "Neural Contextual Bandits with Deep Representation and Shallow\n  Exploration", "comments": "28 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a general class of contextual bandits, where each context-action\npair is associated with a raw feature vector, but the reward generating\nfunction is unknown. We propose a novel learning algorithm that transforms the\nraw feature vector using the last hidden layer of a deep ReLU neural network\n(deep representation learning), and uses an upper confidence bound (UCB)\napproach to explore in the last linear layer (shallow exploration). We prove\nthat under standard assumptions, our proposed algorithm achieves\n$\\tilde{O}(\\sqrt{T})$ finite-time regret, where $T$ is the learning time\nhorizon. Compared with existing neural contextual bandit algorithms, our\napproach is computationally much more efficient since it only needs to explore\nin the last layer of the deep neural network.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 09:17:55 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Xu", "Pan", ""], ["Wen", "Zheng", ""], ["Zhao", "Handong", ""], ["Gu", "Quanquan", ""]]}, {"id": "2012.01929", "submitter": "Gersende Fort", "authors": "Gersende Fort (IMT), Eric Moulines (X-DEP-MATHAPP), Hoi-To Wai", "title": "A Stochastic Path-Integrated Differential EstimatoR Expectation\n  Maximization Algorithm", "comments": null, "journal-ref": "Proceedings of the Conference on Neural Information Processing\n  Systems (NeurIPS 2020), 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Expectation Maximization (EM) algorithm is of key importance for\ninference in latent variable models including mixture of regressors and\nexperts, missing observations. This paper introduces a novel EM algorithm,\ncalled \\texttt{SPIDER-EM}, for inference from a training set of size $n$, $n\n\\gg 1$. At the core of our algorithm is an estimator of the full conditional\nexpectation in the {\\sf E}-step, adapted from the stochastic path-integrated\ndifferential estimator ({\\tt SPIDER}) technique. We derive finite-time\ncomplexity bounds for smooth non-convex likelihood: we show that for\nconvergence to an $\\epsilon$-approximate stationary point, the complexity\nscales as $K_{\\operatorname{Opt}} (n,\\epsilon )={\\cal O}(\\epsilon^{-1})$ and\n$K_{\\operatorname{CE}}( n,\\epsilon ) = n+ \\sqrt{n} {\\cal O}(\\epsilon^{-1} )$,\nwhere $K_{\\operatorname{Opt}}( n,\\epsilon )$ and $K_{\\operatorname{CE}}(n,\n\\epsilon )$ are respectively the number of {\\sf M}-steps and the number of\nper-sample conditional expectations evaluations. This improves over the\nstate-of-the-art algorithms. Numerical results support our findings.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 15:49:31 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Fort", "Gersende", "", "IMT"], ["Moulines", "Eric", "", "X-DEP-MATHAPP"], ["Wai", "Hoi-To", ""]]}, {"id": "2012.01978", "submitter": "Jaron Sanders", "authors": "Albert Senen-Cerda, Jaron Sanders", "title": "Asymptotic convergence rate of Dropout on shallow linear neural networks", "comments": "45 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the convergence rate of gradient flows on objective functions\ninduced by Dropout and Dropconnect, when applying them to shallow linear Neural\nNetworks (NNs) - which can also be viewed as doing matrix factorization using a\nparticular regularizer. Dropout algorithms such as these are thus\nregularization techniques that use 0,1-valued random variables to filter\nweights during training in order to avoid coadaptation of features. By\nleveraging a recent result on nonconvex optimization and conducting a careful\nanalysis of the set of minimizers as well as the Hessian of the loss function,\nwe are able to obtain (i) a local convergence proof of the gradient flow and\n(ii) a bound on the convergence rate that depends on the data, the dropout\nprobability, and the width of the NN. Finally, we compare this theoretical\nbound to numerical simulations, which are in qualitative agreement with the\nconvergence bound and match it when starting sufficiently close to a minimizer.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 19:02:37 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Senen-Cerda", "Albert", ""], ["Sanders", "Jaron", ""]]}, {"id": "2012.02006", "submitter": "Shenghua Liu", "authors": "Jiabao Zhang, Shenghua Liu, Wenting Hou, Siddharth Bhatia, Huawei\n  Shen, Wenjian Yu, Xueqi Cheng", "title": "AugSplicing: Synchronized Behavior Detection in Streaming Tensors", "comments": "AAAI Conference on Artificial Intelligence (AAAI), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG cs.SI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How can we track synchronized behavior in a stream of time-stamped tuples,\nsuch as mobile devices installing and uninstalling applications in the\nlockstep, to boost their ranks in the app store? We model such tuples as\nentries in a streaming tensor, which augments attribute sizes in its modes over\ntime. Synchronized behavior tends to form dense blocks (i.e. subtensors) in\nsuch a tensor, signaling anomalous behavior, or interesting communities.\nHowever, existing dense block detection methods are either based on a static\ntensor, or lack an efficient algorithm in a streaming setting. Therefore, we\npropose a fast streaming algorithm, AugSplicing, which can detect the top dense\nblocks by incrementally splicing the previous detection with the incoming ones\nin new tuples, avoiding re-runs over all the history data at every tracking\ntime step. AugSplicing is based on a splicing condition that guides the\nalgorithm (Section 4). Compared to the state-of-the-art methods, our method is\n(1) effective to detect fraudulent behavior in installing data of real-world\napps and find a synchronized group of students with interesting features in\ncampus Wi-Fi data; (2) robust with splicing theory for dense block detection;\n(3) streaming and faster than the existing streaming algorithm, with closely\ncomparable accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 15:39:58 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2020 06:48:30 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2020 11:17:10 GMT"}, {"version": "v4", "created": "Tue, 5 Jan 2021 01:53:49 GMT"}, {"version": "v5", "created": "Tue, 30 Mar 2021 14:42:58 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Zhang", "Jiabao", ""], ["Liu", "Shenghua", ""], ["Hou", "Wenting", ""], ["Bhatia", "Siddharth", ""], ["Shen", "Huawei", ""], ["Yu", "Wenjian", ""], ["Cheng", "Xueqi", ""]]}, {"id": "2012.02026", "submitter": "ANtoine Marot", "authors": "Antoine Marot, Alexandre Rozier, Matthieu Dussartre, Laure\n  Crochepierre, Benjamin Donnot", "title": "Towards an AI assistant for human grid operators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Power systems are becoming more complex to operate in the digital age. As a\nresult, real-time decision-making is getting more challenging as the human\noperator has to deal with more information, more uncertainty, more applications\nand more coordination. While supervision has been primarily used to help them\nmake decisions over the last decades, it cannot reasonably scale up anymore.\nThere is a great need for rethinking the human-machine interface under more\nunified and interactive frameworks. Taking advantage of the latest developments\nin Human-machine Interactions and Artificial intelligence, we share the vision\nof a new assistant framework relying on an hypervision interface and greater\nbidirectional interactions. We review the known principles of decision-making\nthat drives the assistant design and supporting assistance functions we\npresent. We finally share some guidelines to make progress towards the\ndevelopment of such an assistant.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 16:12:58 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Marot", "Antoine", ""], ["Rozier", "Alexandre", ""], ["Dussartre", "Matthieu", ""], ["Crochepierre", "Laure", ""], ["Donnot", "Benjamin", ""]]}, {"id": "2012.02035", "submitter": "David Pfau", "authors": "David Pfau, Danilo Rezende", "title": "Integrable Nonparametric Flows", "comments": "Accepted to 3rd NeurIPS Workshop on Machine Learning and Physical\n  Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method for reconstructing an infinitesimal normalizing flow\ngiven only an infinitesimal change to a (possibly unnormalized) probability\ndistribution. This reverses the conventional task of normalizing flows --\nrather than being given samples from a unknown target distribution and learning\na flow that approximates the distribution, we are given a perturbation to an\ninitial distribution and aim to reconstruct a flow that would generate samples\nfrom the known perturbed distribution. While this is an underdetermined\nproblem, we find that choosing the flow to be an integrable vector field yields\na solution closely related to electrostatics, and a solution can be computed by\nthe method of Green's functions. Unlike conventional normalizing flows, this\nflow can be represented in an entirely nonparametric manner. We validate this\nderivation on low-dimensional problems, and discuss potential applications to\nproblems in quantum Monte Carlo and machine learning.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 16:19:52 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Pfau", "David", ""], ["Rezende", "Danilo", ""]]}, {"id": "2012.02081", "submitter": "Zhongzheng Xiong", "authors": "Zhongzheng Xiong, Zengfeng Huang, Xiaojun Mao, Jian Wang, Shan Ying", "title": "Compressive Privatization: Sparse Distribution Estimation under Locally\n  Differentially Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of discrete distribution estimation under locally\ndifferential privacy. Distribution estimation is one of the most fundamental\nestimation problems, which is widely studied in both non-private and private\nsettings. In the local model, private mechanisms with provably optimal sample\ncomplexity are known. However, they are optimal only in the worst-case sense;\ntheir sample complexity is proportional to the size of the entire universe,\nwhich could be huge in practice (e.g., all IP addresses). We show that as long\nas the target distribution is sparse or approximately sparse (e.g., highly\nskewed), the number of samples needed could be significantly reduced. The\nsample complexity of our new mechanism is characterized by the sparsity of the\ntarget distribution and only weakly depends on the size the universe. Our\nmechanism does privatization and dimensionality reduction simultaneously, and\nthe sample complexity will only depend on the reduced dimensionality. The\noriginal distribution is then recovered using tools from compressive sensing.\nTo complement our theoretical results, we conduct experimental studies, the\nresults of which clearly demonstrate the advantages of our method and confirm\nour theoretical findings.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 17:14:23 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Xiong", "Zhongzheng", ""], ["Huang", "Zengfeng", ""], ["Mao", "Xiaojun", ""], ["Wang", "Jian", ""], ["Ying", "Shan", ""]]}, {"id": "2012.02119", "submitter": "Ainesh Bakshi", "authors": "Ainesh Bakshi, Ilias Diakonikolas, He Jia, Daniel M. Kane, Pravesh K.\n  Kothari and Santosh S. Vempala", "title": "Robustly Learning Mixtures of $k$ Arbitrary Gaussians", "comments": "This version extends the previous one to yield 1) robust proper\n  learning algorithm with poly(eps) error and 2) an information theoretic\n  argument proving that the same algorithms in fact also yield parameter\n  recovery guarantees. The updates are included in Sections 7,8, and 9 and the\n  main result from the previous version (Thm 1.4) is presented and proved in\n  Section 6", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give a polynomial-time algorithm for the problem of robustly estimating a\nmixture of $k$ arbitrary Gaussians in $\\mathbb{R}^d$, for any fixed $k$, in the\npresence of a constant fraction of arbitrary corruptions. This resolves the\nmain open problem in several previous works on algorithmic robust statistics,\nwhich addressed the special cases of robustly estimating (a) a single Gaussian,\n(b) a mixture of TV-distance separated Gaussians, and (c) a uniform mixture of\ntwo Gaussians. Our main tools are an efficient \\emph{partial clustering}\nalgorithm that relies on the sum-of-squares method, and a novel \\emph{tensor\ndecomposition} algorithm that allows errors in both Frobenius norm and low-rank\nterms.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 17:54:03 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2020 17:24:52 GMT"}, {"version": "v3", "created": "Mon, 7 Jun 2021 16:26:50 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Bakshi", "Ainesh", ""], ["Diakonikolas", "Ilias", ""], ["Jia", "He", ""], ["Kane", "Daniel M.", ""], ["Kothari", "Pravesh K.", ""], ["Vempala", "Santosh S.", ""]]}, {"id": "2012.02125", "submitter": "Vidya Muthukumar", "authors": "Vidya Muthukumar, Soham Phade, Anant Sahai", "title": "On the Impossibility of Convergence of Mixed Strategies with No Regret\n  Learning", "comments": "44 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study convergence properties of the mixed strategies that result from a\ngeneral class of optimal no regret learning strategies in a repeated game\nsetting where the stage game is any 2 by 2 competitive game (i.e. game for\nwhich all the Nash equilibria (NE) of the game are completely mixed). We\nconsider the class of strategies whose information set at each step is the\nempirical average of the opponent's realized play (and the step number), that\nwe call mean based strategies. We first show that there does not exist any\noptimal no regret, mean based strategy for player 1 that would result in the\nconvergence of her mixed strategies (in probability) against an opponent that\nplays his Nash equilibrium mixed strategy at each step. Next, we show that this\nlast iterate divergence necessarily occurs if player 2 uses any adaptive\nstrategy with a minimal randomness property. This property is satisfied, for\nexample, by any fixed sequence of mixed strategies for player 2 that converges\nto NE. We conjecture that this property holds when both players use optimal no\nregret learning strategies against each other, leading to the divergence of the\nmixed strategies with a positive probability. Finally, we show that variants of\nmean based strategies using recency bias, which have yielded last iterate\nconvergence in deterministic min max optimization, continue to lead to this\nlast iterate divergence. This demonstrates a crucial difference in outcomes\nbetween using the opponent's mixtures and realizations to make strategy\nupdates.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 18:02:40 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Muthukumar", "Vidya", ""], ["Phade", "Soham", ""], ["Sahai", "Anant", ""]]}, {"id": "2012.02130", "submitter": "Tianfang Zhang", "authors": "Tianfang Zhang and Rasmus Bokrantz and Jimmy Olsson", "title": "A similarity-based Bayesian mixture-of-experts model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new nonparametric mixture-of-experts model for multivariate\nregression problems, inspired by the probabilistic $k$-nearest neighbors\nalgorithm. Using a conditionally specified model, predictions for out-of-sample\ninputs are based on similarities to each observed data point, yielding\npredictive distributions represented by Gaussian mixtures. Posterior inference\nis performed on the parameters of the mixture components as well as the\ndistance metric using a mean-field variational Bayes algorithm accompanied with\na stochastic gradient-based optimization procedure. The proposed method is\nespecially advantageous in settings where inputs are of relatively high\ndimension in comparison to the data size, where input--output relationships are\ncomplex, and where predictive distributions may be skewed or multimodal.\nComputational studies on two synthetic datasets and one dataset comprising dose\nstatistics of radiation therapy treatment plans show that our\nmixture-of-experts method performs similarly or better than a conditional\nDirichlet process mixture model both in terms of validation metrics and visual\ninspection.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 18:08:30 GMT"}, {"version": "v2", "created": "Sun, 9 May 2021 10:14:23 GMT"}, {"version": "v3", "created": "Sun, 4 Jul 2021 12:33:04 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Zhang", "Tianfang", ""], ["Bokrantz", "Rasmus", ""], ["Olsson", "Jimmy", ""]]}, {"id": "2012.02160", "submitter": "Brian Kim", "authors": "Brian Kim and Yalin E. Sagduyu and Tugba Erpek and Kemal Davaslioglu\n  and Sennur Ulukus", "title": "Channel Effects on Surrogate Models of Adversarial Attacks against\n  Wireless Signal Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a wireless communication system that consists of a background\nemitter, a transmitter, and an adversary. The transmitter is equipped with a\ndeep neural network (DNN) classifier for detecting the ongoing transmissions\nfrom the background emitter and transmits a signal if the spectrum is idle.\nConcurrently, the adversary trains its own DNN classifier as the surrogate\nmodel by observing the spectrum to detect the ongoing transmissions of the\nbackground emitter and generate adversarial attacks to fool the transmitter\ninto misclassifying the channel as idle. This surrogate model may differ from\nthe transmitter's classifier significantly because the adversary and the\ntransmitter experience different channels from the background emitter and\ntherefore their classifiers are trained with different distributions of inputs.\nThis system model may represent a setting where the background emitter is a\nprimary user, the transmitter is a secondary user, and the adversary is trying\nto fool the secondary user to transmit even though the channel is occupied by\nthe primary user. We consider different topologies to investigate how different\nsurrogate models that are trained by the adversary (depending on the\ndifferences in channel effects experienced by the adversary) affect the\nperformance of the adversarial attack. The simulation results show that the\nsurrogate models that are trained with different distributions of\nchannel-induced inputs severely limit the attack performance and indicate that\nthe transferability of adversarial attacks is neither readily available nor\nstraightforward to achieve since surrogate models for wireless applications may\nsignificantly differ from the target model depending on channel effects.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 18:46:28 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 00:01:27 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Kim", "Brian", ""], ["Sagduyu", "Yalin E.", ""], ["Erpek", "Tugba", ""], ["Davaslioglu", "Kemal", ""], ["Ulukus", "Sennur", ""]]}, {"id": "2012.02178", "submitter": "George Atia", "authors": "George K. Atia, Andre Beckus, Ismail Alkhouri, Alvaro Velasquez", "title": "Verifiable Planning in Expected Reward Multichain MDPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The planning domain has experienced increased interest in the formal\nsynthesis of decision-making policies. This formal synthesis typically entails\nfinding a policy which satisfies formal specifications in the form of some\nwell-defined logic, such as Linear Temporal Logic (LTL) or Computation Tree\nLogic (CTL), among others. While such logics are very powerful and expressive\nin their capacity to capture desirable agent behavior, their value is limited\nwhen deriving decision-making policies which satisfy certain types of\nasymptotic behavior. In particular, we are interested in specifying constraints\non the steady-state behavior of an agent, which captures the proportion of time\nan agent spends in each state as it interacts for an indefinite period of time\nwith its environment. This is sometimes called the average or expected behavior\nof the agent. In this paper, we explore the steady-state planning problem of\nderiving a decision-making policy for an agent such that constraints on its\nsteady-state behavior are satisfied. A linear programming solution for the\ngeneral case of multichain Markov Decision Processes (MDPs) is proposed and we\nprove that optimal solutions to the proposed programs yield stationary policies\nwith rigorous guarantees of behavior.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 18:54:24 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Atia", "George K.", ""], ["Beckus", "Andre", ""], ["Alkhouri", "Ismail", ""], ["Velasquez", "Alvaro", ""]]}, {"id": "2012.02295", "submitter": "Da Xu", "authors": "Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, Kannan Achan", "title": "Adversarial Counterfactual Learning and Evaluation for Recommender\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The feedback data of recommender systems are often subject to what was\nexposed to the users; however, most learning and evaluation methods do not\naccount for the underlying exposure mechanism. We first show in theory that\napplying supervised learning to detect user preferences may end up with\ninconsistent results in the absence of exposure information. The counterfactual\npropensity-weighting approach from causal inference can account for the\nexposure mechanism; nevertheless, the partial-observation nature of the\nfeedback data can cause identifiability issues. We propose a principled\nsolution by introducing a minimax empirical risk formulation. We show that the\nrelaxation of the dual problem can be converted to an adversarial game between\ntwo recommendation models, where the opponent of the candidate model\ncharacterizes the underlying exposure mechanism. We provide learning bounds and\nconduct extensive simulation studies to illustrate and justify the proposed\napproach over a broad range of recommendation settings, which shed insights on\nthe various benefits of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 00:40:51 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Xu", "Da", ""], ["Ruan", "Chuanwei", ""], ["Korpeoglu", "Evren", ""], ["Kumar", "Sushant", ""], ["Achan", "Kannan", ""]]}, {"id": "2012.02364", "submitter": "Tharindu Fernando", "authors": "Tharindu Fernando, Harshala Gammulle, Simon Denman, Sridha Sridharan,\n  Clinton Fookes", "title": "Deep Learning for Medical Anomaly Detection -- A Survey", "comments": "Preprint submitted to ACM Computing Surveys", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning-based medical anomaly detection is an important problem that\nhas been extensively studied. Numerous approaches have been proposed across\nvarious medical application domains and we observe several similarities across\nthese distinct applications. Despite this comparability, we observe a lack of\nstructured organisation of these diverse research applications such that their\nadvantages and limitations can be studied. The principal aim of this survey is\nto provide a thorough theoretical analysis of popular deep learning techniques\nin medical anomaly detection. In particular, we contribute a coherent and\nsystematic review of state-of-the-art techniques, comparing and contrasting\ntheir architectural differences as well as training algorithms. Furthermore, we\nprovide a comprehensive overview of deep model interpretation strategies that\ncan be used to interpret model decisions. In addition, we outline the key\nlimitations of existing deep medical anomaly detection techniques and propose\nkey research directions for further investigation.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 02:09:37 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 04:43:59 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Fernando", "Tharindu", ""], ["Gammulle", "Harshala", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "2012.02387", "submitter": "Sukannya Purkayastha", "authors": "Saugata Purkayastha and Sukannya Purkayastha", "title": "A Variant of Gradient Descent Algorithm Based on Gradient Averaging", "comments": "9 pages, 4 figures. Accepted at OPT2020: 12th Annual Workshop on\n  Optimization for Machine Learning @ NeurIPS, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we study an optimizer, Grad-Avg to optimize error functions. We\nestablish the convergence of the sequence of iterates of Grad-Avg\nmathematically to a minimizer (under boundedness assumption). We apply Grad-Avg\nalong with some of the popular optimizers on regression as well as\nclassification tasks. In regression tasks, it is observed that the behaviour of\nGrad-Avg is almost identical with Stochastic Gradient Descent (SGD). We present\na mathematical justification of this fact. In case of classification tasks, it\nis observed that the performance of Grad-Avg can be enhanced by suitably\nscaling the parameters. Experimental results demonstrate that Grad-Avg\nconverges faster than the other state-of-the-art optimizers for the\nclassification task on two benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 03:43:12 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 06:48:52 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Purkayastha", "Saugata", ""], ["Purkayastha", "Sukannya", ""]]}, {"id": "2012.02409", "submitter": "Niladri Chatterji", "authors": "Niladri S. Chatterji, Philip M. Long, Peter L. Bartlett", "title": "When does gradient descent with logistic loss find interpolating\n  two-layer networks?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the training of finite-width two-layer smoothed ReLU networks for\nbinary classification using the logistic loss. We show that gradient descent\ndrives the training loss to zero if the initial loss is small enough. When the\ndata satisfies certain cluster and separation conditions and the network is\nwide enough, we show that one step of gradient descent reduces the loss\nsufficiently that the first result applies.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 05:16:51 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 04:52:04 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 05:26:10 GMT"}, {"version": "v4", "created": "Thu, 1 Jul 2021 04:36:29 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Chatterji", "Niladri S.", ""], ["Long", "Philip M.", ""], ["Bartlett", "Peter L.", ""]]}, {"id": "2012.02414", "submitter": "Takeshi Teshima", "authors": "Takeshi Teshima, Koichi Tojo, Masahiro Ikeda, Isao Ishikawa, Kenta\n  Oono", "title": "Universal Approximation Property of Neural Ordinary Differential\n  Equations", "comments": "10 pages, 1 table. Accepted at NeurIPS 2020 Workshop on Differential\n  Geometry meets Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.DG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural ordinary differential equations (NODEs) is an invertible neural\nnetwork architecture promising for its free-form Jacobian and the availability\nof a tractable Jacobian determinant estimator. Recently, the representation\npower of NODEs has been partly uncovered: they form an $L^p$-universal\napproximator for continuous maps under certain conditions. However, the\n$L^p$-universality may fail to guarantee an approximation for the entire input\ndomain as it may still hold even if the approximator largely differs from the\ntarget function on a small region of the input space. To further uncover the\npotential of NODEs, we show their stronger approximation property, namely the\n$\\sup$-universality for approximating a large class of diffeomorphisms. It is\nshown by leveraging a structure theorem of the diffeomorphism group, and the\nresult complements the existing literature by establishing a fairly large set\nof mappings that NODEs can approximate with a stronger guarantee.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 05:53:21 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Teshima", "Takeshi", ""], ["Tojo", "Koichi", ""], ["Ikeda", "Masahiro", ""], ["Ishikawa", "Isao", ""], ["Oono", "Kenta", ""]]}, {"id": "2012.02424", "submitter": "Matthew J. Holland", "authors": "Matthew J. Holland", "title": "Learning with risks based on M-location", "comments": "Substantial update to initial version; refined theory, improved\n  exposition, added experimental analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study a new class of risks defined in terms of the location\nand deviation of the loss distribution, generalizing far beyond classical\nmean-variance risk functions. The class is easily implemented as a wrapper\naround any smooth loss, it admits finite-sample stationarity guarantees for\nstochastic gradient methods, it is straightforward to interpret and adjust,\nwith close links to M-estimators of the loss location, and has a salient effect\non the test loss distribution.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 06:21:51 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 00:37:11 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Holland", "Matthew J.", ""]]}, {"id": "2012.02439", "submitter": "Wangshu Zhu", "authors": "Wangshu Zhu and Andre Rosendo", "title": "Proximal Policy Optimization Smoothed Algorithm", "comments": "13 pages, 6 figures, 3 tables, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proximal policy optimization (PPO) has yielded state-of-the-art results in\npolicy search, a subfield of reinforcement learning, with one of its key points\nbeing the use of a surrogate objective function to restrict the step size at\neach policy update. Although such restriction is helpful, the algorithm still\nsuffers from performance instability and optimization inefficiency from the\nsudden flattening of the curve. To address this issue we present a PPO variant,\nnamed Proximal Policy Optimization Smooth Algorithm (PPOS), and its critical\nimprovement is the use of a functional clipping method instead of a flat\nclipping method. We compare our method with PPO and PPORB, which adopts a\nrollback clipping method, and prove that our method can conduct more accurate\nupdates at each time step than other PPO methods. Moreover, we show that it\noutperforms the latest PPO variants on both performance and stability in\nchallenging continuous control tasks.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 07:43:50 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Zhu", "Wangshu", ""], ["Rosendo", "Andre", ""]]}, {"id": "2012.02447", "submitter": "Annie Abay", "authors": "Annie Abay, Yi Zhou, Nathalie Baracaldo, Shashank Rajamoni, Ebube\n  Chuba, Heiko Ludwig", "title": "Mitigating Bias in Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As methods to create discrimination-aware models develop, they focus on\ncentralized ML, leaving federated learning (FL) unexplored. FL is a rising\napproach for collaborative ML, in which an aggregator orchestrates multiple\nparties to train a global model without sharing their training data. In this\npaper, we discuss causes of bias in FL and propose three pre-processing and\nin-processing methods to mitigate bias, without compromising data privacy, a\nkey FL requirement. As data heterogeneity among parties is one of the\nchallenging characteristics of FL, we conduct experiments over several data\ndistributions to analyze their effects on model performance, fairness metrics,\nand bias learning patterns. We conduct a comprehensive analysis of our proposed\ntechniques, the results demonstrating that these methods are effective even\nwhen parties have skewed data distributions or as little as 20% of parties\nemploy the methods.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 08:04:12 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Abay", "Annie", ""], ["Zhou", "Yi", ""], ["Baracaldo", "Nathalie", ""], ["Rajamoni", "Shashank", ""], ["Chuba", "Ebube", ""], ["Ludwig", "Heiko", ""]]}, {"id": "2012.02505", "submitter": "Hanna Abi Akl", "authors": "Dominique Mariko, Hanna Abi Akl, Estelle Labidurie, St\\'ephane\n  Durfort, Hugues de Mazancourt, Mahmoud El-Haj", "title": "Financial Document Causality Detection Shared Task (FinCausal 2020)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the FinCausal 2020 Shared Task on Causality Detection in Financial\nDocuments and the associated FinCausal dataset, and discuss the participating\nsystems and results. Two sub-tasks are proposed: a binary classification task\n(Task 1) and a relation extraction task (Task 2). A total of 16 teams submitted\nruns across the two Tasks and 13 of them contributed with a system description\npaper. This workshop is associated to the Joint Workshop on Financial Narrative\nProcessing and MultiLing Financial Summarisation (FNP-FNS 2020), held at The\n28th International Conference on Computational Linguistics (COLING'2020),\nBarcelona, Spain on September 12, 2020.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 10:17:42 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Mariko", "Dominique", ""], ["Akl", "Hanna Abi", ""], ["Labidurie", "Estelle", ""], ["Durfort", "St\u00e9phane", ""], ["de Mazancourt", "Hugues", ""], ["El-Haj", "Mahmoud", ""]]}, {"id": "2012.02558", "submitter": "Matthias A{\\ss}enmacher", "authors": "V. D. Viellieber and M. A{\\ss}enmacher", "title": "Pre-trained language models as knowledge bases for Automotive Complaint\n  Analysis", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently it has been shown that large pre-trained language models like BERT\n(Devlin et al., 2018) are able to store commonsense factual knowledge captured\nin its pre-training corpus (Petroni et al., 2019). In our work we further\nevaluate this ability with respect to an application from industry creating a\nset of probes specifically designed to reveal technical quality issues captured\nas described incidents out of unstructured customer feedback in the automotive\nindustry. After probing the out-of-the-box versions of the pre-trained models\nwith fill-in-the-mask tasks we dynamically provide it with more knowledge via\ncontinual pre-training on the Office of Defects Investigation (ODI) Complaints\ndata set. In our experiments the models exhibit performance regarding queries\non domain-specific topics compared to when queried on factual knowledge itself,\nas Petroni et al. (2019) have done. For most of the evaluated architectures the\ncorrect token is predicted with a $Precision@1$ ($P@1$) of above 60\\%, while\nfor $P@5$ and $P@10$ even values of well above 80\\% and up to 90\\% respectively\nare reached. These results show the potential of using language models as a\nknowledge base for structured analysis of customer feedback.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 12:49:47 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Viellieber", "V. D.", ""], ["A\u00dfenmacher", "M.", ""]]}, {"id": "2012.02655", "submitter": "\\.Ismail G\\\"uzel", "authors": "\\.Ismail G\\\"uzel and Atabey Kaygun", "title": "A New Non-archimedan Metric on Persistent Homology", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we define a new non-archimedian metric structure, called\ncophenetic metric, on persistent homology classes for all degrees. We then show\nthat zeroth persistent homology together with the cophenetic metric and\nhierarchical clustering algorithms with a number of different metrics do\ndeliver statistically verifiable commensurate topological information based on\nexperimental results we obtained on different datasets. We also observe that\nthe resulting clusters coming from cophenetic distance do shine in terms of\ninternal and external evaluation measures such as silhouette score and the Rand\nindex. Moreover, since the cophenetic metric is defined for all homology\ndegrees, one can now display the inter-relations of persistent homology classes\nin all degrees via rooted trees.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 15:30:15 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 11:51:20 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["G\u00fczel", "\u0130smail", ""], ["Kaygun", "Atabey", ""]]}, {"id": "2012.02661", "submitter": "Po-Wei Wang", "authors": "Chirag Pabbaraju, Po-Wei Wang, J. Zico Kolter", "title": "Efficient semidefinite-programming-based inference for binary and\n  multi-class MRFs", "comments": "Accepted at NeurIPS'20. The code can be found at\n  https://github.com/locuslab/sdp_mrf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic inference in pairwise Markov Random Fields (MRFs), i.e.\ncomputing the partition function or computing a MAP estimate of the variables,\nis a foundational problem in probabilistic graphical models. Semidefinite\nprogramming relaxations have long been a theoretically powerful tool for\nanalyzing properties of probabilistic inference, but have not been practical\nowing to the high computational cost of typical solvers for solving the\nresulting SDPs. In this paper, we propose an efficient method for computing the\npartition function or MAP estimate in a pairwise MRF by instead exploiting a\nrecently proposed coordinate-descent-based fast semidefinite solver. We also\nextend semidefinite relaxations from the typical binary MRF to the full\nmulti-class setting, and develop a compact semidefinite relaxation that can\nagain be solved efficiently using the solver. We show that the method\nsubstantially outperforms (both in terms of solution quality and speed) the\nexisting state of the art in approximate inference, on benchmark problems drawn\nfrom previous work. We also show that our approach can scale to large MRF\ndomains such as fully-connected pairwise CRF models used in computer vision.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 15:36:29 GMT"}, {"version": "v2", "created": "Sat, 1 May 2021 03:44:49 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Pabbaraju", "Chirag", ""], ["Wang", "Po-Wei", ""], ["Kolter", "J. Zico", ""]]}, {"id": "2012.02684", "submitter": "Arnout Devos", "authors": "Arnout Devos, Yatin Dandi", "title": "Model-Agnostic Learning to Meta-Learn", "comments": "Published in Proceedings of Machine Learning Research, PMLR\n  148:155-175", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a learning algorithm that enables a model to\nquickly exploit commonalities among related tasks from an unseen task\ndistribution, before quickly adapting to specific tasks from that same\ndistribution. We investigate how learning with different task distributions can\nfirst improve adaptability by meta-finetuning on related tasks before improving\ngoal task generalization with finetuning. Synthetic regression experiments\nvalidate the intuition that learning to meta-learn improves adaptability and\nconsecutively generalization. Experiments on more complex image classification,\ncontinual regression, and reinforcement learning tasks demonstrate that\nlearning to meta-learn generally improves task-specific adaptation. The\nmethodology, setup, and hypotheses in this proposal were positively evaluated\nby peer review before conclusive experiments were carried out.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 15:55:08 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 19:48:57 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Devos", "Arnout", ""], ["Dandi", "Yatin", ""]]}, {"id": "2012.02702", "submitter": "Gautham Krishna Gudur", "authors": "Abhijith Ragav, Gautham Krishna Gudur", "title": "Bayesian Active Learning for Wearable Stress and Affect Detection", "comments": "Machine Learning for Mobile Health Workshop at NeurIPS 2020. arXiv\n  admin note: substantial text overlap with arXiv:1906.00108", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent past, psychological stress has been increasingly observed in\nhumans, and early detection is crucial to prevent health risks. Stress\ndetection using on-device deep learning algorithms has been on the rise owing\nto advancements in pervasive computing. However, an important challenge that\nneeds to be addressed is handling unlabeled data in real-time via suitable\nground truthing techniques (like Active Learning), which should help establish\naffective states (labels) while also selecting only the most informative data\npoints to query from an oracle. In this paper, we propose a framework with\ncapabilities to represent model uncertainties through approximations in\nBayesian Neural Networks using Monte-Carlo (MC) Dropout. This is combined with\nsuitable acquisition functions for active learning. Empirical results on a\npopular stress and affect detection dataset experimented on a Raspberry Pi 2\nindicate that our proposed framework achieves a considerable efficiency boost\nduring inference, with a substantially low number of acquired pool points\nduring active learning across various acquisition functions. Variation Ratios\nachieves an accuracy of 90.38% which is comparable to the maximum test accuracy\nachieved while training on about 40% lesser data.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 16:19:37 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Ragav", "Abhijith", ""], ["Gudur", "Gautham Krishna", ""]]}, {"id": "2012.02704", "submitter": "Sergei Manzhos", "authors": "Owen Ren, Mohamed Ali Boussaidi, Dmitry Voytsekhovsky, Manabu Ihara,\n  and Sergei Manzhos", "title": "Random Sampling High Dimensional Model Representation Gaussian Process\n  Regression (RS-HDMR-GPR): a Python module for representing multidimensional\n  functions with machine-learned lower-dimensional terms", "comments": "48 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present a Python implementation for RS-HDMR-GPR (Random Sampling High\nDimensional Model Representation Gaussian Process Regression). The method\nbuilds representations of multivariate functions with lower-dimensional terms,\neither as an expansion over orders of coupling or using terms of only a given\ndimensionality. This facilitates, in particular, recovering functional\ndependence from sparse data. The code also allows for imputation of missing\nvalues of the variables and for a significant pruning of the useful number of\nHDMR terms. The code can also be used for estimating relative importance of\ndifferent combinations of input variables, thereby adding an element of insight\nto a general machine learning method. The capabilities of this regression tool\nare demonstrated on test cases involving synthetic analytic functions, the\npotential energy surface of the water molecule, kinetic energy densities of\nmaterials (crystalline magnesium, aluminum, and silicon), and financial market\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 00:12:05 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 18:12:59 GMT"}, {"version": "v3", "created": "Sat, 26 Jun 2021 10:01:57 GMT"}, {"version": "v4", "created": "Thu, 8 Jul 2021 10:27:38 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Ren", "Owen", ""], ["Boussaidi", "Mohamed Ali", ""], ["Voytsekhovsky", "Dmitry", ""], ["Ihara", "Manabu", ""], ["Manzhos", "Sergei", ""]]}, {"id": "2012.02788", "submitter": "Deepak Pathak", "authors": "Shikhar Bahl, Mustafa Mukadam, Abhinav Gupta, Deepak Pathak", "title": "Neural Dynamic Policies for End-to-End Sensorimotor Learning", "comments": "NeurIPS 2020 (Spotlight). Code and videos at\n  https://shikharbahl.github.io/neural-dynamic-policies/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current dominant paradigm in sensorimotor control, whether imitation or\nreinforcement learning, is to train policies directly in raw action spaces such\nas torque, joint angle, or end-effector position. This forces the agent to make\ndecisions individually at each timestep in training, and hence, limits the\nscalability to continuous, high-dimensional, and long-horizon tasks. In\ncontrast, research in classical robotics has, for a long time, exploited\ndynamical systems as a policy representation to learn robot behaviors via\ndemonstrations. These techniques, however, lack the flexibility and\ngeneralizability provided by deep learning or reinforcement learning and have\nremained under-explored in such settings. In this work, we begin to close this\ngap and embed the structure of a dynamical system into deep neural\nnetwork-based policies by reparameterizing action spaces via second-order\ndifferential equations. We propose Neural Dynamic Policies (NDPs) that make\npredictions in trajectory distribution space as opposed to prior policy\nlearning methods where actions represent the raw control space. The embedded\nstructure allows end-to-end policy learning for both reinforcement and\nimitation learning setups. We show that NDPs outperform the prior\nstate-of-the-art in terms of either efficiency or performance across several\nrobotic control tasks for both imitation and reinforcement learning setups.\nProject video and code are available at\nhttps://shikharbahl.github.io/neural-dynamic-policies/\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 18:59:32 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Bahl", "Shikhar", ""], ["Mukadam", "Mustafa", ""], ["Gupta", "Abhinav", ""], ["Pathak", "Deepak", ""]]}, {"id": "2012.02805", "submitter": "Amir Aradnia", "authors": "Amir Aradnia, Maryam Amir Haeri and Mohammad Mehdi Ebadzadeh", "title": "Adaptive Explicit Kernel Minkowski Weighted K-means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The K-means algorithm is among the most commonly used data clustering\nmethods. However, the regular K-means can only be applied in the input space\nand it is applicable when clusters are linearly separable. The kernel K-means,\nwhich extends K-means into the kernel space, is able to capture nonlinear\nstructures and identify arbitrarily shaped clusters. However, kernel methods\noften operate on the kernel matrix of the data, which scale poorly with the\nsize of the matrix or suffer from the high clustering cost due to the\nrepetitive calculations of kernel values. Another issue is that algorithms\naccess the data only through evaluations of $K(x_i, x_j)$, which limits many\nprocesses that can be done on data through the clustering task. This paper\nproposes a method to combine the advantages of the linear and nonlinear\napproaches by using driven corresponding approximate finite-dimensional feature\nmaps based on spectral analysis. Applying approximate finite-dimensional\nfeature maps were only discussed in the Support Vector Machines (SVM) problems\nbefore. We suggest using this method in kernel K-means era as alleviates\nstoring huge kernel matrix in memory, further calculating cluster centers more\nefficiently and access the data explicitly in feature space. These explicit\nfeature maps enable us to access the data in the feature space explicitly and\ntake advantage of K-means extensions in that space. We demonstrate our Explicit\nKernel Minkowski Weighted K-mean (Explicit KMWK-mean) method is able to be more\nadopted and find best-fitting values in new space by applying additional\nMinkowski exponent and feature weights parameter. Moreover, it can reduce the\nimpact of concentration on nearest neighbour search by suggesting investigate\namong other norms instead of Euclidean norm, includes Minkowski norms and\nfractional norms (as an extension of the Minkowski norms with p<1).\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 19:14:09 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Aradnia", "Amir", ""], ["Haeri", "Maryam Amir", ""], ["Ebadzadeh", "Mohammad Mehdi", ""]]}, {"id": "2012.02807", "submitter": "Pedro L. C. Rodrigues", "authors": "Pedro L. C. Rodrigues, Alexandre Gramfort", "title": "Learning summary features of time series for likelihood free inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There has been an increasing interest from the scientific community in using\nlikelihood-free inference (LFI) to determine which parameters of a given\nsimulator model could best describe a set of experimental data. Despite\nexciting recent results and a wide range of possible applications, an important\nbottleneck of LFI when applied to time series data is the necessity of defining\na set of summary features, often hand-tailored based on domain knowledge. In\nthis work, we present a data-driven strategy for automatically learning summary\nfeatures from univariate time series and apply it to signals generated from\nautoregressive-moving-average (ARMA) models and the Van der Pol Oscillator. Our\nresults indicate that learning summary features from data can compete and even\noutperform LFI methods based on hand-crafted values such as autocorrelation\ncoefficients even in the linear case.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 19:21:37 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Rodrigues", "Pedro L. C.", ""], ["Gramfort", "Alexandre", ""]]}, {"id": "2012.02818", "submitter": "Gianni Franchi", "authors": "Gianni Franchi, Andrei Bursuc, Emanuel Aldea, Severine Dubuisson,\n  Isabelle Bloch", "title": "Encoding the latent posterior of Bayesian Neural Networks for\n  uncertainty quantification", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian neural networks (BNNs) have been long considered an ideal, yet\nunscalable solution for improving the robustness and the predictive uncertainty\nof deep neural networks. While they could capture more accurately the posterior\ndistribution of the network parameters, most BNN approaches are either limited\nto small networks or rely on constraining assumptions such as parameter\nindependence. These drawbacks have enabled prominence of simple, but\ncomputationally heavy approaches such as Deep Ensembles, whose training and\ntesting costs increase linearly with the number of networks. In this work we\naim for efficient deep BNNs amenable to complex computer vision architectures,\ne.g. ResNet50 DeepLabV3+, and tasks, e.g. semantic segmentation, with fewer\nassumptions on the parameters. We achieve this by leveraging variational\nautoencoders (VAEs) to learn the interaction and the latent distribution of the\nparameters at each network layer. Our approach, Latent-Posterior BNN (LP-BNN),\nis compatible with the recent BatchEnsemble method, leading to highly efficient\n({in terms of computation and} memory during both training and testing)\nensembles. LP-BNN s attain competitive results across multiple metrics in\nseveral challenging benchmarks for image classification, semantic segmentation\nand out-of-distribution detection.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 19:50:09 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 12:12:19 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Franchi", "Gianni", ""], ["Bursuc", "Andrei", ""], ["Aldea", "Emanuel", ""], ["Dubuisson", "Severine", ""], ["Bloch", "Isabelle", ""]]}, {"id": "2012.02876", "submitter": "Daniel Vial", "authors": "Daniel Vial, Sanjay Shakkottai, R. Srikant", "title": "One-bit feedback is sufficient for upper confidence bound policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a variant of the traditional multi-armed bandit problem in which\neach arm is only able to provide one-bit feedback during each pull based on its\npast history of rewards. Our main result is the following: given an upper\nconfidence bound policy which uses full-reward feedback, there exists a coding\nscheme for generating one-bit feedback, and a corresponding decoding scheme and\narm selection policy, such that the ratio of the regret achieved by our policy\nand the regret of the full-reward feedback policy asymptotically approaches\none.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 22:33:53 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Vial", "Daniel", ""], ["Shakkottai", "Sanjay", ""], ["Srikant", "R.", ""]]}, {"id": "2012.02898", "submitter": "Isaac Lage", "authors": "Isaac Lage, Finale Doshi-Velez", "title": "Learning Interpretable Concept-Based Models with Human Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning models that first learn a representation of a domain in\nterms of human-understandable concepts, then use it to make predictions, have\nbeen proposed to facilitate interpretation and interaction with models trained\non high-dimensional data. However these methods have important limitations: the\nway they define concepts are not inherently interpretable, and they assume that\nconcept labels either exist for individual instances or can easily be acquired\nfrom users. These limitations are particularly acute for high-dimensional\ntabular features. We propose an approach for learning a set of transparent\nconcept definitions in high-dimensional tabular data that relies on users\nlabeling concept features instead of individual instances. Our method produces\nconcepts that both align with users' intuitive sense of what a concept means,\nand facilitate prediction of the downstream label by a transparent machine\nlearning model. This ensures that the full model is transparent and intuitive,\nand as predictive as possible given this constraint. We demonstrate with\nsimulated user feedback on real prediction problems, including one in a\nclinical domain, that this kind of direct feedback is much more efficient at\nlearning solutions that align with ground truth concept definitions than\nalternative transparent approaches that rely on labeling instances or other\nexisting interaction mechanisms, while maintaining similar predictive\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 23:41:05 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Lage", "Isaac", ""], ["Doshi-Velez", "Finale", ""]]}, {"id": "2012.02901", "submitter": "Dmitrii Ostrovskii", "authors": "Dmitrii M. Ostrovskii, Mohamed Ndaoud, Adel Javanmard, Meisam\n  Razaviyayn", "title": "Near-Optimal Procedures for Model Discrimination with Non-Disclosure\n  Properties", "comments": "52 pages, 2 figures; corrected the proof of the lower bound; added\n  new applications and the Fisher information-based argument in Appendix F", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Let $\\theta_0,\\theta_1 \\in \\mathbb{R}^d$ be the population risk minimizers\nassociated to some loss $\\ell:\\mathbb{R}^d\\times \\mathcal{Z}\\to\\mathbb{R}$ and\ntwo distributions $\\mathbb{P}_0,\\mathbb{P}_1$ on $\\mathcal{Z}$. The models\n$\\theta_0,\\theta_1$ are unknown, and $\\mathbb{P}_0,\\mathbb{P}_1$ can be\naccessed by drawing i.i.d samples from them. Our work is motivated by the\nfollowing model discrimination question: \"What sizes of the samples from\n$\\mathbb{P}_0$ and $\\mathbb{P}_1$ allow to distinguish between the two\nhypotheses $\\theta^*=\\theta_0$ and $\\theta^*=\\theta_1$ for given\n$\\theta^*\\in\\{\\theta_0,\\theta_1\\}$?\" Making the first steps towards answering\nit in full generality, we first consider the case of a well-specified linear\nmodel with squared loss. Here we provide matching upper and lower bounds on the\nsample complexity as given by $\\min\\{1/\\Delta^2,\\sqrt{r}/\\Delta\\}$ up to a\nconstant factor; here $\\Delta$ is a measure of separation between\n$\\mathbb{P}_0$ and $\\mathbb{P}_1$ and $r$ is the rank of the design covariance\nmatrix. We then extend this result in two directions: (i) for general\nparametric models in asymptotic regime; (ii) for generalized linear models in\nsmall samples ($n\\le r$) under weak moment assumptions. In both cases we derive\nsample complexity bounds of a similar form while allowing for model\nmisspecification. In fact, our testing procedures only access $\\theta^*$ via a\ncertain functional of empirical risk. In addition, the number of observations\nthat allows us to reach statistical confidence does not allow to \"resolve\" the\ntwo models $-$ that is, recover $\\theta_0,\\theta_1$ up to $O(\\Delta)$\nprediction accuracy. These two properties allow to use our framework in applied\ntasks where one would like to $\\textit{identify}$ a prediction model, which can\nbe proprietary, while guaranteeing that the model cannot be actually\n$\\textit{inferred}$ by the identifying agent.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 23:52:54 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 04:56:43 GMT"}, {"version": "v3", "created": "Sat, 10 Jul 2021 12:46:22 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Ostrovskii", "Dmitrii M.", ""], ["Ndaoud", "Mohamed", ""], ["Javanmard", "Adel", ""], ["Razaviyayn", "Meisam", ""]]}, {"id": "2012.02936", "submitter": "Lucy Gao", "authors": "Lucy L. Gao, Jacob Bien and Daniela Witten", "title": "Selective Inference for Hierarchical Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing for a difference in means between two groups is fundamental to\nanswering research questions across virtually every scientific area. Classical\ntests control the Type I error rate when the groups are defined a priori.\nHowever, when the groups are instead defined via a clustering algorithm, then\napplying a classical test for a difference in means between the groups yields\nan extremely inflated Type I error rate. Notably, this problem persists even if\ntwo separate and independent data sets are used to define the groups and to\ntest for a difference in their means. To address this problem, in this paper,\nwe propose a selective inference approach to test for a difference in means\nbetween two clusters obtained from any clustering method. Our procedure\ncontrols the selective Type I error rate by accounting for the fact that the\nnull hypothesis was generated from the data. We describe how to efficiently\ncompute exact p-values for clusters obtained using agglomerative hierarchical\nclustering with many commonly used linkages. We apply our method to simulated\ndata and to single-cell RNA-seq data.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 03:03:19 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Gao", "Lucy L.", ""], ["Bien", "Jacob", ""], ["Witten", "Daniela", ""]]}, {"id": "2012.02998", "submitter": "Jordi Mu\\~noz-Mar\\'i", "authors": "Luca Pipia, Jordi Mu\\~noz-Mar\\'i, Eatidal Amin, Santiago Belda, Gustau\n  Camps-Valls, Jochem Verrelst", "title": "Fusing Optical and SAR time series for LAI gap filling with multioutput\n  Gaussian processes", "comments": "43 pages, 12 figures", "journal-ref": "Remote Sensing of Environment Volume 235, 15 December 2019, 111452", "doi": "10.1016/j.rse.2019.111452", "report-no": null, "categories": "eess.IV cs.LG physics.data-an stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The availability of satellite optical information is often hampered by the\nnatural presence of clouds, which can be problematic for many applications.\nPersistent clouds over agricultural fields can mask key stages of crop growth,\nleading to unreliable yield predictions. Synthetic Aperture Radar (SAR)\nprovides all-weather imagery which can potentially overcome this limitation,\nbut given its high and distinct sensitivity to different surface properties,\nthe fusion of SAR and optical data still remains an open challenge. In this\nwork, we propose the use of Multi-Output Gaussian Process (MOGP) regression, a\nmachine learning technique that learns automatically the statistical\nrelationships among multisensor time series, to detect vegetated areas over\nwhich the synergy between SAR-optical imageries is profitable. For this\npurpose, we use the Sentinel-1 Radar Vegetation Index (RVI) and Sentinel-2 Leaf\nArea Index (LAI) time series over a study area in north west of the Iberian\npeninsula. Through a physical interpretation of MOGP trained models, we show\nits ability to provide estimations of LAI even over cloudy periods using the\ninformation shared with RVI, which guarantees the solution keeps always tied to\nreal measurements. Results demonstrate the advantage of MOGP especially for\nlong data gaps, where optical-based methods notoriously fail. The\nleave-one-image-out assessment technique applied to the whole vegetation cover\nshows MOGP predictions improve standard GP estimations over short-time gaps\n(R$^2$ of 74\\% vs 68\\%, RMSE of 0.4 vs 0.44 $[m^2m^{-2}]$) and especially over\nlong-time gaps (R$^2$ of 33\\% vs 12\\%, RMSE of 0.5 vs 1.09 $[m^2m^{-2}]$).\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 10:36:45 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Pipia", "Luca", ""], ["Mu\u00f1oz-Mar\u00ed", "Jordi", ""], ["Amin", "Eatidal", ""], ["Belda", "Santiago", ""], ["Camps-Valls", "Gustau", ""], ["Verrelst", "Jochem", ""]]}, {"id": "2012.03016", "submitter": "Vugar Ismailov", "authors": "Vugar Ismailov", "title": "A three layer neural network can represent any discontinuous\n  multivariate function", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.FA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1987, Hecht-Nielsen showed that any continuous multivariate function could\nbe implemented by a certain type three-layer neural network. This result was\nvery much discussed in neural network literature. In this paper we prove that\nnot only continuous functions but also all discontinuous functions can be\nimplemented by such neural networks.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 12:11:09 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Ismailov", "Vugar", ""]]}, {"id": "2012.03082", "submitter": "Janis Postels", "authors": "Janis Postels, Hermann Blum, Yannick Str\\\"umpler, Cesar Cadena, Roland\n  Siegwart, Luc Van Gool, Federico Tombari", "title": "The Hidden Uncertainty in a Neural Networks Activations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distribution of a neural network's latent representations has been\nsuccessfully used to detect out-of-distribution (OOD) data. This work\ninvestigates whether this distribution moreover correlates with a model's\nepistemic uncertainty, thus indicates its ability to generalise to novel\ninputs. We first empirically verify that epistemic uncertainty can be\nidentified with the surprise, thus the negative log-likelihood, of observing a\nparticular latent representation. Moreover, we demonstrate that the\noutput-conditional distribution of hidden representations also allows\nquantifying aleatoric uncertainty via the entropy of the predictive\ndistribution. We analyse epistemic and aleatoric uncertainty inferred from the\nrepresentations of different layers and conclude that deeper layers lead to\nuncertainty with similar behaviour as established - but computationally more\nexpensive - methods (e.g. deep ensembles). While our approach does not require\nmodifying the training process, we follow prior work and experiment with an\nadditional regularising loss that increases the information in the latent\nrepresentations. We find that this leads to improved OOD detection of epistemic\nuncertainty at the cost of ambiguous calibration close to the data\ndistribution. We verify our findings on both classification and regression\nmodels.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 17:30:35 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 08:43:00 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Postels", "Janis", ""], ["Blum", "Hermann", ""], ["Str\u00fcmpler", "Yannick", ""], ["Cadena", "Cesar", ""], ["Siegwart", "Roland", ""], ["Van Gool", "Luc", ""], ["Tombari", "Federico", ""]]}, {"id": "2012.03085", "submitter": "Federico Errica", "authors": "Federico Errica, Davide Bacciu, Alessio Micheli", "title": "Graph Mixture Density Networks", "comments": null, "journal-ref": "Proceedings of the 38th International Conference on Machine\n  Learning, PMLR 139 (2021)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Graph Mixture Density Networks, a new family of machine\nlearning models that can fit multimodal output distributions conditioned on\ngraphs of arbitrary topology. By combining ideas from mixture models and graph\nrepresentation learning, we address a broader class of challenging conditional\ndensity estimation problems that rely on structured data. In this respect, we\nevaluate our method on a new benchmark application that leverages random graphs\nfor stochastic epidemic simulations. We show a significant improvement in the\nlikelihood of epidemic outcomes when taking into account both multimodality and\nstructure. The empirical analysis is complemented by two real-world regression\ntasks showing the effectiveness of our approach in modeling the output\nprediction uncertainty. Graph Mixture Density Networks open appealing research\nopportunities in the study of structure-dependent phenomena that exhibit\nnon-trivial conditional output distributions.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 17:39:38 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 06:21:40 GMT"}, {"version": "v3", "created": "Fri, 25 Jun 2021 09:03:19 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Errica", "Federico", ""], ["Bacciu", "Davide", ""], ["Micheli", "Alessio", ""]]}, {"id": "2012.03092", "submitter": "Yuning Yang", "authors": "Xianpeng Mao and Yuning Yang", "title": "Approximation Algorithms for Sparse Best Rank-1 Approximation to\n  Higher-Order Tensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sparse tensor best rank-1 approximation (BR1Approx), which is a sparsity\ngeneralization of the dense tensor BR1Approx, and is a higher-order extension\nof the sparse matrix BR1Approx, is one of the most important problems in sparse\ntensor decomposition and related problems arising from statistics and machine\nlearning. By exploiting the multilinearity as well as the sparsity structure of\nthe problem, four approximation algorithms are proposed, which are easily\nimplemented, of low computational complexity, and can serve as initial\nprocedures for iterative algorithms. In addition, theoretically guaranteed\nworst-case approximation lower bounds are proved for all the algorithms. We\nprovide numerical experiments on synthetic and real data to illustrate the\neffectiveness of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 18:13:14 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Mao", "Xianpeng", ""], ["Yang", "Yuning", ""]]}, {"id": "2012.03107", "submitter": "Xiaoixa Wu", "authors": "Xiaoxia Wu and Ethan Dyer and Behnam Neyshabur", "title": "When Do Curricula Work?", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inspired by human learning, researchers have proposed ordering examples\nduring training based on their difficulty. Both curriculum learning, exposing a\nnetwork to easier examples early in training, and anti-curriculum learning,\nshowing the most difficult examples first, have been suggested as improvements\nto the standard i.i.d. training. In this work, we set out to investigate the\nrelative benefits of ordered learning. We first investigate the \\emph{implicit\ncurricula} resulting from architectural and optimization bias and find that\nsamples are learned in a highly consistent order. Next, to quantify the benefit\nof \\emph{explicit curricula}, we conduct extensive experiments over thousands\nof orderings spanning three kinds of learning: curriculum, anti-curriculum, and\nrandom-curriculum -- in which the size of the training dataset is dynamically\nincreased over time, but the examples are randomly ordered. We find that for\nstandard benchmark datasets, curricula have only marginal benefits, and that\nrandomly ordered samples perform as well or better than curricula and\nanti-curricula, suggesting that any benefit is entirely due to the dynamic\ntraining set size. Inspired by common use cases of curriculum learning in\npractice, we investigate the role of limited training time budget and noisy\ndata in the success of curriculum learning. Our experiments demonstrate that\ncurriculum, but not anti-curriculum can indeed improve the performance either\nwith limited training time budget or in existence of noisy data.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 19:41:30 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 05:08:30 GMT"}, {"version": "v3", "created": "Tue, 9 Feb 2021 17:38:58 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Wu", "Xiaoxia", ""], ["Dyer", "Ethan", ""], ["Neyshabur", "Behnam", ""]]}, {"id": "2012.03130", "submitter": "Nathan Kallus", "authors": "Nathan Kallus", "title": "Rejoinder: New Objectives for Policy Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I provide a rejoinder for discussion of \"More Efficient Policy Learning via\nOptimal Retargeting\" to appear in the Journal of the American Statistical\nAssociation with discussion by Oliver Dukes and Stijn Vansteelandt; Sijia Li,\nXiudi Li, and Alex Luedtkeand; and Muxuan Liang and Yingqi Zhao.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 22:10:25 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Kallus", "Nathan", ""]]}, {"id": "2012.03173", "submitter": "Zhuoning Yuan", "authors": "Zhuoning Yuan, Yan Yan, Milan Sonka, Tianbao Yang", "title": "Robust Deep AUC Maximization: A New Surrogate Loss and Empirical Studies\n  on Medical Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep AUC Maximization (DAM) is a paradigm for learning a deep neural network\nby maximizing the AUC score of the model on a dataset. Most previous works of\nAUC maximization focus on the perspective of optimization by designing\nefficient stochastic algorithms, and studies on generalization performance of\nDAM on difficult tasks are missing. In this work, we aim to make DAM more\npractical for interesting real-world applications (e.g., medical image\nclassification). First, we propose a new margin-based surrogate loss function\nfor the AUC score (named as the AUC margin loss). It is more robust than the\ncommonly used AUC square loss, while enjoying the same advantage in terms of\nlarge-scale stochastic optimization. Second, we conduct empirical studies of\nour DAM method on difficult medical image classification tasks, namely\nclassification of chest x-ray images for identifying many threatening diseases\nand classification of images of skin lesions for identifying melanoma. Our DAM\nmethod has achieved great success on these difficult tasks, i.e., the 1st place\non Stanford CheXpert competition (by the paper submission date) and Top 1% rank\n(rank 33 out of 3314 teams) on Kaggle 2020 Melanoma classification competition.\nWe also conduct extensive ablation studies to demonstrate the advantages of the\nnew AUC margin loss over the AUC square loss on benchmark datasets. To the best\nof our knowledge, this is the first work that makes DAM succeed on large-scale\nmedical image datasets.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 03:41:51 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Yuan", "Zhuoning", ""], ["Yan", "Yan", ""], ["Sonka", "Milan", ""], ["Yang", "Tianbao", ""]]}, {"id": "2012.03199", "submitter": "Harish S. Bhat", "authors": "Harish S. Bhat, Majerle Reeves, Ramin Raziperchikolaei", "title": "Estimating Vector Fields from Noisy Time Series", "comments": "Accepted for publication in the Proceedings of the 54th Asilomar\n  Conference on Signals, Systems, and Computers", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SY eess.SY math.DS math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While there has been a surge of recent interest in learning differential\nequation models from time series, methods in this area typically cannot cope\nwith highly noisy data. We break this problem into two parts: (i) approximating\nthe unknown vector field (or right-hand side) of the differential equation, and\n(ii) dealing with noise. To deal with (i), we describe a neural network\narchitecture consisting of tensor products of one-dimensional neural shape\nfunctions. For (ii), we propose an alternating minimization scheme that\nswitches between vector field training and filtering steps, together with\nmultiple trajectories of training data. We find that the neural shape function\narchitecture retains the approximation properties of dense neural networks,\nenables effective computation of vector field error, and allows for graphical\ninterpretability, all for data/systems in any finite dimension $d$. We also\nstudy the combination of either our neural shape function method or existing\ndifferential equation learning methods with alternating minimization and\nmultiple trajectories. We find that retrofitting any learning method in this\nway boosts the method's robustness to noise. While in their raw form the\nmethods struggle with 1% Gaussian noise, after retrofitting, they learn\naccurate vector fields from data with 10% Gaussian noise.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 07:27:56 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Bhat", "Harish S.", ""], ["Reeves", "Majerle", ""], ["Raziperchikolaei", "Ramin", ""]]}, {"id": "2012.03224", "submitter": "Taiji Suzuki", "authors": "Taiji Suzuki and Shunta Akiyama", "title": "Benefit of deep learning with non-convex noisy gradient descent:\n  Provable excess risk bound and superiority to kernel methods", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Establishing a theoretical analysis that explains why deep learning can\noutperform shallow learning such as kernel methods is one of the biggest issues\nin the deep learning literature. Towards answering this question, we evaluate\nexcess risk of a deep learning estimator trained by a noisy gradient descent\nwith ridge regularization on a mildly overparameterized neural network, and\ndiscuss its superiority to a class of linear estimators that includes neural\ntangent kernel approach, random feature model, other kernel methods, $k$-NN\nestimator and so on. We consider a teacher-student regression model, and\neventually show that any linear estimator can be outperformed by deep learning\nin a sense of the minimax optimal rate especially for a high dimension setting.\nThe obtained excess bounds are so-called fast learning rate which is faster\nthan $O(1/\\sqrt{n})$ that is obtained by usual Rademacher complexity analysis.\nThis discrepancy is induced by the non-convex geometry of the model and the\nnoisy gradient descent used for neural network training provably reaches a near\nglobal optimal solution even though the loss landscape is highly non-convex.\nAlthough the noisy gradient descent does not employ any explicit or implicit\nsparsity inducing regularization, it shows a preferable generalization\nperformance that dominates linear estimators.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 09:22:16 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Suzuki", "Taiji", ""], ["Akiyama", "Shunta", ""]]}, {"id": "2012.03295", "submitter": "Omer Yair", "authors": "Omer Yair, Tomer Michaeli", "title": "Contrastive Divergence Learning is a Time Reversal Adversarial Game", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Contrastive divergence (CD) learning is a classical method for fitting\nunnormalized statistical models to data samples. Despite its wide-spread use,\nthe convergence properties of this algorithm are still not well understood. The\nmain source of difficulty is an unjustified approximation which has been used\nto derive the gradient of the loss. In this paper, we present an alternative\nderivation of CD that does not require any approximation and sheds new light on\nthe objective that is actually being optimized by the algorithm. Specifically,\nwe show that CD is an adversarial learning procedure, where a discriminator\nattempts to classify whether a Markov chain generated from the model has been\ntime-reversed. Thus, although predating generative adversarial networks (GANs)\nby more than a decade, CD is, in fact, closely related to these techniques. Our\nderivation settles well with previous observations, which have concluded that\nCD's update steps cannot be expressed as the gradients of any fixed objective\nfunction. In addition, as a byproduct, our derivation reveals a simple\ncorrection that can be used as an alternative to Metropolis-Hastings rejection,\nwhich is required when the underlying Markov chain is inexact (e.g. when using\nLangevin dynamics with a large step).\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 15:54:05 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 12:17:14 GMT"}, {"version": "v3", "created": "Mon, 15 Mar 2021 20:03:43 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Yair", "Omer", ""], ["Michaeli", "Tomer", ""]]}, {"id": "2012.03310", "submitter": "Fan Yao", "authors": "Ravi Sundaram, Anil Vullikanti, Haifeng Xu, Fan Yao", "title": "PAC-Learning for Strategic Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The study of strategic or adversarial manipulation of testing data to fool a\nclassifier has attracted much recent attention. Most previous works have\nfocused on two extreme situations where any testing data point either is\ncompletely adversarial or always equally prefers the positive label. In this\npaper, we generalize both of these through a unified framework for strategic\nclassification, and introduce the notion of strategic VC-dimension (SVC) to\ncapture the PAC-learnability in our general strategic setup. SVC provably\ngeneralizes the recent concept of adversarial VC-dimension (AVC) introduced by\nCullina et al. arXiv:1806.01471. We instantiate our framework for the\nfundamental strategic linear classification problem. We fully characterize: (1)\nthe statistical learnability of linear classifiers by pinning down its SVC; (2)\nits computational tractability by pinning down the complexity of the empirical\nrisk minimization problem. Interestingly, the SVC of linear classifiers is\nalways upper bounded by its standard VC-dimension. This characterization also\nstrictly generalizes the AVC bound for linear classifiers in arXiv:1806.01471.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 16:29:27 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 06:04:16 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2021 22:05:13 GMT"}, {"version": "v4", "created": "Fri, 11 Jun 2021 04:41:41 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Sundaram", "Ravi", ""], ["Vullikanti", "Anil", ""], ["Xu", "Haifeng", ""], ["Yao", "Fan", ""]]}, {"id": "2012.03351", "submitter": "Felix Voigtlaender", "authors": "Felix Voigtlaender", "title": "The universal approximation theorem for complex-valued neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize the classical universal approximation theorem for neural\nnetworks to the case of complex-valued neural networks. Precisely, we consider\nfeedforward networks with a complex activation function $\\sigma : \\mathbb{C}\n\\to \\mathbb{C}$ in which each neuron performs the operation $\\mathbb{C}^N \\to\n\\mathbb{C}, z \\mapsto \\sigma(b + w^T z)$ with weights $w \\in \\mathbb{C}^N$ and\na bias $b \\in \\mathbb{C}$, and with $\\sigma$ applied componentwise. We\ncompletely characterize those activation functions $\\sigma$ for which the\nassociated complex networks have the universal approximation property, meaning\nthat they can uniformly approximate any continuous function on any compact\nsubset of $\\mathbb{C}^d$ arbitrarily well.\n  Unlike the classical case of real networks, the set of \"good activation\nfunctions\" which give rise to networks with the universal approximation\nproperty differs significantly depending on whether one considers deep networks\nor shallow networks: For deep networks with at least two hidden layers, the\nuniversal approximation property holds as long as $\\sigma$ is neither a\npolynomial, a holomorphic function, or an antiholomorphic function. Shallow\nnetworks, on the other hand, are universal if and only if the real part or the\nimaginary part of $\\sigma$ is not a polyharmonic function.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 18:51:10 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Voigtlaender", "Felix", ""]]}, {"id": "2012.03391", "submitter": "Edmondo Trentin", "authors": "Edmondo Trentin (DIISM, University of Siena, Italy)", "title": "Multivariate Density Estimation with Deep Neural Mixture Models", "comments": "Extended journal version of E. Trentin, \"Maximum-Likelihood\n  Estimation of Neural Mixture Densities: Model, Algorithm, and Preliminary\n  Experimental Evaluation\". In Proc. of ANNPR 2018: 178-189, Springer, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Albeit worryingly underrated in the recent literature on machine learning in\ngeneral (and, on deep learning in particular), multivariate density estimation\nis a fundamental task in many applications, at least implicitly, and still an\nopen issue. With a few exceptions, deep neural networks (DNNs) have seldom been\napplied to density estimation, mostly due to the unsupervised nature of the\nestimation task, and (especially) due to the need for constrained training\nalgorithms that ended up realizing proper probabilistic models that satisfy\nKolmogorov's axioms. Moreover, in spite of the well-known improvement in terms\nof modeling capabilities yielded by mixture models over plain single-density\nstatistical estimators, no proper mixtures of multivariate DNN-based component\ndensities have been investigated so far. The paper fills this gap by extending\nour previous work on Neural Mixture Densities (NMMs) to multivariate DNN\nmixtures. A maximum-likelihood (ML) algorithm for estimating Deep NMMs (DNMMs)\nis handed out, which satisfies numerically a combination of hard and soft\nconstraints aimed at ensuring satisfaction of Kolmogorov's axioms. The class of\nprobability density functions that can be modeled to any degree of precision\nvia DNMMs is formally defined. A procedure for the automatic selection of the\nDNMM architecture, as well as of the hyperparameters for its ML training\nalgorithm, is presented (exploiting the probabilistic nature of the DNMM).\nExperimental results on univariate and multivariate data are reported on,\ncorroborating the effectiveness of the approach and its superiority to the most\npopular statistical estimation techniques.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 23:03:48 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Trentin", "Edmondo", "", "DIISM, University of Siena, Italy"]]}, {"id": "2012.03420", "submitter": "Minkai Xu", "authors": "Minkai Xu, Zhiming Zhou, Guansong Lu, Jian Tang, Weinan Zhang, Yong Yu", "title": "Towards Generalized Implementation of Wasserstein Distance in GANs", "comments": "Accepted by AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Wasserstein GANs (WGANs), built upon the Kantorovich-Rubinstein (KR) duality\nof Wasserstein distance, is one of the most theoretically sound GAN models.\nHowever, in practice it does not always outperform other variants of GANs. This\nis mostly due to the imperfect implementation of the Lipschitz condition\nrequired by the KR duality. Extensive work has been done in the community with\ndifferent implementations of the Lipschitz constraint, which, however, is still\nhard to satisfy the restriction perfectly in practice. In this paper, we argue\nthat the strong Lipschitz constraint might be unnecessary for optimization.\nInstead, we take a step back and try to relax the Lipschitz constraint.\nTheoretically, we first demonstrate a more general dual form of the Wasserstein\ndistance called the Sobolev duality, which relaxes the Lipschitz constraint but\nstill maintains the favorable gradient property of the Wasserstein distance.\nMoreover, we show that the KR duality is actually a special case of the Sobolev\nduality. Based on the relaxed duality, we further propose a generalized WGAN\ntraining scheme named Sobolev Wasserstein GAN (SWGAN), and empirically\ndemonstrate the improvement of SWGAN over existing methods with extensive\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 02:22:23 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 11:30:57 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Xu", "Minkai", ""], ["Zhou", "Zhiming", ""], ["Lu", "Guansong", ""], ["Tang", "Jian", ""], ["Zhang", "Weinan", ""], ["Yu", "Yong", ""]]}, {"id": "2012.03454", "submitter": "Mingda Qiao", "authors": "Mingda Qiao, Gregory Valiant", "title": "Stronger Calibration Lower Bounds via Sidestepping", "comments": "To appear in STOC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an online binary prediction setting where a forecaster observes a\nsequence of $T$ bits one by one. Before each bit is revealed, the forecaster\npredicts the probability that the bit is $1$. The forecaster is called\nwell-calibrated if for each $p \\in [0, 1]$, among the $n_p$ bits for which the\nforecaster predicts probability $p$, the actual number of ones, $m_p$, is\nindeed equal to $p \\cdot n_p$. The calibration error, defined as $\\sum_p |m_p -\np n_p|$, quantifies the extent to which the forecaster deviates from being\nwell-calibrated. It has long been known that an $O(T^{2/3})$ calibration error\nis achievable even when the bits are chosen adversarially, and possibly based\non the previous predictions. However, little is known on the lower bound side,\nexcept an $\\Omega(\\sqrt{T})$ bound that follows from the trivial example of\nindependent fair coin flips.\n  In this paper, we prove an $\\Omega(T^{0.528})$ bound on the calibration\nerror, which is the first super-$\\sqrt{T}$ lower bound for this setting to the\nbest of our knowledge. The technical contributions of our work include two\nlower bound techniques, early stopping and sidestepping, which circumvent the\nobstacles that have previously hindered strong calibration lower bounds. We\nalso propose an abstraction of the prediction setting, termed the\nSign-Preservation game, which may be of independent interest. This game has a\nmuch smaller state space than the full prediction setting and allows simpler\nanalyses. The $\\Omega(T^{0.528})$ lower bound follows from a general reduction\ntheorem that translates lower bounds on the game value of Sign-Preservation\ninto lower bounds on the calibration error.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 05:29:28 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 18:27:14 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Qiao", "Mingda", ""], ["Valiant", "Gregory", ""]]}, {"id": "2012.03501", "submitter": "Taehyeon Kim", "authors": "Taehyeon Kim, Jaeyeon Ahn, Nakyil Kim, Seyoung Yun", "title": "Adaptive Local Bayesian Optimization Over Multiple Discrete Variables", "comments": "workshop at NeurIPS 2020 Competition Track on Black-Box Optimization\n  Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the machine learning algorithms, the choice of the hyperparameter is often\nan art more than a science, requiring labor-intensive search with expert\nexperience. Therefore, automation on hyperparameter optimization to exclude\nhuman intervention is a great appeal, especially for the black-box functions.\nRecently, there have been increasing demands of solving such concealed tasks\nfor better generalization, though the task-dependent issue is not easy to\nsolve. The Black-Box Optimization challenge (NeurIPS 2020) required competitors\nto build a robust black-box optimizer across different domains of standard\nmachine learning problems. This paper describes the approach of team KAIST OSI\nin a step-wise manner, which outperforms the baseline algorithms by up to\n+20.39%. We first strengthen the local Bayesian search under the concept of\nregion reliability. Then, we design a combinatorial kernel for a Gaussian\nprocess kernel. In a similar vein, we combine the methodology of Bayesian and\nmulti-armed bandit,(MAB) approach to select the values with the consideration\nof the variable types; the real and integer variables are with Bayesian, while\nthe boolean and categorical variables are with MAB. Empirical evaluations\ndemonstrate that our method outperforms the existing methods across different\ntasks.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 07:51:23 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Kim", "Taehyeon", ""], ["Ahn", "Jaeyeon", ""], ["Kim", "Nakyil", ""], ["Yun", "Seyoung", ""]]}, {"id": "2012.03503", "submitter": "Hanbaek Lyu", "authors": "Hanbaek Lyu", "title": "Convergence of block coordinate descent with diminishing radius for\n  nonconvex optimization", "comments": "12 pages, 2 figure. Rate of convergence added. arXiv admin note: text\n  overlap with arXiv:2009.07612", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Block coordinate descent (BCD), also known as nonlinear Gauss-Seidel, is a\nsimple iterative algorithm for nonconvex optimization that sequentially\nminimizes the objective function in each block coordinate while the other\ncoordinates are held fixed. We propose a version of BCD that is guaranteed to\nconverge to the stationary points of block-wise convex and differentiable\nobjective functions under constraints. Furthermore, we obtain a best-case rate\nof convergence of order $\\log n/\\sqrt{n}$, where $n$ denotes the number of\niterations. A key idea is to restrict the parameter search within a diminishing\nradius to promote stability of iterates, and then to show that such auxiliary\nconstraints vanish in the limit. As an application, we provide a modified\nalternating least squares algorithm for nonnegative CP tensor factorization\nthat converges to the stationary points of the reconstruction error with the\nsame bound on the best-case rate of convergence. We also experimentally\nvalidate our results with both synthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 07:53:09 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 10:57:26 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Lyu", "Hanbaek", ""]]}, {"id": "2012.03522", "submitter": "Leonardo Cella", "authors": "Leonardo Cella and Claudio Gentile and Massimiliano Pontil", "title": "Online Model Selection: a Rested Bandit Formulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a natural problem in online model selection with bandit\ninformation, we introduce and analyze a best arm identification problem in the\nrested bandit setting, wherein arm expected losses decrease with the number of\ntimes the arm has been played. The shape of the expected loss functions is\nsimilar across arms, and is assumed to be available up to unknown parameters\nthat have to be learned on the fly. We define a novel notion of regret for this\nproblem, where we compare to the policy that always plays the arm having the\nsmallest expected loss at the end of the game. We analyze an arm elimination\nalgorithm whose regret vanishes as the time horizon increases. The actual rate\nof convergence depends in a detailed way on the postulated functional form of\nthe expected losses. Unlike known model selection efforts in the recent bandit\nliterature, our algorithm exploits the specific structure of the problem to\nlearn the unknown parameters of the expected loss function so as to identify\nthe best arm as quickly as possible. We complement our analysis with a lower\nbound, indicating strengths and limitations of the proposed solution.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 08:23:08 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Cella", "Leonardo", ""], ["Gentile", "Claudio", ""], ["Pontil", "Massimiliano", ""]]}, {"id": "2012.03531", "submitter": "Ellen De Mello Koch Ms", "authors": "Anita de Mello Koch, Ellen de Mello Koch, Robert de Mello Koch", "title": "Why Unsupervised Deep Networks Generalize", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Promising resolutions of the generalization puzzle observe that the actual\nnumber of parameters in a deep network is much smaller than naive estimates\nsuggest. The renormalization group is a compelling example of a problem which\nhas very few parameters, despite the fact that naive estimates suggest\notherwise. Our central hypothesis is that the mechanisms behind the\nrenormalization group are also at work in deep learning, and that this leads to\na resolution of the generalization puzzle. We show detailed quantitative\nevidence that proves the hypothesis for an RBM, by showing that the trained RBM\nis discarding high momentum modes. Specializing attention mainly to\nautoencoders, we give an algorithm to determine the network's parameters\ndirectly from the learning data set. The resulting autoencoder almost performs\nas well as one trained by deep learning, and it provides an excellent initial\ncondition for training, reducing training times by a factor between 4 and 100\nfor the experiments we considered. Further, we are able to suggest a simple\ncriterion to decide if a given problem can or can not be solved using a deep\nnetwork.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 08:45:20 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Koch", "Anita de Mello", ""], ["Koch", "Ellen de Mello", ""], ["Koch", "Robert de Mello", ""]]}, {"id": "2012.03554", "submitter": "Andreas Meid", "authors": "Andreas D. Meid", "title": "Teaching reproducible research for medical students and postgraduate\n  pharmaceutical scientists", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In many academic settings, medical students start their scientific work\nalready during their studies. Like at our institution, they often work in\ninterdisciplinary teams with more or less experienced (postgraduate)\nresearchers of pharmaceutical sciences, natural sciences in general, or\nbiostatistics. All of them should be taught good research practices as an\nintegral part of their education, especially in terms of statistical analysis.\nThis includes reproducibility as a central aspect of modern research.\nAcknowledging that even educators might be unfamiliar with necessary aspects of\na perfectly reproducible workflow, I agreed to give a lecture series on\nreproducible research (RR) for medical students and postgraduate pharmacists\ninvolved in several areas of clinical research. Thus, I designed a piloting\nlecture series to highlight definitions of RR, reasons for RR, potential merits\nof RR, and ways to work accordingly. In trying to actually reproduce a\npublished analysis, I encountered several practical obstacles. In this article,\nI focus on this working example to emphasize the manifold facets of RR, to\nprovide possible explanations and solutions, and argue that harmonized\ncurricula for (quantitative) clinical researchers should include RR principles.\nI therefore hope these experiences are helpful to raise awareness among\neducators and students. RR working habits are not only beneficial for ourselves\nor our students, but also for other researchers within an institution, for\nscientific partners, for the scientific community, and eventually for the\npublic profiting from research findings.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 09:44:23 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Meid", "Andreas D.", ""]]}, {"id": "2012.03612", "submitter": "Hiroyuki Kasai", "authors": "Jianming Huang, Zhongxi Fang, Hiroyuki Kasai", "title": "LCS Graph Kernel Based on Wasserstein Distance in Longest Common\n  Subsequence Metric Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For graph classification tasks, many methods use a common strategy to\naggregate information of vertex neighbors. Although this strategy provides an\nefficient means of extracting graph topological features, it brings excessive\namounts of information that might greatly reduce its accuracy when dealing with\nlarge-scale neighborhoods. Learning graphs using paths or walks will not suffer\nfrom this difficulty, but many have low utilization of each path or walk, which\nmight engender information loss and high computational costs. To solve this, we\npropose a graph kernel using a longest common subsequence (LCS kernel) to\ncompute more comprehensive similarity between paths and walks, which resolves\nsubstructure isomorphism difficulties. We also combine it with optimal\ntransport theory to extract more in-depth features of graphs. Furthermore, we\npropose an LCS metric space and apply an adjacent point merge operation to\nreduce its computational costs. Finally, we demonstrate that our proposed\nmethod outperforms many state-of-the-art graph kernel methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 11:59:14 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Huang", "Jianming", ""], ["Fang", "Zhongxi", ""], ["Kasai", "Hiroyuki", ""]]}, {"id": "2012.03618", "submitter": "David Mart\\'inez-Rubio", "authors": "David Mart\\'inez-Rubio", "title": "Global Riemannian Acceleration in Hyperbolic and Spherical Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We further research on the acceleration phenomenon on Riemannian manifolds by\nintroducing the first global first-order method that achieves the same rates as\naccelerated gradient descent in the Euclidean space for the optimization of\nsmooth and geodesically convex (g-convex) or strongly g-convex functions\ndefined on the hyperbolic space or a subset of the sphere, up to constants and\nlog factors. To the best of our knowledge, this is the first method that is\nproved to achieve these rates globally on functions defined on a Riemannian\nmanifold $\\mathcal{M}$ other than the Euclidean space. As a proxy, we solve a\nconstrained non-convex Euclidean problem, under a condition between convexity\nand quasar-convexity, of independent interest. Additionally, for any Riemannian\nmanifold of bounded sectional curvature, we provide reductions from\noptimization methods for smooth and g-convex functions to methods for smooth\nand strongly g-convex functions and vice versa.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 12:09:30 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 12:59:29 GMT"}, {"version": "v3", "created": "Fri, 29 Jan 2021 12:59:58 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Mart\u00ednez-Rubio", "David", ""]]}, {"id": "2012.03625", "submitter": "Andreas Brands{\\ae}ter", "authors": "Andreas Brands{\\ae}ter, Ingrid K. Glad", "title": "Explainable Artificial Intelligence: How Subsets of the Training Data\n  Affect a Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing interest in and demand for interpretations and\nexplanations of machine learning models and predictions in various application\nareas. In this paper, we consider data-driven models which are already\ndeveloped, implemented and trained. Our goal is to interpret the models and\nexplain and understand their predictions. Since the predictions made by\ndata-driven models rely heavily on the data used for training, we believe\nexplanations should convey information about how the training data affects the\npredictions. To do this, we propose a novel methodology which we call Shapley\nvalues for training data subset importance. The Shapley value concept\noriginates from coalitional game theory, developed to fairly distribute the\npayout among a set of cooperating players. We extend this to subset importance,\nwhere a prediction is explained by treating the subsets of the training data as\nplayers in a game where the predictions are the payouts. We describe and\nillustrate how the proposed method can be useful and demonstrate its\ncapabilities on several examples. We show how the proposed explanations can be\nused to reveal biasedness in models and erroneous training data. Furthermore,\nwe demonstrate that when predictions are accurately explained in a known\nsituation, then explanations of predictions by simple models correspond to the\nintuitive explanations. We argue that the explanations enable us to perceive\nmore of the inner workings of the algorithms, and illustrate how models\nproducing similar predictions can be based on very different parts of the\ntraining data. Finally, we show how we can use Shapley values for subset\nimportance to enhance our training data acquisition, and by this reducing\nprediction error.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 12:15:47 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Brands\u00e6ter", "Andreas", ""], ["Glad", "Ingrid K.", ""]]}, {"id": "2012.03628", "submitter": "Arkaitz Bidaurrazaga Barrueta", "authors": "Arkaitz Bidaurrazaga, Aritz P\\'erez and Marco Cap\\'o", "title": "Passive Approach for the K-means Problem on Streaming Data", "comments": "This version contains the supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Currently the amount of data produced worldwide is increasing beyond measure,\nthus a high volume of unsupervised data must be processed continuously. One of\nthe main unsupervised data analysis is clustering. In streaming data scenarios,\nthe data is composed by an increasing sequence of batches of samples where the\nconcept drift phenomenon may happen. In this paper, we formally define the\nStreaming $K$-means(S$K$M) problem, which implies a restart of the error\nfunction when a concept drift occurs. We propose a surrogate error function\nthat does not rely on concept drift detection. We proof that the surrogate is a\ngood approximation of the S$K$M error. Hence, we suggest an algorithm which\nminimizes this alternative error each time a new batch arrives. We present some\ninitialization techniques for streaming data scenarios as well. Besides\nproviding theoretical results, experiments demonstrate an improvement of the\nconverged error for the non-trivial initialization methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 12:23:33 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Bidaurrazaga", "Arkaitz", ""], ["P\u00e9rez", "Aritz", ""], ["Cap\u00f3", "Marco", ""]]}, {"id": "2012.03636", "submitter": "Kangqiao Liu", "authors": "Kangqiao Liu, Liu Ziyin, Masahito Ueda", "title": "Noise and Fluctuation of Finite Learning Rate Stochastic Gradient\n  Descent", "comments": "Camera-ready version for the Thirty-eighth International Conference\n  on Machine Learning (ICML 2021). 12 + 14 pages, 6 + 3 figures, 1 + 0 table.\n  *First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the vanishing learning rate regime, stochastic gradient descent (SGD) is\nnow relatively well understood. In this work, we propose to study the basic\nproperties of SGD and its variants in the non-vanishing learning rate regime.\nThe focus is on deriving exactly solvable results and discussing their\nimplications. The main contributions of this work are to derive the stationary\ndistribution for discrete-time SGD in a quadratic loss function with and\nwithout momentum; in particular, one implication of our result is that the\nfluctuation caused by discrete-time dynamics takes a distorted shape and is\ndramatically larger than a continuous-time theory could predict. Examples of\napplications of the proposed theory considered in this work include the\napproximation error of variants of SGD, the effect of minibatch noise, the\noptimal Bayesian inference, the escape rate from a sharp minimum, and the\nstationary covariance of a few second-order methods including damped Newton's\nmethod, natural gradient descent, and Adam.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 12:31:43 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 09:20:52 GMT"}, {"version": "v3", "created": "Fri, 12 Feb 2021 08:43:27 GMT"}, {"version": "v4", "created": "Fri, 11 Jun 2021 08:31:26 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Liu", "Kangqiao", ""], ["Ziyin", "Liu", ""], ["Ueda", "Masahito", ""]]}, {"id": "2012.03653", "submitter": "Yaniv Shulman", "authors": "Yaniv Shulman", "title": "DiffPrune: Neural Network Pruning with Deterministic Approximate Binary\n  Gates and $L_0$ Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern neural network architectures typically have many millions of\nparameters and can be pruned significantly without substantial loss in\neffectiveness which demonstrates they are over-parameterized. The contribution\nof this work is two-fold. The first is a method for approximating a\nmultivariate Bernoulli random variable by means of a deterministic and\ndifferentiable transformation of any real-valued multivariate random variable.\nThe second is a method for model selection by element-wise multiplication of\nparameters with approximate binary gates that may be computed deterministically\nor stochastically and take on exact zero values. Sparsity is encouraged by the\ninclusion of a surrogate regularization to the $L_0$ loss. Since the method is\ndifferentiable it enables straightforward and efficient learning of model\narchitectures by an empirical risk minimization procedure with stochastic\ngradient descent and theoretically enables conditional computation during\ntraining. The method also supports any arbitrary group sparsity over parameters\nor activations and therefore offers a framework for unstructured or flexible\nstructured model pruning. To conclude experiments are performed to demonstrate\nthe effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 13:08:56 GMT"}, {"version": "v2", "created": "Sat, 6 Mar 2021 06:55:10 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Shulman", "Yaniv", ""]]}, {"id": "2012.03694", "submitter": "Ernest Fokoue", "authors": "Matthew Corsetti and Ernest Fokou\\'e", "title": "Nonnegative Matrix Factorization with Toeplitz Penalty", "comments": "15 pages, 6 figures, 3 tables", "journal-ref": "Journal.of.Informatics.and.Mathematical.Sciences 10 (2018) 201-215", "doi": "10.26713/jims.v10i1-2.851", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nonnegative Matrix Factorization (NMF) is an unsupervised learning algorithm\nthat produces a linear, parts-based approximation of a data matrix. NMF\nconstructs a nonnegative low rank basis matrix and a nonnegative low rank\nmatrix of weights which, when multiplied together, approximate the data matrix\nof interest using some cost function. The NMF algorithm can be modified to\ninclude auxiliary constraints which impose task-specific penalties or\nrestrictions on the cost function of the matrix factorization. In this paper we\npropose a new NMF algorithm that makes use of non-data dependent auxiliary\nconstraints which incorporate a Toeplitz matrix into the multiplicative\nupdating of the basis and weight matrices. We compare the facial recognition\nperformance of our new Toeplitz Nonnegative Matrix Factorization (TNMF)\nalgorithm to the performance of the Zellner Nonnegative Matrix Factorization\n(ZNMF) algorithm which makes use of data-dependent auxiliary constraints. We\nalso compare the facial recognition performance of the two aforementioned\nalgorithms with the performance of several preexisting constrained NMF\nalgorithms that have non-data-dependent penalties. The facial recognition\nperformances are evaluated using the Cambridge ORL Database of Faces and the\nYale Database of Faces.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 13:49:23 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Corsetti", "Matthew", ""], ["Fokou\u00e9", "Ernest", ""]]}, {"id": "2012.03715", "submitter": "Ali Taylan Cemgil", "authors": "A. Taylan Cemgil, Sumedh Ghaisas, Krishnamurthy Dvijotham, Sven Gowal,\n  Pushmeet Kohli", "title": "Autoencoding Variational Autoencoder", "comments": "Neurips 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Does a Variational AutoEncoder (VAE) consistently encode typical samples\ngenerated from its decoder? This paper shows that the perhaps surprising answer\nto this question is `No'; a (nominally trained) VAE does not necessarily\namortize inference for typical samples that it is capable of generating. We\nstudy the implications of this behaviour on the learned representations and\nalso the consequences of fixing it by introducing a notion of self consistency.\nOur approach hinges on an alternative construction of the variational\napproximation distribution to the true posterior of an extended VAE model with\na Markov chain alternating between the encoder and the decoder. The method can\nbe used to train a VAE model from scratch or given an already trained VAE, it\ncan be run as a post processing step in an entirely self supervised way without\naccess to the original training data. Our experimental analysis reveals that\nencoders trained with our self-consistency approach lead to representations\nthat are robust (insensitive) to perturbations in the input introduced by\nadversarial attacks. We provide experimental results on the ColorMnist and\nCelebA benchmark datasets that quantify the properties of the learned\nrepresentations and compare the approach with a baseline that is specifically\ntrained for the desired property.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 14:16:14 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Cemgil", "A. Taylan", ""], ["Ghaisas", "Sumedh", ""], ["Dvijotham", "Krishnamurthy", ""], ["Gowal", "Sven", ""], ["Kohli", "Pushmeet", ""]]}, {"id": "2012.03725", "submitter": "Jingli Wang", "authors": "Huan Qing and Jingli Wang", "title": "Mixed-SCORE+ for mixed membership community detection", "comments": "17 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed-SCORE is a recent approach for mixed membership community detection\nproposed by Jin et al. (2017) which is an extension of SCORE (Jin, 2015). In\nthe note Jin et al. (2018), the authors propose SCORE+ as an improvement of\nSCORE to handle with weak signal networks. In this paper, we propose a method\ncalled Mixed-SCORE+ designed based on the Mixed-SCORE and SCORE+, therefore\nMixed-SCORE+ inherits nice properties of both Mixed-SCORE and SCORE+. In the\nproposed method, we consider K+1 eigenvectors when there are K communities to\ndetect weak signal networks. And we also construct vertices hunting and\nmembership reconstruction steps to solve the problem of mixed membership\ncommunity detection. Compared with several benchmark methods, numerical results\nshow that Mixed-SCORE+ provides a significant improvement on the Polblogs\nnetwork and two weak signal networks Simmons and Caltech, with error rates\n54/1222, 125/1137 and 94/590, respectively. Furthermore, Mixed-SCORE+ enjoys\nexcellent performances on the SNAP ego-networks.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 14:21:19 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Qing", "Huan", ""], ["Wang", "Jingli", ""]]}, {"id": "2012.03740", "submitter": "Ahc\\`ene Boubekki", "authors": "Ahc\\`ene Boubekki, Michael Kampffmeyer, Robert Jenssen, Ulf Brefeld", "title": "Joint Optimization of an Autoencoder for Clustering and Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep embedded clustering has become a dominating approach to unsupervised\ncategorization of objects with deep neural networks. The optimization of the\nmost popular methods alternates between the training of a deep autoencoder and\na k-means clustering of the autoencoder's embedding. The diachronic setting,\nhowever, prevents the former to benefit from valuable information acquired by\nthe latter. In this paper, we present an alternative where the autoencoder and\nthe clustering are learned simultaneously. This is achieved by providing novel\ntheoretical insight, where we show that the objective function of a certain\nclass of Gaussian mixture models (GMMs) can naturally be rephrased as the loss\nfunction of a one-hidden layer autoencoder thus inheriting the built-in\nclustering capabilities of the GMM. That simple neural network, referred to as\nthe clustering module, can be integrated into a deep autoencoder resulting in a\ndeep clustering model able to jointly learn a clustering and an embedding.\nExperiments confirm the equivalence between the clustering module and Gaussian\nmixture models. Further evaluations affirm the empirical relevance of our deep\narchitecture as it outperforms related baselines on several data sets.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 14:38:10 GMT"}, {"version": "v2", "created": "Sat, 1 May 2021 20:24:34 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Boubekki", "Ahc\u00e8ne", ""], ["Kampffmeyer", "Michael", ""], ["Jenssen", "Robert", ""], ["Brefeld", "Ulf", ""]]}, {"id": "2012.03761", "submitter": "Raghu Pasupathy", "authors": "Raghu Pasupathy and Yongjia Song", "title": "Adaptive Sequential SAA for Solving Two-stage Stochastic Linear Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present adaptive sequential SAA (sample average approximation) algorithms\nto solve large-scale two-stage stochastic linear programs. The iterative\nalgorithm framework we propose is organized into \\emph{outer} and \\emph{inner}\niterations as follows: during each outer iteration, a sample-path problem is\nimplicitly generated using a sample of observations or ``scenarios,\" and solved\nonly \\emph{imprecisely}, to within a tolerance that is chosen\n\\emph{adaptively}, by balancing the estimated statistical error against\nsolution error. The solutions from prior iterations serve as \\emph{warm starts}\nto aid efficient solution of the (piecewise linear convex) sample-path\noptimization problems generated on subsequent iterations. The generated\nscenarios can be independent and identically distributed (iid), or dependent,\nas in Monte Carlo generation using Latin-hypercube sampling, antithetic\nvariates, or randomized quasi-Monte Carlo. We first characterize the\nalmost-sure convergence (and convergence in mean) of the optimality gap and the\ndistance of the generated stochastic iterates to the true solution set. We then\ncharacterize the corresponding iteration complexity and work complexity rates\nas a function of the sample size schedule, demonstrating that the best\nachievable work complexity rate is Monte Carlo canonical and analogous to the\ngeneric $\\mathcal{O}(\\epsilon^{-2})$ optimal complexity for non-smooth convex\noptimization. We report extensive numerical tests that indicate favorable\nperformance, due primarily to the use of a sequential framework with an optimal\nsample size schedule, and the use of warm starts. The proposed algorithm can be\nstopped in finite-time to return a solution endowed with a probabilistic\nguarantee on quality.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 14:58:16 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Pasupathy", "Raghu", ""], ["Song", "Yongjia", ""]]}, {"id": "2012.03772", "submitter": "Leon Bungert", "authors": "Tim Roith, Leon Bungert", "title": "Continuum Limit of Lipschitz Learning on Graphs", "comments": "36 pages, acknowledgement added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.AP math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tackling semi-supervised learning problems with graph-based methods have\nbecome a trend in recent years since graphs can represent all kinds of data and\nprovide a suitable framework for studying continuum limits, e.g., of\ndifferential operators. A popular strategy here is $p$-Laplacian learning,\nwhich poses a smoothness condition on the sought inference function on the set\nof unlabeled data. For $p<\\infty$ continuum limits of this approach were\nstudied using tools from $\\Gamma$-convergence. For the case $p=\\infty$, which\nis referred to as Lipschitz learning, continuum limits of the related\ninfinity-Laplacian equation were studied using the concept of viscosity\nsolutions.\n  In this work, we prove continuum limits of Lipschitz learning using\n$\\Gamma$-convergence. In particular, we define a sequence of functionals which\napproximate the largest local Lipschitz constant of a graph function and prove\n$\\Gamma$-convergence in the $L^\\infty$-topology to the supremum norm of the\ngradient as the graph becomes denser. Furthermore, we show compactness of the\nfunctionals which implies convergence of minimizers. In our analysis we allow a\nvarying set of labeled data which converges to a general closed set in the\nHausdorff distance. We apply our results to nonlinear ground states and, as a\nby-product, prove convergence of graph distance functions to geodesic distance\nfunctions.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 15:10:35 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 17:07:44 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Roith", "Tim", ""], ["Bungert", "Leon", ""]]}, {"id": "2012.03780", "submitter": "Benjamin Guedj", "authors": "Th\\'eophile Cantelobre and Benjamin Guedj and Mar\\'ia P\\'erez-Ortiz\n  and John Shawe-Taylor", "title": "A PAC-Bayesian Perspective on Structured Prediction with Implicit Loss\n  Embeddings", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many practical machine learning tasks can be framed as Structured prediction\nproblems, where several output variables are predicted and considered\ninterdependent. Recent theoretical advances in structured prediction have\nfocused on obtaining fast rates convergence guarantees, especially in the\nImplicit Loss Embedding (ILE) framework. PAC-Bayes has gained interest recently\nfor its capacity of producing tight risk bounds for predictor distributions.\nThis work proposes a novel PAC-Bayes perspective on the ILE Structured\nprediction framework. We present two generalization bounds, on the risk and\nexcess risk, which yield insights into the behavior of ILE predictors. Two\nlearning algorithms are derived from these bounds. The algorithms are\nimplemented and their behavior analyzed, with source code available at\n\\url{https://github.com/theophilec/PAC-Bayes-ILE-Structured-Prediction}.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 15:19:43 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 17:20:30 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Cantelobre", "Th\u00e9ophile", ""], ["Guedj", "Benjamin", ""], ["P\u00e9rez-Ortiz", "Mar\u00eda", ""], ["Shawe-Taylor", "John", ""]]}, {"id": "2012.03787", "submitter": "Eric Pahl", "authors": "Eric S. Pahl, W. Nick Street, Hans J. Johnson, Alan I. Reed", "title": "A predictive model for kidney transplant graft survival using machine\n  learning", "comments": "This work has been published: Pahl ES, Street WN, Johnson HJ, Reed\n  AI. \"A Predictive Model for Kidney Transplant Graft Survival Using Machine\n  Learning.\" 4th International Conference on Computer Science and Information\n  Technology (COMIT 2020), November 28-29, 2020, Dubai, UAE. ISBN:\n  978-1-925953-30-5. Volume 10, Number 16.10.5121/csit.2020.101609", "journal-ref": "4th International Conference on Computer Science and Information\n  Technology (COMIT 2020), November 28-29, 2020, Dubai, UAE. ISBN:\n  978-1-925953-30-5. Volume 10, Number 16", "doi": "10.5121/csit.2020.101609", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Kidney transplantation is the best treatment for end-stage renal failure\npatients. The predominant method used for kidney quality assessment is the Cox\nregression-based, kidney donor risk index. A machine learning method may\nprovide improved prediction of transplant outcomes and help decision-making. A\npopular tree-based machine learning method, random forest, was trained and\nevaluated with the same data originally used to develop the risk index (70,242\nobservations from 1995-2005). The random forest successfully predicted an\nadditional 2,148 transplants than the risk index with equal type II error rates\nof 10%. Predicted results were analyzed with follow-up survival outcomes up to\n240 months after transplant using Kaplan-Meier analysis and confirmed that the\nrandom forest performed significantly better than the risk index (p<0.05). The\nrandom forest predicted significantly more successful and longer-surviving\ntransplants than the risk index. Random forests and other machine learning\nmodels may improve transplant decisions.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 15:29:51 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Pahl", "Eric S.", ""], ["Street", "W. Nick", ""], ["Johnson", "Hans J.", ""], ["Reed", "Alan I.", ""]]}, {"id": "2012.03808", "submitter": "Laurent Dinh", "authors": "Charline Le Lan, Laurent Dinh", "title": "Perfect density models cannot guarantee anomaly detection", "comments": "8 pages and 7 figures in main content, 4 pages of bibliography, and 2\n  pages and 3 figures in Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to the tractability of their likelihood, some deep generative models\nshow promise for seemingly straightforward but important applications like\nanomaly detection, uncertainty estimation, and active learning. However, the\nlikelihood values empirically attributed to anomalies conflict with the\nexpectations these proposed applications suggest. In this paper, we take a\ncloser look at the behavior of distribution densities and show that these\nquantities carry less meaningful information than previously thought, beyond\nestimation issues or the curse of dimensionality. We conclude that the use of\nthese likelihoods for out-of-distribution detection relies on strong and\nimplicit hypotheses, and highlight the necessity of explicitly formulating\nthese assumptions for reliable anomaly detection.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 15:50:11 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 18:03:29 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Lan", "Charline Le", ""], ["Dinh", "Laurent", ""]]}, {"id": "2012.03809", "submitter": "Song Fang", "authors": "Song Fang and Quanyan Zhu", "title": "Independent Elliptical Distributions Minimize Their $\\mathcal{W}_2$\n  Wasserstein Distance from Independent Elliptical Distributions with the Same\n  Density Generator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI cs.LG eess.SP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short note is on a property of the $\\mathcal{W}_2$ Wasserstein distance\nwhich indicates that independent elliptical distributions minimize their\n$\\mathcal{W}_2$ Wasserstein distance from given independent elliptical\ndistributions with the same density generators. Furthermore, we examine the\nimplications of this property in the Gelbrich bound when the distributions are\nnot necessarily elliptical. Meanwhile, we also generalize the results to the\ncases when the distributions are not independent. The primary purpose of this\nnote is for the referencing of papers that need to make use of this property or\nits implications.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 15:52:02 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Fang", "Song", ""], ["Zhu", "Quanyan", ""]]}, {"id": "2012.03842", "submitter": "Jong Chul Ye", "authors": "Gyutaek Oh, Hyokyoung Bae, Hyun-Seo Ahn, Sung-Hong Park, and Jong Chul\n  Ye", "title": "CycleQSM: Unsupervised QSM Deep Learning using Physics-Informed CycleGAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantitative susceptibility mapping (QSM) is a useful magnetic resonance\nimaging (MRI) technique which provides spatial distribution of magnetic\nsusceptibility values of tissues. QSMs can be obtained by deconvolving the\ndipole kernel from phase images, but the spectral nulls in the dipole kernel\nmake the inversion ill-posed. In recent times, deep learning approaches have\nshown a comparable QSM reconstruction performance as the classic approaches,\ndespite the fast reconstruction time. Most of the existing deep learning\nmethods are, however, based on supervised learning, so matched pairs of input\nphase images and the ground-truth maps are needed. Moreover, it was reported\nthat the supervised learning often leads to underestimated QSM values. To\naddress this, here we propose a novel unsupervised QSM deep learning method\nusing physics-informed cycleGAN, which is derived from optimal transport\nperspective. In contrast to the conventional cycleGAN, our novel cycleGAN has\nonly one generator and one discriminator thanks to the known dipole kernel.\nExperimental results confirm that the proposed method provides more accurate\nQSM maps compared to the existing deep learning approaches, and provide\ncompetitive performance to the best classical approaches despite the ultra-fast\nreconstruction.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 16:46:15 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Oh", "Gyutaek", ""], ["Bae", "Hyokyoung", ""], ["Ahn", "Hyun-Seo", ""], ["Park", "Sung-Hong", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2012.03889", "submitter": "Ernest Fokoue", "authors": "Matthew Corsetti and Ernest Fokou\\'e", "title": "Nonnegative Matrix Factorization with Zellner Penalty", "comments": "10 pages, 4 figures, 2 tables", "journal-ref": "Open Journal of Statistics 5 (2015) 777-786", "doi": "10.4236/ojs.2015.57077", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nonnegative matrix factorization (NMF) is a relatively new unsupervised\nlearning algorithm that decomposes a nonnegative data matrix into a\nparts-based, lower dimensional, linear representation of the data. NMF has\napplications in image processing, text mining, recommendation systems and a\nvariety of other fields. Since its inception, the NMF algorithm has been\nmodified and explored by numerous authors. One such modification involves the\naddition of auxiliary constraints to the objective function of the\nfactorization. The purpose of these auxiliary constraints is to impose\ntask-specific penalties or restrictions on the objective function. Though many\nauxiliary constraints have been studied, none have made use of data-dependent\npenalties. In this paper, we propose Zellner nonnegative matrix factorization\n(ZNMF), which uses data-dependent auxiliary constraints. We assess the facial\nrecognition performance of the ZNMF algorithm and several other well-known\nconstrained NMF algorithms using the Cambridge ORL database.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 18:11:02 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Corsetti", "Matthew", ""], ["Fokou\u00e9", "Ernest", ""]]}, {"id": "2012.04002", "submitter": "Sholom Schechtman", "authors": "A. Barakat, P. Bianchi, W. Hachem, and Sh. Schechtman", "title": "Stochastic optimization with momentum: convergence, fluctuations, and\n  traps avoidance", "comments": "Accepted for publication in Electronic Journal of Statistics. 49\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.PR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, a general stochastic optimization procedure is studied,\nunifying several variants of the stochastic gradient descent such as, among\nothers, the stochastic heavy ball method, the Stochastic Nesterov Accelerated\nGradient algorithm (S-NAG), and the widely used Adam algorithm. The algorithm\nis seen as a noisy Euler discretization of a non-autonomous ordinary\ndifferential equation, recently introduced by Belotto da Silva and Gazeau,\nwhich is analyzed in depth. Assuming that the objective function is non-convex\nand differentiable, the stability and the almost sure convergence of the\niterates to the set of critical points are established. A noteworthy special\ncase is the convergence proof of S-NAG in a non-convex setting. Under some\nassumptions, the convergence rate is provided under the form of a Central Limit\nTheorem. Finally, the non-convergence of the algorithm to undesired critical\npoints, such as local maxima or saddle points, is established. Here, the main\ningredient is a new avoidance of traps result for non-autonomous settings,\nwhich is of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 19:14:49 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 14:43:43 GMT"}, {"version": "v3", "created": "Sat, 10 Jul 2021 20:58:33 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Barakat", "A.", ""], ["Bianchi", "P.", ""], ["Hachem", "W.", ""], ["Schechtman", "Sh.", ""]]}, {"id": "2012.04023", "submitter": "Song Fang", "authors": "Song Fang and Quanyan Zhu", "title": "The Spectral-Domain $\\mathcal{W}_2$ Wasserstein Distance for Elliptical\n  Processes and the Spectral-Domain Gelbrich Bound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG eess.SP math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short note, we introduce the spectral-domain $\\mathcal{W}_2$\nWasserstein distance for elliptical stochastic processes in terms of their\npower spectra. We also introduce the spectral-domain Gelbrich bound for\nprocesses that are not necessarily elliptical.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 20:02:33 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 21:16:36 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Fang", "Song", ""], ["Zhu", "Quanyan", ""]]}, {"id": "2012.04061", "submitter": "Abolfazl Hashemi", "authors": "Rudrajit Das, Anish Acharya, Abolfazl Hashemi, Sujay Sanghavi,\n  Inderjit S. Dhillon, Ufuk Topcu", "title": "Faster Non-Convex Federated Learning via Global and Local Momentum", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose \\texttt{FedGLOMO}, the first (first-order) FL\nalgorithm that achieves the optimal iteration complexity (i.e matching the\nknown lower bound) on smooth non-convex objectives -- without using clients'\nfull gradient in each round. Our key algorithmic idea that enables attaining\nthis optimal complexity is applying judicious momentum terms that promote\nvariance reduction in both the local updates at the clients, and the global\nupdate at the server. Our algorithm is also provably optimal even with\ncompressed communication between the clients and the server, which is an\nimportant consideration in the practical deployment of FL algorithms. Our\nexperiments illustrate the intrinsic variance reduction effect of\n\\texttt{FedGLOMO} which implicitly suppresses client-drift in heterogeneous\ndata distribution settings and promotes communication-efficiency. As a prequel\nto \\texttt{FedGLOMO}, we propose \\texttt{FedLOMO} which applies momentum only\nin the local client updates. We establish that \\texttt{FedLOMO} enjoys improved\nconvergence rates under common non-convex settings compared to prior work, and\nwith fewer assumptions.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 21:05:31 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2020 00:16:40 GMT"}, {"version": "v3", "created": "Fri, 19 Feb 2021 22:57:58 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Das", "Rudrajit", ""], ["Acharya", "Anish", ""], ["Hashemi", "Abolfazl", ""], ["Sanghavi", "Sujay", ""], ["Dhillon", "Inderjit S.", ""], ["Topcu", "Ufuk", ""]]}, {"id": "2012.04084", "submitter": "Thomas Oliver", "authors": "Yang-Hui He, Kyu-Hwan Lee, Thomas Oliver", "title": "Machine-Learning Arithmetic Curves", "comments": "21 pages, 1 figure, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NT hep-th stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that standard machine-learning algorithms may be trained to predict\ncertain invariants of low genus arithmetic curves. Using datasets of size\naround one hundred thousand, we demonstrate the utility of machine-learning in\nclassification problems pertaining to the BSD invariants of an elliptic curve\n(including its rank and torsion subgroup), and the analogous invariants of a\ngenus 2 curve. Our results show that a trained machine can efficiently classify\ncurves according to these invariants with high accuracies (>0.97). For problems\nsuch as distinguishing between torsion orders, and the recognition of integral\npoints, the accuracies can reach 0.998.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 22:04:10 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["He", "Yang-Hui", ""], ["Lee", "Kyu-Hwan", ""], ["Oliver", "Thomas", ""]]}, {"id": "2012.04104", "submitter": "Fereshte Khani", "authors": "Fereshte Khani, Percy Liang", "title": "Removing Spurious Features can Hurt Accuracy and Affect Groups\n  Disproportionately", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of spurious features interferes with the goal of obtaining\nrobust models that perform well across many groups within the population. A\nnatural remedy is to remove spurious features from the model. However, in this\nwork we show that removal of spurious features can decrease accuracy due to the\ninductive biases of overparameterized models. We completely characterize how\nthe removal of spurious features affects accuracy across different groups (more\ngenerally, test distributions) in noiseless overparameterized linear\nregression. In addition, we show that removal of spurious feature can decrease\nthe accuracy even in balanced datasets -- each target co-occurs equally with\neach spurious feature; and it can inadvertently make the model more susceptible\nto other spurious features. Finally, we show that robust self-training can\nremove spurious features without affecting the overall accuracy. Experiments on\nthe Toxic-Comment-Detectoin and CelebA datasets show that our results hold in\nnon-linear models.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 23:08:59 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Khani", "Fereshte", ""], ["Liang", "Percy", ""]]}, {"id": "2012.04115", "submitter": "Guillermo Valle-P\\'erez", "authors": "Guillermo Valle-P\\'erez, Ard A. Louis", "title": "Generalization bounds for deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generalization in deep learning has been the topic of much recent theoretical\nand empirical research. Here we introduce desiderata for techniques that\npredict generalization errors for deep learning models in supervised learning.\nSuch predictions should 1) scale correctly with data complexity; 2) scale\ncorrectly with training set size; 3) capture differences between architectures;\n4) capture differences between optimization algorithms; 5) be quantitatively\nnot too far from the true error (in particular, be non-vacuous); 6) be\nefficiently computable; and 7) be rigorous. We focus on generalization error\nupper bounds, and introduce a categorisation of bounds depending on assumptions\non the algorithm and data. We review a wide range of existing approaches, from\nclassical VC dimension to recent PAC-Bayesian bounds, commenting on how well\nthey perform against the desiderata.\n  We next use a function-based picture to derive a marginal-likelihood\nPAC-Bayesian bound. This bound is, by one definition, optimal up to a\nmultiplicative constant in the asymptotic limit of large training sets, as long\nas the learning curve follows a power law, which is typically found in practice\nfor deep learning problems. Extensive empirical analysis demonstrates that our\nmarginal-likelihood PAC-Bayes bound fulfills desiderata 1-3 and 5. The results\nfor 6 and 7 are promising, but not yet fully conclusive, while only desideratum\n4 is currently beyond the scope of our bound. Finally, we comment on why this\nfunction-based bound performs significantly better than current parameter-based\nPAC-Bayes bounds.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 23:45:09 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 15:00:23 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Valle-P\u00e9rez", "Guillermo", ""], ["Louis", "Ard A.", ""]]}, {"id": "2012.04160", "submitter": "Sahin Lale", "authors": "Sahin Lale, Oguzhan Teke, Babak Hassibi, Anima Anandkumar", "title": "Stability and Identification of Random Asynchronous Linear\n  Time-Invariant Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many computational tasks and dynamical systems, asynchrony and\nrandomization are naturally present and have been considered as ways to\nincrease the speed and reduce the cost of computation while compromising the\naccuracy and convergence rate. In this work, we show the additional benefits of\nrandomization and asynchrony on the stability of linear dynamical systems. We\nintroduce a natural model for random asynchronous linear time-invariant (LTI)\nsystems which generalizes the standard (synchronous) LTI systems. In this\nmodel, each state variable is updated randomly and asynchronously with some\nprobability according to the underlying system dynamics. We examine how the\nmean-square stability of random asynchronous LTI systems vary with respect to\nrandomization and asynchrony. Surprisingly, we show that the stability of\nrandom asynchronous LTI systems does not imply or is not implied by the\nstability of the synchronous variant of the system and an unstable synchronous\nsystem can be stabilized via randomization and/or asynchrony. We further study\na special case of the introduced model, namely randomized LTI systems, where\neach state element is updated randomly with some fixed but unknown probability.\nWe consider the problem of system identification of unknown randomized LTI\nsystems using the precise characterization of mean-square stability via\nextended Lyapunov equation. For unknown randomized LTI systems, we propose a\nsystematic identification method to recover the underlying dynamics. Given a\nsingle input/output trajectory, our method estimates the model parameters that\ngovern the system dynamics, the update probability of state variables, and the\nnoise covariance using the correlation matrices of collected data and the\nextended Lyapunov equation. Finally, we empirically demonstrate that the\nproposed method consistently recovers the underlying system dynamics with the\noptimal rate.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 02:00:04 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Lale", "Sahin", ""], ["Teke", "Oguzhan", ""], ["Hassibi", "Babak", ""], ["Anandkumar", "Anima", ""]]}, {"id": "2012.04171", "submitter": "Joshua Chang", "authors": "Joshua C. Chang, Patrick Fletcher, Jungmin Han, Ted L. Chang,\n  Shashaank Vattikuti, Bart Desmet, Ayah Zirikly, Carson C. Chow", "title": "Sparse encoding for more-interpretable feature-selecting representations\n  in probabilistic matrix factorization", "comments": "Fixed typo in Eq 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Dimensionality reduction methods for count data are critical to a wide range\nof applications in medical informatics and other fields where model\ninterpretability is paramount. For such data, hierarchical Poisson matrix\nfactorization (HPF) and other sparse probabilistic non-negative matrix\nfactorization (NMF) methods are considered to be interpretable generative\nmodels. They consist of sparse transformations for decoding their learned\nrepresentations into predictions. However, sparsity in representation decoding\ndoes not necessarily imply sparsity in the encoding of representations from the\noriginal data features. HPF is often incorrectly interpreted in the literature\nas if it possesses encoder sparsity. The distinction between decoder sparsity\nand encoder sparsity is subtle but important. Due to the lack of encoder\nsparsity, HPF does not possess the column-clustering property of classical NMF\n-- the factor loading matrix does not sufficiently define how each factor is\nformed from the original features. We address this deficiency by\nself-consistently enforcing encoder sparsity, using a generalized additive\nmodel (GAM), thereby allowing one to relate each representation coordinate to a\nsubset of the original data features. In doing so, the method also gains the\nability to perform feature selection. We demonstrate our method on simulated\ndata and give an example of how encoder sparsity is of practical use in a\nconcrete application of representing inpatient comorbidities in Medicare\npatients.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 02:27:22 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 18:19:54 GMT"}, {"version": "v3", "created": "Tue, 29 Dec 2020 19:08:55 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Chang", "Joshua C.", ""], ["Fletcher", "Patrick", ""], ["Han", "Jungmin", ""], ["Chang", "Ted L.", ""], ["Vattikuti", "Shashaank", ""], ["Desmet", "Bart", ""], ["Zirikly", "Ayah", ""], ["Chow", "Carson C.", ""]]}, {"id": "2012.04187", "submitter": "Binghui Wang", "authors": "Binghui Wang, Ang Li, Hai Li, Yiran Chen", "title": "GraphFL: A Federated Learning Framework for Semi-Supervised Node\n  Classification on Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based semi-supervised node classification (GraphSSC) has wide\napplications, ranging from networking and security to data mining and machine\nlearning, etc. However, existing centralized GraphSSC methods are impractical\nto solve many real-world graph-based problems, as collecting the entire graph\nand labeling a reasonable number of labels is time-consuming and costly, and\ndata privacy may be also violated. Federated learning (FL) is an emerging\nlearning paradigm that enables collaborative learning among multiple clients,\nwhich can mitigate the issue of label scarcity and protect data privacy as\nwell. Therefore, performing GraphSSC under the FL setting is a promising\nsolution to solve real-world graph-based problems. However, existing FL methods\n1) perform poorly when data across clients are non-IID, 2) cannot handle data\nwith new label domains, and 3) cannot leverage unlabeled data, while all these\nissues naturally happen in real-world graph-based problems. To address the\nabove issues, we propose the first FL framework, namely GraphFL, for\nsemi-supervised node classification on graphs. Our framework is motivated by\nmeta-learning methods. Specifically, we propose two GraphFL methods to\nrespectively address the non-IID issue in graph data and handle the tasks with\nnew label domains. Furthermore, we design a self-training method to leverage\nunlabeled graph data. We adopt representative graph neural networks as GraphSSC\nmethods and evaluate GraphFL on multiple graph datasets. Experimental results\ndemonstrate that GraphFL significantly outperforms the compared FL baseline and\nGraphFL with self-training can obtain better performance.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 03:13:29 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Wang", "Binghui", ""], ["Li", "Ang", ""], ["Li", "Hai", ""], ["Chen", "Yiran", ""]]}, {"id": "2012.04221", "submitter": "Tian Li", "authors": "Tian Li, Shengyuan Hu, Ahmad Beirami, Virginia Smith", "title": "Ditto: Fair and Robust Federated Learning Through Personalization", "comments": "Accepted by ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fairness and robustness are two important concerns for federated learning\nsystems. In this work, we identify that robustness to data and model poisoning\nattacks and fairness, measured as the uniformity of performance across devices,\nare competing constraints in statistically heterogeneous networks. To address\nthese constraints, we propose employing a simple, general framework for\npersonalized federated learning, Ditto, that can inherently provide fairness\nand robustness benefits, and develop a scalable solver for it. Theoretically,\nwe analyze the ability of Ditto to achieve fairness and robustness\nsimultaneously on a class of linear problems. Empirically, across a suite of\nfederated datasets, we show that Ditto not only achieves competitive\nperformance relative to recent personalization methods, but also enables more\naccurate, robust, and fair models relative to state-of-the-art fair or robust\nbaselines.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 05:15:39 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 18:08:49 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 20:54:14 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Li", "Tian", ""], ["Hu", "Shengyuan", ""], ["Beirami", "Ahmad", ""], ["Smith", "Virginia", ""]]}, {"id": "2012.04225", "submitter": "Hideitsu Hino", "authors": "Hideitsu Hino", "title": "Active Learning: Problem Settings and Recent Developments", "comments": "31 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In supervised learning, acquiring labeled training data for a predictive\nmodel can be very costly, but acquiring a large amount of unlabeled data is\noften quite easy. Active learning is a method of obtaining predictive models\nwith high precision at a limited cost through the adaptive selection of samples\nfor labeling. This paper explains the basic problem settings of active learning\nand recent research trends. In particular, research on learning acquisition\nfunctions to select samples from the data for labeling, theoretical work on\nactive learning algorithms, and stopping criteria for sequential data\nacquisition are highlighted. Application examples for material development and\nmeasurement are introduced.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 05:24:06 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 00:56:31 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Hino", "Hideitsu", ""]]}, {"id": "2012.04228", "submitter": "Han-Hsien Huang", "authors": "Han-Hsien Huang, Mi-Yen Yeh", "title": "Accelerating Continuous Normalizing Flow with Trajectory Polynomial\n  Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an approach to effectively accelerating the\ncomputation of continuous normalizing flow (CNF), which has been proven to be a\npowerful tool for the tasks such as variational inference and density\nestimation. The training time cost of CNF can be extremely high because the\nrequired number of function evaluations (NFE) for solving corresponding\nordinary differential equations (ODE) is very large. We think that the high NFE\nresults from large truncation errors of solving ODEs. To address the problem,\nwe propose to add a regularization. The regularization penalizes the difference\nbetween the trajectory of the ODE and its fitted polynomial regression. The\ntrajectory of ODE will approximate a polynomial function, and thus the\ntruncation error will be smaller. Furthermore, we provide two proofs and claim\nthat the additional regularization does not harm training quality. Experimental\nresults show that our proposed method can result in 42.3% to 71.3% reduction of\nNFE on the task of density estimation, and 19.3% to 32.1% reduction of NFE on\nvariational auto-encoder, while the testing losses are not affected.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 05:41:23 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 22:41:51 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Huang", "Han-Hsien", ""], ["Yeh", "Mi-Yen", ""]]}, {"id": "2012.04231", "submitter": "Ziqi Chen", "authors": "Ziqi Chen, Martin Renqiang Min, Srinivasan Parthasarathy, Xia Ning", "title": "Molecule Optimization via Fragment-based Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In drug discovery, molecule optimization is an important step in order to\nmodify drug candidates into better ones in terms of desired drug properties.\nWith the recent advance of Artificial Intelligence, this traditionally in vitro\nprocess has been increasingly facilitated by in silico approaches. We present\nan innovative in silico approach to computationally optimizing molecules and\nformulate the problem as to generate optimized molecular graphs via deep\ngenerative models. Our generative models follow the key idea of fragment-based\ndrug design, and optimize molecules by modifying their small fragments. Our\nmodels learn how to identify the to-be-optimized fragments and how to modify\nsuch fragments by learning from the difference of molecules that have good and\nbad properties. In optimizing a new molecule, our models apply the learned\nsignals to decode optimized fragments at the predicted location of the\nfragments. We also construct multiple such models into a pipeline such that\neach of the models in the pipeline is able to optimize one fragment, and thus\nthe entire pipeline is able to modify multiple fragments of molecule if needed.\nWe compare our models with other state-of-the-art methods on benchmark datasets\nand demonstrate that our methods significantly outperform others with more than\n80% property improvement under moderate molecular similarity constraints, and\nmore than 10% property improvement under high molecular similarity constraints.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 05:52:16 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 16:39:36 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Chen", "Ziqi", ""], ["Min", "Martin Renqiang", ""], ["Parthasarathy", "Srinivasan", ""], ["Ning", "Xia", ""]]}, {"id": "2012.04280", "submitter": "Hui Tang", "authors": "Hui Tang, Xiatian Zhu, Ke Chen, Kui Jia, C. L. Philip Chen", "title": "Towards Uncovering the Intrinsic Data Structures for Unsupervised Domain\n  Adaptation using Structurally Regularized Deep Clustering", "comments": "Journal extension of our preliminary CVPR conference paper, under\n  review, 16 pages, 8 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation (UDA) is to learn classification models that\nmake predictions for unlabeled data on a target domain, given labeled data on a\nsource domain whose distribution diverges from the target one. Mainstream UDA\nmethods strive to learn domain-aligned features such that classifiers trained\non the source features can be readily applied to the target ones. Although\nimpressive results have been achieved, these methods have a potential risk of\ndamaging the intrinsic data structures of target discrimination, raising an\nissue of generalization particularly for UDA tasks in an inductive setting. To\naddress this issue, we are motivated by a UDA assumption of structural\nsimilarity across domains, and propose to directly uncover the intrinsic target\ndiscrimination via constrained clustering, where we constrain the clustering\nsolutions using structural source regularization that hinges on the very same\nassumption. Technically, we propose a hybrid model of Structurally Regularized\nDeep Clustering, which integrates the regularized discriminative clustering of\ntarget data with a generative one, and we thus term our method as H-SRDC. Our\nhybrid model is based on a deep clustering framework that minimizes the\nKullback-Leibler divergence between the distribution of network prediction and\nan auxiliary one, where we impose structural regularization by learning\ndomain-shared classifier and cluster centroids. By enriching the structural\nsimilarity assumption, we are able to extend H-SRDC for a pixel-level UDA task\nof semantic segmentation. We conduct extensive experiments on seven UDA\nbenchmarks of image classification and semantic segmentation. With no explicit\nfeature alignment, our proposed H-SRDC outperforms all the existing methods\nunder both the inductive and transductive settings. We make our implementation\ncodes publicly available at https://github.com/huitangtang/H-SRDC.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 08:52:00 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 03:38:39 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Tang", "Hui", ""], ["Zhu", "Xiatian", ""], ["Chen", "Ke", ""], ["Jia", "Kui", ""], ["Chen", "C. L. Philip", ""]]}, {"id": "2012.04284", "submitter": "Agni Orfanoudaki", "authors": "Dimitris Bertsimas, Jack Dunn, Emma Gibson, Agni Orfanoudaki", "title": "Optimal Survival Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tree-based models are increasingly popular due to their ability to identify\ncomplex relationships that are beyond the scope of parametric models. Survival\ntree methods adapt these models to allow for the analysis of censored outcomes,\nwhich often appear in medical data. We present a new Optimal Survival Trees\nalgorithm that leverages mixed-integer optimization (MIO) and local search\ntechniques to generate globally optimized survival tree models. We demonstrate\nthat the OST algorithm improves on the accuracy of existing survival tree\nmethods, particularly in large datasets.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 09:00:57 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["Dunn", "Jack", ""], ["Gibson", "Emma", ""], ["Orfanoudaki", "Agni", ""]]}, {"id": "2012.04322", "submitter": "Konstantinos Chatzilygeroudis", "authors": "Konstantinos Chatzilygeroudis, Antoine Cully, Vassilis Vassiliades and\n  Jean-Baptiste Mouret", "title": "Quality-Diversity Optimization: a novel branch of stochastic\n  optimization", "comments": "13 pages, 4 figures, 3 algorithms, to be published in \"Black Box\n  Optimization, Machine Learning and No-Free Lunch Theorems\", P. Pardalos, V.\n  Rasskazova, M.N. Vrahatis, Ed., Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional optimization algorithms search for a single global optimum that\nmaximizes (or minimizes) the objective function. Multimodal optimization\nalgorithms search for the highest peaks in the search space that can be more\nthan one. Quality-Diversity algorithms are a recent addition to the\nevolutionary computation toolbox that do not only search for a single set of\nlocal optima, but instead try to illuminate the search space. In effect, they\nprovide a holistic view of how high-performing solutions are distributed\nthroughout a search space. The main differences with multimodal optimization\nalgorithms are that (1) Quality-Diversity typically works in the behavioral\nspace (or feature space), and not in the genotypic (or parameter) space, and\n(2) Quality-Diversity attempts to fill the whole behavior space, even if the\nniche is not a peak in the fitness landscape. In this chapter, we provide a\ngentle introduction to Quality-Diversity optimization, discuss the main\nrepresentative algorithms, and the main current topics under consideration in\nthe community. Throughout the chapter, we also discuss several successful\napplications of Quality-Diversity algorithms, including deep learning,\nrobotics, and reinforcement learning.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 09:52:50 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 00:50:04 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Chatzilygeroudis", "Konstantinos", ""], ["Cully", "Antoine", ""], ["Vassiliades", "Vassilis", ""], ["Mouret", "Jean-Baptiste", ""]]}, {"id": "2012.04371", "submitter": "Yang Li", "authors": "Yang Li, Jiawei Jiang, Jinyang Gao, Yingxia Shao, Ce Zhang, Bin Cui", "title": "Efficient Automatic CASH via Rising Bandits", "comments": null, "journal-ref": "Proceedings of the AAAI Conference on Artificial Intelligence,\n  34(04), 4763-4771 (2020)", "doi": "10.1609/aaai.v34i04.5910", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Combined Algorithm Selection and Hyperparameter optimization (CASH) is\none of the most fundamental problems in Automatic Machine Learning (AutoML).\nThe existing Bayesian optimization (BO) based solutions turn the CASH problem\ninto a Hyperparameter Optimization (HPO) problem by combining the\nhyperparameters of all machine learning (ML) algorithms, and use BO methods to\nsolve it. As a result, these methods suffer from the low-efficiency problem due\nto the huge hyperparameter space in CASH. To alleviate this issue, we propose\nthe alternating optimization framework, where the HPO problem for each ML\nalgorithm and the algorithm selection problem are optimized alternately. In\nthis framework, the BO methods are used to solve the HPO problem for each ML\nalgorithm separately, incorporating a much smaller hyperparameter space for BO\nmethods. Furthermore, we introduce Rising Bandits, a CASH-oriented Multi-Armed\nBandits (MAB) variant, to model the algorithm selection in CASH. This framework\ncan take the advantages of both BO in solving the HPO problem with a relatively\nsmall hyperparameter space and the MABs in accelerating the algorithm\nselection. Moreover, we further develop an efficient online algorithm to solve\nthe Rising Bandits with provably theoretical guarantees. The extensive\nexperiments on 30 OpenML datasets demonstrate the superiority of the proposed\napproach over the competitive baselines.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 11:29:57 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Li", "Yang", ""], ["Jiang", "Jiawei", ""], ["Gao", "Jinyang", ""], ["Shao", "Yingxia", ""], ["Zhang", "Ce", ""], ["Cui", "Bin", ""]]}, {"id": "2012.04378", "submitter": "Christoph Schlembach", "authors": "Christoph Schlembach, Sascha L. Schmidt, Dominik Schreyer, Linus\n  Wunderlich", "title": "Forecasting the Olympic medal distribution during a pandemic: a\n  socio-economic machine learning model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG econ.EM stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting the number of Olympic medals for each nation is highly relevant\nfor different stakeholders: Ex ante, sports betting companies can determine the\nodds while sponsors and media companies can allocate their resources to\npromising teams. Ex post, sports politicians and managers can benchmark the\nperformance of their teams and evaluate the drivers of success. To\nsignificantly increase the Olympic medal forecasting accuracy, we apply machine\nlearning, more specifically a two-staged Random Forest, thus outperforming more\ntraditional na\\\"ive forecast for three previous Olympics held between 2008 and\n2016 for the first time. Regarding the Tokyo 2020 Games in 2021, our model\nsuggests that the United States will lead the Olympic medal table, winning 120\nmedals, followed by China (87) and Great Britain (74). Intriguingly, we predict\nthat the current COVID-19 pandemic will not significantly alter the medal count\nas all countries suffer from the pandemic to some extent (data inherent) and\nlimited historical data points on comparable diseases (model inherent).\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 11:50:14 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 08:35:58 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Schlembach", "Christoph", ""], ["Schmidt", "Sascha L.", ""], ["Schreyer", "Dominik", ""], ["Wunderlich", "Linus", ""]]}, {"id": "2012.04407", "submitter": "Arsam Aryandoust", "authors": "Arsam Aryandoust, Stefan Pfenninger", "title": "Active machine learning for spatio-temporal predictions using feature\n  embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Active learning (AL) could contribute to solving critical environmental\nproblems through improved spatio-temporal predictions. Yet such predictions\ninvolve high-dimensional feature spaces with mixed data types and missing data,\nwhich existing methods have difficulties dealing with. Here, we propose a novel\nbatch AL method that fills this gap. We encode and cluster features of\ncandidate data points, and query the best data based on the distance of\nembedded features to their cluster centers. We introduce a new metric of\ninformativeness that we call embedding entropy and a general class of neural\nnetworks that we call embedding networks for using it. Empirical tests on\nforecasting electricity demand show a simultaneous reduction in prediction\nerror by up to 63-88% and data usage by up to 50-69% compared to passive\nlearning (PL) benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 12:55:29 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Aryandoust", "Arsam", ""], ["Pfenninger", "Stefan", ""]]}, {"id": "2012.04428", "submitter": "Yutong Xie", "authors": "Yutong Xie, Gaoxiang Chen and Quanzheng Li", "title": "A General Computational Framework to Measure the Expressiveness of\n  Complex Networks Using a Tighter Upper Bound of Linear Regions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The expressiveness of deep neural network (DNN) is a perspective to\nunderstandthe surprising performance of DNN. The number of linear regions, i.e.\npieces thata piece-wise-linear function represented by a DNN, is generally used\nto measurethe expressiveness. And the upper bound of regions number partitioned\nby a rec-tifier network, instead of the number itself, is a more practical\nmeasurement ofexpressiveness of a rectifier DNN. In this work, we propose a new\nand tighter up-per bound of regions number. Inspired by the proof of this upper\nbound and theframework of matrix computation in Hinz & Van de Geer (2019), we\npropose ageneral computational approach to compute a tight upper bound of\nregions numberfor theoretically any network structures (e.g. DNN with all kind\nof skip connec-tions and residual structures). Our experiments show our upper\nbound is tighterthan existing ones, and explain why skip connections and\nresidual structures canimprove network performance.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 14:01:20 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Xie", "Yutong", ""], ["Chen", "Gaoxiang", ""], ["Li", "Quanzheng", ""]]}, {"id": "2012.04444", "submitter": "{\\L}ukasz Maziarka", "authors": "Agnieszka Pocha, Tomasz Danel, {\\L}ukasz Maziarka", "title": "Comparison of Atom Representations in Graph Neural Networks for\n  Molecular Property Prediction", "comments": "Machine Learning for Molecules Workshop at NeurIPS 2020 (spotlight\n  talk)", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.chem-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks have recently become a standard method for analysing\nchemical compounds. In the field of molecular property prediction, the emphasis\nis now put on designing new model architectures, and the importance of atom\nfeaturisation is oftentimes belittled. When contrasting two graph neural\nnetworks, the use of different atom features possibly leads to the incorrect\nattribution of the results to the network architecture. To provide a better\nunderstanding of this issue, we compare multiple atom representations for graph\nmodels and evaluate them on the prediction of free energy, solubility, and\nmetabolic stability. To the best of our knowledge, this is the first\nmethodological study that focuses on the relevance of atom representation to\nthe predictive performance of graph neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 15:01:44 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 09:08:20 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Pocha", "Agnieszka", ""], ["Danel", "Tomasz", ""], ["Maziarka", "\u0141ukasz", ""]]}, {"id": "2012.04456", "submitter": "Yingfan Wang", "authors": "Yingfan Wang, Haiyang Huang, Cynthia Rudin, Yaron Shaposhnik", "title": "Understanding How Dimension Reduction Tools Work: An Empirical Approach\n  to Deciphering t-SNE, UMAP, TriMAP, and PaCMAP for Data Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Dimension reduction (DR) techniques such as t-SNE, UMAP, and TriMAP have\ndemonstrated impressive visualization performance on many real world datasets.\nOne tension that has always faced these methods is the trade-off between\npreservation of global structure and preservation of local structure: these\nmethods can either handle one or the other, but not both. In this work, our\nmain goal is to understand what aspects of DR methods are important for\npreserving both local and global structure: it is difficult to design a better\nmethod without a true understanding of the choices we make in our algorithms\nand their empirical impact on the lower-dimensional embeddings they produce.\nTowards the goal of local structure preservation, we provide several useful\ndesign principles for DR loss functions based on our new understanding of the\nmechanisms behind successful DR methods. Towards the goal of global structure\npreservation, our analysis illuminates that the choice of which components to\npreserve is important. We leverage these insights to design a new algorithm for\nDR, called Pairwise Controlled Manifold Approximation Projection (PaCMAP),\nwhich preserves both local and global structure. Our work provides several\nunexpected insights into what design choices both to make and avoid when\nconstructing DR algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 14:50:45 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Wang", "Yingfan", ""], ["Huang", "Haiyang", ""], ["Rudin", "Cynthia", ""], ["Shaposhnik", "Yaron", ""]]}, {"id": "2012.04477", "submitter": "Mariia Seleznova", "authors": "Mariia Seleznova and Gitta Kutyniok", "title": "Analyzing Finite Neural Networks: Can We Trust Neural Tangent Kernel\n  Theory?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Tangent Kernel (NTK) theory is widely used to study the dynamics of\ninfinitely-wide deep neural networks (DNNs) under gradient descent. But do the\nresults for infinitely-wide networks give us hints about the behavior of real\nfinite-width ones? In this paper, we study empirically when NTK theory is valid\nin practice for fully-connected ReLU and sigmoid DNNs. We find out that whether\na network is in the NTK regime depends on the hyperparameters of random\ninitialization and the network's depth. In particular, NTK theory does not\nexplain the behavior of sufficiently deep networks initialized so that their\ngradients explode as they propagate through the network's layers: the kernel is\nrandom at initialization and changes significantly during training in this\ncase, contrary to NTK theory. On the other hand, in the case of vanishing\ngradients, DNNs are in the the NTK regime but become untrainable rapidly with\ndepth. We also describe a framework to study generalization properties of DNNs,\nin particular the variance of network's output function, by means of NTK theory\nand discuss its limits.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 15:19:45 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 19:06:43 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Seleznova", "Mariia", ""], ["Kutyniok", "Gitta", ""]]}, {"id": "2012.04533", "submitter": "Benjamin Nachman", "authors": "Patrick J. Fox, Shangqing Huang, Joshua Isaacson, Xiangyang Ju, and\n  Benjamin Nachman", "title": "Beyond 4D Tracking: Using Cluster Shapes for Track Seeding", "comments": "19 pages, 14 figures", "journal-ref": null, "doi": "10.1088/1748-0221/16/05/P05001", "report-no": "FERMILAB-PUB-20-650-T", "categories": "physics.ins-det hep-ex hep-ph physics.data-an stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tracking is one of the most time consuming aspects of event reconstruction at\nthe Large Hadron Collider (LHC) and its high-luminosity upgrade (HL-LHC).\nInnovative detector technologies extend tracking to four-dimensions by\nincluding timing in the pattern recognition and parameter estimation. However,\npresent and future hardware already have additional information that is largely\nunused by existing track seeding algorithms. The shape of clusters provides an\nadditional dimension for track seeding that can significantly reduce the\ncombinatorial challenge of track finding. We use neural networks to show that\ncluster shapes can reduce significantly the rate of fake combinatorical\nbackgrounds while preserving a high efficiency. We demonstrate this using the\ninformation in cluster singlets, doublets and triplets. Numerical results are\npresented with simulations from the TrackML challenge.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 16:20:17 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Fox", "Patrick J.", ""], ["Huang", "Shangqing", ""], ["Isaacson", "Joshua", ""], ["Ju", "Xiangyang", ""], ["Nachman", "Benjamin", ""]]}, {"id": "2012.04550", "submitter": "Sang Michael Xie", "authors": "Sang Michael Xie, Ananya Kumar, Robbie Jones, Fereshte Khani, Tengyu\n  Ma, Percy Liang", "title": "In-N-Out: Pre-Training and Self-Training using Auxiliary Information for\n  Out-of-Distribution Robustness", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a prediction setting with few in-distribution labeled examples and\nmany unlabeled examples both in- and out-of-distribution (OOD). The goal is to\nlearn a model which performs well both in-distribution and OOD. In these\nsettings, auxiliary information is often cheaply available for every input. How\nshould we best leverage this auxiliary information for the prediction task?\nEmpirically across three image and time-series datasets, and theoretically in a\nmulti-task linear regression setting, we show that (i) using auxiliary\ninformation as input features improves in-distribution error but can hurt OOD\nerror; but (ii) using auxiliary information as outputs of auxiliary\npre-training tasks improves OOD error. To get the best of both worlds, we\nintroduce In-N-Out, which first trains a model with auxiliary inputs and uses\nit to pseudolabel all the in-distribution inputs, then pre-trains a model on\nOOD auxiliary outputs and fine-tunes this model with the pseudolabels\n(self-training). We show both theoretically and empirically that In-N-Out\noutperforms auxiliary inputs or outputs alone on both in-distribution and OOD\nerror.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 16:43:07 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 22:00:59 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 16:47:17 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Xie", "Sang Michael", ""], ["Kumar", "Ananya", ""], ["Jones", "Robbie", ""], ["Khani", "Fereshte", ""], ["Ma", "Tengyu", ""], ["Liang", "Percy", ""]]}, {"id": "2012.04567", "submitter": "Razvan Marinescu", "authors": "Razvan V Marinescu, Daniel Moyer, Polina Golland", "title": "Bayesian Image Reconstruction using Deep Generative Models", "comments": "25 pages, 18 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning models are commonly trained end-to-end and in a supervised\nsetting, using paired (input, output) data. Examples include recent\nsuper-resolution methods that train on pairs of (low-resolution,\nhigh-resolution) images. However, these end-to-end approaches require\nre-training every time there is a distribution shift in the inputs (e.g., night\nimages vs daylight) or relevant latent variables (e.g., camera blur or hand\nmotion). In this work, we leverage state-of-the-art (SOTA) generative models\n(here StyleGAN2) for building powerful image priors, which enable application\nof Bayes' theorem for many downstream reconstruction tasks. Our method,\nBayesian Reconstruction through Generative Models (BRGM), uses a single\npre-trained generator model to solve different image restoration tasks, i.e.,\nsuper-resolution and in-painting, by combining it with different forward\ncorruption models. We keep the weights of the generator model fixed, and\nreconstruct the image by estimating the Bayesian maximum a-posteriori (MAP)\nestimate over the input latent vector that generated the reconstructed image.\nWe further use variational inference to approximate the posterior distribution\nover the latent vectors, from which we sample multiple solutions. We\ndemonstrate BRGM on three large and diverse datasets: (i) 60,000 images from\nthe Flick Faces High Quality dataset (ii) 240,000 chest X-rays from MIMIC III\nand (iii) a combined collection of 5 brain MRI datasets with 7,329 scans.\nAcross all three datasets and without any dataset-specific hyperparameter\ntuning, our simple approach yields performance competitive with current\ntask-specific state-of-the-art methods on super-resolution and in-painting,\nwhile being more generalisable and without requiring any training. Our source\ncode and pre-trained models are available online:\nhttps://razvanmarinescu.github.io/brgm/.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 17:11:26 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 21:48:44 GMT"}, {"version": "v3", "created": "Sun, 21 Feb 2021 21:44:29 GMT"}, {"version": "v4", "created": "Tue, 8 Jun 2021 13:44:01 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Marinescu", "Razvan V", ""], ["Moyer", "Daniel", ""], ["Golland", "Polina", ""]]}, {"id": "2012.04573", "submitter": "Zuofeng Shang", "authors": "Shuoyang Wang, Guanqun Cao, Zuofeng Shang", "title": "Estimation of the Mean Function of Functional Data via Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a deep neural network method to perform\nnonparametric regression for functional data. The proposed estimators are based\non sparsely connected deep neural networks with ReLU activation function. By\nproperly choosing network architecture, our estimator achieves the optimal\nnonparametric convergence rate in empirical norm. Under certain circumstances\nsuch as trigonometric polynomial kernel and a sufficiently large sampling\nfrequency, the convergence rate is even faster than root-$n$ rate. Through\nMonte Carlo simulation studies we examine the finite-sample performance of the\nproposed method. Finally, the proposed method is applied to analyze positron\nemission tomography images of patients with Alzheimer disease obtained from the\nAlzheimer Disease Neuroimaging Initiative database.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 17:18:16 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Wang", "Shuoyang", ""], ["Cao", "Guanqun", ""], ["Shang", "Zuofeng", ""]]}, {"id": "2012.04586", "submitter": "Dirk Johann{\\ss}en", "authors": "Dirk Johann{\\ss}en, Chris Biemann", "title": "Social Media Unrest Prediction during the {COVID}-19 Pandemic: Neural\n  Implicit Motive Pattern Recognition as Psychometric Signs of Severe Crises", "comments": "8 pages", "journal-ref": "Proceedings of the Third Workshop on Computational Modeling of\n  People's Opinions, Personality, and Emotion's in Social Media. Barcelona,\n  Spain (Online). 2020", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The COVID-19 pandemic has caused international social tension and unrest.\nBesides the crisis itself, there are growing signs of rising conflict potential\nof societies around the world. Indicators of global mood changes are hard to\ndetect and direct questionnaires suffer from social desirability biases.\nHowever, so-called implicit methods can reveal humans intrinsic desires from\ne.g. social media texts. We present psychologically validated social unrest\npredictors and replicate scalable and automated predictions, setting a new\nstate of the art on a recent German shared task dataset. We employ this model\nto investigate a change of language towards social unrest during the COVID-19\npandemic by comparing established psychological predictors on samples of tweets\nfrom spring 2019 with spring 2020. The results show a significant increase of\nthe conflict indicating psychometrics. With this work, we demonstrate the\napplicability of automated NLP-based approaches to quantitative psychological\nresearch.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 17:40:35 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Johann\u00dfen", "Dirk", ""], ["Biemann", "Chris", ""]]}, {"id": "2012.04634", "submitter": "Fredrik K. Gustafsson", "authors": "Fredrik K. Gustafsson, Martin Danelljan, Thomas B. Sch\\\"on", "title": "Accurate 3D Object Detection using Energy-Based Models", "comments": "Code is available at https://github.com/fregu856/ebms_3dod", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate 3D object detection (3DOD) is crucial for safe navigation of complex\nenvironments by autonomous robots. Regressing accurate 3D bounding boxes in\ncluttered environments based on sparse LiDAR data is however a highly\nchallenging problem. We address this task by exploring recent advances in\nconditional energy-based models (EBMs) for probabilistic regression. While\nmethods employing EBMs for regression have demonstrated impressive performance\non 2D object detection in images, these techniques are not directly applicable\nto 3D bounding boxes. In this work, we therefore design a differentiable\npooling operator for 3D bounding boxes, serving as the core module of our EBM\nnetwork. We further integrate this general approach into the state-of-the-art\n3D object detector SA-SSD. On the KITTI dataset, our proposed approach\nconsistently outperforms the SA-SSD baseline across all 3DOD metrics,\ndemonstrating the potential of EBM-based regression for highly accurate 3DOD.\nCode is available at https://github.com/fregu856/ebms_3dod.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 18:53:42 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Gustafsson", "Fredrik K.", ""], ["Danelljan", "Martin", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "2012.04646", "submitter": "Yang Feng", "authors": "Sihan Huang, Haolei Weng, Yang Feng", "title": "Spectral clustering via adaptive layer aggregation for multi-layer\n  networks", "comments": "71 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the fundamental problems in network analysis is detecting community\nstructure in multi-layer networks, of which each layer represents one type of\nedge information among the nodes. We propose integrative spectral clustering\napproaches based on effective convex layer aggregations. Our aggregation\nmethods are strongly motivated by a delicate asymptotic analysis of the\nspectral embedding of weighted adjacency matrices and the downstream $k$-means\nclustering, in a challenging regime where community detection consistency is\nimpossible. In fact, the methods are shown to estimate the optimal convex\naggregation, which minimizes the mis-clustering error under some specialized\nmulti-layer network models. Our analysis further suggests that clustering using\nGaussian mixture models is generally superior to the commonly used $k$-means in\nspectral clustering. Extensive numerical studies demonstrate that our adaptive\naggregation techniques, together with Gaussian mixture model clustering, make\nthe new spectral clustering remarkably competitive compared to several\npopularly used methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 21:58:18 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Huang", "Sihan", ""], ["Weng", "Haolei", ""], ["Feng", "Yang", ""]]}, {"id": "2012.04723", "submitter": "Tineke Blom", "authors": "Tineke Blom and Joris M. Mooij", "title": "Robustness of Model Predictions under Extension", "comments": "Accepted for oral presentation at the Causal Discovery &\n  Causality-Inspired Machine Learning Workshop at Neural Information Processing\n  Systems, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often, mathematical models of the real world are simplified representations\nof complex systems. A caveat to using models for analysis is that predicted\ncausal effects and conditional independences may not be robust under model\nextensions, and therefore applicability of such models is limited. In this\nwork, we consider conditions under which qualitative model predictions are\npreserved when two models are combined. We show how to use the technique of\ncausal ordering to efficiently assess the robustness of qualitative model\npredictions and characterize a large class of model extensions that preserve\nthese predictions. For dynamical systems at equilibrium, we demonstrate how\nnovel insights help to select appropriate model extensions and to reason about\nthe presence of feedback loops. We apply our ideas to a viral infection model\nwith immune responses.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 20:21:03 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Blom", "Tineke", ""], ["Mooij", "Joris M.", ""]]}, {"id": "2012.04728", "submitter": "Daniel Kunin", "authors": "Daniel Kunin, Javier Sagastuy-Brena, Surya Ganguli, Daniel L.K.\n  Yamins, Hidenori Tanaka", "title": "Neural Mechanics: Symmetry and Broken Conservation Laws in Deep Learning\n  Dynamics", "comments": "30 pages, 17 figures, ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn cond-mat.stat-mech q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the dynamics of neural network parameters during training is\none of the key challenges in building a theoretical foundation for deep\nlearning. A central obstacle is that the motion of a network in\nhigh-dimensional parameter space undergoes discrete finite steps along complex\nstochastic gradients derived from real-world datasets. We circumvent this\nobstacle through a unifying theoretical framework based on intrinsic symmetries\nembedded in a network's architecture that are present for any dataset. We show\nthat any such symmetry imposes stringent geometric constraints on gradients and\nHessians, leading to an associated conservation law in the continuous-time\nlimit of stochastic gradient descent (SGD), akin to Noether's theorem in\nphysics. We further show that finite learning rates used in practice can\nactually break these symmetry induced conservation laws. We apply tools from\nfinite difference methods to derive modified gradient flow, a differential\nequation that better approximates the numerical trajectory taken by SGD at\nfinite learning rates. We combine modified gradient flow with our framework of\nsymmetries to derive exact integral expressions for the dynamics of certain\nparameter combinations. We empirically validate our analytic expressions for\nlearning dynamics on VGG-16 trained on Tiny ImageNet. Overall, by exploiting\nsymmetry, our work demonstrates that we can analytically describe the learning\ndynamics of various parameter combinations at finite learning rates and batch\nsizes for state of the art architectures trained on any dataset.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 20:33:30 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 16:02:08 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Kunin", "Daniel", ""], ["Sagastuy-Brena", "Javier", ""], ["Ganguli", "Surya", ""], ["Yamins", "Daniel L. K.", ""], ["Tanaka", "Hidenori", ""]]}, {"id": "2012.04756", "submitter": "Michael Weylandt", "authors": "Michael Weylandt and George Michailidis", "title": "Automatic Registration and Clustering of Time Series", "comments": "To appear in ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Clustering of time series data exhibits a number of challenges not present in\nother settings, notably the problem of registration (alignment) of observed\nsignals. Typical approaches include pre-registration to a user-specified\ntemplate or time warping approaches which attempt to optimally align series\nwith a minimum of distortion. For many signals obtained from recording or\nsensing devices, these methods may be unsuitable as a template signal is not\navailable for pre-registration, while the distortion of warping approaches may\nobscure meaningful temporal information. We propose a new method for automatic\ntime series alignment within a clustering problem. Our approach, Temporal\nRegistration using Optimal Unitary Transformations (TROUT), is based on a novel\ndissimilarity measure between time series that is easy to compute and\nautomatically identifies optimal alignment between pairs of time series. By\nembedding our new measure in a optimization formulation, we retain well-known\nadvantages of computational and statistical performance. We provide an\nefficient algorithm for TROUT-based clustering and demonstrate its superior\nperformance over a range of competitors.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 21:51:21 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 18:30:11 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Weylandt", "Michael", ""], ["Michailidis", "George", ""]]}, {"id": "2012.04762", "submitter": "Michael Weylandt", "authors": "Michael Weylandt and T. Mitchell Roddenberry and Genevera I. Allen", "title": "Simultaneous Grouping and Denoising via Sparse Convex Wavelet Clustering", "comments": "To appear in IEEE DSLW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Clustering is a ubiquitous problem in data science and signal processing. In\nmany applications where we observe noisy signals, it is common practice to\nfirst denoise the data, perhaps using wavelet denoising, and then to apply a\nclustering algorithm. In this paper, we develop a sparse convex wavelet\nclustering approach that simultaneously denoises and discovers groups. Our\napproach utilizes convex fusion penalties to achieve agglomeration and\ngroup-sparse penalties to denoise through sparsity in the wavelet domain. In\ncontrast to common practice which denoises then clusters, our method is a\nunified, convex approach that performs both simultaneously. Our method yields\ndenoised (wavelet-sparse) cluster centroids that both improve interpretability\nand data compression. We demonstrate our method on synthetic examples and in an\napplication to NMR spectroscopy.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 22:00:38 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 00:30:03 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Weylandt", "Michael", ""], ["Roddenberry", "T. Mitchell", ""], ["Allen", "Genevera I.", ""]]}, {"id": "2012.04800", "submitter": "Bahar Taskesen", "authors": "Bahar Taskesen, Jose Blanchet, Daniel Kuhn, Viet Anh Nguyen", "title": "A Statistical Test for Probabilistic Fairness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Algorithms are now routinely used to make consequential decisions that affect\nhuman lives. Examples include college admissions, medical interventions or law\nenforcement. While algorithms empower us to harness all information hidden in\nvast amounts of data, they may inadvertently amplify existing biases in the\navailable datasets. This concern has sparked increasing interest in fair\nmachine learning, which aims to quantify and mitigate algorithmic\ndiscrimination. Indeed, machine learning models should undergo intensive tests\nto detect algorithmic biases before being deployed at scale. In this paper, we\nuse ideas from the theory of optimal transport to propose a statistical\nhypothesis test for detecting unfair classifiers. Leveraging the geometry of\nthe feature space, the test statistic quantifies the distance of the empirical\ndistribution supported on the test samples to the manifold of distributions\nthat render a pre-trained classifier fair. We develop a rigorous hypothesis\ntesting mechanism for assessing the probabilistic fairness of any pre-trained\nlogistic classifier, and we show both theoretically as well as empirically that\nthe proposed test is asymptotically correct. In addition, the proposed\nframework offers interpretability by identifying the most favorable\nperturbation of the data so that the given classifier becomes fair.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 00:20:02 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Taskesen", "Bahar", ""], ["Blanchet", "Jose", ""], ["Kuhn", "Daniel", ""], ["Nguyen", "Viet Anh", ""]]}, {"id": "2012.04809", "submitter": "Aaron Sonabend", "authors": "Aaron Sonabend-W, Nilanjana Laha, Ashwin N. Ananthakrishnan, Tianxi\n  Cai, Rajarshi Mukherjee", "title": "Semi-Supervised Off Policy Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reinforcement learning (RL) has shown great success in estimating sequential\ntreatment strategies which take into account patient heterogeneity. However,\nhealth-outcome information, which is used as the reward for reinforcement\nlearning methods, is often not well coded but rather embedded in clinical\nnotes. Extracting precise outcome information is a resource intensive task, so\nmost of the available well-annotated cohorts are small. To address this issue,\nwe propose a semi-supervised learning (SSL) approach that efficiently leverages\na small sized labeled data with true outcome observed, and a large unlabeled\ndata with outcome surrogates. In particular, we propose a semi-supervised,\nefficient approach to Q-learning and doubly robust off policy value estimation.\nGeneralizing SSL to sequential treatment regimes brings interesting challenges:\n1) Feature distribution for Q-learning is unknown as it includes previous\noutcomes. 2) The surrogate variables we leverage in the modified SSL framework\nare predictive of the outcome but not informative to the optimal policy or\nvalue function. We provide theoretical results for our Q-function and value\nfunction estimators to understand to what degree efficiency can be gained from\nSSL. Our method is at least as efficient as the supervised approach, and\nmoreover safe as it robust to mis-specification of the imputation models.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 00:59:12 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 15:43:46 GMT"}, {"version": "v3", "created": "Thu, 21 Jan 2021 02:54:08 GMT"}, {"version": "v4", "created": "Mon, 22 Feb 2021 14:15:13 GMT"}, {"version": "v5", "created": "Tue, 23 Feb 2021 02:35:02 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Sonabend-W", "Aaron", ""], ["Laha", "Nilanjana", ""], ["Ananthakrishnan", "Ashwin N.", ""], ["Cai", "Tianxi", ""], ["Mukherjee", "Rajarshi", ""]]}, {"id": "2012.04837", "submitter": "Chaoqin Huang", "authors": "Fei Ye, Huangjie Zheng, Chaoqin Huang, Ya Zhang", "title": "Deep Unsupervised Image Anomaly Detection: An Information Theoretic\n  Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Surrogate task based methods have recently shown great promise for\nunsupervised image anomaly detection. However, there is no guarantee that the\nsurrogate tasks share the consistent optimization direction with anomaly\ndetection. In this paper, we return to a direct objective function for anomaly\ndetection with information theory, which maximizes the distance between normal\nand anomalous data in terms of the joint distribution of images and their\nrepresentation. Unfortunately, this objective function is not directly\noptimizable under the unsupervised setting where no anomalous data is provided\nduring training. Through mathematical analysis of the above objective function,\nwe manage to decompose it into four components. In order to optimize in an\nunsupervised fashion, we show that, under the assumption that distribution of\nthe normal and anomalous data are separable in the latent space, its lower\nbound can be considered as a function which weights the trade-off between\nmutual information and entropy. This objective function is able to explain why\nthe surrogate task based methods are effective for anomaly detection and\nfurther point out the potential direction of improvement. Based on this object\nfunction we introduce a novel information theoretic framework for unsupervised\nimage anomaly detection. Extensive experiments have demonstrated that the\nproposed framework significantly outperforms several state-of-the-arts on\nmultiple benchmark data sets.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 03:07:00 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Ye", "Fei", ""], ["Zheng", "Huangjie", ""], ["Huang", "Chaoqin", ""], ["Zhang", "Ya", ""]]}, {"id": "2012.04859", "submitter": "Sina Alemohammad", "authors": "Sina Alemohammad, Randall Balestriero, Zichao Wang, Richard Baraniuk", "title": "Scalable Neural Tangent Kernel of Recurrent Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernels derived from deep neural networks (DNNs) in the infinite-width\nprovide not only high performance in a range of machine learning tasks but also\nnew theoretical insights into DNN training dynamics and generalization. In this\npaper, we extend the family of kernels associated with recurrent neural\nnetworks (RNNs), which were previously derived only for simple RNNs, to more\ncomplex architectures that are bidirectional RNNs and RNNs with average\npooling. We also develop a fast GPU implementation to exploit its full\npractical potential. While RNNs are typically only applied to time-series data,\nwe demonstrate that classifiers using RNN-based kernels outperform a range of\nbaseline methods on 90 non-time-series datasets from the UCI data repository.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 04:36:34 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Alemohammad", "Sina", ""], ["Balestriero", "Randall", ""], ["Wang", "Zichao", ""], ["Baraniuk", "Richard", ""]]}, {"id": "2012.04920", "submitter": "Valero Laparra", "authors": "Jos\\'e A. Padr\\'on-Hidalgo and Valero Laparra and Nathan Longbotham\n  and Gustau Camps-Valls", "title": "Kernel Anomalous Change Detection for Remote Sensing Imagery", "comments": null, "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing ( Volume: 57,\n  Issue: 10, Oct. 2019)", "doi": "10.1109/TGRS.2019.2916212", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Anomalous change detection (ACD) is an important problem in remote sensing\nimage processing. Detecting not only pervasive but also anomalous or extreme\nchanges has many applications for which methodologies are available. This paper\nintroduces a nonlinear extension of a full family of anomalous change\ndetectors. In particular, we focus on algorithms that utilize Gaussian and\nelliptically contoured (EC) distribution and extend them to their nonlinear\ncounterparts based on the theory of reproducing kernels' Hilbert space. We\nillustrate the performance of the kernel methods introduced in both pervasive\nand ACD problems with real and simulated changes in multispectral and\nhyperspectral imagery with different resolutions (AVIRIS, Sentinel-2,\nWorldView-2, and Quickbird). A wide range of situations is studied in real\nexamples, including droughts, wildfires, and urbanization. Excellent\nperformance in terms of detection accuracy compared to linear formulations is\nachieved, resulting in improved detection accuracy and reduced false-alarm\nrates. Results also reveal that the EC assumption may be still valid in Hilbert\nspaces. We provide an implementation of the algorithms as well as a database of\nnatural anomalous changes in real scenarios http://isp.uv.es/kacd.html.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 08:57:36 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Padr\u00f3n-Hidalgo", "Jos\u00e9 A.", ""], ["Laparra", "Valero", ""], ["Longbotham", "Nathan", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "2012.04922", "submitter": "Adrian Perez-Suay", "authors": "Emiliano D\\'iaz, Adri\\'an P\\'erez-Suay, Valero Laparra, Gustau\n  Camps-Valls", "title": "Consistent regression of biophysical parameters with kernel methods", "comments": "arXiv admin note: substantial text overlap with arXiv:1710.05578", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper introduces a novel statistical regression framework that allows\nthe incorporation of consistency constraints. A linear and nonlinear\n(kernel-based) formulation are introduced, and both imply closed-form\nanalytical solutions. The models exploit all the information from a set of\ndrivers while being maximally independent of a set of auxiliary, protected\nvariables. We successfully illustrate the performance in the estimation of\nchlorophyll content.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 08:59:16 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["D\u00edaz", "Emiliano", ""], ["P\u00e9rez-Suay", "Adri\u00e1n", ""], ["Laparra", "Valero", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "2012.04947", "submitter": "J. Emmanuel Johnson", "authors": "Juan Emmanuel Johnson and Valero Laparra and Gustau Camps-Valls", "title": "Disentangling Derivatives, Uncertainty and Error in Gaussian Process\n  Models", "comments": "arXiv admin note: text overlap with arXiv:2005.09907", "journal-ref": "2018 IEEE International Geoscience and Remote Sensing Symposium\n  (IGARSS)", "doi": "10.1109/IGARSS.2018.8519020", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Gaussian Processes (GPs) are a class of kernel methods that have shown to be\nvery useful in geoscience applications. They are widely used because they are\nsimple, flexible and provide very accurate estimates for nonlinear problems,\nespecially in parameter retrieval. An addition to a predictive mean function,\nGPs come equipped with a useful property: the predictive variance function\nwhich provides confidence intervals for the predictions. The GP formulation\nusually assumes that there is no input noise in the training and testing\npoints, only in the observations. However, this is often not the case in Earth\nobservation problems where an accurate assessment of the instrument error is\nusually available. In this paper, we showcase how the derivative of a GP model\ncan be used to provide an analytical error propagation formulation and we\nanalyze the predictive variance and the propagated error terms in a temperature\nprediction problem from infrared sounding data.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 10:03:13 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Johnson", "Juan Emmanuel", ""], ["Laparra", "Valero", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "2012.04951", "submitter": "Radu Horaud P", "authors": "Vasil Khalidov, Florence Forbes and Radu Horaud", "title": "Conjugate Mixture Models for Clustering Multimodal Data", "comments": null, "journal-ref": "Neural Computation, 23(2), 2011", "doi": "10.1162/NECO_a_00074", "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of multimodal clustering arises whenever the data are gathered\nwith several physically different sensors. Observations from different\nmodalities are not necessarily aligned in the sense there there is no obvious\nway to associate or to compare them in some common space. A solution may\nconsist in considering multiple clustering tasks independently for each\nmodality. The main difficulty with such an approach is to guarantee that the\nunimodal clusterings are mutually consistent. In this paper we show that\nmultimodal clustering can be addressed within a novel framework, namely\nconjugate mixture models. These models exploit the explicit transformations\nthat are often available between an unobserved parameter space (objects) and\neach one of the observation spaces (sensors). We formulate the problem as a\nlikelihood maximization task and we derive the associated conjugate\nexpectation-maximization algorithm. The convergence properties of the proposed\nalgorithm are thoroughly investigated. Several local/global optimization\ntechniques are proposed in order to increase its convergence speed. Two\ninitialization strategies are proposed and compared. A consistent\nmodel-selection criterion is proposed. The algorithm and its variants are\ntested and evaluated within the task of 3D localization of several speakers\nusing both auditory and visual data.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 10:13:22 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Khalidov", "Vasil", ""], ["Forbes", "Florence", ""], ["Horaud", "Radu", ""]]}, {"id": "2012.04957", "submitter": "Lasse Vuursteen", "authors": "Botond Szabo, Lasse Vuursteen, Harry van Zanten", "title": "Optimal distributed testing in high-dimensional Gaussian models", "comments": "33 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper study the problem of signal detection in Gaussian noise in a\ndistributed setting. We derive a lower bound on the size that the signal needs\nto have in order to be detectable. Moreover, we exhibit optimal distributed\ntesting strategies that attain the lower bound.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 10:23:54 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Szabo", "Botond", ""], ["Vuursteen", "Lasse", ""], ["van Zanten", "Harry", ""]]}, {"id": "2012.05055", "submitter": "Anastasios Tsourtis", "authors": "Anastasios Tsourtis, Yannis Pantazis, Ioannis Tsamardinos", "title": "Inference of Stochastic Dynamical Systems from Cross-Sectional\n  Population Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring the driving equations of a dynamical system from population or\ntime-course data is important in several scientific fields such as\nbiochemistry, epidemiology, financial mathematics and many others. Despite the\nexistence of algorithms that learn the dynamics from trajectorial measurements\nthere are few attempts to infer the dynamical system straight from population\ndata. In this work, we deduce and then computationally estimate the\nFokker-Planck equation which describes the evolution of the population's\nprobability density, based on stochastic differential equations. Then,\nfollowing the USDL approach, we project the Fokker-Planck equation to a proper\nset of test functions, transforming it into a linear system of equations.\nFinally, we apply sparse inference methods to solve the latter system and thus\ninduce the driving forces of the dynamical system. Our approach is illustrated\nin both synthetic and real data including non-linear, multimodal stochastic\ndifferential equations, biochemical reaction networks as well as mass cytometry\nbiological measurements.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 14:02:29 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Tsourtis", "Anastasios", ""], ["Pantazis", "Yannis", ""], ["Tsamardinos", "Ioannis", ""]]}, {"id": "2012.05072", "submitter": "Jarrad Courts", "authors": "Jarrad Courts, Adrian Wills, Thomas Sch\\\"on, Brett Ninness", "title": "Variational System Identification for Nonlinear State-Space Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SY eess.SY stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers parameter estimation for nonlinear state-space models,\nwhich is an important but challenging problem. We address this challenge by\nemploying a variational inference (VI) approach, which is a principled method\nthat has deep connections to maximum likelihood estimation. This VI approach\nultimately provides estimates of the model as solutions to an optimisation\nproblem, which is deterministic, tractable and can be solved using standard\noptimisation tools. A specialisation of this approach for systems with additive\nGaussian noise is also detailed. The proposed method is examined numerically on\na range of simulated and real examples focusing on the robustness to parameter\ninitialisation; additionally, favourable comparisons are performed against\nstate-of-the-art alternatives.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 05:43:50 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 04:39:01 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Courts", "Jarrad", ""], ["Wills", "Adrian", ""], ["Sch\u00f6n", "Thomas", ""], ["Ninness", "Brett", ""]]}, {"id": "2012.05142", "submitter": "Arnab Maiti", "authors": "Arnab Maiti, Vishakha Patil, Arindam Khan", "title": "Streaming Algorithms for Stochastic Multi-armed Bandits", "comments": "24 pages, 2 figures, 4 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Stochastic Multi-armed Bandit problem under bounded arm-memory.\nIn this setting, the arms arrive in a stream, and the number of arms that can\nbe stored in the memory at any time, is bounded. The decision-maker can only\npull arms that are present in the memory. We address the problem from the\nperspective of two standard objectives: 1) regret minimization, and 2) best-arm\nidentification. For regret minimization, we settle an important open question\nby showing an almost tight hardness. We show {\\Omega}(T^{2/3}) cumulative\nregret in expectation for arm-memory size of (n-1), where n is the number of\narms. For best-arm identification, we study two algorithms. First, we present\nan O(r) arm-memory r-round adaptive streaming algorithm to find an\n{\\epsilon}-best arm. In r-round adaptive streaming algorithm for best-arm\nidentification, the arm pulls in each round are decided based on the observed\noutcomes in the earlier rounds. The best-arm is the output at the end of r\nrounds. The upper bound on the sample complexity of our algorithm matches with\nthe lower bound for any r-round adaptive streaming algorithm. Secondly, we\npresent a heuristic to find the {\\epsilon}-best arm with optimal sample\ncomplexity, by storing only one extra arm in the memory.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 16:28:05 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Maiti", "Arnab", ""], ["Patil", "Vishakha", ""], ["Khan", "Arindam", ""]]}, {"id": "2012.05156", "submitter": "Gal Vardi", "authors": "Gal Vardi and Ohad Shamir", "title": "Implicit Regularization in ReLU Networks with the Square Loss", "comments": "Small changes due to reviews", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the implicit regularization (or implicit bias) of gradient\ndescent has recently been a very active research area. However, the implicit\nregularization in nonlinear neural networks is still poorly understood,\nespecially for regression losses such as the square loss. Perhaps surprisingly,\nwe prove that even for a single ReLU neuron, it is impossible to characterize\nthe implicit regularization with the square loss by any explicit function of\nthe model parameters (although on the positive side, we show it can be\ncharacterized approximately). For one hidden-layer networks, we prove a similar\nresult, where in general it is impossible to characterize implicit\nregularization properties in this manner, except for the \"balancedness\"\nproperty identified in Du et al. [2018]. Our results suggest that a more\ngeneral framework than the one considered so far may be needed to understand\nimplicit regularization for nonlinear predictors, and provides some clues on\nwhat this framework should be.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 16:48:03 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 18:49:53 GMT"}, {"version": "v3", "created": "Tue, 8 Jun 2021 11:59:57 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Vardi", "Gal", ""], ["Shamir", "Ohad", ""]]}, {"id": "2012.05169", "submitter": "Arda Sahiner", "authors": "Arda Sahiner, Morteza Mardani, Batu Ozturkler, Mert Pilanci, John\n  Pauly", "title": "Convex Regularization Behind Neural Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural networks have shown tremendous potential for reconstructing\nhigh-resolution images in inverse problems. The non-convex and opaque nature of\nneural networks, however, hinders their utility in sensitive applications such\nas medical imaging. To cope with this challenge, this paper advocates a convex\nduality framework that makes a two-layer fully-convolutional ReLU denoising\nnetwork amenable to convex optimization. The convex dual network not only\noffers the optimum training with convex solvers, but also facilitates\ninterpreting training and prediction. In particular, it implies training neural\nnetworks with weight decay regularization induces path sparsity while the\nprediction is piecewise linear filtering. A range of experiments with MNIST and\nfastMRI datasets confirm the efficacy of the dual network optimization problem.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 16:57:16 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Sahiner", "Arda", ""], ["Mardani", "Morteza", ""], ["Ozturkler", "Batu", ""], ["Pilanci", "Mert", ""], ["Pauly", "John", ""]]}, {"id": "2012.05196", "submitter": "Patrick Gikunda Mr.", "authors": "Patrick K. Gikunda, Nicolas Jouandeau", "title": "Cost-Based Budget Active Learning for Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Majorly classical Active Learning (AL) approach usually uses statistical\ntheory such as entropy and margin to measure instance utility, however it fails\nto capture the data distribution information contained in the unlabeled data.\nThis can eventually cause the classifier to select outlier instances to label.\nMeanwhile, the loss associated with mislabeling an instance in a typical\nclassification task is much higher than the loss associated with the opposite\nerror. To address these challenges, we propose a Cost-Based Bugdet Active\nLearning (CBAL) which considers the classification uncertainty as well as\ninstance diversity in a population constrained by a budget. A principled\napproach based on the min-max is considered to minimize both the labeling and\ndecision cost of the selected instances, this ensures a near-optimal results\nwith significantly less computational effort. Extensive experimental results\nshow that the proposed approach outperforms several state-of -the-art active\nlearning approaches.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 17:42:44 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Gikunda", "Patrick K.", ""], ["Jouandeau", "Nicolas", ""]]}, {"id": "2012.05199", "submitter": "Minhui Huang", "authors": "Minhui Huang, Shiqian Ma and Lifeng Lai", "title": "A Riemannian Block Coordinate Descent Method for Computing the\n  Projection Robust Wasserstein Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wasserstein distance has become increasingly important in machine\nlearning and deep learning. Despite its popularity, the Wasserstein distance is\nhard to approximate because of the curse of dimensionality. A recently proposed\napproach to alleviate the curse of dimensionality is to project the sampled\ndata from the high dimensional probability distribution onto a\nlower-dimensional subspace, and then compute the Wasserstein distance between\nthe projected data. However, this approach requires to solve a max-min problem\nover the Stiefel manifold, which is very challenging in practice. The only\nexisting work that solves this problem directly is the RGAS (Riemannian\nGradient Ascent with Sinkhorn Iteration) algorithm, which requires to solve an\nentropy-regularized optimal transport problem in each iteration, and thus can\nbe costly for large-scale problems. In this paper, we propose a Riemannian\nblock coordinate descent (RBCD) method to solve this problem, which is based on\na novel reformulation of the regularized max-min problem over the Stiefel\nmanifold. We show that the complexity of arithmetic operations for RBCD to\nobtain an $\\epsilon$-stationary point is $O(\\epsilon^{-3})$. This significantly\nimproves the corresponding complexity of RGAS, which is $O(\\epsilon^{-12})$.\nMoreover, our RBCD has very low per-iteration complexity, and hence is suitable\nfor large-scale problems. Numerical results on both synthetic and real datasets\ndemonstrate that our method is more efficient than existing methods, especially\nwhen the number of sampled data is very large.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 17:47:56 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2020 05:13:06 GMT"}, {"version": "v3", "created": "Wed, 13 Jan 2021 22:11:48 GMT"}, {"version": "v4", "created": "Fri, 16 Jul 2021 01:12:50 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Huang", "Minhui", ""], ["Ma", "Shiqian", ""], ["Lai", "Lifeng", ""]]}, {"id": "2012.05207", "submitter": "Tijs Maas", "authors": "Tijs Maas, Peter Bloem", "title": "Uncertainty Intervals for Graph-based Spatio-Temporal Traffic Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Many traffic prediction applications rely on uncertainty estimates instead of\nthe mean prediction. Statistical traffic prediction literature has a complete\nsubfield devoted to uncertainty modelling, but recent deep learning traffic\nprediction models either lack this feature or make specific assumptions that\nrestrict its practicality. We propose Quantile Graph Wavenet, a Spatio-Temporal\nneural network that is trained to estimate a density given the measurements of\nprevious timesteps, conditioned on a quantile. Our method of density estimation\nis fully parameterised by our neural network and does not use a likelihood\napproximation internally. The quantile loss function is asymmetric and this\nmakes it possible to model skewed densities. This approach produces uncertainty\nestimates without the need to sample during inference, such as in Monte Carlo\nDropout, which makes our method also efficient.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 18:02:26 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Maas", "Tijs", ""], ["Bloem", "Peter", ""]]}, {"id": "2012.05210", "submitter": "Amra Omanovi\\'c", "authors": "Amra Omanovi\\'c, Hilal Kazan, Polona Oblak and Toma\\v{z} Curk", "title": "Data embedding and prediction by sparse tropical matrix factorization", "comments": null, "journal-ref": "Amra Omanovic, Hilal Kazan, Polona Oblak and Tomaz Curk. BMC\n  Bioinformatics 22: 89 (2021)", "doi": "10.1186/s12859-021-04023-9", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Matrix factorization methods are linear models, with limited capability to\nmodel complex relations. In our work, we use tropical semiring to introduce\nnon-linearity into matrix factorization models. We propose a method called\nSparse Tropical Matrix Factorization (STMF) for the estimation of missing\n(unknown) values. We evaluate the efficiency of the STMF method on both\nsynthetic data and biological data in the form of gene expression measurements\ndownloaded from The Cancer Genome Atlas (TCGA) database. Tests on unique\nsynthetic data showed that STMF approximation achieves a higher correlation\nthan non-negative matrix factorization (NMF), which is unable to recover\npatterns effectively. On real data, STMF outperforms NMF on six out of nine\ngene expression datasets. While NMF assumes normal distribution and tends\ntoward the mean value, STMF can better fit to extreme values and distributions.\nSTMF is the first work that uses tropical semiring on sparse data. We show that\nin certain cases semirings are useful because they consider the structure,\nwhich is different and simpler to understand than it is with standard linear\nalgebra.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 18:09:17 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Omanovi\u0107", "Amra", ""], ["Kazan", "Hilal", ""], ["Oblak", "Polona", ""], ["Curk", "Toma\u017e", ""]]}, {"id": "2012.05269", "submitter": "Marco Scutari", "authors": "Andrea Ruggieri, Francesco Stranieri, Fabio Stella and Marco Scutari", "title": "Hard and Soft EM in Bayesian Network Learning from Incomplete Data", "comments": "16 pages, 5 figures", "journal-ref": "Algorithms 2020, 13(12):329, 1-16", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Incomplete data are a common feature in many domains, from clinical trials to\nindustrial applications. Bayesian networks (BNs) are often used in these\ndomains because of their graphical and causal interpretations. BN parameter\nlearning from incomplete data is usually implemented with the\nExpectation-Maximisation algorithm (EM), which computes the relevant sufficient\nstatistics (\"soft EM\") using belief propagation. Similarly, the Structural\nExpectation-Maximisation algorithm (Structural EM) learns the network structure\nof the BN from those sufficient statistics using algorithms designed for\ncomplete data. However, practical implementations of parameter and structure\nlearning often impute missing data (\"hard EM\") to compute sufficient statistics\ninstead of using belief propagation, for both ease of implementation and\ncomputational speed. In this paper, we investigate the question: what is the\nimpact of using imputation instead of belief propagation on the quality of the\nresulting BNs? From a simulation study using synthetic data and reference BNs,\nwe find that it is possible to recommend one approach over the other in several\nscenarios based on the characteristics of the data. We then use this\ninformation to build a simple decision tree to guide practitioners in choosing\nthe EM algorithm best suited to their problem.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 19:13:32 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Ruggieri", "Andrea", ""], ["Stranieri", "Francesco", ""], ["Stella", "Fabio", ""], ["Scutari", "Marco", ""]]}, {"id": "2012.05299", "submitter": "Wenlong Mou", "authors": "Wenlong Mou, Ashwin Pananjady, Martin J. Wainwright", "title": "Optimal oracle inequalities for solving projected fixed-point equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear fixed point equations in Hilbert spaces arise in a variety of\nsettings, including reinforcement learning, and computational methods for\nsolving differential and integral equations. We study methods that use a\ncollection of random observations to compute approximate solutions by searching\nover a known low-dimensional subspace of the Hilbert space. First, we prove an\ninstance-dependent upper bound on the mean-squared error for a linear\nstochastic approximation scheme that exploits Polyak--Ruppert averaging. This\nbound consists of two terms: an approximation error term with an\ninstance-dependent approximation factor, and a statistical error term that\ncaptures the instance-specific complexity of the noise when projected onto the\nlow-dimensional subspace. Using information theoretic methods, we also\nestablish lower bounds showing that both of these terms cannot be improved,\nagain in an instance-dependent sense. A concrete consequence of our\ncharacterization is that the optimal approximation factor in this problem can\nbe much larger than a universal constant. We show how our results precisely\ncharacterize the error of a class of temporal difference learning methods for\nthe policy evaluation problem with linear function approximation, establishing\ntheir optimality.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 20:19:32 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Mou", "Wenlong", ""], ["Pananjady", "Ashwin", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "2012.05326", "submitter": "Aur\\'elien Bellet", "authors": "Edwige Cyffers, Aur\\'elien Bellet", "title": "Privacy Amplification by Decentralization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Analyzing data owned by several parties while achieving a good trade-off\nbetween utility and privacy is a key challenge in federated learning and\nanalytics. In this work, we introduce a novel relaxation of local differential\nprivacy (LDP) that naturally arises in fully decentralized protocols, i.e.,\nwhen participants exchange information by communicating along the edges of a\nnetwork graph. This relaxation, that we call network DP, captures the fact that\nusers have only a local view of the decentralized system. To show the relevance\nof network DP, we study a decentralized model of computation where a token\nperforms a walk on the network graph and is updated sequentially by the party\nwho receives it. For tasks such as real summation, histogram computation and\noptimization with gradient descent, we propose simple algorithms on ring and\ncomplete topologies. We prove that the privacy-utility trade-offs of our\nalgorithms significantly improve upon LDP, and in some cases even match what\ncan be achieved with methods based on trusted/secure aggregation and shuffling.\nOur experiments illustrate the superior utility of our approach when training a\nmachine learning model with stochastic gradient descent.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 21:33:33 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 14:33:33 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Cyffers", "Edwige", ""], ["Bellet", "Aur\u00e9lien", ""]]}, {"id": "2012.05420", "submitter": "Stephan Wojtowytsch", "authors": "Weinan E and Stephan Wojtowytsch", "title": "On the emergence of simplex symmetry in the final and penultimate layers\n  of neural network classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent numerical study observed that neural network classifiers enjoy a\nlarge degree of symmetry in the penultimate layer. Namely, if $h(x) = Af(x) +b$\nwhere $A$ is a linear map and $f$ is the output of the penultimate layer of the\nnetwork (after activation), then all data points $x_{i, 1}, \\dots, x_{i, N_i}$\nin a class $C_i$ are mapped to a single point $y_i$ by $f$ and the points $y_i$\nare located at the vertices of a regular $k-1$-dimensional standard simplex in\na high-dimensional Euclidean space.\n  We explain this observation analytically in toy models for highly expressive\ndeep neural networks. In complementary examples, we demonstrate rigorously that\neven the final output of the classifier $h$ is not uniform over data samples\nfrom a class $C_i$ if $h$ is a shallow network (or if the deeper layers do not\nbring the data samples into a convenient geometric configuration).\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 02:32:52 GMT"}, {"version": "v2", "created": "Sat, 19 Dec 2020 17:22:05 GMT"}, {"version": "v3", "created": "Fri, 4 Jun 2021 17:06:08 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["E", "Weinan", ""], ["Wojtowytsch", "Stephan", ""]]}, {"id": "2012.05465", "submitter": "Hongxiang Qiu", "authors": "Hongxiang Qiu, Alex Luedtke", "title": "Leveraging vague prior information in general models via iteratively\n  constructed Gamma-minimax estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gamma-minimax estimation is an approach to incorporate prior information into\nan estimation procedure when it is implausible to specify one particular prior\ndistribution. In this approach, we aim for an estimator that minimizes the\nworst-case Bayes risk over a set $\\Gamma$ of prior distributions.\nTraditionally, Gamma-minimax estimation is defined for parametric models. In\nthis paper, we define Gamma-minimaxity for general models and propose iterative\nalgorithms with convergence guarantees to compute Gamma-minimax estimators for\na general model space and a set of prior distributions constrained by\ngeneralized moments. We also propose encoding the space of candidate estimators\nby neural networks to enable flexible estimation. We illustrate our method in\ntwo settings, namely entropy estimation and a problem that arises in\nbiodiversity studies.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 05:39:17 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Qiu", "Hongxiang", ""], ["Luedtke", "Alex", ""]]}, {"id": "2012.05506", "submitter": "Debraj Basu", "authors": "Debraj Basu", "title": "On Shapley Credit Allocation for Interpretability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We emphasize the importance of asking the right question when interpreting\nthe decisions of a learning model. We discuss a natural extension of the\ntheoretical machinery from Janzing et. al. 2020, which answers the question\n\"Why did my model predict a person has cancer?\" for answering a more involved\nquestion, \"What caused my model to predict a person has cancer?\" While the\nformer quantifies the direct effects of variables on the model, the latter also\naccounts for indirect effects, thereby providing meaningful insights wherever\nhuman beings can reason in terms of cause and effect. We propose three broad\ncategories for interpretations: observational, model-specific and causal each\nof which are significant in their own right. Furthermore, this paper quantifies\nfeature relevance by weaving different natures of interpretations together with\ndifferent measures as characteristic functions for Shapley symmetrization.\nBesides the widely used expected value of the model, we also discuss measures\nof statistical uncertainty and dispersion as informative candidates, and their\nmerits in generating explanations for each data point, some of which are used\nin this context for the first time. These measures are not only useful for\nstudying the influence of variables on the model output, but also on the\npredictive performance of the model, and for that we propose relevant\ncharacteristic functions that are also used for the first time.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 08:25:32 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Basu", "Debraj", ""]]}, {"id": "2012.05613", "submitter": "Sara Grassi", "authors": "Sara Grassi, Lorenzo Pareschi", "title": "From particle swarm optimization to consensus based optimization:\n  stochastic modeling and mean-field limit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a continuous description based on stochastic\ndifferential equations of the popular particle swarm optimization (PSO) process\nfor solving global optimization problems and derive in the large particle limit\nthe corresponding mean-field approximation based on Vlasov-Fokker-Planck-type\nequations. The disadvantage of memory effects induced by the need to store the\nlocal best position is overcome by the introduction of an additional\ndifferential equation describing the evolution of the local best. A\nregularization process for the global best permits to formally derive the\nrespective mean-field description. Subsequently, in the small inertia limit, we\ncompute the related macroscopic hydrodynamic equations that clarify the link\nwith the recently introduced consensus based optimization (CBO) methods.\nSeveral numerical examples illustrate the mean field process, the small inertia\nlimit and the potential of this general class of global optimization methods.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 11:58:19 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Grassi", "Sara", ""], ["Pareschi", "Lorenzo", ""]]}, {"id": "2012.05640", "submitter": "Sebastien Gadat", "authors": "S\\'ebastien Gadat and Ioana Gavra", "title": "Asymptotic study of stochastic adaptive algorithm in non-convex\n  landscape", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies some asymptotic properties of adaptive algorithms widely\nused in optimization and machine learning, and among them Adagrad and Rmsprop,\nwhich are involved in most of the blackbox deep learning algorithms. Our setup\nis the non-convex landscape optimization point of view, we consider a one time\nscale parametrization and we consider the situation where these algorithms may\nbe used or not with mini-batches. We adopt the point of view of stochastic\nalgorithms and establish the almost sure convergence of these methods when\nusing a decreasing step-size point of view towards the set of critical points\nof the target function. With a mild extra assumption on the noise, we also\nobtain the convergence towards the set of minimizer of the function. Along our\nstudy, we also obtain a \"convergence rate\" of the methods, in the vein of the\nworks of \\cite{GhadimiLan}.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 12:54:45 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 10:42:40 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Gadat", "S\u00e9bastien", ""], ["Gavra", "Ioana", ""]]}, {"id": "2012.05644", "submitter": "Hongteng Xu", "authors": "Hongteng Xu, Dixin Luo, Lawrence Carin, Hongyuan Zha", "title": "Learning Graphons via Structured Gromov-Wasserstein Barycenters", "comments": null, "journal-ref": "AAAI 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel and principled method to learn a nonparametric graph model\ncalled graphon, which is defined in an infinite-dimensional space and\nrepresents arbitrary-size graphs. Based on the weak regularity lemma from the\ntheory of graphons, we leverage a step function to approximate a graphon. We\nshow that the cut distance of graphons can be relaxed to the Gromov-Wasserstein\ndistance of their step functions. Accordingly, given a set of graphs generated\nby an underlying graphon, we learn the corresponding step function as the\nGromov-Wasserstein barycenter of the given graphs. Furthermore, we develop\nseveral enhancements and extensions of the basic algorithm, $e.g.$, the\nsmoothed Gromov-Wasserstein barycenter for guaranteeing the continuity of the\nlearned graphons and the mixed Gromov-Wasserstein barycenters for learning\nmultiple structured graphons. The proposed approach overcomes drawbacks of\nprior state-of-the-art methods, and outperforms them on both synthetic and\nreal-world data. The code is available at\nhttps://github.com/HongtengXu/SGWB-Graphon.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 13:04:29 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 05:18:23 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Xu", "Hongteng", ""], ["Luo", "Dixin", ""], ["Carin", "Lawrence", ""], ["Zha", "Hongyuan", ""]]}, {"id": "2012.05688", "submitter": "Ke Xu", "authors": "Tiancheng Huang, Ke Xu, Donglin Wang", "title": "DA-HGT: Domain Adaptive Heterogeneous Graph Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation using graph networks is to learn label-discriminative and\nnetwork-invariant node embeddings by sharing graph parameters. Most existing\nworks focus on domain adaptation of homogeneous networks, and just a few works\nbegin to study heterogeneous cases that only consider the shared node types but\nignore the private node types in individual networks. However, for a given\nsource and target heterogeneous networks, they generally contain shared and\nprivate node types, where private types bring an extra challenge for graph\ndomain adaptation. In this paper, we investigate Heterogeneous Information\nNetworks (HINs) with partial shared node types and propose a novel domain\nadaptive heterogeneous graph transformer (DA-HGT) to handle the domain shift\nbetween them. DA-HGT can not only align the distributions of identical-type\nnodes and edges in two HINs but also make full use of different-type nodes and\nedges to improve the performance of knowledge transfer. Extensive experiments\non several datasets demonstrate that DA-HGT can outperform state-of-the-art\nmethods in various domain adaptation tasks across heterogeneous networks.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 14:16:46 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Huang", "Tiancheng", ""], ["Xu", "Ke", ""], ["Wang", "Donglin", ""]]}, {"id": "2012.05757", "submitter": "Vincent Tan", "authors": "Vincent W. C. Tan, Stefan Zohren", "title": "Large Non-Stationary Noisy Covariance Matrices: A Cross-Validation\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel covariance estimator that exploits the heteroscedastic\nnature of financial time series by employing exponential weighted moving\naverages and shrinking the in-sample eigenvalues through cross-validation. Our\nestimator is model-agnostic in that we make no assumptions on the distribution\nof the random entries of the matrix or structure of the covariance matrix.\nAdditionally, we show how Random Matrix Theory can provide guidance for\nautomatic tuning of the hyperparameter which characterizes the time scale for\nthe dynamics of the estimator. By attenuating the noise from both the\ncross-sectional and time-series dimensions, we empirically demonstrate the\nsuperiority of our estimator over competing estimators that are based on\nexponentially-weighted and uniformly-weighted covariance matrices.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 15:41:17 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Tan", "Vincent W. C.", ""], ["Zohren", "Stefan", ""]]}, {"id": "2012.05820", "submitter": "Ben Moews", "authors": "Ben Moews, Romeel Dav\\'e, Sourav Mitra, Sultan Hassan, Weiguang Cui", "title": "Hybrid analytic and machine-learned baryonic property insertion into\n  galactic dark matter haloes", "comments": "13 pages, 8 figures, preprint submitted to MNRAS", "journal-ref": null, "doi": "10.1093/mnras/stab1120", "report-no": null, "categories": "astro-ph.GA astro-ph.IM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While cosmological dark matter-only simulations relying solely on\ngravitational effects are comparably fast to compute, baryonic properties in\nsimulated galaxies require complex hydrodynamic simulations that are\ncomputationally costly to run. We explore the merging of an extended version of\nthe equilibrium model, an analytic formalism describing the evolution of the\nstellar, gas, and metal content of galaxies, into a machine learning framework.\nIn doing so, we are able to recover more properties than the analytic formalism\nalone can provide, creating a high-speed hydrodynamic simulation emulator that\npopulates galactic dark matter haloes in N-body simulations with baryonic\nproperties. While there exists a trade-off between the reached accuracy and the\nspeed advantage this approach offers, our results outperform an approach using\nonly machine learning for a subset of baryonic properties. We demonstrate that\nthis novel hybrid system enables the fast completion of dark matter-only\ninformation by mimicking the properties of a full hydrodynamic suite to a\nreasonable degree, and discuss the advantages and disadvantages of hybrid\nversus machine learning-only frameworks. In doing so, we offer an acceleration\nof commonly deployed simulations in cosmology.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 16:50:33 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Moews", "Ben", ""], ["Dav\u00e9", "Romeel", ""], ["Mitra", "Sourav", ""], ["Hassan", "Sultan", ""], ["Cui", "Weiguang", ""]]}, {"id": "2012.05824", "submitter": "Siegfried H\\\"ormann", "authors": "Siegfried H\\\"ormann and Fatima Jammoul", "title": "Preprocessing noisy functional data using factor models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider functional data which are measured on a discrete set of\nobservation points. Often such data are measured with noise, and then the\ntarget is to recover the underlying signal. Most commonly, practitioners use\nsome smoothing approach, e.g.,\\ kernel smoothing or spline fitting towards this\ngoal. The drawback of such curve fitting techniques is that they act function\nby function, and don't take into account information from the entire sample. In\nthis paper we argue that signal and noise can be naturally represented as the\ncommon and idiosyncratic component, respectively, of a factor model.\nAccordingly, we propose to an estimation scheme which is based on factor\nmodels. The purpose of this paper is to explain the reasoning behind our\napproach and to compare its performance on simulated and on real data to\ncompeting methods.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 16:54:44 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["H\u00f6rmann", "Siegfried", ""], ["Jammoul", "Fatima", ""]]}, {"id": "2012.05895", "submitter": "Mengye Ren", "authors": "Mengye Ren, Eleni Triantafillou, Kuan-Chieh Wang, James Lucas, Jake\n  Snell, Xaq Pitkow, Andreas S. Tolias, Richard Zemel", "title": "Flexible Few-Shot Learning with Contextual Similarity", "comments": "Technical report, 29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches to few-shot learning deal with tasks that have\npersistent, rigid notions of classes. Typically, the learner observes data only\nfrom a fixed number of classes at training time and is asked to generalize to a\nnew set of classes at test time. Two examples from the same class would always\nbe assigned the same labels in any episode. In this work, we consider a\nrealistic setting where the similarities between examples can change from\nepisode to episode depending on the task context, which is not given to the\nlearner. We define new benchmark datasets for this flexible few-shot scenario,\nwhere the tasks are based on images of faces (Celeb-A), shoes (Zappos50K), and\ngeneral objects (ImageNet-with-Attributes). While classification baselines and\nepisodic approaches learn representations that work well for standard few-shot\nlearning, they suffer in our flexible tasks as novel similarity definitions\narise during testing. We propose to build upon recent contrastive unsupervised\nlearning techniques and use a combination of instance and class invariance\nlearning, aiming to obtain general and flexible features. We find that our\napproach performs strongly on our new flexible few-shot learning benchmarks,\ndemonstrating that unsupervised learning obtains more generalizable\nrepresentations.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 18:58:02 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Ren", "Mengye", ""], ["Triantafillou", "Eleni", ""], ["Wang", "Kuan-Chieh", ""], ["Lucas", "James", ""], ["Snell", "Jake", ""], ["Pitkow", "Xaq", ""], ["Tolias", "Andreas S.", ""], ["Zemel", "Richard", ""]]}, {"id": "2012.05912", "submitter": "Jordi Mu\\~noz-Mar\\'i", "authors": "Anna Mateo-Sanchis, Jordi Munoz-Mari, Manuel Campos-Taberner, Javier\n  Garcia-Haro, Gustau Camps-Valls", "title": "Gap Filling of Biophysical Parameter Time Series with Multi-Output\n  Gaussian Processes", "comments": "4 pages, 3 figures", "journal-ref": "2018 IEEE International Geoscience and Remote Sensing Symposium,\n  Valencia, 2018, pp. 4039-4042", "doi": "10.1109/IGARSS.2018.8519254", "report-no": null, "categories": "q-bio.QM cs.LG physics.data-an stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this work we evaluate multi-output (MO) Gaussian Process (GP) models based\non the linear model of coregionalization (LMC) for estimation of biophysical\nparameter variables under a gap filling setup. In particular, we focus on LAI\nand fAPAR over rice areas. We show how this problem cannot be solved with\nstandard single-output (SO) GP models, and how the proposed MO-GP models are\nable to successfully predict these variables even in high missing data regimes,\nby implicitly performing an across-domain information transfer.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 13:10:19 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Mateo-Sanchis", "Anna", ""], ["Munoz-Mari", "Jordi", ""], ["Campos-Taberner", "Manuel", ""], ["Garcia-Haro", "Javier", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "2012.05940", "submitter": "Carlos G\\'omez Tapia", "authors": "Carlos G\\'omez, Niamh Belton, Boi Quach, Jack Nicholls, Devanshu Anand", "title": "A Simplistic Machine Learning Approach to Contact Tracing", "comments": "6 pages, 7 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This report is based on the modified NIST challenge, Too Close For Too Long,\nprovided by the SFI Centre for Machine Learning (ML-Labs). The modified\nchallenge excludes the time calculation (too long) aspect. By handcrafting\nfeatures from phone instrumental data we develop two machine learning models, a\nGBM and an MLP, to estimate distance between two phones. Our method is able to\noutperform the leading NIST challenge result by the Hong Kong University of\nScience and Technology (HKUST) by a significant margin.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 19:34:48 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["G\u00f3mez", "Carlos", ""], ["Belton", "Niamh", ""], ["Quach", "Boi", ""], ["Nicholls", "Jack", ""], ["Anand", "Devanshu", ""]]}, {"id": "2012.05973", "submitter": "Steven Golovkine", "authors": "Steven Golovkine and Nicolas Klutchnikoff and Valentin Patilea", "title": "Clustering multivariate functional data using unsupervised binary trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a model-based clustering algorithm for a general class of\nfunctional data for which the components could be curves or images. The random\nfunctional data realizations could be measured with error at discrete, and\npossibly random, points in the definition domain. The idea is to build a set of\nbinary trees by recursive splitting of the observations. The number of groups\nare determined in a data-driven way. The new algorithm provides easily\ninterpretable results and fast predictions for online data sets. Results on\nsimulated datasets reveal good performance in various complex settings. The\nmethodology is applied to the analysis of vehicle trajectories on a German\nroundabout.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 20:56:49 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 09:10:03 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Golovkine", "Steven", ""], ["Klutchnikoff", "Nicolas", ""], ["Patilea", "Valentin", ""]]}, {"id": "2012.06046", "submitter": "Benedikt Boecking", "authors": "Benedikt Boecking, Willie Neiswanger, Eric Xing, Artur Dubrawski", "title": "Interactive Weak Supervision: Learning Useful Heuristics for Data\n  Labeling", "comments": "Accepted as a conference paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Obtaining large annotated datasets is critical for training successful\nmachine learning models and it is often a bottleneck in practice. Weak\nsupervision offers a promising alternative for producing labeled datasets\nwithout ground truth annotations by generating probabilistic labels using\nmultiple noisy heuristics. This process can scale to large datasets and has\ndemonstrated state of the art performance in diverse domains such as healthcare\nand e-commerce. One practical issue with learning from user-generated\nheuristics is that their creation requires creativity, foresight, and domain\nexpertise from those who hand-craft them, a process which can be tedious and\nsubjective. We develop the first framework for interactive weak supervision in\nwhich a method proposes heuristics and learns from user feedback given on each\nproposed heuristic. Our experiments demonstrate that only a small number of\nfeedback iterations are needed to train models that achieve highly competitive\ntest set performance without access to ground truth training labels. We conduct\nuser studies, which show that users are able to effectively provide feedback on\nheuristics and that test set results track the performance of simulated\noracles.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 00:10:38 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 20:03:15 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Boecking", "Benedikt", ""], ["Neiswanger", "Willie", ""], ["Xing", "Eric", ""], ["Dubrawski", "Artur", ""]]}, {"id": "2012.06063", "submitter": "Saeid Mehrdad", "authors": "Saeid Mehrdad, Mohammad Hossein Kahaei", "title": "Deep Learning Approach for Matrix Completion Using Manifold Learning", "comments": null, "journal-ref": null, "doi": "10.1016/j.sigpro.2021.108231", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Matrix completion has received vast amount of attention and research due to\nits wide applications in various study fields. Existing methods of matrix\ncompletion consider only nonlinear (or linear) relations among entries in a\ndata matrix and ignore linear (or nonlinear) relationships latent. This paper\nintroduces a new latent variables model for data matrix which is a combination\nof linear and nonlinear models and designs a novel deep-neural-network-based\nmatrix completion algorithm to address both linear and nonlinear relations\namong entries of data matrix. The proposed method consists of two branches. The\nfirst branch learns the latent representations of columns and reconstructs the\ncolumns of the partially observed matrix through a series of hidden neural\nnetwork layers. The second branch does the same for the rows. In addition,\nbased on multi-task learning principles, we enforce these two branches work\ntogether and introduce a new regularization technique to reduce over-fitting.\nMore specifically, the missing entries of data are recovered as a main task and\nmanifold learning is performed as an auxiliary task. The auxiliary task\nconstrains the weights of the network so it can be considered as a regularizer,\nimproving the main task and reducing over-fitting. Experimental results\nobtained on the synthetic data and several real-world data verify the\neffectiveness of the proposed method compared with state-of-the-art matrix\ncompletion methods.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 01:01:54 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Mehrdad", "Saeid", ""], ["Kahaei", "Mohammad Hossein", ""]]}, {"id": "2012.06076", "submitter": "Yusha Liu", "authors": "Yusha Liu, Yining Wang, Aarti Singh", "title": "Smooth Bandit Optimization: Generalization to H\\\"older Space", "comments": "11 main pages, 2 figures, 13 appendix pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider bandit optimization of a smooth reward function, where the goal\nis cumulative regret minimization. This problem has been studied for\n$\\alpha$-H\\\"older continuous (including Lipschitz) functions with $0<\\alpha\\leq\n1$. Our main result is in generalization of the reward function to H\\\"older\nspace with exponent $\\alpha>1$ to bridge the gap between Lipschitz bandits and\ninfinitely-differentiable models such as linear bandits. For H\\\"older\ncontinuous functions, approaches based on random sampling in bins of a\ndiscretized domain suffices as optimal. In contrast, we propose a class of\ntwo-layer algorithms that deploy misspecified linear/polynomial bandit\nalgorithms in bins. We demonstrate that the proposed algorithm can exploit\nhigher-order smoothness of the function by deriving a regret upper bound of\n$\\tilde{O}(T^\\frac{d+\\alpha}{d+2\\alpha})$ for when $\\alpha>1$, which matches\nexisting lower bound. We also study adaptation to unknown function smoothness\nover a continuous scale of H\\\"older spaces indexed by $\\alpha$, with a bandit\nmodel selection approach applied with our proposed two-layer algorithms. We\nshow that it achieves regret rate that matches the existing lower bound for\nadaptation within the $\\alpha\\leq 1$ subset.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 01:43:25 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Liu", "Yusha", ""], ["Wang", "Yining", ""], ["Singh", "Aarti", ""]]}, {"id": "2012.06110", "submitter": "Haoxi Zhan", "authors": "Haoxi Zhan, Xiaobing Pei", "title": "I-GCN: Robust Graph Convolutional Network via Influence Mechanism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models for graphs, especially Graph Convolutional Networks\n(GCNs), have achieved remarkable performance in the task of semi-supervised\nnode classification. However, recent studies show that GCNs suffer from\nadversarial perturbations. Such vulnerability to adversarial attacks\nsignificantly decreases the stability of GCNs when being applied to\nsecurity-critical applications. Defense methods such as preprocessing,\nattention mechanism and adversarial training have been discussed by various\nstudies. While being able to achieve desirable performance when the\nperturbation rates are low, such methods are still vulnerable to high\nperturbation rates. Meanwhile, some defending algorithms perform poorly when\nthe node features are not visible. Therefore, in this paper, we propose a novel\nmechanism called influence mechanism, which is able to enhance the robustness\nof the GCNs significantly. The influence mechanism divides the effect of each\nnode into two parts: introverted influence which tries to maintain its own\nfeatures and extroverted influence which exerts influences on other nodes.\nUtilizing the influence mechanism, we propose the Influence GCN (I-GCN) model.\nExtensive experiments show that our proposed model is able to achieve higher\naccuracy rates than state-of-the-art methods when defending against\nnon-targeted attacks.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 04:03:15 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Zhan", "Haoxi", ""], ["Pei", "Xiaobing", ""]]}, {"id": "2012.06191", "submitter": "Tomoharu Iwata", "authors": "Tomoharu Iwata, Yoshinobu Kawahara", "title": "Neural Dynamic Mode Decomposition for End-to-End Modeling of Nonlinear\n  Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Koopman spectral analysis has attracted attention for understanding nonlinear\ndynamical systems by which we can analyze nonlinear dynamics with a linear\nregime by lifting observations using a nonlinear function. For analysis, we\nneed to find an appropriate lift function. Although several methods have been\nproposed for estimating a lift function based on neural networks, the existing\nmethods train neural networks without spectral analysis. In this paper, we\npropose neural dynamic mode decomposition, in which neural networks are trained\nsuch that the forecast error is minimized when the dynamics is modeled based on\nspectral decomposition in the lifted space. With our proposed method, the\nforecast error is backpropagated through the neural networks and the spectral\ndecomposition, enabling end-to-end learning of Koopman spectral analysis. When\ninformation is available on the frequencies or the growth rates of the\ndynamics, the proposed method can exploit it as regularizers for training. We\nalso propose an extension of our approach when observations are influenced by\nexogenous control time-series. Our experiments demonstrate the effectiveness of\nour proposed method in terms of eigenvalue estimation and forecast performance.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 08:34:26 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Iwata", "Tomoharu", ""], ["Kawahara", "Yoshinobu", ""]]}, {"id": "2012.06287", "submitter": "Michael Stephanou", "authors": "Michael Stephanou and Melvin Varughese", "title": "Sequential estimation of Spearman rank correlation using Hermite series\n  estimators", "comments": "Revised article incorporating comments by the reviewers, associate\n  editor and editor of the Journal of Multivariate Analysis (accepted\n  manuscript). Changes include a modified title and a significantly shortened\n  article", "journal-ref": null, "doi": "10.1016/j.jmva.2021.104783", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this article we describe a new Hermite series based sequential estimator\nfor the Spearman rank correlation coefficient and provide algorithms applicable\nin both the stationary and non-stationary settings. To treat the non-stationary\nsetting, we introduce a novel, exponentially weighted estimator for the\nSpearman rank correlation, which allows the local nonparametric correlation of\na bivariate data stream to be tracked. To the best of our knowledge this is the\nfirst algorithm to be proposed for estimating a time varying Spearman rank\ncorrelation that does not rely on a moving window approach. We explore the\npractical effectiveness of the Hermite series based estimators through real\ndata and simulation studies demonstrating good practical performance. The\nsimulation studies in particular reveal competitive performance compared to an\nexisting algorithm. The potential applications of this work are manifold. The\nHermite series based Spearman rank correlation estimator can be applied to fast\nand robust online calculation of correlation which may vary over time. Possible\nmachine learning applications include, amongst others, fast feature selection\nand hierarchical clustering on massive data sets.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 12:43:19 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 09:46:09 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Stephanou", "Michael", ""], ["Varughese", "Melvin", ""]]}, {"id": "2012.06341", "submitter": "Antonio H. Ribeiro", "authors": "Ant\\^onio H. Ribeiro, Johannes N. Hendriks, Adrian G. Wills, Thomas B.\n  Sch\\\"on", "title": "Beyond Occam's Razor in System Identification: Double-Descent when\n  Modeling Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  System identification aims to build models of dynamical systems from data.\nTraditionally, choosing the model requires the designer to balance between two\ngoals of conflicting nature; the model must be rich enough to capture the\nsystem dynamics, but not so flexible that it learns spurious random effects\nfrom the dataset. It is typically observed that model validation performance\nfollows a U-shaped curve as the model complexity increases. Recent developments\nin machine learning and statistics, however, have observed situations where a\n\"double-descent\" curve subsumes this U-shaped model-performance curve. With a\nsecond decrease in performance occurring beyond the point where the model has\nreached the capacity of interpolating - i.e., (near) perfectly fitting - the\ntraining data. To the best of our knowledge, however, such phenomena have not\nbeen studied within the context of the identification of dynamic systems. The\npresent paper aims to answer the question: \"Can such a phenomenon also be\nobserved when estimating parameters of dynamic systems?\" We show the answer is\nyes, verifying such behavior experimentally both for artificially generated and\nreal-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 13:34:56 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Ribeiro", "Ant\u00f4nio H.", ""], ["Hendriks", "Johannes N.", ""], ["Wills", "Adrian G.", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "2012.06362", "submitter": "Matthew Lang", "authors": "Matthew Lang, Jake Witherington, Harriet Turner, Matt Owens, Pete\n  Riley", "title": "Improving solar wind forecasting using Data Assimilation", "comments": "29 pages, 9 figures, 3 tables, under review in Space Weather journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.space-ph astro-ph.SR physics.data-an stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data Assimilation (DA) has enabled huge improvements in the skill of\nterrestrial operational weather forecasting. In this study, we use a\nvariational DA scheme with a computationally efficient solar wind model and in\nsitu observations from STEREO-A, STEREO-B and ACE. This scheme enables\nsolar-wind observations far from the Sun, such as at 1 AU, to update and\nimprove the inner boundary conditions of the solar wind model (at 30 solar\nradii). In this way, observational information can be used to improve estimates\nof the near-Earth solar wind, even when the observations are not directly\ndownstream of the Earth. This allows improved initial conditions of the solar\nwind to be passed into forecasting models. To this effect, we employ the HUXt\nsolar wind model to produce 27-day forecasts of the solar wind during the\noperational lifetime of STEREO-B (01 November 2007 - 30 September 2014). In\nnear-Earth space, we compare the accuracy of these DA forecasts with both\nnon-DA forecasts and simple corotation of STEREO-B observations. We find that\n27-day root mean-square error (RMSE) for STEREO-B corotation and DA forecasts\nare comparable and both are significantly lower than non-DA forecasts. However,\nthe DA forecast is shown to improve solar wind forecasts when STEREO-B's\nlatitude is offset from Earth, which is an issue for corotation forecasts. And\nthe DA scheme enables the representation of the solar wind in the whole model\ndomain between the Sun and the Earth to be improved, which will enable improved\nforecasting of CME arrival time and speed.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 14:05:12 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 08:07:14 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 10:26:56 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Lang", "Matthew", ""], ["Witherington", "Jake", ""], ["Turner", "Harriet", ""], ["Owens", "Matt", ""], ["Riley", "Pete", ""]]}, {"id": "2012.06373", "submitter": "Julien Launay", "authors": "Julien Launay, Iacopo Poli, Kilian M\\\"uller, Gustave Pariente, Igor\n  Carron, Laurent Daudet, Florent Krzakala, Sylvain Gigan", "title": "Hardware Beyond Backpropagation: a Photonic Co-Processor for Direct\n  Feedback Alignment", "comments": "6 pages, 2 figures, 1 table. Oral at the Beyond Backpropagation\n  Workshop, NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.AR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scaling hypothesis motivates the expansion of models past trillions of\nparameters as a path towards better performance. Recent significant\ndevelopments, such as GPT-3, have been driven by this conjecture. However, as\nmodels scale-up, training them efficiently with backpropagation becomes\ndifficult. Because model, pipeline, and data parallelism distribute parameters\nand gradients over compute nodes, communication is challenging to orchestrate:\nthis is a bottleneck to further scaling. In this work, we argue that\nalternative training methods can mitigate these issues, and can inform the\ndesign of extreme-scale training hardware. Indeed, using a synaptically\nasymmetric method with a parallelizable backward pass, such as Direct Feedback\nAlignement, communication needs are drastically reduced. We present a photonic\naccelerator for Direct Feedback Alignment, able to compute random projections\nwith trillions of parameters. We demonstrate our system on benchmark tasks,\nusing both fully-connected and graph convolutional networks. Our hardware is\nthe first architecture-agnostic photonic co-processor for training neural\nnetworks. This is a significant step towards building scalable hardware, able\nto go beyond backpropagation, and opening new avenues for deep learning.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 14:20:45 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Launay", "Julien", ""], ["Poli", "Iacopo", ""], ["M\u00fcller", "Kilian", ""], ["Pariente", "Gustave", ""], ["Carron", "Igor", ""], ["Daudet", "Laurent", ""], ["Krzakala", "Florent", ""], ["Gigan", "Sylvain", ""]]}, {"id": "2012.06508", "submitter": "Charles Corbi\\`ere", "authors": "Charles Corbi\\`ere, Nicolas Thome, Antoine Saporta, Tuan-Hung Vu,\n  Matthieu Cord, Patrick P\\'erez", "title": "Confidence Estimation via Auxiliary Models", "comments": "Accepted to TPAMI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reliably quantifying the confidence of deep neural classifiers is a\nchallenging yet fundamental requirement for deploying such models in\nsafety-critical applications. In this paper, we introduce a novel target\ncriterion for model confidence, namely the true class probability (TCP). We\nshow that TCP offers better properties for confidence estimation than standard\nmaximum class probability (MCP). Since the true class is by essence unknown at\ntest time, we propose to learn TCP criterion from data with an auxiliary model,\nintroducing a specific learning scheme adapted to this context. We evaluate our\napproach on the task of failure prediction and of self-training with\npseudo-labels for domain adaptation, which both necessitate effective\nconfidence estimates. Extensive experiments are conducted for validating the\nrelevance of the proposed approach in each task. We study various network\narchitectures and experiment with small and large datasets for image\nclassification and semantic segmentation. In every tested benchmark, our\napproach outperforms strong baselines.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 17:21:12 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 17:24:34 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Corbi\u00e8re", "Charles", ""], ["Thome", "Nicolas", ""], ["Saporta", "Antoine", ""], ["Vu", "Tuan-Hung", ""], ["Cord", "Matthieu", ""], ["P\u00e9rez", "Patrick", ""]]}, {"id": "2012.06564", "submitter": "Marcos Matabuena", "authors": "Marcos Matabuena, Paulo F\\'elix, Carlos Meijide-Garcia and Francisco\n  Gude", "title": "Glucose values prediction five years ahead with a new framework of\n  missing responses in reproducing kernel Hilbert spaces, and the use of\n  continuous glucose monitoring technology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  AEGIS study possesses unique information on longitudinal changes in\ncirculating glucose through continuous glucose monitoring technology (CGM).\nHowever, as usual in longitudinal medical studies, there is a significant\namount of missing data in the outcome variables. For example, 40 percent of\nglycosylated hemoglobin (A1C) biomarker data are missing five years ahead. With\nthe purpose to reduce the impact of this issue, this article proposes a new\ndata analysis framework based on learning in reproducing kernel Hilbert spaces\n(RKHS) with missing responses that allows to capture non-linear relations\nbetween variable studies in different supervised modeling tasks. First, we\nextend the Hilbert-Schmidt dependence measure to test statistical independence\nin this context introducing a new bootstrap procedure, for which we prove\nconsistency. Next, we adapt or use existing models of variable selection,\nregression, and conformal inference to obtain new clinical findings about\nglucose changes five years ahead with the AEGIS data. The most relevant\nfindings are summarized below: i) We identify new factors associated with\nlong-term glucose evolution; ii) We show the clinical sensibility of CGM data\nto detect changes in glucose metabolism; iii) We can improve clinical\ninterventions based on our algorithms' expected glucose changes according to\npatients' baseline characteristics.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 18:51:44 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 18:47:42 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Matabuena", "Marcos", ""], ["F\u00e9lix", "Paulo", ""], ["Meijide-Garcia", "Carlos", ""], ["Gude", "Francisco", ""]]}, {"id": "2012.06573", "submitter": "Alexis Marchal", "authors": "Alexis Marchal", "title": "Risk & returns around FOMC press conferences: a novel perspective from\n  computer vision", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I propose a new tool to characterize the resolution of uncertainty around\nFOMC press conferences. It relies on the construction of a measure capturing\nthe level of discussion complexity between the Fed Chair and reporters during\nthe Q&A sessions. I show that complex discussions are associated with higher\nequity returns and a drop in realized volatility. The method creates an\nattention score by quantifying how much the Chair needs to rely on reading\ninternal documents to be able to answer a question. This is accomplished by\nbuilding a novel dataset of video images of the press conferences and\nleveraging recent deep learning algorithms from computer vision. This\nalternative data provides new information on nonverbal communication that\ncannot be extracted from the widely analyzed FOMC transcripts. This paper can\nbe seen as a proof of concept that certain videos contain valuable information\nfor the study of financial markets.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 18:59:47 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 16:31:18 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Marchal", "Alexis", ""]]}, {"id": "2012.06667", "submitter": "Kelvin Kan", "authors": "Kelvin Kan, James G Nagy and Lars Ruthotto", "title": "Avoiding The Double Descent Phenomenon of Random Feature Models Using\n  Hybrid Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate the ability of hybrid regularization methods to automatically\navoid the double descent phenomenon arising in the training of random feature\nmodels (RFM). The hallmark feature of the double descent phenomenon is a spike\nin the regularization gap at the interpolation threshold, i.e. when the number\nof features in the RFM equals the number of training samples. To close this\ngap, the hybrid method considered in our paper combines the respective\nstrengths of the two most common forms of regularization: early stopping and\nweight decay. The scheme does not require hyperparameter tuning as it\nautomatically selects the stopping iteration and weight decay hyperparameter by\nusing generalized cross-validation (GCV). This also avoids the necessity of a\ndedicated validation set. While the benefits of hybrid methods have been\nwell-documented for ill-posed inverse problems, our work presents the first use\ncase in machine learning. To expose the need for regularization and motivate\nhybrid methods, we perform detailed numerical experiments inspired by image\nclassification. In those examples, the hybrid scheme successfully avoids the\ndouble descent phenomenon and yields RFMs whose generalization is comparable\nwith classical regularization approaches whose hyperparameters are tuned\noptimally using the test data. We provide our MATLAB codes for implementing the\nnumerical experiments in this paper at https://github.com/EmoryMLIP/HybridRFM.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 22:42:34 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Kan", "Kelvin", ""], ["Nagy", "James G", ""], ["Ruthotto", "Lars", ""]]}, {"id": "2012.06684", "submitter": "Samuel Ainsworth", "authors": "Samuel Ainsworth and Kendall Lowrey and John Thickstun and Zaid\n  Harchaoui and Siddhartha Srinivasa", "title": "Faster Policy Learning with Continuous-Time Gradients", "comments": null, "journal-ref": "L4DC 2021", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the estimation of policy gradients for continuous-time systems with\nknown dynamics. By reframing policy learning in continuous-time, we show that\nit is possible construct a more efficient and accurate gradient estimator. The\nstandard back-propagation through time estimator (BPTT) computes exact\ngradients for a crude discretization of the continuous-time system. In\ncontrast, we approximate continuous-time gradients in the original system. With\nthe explicit goal of estimating continuous-time gradients, we are able to\ndiscretize adaptively and construct a more efficient policy gradient estimator\nwhich we call the Continuous-Time Policy Gradient (CTPG). We show that\nreplacing BPTT policy gradients with more efficient CTPG estimates results in\nfaster and more robust learning in a variety of control tasks and simulators.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 00:22:56 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 04:31:03 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Ainsworth", "Samuel", ""], ["Lowrey", "Kendall", ""], ["Thickstun", "John", ""], ["Harchaoui", "Zaid", ""], ["Srinivasa", "Siddhartha", ""]]}, {"id": "2012.06691", "submitter": "Johann Rudi", "authors": "Johann Rudi, Julie Bessac, Amanda Lenzi", "title": "Parameter Estimation with Dense and Convolutional Neural Networks\n  Applied to the FitzHugh-Nagumo ODE", "comments": "accepted to MSML21: Mathematical and Scientific Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning algorithms have been successfully used to approximate\nnonlinear maps under weak assumptions on the structure and properties of the\nmaps. We present deep neural networks using dense and convolutional layers to\nsolve an inverse problem, where we seek to estimate parameters of a\nFitzHugh-Nagumo model, which consists of a nonlinear system of ordinary\ndifferential equations (ODEs). We employ the neural networks to approximate\nreconstruction maps for model parameter estimation from observational data,\nwhere the data comes from the solution of the ODE and takes the form of a time\nseries representing dynamically spiking membrane potential of a biological\nneuron. We target this dynamical model because of the computational challenges\nit poses in an inference setting, namely, having a highly nonlinear and\nnonconvex data misfit term and permitting only weakly informative priors on\nparameters. These challenges cause traditional optimization to fail and\nalternative algorithms to exhibit large computational costs. We quantify the\nprediction errors of model parameters obtained from the neural networks and\ninvestigate the effects of network architectures with and without the presence\nof noise in observational data. We generalize our framework for neural\nnetwork-based reconstruction maps to simultaneously estimate ODE parameters and\nparameters of autocorrelated observational noise. Our results demonstrate that\ndeep neural networks have the potential to estimate parameters in dynamical\nmodels and stochastic processes, and they are capable of predicting parameters\naccurately for the FitzHugh-Nagumo model.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 01:20:42 GMT"}, {"version": "v2", "created": "Sat, 6 Mar 2021 20:52:55 GMT"}, {"version": "v3", "created": "Tue, 4 May 2021 16:27:18 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Rudi", "Johann", ""], ["Bessac", "Julie", ""], ["Lenzi", "Amanda", ""]]}, {"id": "2012.06695", "submitter": "Udaya Ghai", "authors": "Udaya Ghai, David Snyder, Anirudha Majumdar, Elad Hazan", "title": "Generating Adversarial Disturbances for Controller Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of generating maximally adversarial disturbances for\na given controller assuming only blackbox access to it. We propose an online\nlearning approach to this problem that adaptively generates disturbances based\non control inputs chosen by the controller. The goal of the disturbance\ngenerator is to minimize regret versus a benchmark disturbance-generating\npolicy class, i.e., to maximize the cost incurred by the controller as well as\npossible compared to the best possible disturbance generator in hindsight\n(chosen from a benchmark policy class). In the setting where the dynamics are\nlinear and the costs are quadratic, we formulate our problem as an online trust\nregion (OTR) problem with memory and present a new online learning algorithm\n(MOTR) for this problem. We prove that this method competes with the best\ndisturbance generator in hindsight (chosen from a rich class of benchmark\npolicies that includes linear-dynamical disturbance generating policies). We\ndemonstrate our approach on two simulated examples: (i) synthetically generated\nlinear systems, and (ii) generating wind disturbances for the popular PX4\ncontroller in the AirSim simulator. On these examples, we demonstrate that our\napproach outperforms several baseline approaches, including $H_{\\infty}$\ndisturbance generation and gradient-based methods.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 01:31:32 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Ghai", "Udaya", ""], ["Snyder", "David", ""], ["Majumdar", "Anirudha", ""], ["Hazan", "Elad", ""]]}, {"id": "2012.06718", "submitter": "Gabriel Hope", "authors": "Gabriel Hope, Madina Abdrakhmanova, Xiaoyin Chen, Michael C. Hughes,\n  Michael C. Hughes and Erik B. Sudderth", "title": "Learning Consistent Deep Generative Models from Sparse Data via\n  Prediction Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a new framework for learning variational autoencoders and other\ndeep generative models that balances generative and discriminative goals. Our\nframework optimizes model parameters to maximize a variational lower bound on\nthe likelihood of observed data, subject to a task-specific prediction\nconstraint that prevents model misspecification from leading to inaccurate\npredictions. We further enforce a consistency constraint, derived naturally\nfrom the generative model, that requires predictions on reconstructed data to\nmatch those on the original data. We show that these two contributions --\nprediction constraints and consistency constraints -- lead to promising image\nclassification performance, especially in the semi-supervised scenario where\ncategory labels are sparse but unlabeled data is plentiful. Our approach\nenables advances in generative modeling to directly boost semi-supervised\nclassification performance, an ability we demonstrate by augmenting deep\ngenerative models with latent variables capturing spatial transformations.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 04:18:50 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Hope", "Gabriel", ""], ["Abdrakhmanova", "Madina", ""], ["Chen", "Xiaoyin", ""], ["Hughes", "Michael C.", ""], ["Hughes", "Michael C.", ""], ["Sudderth", "Erik B.", ""]]}, {"id": "2012.06750", "submitter": "Philip Thompson", "authors": "Philip Thompson", "title": "Outlier-robust sparse/low-rank least-squares regression and robust\n  matrix completion", "comments": "Correction of typos; addition of simulation results; addition of new\n  oracle inequalities for Lasso and Slope in Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study high-dimensional least-squares regression within a subgaussian\nstatistical learning framework with heterogeneous noise. It includes $s$-sparse\nand $r$-low-rank least-squares regression when a fraction $\\epsilon$ of the\nlabels are adversarially contaminated. We also present a novel theory of\ntrace-regression with matrix decomposition based on a new application of the\nproduct process. For these problems, we show novel near-optimal \"subgaussian\"\nestimation rates of the form\n$r(n,d_{e})+\\sqrt{\\log(1/\\delta)/n}+\\epsilon\\log(1/\\epsilon)$, valid with\nprobability at least $1-\\delta$. Here, $r(n,d_{e})$ is the optimal\nuncontaminated rate as a function of the effective dimension $d_{e}$ but\nindependent of the failure probability $\\delta$. These rates are valid\nuniformly on $\\delta$, i.e., the estimators' tuning do not depend on $\\delta$.\nLastly, we consider noisy robust matrix completion with non-uniform sampling.\nIf only the low-rank matrix is of interest, we present a novel near-optimal\nrate that is independent of the corruption level $a$. Our estimators are\ntractable and based on a new \"sorted\" Huber-type loss. No information on\n$(s,r,\\epsilon,a,\\delta)$ are needed to tune these estimators. Our analysis\nmakes use of novel $\\delta$-optimal concentration inequalities for the\nmultiplier and product processes which could be useful elsewhere. For instance,\nthey imply novel sharp oracle inequalities for Lasso and Slope with optimal\ndependence on $\\delta$. Numerical simulations confirm our theoretical\npredictions. In particular, \"sorted\" Huber regression can outperform classical\nHuber regression.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 07:42:47 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 15:02:32 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Thompson", "Philip", ""]]}, {"id": "2012.06784", "submitter": "Lorenzo Fassina PhD", "authors": "Lorenzo Fassina, Alessandro Faragli, Francesco Paolo Lo Muzio,\n  Sebastian Kelle, Carlo Campana, Burkert Pieske, Frank Edelmann, Alessio\n  Alogna", "title": "A random shuffle method to expand a narrow dataset and overcome the\n  associated challenges in a clinical study: a heart failure cohort example", "comments": null, "journal-ref": "Frontiers in Cardiovascular Medicine 2020;7:599923", "doi": "10.3389/fcvm.2020.599923", "report-no": null, "categories": "q-bio.QM cs.LG stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Heart failure (HF) affects at least 26 million people worldwide, so\npredicting adverse events in HF patients represents a major target of clinical\ndata science. However, achieving large sample sizes sometimes represents a\nchallenge due to difficulties in patient recruiting and long follow-up times,\nincreasing the problem of missing data. To overcome the issue of a narrow\ndataset cardinality (in a clinical dataset, the cardinality is the number of\npatients in that dataset), population-enhancing algorithms are therefore\ncrucial. The aim of this study was to design a random shuffle method to enhance\nthe cardinality of an HF dataset while it is statistically legitimate, without\nthe need of specific hypotheses and regression models. The cardinality\nenhancement was validated against an established random repeated-measures\nmethod with regard to the correctness in predicting clinical conditions and\nendpoints. In particular, machine learning and regression models were employed\nto highlight the benefits of the enhanced datasets. The proposed random shuffle\nmethod was able to enhance the HF dataset cardinality (711 patients before\ndataset preprocessing) circa 10 times and circa 21 times when followed by a\nrandom repeated-measures approach. We believe that the random shuffle method\ncould be used in the cardiovascular field and in other data science problems\nwhen missing data and the narrow dataset cardinality represent an issue.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 10:59:38 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Fassina", "Lorenzo", ""], ["Faragli", "Alessandro", ""], ["Muzio", "Francesco Paolo Lo", ""], ["Kelle", "Sebastian", ""], ["Campana", "Carlo", ""], ["Pieske", "Burkert", ""], ["Edelmann", "Frank", ""], ["Alogna", "Alessio", ""]]}, {"id": "2012.06846", "submitter": "Alessio Benavoli", "authors": "Alessio Benavoli and Dario Azzimonti and Dario Piga", "title": "A unified framework for closed-form nonparametric regression,\n  classification, preference and mixed problems with Skew Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skew-Gaussian processes (SkewGPs) extend the multivariate Unified Skew-Normal\ndistributions over finite dimensional vectors to distribution over functions.\nSkewGPs are more general and flexible than Gaussian processes, as SkewGPs may\nalso represent asymmetric distributions. In a recent contribution we showed\nthat SkewGP and probit likelihood are conjugate, which allows us to compute the\nexact posterior for non-parametric binary classification and preference\nlearning. In this paper, we generalize previous results and we prove that\nSkewGP is conjugate with both the normal and affine probit likelihood, and more\nin general, with their product. This allows us to (i) handle classification,\npreference, numeric and ordinal regression, and mixed problems in a unified\nframework; (ii) derive closed-form expression for the corresponding posterior\ndistributions. We show empirically that the proposed framework based on SkewGP\nprovides better performance than Gaussian processes in active learning and\nBayesian (constrained) optimization. These two tasks are fundamental for design\nof experiments and in Data Science.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 15:58:16 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 10:47:59 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Benavoli", "Alessio", ""], ["Azzimonti", "Dario", ""], ["Piga", "Dario", ""]]}, {"id": "2012.06848", "submitter": "Arnaud Liehrmann", "authors": "Arnaud Liehrmann, Guillem Rigaill and Toby Dylan Hocking", "title": "Increased peak detection accuracy in over-dispersed ChIP-seq data with\n  supervised segmentation models", "comments": "20 pages, 8 figures; updated broken citations and references", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Histone modification constitutes a basic mechanism for the\ngenetic regulation of gene expression. In early 2000s, a powerful technique has\nemerged that couples chromatin immunoprecipitation with high-throughput\nsequencing (ChIP-seq). This technique provides a direct survey of the DNA\nregions associated to these modifications. In order to realize the full\npotential of this technique, increasingly sophisticated statistical algorithms\nhave been developed or adapted to analyze the massive amount of data it\ngenerates. Many of these algorithms were built around natural assumptions such\nas the Poisson one to model the noise in the count data. In this work we start\nfrom these natural assumptions and show that it is possible to improve upon\nthem. Results: The results of our comparisons on seven reference datasets of\nhistone modifications (H3K36me3 and H3K4me3) suggest that natural assumptions\nare not always realistic under application conditions. We show that the\nunconstrained multiple changepoint detection model, with alternative noise\nassumptions and a suitable setup, reduces the over-dispersion exhibited by\ncount data and turns out to detect peaks more accurately than algorithms which\nrely on these natural assumptions.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 16:03:27 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 12:34:48 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Liehrmann", "Arnaud", ""], ["Rigaill", "Guillem", ""], ["Hocking", "Toby Dylan", ""]]}, {"id": "2012.06914", "submitter": "Yinan Wang", "authors": "Yinan Wang, Kaiwen Wang, Wenjun Cai, Xiaowei Yue", "title": "NP-ODE: Neural Process Aided Ordinary Differential Equations for\n  Uncertainty Quantification of Finite Element Analysis", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite element analysis (FEA) has been widely used to generate simulations of\ncomplex and nonlinear systems. Despite its strength and accuracy, the\nlimitations of FEA can be summarized into two aspects: a) running high-fidelity\nFEA often requires significant computational cost and consumes a large amount\nof time; b) FEA is a deterministic method that is insufficient for uncertainty\nquantification (UQ) when modeling complex systems with various types of\nuncertainties. In this paper, a physics-informed data-driven surrogate model,\nnamed Neural Process Aided Ordinary Differential Equation (NP-ODE), is proposed\nto model the FEA simulations and capture both input and output uncertainties.\nTo validate the advantages of the proposed NP-ODE, we conduct experiments on\nboth the simulation data generated from a given ordinary differential equation\nand the data collected from a real FEA platform for tribocorrosion. The\nperformances of the proposed NP-ODE and several benchmark methods are compared.\nThe results show that the proposed NP-ODE outperforms benchmark methods. The\nNP-ODE method realizes the smallest predictive error as well as generates the\nmost reasonable confidence interval having the best coverage on testing data\npoints.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 22:38:16 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Wang", "Yinan", ""], ["Wang", "Kaiwen", ""], ["Cai", "Wenjun", ""], ["Yue", "Xiaowei", ""]]}, {"id": "2012.06916", "submitter": "Kungang Zhang", "authors": "Kungang Zhang, Anh T. Bui, Daniel W. Apley", "title": "Concept Drift Monitoring and Diagnostics of Supervised Learning Models\n  via Score Vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised learning models are one of the most fundamental classes of models.\nViewing supervised learning from a probabilistic perspective, the set of\ntraining data to which the model is fitted is usually assumed to follow a\nstationary distribution. However, this stationarity assumption is often\nviolated in a phenomenon called concept drift, which refers to changes over\ntime in the predictive relationship between covariates $\\mathbf{X}$ and a\nresponse variable $Y$ and can render trained models suboptimal or obsolete. We\ndevelop a comprehensive and computationally efficient framework for detecting,\nmonitoring, and diagnosing concept drift. Specifically, we monitor the Fisher\nscore vector, defined as the gradient of the log-likelihood for the fitted\nmodel, using a form of multivariate exponentially weighted moving average,\nwhich monitors for general changes in the mean of a random vector. In spite of\nthe substantial performance advantages that we demonstrate over popular\nerror-based methods, a score-based approach has not been previously considered\nfor concept drift monitoring. Advantages of the proposed score-based framework\ninclude applicability to any parametric model, more powerful detection of\nchanges as shown in theory and experiments, and inherent diagnostic\ncapabilities for helping to identify the nature of the changes.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 22:52:45 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Zhang", "Kungang", ""], ["Bui", "Anh T.", ""], ["Apley", "Daniel W.", ""]]}, {"id": "2012.06951", "submitter": "Qi Qi", "authors": "Qi Qi, Yi Xu, Rong Jin, Wotao Yin, Tianbao Yang", "title": "Attentional Biased Stochastic Gradient for Imbalanced Classification", "comments": "29pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a simple yet effective method (ABSGD) for\naddressing the data imbalance issue in deep learning. Our method is a simple\nmodification to momentum SGD where we leverage an attentional mechanism to\nassign an individual importance weight to each gradient in the mini-batch.\nUnlike many existing heuristic-driven methods for tackling data imbalance, our\nmethod is grounded in {\\it theoretically justified distributionally robust\noptimization (DRO)}, which is guaranteed to converge to a stationary point of\nan information-regularized DRO problem. The individual-level weight of a\nsampled data is systematically proportional to the exponential of a scaled loss\nvalue of the data, where the scaling factor is interpreted as the\nregularization parameter in the framework of information-regularized DRO.\nCompared with existing class-level weighting schemes, our method can capture\nthe diversity between individual examples within each class. Compared with\nexisting individual-level weighting methods using meta-learning that require\nthree backward propagations for computing mini-batch stochastic gradients, our\nmethod is more efficient with only one backward propagation at each iteration\nas in standard deep learning methods. To balance between the learning of\nfeature extraction layers and the learning of the classifier layer, we employ a\ntwo-stage method that uses SGD for pretraining followed by ABSGD for learning a\nrobust classifier and finetuning lower layers. Our empirical studies on several\nbenchmark datasets demonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 03:41:52 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 02:29:33 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Qi", "Qi", ""], ["Xu", "Yi", ""], ["Jin", "Rong", ""], ["Yin", "Wotao", ""], ["Yang", "Tianbao", ""]]}, {"id": "2012.06958", "submitter": "Justin Solomon", "authors": "Justin Solomon, Kristjan Greenewald, Haikady N. Nagaraja", "title": "$k$-Variance: A Clustered Notion of Variance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG cs.NA math.NA stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce $k$-variance, a generalization of variance built on the\nmachinery of random bipartite matchings. $K$-variance measures the expected\ncost of matching two sets of $k$ samples from a distribution to each other,\ncapturing local rather than global information about a measure as $k$\nincreases; it is easily approximated stochastically using sampling and linear\nprogramming. In addition to defining $k$-variance and proving its basic\nproperties, we provide in-depth analysis of this quantity in several key cases,\nincluding one-dimensional measures, clustered measures, and measures\nconcentrated on low-dimensional subsets of $\\mathbb R^n$. We conclude with\nexperiments and open problems motivated by this new way to summarize\ndistributional shape.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 04:25:32 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Solomon", "Justin", ""], ["Greenewald", "Kristjan", ""], ["Nagaraja", "Haikady N.", ""]]}, {"id": "2012.06969", "submitter": "Vamshi Chowdary Madala", "authors": "Abhejit Rajagopal, Vamshi C. Madala, Shivkumar Chandrasekaran, Peder\n  E. Z. Larson", "title": "Predicting Generalization in Deep Learning via Local Measures of\n  Distortion", "comments": "Added preprint footnote", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study generalization in deep learning by appealing to complexity measures\noriginally developed in approximation and information theory. While these\nconcepts are challenged by the high-dimensional and data-defined nature of deep\nlearning, we show that simple vector quantization approaches such as PCA, GMMs,\nand SVMs capture their spirit when applied layer-wise to deep extracted\nfeatures giving rise to relatively inexpensive complexity measures that\ncorrelate well with generalization performance. We discuss our results in 2020\nNeurIPS PGDL challenge.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 05:46:46 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 02:22:46 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Rajagopal", "Abhejit", ""], ["Madala", "Vamshi C.", ""], ["Chandrasekaran", "Shivkumar", ""], ["Larson", "Peder E. Z.", ""]]}, {"id": "2012.06979", "submitter": "Sivan Sabato", "authors": "Shachar Schnapp and Sivan Sabato", "title": "Active Feature Selection for the Mutual Information Criterion", "comments": "To appear in AAAI-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study active feature selection, a novel feature selection setting in which\nunlabeled data is available, but the budget for labels is limited, and the\nexamples to label can be actively selected by the algorithm. We focus on\nfeature selection using the classical mutual information criterion, which\nselects the $k$ features with the largest mutual information with the label. In\nthe active feature selection setting, the goal is to use significantly fewer\nlabels than the data set size and still find $k$ features whose mutual\ninformation with the label based on the \\emph{entire} data set is large. We\nexplain and experimentally study the choices that we make in the algorithm, and\nshow that they lead to a successful algorithm, compared to other more naive\napproaches. Our design draws on insights which relate the problem of active\nfeature selection to the study of pure-exploration multi-armed bandits\nsettings. While we focus here on mutual information, our general methodology\ncan be adapted to other feature-quality measures as well. The code is available\nat the following url: https://github.com/ShacharSchnapp/ActiveFeatureSelection.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 06:40:35 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Schnapp", "Shachar", ""], ["Sabato", "Sivan", ""]]}, {"id": "2012.07038", "submitter": "Christina Petschnigg", "authors": "Christina Petschnigg and Juergen Pilz", "title": "Uncertainty Estimation in Deep Neural Networks for Point Cloud\n  Segmentation in Factory Planning", "comments": "17 pages, 5 figures, submitted to MDPI Modelling journal for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The digital factory provides undoubtedly a great potential for future\nproduction systems in terms of efficiency and effectivity. A key aspect on the\nway to realize the digital copy of a real factory is the understanding of\ncomplex indoor environments on the basis of 3D data. In order to generate an\naccurate factory model including the major components, i.e. building parts,\nproduct assets and process details, the 3D data collected during digitalization\ncan be processed with advanced methods of deep learning. In this work, we\npropose a fully Bayesian and an approximate Bayesian neural network for point\ncloud segmentation. This allows us to analyze how different ways of estimating\nuncertainty in these networks improve segmentation results on raw 3D point\nclouds. We achieve superior model performance for both, the Bayesian and the\napproximate Bayesian model compared to the frequentist one. This performance\ndifference becomes even more striking when incorporating the networks'\nuncertainty in their predictions. For evaluation we use the scientific data set\nS3DIS as well as a data set, which was collected by the authors at a German\nautomotive production plant. The methods proposed in this work lead to more\naccurate segmentation results and the incorporation of uncertainty information\nmakes this approach especially applicable to safety critical applications.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 11:18:52 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Petschnigg", "Christina", ""], ["Pilz", "Juergen", ""]]}, {"id": "2012.07044", "submitter": "Jingxin Zhang", "authors": "Jingxin Zhang, Donghua Zhou, and Maoyin Chen", "title": "Monitoring multimode processes: a modified PCA algorithm with continual\n  learning ability", "comments": "This paper has been accepted by Journal of Process Control", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For multimode processes, one generally establishes local monitoring models\ncorresponding to local modes. However, the significant features of previous\nmodes may be catastrophically forgotten when a monitoring model for the current\nmode is built. It would result in an abrupt performance decrease. It could be\nan effective manner to make local monitoring model remember the features of\nprevious modes. Choosing the principal component analysis (PCA) as a basic\nmonitoring model, we try to resolve this problem. A modified PCA algorithm is\nbuilt with continual learning ability for monitoring multimode processes, which\nadopts elastic weight consolidation (EWC) to overcome catastrophic forgetting\nof PCA for successive modes. It is called PCA-EWC, where the significant\nfeatures of previous modes are preserved when a PCA model is established for\nthe current mode. The optimal parameters are acquired by differences of convex\nfunctions. Moreover, the proposed PCA-EWC is extended to general multimode\nprocesses and the procedure is presented. The computational complexity and key\nparameters are discussed to further understand the relationship between PCA and\nthe proposed algorithm. Potential limitations and relevant solutions are\npointed to understand the algorithm further. Numerical case study and a\npractical industrial system in China are employed to illustrate the\neffectiveness of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 12:09:38 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 03:33:54 GMT"}, {"version": "v3", "created": "Mon, 28 Dec 2020 12:46:44 GMT"}, {"version": "v4", "created": "Mon, 26 Apr 2021 13:45:40 GMT"}, {"version": "v5", "created": "Fri, 28 May 2021 08:50:55 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Zhang", "Jingxin", ""], ["Zhou", "Donghua", ""], ["Chen", "Maoyin", ""]]}, {"id": "2012.07048", "submitter": "Siwei Wang", "authors": "Siwei Wang, Haoyun Wang, Longbo Huang", "title": "Adaptive Algorithms for Multi-armed Bandit with Composite and Anonymous\n  Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the multi-armed bandit (MAB) problem with composite and anonymous\nfeedback. In this model, the reward of pulling an arm spreads over a period of\ntime (we call this period as reward interval) and the player receives partial\nrewards of the action, convoluted with rewards from pulling other arms,\nsuccessively. Existing results on this model require prior knowledge about the\nreward interval size as an input to their algorithms. In this paper, we propose\nadaptive algorithms for both the stochastic and the adversarial cases, without\nrequiring any prior information about the reward interval. For the stochastic\ncase, we prove that our algorithm guarantees a regret that matches the lower\nbounds (in order). For the adversarial case, we propose the first algorithm to\njointly handle non-oblivious adversary and unknown reward interval size. We\nalso conduct simulations based on real-world dataset. The results show that our\nalgorithms outperform existing benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 12:25:41 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 11:44:06 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Wang", "Siwei", ""], ["Wang", "Haoyun", ""], ["Huang", "Longbo", ""]]}, {"id": "2012.07058", "submitter": "Vineet Nair", "authors": "Vineet Nair, Vishakha Patil, Gaurav Sinha", "title": "Budgeted and Non-budgeted Causal Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Learning good interventions in a causal graph can be modelled as a stochastic\nmulti-armed bandit problem with side-information. First, we study this problem\nwhen interventions are more expensive than observations and a budget is\nspecified. If there are no backdoor paths from an intervenable node to the\nreward node then we propose an algorithm to minimize simple regret that\noptimally trades-off observations and interventions based on the cost of\nintervention. We also propose an algorithm that accounts for the cost of\ninterventions, utilizes causal side-information, and minimizes the expected\ncumulative regret without exceeding the budget. Our cumulative-regret\nminimization algorithm performs better than standard algorithms that do not\ntake side-information into account. Finally, we study the problem of learning\nbest interventions without budget constraint in general graphs and give an\nalgorithm that achieves constant expected cumulative regret in terms of the\ninstance parameters when the parent distribution of the reward variable for\neach intervention is known. Our results are experimentally validated and\ncompared to the best-known bounds in the current literature.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 13:31:14 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Nair", "Vineet", ""], ["Patil", "Vishakha", ""], ["Sinha", "Gaurav", ""]]}, {"id": "2012.07176", "submitter": "Reza Esfandiarpoor", "authors": "Reza Esfandiarpoor, Amy Pu, Mohsen Hajabdollahi, Stephen H. Bach", "title": "Extended Few-Shot Learning: Exploiting Existing Resources for Novel\n  Tasks", "comments": "Added the new version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many practical few-shot learning problems, even though labeled examples\nare scarce, there are abundant auxiliary datasets that potentially contain\nuseful information. We propose the problem of extended few-shot learning to\nstudy these scenarios. We then introduce a framework to address the challenges\nof efficiently selecting and effectively using auxiliary data in few-shot image\nclassification. Given a large auxiliary dataset and a notion of semantic\nsimilarity among classes, we automatically select pseudo shots, which are\nlabeled examples from other classes related to the target task. We show that\nnaive approaches, such as (1) modeling these additional examples the same as\nthe target task examples or (2) using them to learn features via transfer\nlearning, only increase accuracy by a modest amount. Instead, we propose a\nmasking module that adjusts the features of auxiliary data to be more similar\nto those of the target classes. We show that this masking module performs\nbetter than naively modeling the support examples and transfer learning by 4.68\nand 6.03 percentage points, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 22:45:44 GMT"}, {"version": "v2", "created": "Sat, 16 Jan 2021 15:53:04 GMT"}, {"version": "v3", "created": "Sat, 3 Jul 2021 19:47:58 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Esfandiarpoor", "Reza", ""], ["Pu", "Amy", ""], ["Hajabdollahi", "Mohsen", ""], ["Bach", "Stephen H.", ""]]}, {"id": "2012.07179", "submitter": "Harish Naik", "authors": "Harish Naik, Gy\\\"orgy Tur\\'an", "title": "Explanation from Specification", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Explainable components in XAI algorithms often come from a familiar set of\nmodels, such as linear models or decision trees. We formulate an approach where\nthe type of explanation produced is guided by a specification. Specifications\nare elicited from the user, possibly using interaction with the user and\ncontributions from other areas. Areas where a specification could be obtained\ninclude forensic, medical, and scientific applications. Providing a menu of\npossible types of specifications in an area is an exploratory knowledge\nrepresentation and reasoning task for the algorithm designer, aiming at\nunderstanding the possibilities and limitations of efficiently computable modes\nof explanations. Two examples are discussed: explanations for Bayesian networks\nusing the theory of argumentation, and explanations for graph neural networks.\nThe latter case illustrates the possibility of having a representation\nformalism available to the user for specifying the type of explanation\nrequested, for example, a chemical query language for classifying molecules.\nThe approach is motivated by a theory of explanation in the philosophy of\nscience, and it is related to current questions in the philosophy of science on\nthe role of machine learning.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 23:27:48 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Naik", "Harish", ""], ["Tur\u00e1n", "Gy\u00f6rgy", ""]]}, {"id": "2012.07245", "submitter": "Kentaro Minami", "authors": "Kentaro Imajo and Kentaro Minami and Katsuya Ito and Kei Nakagawa", "title": "Deep Portfolio Optimization via Distributional Prediction of Residual\n  Factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in deep learning techniques have motivated intensive\nresearch in machine learning-aided stock trading strategies. However, since the\nfinancial market has a highly non-stationary nature hindering the application\nof typical data-hungry machine learning methods, leveraging financial inductive\nbiases is important to ensure better sample efficiency and robustness. In this\nstudy, we propose a novel method of constructing a portfolio based on\npredicting the distribution of a financial quantity called residual factors,\nwhich is known to be generally useful for hedging the risk exposure to common\nmarket factors. The key technical ingredients are twofold. First, we introduce\na computationally efficient extraction method for the residual information,\nwhich can be easily combined with various prediction algorithms. Second, we\npropose a novel neural network architecture that allows us to incorporate\nwidely acknowledged financial inductive biases such as amplitude invariance and\ntime-scale invariance. We demonstrate the efficacy of our method on U.S. and\nJapanese stock market data. Through ablation experiments, we also verify that\neach individual technique contributes to improving the performance of trading\nstrategies. We anticipate our techniques may have wide applications in various\nfinancial problems.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 04:09:52 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Imajo", "Kentaro", ""], ["Minami", "Kentaro", ""], ["Ito", "Katsuya", ""], ["Nakagawa", "Kei", ""]]}, {"id": "2012.07269", "submitter": "Jarrad Courts", "authors": "Jarrad Courts and Johannes Hendriks and Adrian Wills and Thomas\n  Sch\\\"on and Brett Ninness", "title": "Variational State and Parameter Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of computing Bayesian estimates of both\nstates and model parameters for nonlinear state-space models. Generally, this\nproblem does not have a tractable solution and approximations must be utilised.\nIn this work, a variational approach is used to provide an assumed density\nwhich approximates the desired, intractable, distribution. The approach is\ndeterministic and results in an optimisation problem of a standard form. Due to\nthe parametrisation of the assumed density selected first- and second-order\nderivatives are readily available which allows for efficient solutions. The\nproposed method is compared against state-of-the-art Hamiltonian Monte Carlo in\ntwo numerical examples.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 05:35:29 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Courts", "Jarrad", ""], ["Hendriks", "Johannes", ""], ["Wills", "Adrian", ""], ["Sch\u00f6n", "Thomas", ""], ["Ninness", "Brett", ""]]}, {"id": "2012.07278", "submitter": "Jean Feng", "authors": "Jean Feng", "title": "Learning how to approve updates to machine learning algorithms in\n  non-stationary settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning algorithms in healthcare have the potential to continually\nlearn from real-world data generated during healthcare delivery and adapt to\ndataset shifts. As such, the FDA is looking to design policies that can\nautonomously approve modifications to machine learning algorithms while\nmaintaining or improving the safety and effectiveness of the deployed models.\nHowever, selecting a fixed approval strategy, a priori, can be difficult\nbecause its performance depends on the stationarity of the data and the quality\nof the proposed modifications. To this end, we investigate a\nlearning-to-approve approach (L2A) that uses accumulating monitoring data to\nlearn how to approve modifications. L2A defines a family of strategies that\nvary in their \"optimism''---where more optimistic policies have faster approval\nrates---and searches over this family using an exponentially weighted average\nforecaster. To control the cumulative risk of the deployed model, we give L2A\nthe option to abstain from making a prediction and incur some fixed abstention\ncost instead. We derive bounds on the average risk of the model deployed by\nL2A, assuming the distributional shifts are smooth. In simulation studies and\nempirical analyses, L2A tailors the level of optimism for each problem-setting:\nIt learns to abstain when performance drops are common and approve beneficial\nmodifications quickly when the distribution is stable.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 05:54:55 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Feng", "Jean", ""]]}, {"id": "2012.07283", "submitter": "Sepanta Zeighami", "authors": "Sirisha Rambhatla, Sepanta Zeighami, Kameron Shahabi, Cyrus Shahabi,\n  Yan Liu", "title": "Towards Accurate Spatiotemporal COVID-19 Risk Scores using High\n  Resolution Real-World Mobility Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As countries look towards re-opening of economic activities amidst the\nongoing COVID-19 pandemic, ensuring public health has been challenging. While\ncontact tracing only aims to track past activities of infected users, one path\nto safe reopening is to develop reliable spatiotemporal risk scores to indicate\nthe propensity of the disease. Existing works which aim to develop risk scores\neither rely on compartmental model-based reproduction numbers (which assume\nuniform population mixing) or develop coarse-grain spatial scores based on\nreproduction number (R0) and macro-level density-based mobility statistics.\nInstead, in this paper, we develop a Hawkes process-based technique to assign\nrelatively fine-grain spatial and temporal risk scores by leveraging\nhigh-resolution mobility data based on cell-phone originated location signals.\nWhile COVID-19 risk scores also depend on a number of factors specific to an\nindividual, including demography and existing medical conditions, the primary\nmode of disease transmission is via physical proximity and contact. Therefore,\nwe focus on developing risk scores based on location density and mobility\nbehaviour. We demonstrate the efficacy of the developed risk scores via\nsimulation based on real-world mobility data. Our results show that fine-grain\nspatiotemporal risk scores based on high-resolution mobility data can provide\nuseful insights and facilitate safe re-opening.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 06:31:28 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Rambhatla", "Sirisha", ""], ["Zeighami", "Sepanta", ""], ["Shahabi", "Kameron", ""], ["Shahabi", "Cyrus", ""], ["Liu", "Yan", ""]]}, {"id": "2012.07346", "submitter": "Matthew J. Holland", "authors": "Matthew J. Holland", "title": "Better scalability under potentially heavy-tailed feedback", "comments": "This work merges arXiv:2006.00784 and arXiv:2006.01364, providing\n  additional empirical analysis using real-world benchmark datasets", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study scalable alternatives to robust gradient descent (RGD) techniques\nthat can be used when the losses and/or gradients can be heavy-tailed, though\nthis will be unknown to the learner. The core technique is simple: instead of\ntrying to robustly aggregate gradients at each step, which is costly and leads\nto sub-optimal dimension dependence in risk bounds, we instead focus\ncomputational effort on robustly choosing (or newly constructing) a strong\ncandidate based on a collection of cheap stochastic sub-processes which can be\nrun in parallel. The exact selection process depends on the convexity of the\nunderlying objective, but in all cases, our selection technique amounts to a\nrobust form of boosting the confidence of weak learners. In addition to formal\nguarantees, we also provide empirical analysis of robustness to perturbations\nto experimental conditions, under both sub-Gaussian and heavy-tailed data,\nalong with applications to a variety of benchmark datasets. The overall\ntake-away is an extensible procedure that is simple to implement, trivial to\nparallelize, which keeps the formal merits of RGD methods but scales much\nbetter to large learning problems.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 08:56:04 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Holland", "Matthew J.", ""]]}, {"id": "2012.07348", "submitter": "Lydia T. Liu", "authors": "Lydia T. Liu, Feng Ruan, Horia Mania, Michael I. Jordan", "title": "Bandit Learning in Decentralized Matching Markets", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study two-sided matching markets in which one side of the market (the\nplayers) does not have a priori knowledge about its preferences for the other\nside (the arms) and is required to learn its preferences from experience. Also,\nwe assume the players have no direct means of communication. This model extends\nthe standard stochastic multi-armed bandit framework to a decentralized\nmultiple player setting with competition. We introduce a new algorithm for this\nsetting that, over a time horizon $T$, attains $\\mathcal{O}(\\log(T))$ stable\nregret when preferences of the arms over players are shared, and\n$\\mathcal{O}(\\log(T)^2)$ regret when there are no assumptions on the\npreferences on either side. Moreover, in the setting where a single player may\ndeviate, we show that the algorithm is incentive compatible whenever the arms'\npreferences are shared, but not necessarily so when preferences are fully\ngeneral.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 08:58:07 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2020 00:29:03 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2021 03:55:05 GMT"}, {"version": "v4", "created": "Mon, 21 Jun 2021 19:56:34 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Liu", "Lydia T.", ""], ["Ruan", "Feng", ""], ["Mania", "Horia", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2012.07386", "submitter": "Marylou Gabri\\'e", "authors": "Hannah Lawrence, David A. Barmherzig, Henry Li, Michael Eickenberg and\n  Marylou Gabri\\'e", "title": "Phase Retrieval with Holography and Untrained Priors: Tackling the\n  Challenges of Low-Photon Nanoscale Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV physics.optics stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phase retrieval is the inverse problem of recovering a signal from\nmagnitude-only Fourier measurements, and underlies numerous imaging modalities,\nsuch as Coherent Diffraction Imaging (CDI). A variant of this setup, known as\nholography, includes a reference object that is placed adjacent to the specimen\nof interest before measurements are collected. The resulting inverse problem,\nknown as holographic phase retrieval, is well-known to have improved problem\nconditioning relative to the original. This innovation, i.e. Holographic CDI,\nbecomes crucial at the nanoscale, where imaging specimens such as viruses,\nproteins, and crystals require low-photon measurements. This data is highly\ncorrupted by Poisson shot noise, and often lacks low-frequency content as well.\nIn this work, we introduce a dataset-free deep learning framework for\nholographic phase retrieval adapted to these challenges. The key ingredients of\nour approach are the explicit and flexible incorporation of the physical\nforward model into an automatic differentiation procedure, the Poisson\nlog-likelihood objective function, and an optional untrained deep image prior.\nWe perform extensive evaluation under realistic conditions. Compared to\ncompeting classical methods, our method recovers signal from higher noise\nlevels and is more resilient to suboptimal reference design, as well as to\nlarge missing regions of low frequencies in the observations. Finally, we show\nthat these properties carry over to experimental data acquired on optical\nwavelengths. To the best of our knowledge, this is the first work to consider a\ndataset-free machine learning approach for holographic phase retrieval.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 10:15:07 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 16:46:13 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 03:48:17 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Lawrence", "Hannah", ""], ["Barmherzig", "David A.", ""], ["Li", "Henry", ""], ["Eickenberg", "Michael", ""], ["Gabri\u00e9", "Marylou", ""]]}, {"id": "2012.07397", "submitter": "Pietro Bongini", "authors": "Pietro Bongini, Monica Bianchini, Franco Scarselli", "title": "Molecular graph generation with Graph Neural Networks", "comments": "20 pages, 4 figures (2 figures are composed of double images, for a\n  total of 6 images)", "journal-ref": "Neurocomputing 2021", "doi": "10.1016/j.neucom.2021.04.039", "report-no": null, "categories": "stat.ML cs.LG q-bio.BM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Drug Discovery is a fundamental and ever-evolving field of research. The\ndesign of new candidate molecules requires large amounts of time and money, and\ncomputational methods are being increasingly employed to cut these costs.\nMachine learning methods are ideal for the design of large amounts of potential\nnew candidate molecules, which are naturally represented as graphs. Graph\ngeneration is being revolutionized by deep learning methods, and molecular\ngeneration is one of its most promising applications. In this paper, we\nintroduce a sequential molecular graph generator based on a set of graph neural\nnetwork modules, which we call MG^2N^2. At each step, a node or a group of\nnodes is added to the graph, along with its connections. The modular\narchitecture simplifies the training procedure, also allowing an independent\nretraining of a single module. Sequentiality and modularity make the generation\nprocess interpretable. The use of graph neural networks maximizes the\ninformation in input at each generative step, which consists of the subgraph\nproduced during the previous steps. Experiments of unconditional generation on\nthe QM9 and Zinc datasets show that our model is capable of generalizing\nmolecular patterns seen during the training phase, without overfitting. The\nresults indicate that our method is competitive, and outperforms challenging\nbaselines for unconditional generation.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 10:32:57 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 10:07:40 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Bongini", "Pietro", ""], ["Bianchini", "Monica", ""], ["Scarselli", "Franco", ""]]}, {"id": "2012.07460", "submitter": "Xurong Xie", "authors": "Xurong Xie, Xunying Liu, Tan Lee, Lan Wang", "title": "Bayesian Learning for Deep Neural Network Adaptation", "comments": "13 pages, submitted to TASLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD eess.AS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A key task for speech recognition systems is to reduce the mismatch between\nthe training and evaluation data that is often attributable to speaker\ndifferences. To this end, speaker adaptation techniques play a vital role to\nreduce the mismatch. Model-based speaker adaptation approaches often require\nsufficient amounts of target speaker data to ensure robustness. When the amount\nof speaker level data is limited, speaker adaptation is prone to overfitting\nand poor generalization. To address the issue, this paper proposes a full\nBayesian learning based DNN speaker adaptation framework to model\nspeaker-dependent (SD) parameter uncertainty given limited speaker specific\nadaptation data. This framework is investigated in three forms of model based\nDNN adaptation techniques: Bayesian learning of hidden unit contributions\n(BLHUC), Bayesian parameterized activation functions (BPAct), and Bayesian\nhidden unit bias vectors (BHUB). In all three Bayesian adaptation methods,\ndeterministic SD parameters are replaced by latent variable posterior\ndistributions to be learned for each speaker, whose parameters are efficiently\nestimated using a variational inference based approach. Experiments conducted\non 300-hour speed perturbed Switchboard corpus trained LF-MMI factored\nTDNN/CNN-TDNN systems featuring i-vector speaker adaptation suggest the\nproposed Bayesian adaptation approaches consistently outperform the adapted\nsystems using deterministic parameters on the NIST Hub5'00 and RT03 evaluation\nsets in both unsupervised test time speaker adaptation and speaker adaptive\ntraining. The efficacy of the proposed Bayesian adaptation techniques is\nfurther demonstrated in a comparison against the state-of-the-art performance\nobtained on the same task using the most recent hybrid and end-to-end systems\nreported in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 12:30:41 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Xie", "Xurong", ""], ["Liu", "Xunying", ""], ["Lee", "Tan", ""], ["Wang", "Lan", ""]]}, {"id": "2012.07499", "submitter": "Petar Milin", "authors": "Petar Milin, Benjamin V. Tucker, and Dagmar Divjak", "title": "A learning perspective on the emergence of abstractions: the curious\n  case of phonemes", "comments": "45 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper we use a range of modeling techniques to investigate\nwhether an abstract phone could emerge from exposure to speech sounds. We test\ntwo opposing principles regarding the development of language knowledge in\nlinguistically untrained language users: Memory-Based Learning (MBL) and\nError-Correction Learning (ECL). A process of generalization underlies the\nabstractions linguists operate with, and we probed whether MBL and ECL could\ngive rise to a type of language knowledge that resembles linguistic\nabstractions. Each model was presented with a significant amount of\npre-processed speech produced by one speaker. We assessed the consistency or\nstability of what the models have learned and their ability to give rise to\nabstract categories. Both types of models fare differently with regard to these\ntests. We show that ECL learning models can learn abstractions and that at\nleast part of the phone inventory can be reliably identified from the input.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 13:33:34 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 15:08:48 GMT"}, {"version": "v3", "created": "Thu, 17 Dec 2020 14:06:59 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Milin", "Petar", ""], ["Tucker", "Benjamin V.", ""], ["Divjak", "Dagmar", ""]]}, {"id": "2012.07513", "submitter": "Raanan Yehezkel Rohekar", "authors": "Raanan Y. Rohekar, Yaniv Gurwicz, Shami Nisimov, Gal Novik", "title": "A Single Iterative Step for Anytime Causal Discovery", "comments": "34th Conference on Neural Information Processing Systems (NeurIPS\n  2020), Vancouver, Canada, Workshop on Causal Discovery & Causality-Inspired\n  Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a sound and complete algorithm for recovering causal graphs from\nobserved, non-interventional data, in the possible presence of latent\nconfounders and selection bias. We rely on the causal Markov and faithfulness\nassumptions and recover the equivalence class of the underlying causal graph by\nperforming a series of conditional independence (CI) tests between observed\nvariables. We propose a single step that is applied iteratively, such that the\nindependence and causal relations entailed from the resulting graph, after any\niteration, is correct and becomes more informative with successive iteration.\nEssentially, we tie the size of the CI condition set to its distance from the\ntested nodes on the resulting graph. Each iteration refines the skeleton and\norientation by performing CI tests having condition sets that are larger than\nin the preceding iteration. In an iteration, condition sets of CI tests are\nconstructed from nodes that are within a specified search distance, and the\nsizes of these condition sets is equal to this search distance. The algorithm\nthen iteratively increases the search distance along with the condition set\nsizes. Thus, each iteration refines a graph, that was recovered by previous\niterations having smaller condition sets -- having a higher statistical power.\nWe demonstrate that our algorithm requires significantly fewer CI tests and\nsmaller condition sets compared to the FCI algorithm. This is evident for both\nrecovering the true underlying graph using a perfect CI oracle, and accurately\nestimating the graph using limited observed data.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 13:46:01 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2020 11:29:06 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Rohekar", "Raanan Y.", ""], ["Gurwicz", "Yaniv", ""], ["Nisimov", "Shami", ""], ["Novik", "Gal", ""]]}, {"id": "2012.07527", "submitter": "Amir Najafi", "authors": "Armin Karamzade, Amir Najafi and Seyed Abolfazl Motahari", "title": "Regularizing Recurrent Neural Networks via Sequence Mixup", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we extend a class of celebrated regularization techniques\noriginally proposed for feed-forward neural networks, namely Input Mixup (Zhang\net al., 2017) and Manifold Mixup (Verma et al., 2018), to the realm of\nRecurrent Neural Networks (RNN). Our proposed methods are easy to implement and\nhave a low computational complexity, while leverage the performance of simple\nneural architectures in a variety of tasks. We have validated our claims\nthrough several experiments on real-world datasets, and also provide an\nasymptotic theoretical analysis to further investigate the properties and\npotential impacts of our proposed techniques. Applying sequence mixup to\nBiLSTM-CRF model (Huang et al., 2015) to Named Entity Recognition task on\nCoNLL-2003 data (Sang and De Meulder, 2003) has improved the F-1 score on the\ntest stage and reduced the loss, considerably.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 05:43:40 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Karamzade", "Armin", ""], ["Najafi", "Amir", ""], ["Motahari", "Seyed Abolfazl", ""]]}, {"id": "2012.07596", "submitter": "Mariana Da Silva", "authors": "Mariana da Silva, Kara Garcia, Carole H. Sudre, Cher Bass, M. Jorge\n  Cardoso, Emma Robinson", "title": "Biomechanical modelling of brain atrophy through deep learning", "comments": "Submitted to Medical Imaging Meets NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV q-bio.TO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a proof-of-concept, deep learning (DL) based, differentiable\nbiomechanical model of realistic brain deformations. Using prescribed maps of\nlocal atrophy and growth as input, the network learns to deform images\naccording to a Neo-Hookean model of tissue deformation. The tool is validated\nusing longitudinal brain atrophy data from the Alzheimer's Disease Neuroimaging\nInitiative (ADNI) dataset, and we demonstrate that the trained model is capable\nof rapidly simulating new brain deformations with minimal residuals. This\nmethod has the potential to be used in data augmentation or for the exploration\nof different causal hypotheses reflecting brain growth and atrophy.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 14:40:47 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["da Silva", "Mariana", ""], ["Garcia", "Kara", ""], ["Sudre", "Carole H.", ""], ["Bass", "Cher", ""], ["Cardoso", "M. Jorge", ""], ["Robinson", "Emma", ""]]}, {"id": "2012.07621", "submitter": "Ximena Fern\\'andez", "authors": "Eugenio Borghini, Ximena Fern\\'andez, Pablo Groisman, Gabriel Mindlin", "title": "Intrinsic persistent homology via density-based metric learning", "comments": "30 pages. v2: minor corrections, new applications to signal analysis\n  and a new result about robustness to outliers added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.AT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of estimating intrinsic distances in a manifold from a\nfinite sample. We prove that the metric space defined by the sample endowed\nwith a computable metric known as sample Fermat distance converges a.s. in the\nsense of Gromov-Hausdorff. The limiting object is the manifold itself endowed\nwith the population Fermat distance, an intrinsic metric that accounts for both\nthe geometry of the manifold and the density that produces the sample. This\nresult is applied to obtain intrinsic persistence diagrams, which are less\nsensitive to the particular embedding of the manifold in the Euclidean space.\nWe show that this approach is robust to outliers and deduce a method for\npattern recognition in signals, with applications in real data.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 18:54:36 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 17:50:30 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Borghini", "Eugenio", ""], ["Fern\u00e1ndez", "Ximena", ""], ["Groisman", "Pablo", ""], ["Mindlin", "Gabriel", ""]]}, {"id": "2012.07662", "submitter": "Romain Cosentino Mr", "authors": "Romain Cosentino, Randall Balestriero", "title": "Sparse Multi-Family Deep Scattering Network", "comments": "arXiv admin note: substantial text overlap with arXiv:1712.09117", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we propose the Sparse Multi-Family Deep Scattering Network\n(SMF-DSN), a novel architecture exploiting the interpretability of the Deep\nScattering Network (DSN) and improving its expressive power. The DSN extracts\nsalient and interpretable features in signals by cascading wavelet transforms,\ncomplex modulus and extract the representation of the data via a\ntranslation-invariant operator. First, leveraging the development of highly\nspecialized wavelet filters over the last decades, we propose a multi-family\napproach to DSN. In particular, we propose to cross multiple wavelet transforms\nat each layer of the network, thus increasing the feature diversity and\nremoving the need for an expert to select the appropriate filter. Secondly, we\ndevelop an optimal thresholding strategy adequate for the DSN that regularizes\nthe network and controls possible instabilities induced by the signals, such as\nnon-stationary noise. Our systematic and principled solution sparsifies the\nnetwork's latent representation by acting as a local mask distinguishing\nbetween activity and noise. The SMF-DSN enhances the DSN by (i) increasing the\ndiversity of the scattering coefficients and (ii) improves its robustness with\nrespect to non-stationary noise.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 16:06:14 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Cosentino", "Romain", ""], ["Balestriero", "Randall", ""]]}, {"id": "2012.07718", "submitter": "Stefan Klus", "authors": "Jan-Hendrik Niemann, Stefan Klus, Christof Sch\\\"utte", "title": "Data-driven model reduction of agent-based systems using the Koopman\n  generator", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0250970", "report-no": null, "categories": "math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamical behavior of social systems can be described by agent-based\nmodels. Although single agents follow easily explainable rules, complex\ntime-evolving patterns emerge due to their interaction. The simulation and\nanalysis of such agent-based models, however, is often prohibitively\ntime-consuming if the number of agents is large. In this paper, we show how\nKoopman operator theory can be used to derive reduced models of agent-based\nsystems using only simulation data. Our goal is to learn coarse-grained models\nand to represent the reduced dynamics by ordinary or stochastic differential\nequations. The new variables are, for instance, aggregated state variables of\nthe agent-based model, modeling the collective behavior of larger groups or the\nentire population. Using benchmark problems with known coarse-grained models,\nwe demonstrate that the obtained reduced systems are in good agreement with the\nanalytical results, provided that the numbers of agents is sufficiently large.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 17:12:54 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 15:43:49 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Niemann", "Jan-Hendrik", ""], ["Klus", "Stefan", ""], ["Sch\u00fctte", "Christof", ""]]}, {"id": "2012.07729", "submitter": "Ashlynn Daughton", "authors": "Dax Gerts, Courtney D. Shelley, Nidhi Parikh, Travis Pitts, Chrysm\n  Watson Ross, Geoffrey Fairchild, Nidia Yadria Vaquera Chavez, Ashlynn R.\n  Daughton", "title": "\"Thought I'd Share First\" and Other Conspiracy Theory Tweets from the\n  COVID-19 Infodemic: Exploratory Study", "comments": null, "journal-ref": "JMIR Pub Hlth Surv 2021 7(4)", "doi": "10.2196/26527", "report-no": "LA-UR-20-28305", "categories": "cs.SI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background: The COVID-19 outbreak has left many people isolated within their\nhomes; these people are turning to social media for news and social connection,\nwhich leaves them vulnerable to believing and sharing misinformation.\nHealth-related misinformation threatens adherence to public health messaging,\nand monitoring its spread on social media is critical to understanding the\nevolution of ideas that have potentially negative public health impacts.\nResults: Analysis using model-labeled data was beneficial for increasing the\nproportion of data matching misinformation indicators. Random forest classifier\nmetrics varied across the four conspiracy theories considered (F1 scores\nbetween 0.347 and 0.857); this performance increased as the given conspiracy\ntheory was more narrowly defined. We showed that misinformation tweets\ndemonstrate more negative sentiment when compared to nonmisinformation tweets\nand that theories evolve over time, incorporating details from unrelated\nconspiracy theories as well as real-world events. Conclusions: Although we\nfocus here on health-related misinformation, this combination of approaches is\nnot specific to public health and is valuable for characterizing misinformation\nin general, which is an important first step in creating targeted messaging to\ncounteract its spread. Initial messaging should aim to preempt generalized\nmisinformation before it becomes widespread, while later messaging will need to\ntarget evolving conspiracy theories and the new facets of each as they become\nincorporated.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 17:24:59 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 13:56:54 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Gerts", "Dax", ""], ["Shelley", "Courtney D.", ""], ["Parikh", "Nidhi", ""], ["Pitts", "Travis", ""], ["Ross", "Chrysm Watson", ""], ["Fairchild", "Geoffrey", ""], ["Chavez", "Nidia Yadria Vaquera", ""], ["Daughton", "Ashlynn R.", ""]]}, {"id": "2012.07784", "submitter": "Ziyang Ding", "authors": "Ziyang Ding and Sayan Mukherjee", "title": "At the Intersection of Deep Sequential Model Framework and State-space\n  Model Framework: Study on Option Pricing", "comments": "37 pages, 12 figures, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference and forecast problems of the nonlinear dynamical system have arisen\nin a variety of contexts. Reservoir computing and deep sequential models, on\nthe one hand, have demonstrated efficient, robust, and superior performance in\nmodeling simple and chaotic dynamical systems. However, their innate\ndeterministic feature has partially detracted their robustness to noisy system,\nand their inability to offer uncertainty measurement has also been an\ninsufficiency of the framework. On the other hand, the traditional state-space\nmodel framework is robust to noise. It also carries measured uncertainty,\nforming a just-right complement to the reservoir computing and deep sequential\nmodel framework. We propose the unscented reservoir smoother, a model that\nunifies both deep sequential and state-space models to achieve both frameworks'\nsuperiorities. Evaluated in the option pricing setting on top of noisy\ndatasets, URS strikes highly competitive forecasting accuracy, especially those\nof longer-term, and uncertainty measurement. Further extensions and\nimplications on URS are also discussed to generalize a full integration of both\nframeworks.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 18:21:41 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Ding", "Ziyang", ""], ["Mukherjee", "Sayan", ""]]}, {"id": "2012.07785", "submitter": "Dionysios Kalogerias", "authors": "Dionysios S. Kalogerias", "title": "Noisy Linear Convergence of Stochastic Gradient Descent for CV@R\n  Statistical Learning under Polyak-{\\L}ojasiewicz Conditions", "comments": "17 pages, 2 figures. From v2 onwards: Significant updates to the\n  technical content, fixed some errors/nonsense in the results and their proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SP eess.SY math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conditional Value-at-Risk ($\\mathrm{CV@R}$) is one of the most popular\nmeasures of risk, which has been recently considered as a performance criterion\nin supervised statistical learning, as it is related to desirable operational\nfeatures in modern applications, such as safety, fairness, distributional\nrobustness, and prediction error stability. However, due to its variational\ndefinition, $\\mathrm{CV@R}$ is commonly believed to result in difficult\noptimization problems, even for smooth and strongly convex loss functions. We\ndisprove this statement by establishing noisy (i.e., fixed-accuracy) linear\nconvergence of stochastic gradient descent for sequential $\\mathrm{CV@R}$\nlearning, for a large class of not necessarily strongly-convex (or even convex)\nloss functions satisfying a set-restricted Polyak-Lojasiewicz inequality. This\nclass contains all smooth and strongly convex losses, confirming that classical\nproblems, such as linear least squares regression, can be solved efficiently\nunder the $\\mathrm{CV@R}$ criterion, just as their risk-neutral versions. Our\nresults are illustrated numerically on such a risk-aware ridge regression task,\nalso verifying their validity in practice.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 18:22:53 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 17:10:13 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2021 02:17:57 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Kalogerias", "Dionysios S.", ""]]}, {"id": "2012.07844", "submitter": "Stefano Marano", "authors": "Stefano Marano and Ali H. Sayed", "title": "Decision-Making Algorithms for Learning and Adaptation with Application\n  to COVID-19 Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on the development of a new family of decision-making\nalgorithms for adaptation and learning, which are specifically tailored to\ndecision problems and are constructed by building up on first principles from\ndecision theory. A key observation is that estimation and decision problems are\nstructurally different and, therefore, algorithms that have proven successful\nfor the former need not perform well when adjusted for decision problems. We\npropose a new scheme, referred to as BLLR (barrier log-likelihood ratio\nalgorithm) and demonstrate its applicability to real-data from the COVID-19\npandemic in Italy. The results illustrate the ability of the design tool to\ntrack the different phases of the outbreak.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 18:24:45 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Marano", "Stefano", ""], ["Sayed", "Ali H.", ""]]}, {"id": "2012.07969", "submitter": "Mihaela Rosca", "authors": "Mihaela Rosca, Theophane Weber, Arthur Gretton, Shakir Mohamed", "title": "A case for new neural network smoothness constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How sensitive should machine learning models be to input changes? We tackle\nthe question of model smoothness and show that it is a useful inductive bias\nwhich aids generalization, adversarial robustness, generative modeling and\nreinforcement learning. We explore current methods of imposing smoothness\nconstraints and observe they lack the flexibility to adapt to new tasks, they\ndon't account for data modalities, they interact with losses, architectures and\noptimization in ways not yet fully understood. We conclude that new advances in\nthe field are hinging on finding ways to incorporate data, tasks and learning\ninto our definitions of smoothness.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 22:07:32 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 21:32:28 GMT"}, {"version": "v3", "created": "Wed, 7 Jul 2021 07:37:59 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Rosca", "Mihaela", ""], ["Weber", "Theophane", ""], ["Gretton", "Arthur", ""], ["Mohamed", "Shakir", ""]]}, {"id": "2012.07976", "submitter": "Scott Yak", "authors": "Yiding Jiang (1), Pierre Foret (1), Scott Yak (1), Daniel M. Roy (2),\n  Hossein Mobahi (1), Gintare Karolina Dziugaite (3), Samy Bengio (1), Suriya\n  Gunasekar (4), Isabelle Guyon (5), Behnam Neyshabur (1) ((1) Google Research,\n  (2) University of Toronto, (3) Element AI, (4) Microsoft Research, (5)\n  University Paris-Saclay and ChaLearn)", "title": "NeurIPS 2020 Competition: Predicting Generalization in Deep Learning", "comments": "20 pages, 2 figures. Accepted for NeurIPS 2020 Competitions Track.\n  Lead organizer: Yiding Jiang", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding generalization in deep learning is arguably one of the most\nimportant questions in deep learning. Deep learning has been successfully\nadopted to a large number of problems ranging from pattern recognition to\ncomplex decision making, but many recent researchers have raised many concerns\nabout deep learning, among which the most important is generalization. Despite\nnumerous attempts, conventional statistical learning approaches have yet been\nable to provide a satisfactory explanation on why deep learning works. A recent\nline of works aims to address the problem by trying to predict the\ngeneralization performance through complexity measures. In this competition, we\ninvite the community to propose complexity measures that can accurately predict\ngeneralization of models. A robust and general complexity measure would\npotentially lead to a better understanding of deep learning's underlying\nmechanism and behavior of deep models on unseen data, or shed light on better\ngeneralization bounds. All these outcomes will be important for making deep\nlearning more robust and reliable.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 22:21:37 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Jiang", "Yiding", ""], ["Foret", "Pierre", ""], ["Yak", "Scott", ""], ["Roy", "Daniel M.", ""], ["Mobahi", "Hossein", ""], ["Dziugaite", "Gintare Karolina", ""], ["Bengio", "Samy", ""], ["Gunasekar", "Suriya", ""], ["Guyon", "Isabelle", ""], ["Neyshabur", "Behnam", ""]]}, {"id": "2012.08015", "submitter": "Annie Sauer", "authors": "Annie Sauer, Robert B. Gramacy, David Higdon", "title": "Active Learning for Deep Gaussian Process Surrogates", "comments": "34 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Gaussian processes (DGPs) are increasingly popular as predictive models\nin machine learning (ML) for their non-stationary flexibility and ability to\ncope with abrupt regime changes in training data. Here we explore DGPs as\nsurrogates for computer simulation experiments whose response surfaces exhibit\nsimilar characteristics. In particular, we transport a DGP's automatic warping\nof the input space and full uncertainty quantification (UQ), via a novel\nelliptical slice sampling (ESS) Bayesian posterior inferential scheme, through\nto active learning (AL) strategies that distribute runs non-uniformly in the\ninput space -- something an ordinary (stationary) GP could not do. Building up\nthe design sequentially in this way allows smaller training sets, limiting both\nexpensive evaluation of the simulator code and mitigating cubic costs of DGP\ninference. When training data sizes are kept small through careful acquisition,\nand with parsimonious layout of latent layers, the framework can be both\neffective and computationally tractable. Our methods are illustrated on\nsimulation data and two real computer experiments of varying input\ndimensionality. We provide an open source implementation in the \"deepgp\"\npackage on CRAN.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 00:09:37 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Sauer", "Annie", ""], ["Gramacy", "Robert B.", ""], ["Higdon", "David", ""]]}, {"id": "2012.08036", "submitter": "Marius Hofert", "authors": "Marius Hofert, Avinash Prasad, Mu Zhu", "title": "Applications of multivariate quasi-random sampling with neural networks", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative moment matching networks (GMMNs) are suggested for modeling the\ncross-sectional dependence between stochastic processes. The stochastic\nprocesses considered are geometric Brownian motions and ARMA-GARCH models.\nGeometric Brownian motions lead to an application of pricing American basket\ncall options under dependence and ARMA-GARCH models lead to an application of\nsimulating predictive distributions. In both types of applications the benefit\nof using GMMNs in comparison to parametric dependence models is highlighted and\nthe fact that GMMNs can produce dependent quasi-random samples with no\nadditional effort is exploited to obtain variance reduction.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 01:42:23 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Hofert", "Marius", ""], ["Prasad", "Avinash", ""], ["Zhu", "Mu", ""]]}, {"id": "2012.08037", "submitter": "Takayuki Osogami Ph.D.", "authors": "Takayuki Osogami", "title": "Proofs and additional experiments on Second order techniques for\n  learning time-series with structural breaks", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide complete proofs of the lemmas about the properties of the\nregularized loss function that is used in the second order techniques for\nlearning time-series with structural breaks in Osogami (2021). In addition, we\nshow experimental results that support the validity of the techniques.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 01:44:21 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 10:57:20 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Osogami", "Takayuki", ""]]}, {"id": "2012.08073", "submitter": "Subhojyoti Mukherjee", "authors": "Subhojyoti Mukherjee, Ardhendu Tripathy, Robert Nowak", "title": "Generalized Chernoff Sampling for Active Testing, Active Regression and\n  Structured Bandit Algorithms", "comments": "46 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Active learning and structured stochastic bandit problems are intimately\nrelated to the classical problem of sequential experimental design. This paper\nstudies active learning and best-arm identification in structured bandit\nsettings from the viewpoint of active sequential hypothesis testing, a\nframework initiated by Chernoff (1959). We obtain a novel sample complexity\nbound for Chernoff's original active testing procedure by uncovering\nnon-asymptotic terms that reduce in significance as the allowed error\nprobability $\\delta \\rightarrow 0$. Initially proposed for testing among\nfinitely many hypotheses, we obtain the analogue of Chernoff sampling for the\ncase when the hypotheses belong to a compact space. This allows us to directly\napply it to active learning and structured bandit problems, where the unknown\nparameter specifying the arm means is often assumed to be an element of\nEuclidean space. Empirically, we demonstrate the potential of our proposed\napproach for active learning of neural network models and in linear and\nnon-linear bandit settings, where we observe that our general-purpose approach\ncompares favorably to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 03:44:18 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 09:26:45 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Mukherjee", "Subhojyoti", ""], ["Tripathy", "Ardhendu", ""], ["Nowak", "Robert", ""]]}, {"id": "2012.08101", "submitter": "Aodong Li", "authors": "Aodong Li, Alex Boyd, Padhraic Smyth, Stephan Mandt", "title": "Variational Beam Search for Learning with Distribution Shifts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of online learning in the presence of sudden\ndistribution shifts as frequently encountered in applications such as\nautonomous navigation. Distribution shifts require constant performance\nmonitoring and re-training. They may also be hard to detect and can lead to a\nslow but steady degradation in model performance. To address this problem we\npropose a new Bayesian meta-algorithm that can both (i) make inferences about\nsubtle distribution shifts based on minimal sequential observations and (ii)\naccordingly adapt a model in an online fashion. The approach uses beam search\nover multiple change point hypotheses to perform inference on a hierarchical\nsequential latent variable modeling framework. Our proposed approach is\nmodel-agnostic, applicable to both supervised and unsupervised learning, and\nyields significant improvements over state-of-the-art Bayesian online learning\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 05:28:47 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 23:14:07 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Li", "Aodong", ""], ["Boyd", "Alex", ""], ["Smyth", "Padhraic", ""], ["Mandt", "Stephan", ""]]}, {"id": "2012.08125", "submitter": "Ruiqi Gao", "authors": "Ruiqi Gao, Yang Song, Ben Poole, Ying Nian Wu, Diederik P. Kingma", "title": "Learning Energy-Based Models by Diffusion Recovery Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While energy-based models (EBMs) exhibit a number of desirable properties,\ntraining and sampling on high-dimensional datasets remains challenging.\nInspired by recent progress on diffusion probabilistic models, we present a\ndiffusion recovery likelihood method to tractably learn and sample from a\nsequence of EBMs trained on increasingly noisy versions of a dataset. Each EBM\nis trained with recovery likelihood, which maximizes the conditional\nprobability of the data at a certain noise level given their noisy versions at\na higher noise level. Optimizing recovery likelihood is more tractable than\nmarginal likelihood, as sampling from the conditional distributions is much\neasier than sampling from the marginal distributions. After training,\nsynthesized images can be generated by the sampling process that initializes\nfrom Gaussian white noise distribution and progressively samples the\nconditional distributions at decreasingly lower noise levels. Our method\ngenerates high fidelity samples on various image datasets. On unconditional\nCIFAR-10 our method achieves FID 9.58 and inception score 8.30, superior to the\nmajority of GANs. Moreover, we demonstrate that unlike previous work on EBMs,\nour long-run MCMC samples from the conditional distributions do not diverge and\nstill represent realistic images, allowing us to accurately estimate the\nnormalized density of data even for high-dimensional datasets. Our\nimplementation is available at https://github.com/ruiqigao/recovery_likelihood.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 07:09:02 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 06:35:56 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Gao", "Ruiqi", ""], ["Song", "Yang", ""], ["Poole", "Ben", ""], ["Wu", "Ying Nian", ""], ["Kingma", "Diederik P.", ""]]}, {"id": "2012.08154", "submitter": "Ludvig Hult", "authors": "Ludvig Hult and Dave Zachariah", "title": "Inference of Causal Effects when Control Variables are Unknown", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional methods in causal effect inferencetypically rely on specifying a\nvalid set of control variables. When this set is unknown or misspecified,\ninferences will be erroneous. We propose a method for inferring average causal\neffects when all potential confounders are observed, but thecontrol variables\nare unknown. When the data-generating process belongs to the class of acyclical\nlinear structural causal models, we prove that themethod yields asymptotically\nvalid confidence intervals. Our results build upon a smooth characterization of\nlinear directed acyclic graphs. We verify the capability of the method to\nproduce valid confidence intervals for average causal effects using synthetic\ndata, even when the appropriate specification of control variables is unknown.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 08:54:26 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 14:59:07 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 14:39:52 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Hult", "Ludvig", ""], ["Zachariah", "Dave", ""]]}, {"id": "2012.08180", "submitter": "Noor Awad", "authors": "Noor Awad, Gresa Shala, Difan Deng, Neeratyoy Mallik, Matthias Feurer,\n  Katharina Eggensperger, Andre' Biedenkapp, Diederick Vermetten, Hao Wang,\n  Carola Doerr, Marius Lindauer, Frank Hutter", "title": "Squirrel: A Switching Hyperparameter Optimizer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this short note, we describe our submission to the NeurIPS 2020 BBO\nchallenge. Motivated by the fact that different optimizers work well on\ndifferent problems, our approach switches between different optimizers. Since\nthe team names on the competition's leaderboard were randomly generated\n\"alliteration nicknames\", consisting of an adjective and an animal with the\nsame initial letter, we called our approach the Switching Squirrel, or here,\nshort, Squirrel.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 09:57:08 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 06:56:03 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Awad", "Noor", ""], ["Shala", "Gresa", ""], ["Deng", "Difan", ""], ["Mallik", "Neeratyoy", ""], ["Feurer", "Matthias", ""], ["Eggensperger", "Katharina", ""], ["Biedenkapp", "Andre'", ""], ["Vermetten", "Diederick", ""], ["Wang", "Hao", ""], ["Doerr", "Carola", ""], ["Lindauer", "Marius", ""], ["Hutter", "Frank", ""]]}, {"id": "2012.08196", "submitter": "Aijun Zhang", "authors": "Yifeng Guo, Yu Su, Zebin Yang and Aijun Zhang", "title": "Explainable Recommendation Systems by Generalized Additive Models with\n  Manifest and Latent Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, the field of recommendation systems has attracted increasing\nattention to developing predictive models that provide explanations of why an\nitem is recommended to a user. The explanations can be either obtained by\npost-hoc diagnostics after fitting a relatively complex model or embedded into\nan intrinsically interpretable model. In this paper, we propose the explainable\nrecommendation systems based on a generalized additive model with manifest and\nlatent interactions (GAMMLI). This model architecture is intrinsically\ninterpretable, as it additively consists of the user and item main effects, the\nmanifest user-item interactions based on observed features, and the latent\ninteraction effects from residuals. Unlike conventional collaborative filtering\nmethods, the group effect of users and items are considered in GAMMLI. It is\nbeneficial for enhancing the model interpretability, and can also facilitate\nthe cold-start recommendation problem. A new Python package GAMMLI is developed\nfor efficient model training and visualized interpretation of the results. By\nnumerical experiments based on simulation data and real-world cases, the\nproposed method is shown to have advantages in both predictive performance and\nexplainable recommendation.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 10:29:12 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Guo", "Yifeng", ""], ["Su", "Yu", ""], ["Yang", "Zebin", ""], ["Zhang", "Aijun", ""]]}, {"id": "2012.08202", "submitter": "Nathanael Bosch", "authors": "Nathanael Bosch, Philipp Hennig, Filip Tronarp", "title": "Calibrated Adaptive Probabilistic ODE Solvers", "comments": "17 pages, 10 figures;", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Probabilistic solvers for ordinary differential equations assign a posterior\nmeasure to the solution of an initial value problem. The joint covariance of\nthis distribution provides an estimate of the (global) approximation error. The\ncontraction rate of this error estimate as a function of the solver's step size\nidentifies it as a well-calibrated worst-case error, but its explicit numerical\nvalue for a certain step size is not automatically a good estimate of the\nexplicit error. Addressing this issue, we introduce, discuss, and assess\nseveral probabilistically motivated ways to calibrate the uncertainty estimate.\nNumerical experiments demonstrate that these calibration methods interact\nefficiently with adaptive step-size selection, resulting in descriptive, and\nefficiently computable posteriors. We demonstrate the efficiency of the\nmethodology by benchmarking against the classic, widely used Dormand-Prince 4/5\nRunge-Kutta method.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 10:48:55 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 10:48:28 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Bosch", "Nathanael", ""], ["Hennig", "Philipp", ""], ["Tronarp", "Filip", ""]]}, {"id": "2012.08225", "submitter": "Alberto Maria Metelli", "authors": "Alberto Maria Metelli, Matteo Papini, Pierluca D'Oro, and Marcello\n  Restelli", "title": "Policy Optimization as Online Learning with Mediator Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy Optimization (PO) is a widely used approach to address continuous\ncontrol tasks. In this paper, we introduce the notion of mediator feedback that\nframes PO as an online learning problem over the policy space. The additional\navailable information, compared to the standard bandit feedback, allows reusing\nsamples generated by one policy to estimate the performance of other policies.\nBased on this observation, we propose an algorithm, RANDomized-exploration\npolicy Optimization via Multiple Importance Sampling with Truncation\n(RANDOMIST), for regret minimization in PO, that employs a randomized\nexploration strategy, differently from the existing optimistic approaches. When\nthe policy space is finite, we show that under certain circumstances, it is\npossible to achieve constant regret, while always enjoying logarithmic regret.\nWe also derive problem-dependent regret lower bounds. Then, we extend RANDOMIST\nto compact policy spaces. Finally, we provide numerical simulations on finite\nand compact policy spaces, in comparison with PO and bandit baselines.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 11:34:29 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Metelli", "Alberto Maria", ""], ["Papini", "Matteo", ""], ["D'Oro", "Pierluca", ""], ["Restelli", "Marcello", ""]]}, {"id": "2012.08234", "submitter": "Ignacio Peis", "authors": "Ignacio Peis, Pablo M. Olmos and Antonio Art\\'es-Rodr\\'iguez", "title": "Unsupervised Learning of Global Factors in Deep Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel deep generative model based on non i.i.d. variational\nautoencoders that captures global dependencies among observations in a fully\nunsupervised fashion. In contrast to the recent semi-supervised alternatives\nfor global modeling in deep generative models, our approach combines a mixture\nmodel in the local or data-dependent space and a global Gaussian latent\nvariable, which lead us to obtain three particular insights. First, the induced\nlatent global space captures interpretable disentangled representations with no\nuser-defined regularization in the evidence lower bound (as in $\\beta$-VAE and\nits generalizations). Second, we show that the model performs domain alignment\nto find correlations and interpolate between different databases. Finally, we\nstudy the ability of the global space to discriminate between groups of\nobservations with non-trivial underlying structures, such as face images with\nshared attributes or defined sequences of digits images.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 11:55:31 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 09:15:24 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Peis", "Ignacio", ""], ["Olmos", "Pablo M.", ""], ["Art\u00e9s-Rodr\u00edguez", "Antonio", ""]]}, {"id": "2012.08240", "submitter": "Haitham Bou Ammar PhD", "authors": "Antoine Grosnit, Alexander I. Cowen-Rivers, Rasul Tutunov, Ryan-Rhys\n  Griffiths, Jun Wang, Haitham Bou-Ammar", "title": "Are we Forgetting about Compositional Optimisers in Bayesian\n  Optimisation?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimisation presents a sample-efficient methodology for global\noptimisation. Within this framework, a crucial performance-determining\nsubroutine is the maximisation of the acquisition function, a task complicated\nby the fact that acquisition functions tend to be non-convex and thus\nnontrivial to optimise. In this paper, we undertake a comprehensive empirical\nstudy of approaches to maximise the acquisition function. Additionally, by\nderiving novel, yet mathematically equivalent, compositional forms for popular\nacquisition functions, we recast the maximisation task as a compositional\noptimisation problem, allowing us to benefit from the extensive literature in\nthis field. We highlight the empirical advantages of the compositional approach\nto acquisition function maximisation across 3958 individual experiments\ncomprising synthetic optimisation tasks as well as tasks from Bayesmark. Given\nthe generality of the acquisition function maximisation subroutine, we posit\nthat the adoption of compositional optimisers has the potential to yield\nperformance improvements across all domains in which Bayesian optimisation is\ncurrently being applied.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 12:18:38 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 12:20:16 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Grosnit", "Antoine", ""], ["Cowen-Rivers", "Alexander I.", ""], ["Tutunov", "Rasul", ""], ["Griffiths", "Ryan-Rhys", ""], ["Wang", "Jun", ""], ["Bou-Ammar", "Haitham", ""]]}, {"id": "2012.08291", "submitter": "Benny Avelin", "authors": "Benny Avelin and Vesa Julin", "title": "Approximation of BV functions by neural networks: A regularity theory\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we are concerned with the approximation of functions by single\nhidden layer neural networks with ReLU activation functions on the unit circle.\nIn particular, we are interested in the case when the number of data-points\nexceeds the number of nodes. We first study the convergence to equilibrium of\nthe stochastic gradient flow associated with the cost function with a quadratic\npenalization. Specifically, we prove a Poincar\\'e inequality for a penalized\nversion of the cost function with explicit constants that are independent of\nthe data and of the number of nodes. As our penalization biases the weights to\nbe bounded, this leads us to study how well a network with bounded weights can\napproximate a given function of bounded variation (BV).\n  Our main contribution concerning approximation of BV functions, is a result\nwhich we call the localization theorem. Specifically, it states that the\nexpected error of the constrained problem, where the length of the weights are\nless than $R$, is of order $R^{-1/9}$ with respect to the unconstrained problem\n(the global optimum). The proof is novel in this topic and is inspired by\ntechniques from regularity theory of elliptic partial differential equations.\nFinally we quantify the expected value of the global optimum by proving a\nquantitative version of the universal approximation theorem.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 13:58:44 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 13:35:56 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Avelin", "Benny", ""], ["Julin", "Vesa", ""]]}, {"id": "2012.08420", "submitter": "Mark Grobman", "authors": "Shachar Gluska and Mark Grobman", "title": "Exploring Neural Networks Quantization via Layer-Wise Quantization\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantization is an essential step in the efficient deployment of deep\nlearning models and as such is an increasingly popular research topic. An\nimportant practical aspect that is not addressed in the current literature is\nhow to analyze and fix fail cases where the use of quantization results in\nexcessive degradation. In this paper, we present a simple analytic framework\nthat breaks down overall degradation to its per layer contributions. We analyze\nmany common networks and observe that a layer's contribution is determined by\nboth intrinsic (local) factors - the distribution of the layer's weights and\nactivations - and extrinsic (global) factors having to do with the the\ninteraction with the rest of the layers. Layer-wise analysis of existing\nquantization schemes reveals local fail-cases of existing techniques which are\nnot reflected when inspecting their overall performance. As an example, we\nconsider ResNext26 on which SoTA post-training quantization methods perform\npoorly. We show that almost all of the degradation stems from a single layer.\nThe same analysis also allows for local fixes - applying a common weight\nclipping heuristic only to this layer reduces degradation to a minimum while\napplying the same heuristic globally results in high degradation. More\ngenerally, layer-wise analysis allows for a more nuanced examination of how\nquantization affects the network, enabling the design of better performing\nschemes.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 16:57:53 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Gluska", "Shachar", ""], ["Grobman", "Mark", ""]]}, {"id": "2012.08439", "submitter": "John Anih", "authors": "Tochukwu John Anih, Chika Amadi Bede, and Chima Festus Umeokpala", "title": "Detection of Anomalies in a Time Series Data using InfluxDB and Python", "comments": "12 pages, 9 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Analysis of water and environmental data is an important aspect of many\nintelligent water and environmental system applications where inference from\nsuch analysis plays a significant role in decision making. Quite often these\ndata that are collected through sensible sensors can be anomalous due to\ndifferent reasons such as systems breakdown, malfunctioning of sensor\ndetectors, and more. Regardless of their root causes, such data severely affect\nthe results of the subsequent analysis. This paper demonstrates data cleaning\nand preparation for time-series data and further proposes cost-sensitive\nmachine learning algorithms as a solution to detect anomalous data points in\ntime-series data. The following models: Logistic Regression, Random Forest,\nSupport Vector Machines have been modified to support the cost-sensitive\nlearning which penalizes misclassified samples thereby minimizing the total\nmisclassification cost. Our results showed that Random Forest outperformed the\nrest of the models at predicting the positive class (i.e anomalies). Applying\npredictive model improvement techniques like data oversampling seems to provide\nlittle or no improvement to the Random Forest model. Interestingly, with\nrecursive feature elimination, we achieved a better model performance thereby\nreducing the dimensions in the data. Finally, with Influxdb and Kapacitor the\ndata was ingested and streamed to generate new data points to further evaluate\nthe model performance on unseen data, this will allow for early recognition of\nundesirable changes in the drinking water quality and will enable the water\nsupply companies to rectify on a timely basis whatever undesirable changes\nabound.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 17:27:39 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Anih", "Tochukwu John", ""], ["Bede", "Chika Amadi", ""], ["Umeokpala", "Chima Festus", ""]]}, {"id": "2012.08456", "submitter": "Roberto Calandra", "authors": "Shaoxiong Wang, Mike Lambeta, Po-Wei Chou, Roberto Calandra", "title": "TACTO: A Fast, Flexible and Open-source Simulator for High-Resolution\n  Vision-based Tactile Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulators perform an important role in prototyping, debugging and\nbenchmarking new advances in robotics and learning for control. Although many\nphysics engines exist, some aspects of the real-world are harder than others to\nsimulate. One of the aspects that have so far eluded accurate simulation is\ntouch sensing. To address this gap, we present TACTO -- a fast, flexible and\nopen-source simulator for vision-based tactile sensors. This simulator allows\nto render realistic high-resolution touch readings at hundreds of frames per\nsecond, and can be easily configured to simulate different vision-based tactile\nsensors, including GelSight, DIGIT and OmniTact. In this paper, we detail the\nprinciples that drove the implementation of TACTO and how they are reflected in\nits architecture. We demonstrate TACTO on a perceptual task, by learning to\npredict grasp stability using touch from 1 million grasps, and on a marble\nmanipulation control task. We believe that TACTO is a step towards the\nwidespread adoption of touch sensing in robotic applications, and to enable\nmachine learning practitioners interested in multi-modal learning and control.\nTACTO is open-source at https://github.com/facebookresearch/tacto.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 17:54:07 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Wang", "Shaoxiong", ""], ["Lambeta", "Mike", ""], ["Chou", "Po-Wei", ""], ["Calandra", "Roberto", ""]]}, {"id": "2012.08466", "submitter": "Dmitrii Avdiukhin", "authors": "Stanislav Naumov, Grigory Yaroslavtsev, Dmitrii Avdiukhin", "title": "Objective-Based Hierarchical Clustering of Deep Embedding Vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We initiate a comprehensive experimental study of objective-based\nhierarchical clustering methods on massive datasets consisting of deep\nembedding vectors from computer vision and NLP applications. This includes a\nlarge variety of image embedding (ImageNet, ImageNetV2, NaBirds), word\nembedding (Twitter, Wikipedia), and sentence embedding (SST-2) vectors from\nseveral popular recent models (e.g. ResNet, ResNext, Inception V3, SBERT). Our\nstudy includes datasets with up to $4.5$ million entries with embedding\ndimensions up to $2048$.\n  In order to address the challenge of scaling up hierarchical clustering to\nsuch large datasets we propose a new practical hierarchical clustering\nalgorithm B++&C. It gives a 5%/20% improvement on average for the popular\nMoseley-Wang (MW) / Cohen-Addad et al. (CKMM) objectives (normalized) compared\nto a wide range of classic methods and recent heuristics. We also introduce a\ntheoretical algorithm B2SAT&C which achieves a $0.74$-approximation for the\nCKMM objective in polynomial time. This is the first substantial improvement\nover the trivial $2/3$-approximation achieved by a random binary tree. Prior to\nthis work, the best poly-time approximation of $\\approx 2/3 + 0.0004$ was due\nto Charikar et al. (SODA'19).\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 18:08:34 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Naumov", "Stanislav", ""], ["Yaroslavtsev", "Grigory", ""], ["Avdiukhin", "Dmitrii", ""]]}, {"id": "2012.08489", "submitter": "Valerio Perrone", "authors": "Valerio Perrone, Huibin Shen, Aida Zolic, Iaroslav Shcherbatyi, Amr\n  Ahmed, Tanya Bansal, Michele Donini, Fela Winkelmolen, Rodolphe Jenatton,\n  Jean Baptiste Faddoul, Barbara Pogorzelska, Miroslav Miladinovic, Krishnaram\n  Kenthapadi, Matthias Seeger, C\\'edric Archambeau", "title": "Amazon SageMaker Automatic Model Tuning: Scalable Gradient-Free\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tuning complex machine learning systems is challenging. Machine learning\ntypically requires to set hyperparameters, be it regularization, architecture,\nor optimization parameters, whose tuning is critical to achieve good predictive\nperformance. To democratize access to machine learning systems, it is essential\nto automate the tuning. This paper presents Amazon SageMaker Automatic Model\nTuning (AMT), a fully managed system for gradient-free optimization at scale.\nAMT finds the best version of a trained machine learning model by repeatedly\nevaluating it with different hyperparameter configurations. It leverages either\nrandom search or Bayesian optimization to choose the hyperparameter values\nresulting in the best model, as measured by the metric chosen by the user. AMT\ncan be used with built-in algorithms, custom algorithms, and Amazon SageMaker\npre-built containers for machine learning frameworks. We discuss the core\nfunctionality, system architecture, our design principles, and lessons learned.\nWe also describe more advanced features of AMT, such as automated early\nstopping and warm-starting, showing in experiments their benefits to users.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 18:34:34 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 19:41:09 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Perrone", "Valerio", ""], ["Shen", "Huibin", ""], ["Zolic", "Aida", ""], ["Shcherbatyi", "Iaroslav", ""], ["Ahmed", "Amr", ""], ["Bansal", "Tanya", ""], ["Donini", "Michele", ""], ["Winkelmolen", "Fela", ""], ["Jenatton", "Rodolphe", ""], ["Faddoul", "Jean Baptiste", ""], ["Pogorzelska", "Barbara", ""], ["Miladinovic", "Miroslav", ""], ["Kenthapadi", "Krishnaram", ""], ["Seeger", "Matthias", ""], ["Archambeau", "C\u00e9dric", ""]]}, {"id": "2012.08496", "submitter": "Yuxin Chen", "authors": "Yuxin Chen, Yuejie Chi, Jianqing Fan, Cong Ma", "title": "Spectral Methods for Data Science: A Statistical Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG eess.SP math.IT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spectral methods have emerged as a simple yet surprisingly effective approach\nfor extracting information from massive, noisy and incomplete data. In a\nnutshell, spectral methods refer to a collection of algorithms built upon the\neigenvalues (resp. singular values) and eigenvectors (resp. singular vectors)\nof some properly designed matrices constructed from data. A diverse array of\napplications have been found in machine learning, data science, and signal\nprocessing. Due to their simplicity and effectiveness, spectral methods are not\nonly used as a stand-alone estimator, but also frequently employed to\ninitialize other more sophisticated algorithms to improve performance.\n  While the studies of spectral methods can be traced back to classical matrix\nperturbation theory and methods of moments, the past decade has witnessed\ntremendous theoretical advances in demystifying their efficacy through the lens\nof statistical modeling, with the aid of non-asymptotic random matrix theory.\nThis monograph aims to present a systematic, comprehensive, yet accessible\nintroduction to spectral methods from a modern statistical perspective,\nhighlighting their algorithmic implications in diverse large-scale\napplications. In particular, our exposition gravitates around several central\nquestions that span various applications: how to characterize the sample\nefficiency of spectral methods in reaching a target level of statistical\naccuracy, and how to assess their stability in the face of random noise,\nmissing data, and adversarial corruptions? In addition to conventional $\\ell_2$\nperturbation analysis, we present a systematic $\\ell_{\\infty}$ and\n$\\ell_{2,\\infty}$ perturbation theory for eigenspace and singular subspaces,\nwhich has only recently become available owing to a powerful \"leave-one-out\"\nanalysis framework.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 18:40:56 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Chen", "Yuxin", ""], ["Chi", "Yuejie", ""], ["Fan", "Jianqing", ""], ["Ma", "Cong", ""]]}, {"id": "2012.08507", "submitter": "Quanquan Gu", "authors": "Dongruo Zhou and Quanquan Gu and Csaba Szepesvari", "title": "Nearly Minimax Optimal Reinforcement Learning for Linear Mixture Markov\n  Decision Processes", "comments": "59 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study reinforcement learning (RL) with linear function approximation where\nthe underlying transition probability kernel of the Markov decision process\n(MDP) is a linear mixture model (Jia et al., 2020; Ayoub et al., 2020; Zhou et\nal., 2020) and the learning agent has access to either an integration or a\nsampling oracle of the individual basis kernels. We propose a new\nBernstein-type concentration inequality for self-normalized martingales for\nlinear bandit problems with bounded noise. Based on the new inequality, we\npropose a new, computationally efficient algorithm with linear function\napproximation named $\\text{UCRL-VTR}^{+}$ for the aforementioned linear mixture\nMDPs in the episodic undiscounted setting. We show that $\\text{UCRL-VTR}^{+}$\nattains an $\\tilde O(dH\\sqrt{T})$ regret where $d$ is the dimension of feature\nmapping, $H$ is the length of the episode and $T$ is the number of interactions\nwith the MDP. We also prove a matching lower bound $\\Omega(dH\\sqrt{T})$ for\nthis setting, which shows that $\\text{UCRL-VTR}^{+}$ is minimax optimal up to\nlogarithmic factors. In addition, we propose the $\\text{UCLK}^{+}$ algorithm\nfor the same family of MDPs under discounting and show that it attains an\n$\\tilde O(d\\sqrt{T}/(1-\\gamma)^{1.5})$ regret, where $\\gamma\\in [0,1)$ is the\ndiscount factor. Our upper bound matches the lower bound\n$\\Omega(d\\sqrt{T}/(1-\\gamma)^{1.5})$ proved by Zhou et al. (2020) up to\nlogarithmic factors, suggesting that $\\text{UCLK}^{+}$ is nearly minimax\noptimal. To the best of our knowledge, these are the first computationally\nefficient, nearly minimax optimal algorithms for RL with linear function\napproximation.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 18:56:46 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 18:57:11 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Zhou", "Dongruo", ""], ["Gu", "Quanquan", ""], ["Szepesvari", "Csaba", ""]]}, {"id": "2012.08560", "submitter": "Victor Blanco", "authors": "V\\'ictor Blanco and Alberto Jap\\'on and Justo Puerto", "title": "Robust Optimal Classification Trees under Noisy Labels", "comments": "18 Pages, 2 Figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we propose a novel methodology to construct Optimal\nClassification Trees that takes into account that noisy labels may occur in the\ntraining sample. Our approach rests on two main elements: (1) the splitting\nrules for the classification trees are designed to maximize the separation\nmargin between classes applying the paradigm of SVM; and (2) some of the labels\nof the training sample are allowed to be changed during the construction of the\ntree trying to detect the label noise. Both features are considered and\nintegrated together to design the resulting Optimal Classification Tree. We\npresent a Mixed Integer Non Linear Programming formulation for the problem,\nsuitable to be solved using any of the available off-the-shelf solvers. The\nmodel is analyzed and tested on a battery of standard datasets taken from UCI\nMachine Learning repository, showing the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 19:12:29 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Blanco", "V\u00edctor", ""], ["Jap\u00f3n", "Alberto", ""], ["Puerto", "Justo", ""]]}, {"id": "2012.08565", "submitter": "Michael Zhang", "authors": "Michael Zhang, Karan Sapra, Sanja Fidler, Serena Yeung and Jose M.\n  Alvarez", "title": "Personalized Federated Learning with First Order Model Optimization", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While federated learning traditionally aims to train a single global model\nacross decentralized local datasets, one model may not always be ideal for all\nparticipating clients. Here we propose an alternative, where each client only\nfederates with other relevant clients to obtain a stronger model per\nclient-specific objectives. To achieve this personalization, rather than\ncomputing a single model average with constant weights for the entire\nfederation as in traditional FL, we efficiently calculate optimal weighted\nmodel combinations for each client, based on figuring out how much a client can\nbenefit from another's model. We do not assume knowledge of any underlying data\ndistributions or client similarities, and allow each client to optimize for\narbitrary target distributions of interest, enabling greater flexibility for\npersonalization. We evaluate and characterize our method on a variety of\nfederated settings, datasets, and degrees of local data heterogeneity. Our\nmethod outperforms existing alternatives, while also enabling new features for\npersonalized FL such as transfer outside of local data distributions.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 19:30:29 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 06:58:37 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2021 01:39:36 GMT"}, {"version": "v4", "created": "Fri, 26 Mar 2021 23:13:26 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhang", "Michael", ""], ["Sapra", "Karan", ""], ["Fidler", "Sanja", ""], ["Yeung", "Serena", ""], ["Alvarez", "Jose M.", ""]]}, {"id": "2012.08621", "submitter": "Tianjun Zhang", "authors": "Tianjun Zhang, Huazhe Xu, Xiaolong Wang, Yi Wu, Kurt Keutzer, Joseph\n  E. Gonzalez, Yuandong Tian", "title": "BeBold: Exploration Beyond the Boundary of Explored Regions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient exploration under sparse rewards remains a key challenge in deep\nreinforcement learning. To guide exploration, previous work makes extensive use\nof intrinsic reward (IR). There are many heuristics for IR, including\nvisitation counts, curiosity, and state-difference. In this paper, we analyze\nthe pros and cons of each method and propose the regulated difference of\ninverse visitation counts as a simple but effective criterion for IR. The\ncriterion helps the agent explore Beyond the Boundary of explored regions and\nmitigates common issues in count-based methods, such as short-sightedness and\ndetachment. The resulting method, BeBold, solves the 12 most challenging\nprocedurally-generated tasks in MiniGrid with just 120M environment steps,\nwithout any curriculum learning. In comparison, the previous SoTA only solves\n50% of the tasks. BeBold also achieves SoTA on multiple tasks in NetHack, a\npopular rogue-like game that contains more challenging procedurally-generated\nenvironments.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 21:26:54 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Zhang", "Tianjun", ""], ["Xu", "Huazhe", ""], ["Wang", "Xiaolong", ""], ["Wu", "Yi", ""], ["Keutzer", "Kurt", ""], ["Gonzalez", "Joseph E.", ""], ["Tian", "Yuandong", ""]]}, {"id": "2012.08648", "submitter": "Kirthevasan Kandasamy", "authors": "Kirthevasan Kandasamy, Gur-Eyal Sela, Joseph E Gonzalez, Michael I\n  Jordan, Ion Stoica", "title": "Online Learning Demands in Max-min Fairness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe mechanisms for the allocation of a scarce resource among multiple\nusers in a way that is efficient, fair, and strategy-proof, but when users do\nnot know their resource requirements. The mechanism is repeated for multiple\nrounds and a user's requirements can change on each round. At the end of each\nround, users provide feedback about the allocation they received, enabling the\nmechanism to learn user preferences over time. Such situations are common in\nthe shared usage of a compute cluster among many users in an organisation,\nwhere all teams may not precisely know the amount of resources needed to\nexecute their jobs. By understating their requirements, users will receive less\nthan they need and consequently not achieve their goals. By overstating them,\nthey may siphon away precious resources that could be useful to others in the\norganisation. We formalise this task of online learning in fair division via\nnotions of efficiency, fairness, and strategy-proofness applicable to this\nsetting, and study this problem under three types of feedback: when the users'\nobservations are deterministic, when they are stochastic and follow a\nparametric model, and when they are stochastic and nonparametric. We derive\nmechanisms inspired by the classical max-min fairness procedure that achieve\nthese requisites, and quantify the extent to which they are achieved via\nasymptotic rates. We corroborate these insights with an experimental evaluation\non synthetic problems and a web-serving task.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 22:15:20 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Kandasamy", "Kirthevasan", ""], ["Sela", "Gur-Eyal", ""], ["Gonzalez", "Joseph E", ""], ["Jordan", "Michael I", ""], ["Stoica", "Ion", ""]]}, {"id": "2012.08668", "submitter": "Rebecca Roelofs", "authors": "Rebecca Roelofs, Nicholas Cain, Jonathon Shlens, Michael C. Mozer", "title": "Mitigating Bias in Calibration Error Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building reliable machine learning systems requires that we correctly\nunderstand their level of confidence. Calibration measures the degree of\naccuracy in a model's confidence and most research in calibration focuses on\ntechniques to improve an empirical estimate of calibration error, ECE_bin. We\nintroduce a simulation framework that allows us to empirically show that\nECE_bin can systematically underestimate or overestimate the true calibration\nerror depending on the nature of model miscalibration, the size of the\nevaluation data set, and the number of bins. Critically, we find that ECE_bin\nis more strongly biased for perfectly calibrated models. We propose a simple\nalternative calibration error metric, ECE_sweep, in which the number of bins is\nchosen to be as large as possible while preserving monotonicity in the\ncalibration function. Evaluating our measure on distributions fit to neural\nnetwork confidence scores on CIFAR-10, CIFAR-100, and ImageNet, we show that\nECE_sweep produces a less biased estimator of calibration error and therefore\nshould be used by any researcher wishing to evaluate the calibration of models\ntrained on similar datasets.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 23:28:06 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 19:25:00 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Roelofs", "Rebecca", ""], ["Cain", "Nicholas", ""], ["Shlens", "Jonathon", ""], ["Mozer", "Michael C.", ""]]}, {"id": "2012.08682", "submitter": "Eray Unsal Atay", "authors": "Eray Unsal Atay, Igor Kadota and Eytan Modiano", "title": "Aging Bandits: Regret Analysis and Order-Optimal Learning Algorithm for\n  Wireless Networks with Stochastic Arrivals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.IT cs.LG cs.SY math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a single-hop wireless network with sources transmitting\ntime-sensitive information to the destination over multiple unreliable\nchannels. Packets from each source are generated according to a stochastic\nprocess with known statistics and the state of each wireless channel (ON/OFF)\nvaries according to a stochastic process with unknown statistics. The\nreliability of the wireless channels is to be learned through observation. At\nevery time slot, the learning algorithm selects a single pair (source, channel)\nand the selected source attempts to transmit its packet via the selected\nchannel. The probability of a successful transmission to the destination\ndepends on the reliability of the selected channel. The goal of the learning\nalgorithm is to minimize the Age-of-Information (AoI) in the network over $T$\ntime slots. To analyze the performance of the learning algorithm, we introduce\nthe notion of AoI regret, which is the difference between the expected\ncumulative AoI of the learning algorithm under consideration and the expected\ncumulative AoI of a genie algorithm that knows the reliability of the channels\na priori. The AoI regret captures the penalty incurred by having to learn the\nstatistics of the channels over the $T$ time slots. The results are two-fold:\nfirst, we consider learning algorithms that employ well-known solutions to the\nstochastic multi-armed bandit problem (such as $\\epsilon$-Greedy, Upper\nConfidence Bound, and Thompson Sampling) and show that their AoI regret scales\nas $\\Theta(\\log T)$; second, we develop a novel learning algorithm and show\nthat it has $O(1)$ regret. To the best of our knowledge, this is the first\nlearning algorithm with bounded AoI regret.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 00:58:26 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 20:03:47 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Atay", "Eray Unsal", ""], ["Kadota", "Igor", ""], ["Modiano", "Eytan", ""]]}, {"id": "2012.08749", "submitter": "Samet Oymak", "authors": "Xiangyu Chang, Yingcong Li, Samet Oymak, Christos Thrampoulidis", "title": "Provable Benefits of Overparameterization in Model Compression: From\n  Double Descent to Pruning Neural Networks", "comments": "to appear at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep networks are typically trained with many more parameters than the size\nof the training dataset. Recent empirical evidence indicates that the practice\nof overparameterization not only benefits training large models, but also\nassists - perhaps counterintuitively - building lightweight models.\nSpecifically, it suggests that overparameterization benefits model pruning /\nsparsification. This paper sheds light on these empirical findings by\ntheoretically characterizing the high-dimensional asymptotics of model pruning\nin the overparameterized regime. The theory presented addresses the following\ncore question: \"should one train a small model from the beginning, or first\ntrain a large model and then prune?\". We analytically identify regimes in\nwhich, even if the location of the most informative features is known, we are\nbetter off fitting a large model and then pruning rather than simply training\nwith the known informative features. This leads to a new double descent in the\ntraining of sparse models: growing the original model, while preserving the\ntarget sparsity, improves the test accuracy as one moves beyond the\noverparameterization threshold. Our analysis further reveals the benefit of\nretraining by relating it to feature correlations. We find that the above\nphenomena are already present in linear and random-features models. Our\ntechnical approach advances the toolset of high-dimensional analysis and\nprecisely characterizes the asymptotic distribution of over-parameterized\nleast-squares. The intuition gained by analytically studying simpler models is\nnumerically verified on neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 05:13:30 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Chang", "Xiangyu", ""], ["Li", "Yingcong", ""], ["Oymak", "Samet", ""], ["Thrampoulidis", "Christos", ""]]}, {"id": "2012.08784", "submitter": "Xiongjun Zhang", "authors": "Guang-Jing Song, Michael K. Ng and Xiongjun Zhang", "title": "On $O( \\max \\{n_1, n_2 \\}\\log ( \\max \\{ n_1, n_2 \\} n_3) )$ Sample\n  Entries for $n_1 \\times n_2 \\times n_3$ Tensor Completion via Unitary\n  Transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the key problems in tensor completion is the number of uniformly\nrandom sample entries required for recovery guarantee. The main aim of this\npaper is to study $n_1 \\times n_2 \\times n_3$ third-order tensor completion and\ninvestigate into incoherence conditions of $n_3$ low-rank $n_1$-by-$n_2$ matrix\nslices under the transformed tensor singular value decomposition where the\nunitary transformation is applied along $n_3$-dimension. We show that such\nlow-rank tensors can be recovered exactly with high probability when the number\nof randomly observed entries is of order $O( r\\max \\{n_1, n_2 \\} \\log ( \\max \\{\nn_1, n_2 \\} n_3))$, where $r$ is the sum of the ranks of these $n_3$ matrix\nslices in the transformed tensor. By utilizing synthetic data and imaging data\nsets, we demonstrate that the theoretical result can be obtained under valid\nincoherence conditions, and the tensor completion performance of the proposed\nmethod is also better than that of existing methods in terms of sample sizes\nrequirement.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 08:03:48 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Song", "Guang-Jing", ""], ["Ng", "Michael K.", ""], ["Zhang", "Xiongjun", ""]]}, {"id": "2012.08791", "submitter": "Angus Dempster", "authors": "Angus Dempster, Daniel F. Schmidt, Geoffrey I. Webb", "title": "MINIROCKET: A Very Fast (Almost) Deterministic Transform for Time Series\n  Classification", "comments": "10 pages, 11 figures; Updated to accepted version", "journal-ref": null, "doi": "10.1145/3447548.3467231", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Until recently, the most accurate methods for time series classification were\nlimited by high computational complexity. ROCKET achieves state-of-the-art\naccuracy with a fraction of the computational expense of most existing methods\nby transforming input time series using random convolutional kernels, and using\nthe transformed features to train a linear classifier. We reformulate ROCKET\ninto a new method, MINIROCKET, making it up to 75 times faster on larger\ndatasets, and making it almost deterministic (and optionally, with additional\ncomputational expense, fully deterministic), while maintaining essentially the\nsame accuracy. Using this method, it is possible to train and test a classifier\non all of 109 datasets from the UCR archive to state-of-the-art accuracy in\nless than 10 minutes. MINIROCKET is significantly faster than any other method\nof comparable accuracy (including ROCKET), and significantly more accurate than\nany other method of even roughly-similar computational expense. As such, we\nsuggest that MINIROCKET should now be considered and used as the default\nvariant of ROCKET.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 08:24:09 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 13:31:13 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Dempster", "Angus", ""], ["Schmidt", "Daniel F.", ""], ["Webb", "Geoffrey I.", ""]]}, {"id": "2012.08854", "submitter": "Depen Morwani", "authors": "Depen Morwani, Rahul Vashisht, Harish G. Ramaswamy", "title": "Using noise resilience for ranking generalization of deep neural\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent papers have shown that sufficiently overparameterized neural networks\ncan perfectly fit even random labels. Thus, it is crucial to understand the\nunderlying reason behind the generalization performance of a network on\nreal-world data. In this work, we propose several measures to predict the\ngeneralization error of a network given the training data and its parameters.\nUsing one of these measures, based on noise resilience of the network, we\nsecured 5th position in the predicting generalization in deep learning (PGDL)\ncompetition at NeurIPS 2020.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 10:50:34 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Morwani", "Depen", ""], ["Vashisht", "Rahul", ""], ["Ramaswamy", "Harish G.", ""]]}, {"id": "2012.08855", "submitter": "Dawon  Ahn", "authors": "Dawon Ahn, Jun-Gi Jang, U Kang", "title": "Time-Aware Tensor Decomposition for Missing Entry Prediction", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a time-evolving tensor with missing entries, how can we effectively\nfactorize it for precisely predicting the missing entries? Tensor factorization\nhas been extensively utilized for analyzing various multi-dimensional\nreal-world data. However, existing models for tensor factorization have\ndisregarded the temporal property for tensor factorization while most\nreal-world data are closely related to time. Moreover, they do not address\naccuracy degradation due to the sparsity of time slices. The essential problems\nof how to exploit the temporal property for tensor decomposition and consider\nthe sparsity of time slices remain unresolved. In this paper, we propose TATD\n(Time-Aware Tensor Decomposition), a novel tensor decomposition method for\nreal-world temporal tensors. TATD is designed to exploit temporal dependency\nand time-varying sparsity of real-world temporal tensors. We propose a new\nsmoothing regularization with Gaussian kernel for modeling time dependency.\nMoreover, we improve the performance of TATD by considering time-varying\nsparsity. We design an alternating optimization scheme suitable for temporal\ntensor factorization with our smoothing regularization. Extensive experiments\nshow that TATD provides the state-of-the-art accuracy for decomposing temporal\ntensors.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 10:52:34 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Ahn", "Dawon", ""], ["Jang", "Jun-Gi", ""], ["Kang", "U", ""]]}, {"id": "2012.08859", "submitter": "Bert Moons", "authors": "Bert Moons, Parham Noorzad, Andrii Skliar, Giovanni Mariani, Dushyant\n  Mehta, Chris Lott, Tijmen Blankevoort", "title": "Distilling Optimal Neural Networks: Rapid Search in Diverse Spaces", "comments": "Main text 9 pages, Full text 21 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Today, state-of-the-art Neural Architecture Search (NAS) methods cannot scale\nto many hardware platforms or scenarios at a low training costs and/or can only\nhandle non-diverse, heavily constrained architectural search-spaces. To solve\nthese issues, we present DONNA (Distilling Optimal Neural Network\nArchitectures), a novel pipeline for rapid and diverse NAS, that scales to many\nuser scenarios. In DONNA, a search consists of three phases. First, an accuracy\npredictor is built using blockwise knowledge distillation. This predictor\nenables searching across diverse networks with varying macro-architectural\nparameters such as layer types and attention mechanisms as well as across\nmicro-architectural parameters such as block repeats and expansion rates.\nSecond, a rapid evolutionary search phase finds a set of Pareto-optimal\narchitectures for any scenario using the accuracy predictor and on-device\nmeasurements. Third, optimal models are quickly finetuned to\ntraining-from-scratch accuracy. With this approach, DONNA is up to 100x faster\nthan MNasNet in finding state-of-the-art architectures on-device. Classifying\nImageNet, DONNA architectures are 20% faster than EfficientNet-B0 and\nMobileNetV2 on a Nvidia V100 GPU and 10% faster with 0.5% higher accuracy than\nMobileNetV2-1.4x on a Samsung S20 smartphone. In addition to NAS, DONNA is used\nfor search-space extension and exploration, as well as hardware-aware model\ncompression.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 11:00:19 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 08:14:26 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Moons", "Bert", ""], ["Noorzad", "Parham", ""], ["Skliar", "Andrii", ""], ["Mariani", "Giovanni", ""], ["Mehta", "Dushyant", ""], ["Lott", "Chris", ""], ["Blankevoort", "Tijmen", ""]]}, {"id": "2012.08903", "submitter": "Juan Manuel Gorriz Saez", "authors": "Juan Manuel Gorriz and SIPBA group and John Suckling", "title": "A connection between the pattern classification problem and the General\n  Linear Model for statistical inference", "comments": "20 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A connection between the General Linear Model (GLM) in combination with\nclassical statistical inference and the machine learning (MLE)-based inference\nis described in this paper. Firstly, the estimation of the GLM parameters is\nexpressed as a Linear Regression Model (LRM) of an indicator matrix, that is,\nin terms of the inverse problem of regressing the observations. In other words,\nboth approaches, i.e. GLM and LRM, apply to different domains, the observation\nand the label domains, and are linked by a normalization value at the\nleast-squares solution. Subsequently, from this relationship we derive a\nstatistical test based on a more refined predictive algorithm, i.e. the\n(non)linear Support Vector Machine (SVM) that maximizes the class margin of\nseparation, within a permutation analysis. The MLE-based inference employs a\nresidual score and includes the upper bound to compute a better estimation of\nthe actual (real) error. Experimental results demonstrate how the parameter\nestimations derived from each model resulted in different classification\nperformances in the equivalent inverse problem. Moreover, using real data the\naforementioned predictive algorithms within permutation tests, including such\nmodel-free estimators, are able to provide a good trade-off between type I\nerror and statistical power.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 12:26:26 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Gorriz", "Juan Manuel", ""], ["group", "SIPBA", ""], ["Suckling", "John", ""]]}, {"id": "2012.08916", "submitter": "Yuheng Jia", "authors": "Yuheng Jia, Hui Liu, Junhui Hou, Qingfu Zhang", "title": "Clustering Ensemble Meets Low-rank Tensor Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the problem of clustering ensemble, which aims to combine\nmultiple base clusterings to produce better performance than that of the\nindividual one. The existing clustering ensemble methods generally construct a\nco-association matrix, which indicates the pairwise similarity between samples,\nas the weighted linear combination of the connective matrices from different\nbase clusterings, and the resulting co-association matrix is then adopted as\nthe input of an off-the-shelf clustering algorithm, e.g., spectral clustering.\nHowever, the co-association matrix may be dominated by poor base clusterings,\nresulting in inferior performance. In this paper, we propose a novel low-rank\ntensor approximation-based method to solve the problem from a global\nperspective. Specifically, by inspecting whether two samples are clustered to\nan identical cluster under different base clusterings, we derive a\ncoherent-link matrix, which contains limited but highly reliable relationships\nbetween samples. We then stack the coherent-link matrix and the co-association\nmatrix to form a three-dimensional tensor, the low-rankness property of which\nis further explored to propagate the information of the coherent-link matrix to\nthe co-association matrix, producing a refined co-association matrix. We\nformulate the proposed method as a convex constrained optimization problem and\nsolve it efficiently. Experimental results over 7 benchmark data sets show that\nthe proposed model achieves a breakthrough in clustering performance, compared\nwith 12 state-of-the-art methods. To the best of our knowledge, this is the\nfirst work to explore the potential of low-rank tensor on clustering ensemble,\nwhich is fundamentally different from previous approaches.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 13:01:37 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Jia", "Yuheng", ""], ["Liu", "Hui", ""], ["Hou", "Junhui", ""], ["Zhang", "Qingfu", ""]]}, {"id": "2012.09027", "submitter": "Radu Horaud P", "authors": "Miles Hansard and Radu Horaud", "title": "A Differential Model of the Complex Cell", "comments": null, "journal-ref": "Neural Computation, 23(9), 2011", "doi": "10.1162/NECO_a_00163", "report-no": null, "categories": "q-bio.NC cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The receptive fields of simple cells in the visual cortex can be understood\nas linear filters. These filters can be modelled by Gabor functions, or by\nGaussian derivatives. Gabor functions can also be combined in an `energy model'\nof the complex cell response. This paper proposes an alternative model of the\ncomplex cell, based on Gaussian derivatives. It is most important to account\nfor the insensitivity of the complex response to small shifts of the image. The\nnew model uses a linear combination of the first few derivative filters, at a\nsingle position, to approximate the first derivative filter, at a series of\nadjacent positions. The maximum response, over all positions, gives a signal\nthat is insensitive to small shifts of the image. This model, unlike previous\napproaches, is based on the scale space theory of visual processing. In\nparticular, the complex cell is built from filters that respond to the \\twod\\\ndifferential structure of the image. The computational aspects of the new model\nare studied in one and two dimensions, using the steerability of the Gaussian\nderivatives. The response of the model to basic images, such as edges and\ngratings, is derived formally. The response to natural images is also\nevaluated, using statistical measures of shift insensitivity. The relevance of\nthe new model to the cortical image representation is discussed.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 10:23:23 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Hansard", "Miles", ""], ["Horaud", "Radu", ""]]}, {"id": "2012.09092", "submitter": "Chaochao Lu", "authors": "Chaochao Lu, Biwei Huang, Ke Wang, Jos\\'e Miguel Hern\\'andez-Lobato,\n  Kun Zhang, Bernhard Sch\\\"olkopf", "title": "Sample-Efficient Reinforcement Learning via Counterfactual-Based Data\n  Augmentation", "comments": "Neural Information Processing Systems Workshop on Offline\n  Reinforcement Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) algorithms usually require a substantial amount\nof interaction data and perform well only for specific tasks in a fixed\nenvironment. In some scenarios such as healthcare, however, usually only few\nrecords are available for each patient, and patients may show different\nresponses to the same treatment, impeding the application of current RL\nalgorithms to learn optimal policies. To address the issues of mechanism\nheterogeneity and related data scarcity, we propose a data-efficient RL\nalgorithm that exploits structural causal models (SCMs) to model the state\ndynamics, which are estimated by leveraging both commonalities and differences\nacross subjects. The learned SCM enables us to counterfactually reason what\nwould have happened had another treatment been taken. It helps avoid real\n(possibly risky) exploration and mitigates the issue that limited experiences\nlead to biased policies. We propose counterfactual RL algorithms to learn both\npopulation-level and individual-level policies. We show that counterfactual\noutcomes are identifiable under mild conditions and that Q- learning on the\ncounterfactual-based augmented data set converges to the optimal value\nfunction. Experimental results on synthetic and real-world data demonstrate the\nefficacy of the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 17:21:13 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Lu", "Chaochao", ""], ["Huang", "Biwei", ""], ["Wang", "Ke", ""], ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""], ["Zhang", "Kun", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "2012.09117", "submitter": "Bla\\v{z} \\v{S}krlj", "authors": "Sebastian Me\\v{z}nar and Bla\\v{z} \\v{S}krlj", "title": "Predicting Generalization in Deep Learning via Metric Learning -- PGDL\n  Shared task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The competition \"Predicting Generalization in Deep Learning (PGDL)\" aims to\nprovide a platform for rigorous study of generalization of deep learning models\nand offer insight into the progress of understanding and explaining these\nmodels. This report presents the solution that was submitted by the user\n\\emph{smeznar} which achieved the eight place in the competition. In the\nproposed approach, we create simple metrics and find their best combination\nwith automatic testing on the provided dataset, exploring how combinations of\nvarious properties of the input neural network architectures can be used for\nthe prediction of their generalization.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 17:59:13 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Me\u017enar", "Sebastian", ""], ["\u0160krlj", "Bla\u017e", ""]]}, {"id": "2012.09226", "submitter": "Jiening Zhu", "authors": "Jiening Zhu, Kaiming Xu, Allen Tannenbaum", "title": "Optimal transport for vector Gaussian mixture models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Vector Gaussian mixture models form an important special subset of\nvector-valued distributions. Any physical entity that can mutate or transit\namong alternative manifestations distributed in a given space falls into this\ncategory. A key example is color imagery. In this note, we vectorize the\nGaussian mixture model and study different optimal mass transport related\nproblems for such models. The benefits of using vector Gaussian mixture for\noptimal mass transport include computational efficiency and the ability to\npreserve structure.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 19:46:08 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Zhu", "Jiening", ""], ["Xu", "Kaiming", ""], ["Tannenbaum", "Allen", ""]]}, {"id": "2012.09258", "submitter": "Samuel Ackerman", "authors": "Samuel Ackerman, Eitan Farchi, Orna Raz, Marcel Zalmanovici, Parijat\n  Dube", "title": "Detection of data drift and outliers affecting machine learning model\n  performance over time", "comments": "In: JSM Proceedings, Nonparametric Statistics Section, 20202.\n  Philadelphia, PA: American Statistical Association. 144--160", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A trained ML model is deployed on another `test' dataset where target feature\nvalues (labels) are unknown. Drift is distribution change between the training\nand deployment data, which is concerning if model performance changes. For a\ncat/dog image classifier, for instance, drift during deployment could be rabbit\nimages (new class) or cat/dog images with changed characteristics (change in\ndistribution). We wish to detect these changes but can't measure accuracy\nwithout deployment data labels. We instead detect drift indirectly by\nnonparametrically testing the distribution of model prediction confidence for\nchanges. This generalizes our method and sidesteps domain-specific feature\nrepresentation.\n  We address important statistical issues, particularly Type-1 error control in\nsequential testing, using Change Point Models (CPMs; see Adams and Ross 2012).\nWe also use nonparametric outlier methods to show the user suspicious\nobservations for model diagnosis, since the before/after change confidence\ndistributions overlap significantly. In experiments to demonstrate robustness,\nwe train on a subset of MNIST digit classes, then insert drift (e.g., unseen\ndigit class) in deployment data in various settings (gradual/sudden changes in\nthe drift proportion). A novel loss function is introduced to compare the\nperformance (detection delay, Type-1 and 2 errors) of a drift detector under\ndifferent levels of drift class contamination.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 20:50:12 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 09:31:46 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Ackerman", "Samuel", ""], ["Farchi", "Eitan", ""], ["Raz", "Orna", ""], ["Zalmanovici", "Marcel", ""], ["Dube", "Parijat", ""]]}, {"id": "2012.09265", "submitter": "Patrick Coles", "authors": "M. Cerezo, Andrew Arrasmith, Ryan Babbush, Simon C. Benjamin, Suguru\n  Endo, Keisuke Fujii, Jarrod R. McClean, Kosuke Mitarai, Xiao Yuan, Lukasz\n  Cincio, Patrick J. Coles", "title": "Variational Quantum Algorithms", "comments": "Review Article. 29 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": "LA-UR-20-30142", "categories": "quant-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications such as simulating large quantum systems or solving large-scale\nlinear algebra problems are immensely challenging for classical computers due\ntheir extremely high computational cost. Quantum computers promise to unlock\nthese applications, although fault-tolerant quantum computers will likely not\nbe available for several years. Currently available quantum devices have\nserious constraints, including limited qubit numbers and noise processes that\nlimit circuit depth. Variational Quantum Algorithms (VQAs), which employ a\nclassical optimizer to train a parametrized quantum circuit, have emerged as a\nleading strategy to address these constraints. VQAs have now been proposed for\nessentially all applications that researchers have envisioned for quantum\ncomputers, and they appear to the best hope for obtaining quantum advantage.\nNevertheless, challenges remain including the trainability, accuracy, and\nefficiency of VQAs. In this review article we present an overview of the field\nof VQAs. Furthermore, we discuss strategies to overcome their challenges as\nwell as the exciting prospects for using them as a means to obtain quantum\nadvantage.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 21:00:46 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Cerezo", "M.", ""], ["Arrasmith", "Andrew", ""], ["Babbush", "Ryan", ""], ["Benjamin", "Simon C.", ""], ["Endo", "Suguru", ""], ["Fujii", "Keisuke", ""], ["McClean", "Jarrod R.", ""], ["Mitarai", "Kosuke", ""], ["Yuan", "Xiao", ""], ["Cincio", "Lukasz", ""], ["Coles", "Patrick J.", ""]]}, {"id": "2012.09385", "submitter": "Daniel McKenzie", "authors": "Anna Little, Daniel McKenzie and James Murphy", "title": "Balancing Geometry and Density: Path Distances on High-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  New geometric and computational analyses of power-weighted shortest-path\ndistances (PWSPDs) are presented. By illuminating the way these metrics balance\ndensity and geometry in the underlying data, we clarify their key parameters\nand discuss how they may be chosen in practice. Comparisons are made with\nrelated data-driven metrics, which illustrate the broader role of density in\nkernel-based unsupervised and semi-supervised machine learning.\nComputationally, we relate PWSPDs on complete weighted graphs to their\nanalogues on weighted nearest neighbor graphs, providing high probability\nguarantees on their equivalence that are near-optimal. Connections with\npercolation theory are developed to establish estimates on the bias and\nvariance of PWSPDs in the finite sample setting. The theoretical results are\nbolstered by illustrative experiments, demonstrating the versatility of PWSPDs\nfor a wide range of data settings. Throughout the paper, our results require\nonly that the underlying data is sampled from a low-dimensional manifold, and\ndepend crucially on the intrinsic dimension of this manifold, rather than its\nambient dimension.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 04:03:15 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 21:39:01 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Little", "Anna", ""], ["McKenzie", "Daniel", ""], ["Murphy", "James", ""]]}, {"id": "2012.09390", "submitter": "Edward Raff", "authors": "Edward Raff, William Fleshman, Richard Zak, Hyrum S. Anderson, Bobby\n  Filar, Mark McLean", "title": "Classifying Sequences of Extreme Length with Constant Memory Applied to\n  Malware Detection", "comments": "To appear in AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works within machine learning have been tackling inputs of\never-increasing size, with cybersecurity presenting sequence classification\nproblems of particularly extreme lengths. In the case of Windows executable\nmalware detection, inputs may exceed $100$ MB, which corresponds to a time\nseries with $T=100,000,000$ steps. To date, the closest approach to handling\nsuch a task is MalConv, a convolutional neural network capable of processing up\nto $T=2,000,000$ steps. The $\\mathcal{O}(T)$ memory of CNNs has prevented\nfurther application of CNNs to malware. In this work, we develop a new approach\nto temporal max pooling that makes the required memory invariant to the\nsequence length $T$. This makes MalConv $116\\times$ more memory efficient, and\nup to $25.8\\times$ faster to train on its original dataset, while removing the\ninput length restrictions to MalConv. We re-invest these gains into improving\nthe MalConv architecture by developing a new Global Channel Gating design,\ngiving us an attention mechanism capable of learning feature interactions\nacross 100 million time steps in an efficient manner, a capability lacked by\nthe original MalConv CNN. Our implementation can be found at\nhttps://github.com/NeuromorphicComputationResearchProgram/MalConv2\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 04:45:33 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Raff", "Edward", ""], ["Fleshman", "William", ""], ["Zak", "Richard", ""], ["Anderson", "Hyrum S.", ""], ["Filar", "Bobby", ""], ["McLean", "Mark", ""]]}, {"id": "2012.09422", "submitter": "Andrew Bennett", "authors": "Andrew Bennett, Nathan Kallus", "title": "The Variational Method of Moments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG econ.EM math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The conditional moment problem is a powerful formulation for describing\nstructural causal parameters in terms of observables, a prominent example being\ninstrumental variable regression. A standard approach is to reduce the problem\nto a finite set of marginal moment conditions and apply the optimally weighted\ngeneralized method of moments (OWGMM), but this requires we know a finite set\nof identifying moments, can still be inefficient even if identifying, or can be\ntheoretically efficient but practically unwieldy if we use a growing sieve of\nmoment conditions. Motivated by a variational minimax reformulation of OWGMM,\nwe define a very general class of estimators for the conditional moment\nproblem, which we term the variational method of moments (VMM) and which\nnaturally enables controlling infinitely-many moments. We provide a detailed\ntheoretical analysis of multiple VMM estimators, including ones based on kernel\nmethods and neural nets, and provide appropriate conditions under which these\nestimators are consistent, asymptotically normal, and semiparametrically\nefficient in the full conditional moment model. This is in contrast to other\nrecently proposed methods for solving conditional moment problems based on\nadversarial machine learning, which do not incorporate optimal weighting, do\nnot establish asymptotic normality, and are not semiparametrically efficient.\nIn addition, we provide corresponding inference algorithms based on the same\nkind of variational reformulations, both for kernel- and neural net-based\nvarieties. Finally, we demonstrate the strong performance of our proposed\nestimation and inference algorithms in a detailed series of synthetic\nexperiments.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 07:21:06 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 20:57:53 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Bennett", "Andrew", ""], ["Kallus", "Nathan", ""]]}, {"id": "2012.09430", "submitter": "Armen Allahverdyan", "authors": "A.E. Allahverdyan and N.H. Martirosyan", "title": "Maximum Entropy competes with Maximum Likelihood", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cond-mat.stat-mech cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Maximum entropy (MAXENT) method has a large number of applications in\ntheoretical and applied machine learning, since it provides a convenient\nnon-parametric tool for estimating unknown probabilities. The method is a major\ncontribution of statistical physics to probabilistic inference. However, a\nsystematic approach towards its validity limits is currently missing. Here we\nstudy MAXENT in a Bayesian decision theory set-up, i.e. assuming that there\nexists a well-defined prior Dirichlet density for unknown probabilities, and\nthat the average Kullback-Leibler (KL) distance can be employed for deciding on\nthe quality and applicability of various estimators. These allow to evaluate\nthe relevance of various MAXENT constraints, check its general applicability,\nand compare MAXENT with estimators having various degrees of dependence on the\nprior, viz. the regularized maximum likelihood (ML) and the Bayesian\nestimators. We show that MAXENT applies in sparse data regimes, but needs\nspecific types of prior information. In particular, MAXENT can outperform the\noptimally regularized ML provided that there are prior rank correlations\nbetween the estimated random quantity and its probabilities.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 07:44:22 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Allahverdyan", "A. E.", ""], ["Martirosyan", "N. H.", ""]]}, {"id": "2012.09448", "submitter": "Yiyan Huang", "authors": "Yiyan Huang, Cheuk Hang Leung, Xing Yan, Qi Wu, Nanbo Peng, Dongdong\n  Wang, Zhixiang Huang", "title": "The Causal Learning of Retail Delinquency", "comments": "This paper was accepted and will be published in the Thirty-Fifth\n  AAAI Conference on Artificial Intelligence (AAAI-21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the expected difference in borrower's repayment when\nthere is a change in the lender's credit decisions. Classical estimators\noverlook the confounding effects and hence the estimation error can be\nmagnificent. As such, we propose another approach to construct the estimators\nsuch that the error can be greatly reduced. The proposed estimators are shown\nto be unbiased, consistent, and robust through a combination of theoretical\nanalysis and numerical testing. Moreover, we compare the power of estimating\nthe causal quantities between the classical estimators and the proposed\nestimators. The comparison is tested across a wide range of models, including\nlinear regression models, tree-based models, and neural network-based models,\nunder different simulated datasets that exhibit different levels of causality,\ndifferent degrees of nonlinearity, and different distributional properties.\nMost importantly, we apply our approaches to a large observational dataset\nprovided by a global technology firm that operates in both the e-commerce and\nthe lending business. We find that the relative reduction of estimation error\nis strikingly substantial if the causal effects are accounted for correctly.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 08:46:01 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Huang", "Yiyan", ""], ["Leung", "Cheuk Hang", ""], ["Yan", "Xing", ""], ["Wu", "Qi", ""], ["Peng", "Nanbo", ""], ["Wang", "Dongdong", ""], ["Huang", "Zhixiang", ""]]}, {"id": "2012.09537", "submitter": "Eyal Gofer", "authors": "Eyal Gofer and Guy Gilboa", "title": "Experts with Lower-Bounded Loss Feedback: A Unifying Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most prominent feedback models for the best expert problem are the full\ninformation and bandit models. In this work we consider a simple feedback model\nthat generalizes both, where on every round, in addition to a bandit feedback,\nthe adversary provides a lower bound on the loss of each expert. Such lower\nbounds may be obtained in various scenarios, for instance, in stock trading or\nin assessing errors of certain measurement devices. For this model we prove\noptimal regret bounds (up to logarithmic factors) for modified versions of\nExp3, generalizing algorithms and bounds both for the bandit and the\nfull-information settings. Our second-order unified regret analysis simulates a\ntwo-step loss update and highlights three Hessian or Hessian-like expressions,\nwhich map to the full-information regret, bandit regret, and a hybrid of both.\nOur results intersect with those for bandits with graph-structured feedback, in\nthat both settings can accommodate feedback from an arbitrary subset of experts\non each round. However, our model also accommodates partial feedback at the\nsingle-expert level, by allowing non-trivial lower bounds on each loss.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 12:18:52 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Gofer", "Eyal", ""], ["Gilboa", "Guy", ""]]}, {"id": "2012.09561", "submitter": "Huan Qing", "authors": "Huan Qing and Jingli Wang", "title": "Estimating mixed-memberships using the Symmetric Laplacian Inverse\n  Matrix", "comments": "35 pages, 5 figures, 6 tables. arXiv admin note: text overlap with\n  arXiv:2012.04867", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection has been well studied in network analysis, and one\npopular technique is spectral clustering which is fast and statistically\nanalyzable for detect-ing clusters for given networks. But the more realistic\ncase of mixed membership community detection remains a challenge. In this\npaper, we propose a new spectral clustering method Mixed-SLIM for mixed\nmembership community detection. Mixed-SLIM is designed based on the symmetrized\nLaplacian inverse matrix (SLIM) (Jing et al. 2021) under the degree-corrected\nmixed membership (DCMM) model. We show that this algorithm and its regularized\nversion Mixed-SLIM {\\tau} are asymptotically consistent under mild conditions.\nMeanwhile, we provide Mixed-SLIM appro and its regularized version Mixed-SLIM\n{\\tau}appro by approximating the SLIM matrix when dealing with large networks\nin practice. These four Mixed-SLIM methods outperform state-of-art methods in\nsimulations and substantial empirical datasets for both community detection and\nmixed membership community detection problems.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 13:19:06 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Qing", "Huan", ""], ["Wang", "Jingli", ""]]}, {"id": "2012.09679", "submitter": "Marcel Wien\\\"obst", "authors": "Marcel Wien\\\"obst and Max Bannach and Maciej Li\\'skiewicz", "title": "Polynomial-Time Algorithms for Counting and Sampling Markov Equivalent\n  DAGs", "comments": "Extended version of paper accepted to the Proceedings of the 35th\n  AAAI Conference on Artificial Intelligence (AAAI-2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counting and uniform sampling of directed acyclic graphs (DAGs) from a Markov\nequivalence class are fundamental tasks in graphical causal analysis. In this\npaper, we show that these tasks can be performed in polynomial time, solving a\nlong-standing open problem in this area. Our algorithms are effective and\neasily implementable. Experimental results show that the algorithms\nsignificantly outperform state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 15:47:15 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Wien\u00f6bst", "Marcel", ""], ["Bannach", "Max", ""], ["Li\u015bkiewicz", "Maciej", ""]]}, {"id": "2012.09720", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas and Daniel M. Kane", "title": "Hardness of Learning Halfspaces with Massart Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of PAC learning halfspaces in the presence of Massart\n(bounded) noise. Specifically, given labeled examples $(x, y)$ from a\ndistribution $D$ on $\\mathbb{R}^{n} \\times \\{ \\pm 1\\}$ such that the marginal\ndistribution on $x$ is arbitrary and the labels are generated by an unknown\nhalfspace corrupted with Massart noise at rate $\\eta<1/2$, we want to compute a\nhypothesis with small misclassification error. Characterizing the efficient\nlearnability of halfspaces in the Massart model has remained a longstanding\nopen problem in learning theory.\n  Recent work gave a polynomial-time learning algorithm for this problem with\nerror $\\eta+\\epsilon$. This error upper bound can be far from the\ninformation-theoretically optimal bound of $\\mathrm{OPT}+\\epsilon$. More recent\nwork showed that {\\em exact learning}, i.e., achieving error\n$\\mathrm{OPT}+\\epsilon$, is hard in the Statistical Query (SQ) model. In this\nwork, we show that there is an exponential gap between the\ninformation-theoretically optimal error and the best error that can be achieved\nby a polynomial-time SQ algorithm. In particular, our lower bound implies that\nno efficient SQ algorithm can approximate the optimal error within any\npolynomial factor.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 16:43:11 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""]]}, {"id": "2012.09768", "submitter": "T. Mitchell Roddenberry", "authors": "T. Mitchell Roddenberry, Santiago Segarra, Anastasios Kyrillidis", "title": "Rank-One Measurements of Low-Rank PSD Matrices Have Small Feasible Sets", "comments": "22 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the role of the constraint set in determining the solution to\nlow-rank, positive semidefinite (PSD) matrix sensing problems. The setting we\nconsider involves rank-one sensing matrices: In particular, given a set of\nrank-one projections of an approximately low-rank PSD matrix, we characterize\nthe radius of the set of PSD matrices that satisfy the measurements. This\nresult yields a sampling rate to guarantee singleton solution sets when the\ntrue matrix is exactly low-rank, such that the choice of the objective function\nor the algorithm to be used is inconsequential in its recovery. We discuss\napplications of this contribution and compare it to recent literature regarding\nimplicit regularization for similar problems. We demonstrate practical\nimplications of this result by applying conic projection methods for PSD matrix\nrecovery without incorporating low-rank regularization.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 17:23:27 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 15:10:50 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Roddenberry", "T. Mitchell", ""], ["Segarra", "Santiago", ""], ["Kyrillidis", "Anastasios", ""]]}, {"id": "2012.09783", "submitter": "Joachim Sicking", "authors": "Joachim Sicking, Maximilian Pintz, Maram Akila, Tim Wirtz", "title": "DenseHMM: Learning Hidden Markov Models by Learning Dense\n  Representations", "comments": "Accepted at LMRL workshop at NeurIPS 2020. Code is available on:\n  https://github.com/fraunhofer-iais/dense-hmm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose DenseHMM - a modification of Hidden Markov Models (HMMs) that\nallows to learn dense representations of both the hidden states and the\nobservables. Compared to the standard HMM, transition probabilities are not\natomic but composed of these representations via kernelization. Our approach\nenables constraint-free and gradient-based optimization. We propose two\noptimization schemes that make use of this: a modification of the Baum-Welch\nalgorithm and a direct co-occurrence optimization. The latter one is highly\nscalable and comes empirically without loss of performance compared to standard\nHMMs. We show that the non-linearity of the kernelization is crucial for the\nexpressiveness of the representations. The properties of the DenseHMM like\nlearned co-occurrences and log-likelihoods are studied empirically on synthetic\nand biomedical datasets.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 17:48:27 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Sicking", "Joachim", ""], ["Pintz", "Maximilian", ""], ["Akila", "Maram", ""], ["Wirtz", "Tim", ""]]}, {"id": "2012.09785", "submitter": "Bahman Moraffah", "authors": "Bahman Moraffah, Christ Richmond, Raha Moraffah, and Antonia\n  Papandreou-Suppappola", "title": "Use of Bayesian Nonparametric methods for Estimating the Measurements in\n  High Clutter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robust tracking of a target in a clutter environment is an important and\nchallenging task. In recent years, the nearest neighbor methods and\nprobabilistic data association filters were proposed. However, the performance\nof these methods diminishes as the number of measurements increases. In this\npaper, we propose a robust generative approach to effectively model multiple\nsensor measurements for tracking a moving target in an environment with high\nclutter. We assume a time-dependent number of measurements that include sensor\nobservations with unknown origin, some of which may only contain clutter with\nno additional information. We robustly and accurately estimate the trajectory\nof the moving target in a high clutter environment with an unknown number of\nclutters by employing Bayesian nonparametric modeling. In particular, we employ\na class of joint Bayesian nonparametric models to construct the joint prior\ndistribution of target and clutter measurements such that the conditional\ndistributions follow a Dirichlet process. The marginalized Dirichlet process\nprior of the target measurements is then used in a Bayesian tracker to estimate\nthe dynamically-varying target state. We show through experiments that the\ntracking performance and effectiveness of our proposed framework are increased\nby suppressing high clutter measurements. In addition, we show that our\nproposed method outperforms existing methods such as nearest neighbor and\nprobability data association filters.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 18:32:34 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Moraffah", "Bahman", ""], ["Richmond", "Christ", ""], ["Moraffah", "Raha", ""], ["Papandreou-Suppappola", "Antonia", ""]]}, {"id": "2012.09816", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu and Yuanzhi Li", "title": "Towards Understanding Ensemble, Knowledge Distillation and\n  Self-Distillation in Deep Learning", "comments": "v2 polishes writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formally study how ensemble of deep learning models can improve test\naccuracy, and how the superior performance of ensemble can be distilled into a\nsingle model using knowledge distillation. We consider the challenging case\nwhere the ensemble is simply an average of the outputs of a few independently\ntrained neural networks with the SAME architecture, trained using the SAME\nalgorithm on the SAME data set, and they only differ by the random seeds used\nin the initialization.\n  We empirically show that ensemble/knowledge distillation in deep learning\nworks very differently from traditional learning theory, especially differently\nfrom ensemble of random feature mappings or the neural-tangent-kernel feature\nmappings, and is potentially out of the scope of existing theorems. Thus, to\nproperly understand ensemble and knowledge distillation in deep learning, we\ndevelop a theory showing that when data has a structure we refer to as\n\"multi-view\", then ensemble of independently trained neural networks can\nprovably improve test accuracy, and such superior test accuracy can also be\nprovably distilled into a single model by training a single model to match the\noutput of the ensemble instead of the true label. Our result sheds light on how\nensemble works in deep learning in a way that is completely different from\ntraditional theorems, and how the \"dark knowledge\" is hidden in the outputs of\nthe ensemble -- that can be used in knowledge distillation -- comparing to the\ntrue data labels. In the end, we prove that self-distillation can also be\nviewed as implicitly combining ensemble and knowledge distillation to improve\ntest accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 18:34:45 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 23:21:21 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Li", "Yuanzhi", ""]]}, {"id": "2012.09839", "submitter": "Kaifeng Lyu", "authors": "Zhiyuan Li, Yuping Luo, Kaifeng Lyu", "title": "Towards Resolving the Implicit Bias of Gradient Descent for Matrix\n  Factorization: Greedy Low-Rank Learning", "comments": "49 pages, 7 figures; ICLR 2021 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Matrix factorization is a simple and natural test-bed to investigate the\nimplicit regularization of gradient descent. Gunasekar et al. (2017)\nconjectured that Gradient Flow with infinitesimal initialization converges to\nthe solution that minimizes the nuclear norm, but a series of recent papers\nargued that the language of norm minimization is not sufficient to give a full\ncharacterization for the implicit regularization. In this work, we provide\ntheoretical and empirical evidence that for depth-2 matrix factorization,\ngradient flow with infinitesimal initialization is mathematically equivalent to\na simple heuristic rank minimization algorithm, Greedy Low-Rank Learning, under\nsome reasonable assumptions. This generalizes the rank minimization view from\nprevious works to a much broader setting and enables us to construct\ncounter-examples to refute the conjecture from Gunasekar et al. (2017). We also\nextend the results to the case where depth $\\ge 3$, and we show that the\nbenefit of being deeper is that the above convergence has a much weaker\ndependence over initialization magnitude so that this rank minimization is more\nlikely to take effect for initialization with practical scale.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 18:57:01 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 12:40:57 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Li", "Zhiyuan", ""], ["Luo", "Yuping", ""], ["Lyu", "Kaifeng", ""]]}, {"id": "2012.09854", "submitter": "Ronghang Hu", "authors": "Ronghang Hu, Nikhila Ravi, Alex Berg, Deepak Pathak", "title": "Worldsheet: Wrapping the World in a 3D Sheet for View Synthesis from a\n  Single Image", "comments": "v2 diff: Added occlusion handling via layered Wordsheets. Webpage at\n  https://worldsheet.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Worldsheet, a method for novel view synthesis using just a single\nRGB image as input. The main insight is that simply shrink-wrapping a planar\nmesh sheet onto the input image, consistent with the learned intermediate\ndepth, captures underlying geometry sufficient to generate photorealistic\nunseen views with large viewpoint changes. To operationalize this, we propose a\nnovel differentiable texture sampler that allows our wrapped mesh sheet to be\ntextured and rendered differentiably into an image from a target viewpoint. Our\napproach is category-agnostic, end-to-end trainable without using any 3D\nsupervision, and requires a single image at test time. We also explore a simple\nextension by stacking multiple layers of Worldsheets to better handle\nocclusions. Worldsheet consistently outperforms prior state-of-the-art methods\non single-image view synthesis across several datasets. Furthermore, this\nsimple idea captures novel views surprisingly well on a wide range of\nhigh-resolution in-the-wild images, converting them into navigable 3D pop-ups.\nVideo results and code at https://worldsheet.github.io.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 18:59:52 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 03:46:44 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Hu", "Ronghang", ""], ["Ravi", "Nikhila", ""], ["Berg", "Alex", ""], ["Pathak", "Deepak", ""]]}, {"id": "2012.09922", "submitter": "Ruida Zhou", "authors": "Ruida Zhou, Chao Tian, Tie Liu", "title": "Individually Conditional Individual Mutual Information Bound on\n  Generalization Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new information-theoretic bound on generalization error based on\na combination of the error decomposition technique of Bu et al. and the\nconditional mutual information (CMI) construction of Steinke and Zakynthinou.\nIn a previous work, Haghifam et al. proposed a different bound combining the\ntwo aforementioned techniques, which we refer to as the conditional individual\nmutual information (CIMI) bound. However, in a simple Gaussian setting, both\nthe CMI and the CIMI bounds are order-wise worse than that by Bu et al.. This\nobservation motivated us to propose the new bound, which overcomes this issue\nby reducing the conditioning terms in the conditional mutual information. In\nthe process of establishing this bound, a conditional decoupling lemma is\nestablished, which also leads to a meaningful dichotomy and comparison among\nthese information-theoretic bounds.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 20:35:11 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 00:57:55 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Zhou", "Ruida", ""], ["Tian", "Chao", ""], ["Liu", "Tie", ""]]}, {"id": "2012.09932", "submitter": "Edward Raff", "authors": "Edward Raff", "title": "Research Reproducibility as a Survival Analysis", "comments": "To appear in AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been increasing concern within the machine learning community that\nwe are in a reproducibility crisis. As many have begun to work on this problem,\nall work we are aware of treat the issue of reproducibility as an intrinsic\nbinary property: a paper is or is not reproducible. Instead, we consider\nmodeling the reproducibility of a paper as a survival analysis problem. We\nargue that this perspective represents a more accurate model of the underlying\nmeta-science question of reproducible research, and we show how a survival\nanalysis allows us to draw new insights that better explain prior longitudinal\ndata. The data and code can be found at\nhttps://github.com/EdwardRaff/Research-Reproducibility-Survival-Analysis\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 20:56:53 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Raff", "Edward", ""]]}, {"id": "2012.09935", "submitter": "Alejandro Schuler", "authors": "Alejandro Schuler, David Walsh, Diana Hall, Jon Walsh, Charles Fisher", "title": "Increasing the efficiency of randomized trial estimates via linear\n  adjustment for a prognostic score", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating causal effects from randomized experiments is central to clinical\nresearch. Reducing the statistical uncertainty in these analyses is an\nimportant objective for statisticians. Registries, prior trials, and health\nrecords constitute a growing compendium of historical data on patients under\nstandard-of-care conditions that may be exploitable to this end. However, most\nmethods for historical borrowing achieve reductions in variance by sacrificing\nstrict type-I error rate control. Here, we propose a use of historical data\nthat exploits linear covariate adjustment to improve the efficiency of trial\nanalyses without incurring bias. Specifically, we train a prognostic model on\nthe historical data, then estimate the treatment effect using a linear\nregression while adjusting for the trial subjects' predicted outcomes (their\nprognostic scores). We prove that, under certain conditions, this prognostic\ncovariate adjustment procedure attains the minimum variance possible among a\nlarge class of estimators. When those conditions are not met, prognostic\ncovariate adjustment is still more efficient than raw covariate adjustment and\nthe gain in efficiency is proportional to a measure of the predictive accuracy\nof the prognostic model. We demonstrate the approach using simulations and a\nreanalysis of an Alzheimer's Disease clinical trial and observe meaningful\nreductions in mean-squared error and the estimated variance. Lastly, we provide\na simplified formula for asymptotic variance that enables power and sample size\ncalculations that account for the gains from the prognostic model for clinical\ntrial design. Sample size reductions between 10% and 30% are attainable when\nusing prognostic models that explain a clinically realistic percentage of the\noutcome variance.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 21:10:10 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 01:46:36 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Schuler", "Alejandro", ""], ["Walsh", "David", ""], ["Hall", "Diana", ""], ["Walsh", "Jon", ""], ["Fisher", "Charles", ""]]}, {"id": "2012.09943", "submitter": "Anthony Tai", "authors": "Anthony S. Tai, Chunfeng Huang", "title": "Guiding Neural Network Initialization via Marginal Likelihood\n  Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple, data-driven approach to help guide hyperparameter\nselection for neural network initialization. We leverage the relationship\nbetween neural network and Gaussian process models having corresponding\nactivation and covariance functions to infer the hyperparameter values\ndesirable for model initialization. Our experiment shows that marginal\nlikelihood maximization provides recommendations that yield near-optimal\nprediction performance on MNIST classification task under experiment\nconstraints. Furthermore, our empirical results indicate consistency in the\nproposed technique, suggesting that computation cost for the procedure could be\nsignificantly reduced with smaller training sets.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 21:46:09 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Tai", "Anthony S.", ""], ["Huang", "Chunfeng", ""]]}, {"id": "2012.09973", "submitter": "Huong Ha", "authors": "Huong Ha, Sunil Gupta, Santu Rana, Svetha Venkatesh", "title": "High Dimensional Level Set Estimation with Bayesian Neural Network", "comments": "Accepted at AAAI'2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Level Set Estimation (LSE) is an important problem with applications in\nvarious fields such as material design, biotechnology, machine operational\ntesting, etc. Existing techniques suffer from the scalability issue, that is,\nthese methods do not work well with high dimensional inputs. This paper\nproposes novel methods to solve the high dimensional LSE problems using\nBayesian Neural Networks. In particular, we consider two types of LSE problems:\n(1) \\textit{explicit} LSE problem where the threshold level is a fixed\nuser-specified value, and, (2) \\textit{implicit} LSE problem where the\nthreshold level is defined as a percentage of the (unknown) maximum of the\nobjective function. For each problem, we derive the corresponding theoretic\ninformation based acquisition function to sample the data points so as to\nmaximally increase the level set accuracy. Furthermore, we also analyse the\ntheoretical time complexity of our proposed acquisition functions, and suggest\na practical methodology to efficiently tune the network hyper-parameters to\nachieve high model accuracy. Numerical experiments on both synthetic and\nreal-world datasets show that our proposed method can achieve better results\ncompared to existing state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 23:21:53 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Ha", "Huong", ""], ["Gupta", "Sunil", ""], ["Rana", "Santu", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "2012.09996", "submitter": "Anru R. Zhang", "authors": "Rungang Han, Yuetian Luo, Miaoyan Wang, and Anru R. Zhang", "title": "Exact Clustering in Tensor Block Model: Statistical Optimality and\n  Computational Limit", "comments": "61 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High-order clustering aims to identify heterogeneous substructure in multiway\ndataset that arises commonly in neuroimaging, genomics, and social network\nstudies. The non-convex and discontinuous nature of the problem poses\nsignificant challenges in both statistics and computation. In this paper, we\npropose a tensor block model and the computationally efficient methods,\n\\emph{high-order Lloyd algorithm} (HLloyd) and \\emph{high-order spectral\nclustering} (HSC), for high-order clustering in tensor block model. The\nconvergence of the proposed procedure is established, and we show that our\nmethod achieves exact clustering under reasonable assumptions. We also give the\ncomplete characterization for the statistical-computational trade-off in\nhigh-order clustering based on three different signal-to-noise ratio regimes.\nFinally, we show the merits of the proposed procedures via extensive\nexperiments on both synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 00:48:27 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 14:55:54 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Han", "Rungang", ""], ["Luo", "Yuetian", ""], ["Wang", "Miaoyan", ""], ["Zhang", "Anru R.", ""]]}, {"id": "2012.10047", "submitter": "Sifan Wang", "authors": "Sifan Wang, Hanwen Wang, Paris Perdikaris", "title": "On the eigenvector bias of Fourier feature networks: From regression to\n  solving multi-scale PDEs with physics-informed neural networks", "comments": "27 pages, 18 figures", "journal-ref": null, "doi": "10.1016/j.cma.2021.113938", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physics-informed neural networks (PINNs) are demonstrating remarkable promise\nin integrating physical models with gappy and noisy observational data, but\nthey still struggle in cases where the target functions to be approximated\nexhibit high-frequency or multi-scale features. In this work we investigate\nthis limitation through the lens of Neural Tangent Kernel (NTK) theory and\nelucidate how PINNs are biased towards learning functions along the dominant\neigen-directions of their limiting NTK. Using this observation, we construct\nnovel architectures that employ spatio-temporal and multi-scale random Fourier\nfeatures, and justify how such coordinate embedding layers can lead to robust\nand accurate PINN models. Numerical examples are presented for several\nchallenging cases where conventional PINN models fail, including wave\npropagation and reaction-diffusion dynamics, illustrating how the proposed\nmethods can be used to effectively tackle both forward and inverse problems\ninvolving partial differential equations with multi-scale behavior. All code an\ndata accompanying this manuscript will be made publicly available at\n\\url{https://github.com/PredictiveIntelligenceLab/MultiscalePINNs}.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 04:19:30 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Wang", "Sifan", ""], ["Wang", "Hanwen", ""], ["Perdikaris", "Paris", ""]]}, {"id": "2012.10106", "submitter": "Nicholas Kr\\\"amer", "authors": "Nicholas Kr\\\"amer and Philipp Hennig", "title": "Stable Implementation of Probabilistic ODE Solvers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Probabilistic solvers for ordinary differential equations (ODEs) provide\nefficient quantification of numerical uncertainty associated with simulation of\ndynamical systems. Their convergence rates have been established by a growing\nbody of theoretical analysis. However, these algorithms suffer from numerical\ninstability when run at high order or with small step-sizes -- that is, exactly\nin the regime in which they achieve the highest accuracy. The present work\nproposes and examines a solution to this problem. It involves three components:\naccurate initialisation, a coordinate change preconditioner that makes\nnumerical stability concerns step-size-independent, and square-root\nimplementation. Using all three techniques enables numerical computation of\nprobabilistic solutions of ODEs with algorithms of order up to 11, as\ndemonstrated on a set of challenging test problems. The resulting rapid\nconvergence is shown to be competitive to high-order, state-of-the-art,\nclassical methods. As a consequence, a barrier between analysing probabilistic\nODE solvers and applying them to interesting machine learning problems is\neffectively removed.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 08:35:36 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Kr\u00e4mer", "Nicholas", ""], ["Hennig", "Philipp", ""]]}, {"id": "2012.10141", "submitter": "Ioan Gabriel Bucur", "authors": "Ioan Gabriel Bucur, Tom Claassen and Tom Heskes", "title": "MASSIVE: Tractable and Robust Bayesian Learning of Many-Dimensional\n  Instrumental Variable Models", "comments": "14 pages, 7 figures, Published in the Proceedings of the 36th\n  Conference on Uncertainty in Artificial Intelligence (UAI)", "journal-ref": "PMLR 124:1049-1058, 2020", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent availability of huge, many-dimensional data sets, like those\narising from genome-wide association studies (GWAS), provides many\nopportunities for strengthening causal inference. One popular approach is to\nutilize these many-dimensional measurements as instrumental variables\n(instruments) for improving the causal effect estimate between other pairs of\nvariables. Unfortunately, searching for proper instruments in a\nmany-dimensional set of candidates is a daunting task due to the intractable\nmodel space and the fact that we cannot directly test which of these candidates\nare valid, so most existing search methods either rely on overly stringent\nmodeling assumptions or fail to capture the inherent model uncertainty in the\nselection process. We show that, as long as at least some of the candidates are\n(close to) valid, without knowing a priori which ones, they collectively still\npose enough restrictions on the target interaction to obtain a reliable causal\neffect estimate. We propose a general and efficient causal inference algorithm\nthat accounts for model uncertainty by performing Bayesian model averaging over\nthe most promising many-dimensional instrumental variable models, while at the\nsame time employing weaker assumptions regarding the data generating process.\nWe showcase the efficiency, robustness and predictive performance of our\nalgorithm through experimental results on both simulated and real-world data.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 10:06:55 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Bucur", "Ioan Gabriel", ""], ["Claassen", "Tom", ""], ["Heskes", "Tom", ""]]}, {"id": "2012.10167", "submitter": "Ioan Gabriel Bucur", "authors": "Ioan Gabriel Bucur, Tom Claassen and Tom Heskes", "title": "Inferring the Direction of a Causal Link and Estimating Its Effect via a\n  Bayesian Mendelian Randomization Approach", "comments": "26 pages, 22 figures, published in Statistical Methods in Medical\n  Research", "journal-ref": "Statistical Methods in Medical Research, Vol 29, Issue 4, 2020", "doi": "10.1177/0962280219851817", "report-no": null, "categories": "stat.ME q-bio.GN q-bio.QM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of genetic variants as instrumental variables - an approach known as\nMendelian randomization - is a popular epidemiological method for estimating\nthe causal effect of an exposure (phenotype, biomarker, risk factor) on a\ndisease or health-related outcome from observational data. Instrumental\nvariables must satisfy strong, often untestable assumptions, which means that\nfinding good genetic instruments among a large list of potential candidates is\nchallenging. This difficulty is compounded by the fact that many genetic\nvariants influence more than one phenotype through different causal pathways, a\nphenomenon called horizontal pleiotropy. This leads to errors not only in\nestimating the magnitude of the causal effect but also in inferring the\ndirection of the putative causal link. In this paper, we propose a Bayesian\napproach called BayesMR that is a generalization of the Mendelian randomization\ntechnique in which we allow for pleiotropic effects and, crucially, for the\npossibility of reverse causation. The output of the method is a posterior\ndistribution over the target causal effect, which provides an immediate and\neasily interpretable measure of the uncertainty in the estimation. More\nimportantly, we use Bayesian model averaging to determine how much more likely\nthe inferred direction is relative to the reverse direction.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 11:01:52 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Bucur", "Ioan Gabriel", ""], ["Claassen", "Tom", ""], ["Heskes", "Tom", ""]]}, {"id": "2012.10200", "submitter": "Sultan Javed Majeed", "authors": "Sultan Javed Majeed and Marcus Hutter", "title": "Exact Reduction of Huge Action Spaces in General Reinforcement Learning", "comments": "A variant of this paper was presented at the AAAI-2021 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY eess.SY stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The reinforcement learning (RL) framework formalizes the notion of learning\nwith interactions. Many real-world problems have large state-spaces and/or\naction-spaces such as in Go, StarCraft, protein folding, and robotics or are\nnon-Markovian, which cause significant challenges to RL algorithms. In this\nwork we address the large action-space problem by sequentializing actions,\nwhich can reduce the action-space size significantly, even down to two actions\nat the expense of an increased planning horizon. We provide explicit and exact\nconstructions and equivalence proofs for all quantities of interest for\narbitrary history-based processes. In the case of MDPs, this could help RL\nalgorithms that bootstrap. In this work we show how action-binarization in the\nnon-MDP case can significantly improve Extreme State Aggregation (ESA) bounds.\nESA allows casting any (non-MDP, non-ergodic, history-based) RL problem into a\nfixed-sized non-Markovian state-space with the help of a surrogate Markovian\nprocess. On the upside, ESA enjoys similar optimality guarantees as Markovian\nmodels do. But a downside is that the size of the aggregated state-space\nbecomes exponential in the size of the action-space. In this work, we patch\nthis issue by binarizing the action-space. We provide an upper bound on the\nnumber of states of this binarized ESA that is logarithmic in the original\naction-space size, a double-exponential improvement.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 12:45:03 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Majeed", "Sultan Javed", ""], ["Hutter", "Marcus", ""]]}, {"id": "2012.10215", "submitter": "Kentaro Minami", "authors": "Katsuya Ito and Kentaro Minami and Kentaro Imajo and Kei Nakagawa", "title": "Trader-Company Method: A Metaheuristic for Interpretable Stock Price\n  Prediction", "comments": "AAMAS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investors try to predict returns of financial assets to make successful\ninvestment. Many quantitative analysts have used machine learning-based methods\nto find unknown profitable market rules from large amounts of market data.\nHowever, there are several challenges in financial markets hindering practical\napplications of machine learning-based models. First, in financial markets,\nthere is no single model that can consistently make accurate prediction because\ntraders in markets quickly adapt to newly available information. Instead, there\nare a number of ephemeral and partially correct models called \"alpha factors\".\nSecond, since financial markets are highly uncertain, ensuring interpretability\nof prediction models is quite important to make reliable trading strategies. To\novercome these challenges, we propose the Trader-Company method, a novel\nevolutionary model that mimics the roles of a financial institute and traders\nbelonging to it. Our method predicts future stock returns by aggregating\nsuggestions from multiple weak learners called Traders. A Trader holds a\ncollection of simple mathematical formulae, each of which represents a\ncandidate of an alpha factor and would be interpretable for real-world\ninvestors. The aggregation algorithm, called a Company, maintains multiple\nTraders. By randomly generating new Traders and retraining them, Companies can\nefficiently find financially meaningful formulae whilst avoiding overfitting to\na transient state of the market. We show the effectiveness of our method by\nconducting experiments on real market data.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 13:19:27 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Ito", "Katsuya", ""], ["Minami", "Kentaro", ""], ["Imajo", "Kentaro", ""], ["Nakagawa", "Kei", ""]]}, {"id": "2012.10249", "submitter": "Ranjan Maitra", "authors": "Carlos Llosa-Vite and Ranjan Maitra", "title": "Reduced-Rank Tensor-on-Tensor Regression and Tensor-variate Analysis of\n  Variance", "comments": "30 pages, 12 figures, 2 tables, 2 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST physics.data-an stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fitting regression models with many multivariate responses and covariates can\nbe challenging, but such responses and covariates sometimes have tensor-variate\nstructure. We extend the classical multivariate regression model to exploit\nsuch structure in two ways: first, we impose four types of low-rank tensor\nformats on the regression coefficients. Second, we model the errors using the\ntensor-variate normal distribution that imposes a Kronecker separable format on\nthe covariance matrix. We obtain maximum likelihood estimators via\nblock-relaxation algorithms and derive their asymptotic distributions. Our\nregression framework enables us to formulate tensor-variate analysis of\nvariance (TANOVA) methodology. Application of our methodology in a one-way\nTANOVA layout enables us to identify cerebral regions significantly associated\nwith the interaction of suicide attempters or non-attemptor ideators and\npositive-, negative- or death-connoting words. A separate application performs\nthree-way TANOVA on the Labeled Faces in the Wild image database to distinguish\nfacial characteristics related to ethnic origin, age group and gender.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 14:04:41 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 15:57:42 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Llosa-Vite", "Carlos", ""], ["Maitra", "Ranjan", ""]]}, {"id": "2012.10264", "submitter": "Johanna Vielhaben", "authors": "Johanna Vielhaben, Nils Strodthoff", "title": "Generative Neural Samplers for the Quantum Heisenberg Chain", "comments": "10 figures", "journal-ref": "Phys. Rev. E 103, 063304 (2021)", "doi": "10.1103/PhysRevE.103.063304", "report-no": null, "categories": "cond-mat.stat-mech cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative neural samplers offer a complementary approach to Monte Carlo\nmethods for problems in statistical physics and quantum field theory. This work\ntests the ability of generative neural samplers to estimate observables for\nreal-world low-dimensional spin systems. It maps out how autoregressive models\ncan sample configurations of a quantum Heisenberg chain via a classical\napproximation based on the Suzuki-Trotter transformation. We present results\nfor energy, specific heat and susceptibility for the isotropic XXX and the\nanisotropic XY chain that are in good agreement with Monte Carlo results within\nthe same approximation scheme.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 14:28:13 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Vielhaben", "Johanna", ""], ["Strodthoff", "Nils", ""]]}, {"id": "2012.10278", "submitter": "Yue Xing", "authors": "Yue Xing, Ruizhi Zhang, Guang Cheng", "title": "Adversarially Robust Estimate and Risk Analysis in Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adversarially robust learning aims to design algorithms that are robust to\nsmall adversarial perturbations on input variables. Beyond the existing studies\non the predictive performance to adversarial samples, our goal is to understand\nstatistical properties of adversarially robust estimates and analyze\nadversarial risk in the setup of linear regression models. By discovering the\nstatistical minimax rate of convergence of adversarially robust estimators, we\nemphasize the importance of incorporating model information, e.g., sparsity, in\nadversarially robust learning. Further, we reveal an explicit connection of\nadversarial and standard estimates, and propose a straightforward two-stage\nadversarial learning framework, which facilitates to utilize model structure\ninformation to improve adversarial robustness. In theory, the consistency of\nthe adversarially robust estimator is proven and its Bahadur representation is\nalso developed for the statistical inference purpose. The proposed estimator\nconverges in a sharp rate under either low-dimensional or sparse scenario.\nMoreover, our theory confirms two phenomena in adversarially robust learning:\nadversarial robustness hurts generalization, and unlabeled data help improve\nthe generalization. In the end, we conduct numerical simulations to verify our\ntheory.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 14:55:55 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Xing", "Yue", ""], ["Zhang", "Ruizhi", ""], ["Cheng", "Guang", ""]]}, {"id": "2012.10300", "submitter": "Matthias Templ", "authors": "Matthias Templ", "title": "Artificial Neural Networks to Impute Rounded Zeros in Compositional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Methods of deep learning have become increasingly popular in recent years,\nbut they have not arrived in compositional data analysis. Imputation methods\nfor compositional data are typically applied on additive, centered or isometric\nlog-ratio representations of the data. Generally, methods for compositional\ndata analysis can only be applied to observed positive entries in a data\nmatrix. Therefore one tries to impute missing values or measurements that were\nbelow a detection limit. In this paper, a new method for imputing rounded zeros\nbased on artificial neural networks is shown and compared with conventional\nmethods. We are also interested in the question whether for ANNs, a\nrepresentation of the data in log-ratios for imputation purposes, is relevant.\nIt can be shown, that ANNs are competitive or even performing better when\nimputing rounded zeros of data sets with moderate size. They deliver better\nresults when data sets are big. Also, we can see that log-ratio transformations\nwithin the artificial neural network imputation procedure nevertheless help to\nimprove the results. This proves that the theory of compositional data analysis\nand the fulfillment of all properties of compositional data analysis is still\nvery important in the age of deep learning.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 15:31:23 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Templ", "Matthias", ""]]}, {"id": "2012.10315", "submitter": "Rahul Singh", "authors": "Rahul Singh", "title": "Kernel Methods for Unobserved Confounding: Negative Controls, Proxies,\n  and Instruments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Negative control is a strategy for learning the causal relationship between\ntreatment and outcome in the presence of unmeasured confounding. The treatment\neffect can nonetheless be identified if two auxiliary variables are available:\na negative control treatment (which has no effect on the actual outcome), and a\nnegative control outcome (which is not affected by the actual treatment). These\nauxiliary variables can also be viewed as proxies for a traditional set of\ncontrol variables, and they bear resemblance to instrumental variables. I\npropose a family of algorithms based on kernel ridge regression for learning\nnonparametric treatment effects with negative controls. Examples include dose\nresponse curves, dose response curves with distribution shift, and\nheterogeneous treatment effects. Data may be discrete or continuous, and low,\nhigh, or infinite dimensional. I prove uniform consistency and provide finite\nsample rates of convergence. I estimate the dose response curve of cigarette\nsmoking on infant birth weight adjusting for unobserved confounding due to\nhousehold income, using a data set of singleton births in the state of\nPennsylvania between 1989 and 1991.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 16:00:08 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 17:58:29 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Singh", "Rahul", ""]]}, {"id": "2012.10333", "submitter": "Sai Praneeth Karimireddy", "authors": "Sai Praneeth Karimireddy, Lie He, Martin Jaggi", "title": "Learning from History for Byzantine Robust Optimization", "comments": "ICML 2021. v2 contains stronger theory; v3 fixes some errors in the\n  proof", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Byzantine robustness has received significant attention recently given its\nimportance for distributed and federated learning. In spite of this, we\nidentify severe flaws in existing algorithms even when the data across the\nparticipants is identically distributed. First, we show realistic examples\nwhere current state of the art robust aggregation rules fail to converge even\nin the absence of any Byzantine attackers. Secondly, we prove that even if the\naggregation rules may succeed in limiting the influence of the attackers in a\nsingle round, the attackers can couple their attacks across time eventually\nleading to divergence. To address these issues, we present two surprisingly\nsimple strategies: a new robust iterative clipping procedure, and incorporating\nworker momentum to overcome time-coupled attacks. This is the first provably\nrobust method for the standard stochastic optimization setting. Our code is\nopen sourced at https://github.com/epfml/byzantine-robust-optimizer.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 16:22:32 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 05:33:29 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2021 08:07:48 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Karimireddy", "Sai Praneeth", ""], ["He", "Lie", ""], ["Jaggi", "Martin", ""]]}, {"id": "2012.10351", "submitter": "Jaron Sanders", "authors": "Oxana A. Manita, Mark A. Peletier, Jacobus W. Portegies, Jaron\n  Sanders, Albert Senen-Cerda", "title": "Universal Approximation in Dropout Neural Networks", "comments": "45 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We prove two universal approximation theorems for a range of dropout neural\nnetworks. These are feed-forward neural networks in which each edge is given a\nrandom $\\{0,1\\}$-valued filter, that have two modes of operation: in the first\neach edge output is multiplied by its random filter, resulting in a random\noutput, while in the second each edge output is multiplied by the expectation\nof its filter, leading to a deterministic output. It is common to use the\nrandom mode during training and the deterministic mode during testing and\nprediction.\n  Both theorems are of the following form: Given a function to approximate and\na threshold $\\varepsilon>0$, there exists a dropout network that is\n$\\varepsilon$-close in probability and in $L^q$. The first theorem applies to\ndropout networks in the random mode. It assumes little on the activation\nfunction, applies to a wide class of networks, and can even be applied to\napproximation schemes other than neural networks. The core is an algebraic\nproperty that shows that deterministic networks can be exactly matched in\nexpectation by random networks. The second theorem makes stronger assumptions\nand gives a stronger result. Given a function to approximate, it provides\nexistence of a network that approximates in both modes simultaneously. Proof\ncomponents are a recursive replacement of edges by independent copies, and a\nspecial first-layer replacement that couples the resulting larger network to\nthe input.\n  The functions to be approximated are assumed to be elements of general normed\nspaces, and the approximations are measured in the corresponding norms. The\nnetworks are constructed explicitly. Because of the different methods of proof,\nthe two results give independent insight into the approximation properties of\nrandom dropout networks. With this, we establish that dropout neural networks\nbroadly satisfy a universal-approximation property.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 16:53:15 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Manita", "Oxana A.", ""], ["Peletier", "Mark A.", ""], ["Portegies", "Jacobus W.", ""], ["Sanders", "Jaron", ""], ["Senen-Cerda", "Albert", ""]]}, {"id": "2012.10369", "submitter": "Benjamin Guedj", "authors": "Maxime Haddouche and Benjamin Guedj and Omar Rivasplata and John\n  Shawe-Taylor", "title": "Upper and Lower Bounds on the Performance of Kernel PCA", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Principal Component Analysis (PCA) is a popular method for dimension\nreduction and has attracted an unfailing interest for decades. Recently, kernel\nPCA has emerged as an extension of PCA but, despite its use in practice, a\nsound theoretical understanding of kernel PCA is missing. In this paper, we\ncontribute lower and upper bounds on the efficiency of kernel PCA, involving\nthe empirical eigenvalues of the kernel Gram matrix. Two bounds are for fixed\nestimators, and two are for randomized estimators through the PAC-Bayes theory.\nWe control how much information is captured by kernel PCA on average, and we\ndissect the bounds to highlight strengths and limitations of the kernel PCA\nalgorithm. Therefore, we contribute to the better understanding of kernel PCA.\nOur bounds are briefly illustrated on a toy numerical example.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 17:19:31 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Haddouche", "Maxime", ""], ["Guedj", "Benjamin", ""], ["Rivasplata", "Omar", ""], ["Shawe-Taylor", "John", ""]]}, {"id": "2012.10410", "submitter": "Gabriel Turinici", "authors": "Gabriel Turinici", "title": "Convergence dynamics of Generative Adversarial Networks: the dual metric\n  flows", "comments": null, "journal-ref": "CADL (Computational Aspects of Deep Learning) workshop held during\n  the 25th ICPR conference, Milano, Italy Jan 10-15 2021", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.FA stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Fitting neural networks often resorts to stochastic (or similar) gradient\ndescent which is a noise-tolerant (and efficient) resolution of a gradient\ndescent dynamics. It outputs a sequence of networks parameters, which sequence\nevolves during the training steps. The gradient descent is the limit, when the\nlearning rate is small and the batch size is infinite, of this set of\nincreasingly optimal network parameters obtained during training. In this\ncontribution, we investigate instead the convergence in the Generative\nAdversarial Networks used in machine learning. We study the limit of small\nlearning rate, and show that, similar to single network training, the GAN\nlearning dynamics tend, for vanishing learning rate to some limit dynamics.\nThis leads us to consider evolution equations in metric spaces (which is the\nnatural framework for evolving probability laws) that we call dual flows. We\ngive formal definitions of solutions and prove the convergence. The theory is\nthen applied to specific instances of GANs and we discuss how this insight\nhelps understand and mitigate the mode collapse.\n  Keywords: GAN; metric flow; generative network\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 18:00:12 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 16:59:17 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Turinici", "Gabriel", ""]]}, {"id": "2012.10469", "submitter": "Dan Garber", "authors": "Dan Garber, Atara Kaplan", "title": "On the Efficient Implementation of the Matrix Exponentiated Gradient\n  Algorithm for Low-Rank Matrix Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convex optimization over the spectrahedron, i.e., the set of all real\n$n\\times n$ positive semidefinite matrices with unit trace, has important\napplications in machine learning, signal processing and statistics, mainly as a\nconvex relaxation for optimization with low-rank matrices. It is also one of\nthe most prominent examples in the theory of first-order methods for convex\noptimization in which non-Euclidean methods can be significantly preferable to\ntheir Euclidean counterparts, and in particular the Matrix Exponentiated\nGradient (MEG) method which is based on the Bregman distance induced by the\n(negative) von Neumann entropy. Unfortunately, implementing MEG requires a full\nSVD computation on each iteration, which is not scalable to high-dimensional\nproblems.\n  In this work we propose efficient implementations of MEG, both with\ndeterministic and stochastic gradients, which are tailored for optimization\nwith low-rank matrices, and only use a single low-rank SVD computation on each\niteration. We also provide efficiently-computable certificates for the correct\nconvergence of our methods. Mainly, we prove that under a strict\ncomplementarity condition, the suggested methods converge from a \"warm-start\"\ninitialization with similar rates to their full-SVD-based counterparts.\nFinally, we bring empirical experiments which both support our theoretical\nfindings and demonstrate the practical appeal of our methods.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 19:14:51 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Garber", "Dan", ""], ["Kaplan", "Atara", ""]]}, {"id": "2012.10523", "submitter": "Staal Vinterbo PhD", "authors": "Staal A. Vinterbo", "title": "A closed form scale bound for the $(\\epsilon, \\delta)$-differentially\n  private Gaussian Mechanism valid for all privacy regimes", "comments": "11 pages. Version 2 improves on the bound", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard closed form lower bound on $\\sigma$ for providing $(\\epsilon,\n\\delta)$-differential privacy by adding zero mean Gaussian noise with variance\n$\\sigma^2$ is $\\sigma > \\Delta\\sqrt {2}(\\epsilon^{-1}) \\sqrt {\\log \\left(\n5/4\\delta^{-1} \\right)}$ for $\\epsilon \\in (0,1)$. We present a similar closed\nform bound $\\sigma \\geq \\Delta (\\epsilon\\sqrt{2})^{-1} \\left(\\sqrt{az+\\epsilon}\n+ s\\sqrt{az}\\right)$ for $z=-\\log(4\\delta(1-\\delta))$ and $(a,s)=(1,1)$ if\n$\\delta \\leq 1/2$ and $(a,s)=(\\pi/4,-1)$ otherwise. Our bound is valid for all\n$\\epsilon > 0$ and is always lower (better). We also present a sufficient\ncondition for $(\\epsilon, \\delta)$-differential privacy when adding noise\ndistributed according to even and log-concave densities supported everywhere.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 21:35:26 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2021 14:01:55 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Vinterbo", "Staal A.", ""]]}, {"id": "2012.10559", "submitter": "Tyler McCormick", "authors": "Shane Lubold, Arun G. Chandrasekhar, Tyler H. McCormick", "title": "Identifying the latent space geometry of network models through analysis\n  of curvature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI math.GT stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Statistically modeling networks, across numerous disciplines and contexts, is\nfundamentally challenging because of (often high-order) dependence between\nconnections. A common approach assigns each person in the graph to a position\non a low-dimensional manifold. Distance between individuals in this (latent)\nspace is inversely proportional to the likelihood of forming a connection. The\nchoice of the latent geometry (the manifold class, dimension, and curvature)\nhas consequential impacts on the substantive conclusions of the model. More\npositive curvature in the manifold, for example, encourages more and tighter\ncommunities; negative curvature induces repulsion among nodes. Currently,\nhowever, the choice of the latent geometry is an a priori modeling assumption\nand there is limited guidance about how to make these choices in a data-driven\nway. In this work, we present a method to consistently estimate the manifold\ntype, dimension, and curvature from an empirically relevant class of latent\nspaces: simply connected, complete Riemannian manifolds of constant curvature.\nOur core insight comes by representing the graph as a noisy distance matrix\nbased on the ties between cliques. Leveraging results from statistical\ngeometry, we develop hypothesis tests to determine whether the observed\ndistances could plausibly be embedded isometrically in each of the candidate\ngeometries. We explore the accuracy of our approach with simulations and then\napply our approach to data-sets from economics and sociology as well as\nneuroscience.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 00:35:29 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 22:40:45 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 21:47:53 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Lubold", "Shane", ""], ["Chandrasekhar", "Arun G.", ""], ["McCormick", "Tyler H.", ""]]}, {"id": "2012.10569", "submitter": "Shelby Heinecke", "authors": "Avrim Blum, Shelby Heinecke, Lev Reyzin", "title": "Communication-Aware Collaborative Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms for noiseless collaborative PAC learning have been analyzed and\noptimized in recent years with respect to sample complexity. In this paper, we\nstudy collaborative PAC learning with the goal of reducing communication cost\nat essentially no penalty to the sample complexity. We develop communication\nefficient collaborative PAC learning algorithms using distributed boosting. We\nthen consider the communication cost of collaborative learning in the presence\nof classification noise. As an intermediate step, we show how collaborative PAC\nlearning algorithms can be adapted to handle classification noise. With this\ninsight, we develop communication efficient algorithms for collaborative PAC\nlearning robust to classification noise.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 01:47:02 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Blum", "Avrim", ""], ["Heinecke", "Shelby", ""], ["Reyzin", "Lev", ""]]}, {"id": "2012.10602", "submitter": "Kaiwen Wang", "authors": "Kaiwen Wang, Travis Dick, Maria-Florina Balcan", "title": "Scalable and Provably Accurate Algorithms for Differentially Private\n  Distributed Decision Tree Learning", "comments": "In AAAI Workshop on Privacy-Preserving Artificial Intelligence, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces the first provably accurate algorithms for\ndifferentially private, top-down decision tree learning in the distributed\nsetting (Balcan et al., 2012). We propose DP-TopDown, a general privacy\npreserving decision tree learning algorithm, and present two distributed\nimplementations. Our first method NoisyCounts naturally extends the single\nmachine algorithm by using the Laplace mechanism. Our second method LocalRNM\nsignificantly reduces communication and added noise by performing local\noptimization at each data holder. We provide the first utility guarantees for\ndifferentially private top-down decision tree learning in both the single\nmachine and distributed settings. These guarantees show that the error of the\nprivately-learned decision tree quickly goes to zero provided that the dataset\nis sufficiently large. Our extensive experiments on real datasets illustrate\nthe trade-offs of privacy, accuracy and generalization when learning private\ndecision trees in the distributed setting.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 06:09:36 GMT"}, {"version": "v2", "created": "Sat, 20 Feb 2021 00:43:22 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2021 03:24:20 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Wang", "Kaiwen", ""], ["Dick", "Travis", ""], ["Balcan", "Maria-Florina", ""]]}, {"id": "2012.10650", "submitter": "Yejiang Wang", "authors": "Yejiang Wang and Yuhai Zhao and Zhengkui Wang and Chengqi Zhang", "title": "Towards Coarse and Fine-grained Multi-Graph Multi-Label Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-graph multi-label learning (\\textsc{Mgml}) is a supervised learning\nframework, which aims to learn a multi-label classifier from a set of labeled\nbags each containing a number of graphs. Prior techniques on the \\textsc{Mgml}\nare developed based on transfering graphs into instances and focus on learning\nthe unseen labels only at the bag level. In this paper, we propose a\n\\textit{coarse} and \\textit{fine-grained} Multi-graph Multi-label (cfMGML)\nlearning framework which directly builds the learning model over the graphs and\nempowers the label prediction at both the \\textit{coarse} (aka. bag) level and\n\\textit{fine-grained} (aka. graph in each bag) level. In particular, given a\nset of labeled multi-graph bags, we design the scoring functions at both graph\nand bag levels to model the relevance between the label and data using specific\ngraph kernels. Meanwhile, we propose a thresholding rank-loss objective\nfunction to rank the labels for the graphs and bags and minimize the\nhamming-loss simultaneously at one-step, which aims to addresses the error\naccumulation issue in traditional rank-loss algorithms. To tackle the\nnon-convex optimization problem, we further develop an effective sub-gradient\ndescent algorithm to handle high-dimensional space computation required in\ncfMGML. Experiments over various real-world datasets demonstrate cfMGML\nachieves superior performance than the state-of-arts algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 10:34:21 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Wang", "Yejiang", ""], ["Zhao", "Yuhai", ""], ["Wang", "Zhengkui", ""], ["Zhang", "Chengqi", ""]]}, {"id": "2012.10688", "submitter": "Quoc Phong Nguyen", "authors": "Quoc Phong Nguyen, Sebastian Tay, Bryan Kian Hsiang Low, Patrick\n  Jaillet", "title": "Top-$k$ Ranking Bayesian Optimization", "comments": "35th AAAI Conference on Artificial Intelligence (AAAI 2021), Extended\n  version with derivations, 13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach to top-$k$ ranking Bayesian optimization\n(top-$k$ ranking BO) which is a practical and significant generalization of\npreferential BO to handle top-$k$ ranking and tie/indifference observations. We\nfirst design a surrogate model that is not only capable of catering to the\nabove observations, but is also supported by a classic random utility model.\nAnother equally important contribution is the introduction of the first\ninformation-theoretic acquisition function in BO with preferential observation\ncalled multinomial predictive entropy search (MPES) which is flexible in\nhandling these observations and optimized for all inputs of a query jointly.\nMPES possesses superior performance compared with existing acquisition\nfunctions that select the inputs of a query one at a time greedily. We\nempirically evaluate the performance of MPES using several synthetic benchmark\nfunctions, CIFAR-$10$ dataset, and SUSHI preference dataset.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 13:49:41 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Nguyen", "Quoc Phong", ""], ["Tay", "Sebastian", ""], ["Low", "Bryan Kian Hsiang", ""], ["Jaillet", "Patrick", ""]]}, {"id": "2012.10695", "submitter": "Quoc Phong Nguyen", "authors": "Quoc Phong Nguyen, Bryan Kian Hsiang Low, Patrick Jaillet", "title": "An Information-Theoretic Framework for Unifying Active Learning Problems", "comments": "35th AAAI Conference on Artificial Intelligence (AAAI 2021), Extended\n  version with derivations, 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an information-theoretic framework for unifying active\nlearning problems: level set estimation (LSE), Bayesian optimization (BO), and\ntheir generalized variant. We first introduce a novel active learning criterion\nthat subsumes an existing LSE algorithm and achieves state-of-the-art\nperformance in LSE problems with a continuous input domain. Then, by exploiting\nthe relationship between LSE and BO, we design a competitive\ninformation-theoretic acquisition function for BO that has interesting\nconnections to upper confidence bound and max-value entropy search (MES). The\nlatter connection reveals a drawback of MES which has important implications on\nnot only MES but also on other MES-based acquisition functions. Finally, our\nunifying information-theoretic framework can be applied to solve a generalized\nproblem of LSE and BO involving multiple level sets in a data-efficient manner.\nWe empirically evaluate the performance of our proposed algorithms using\nsynthetic benchmark functions, a real-world dataset, and in hyperparameter\ntuning of machine learning models.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 14:22:48 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Nguyen", "Quoc Phong", ""], ["Low", "Bryan Kian Hsiang", ""], ["Jaillet", "Patrick", ""]]}, {"id": "2012.10713", "submitter": "Han Zhao", "authors": "Han Zhao, Chen Dan, Bryon Aragam, Tommi S. Jaakkola, Geoffrey J.\n  Gordon, Pradeep Ravikumar", "title": "Fundamental Limits and Tradeoffs in Invariant Representation Learning", "comments": "Updated results in the regression setting to fully characterize the\n  frontier. Additional numerical experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning applications, e.g., privacy-preserving learning,\nalgorithmic fairness and domain adaptation/generalization, involve learning the\nso-called invariant representations that achieve two competing goals: To\nmaximize information or accuracy with respect to a target while simultaneously\nmaximizing invariance or independence with respect to a set of protected\nfeatures (e.g.\\ for fairness, privacy, etc). Despite its abundant applications\nin the aforementioned domains, theoretical understanding on the limits and\ntradeoffs of invariant representations is still severely lacking. In this\npaper, we provide an information theoretic analysis of this general and\nimportant problem under both classification and regression settings. In both\ncases, we analyze the inherent tradeoffs between accuracy and invariance by\nproviding a geometric characterization of the feasible region in the\ninformation plane, where we connect the geometric properties of this feasible\nregion to the fundamental limitations of the tradeoff problem. In the\nregression setting, we further give a complete and exact characterization of\nthe frontier between accuracy and invariance. Although our contributions are\nmainly theoretical, we also demonstrate the practical applications of our\nresults in certifying the suboptimality of certain representation learning\nalgorithms in both classification and regression tasks. Our results shed new\nlight on this fundamental problem by providing insights on the interplay\nbetween accuracy and invariance. These results deepen our understanding of this\nfundamental problem and may be useful in guiding the design of future\nrepresentation learning algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 15:24:04 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 17:27:12 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Zhao", "Han", ""], ["Dan", "Chen", ""], ["Aragam", "Bryon", ""], ["Jaakkola", "Tommi S.", ""], ["Gordon", "Geoffrey J.", ""], ["Ravikumar", "Pradeep", ""]]}, {"id": "2012.10737", "submitter": "Dai Feng", "authors": "Dai Feng and Richard Baumgartner", "title": "(Decision and regression) tree ensemble based kernels for regression and\n  classification", "comments": "arXiv admin note: substantial text overlap with arXiv:2009.00089", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tree based ensembles such as Breiman's random forest (RF) and Gradient\nBoosted Trees (GBT) can be interpreted as implicit kernel generators, where the\nensuing proximity matrix represents the data-driven tree ensemble kernel.\nKernel perspective on the RF has been used to develop a principled framework\nfor theoretical investigation of its statistical properties. Recently, it has\nbeen shown that the kernel interpretation is germane to other tree-based\nensembles e.g. GBTs. However, practical utility of the links between kernels\nand the tree ensembles has not been widely explored and systematically\nevaluated.\n  Focus of our work is investigation of the interplay between kernel methods\nand the tree based ensembles including the RF and GBT. We elucidate the\nperformance and properties of the RF and GBT based kernels in a comprehensive\nsimulation study comprising of continuous and binary targets. We show that for\ncontinuous targets, the RF/GBT kernels are competitive to their respective\nensembles in higher dimensional scenarios, particularly in cases with larger\nnumber of noisy features. For the binary target, the RF/GBT kernels and their\nrespective ensembles exhibit comparable performance. We provide the results\nfrom real life data sets for regression and classification to show how these\ninsights may be leveraged in practice. Overall, our results support the tree\nensemble based kernels as a valuable addition to the practitioner's toolbox.\n  Finally, we discuss extensions of the tree ensemble based kernels for\nsurvival targets, interpretable prototype and landmarking classification and\nregression. We outline future line of research for kernels furnished by\nBayesian counterparts of the frequentist tree ensembles.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 16:52:58 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Feng", "Dai", ""], ["Baumgartner", "Richard", ""]]}, {"id": "2012.10791", "submitter": "James Queeney", "authors": "James Queeney, Ioannis Ch. Paschalidis, Christos G. Cassandras", "title": "Uncertainty-Aware Policy Optimization: A Robust, Adaptive Trust Region\n  Approach", "comments": "To appear in Proceedings of the Thirty-Fifth AAAI Conference on\n  Artificial Intelligence (AAAI-21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order for reinforcement learning techniques to be useful in real-world\ndecision making processes, they must be able to produce robust performance from\nlimited data. Deep policy optimization methods have achieved impressive results\non complex tasks, but their real-world adoption remains limited because they\noften require significant amounts of data to succeed. When combined with small\nsample sizes, these methods can result in unstable learning due to their\nreliance on high-dimensional sample-based estimates. In this work, we develop\ntechniques to control the uncertainty introduced by these estimates. We\nleverage these techniques to propose a deep policy optimization approach\ndesigned to produce stable performance even when data is scarce. The resulting\nalgorithm, Uncertainty-Aware Trust Region Policy Optimization, generates robust\npolicy updates that adapt to the level of uncertainty present throughout the\nlearning process.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 21:51:23 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Queeney", "James", ""], ["Paschalidis", "Ioannis Ch.", ""], ["Cassandras", "Christos G.", ""]]}, {"id": "2012.10793", "submitter": "Jie Shen", "authors": "Jie Shen", "title": "On the Power of Localized Perceptron for Label-Optimal Learning of\n  Halfspaces with Adversarial Noise", "comments": "V2 and V3 polished writing; accepted to ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study {\\em online} active learning of homogeneous halfspaces in\n$\\mathbb{R}^d$ with adversarial noise where the overall probability of a noisy\nlabel is constrained to be at most $\\nu$. Our main contribution is a\nPerceptron-like online active learning algorithm that runs in polynomial time,\nand under the conditions that the marginal distribution is isotropic\nlog-concave and $\\nu = \\Omega(\\epsilon)$, where $\\epsilon \\in (0, 1)$ is the\ntarget error rate, our algorithm PAC learns the underlying halfspace with\nnear-optimal label complexity of $\\tilde{O}\\big(d \\cdot\npolylog(\\frac{1}{\\epsilon})\\big)$ and sample complexity of\n$\\tilde{O}\\big(\\frac{d}{\\epsilon} \\big)$. Prior to this work, existing online\nalgorithms designed for tolerating the adversarial noise are subject to either\nlabel complexity polynomial in $\\frac{1}{\\epsilon}$, or suboptimal noise\ntolerance, or restrictive marginal distributions. With the additional prior\nknowledge that the underlying halfspace is $s$-sparse, we obtain\nattribute-efficient label complexity of $\\tilde{O}\\big( s \\cdot polylog(d,\n\\frac{1}{\\epsilon}) \\big)$ and sample complexity of\n$\\tilde{O}\\big(\\frac{s}{\\epsilon} \\cdot polylog(d) \\big)$. As an immediate\ncorollary, we show that under the agnostic model where no assumption is made on\nthe noise rate $\\nu$, our active learner achieves an error rate of $O(OPT) +\n\\epsilon$ with the same running time and label and sample complexity, where\n$OPT$ is the best possible error rate achievable by any homogeneous halfspace.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 22:04:10 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 04:34:16 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 20:47:56 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Shen", "Jie", ""]]}, {"id": "2012.10794", "submitter": "Robi Bhattacharjee", "authors": "Robi Bhattacharjee, Somesh Jha, Kamalika Chaudhuri", "title": "Sample Complexity of Adversarially Robust Linear Classification on\n  Separated Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the sample complexity of learning with adversarial robustness.\nMost prior theoretical results for this problem have considered a setting where\ndifferent classes in the data are close together or overlapping. Motivated by\nsome real applications, we consider, in contrast, the well-separated case where\nthere exists a classifier with perfect accuracy and robustness, and show that\nthe sample complexity narrates an entirely different story. Specifically, for\nlinear classifiers, we show a large class of well-separated distributions where\nthe expected robust loss of any algorithm is at least $\\Omega(\\frac{d}{n})$,\nwhereas the max margin algorithm has expected standard loss $O(\\frac{1}{n})$.\nThis shows a gap in the standard and robust losses that cannot be obtained via\nprior techniques. Additionally, we present an algorithm that, given an instance\nwhere the robustness radius is much smaller than the gap between the classes,\ngives a solution with expected robust loss is $O(\\frac{1}{n})$. This shows that\nfor very well-separated data, convergence rates of $O(\\frac{1}{n})$ are\nachievable, which is not the case otherwise. Our results apply to robustness\nmeasured in any $\\ell_p$ norm with $p > 1$ (including $p = \\infty$).\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 22:04:59 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Bhattacharjee", "Robi", ""], ["Jha", "Somesh", ""], ["Chaudhuri", "Kamalika", ""]]}, {"id": "2012.10885", "submitter": "Hyunjik Kim", "authors": "Michael Hutchinson, Charline Le Lan, Sheheryar Zaidi, Emilien Dupont,\n  Yee Whye Teh, Hyunjik Kim", "title": "LieTransformer: Equivariant self-attention for Lie Groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group equivariant neural networks are used as building blocks of group\ninvariant neural networks, which have been shown to improve generalisation\nperformance and data efficiency through principled parameter sharing. Such\nworks have mostly focused on group equivariant convolutions, building on the\nresult that group equivariant linear maps are necessarily convolutions. In this\nwork, we extend the scope of the literature to self-attention, that is emerging\nas a prominent building block of deep learning models. We propose the\nLieTransformer, an architecture composed of LieSelfAttention layers that are\nequivariant to arbitrary Lie groups and their discrete subgroups. We\ndemonstrate the generality of our approach by showing experimental results that\nare competitive to baseline methods on a wide range of tasks: shape counting on\npoint clouds, molecular property regression and modelling particle trajectories\nunder Hamiltonian dynamics.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 11:02:49 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 12:02:08 GMT"}, {"version": "v3", "created": "Thu, 8 Apr 2021 16:19:14 GMT"}, {"version": "v4", "created": "Wed, 16 Jun 2021 14:08:32 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Hutchinson", "Michael", ""], ["Lan", "Charline Le", ""], ["Zaidi", "Sheheryar", ""], ["Dupont", "Emilien", ""], ["Teh", "Yee Whye", ""], ["Kim", "Hyunjik", ""]]}, {"id": "2012.10903", "submitter": "Lorenzo Pacchiardi", "authors": "Lorenzo Pacchiardi, Ritabrata Dutta", "title": "Score Matched Conditional Exponential Families for Likelihood-Free\n  Inference", "comments": "48 pages, 12 figures. Code for reproducing the experiments is\n  available at http://github.com/LoryPack/SM-ExpFam-LFI", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To perform Bayesian inference for stochastic simulator models for which the\nlikelihood is not accessible, Likelihood-Free Inference (LFI) relies on\nsimulations from the model. Standard LFI methods can be split according to how\nthese simulations are used: to build an explicit Surrogate Likelihood, or to\naccept/reject parameter values according to a measure of distance from the\nobservations (Approximate Bayesian Computation (ABC)). In both cases,\nsimulations are adaptively tailored to the value of the observation. Here, we\ngenerate parameter-simulation pairs from the model independently on the\nobservation, and use them to learn a conditional exponential family likelihood\napproximation; to parametrize it, we use Neural Networks whose weights are\ntuned with Score Matching. With our likelihood approximation, we can employ\nMCMC for doubly intractable distributions to draw samples from the posterior\nfor any number of observations without additional model simulations, with\nperformance competitive to comparable approaches. Further, the sufficient\nstatistics of the exponential family can be used as summaries in ABC,\noutperforming the state-of-the-art method in five different models with known\nlikelihood. Finally, we apply our method to a challenging model from\nmeteorology.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 11:57:30 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 09:18:48 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Pacchiardi", "Lorenzo", ""], ["Dutta", "Ritabrata", ""]]}, {"id": "2012.10923", "submitter": "Christian Tomani", "authors": "Christian Tomani, Florian Buettner", "title": "Towards Trustworthy Predictions from Deep Neural Networks with Fast\n  Adversarial Calibration", "comments": "In Thirty-Fifth AAAI Conference on Artificial Intelligence\n  (AAAI-2021). Code available at https://github.com/tochris/falcon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To facilitate a wide-spread acceptance of AI systems guiding decision making\nin real-world applications, trustworthiness of deployed models is key. That is,\nit is crucial for predictive models to be uncertainty-aware and yield\nwell-calibrated (and thus trustworthy) predictions for both in-domain samples\nas well as under domain shift. Recent efforts to account for predictive\nuncertainty include post-processing steps for trained neural networks, Bayesian\nneural networks as well as alternative non-Bayesian approaches such as ensemble\napproaches and evidential deep learning. Here, we propose an efficient yet\ngeneral modelling approach for obtaining well-calibrated, trustworthy\nprobabilities for samples obtained after a domain shift. We introduce a new\ntraining strategy combining an entropy-encouraging loss term with an\nadversarial calibration loss term and demonstrate that this results in\nwell-calibrated and technically trustworthy predictions for a wide range of\ndomain drifts. We comprehensively evaluate previously proposed approaches on\ndifferent data modalities, a large range of data sets including sequence data,\nnetwork architectures and perturbation strategies. We observe that our\nmodelling approach substantially outperforms existing state-of-the-art\napproaches, yielding well-calibrated predictions under domain drift.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 13:39:29 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 19:27:46 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Tomani", "Christian", ""], ["Buettner", "Florian", ""]]}, {"id": "2012.10929", "submitter": "Saptarshi Chakraborty", "authors": "Saptarshi Chakraborty, Debolina Paul and Swagatam Das", "title": "Automated Clustering of High-dimensional Data with a Feature Weighted\n  Mean Shift Algorithm", "comments": "To appear at the 35-th AAAI Conference on Artificial Intelligence,\n  February 2-9, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean shift is a simple interactive procedure that gradually shifts data\npoints towards the mode which denotes the highest density of data points in the\nregion. Mean shift algorithms have been effectively used for data denoising,\nmode seeking, and finding the number of clusters in a dataset in an automated\nfashion. However, the merits of mean shift quickly fade away as the data\ndimensions increase and only a handful of features contain useful information\nabout the cluster structure of the data. We propose a simple yet elegant\nfeature-weighted variant of mean shift to efficiently learn the feature\nimportance and thus, extending the merits of mean shift to high-dimensional\ndata. The resulting algorithm not only outperforms the conventional mean shift\nclustering procedure but also preserves its computational simplicity. In\naddition, the proposed method comes with rigorous theoretical convergence\nguarantees and a convergence rate of at least a cubic order. The efficacy of\nour proposal is thoroughly assessed through experimental comparison against\nbaseline and state-of-the-art clustering methods on synthetic as well as\nreal-world datasets.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 14:00:40 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 15:25:27 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Chakraborty", "Saptarshi", ""], ["Paul", "Debolina", ""], ["Das", "Swagatam", ""]]}, {"id": "2012.10931", "submitter": "Fengxiang He", "authors": "Fengxiang He, Dacheng Tao", "title": "Recent advances in deep learning theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is usually described as an experiment-driven field under\ncontinuous criticizes of lacking theoretical foundations. This problem has been\npartially fixed by a large volume of literature which has so far not been well\norganized. This paper reviews and organizes the recent advances in deep\nlearning theory. The literature is categorized in six groups: (1) complexity\nand capacity-based approaches for analyzing the generalizability of deep\nlearning; (2) stochastic differential equations and their dynamic systems for\nmodelling stochastic gradient descent and its variants, which characterize the\noptimization and generalization of deep learning, partially inspired by\nBayesian inference; (3) the geometrical structures of the loss landscape that\ndrives the trajectories of the dynamic systems; (4) the roles of\nover-parameterization of deep neural networks from both positive and negative\nperspectives; (5) theoretical foundations of several special structures in\nnetwork architectures; and (6) the increasingly intensive concerns in ethics\nand security and their relationships with generalizability.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 14:16:41 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 09:16:45 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["He", "Fengxiang", ""], ["Tao", "Dacheng", ""]]}, {"id": "2012.10943", "submitter": "Torben Sell", "authors": "Torben Sell, Sumeetpal S. Singh", "title": "Dimension-robust Function Space MCMC With Neural Network Priors", "comments": "24 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a new prior on functions spaces which scales more\nfavourably in the dimension of the function's domain compared to the usual\nKarhunen-Lo\\'eve function space prior, a property we refer to as\ndimension-robustness. The proposed prior is a Bayesian neural network prior,\nwhere each weight and bias has an independent Gaussian prior, but with the key\ndifference that the variances decrease in the width of the network, such that\nthe variances form a summable sequence and the infinite width limit neural\nnetwork is well defined. We show that our resulting posterior of the unknown\nfunction is amenable to sampling using Hilbert space Markov chain Monte Carlo\nmethods. These sampling methods are favoured because they are stable under\nmesh-refinement, in the sense that the acceptance probability does not shrink\nto 0 as more parameters are introduced to better approximate the well-defined\ninfinite limit. We show that our priors are competitive and have distinct\nadvantages over other function space priors. Upon defining a suitable\nlikelihood for continuous value functions in a Bayesian approach to\nreinforcement learning, our new prior is used in numerical examples to\nillustrate its performance and dimension-robustness.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 14:52:57 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Sell", "Torben", ""], ["Singh", "Sumeetpal S.", ""]]}, {"id": "2012.10945", "submitter": "V. Roshan Joseph", "authors": "V. Roshan Joseph and Akhil Vakayil", "title": "SPlit: An Optimal Method for Data Splitting", "comments": null, "journal-ref": null, "doi": "10.1080/00401706.2021.1921037", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this article we propose an optimal method referred to as SPlit for\nsplitting a dataset into training and testing sets. SPlit is based on the\nmethod of Support Points (SP), which was initially developed for finding the\noptimal representative points of a continuous distribution. We adapt SP for\nsubsampling from a dataset using a sequential nearest neighbor algorithm. We\nalso extend SP to deal with categorical variables so that SPlit can be applied\nto both regression and classification problems. The implementation of SPlit on\nreal datasets shows substantial improvement in the worst-case testing\nperformance for several modeling methods compared to the commonly used random\nsplitting procedure.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 14:54:29 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 18:15:49 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Joseph", "V. Roshan", ""], ["Vakayil", "Akhil", ""]]}, {"id": "2012.10985", "submitter": "Ori Kelner", "authors": "Ori Kelner", "title": "Learning Halfspaces With Membership Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning is a subfield of machine learning, in which the learning\nalgorithm is allowed to choose the data from which it learns. In some cases, it\nhas been shown that active learning can yield an exponential gain in the number\nof samples the algorithm needs to see, in order to reach generalization error\n$\\leq \\epsilon$. In this work we study the problem of learning halfspaces with\nmembership queries. In the membership query scenario, we allow the learning\nalgorithm to ask for the label of every sample in the input space. We suggest a\nnew algorithm for this problem, and prove it achieves a near optimal label\ncomplexity in some cases. We also show that the algorithm works well in\npractice, and significantly outperforms uncertainty sampling.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 18:02:47 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Kelner", "Ori", ""]]}, {"id": "2012.10988", "submitter": "Christian Tomani", "authors": "Christian Tomani, Sebastian Gruber, Muhammed Ebrar Erdem, Daniel\n  Cremers, Florian Buettner", "title": "Post-hoc Uncertainty Calibration for Domain Drift Scenarios", "comments": "In Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR), 2021. Code available at\n  https://github.com/tochris/calibration-domain-drift", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of uncertainty calibration. While standard deep neural\nnetworks typically yield uncalibrated predictions, calibrated confidence scores\nthat are representative of the true likelihood of a prediction can be achieved\nusing post-hoc calibration methods. However, to date the focus of these\napproaches has been on in-domain calibration. Our contribution is two-fold.\nFirst, we show that existing post-hoc calibration methods yield highly\nover-confident predictions under domain shift. Second, we introduce a simple\nstrategy where perturbations are applied to samples in the validation set\nbefore performing the post-hoc calibration step. In extensive experiments, we\ndemonstrate that this perturbation step results in substantially better\ncalibration under domain shift on a wide range of architectures and modelling\ntasks.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 18:21:13 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 15:01:51 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Tomani", "Christian", ""], ["Gruber", "Sebastian", ""], ["Erdem", "Muhammed Ebrar", ""], ["Cremers", "Daniel", ""], ["Buettner", "Florian", ""]]}, {"id": "2012.11035", "submitter": "Shahab Asoodeh", "authors": "Shahab Asoodeh, Mario Diaz, and Flavio P. Calmon", "title": "Privacy Analysis of Online Learning Algorithms via Contraction\n  Coefficients", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.LG math.IT stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an information-theoretic technique for analyzing privacy\nguarantees of online algorithms. Specifically, we demonstrate that differential\nprivacy guarantees of iterative algorithms can be determined by a direct\napplication of contraction coefficients derived from strong data processing\ninequalities for $f$-divergences. Our technique relies on generalizing the\nDobrushin's contraction coefficient for total variation distance to an\n$f$-divergence known as $E_\\gamma$-divergence. $E_\\gamma$-divergence, in turn,\nis equivalent to approximate differential privacy. As an example, we apply our\ntechnique to derive the differential privacy parameters of gradient descent.\nMoreover, we also show that this framework can be tailored to batch learning\nalgorithms that can be implemented with one pass over the training dataset.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 22:02:15 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Asoodeh", "Shahab", ""], ["Diaz", "Mario", ""], ["Calmon", "Flavio P.", ""]]}, {"id": "2012.11048", "submitter": "Panagiotis Traganitis", "authors": "Panagiotis A. Traganitis and Georgios B. Giannakis", "title": "Bayesian Crowdsourcing with Constraints", "comments": "To appear in ECML-PKDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing has emerged as a powerful paradigm for efficiently labeling\nlarge datasets and performing various learning tasks, by leveraging crowds of\nhuman annotators. When additional information is available about the data,\nsemi-supervised crowdsourcing approaches that enhance the aggregation of labels\nfrom human annotators are well motivated. This work deals with semi-supervised\ncrowdsourced classification, under two regimes of semi-supervision: a) label\nconstraints, that provide ground-truth labels for a subset of data; and b)\npotentially easier to obtain instance-level constraints, that indicate\nrelationships between pairs of data. Bayesian algorithms based on variational\ninference are developed for each regime, and their quantifiably improved\nperformance, compared to unsupervised crowdsourcing, is analytically and\nempirically validated on several crowdsourcing datasets.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 23:18:51 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 16:56:01 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Traganitis", "Panagiotis A.", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "2012.11066", "submitter": "Angela Zhou", "authors": "Nathan Kallus, Angela Zhou", "title": "Fairness, Welfare, and Equity in Personalized Pricing", "comments": "Accepted at FAccT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the interplay of fairness, welfare, and equity considerations in\npersonalized pricing based on customer features. Sellers are increasingly able\nto conduct price personalization based on predictive modeling of demand\nconditional on covariates: setting customized interest rates, targeted\ndiscounts of consumer goods, and personalized subsidies of scarce resources\nwith positive externalities like vaccines and bed nets. These different\napplication areas may lead to different concerns around fairness, welfare, and\nequity on different objectives: price burdens on consumers, price envy, firm\nrevenue, access to a good, equal access, and distributional consequences when\nthe good in question further impacts downstream outcomes of interest. We\nconduct a comprehensive literature review in order to disentangle these\ndifferent normative considerations and propose a taxonomy of different\nobjectives with mathematical definitions. We focus on observational metrics\nthat do not assume access to an underlying valuation distribution which is\neither unobserved due to binary feedback or ill-defined due to overriding\nbehavioral concerns regarding interpreting revealed preferences. In the setting\nof personalized pricing for the provision of goods with positive benefits, we\ndiscuss how price optimization may provide unambiguous benefit by achieving a\n\"triple bottom line\": personalized pricing enables expanding access, which in\nturn may lead to gains in welfare due to heterogeneous utility, and improve\nrevenue or budget utilization. We empirically demonstrate the potential\nbenefits of personalized pricing in two settings: pricing subsidies for an\nelective vaccine, and the effects of personalized interest rates on downstream\noutcomes in microcredit.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 01:01:56 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2020 17:21:29 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Kallus", "Nathan", ""], ["Zhou", "Angela", ""]]}, {"id": "2012.11094", "submitter": "Jianfeng Lu", "authors": "Jianfeng Lu and Lihan Wang", "title": "Complexity of zigzag sampling algorithm for strongly log-concave\n  distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational complexity of zigzag sampling algorithm for\nstrongly log-concave distributions. The zigzag process has the advantage of not\nrequiring time discretization for implementation, and that each proposed\nbouncing event requires only one evaluation of partial derivative of the\npotential, while its convergence rate is dimension independent. Using these\nproperties, we prove that the zigzag sampling algorithm achieves $\\varepsilon$\nerror in chi-square divergence with a computational cost equivalent to\n$O\\bigl(\\kappa^2 d^\\frac{1}{2}(\\log\\frac{1}{\\varepsilon})^{\\frac{3}{2}}\\bigr)$\ngradient evaluations in the regime $\\kappa \\ll \\frac{d}{\\log d}$ under a warm\nstart assumption, where $\\kappa$ is the condition number and $d$ is the\ndimension.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 03:10:21 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Lu", "Jianfeng", ""], ["Wang", "Lihan", ""]]}, {"id": "2012.11102", "submitter": "Naveed Naimipour", "authors": "Naveed Naimipour, Shahin Khobahi, Mojtaba Soltanalian", "title": "Unfolded Algorithms for Deep Phase Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploring the idea of phase retrieval has been intriguing researchers for\ndecades, due to its appearance in a wide range of applications. The task of a\nphase retrieval algorithm is typically to recover a signal from linear\nphaseless measurements. In this paper, we approach the problem by proposing a\nhybrid model-based data-driven deep architecture, referred to as Unfolded Phase\nRetrieval (UPR), that exhibits significant potential in improving the\nperformance of state-of-the art data-driven and model-based phase retrieval\nalgorithms. The proposed method benefits from versatility and interpretability\nof well-established model-based algorithms, while simultaneously benefiting\nfrom the expressive power of deep neural networks. In particular, our proposed\nmodel-based deep architecture is applied to the conventional phase retrieval\nproblem (via the incremental reshaped Wirtinger flow algorithm) and the sparse\nphase retrieval problem (via the sparse truncated amplitude flow algorithm),\nshowing immense promise in both cases. Furthermore, we consider a joint design\nof the sensing matrix and the signal processing algorithm and utilize the deep\nunfolding technique in the process. Our numerical results illustrate the\neffectiveness of such hybrid model-based and data-driven frameworks and\nshowcase the untapped potential of data-aided methodologies to enhance the\nexisting phase retrieval algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 03:46:17 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Naimipour", "Naveed", ""], ["Khobahi", "Shahin", ""], ["Soltanalian", "Mojtaba", ""]]}, {"id": "2012.11140", "submitter": "Alessandro Achille", "authors": "Alessandro Achille, Aditya Golatkar, Avinash Ravichandran, Marzia\n  Polito, Stefano Soatto", "title": "LQF: Linear Quadratic Fine-Tuning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifiers that are linear in their parameters, and trained by optimizing a\nconvex loss function, have predictable behavior with respect to changes in the\ntraining data, initial conditions, and optimization. Such desirable properties\nare absent in deep neural networks (DNNs), typically trained by non-linear\nfine-tuning of a pre-trained model. Previous attempts to linearize DNNs have\nled to interesting theoretical insights, but have not impacted the practice due\nto the substantial performance gap compared to standard non-linear\noptimization. We present the first method for linearizing a pre-trained model\nthat achieves comparable performance to non-linear fine-tuning on most of\nreal-world image classification tasks tested, thus enjoying the\ninterpretability of linear models without incurring punishing losses in\nperformance. LQF consists of simple modifications to the architecture, loss\nfunction and optimization typically used for classification: Leaky-ReLU instead\nof ReLU, mean squared loss instead of cross-entropy, and pre-conditioning using\nKronecker factorization. None of these changes in isolation is sufficient to\napproach the performance of non-linear fine-tuning. When used in combination,\nthey allow us to reach comparable performance, and even superior in the\nlow-data regime, while enjoying the simplicity, robustness and interpretability\nof linear-quadratic optimization.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 06:40:20 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Achille", "Alessandro", ""], ["Golatkar", "Aditya", ""], ["Ravichandran", "Avinash", ""], ["Polito", "Marzia", ""], ["Soatto", "Stefano", ""]]}, {"id": "2012.11198", "submitter": "Muneki Yasuda", "authors": "Muneki Yasuda and Kaiji Sekimoto", "title": "Spatial Monte Carlo Integration with Annealed Importance Sampling", "comments": null, "journal-ref": "Phys. Rev. E 103, 052118 (2021)", "doi": "10.1103/PhysRevE.103.052118", "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.LG physics.data-an", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Evaluating expectations on an Ising model (or Boltzmann machine) is essential\nfor various applications, including statistical machine learning. However, in\ngeneral, the evaluation is computationally difficult because it involves\nintractable multiple summations or integrations; therefore, it requires\napproximation. Monte Carlo integration (MCI) is a well-known approximation\nmethod; a more effective MCI-like approximation method was proposed recently,\ncalled spatial Monte Carlo integration (SMCI). However, the estimations\nobtained using SMCI (and MCI) exhibit a low accuracy in Ising models under a\nlow temperature owing to degradation of the sampling quality. Annealed\nimportance sampling (AIS) is a type of importance sampling based on Markov\nchain Monte Carlo methods that can suppress performance degradation in\nlow-temperature regions with the force of importance weights. In this study, a\nnew method is proposed to evaluate the expectations on Ising models combining\nAIS and SMCI. The proposed method performs efficiently in both high- and\nlow-temperature regions, which is demonstrated theoretically and numerically.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 09:26:40 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 08:24:02 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Yasuda", "Muneki", ""], ["Sekimoto", "Kaiji", ""]]}, {"id": "2012.11339", "submitter": "Anh Tong", "authors": "Anh Tong, Toan Tran, Hung Bui, Jaesik Choi", "title": "Learning Compositional Sparse Gaussian Processes with a Shrinkage Prior", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choosing a proper set of kernel functions is an important problem in learning\nGaussian Process (GP) models since each kernel structure has different model\ncomplexity and data fitness. Recently, automatic kernel composition methods\nprovide not only accurate prediction but also attractive interpretability\nthrough search-based methods. However, existing methods suffer from slow kernel\ncomposition learning. To tackle large-scaled data, we propose a new sparse\napproximate posterior for GPs, MultiSVGP, constructed from groups of inducing\npoints associated with individual additive kernels in compositional kernels. We\ndemonstrate that this approximation provides a better fit to learn\ncompositional kernels given empirical observations. We also provide\ntheoretically justification on error bound when compared to the traditional\nsparse GP. In contrast to the search-based approach, we present a novel\nprobabilistic algorithm to learn a kernel composition by handling the sparsity\nin the kernel selection with Horseshoe prior. We demonstrate that our model can\ncapture characteristics of time series with significant reductions in\ncomputational time and have competitive regression performance on real-world\ndata sets.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 13:41:15 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 07:11:56 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Tong", "Anh", ""], ["Tran", "Toan", ""], ["Bui", "Hung", ""], ["Choi", "Jaesik", ""]]}, {"id": "2012.11365", "submitter": "Alexandre Abraham", "authors": "Alexandre Abraham and L\\'eo Dreyfus-Schmidt", "title": "Rebuilding Trust in Active Learning with Actionable Metrics", "comments": "16 pages, 38 figures", "journal-ref": "In the Proceedings of the 20th IEEE International Conference on\n  Data Mining (ICDM), 2020", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active Learning (AL) is an active domain of research, but is seldom used in\nthe industry despite the pressing needs. This is in part due to a misalignment\nof objectives, while research strives at getting the best results on selected\ndatasets, the industry wants guarantees that Active Learning will perform\nconsistently and at least better than random labeling. The very one-off nature\nof Active Learning makes it crucial to understand how strategy selection can be\ncarried out and what drives poor performance (lack of exploration, selection of\nsamples that are too hard to classify, ...).\n  To help rebuild trust of industrial practitioners in Active Learning, we\npresent various actionable metrics. Through extensive experiments on reference\ndatasets such as CIFAR100, Fashion-MNIST, and 20Newsgroups, we show that those\nmetrics brings interpretability to AL strategies that can be leveraged by the\npractitioner.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 09:34:59 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 13:36:38 GMT"}, {"version": "v3", "created": "Fri, 19 Feb 2021 08:15:25 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Abraham", "Alexandre", ""], ["Dreyfus-Schmidt", "L\u00e9o", ""]]}, {"id": "2012.11390", "submitter": "Lo\\\"ic Omnes", "authors": "Lo\\\"ic Omnes, Antoine Marot, Benjamin Donnot", "title": "Adversarial Training for a Continuous Robustness Control Problem in\n  Power Systems", "comments": "6 pages, 5 figures, to be published in the PowerTech 2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new adversarial training approach for injecting robustness when\ndesigning controllers for upcoming cyber-physical power systems. Previous\napproaches relying deeply on simulations are not able to cope with the rising\ncomplexity and are too costly when used online in terms of computation budget.\nIn comparison, our method proves to be computationally efficient online while\ndisplaying useful robustness properties. To do so we model an adversarial\nframework, propose the implementation of a fixed opponent policy and test it on\na L2RPN (Learning to Run a Power Network) environment. This environment is a\nsynthetic but realistic modeling of a cyber-physical system accounting for one\nthird of the IEEE 118 grid. Using adversarial testing, we analyze the results\nof submitted trained agents from the robustness track of the L2RPN competition.\nWe then further assess the performance of these agents in regards to the\ncontinuous N-1 problem through tailored evaluation metrics. We discover that\nsome agents trained in an adversarial way demonstrate interesting preventive\nbehaviors in that regard, which we discuss.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 14:42:51 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 17:25:17 GMT"}, {"version": "v3", "created": "Fri, 16 Apr 2021 12:05:28 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Omnes", "Lo\u00efc", ""], ["Marot", "Antoine", ""], ["Donnot", "Benjamin", ""]]}, {"id": "2012.11432", "submitter": "Samuel Ofosu Mensah", "authors": "Samuel Ofosu Mensah, Bubacarr Bah, Willie Brink", "title": "Towards the Localisation of Lesions in Diabetic Retinopathy", "comments": "8 pages, 4 figures, used svproc document class, Computing Conference\n  2021 - London", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional Neural Networks (CNNs) have successfully been used to classify\ndiabetic retinopathy (DR) fundus images in recent times. However, deeper\nrepresentations in CNNs may capture higher-level semantics at the expense of\nspatial resolution. To make predictions usable for ophthalmologists, we use a\npost-attention technique called Gradient-weighted Class Activation Mapping\n(Grad-CAM) on the penultimate layer of deep learning models to produce coarse\nlocalisation maps on DR fundus images. This is to help identify discriminative\nregions in the images, consequently providing evidence for ophthalmologists to\nmake a diagnosis and potentially save lives by early diagnosis. Specifically,\nthis study uses pre-trained weights from four state-of-the-art deep learning\nmodels to produce and compare localisation maps of DR fundus images. The models\nused include VGG16, ResNet50, InceptionV3, and InceptionResNetV2. We find that\nInceptionV3 achieves the best performance with a test classification accuracy\nof 96.07%, and localise lesions better and faster than the other models.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 15:39:17 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 09:29:26 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Mensah", "Samuel Ofosu", ""], ["Bah", "Bubacarr", ""], ["Brink", "Willie", ""]]}, {"id": "2012.11518", "submitter": "Pranay Sharma", "authors": "Pranay Sharma, Kaidi Xu, Sijia Liu, Pin-Yu Chen, Xue Lin and Pramod K.\n  Varshney", "title": "Zeroth-Order Hybrid Gradient Descent: Towards A Principled Black-Box\n  Optimization Framework", "comments": "27 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we focus on the study of stochastic zeroth-order (ZO)\noptimization which does not require first-order gradient information and uses\nonly function evaluations. The problem of ZO optimization has emerged in many\nrecent machine learning applications, where the gradient of the objective\nfunction is either unavailable or difficult to compute. In such cases, we can\napproximate the full gradients or stochastic gradients through function value\nbased gradient estimates. Here, we propose a novel hybrid gradient estimator\n(HGE), which takes advantage of the query-efficiency of random gradient\nestimates as well as the variance-reduction of coordinate-wise gradient\nestimates. We show that with a graceful design in coordinate importance\nsampling, the proposed HGE-based ZO optimization method is efficient both in\nterms of iteration complexity as well as function query cost. We provide a\nthorough theoretical analysis of the convergence of our proposed method for\nnon-convex, convex, and strongly-convex optimization. We show that the\nconvergence rate that we derive generalizes the results for some prominent\nexisting methods in the nonconvex case, and matches the optimal result in the\nconvex case. We also corroborate the theory with a real-world black-box attack\ngeneration application to demonstrate the empirical advantage of our method\nover state-of-the-art ZO optimization approaches.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 17:29:58 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Sharma", "Pranay", ""], ["Xu", "Kaidi", ""], ["Liu", "Sijia", ""], ["Chen", "Pin-Yu", ""], ["Lin", "Xue", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "2012.11554", "submitter": "Zhuoran Yang", "authors": "Zhuoran Yang, Yufeng Zhang, Yongxin Chen, Zhaoran Wang", "title": "Variational Transport: A Convergent Particle-BasedAlgorithm for\n  Distributional Optimization", "comments": "58 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the optimization problem of minimizing a functional defined over\na family of probability distributions, where the objective functional is\nassumed to possess a variational form. Such a distributional optimization\nproblem arises widely in machine learning and statistics, with Monte-Carlo\nsampling, variational inference, policy optimization, and generative\nadversarial network as examples. For this problem, we propose a novel\nparticle-based algorithm, dubbed as variational transport, which approximately\nperforms Wasserstein gradient descent over the manifold of probability\ndistributions via iteratively pushing a set of particles. Specifically, we\nprove that moving along the geodesic in the direction of functional gradient\nwith respect to the second-order Wasserstein distance is equivalent to applying\na pushforward mapping to a probability distribution, which can be approximated\naccurately by pushing a set of particles. Specifically, in each iteration of\nvariational transport, we first solve the variational problem associated with\nthe objective functional using the particles, whose solution yields the\nWasserstein gradient direction. Then we update the current distribution by\npushing each particle along the direction specified by such a solution. By\ncharacterizing both the statistical error incurred in estimating the\nWasserstein gradient and the progress of the optimization algorithm, we prove\nthat when the objective function satisfies a functional version of the\nPolyak-\\L{}ojasiewicz (PL) (Polyak, 1963) and smoothness conditions,\nvariational transport converges linearly to the global minimum of the objective\nfunctional up to a certain statistical error, which decays to zero sublinearly\nas the number of particles goes to infinity.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 18:33:13 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Yang", "Zhuoran", ""], ["Zhang", "Yufeng", ""], ["Chen", "Yongxin", ""], ["Wang", "Zhaoran", ""]]}, {"id": "2012.11646", "submitter": "Marianne Menictas", "authors": "Marianne Menictas and Sabina Tomkins and Susan Murphy", "title": "Fast Physical Activity Suggestions: Efficient Hyperparameter Learning in\n  Mobile Health", "comments": "Neurips 2020 workshop: Machine Learning in Mobile Health. arXiv admin\n  note: substantial text overlap with arXiv:2003.12881", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Users can be supported to adopt healthy behaviors, such as regular physical\nactivity, via relevant and timely suggestions on their mobile devices.\nRecently, reinforcement learning algorithms have been found to be effective for\nlearning the optimal context under which to provide suggestions. However, these\nalgorithms are not necessarily designed for the constraints posed by mobile\nhealth (mHealth) settings, that they be efficient, domain-informed and\ncomputationally affordable. We propose an algorithm for providing physical\nactivity suggestions in mHealth settings. Using domain-science, we formulate a\ncontextual bandit algorithm which makes use of a linear mixed effects model. We\nthen introduce a procedure to efficiently perform hyper-parameter updating,\nusing far less computational resources than competing approaches. Not only is\nour approach computationally efficient, it is also easily implemented with\nclosed form matrix algebraic updates and we show improvements over state of the\nart approaches both in speed and accuracy of up to 99% and 56% respectively.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 19:17:31 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Menictas", "Marianne", ""], ["Tomkins", "Sabina", ""], ["Murphy", "Susan", ""]]}, {"id": "2012.11654", "submitter": "Quynh Nguyen", "authors": "Quynh Nguyen, Marco Mondelli, Guido Montufar", "title": "Tight Bounds on the Smallest Eigenvalue of the Neural Tangent Kernel for\n  Deep ReLU Networks", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent line of work has analyzed the theoretical properties of deep neural\nnetworks via the Neural Tangent Kernel (NTK). In particular, the smallest\neigenvalue of the NTK has been related to the memorization capacity, the global\nconvergence of gradient descent algorithms and the generalization of deep nets.\nHowever, existing results either provide bounds in the two-layer setting or\nassume that the spectrum of the NTK matrices is bounded away from 0 for\nmulti-layer networks. In this paper, we provide tight bounds on the smallest\neigenvalue of NTK matrices for deep ReLU nets, both in the limiting case of\ninfinite widths and for finite widths. In the finite-width setting, the network\narchitectures we consider are fairly general: we require the existence of a\nwide layer with roughly order of $N$ neurons, $N$ being the number of data\nsamples; and the scaling of the remaining layer widths is arbitrary (up to\nlogarithmic factors). To obtain our results, we analyze various quantities of\nindependent interest: we give lower bounds on the smallest singular value of\nhidden feature matrices, and upper bounds on the Lipschitz constant of\ninput-output feature maps.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 19:32:17 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 20:50:38 GMT"}, {"version": "v3", "created": "Tue, 8 Jun 2021 21:27:38 GMT"}, {"version": "v4", "created": "Fri, 11 Jun 2021 10:11:24 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Nguyen", "Quynh", ""], ["Mondelli", "Marco", ""], ["Montufar", "Guido", ""]]}, {"id": "2012.11676", "submitter": "Zhou Fan", "authors": "Xinyi Zhong and Chang Su and Zhou Fan", "title": "Empirical Bayes PCA in high dimensions", "comments": "v2: Shorten exposition, clarify some theoretical details", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the dimension of data is comparable to or larger than the number of data\nsamples, Principal Components Analysis (PCA) may exhibit problematic\nhigh-dimensional noise. In this work, we propose an Empirical Bayes PCA method\nthat reduces this noise by estimating a joint prior distribution for the\nprincipal components. EB-PCA is based on the classical Kiefer-Wolfowitz\nnonparametric MLE for empirical Bayes estimation, distributional results\nderived from random matrix theory for the sample PCs, and iterative refinement\nusing an Approximate Message Passing (AMP) algorithm. In theoretical \"spiked\"\nmodels, EB-PCA achieves Bayes-optimal estimation accuracy in the same settings\nas an oracle Bayes AMP procedure that knows the true priors. Empirically,\nEB-PCA significantly improves over PCA when there is strong prior structure,\nboth in simulation and on quantitative benchmarks constructed from the 1000\nGenomes Project and the International HapMap Project. An illustration is\npresented for analysis of gene expression data obtained by single-cell RNA-seq.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 20:43:44 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 15:20:16 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Zhong", "Xinyi", ""], ["Su", "Chang", ""], ["Fan", "Zhou", ""]]}, {"id": "2012.11782", "submitter": "Kentaro Kanamori", "authors": "Kentaro Kanamori, Takuya Takagi, Ken Kobayashi, Yuichi Ike, Kento\n  Uemura, Hiroki Arimura", "title": "Ordered Counterfactual Explanation by Mixed-Integer Linear Optimization", "comments": "20 pages, 5 figures, to appear in the 35th AAAI Conference on\n  Artificial Intelligence (AAAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Post-hoc explanation methods for machine learning models have been widely\nused to support decision-making. One of the popular methods is Counterfactual\nExplanation (CE), also known as Actionable Recourse, which provides a user with\na perturbation vector of features that alters the prediction result. Given a\nperturbation vector, a user can interpret it as an \"action\" for obtaining one's\ndesired decision result. In practice, however, showing only a perturbation\nvector is often insufficient for users to execute the action. The reason is\nthat if there is an asymmetric interaction among features, such as causality,\nthe total cost of the action is expected to depend on the order of changing\nfeatures. Therefore, practical CE methods are required to provide an\nappropriate order of changing features in addition to a perturbation vector.\nFor this purpose, we propose a new framework called Ordered Counterfactual\nExplanation (OrdCE). We introduce a new objective function that evaluates a\npair of an action and an order based on feature interaction. To extract an\noptimal pair, we propose a mixed-integer linear optimization approach with our\nobjective function. Numerical experiments on real datasets demonstrated the\neffectiveness of our OrdCE in comparison with unordered CE methods.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 01:41:23 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 05:47:43 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Kanamori", "Kentaro", ""], ["Takagi", "Takuya", ""], ["Kobayashi", "Ken", ""], ["Ike", "Yuichi", ""], ["Uemura", "Kento", ""], ["Arimura", "Hiroki", ""]]}, {"id": "2012.11849", "submitter": "Tomohiro Hayase", "authors": "Tomohiro Hayase, Suguru Yasutomi, Takashi Katoh", "title": "Selective Forgetting of Deep Networks at a Finer Level than Samples", "comments": "8+2 pages, 6 figures, accepted to AAAI RSEML", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selective forgetting or removing information from deep neural networks (DNNs)\nis essential for continual learning and is challenging in controlling the DNNs.\nSuch forgetting is crucial also in a practical sense since the deployed DNNs\nmay be trained on the data with outliers, poisoned by attackers, or with\nleaked/sensitive information. In this paper, we formulate selective forgetting\nfor classification tasks at a finer level than the samples' level. We specify\nthe finer level based on four datasets distinguished by two conditions: whether\nthey contain information to be forgotten and whether they are available for the\nforgetting procedure. Additionally, we reveal the need for such formulation\nwith the datasets by showing concrete and practical situations. Moreover, we\nintroduce the forgetting procedure as an optimization problem on three\ncriteria; the forgetting, the correction, and the remembering term.\nExperimental results show that the proposed methods can make the model forget\nto use specific information for classification. Notably, in specific cases, our\nmethods improved the model's accuracy on the datasets, which contains\ninformation to be forgotten but is unavailable in the forgetting procedure.\nSuch data are unexpectedly found and misclassified in actual situations.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 06:17:31 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2020 12:26:34 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Hayase", "Tomohiro", ""], ["Yasutomi", "Suguru", ""], ["Katoh", "Takashi", ""]]}, {"id": "2012.11922", "submitter": "Gustau Camps-Valls", "authors": "Fernando Mateo, Jordi Munoz-Mari, Valero Laparra, Jochem Verrelst,\n  Gustau Camps-Valls", "title": "Learning Structures in Earth Observation Data with Gaussian Processes", "comments": null, "journal-ref": "in Advanced Analysis and Learning on Temporal Data. AALTD\n  2015.Lecture Notes in Computer Science, vol 9785. Springer, Cham", "doi": "10.1007/978-3-319-44412-3_6", "report-no": null, "categories": "physics.geo-ph physics.app-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Processes (GPs) has experienced tremendous success in geoscience in\ngeneral and for bio-geophysical parameter retrieval in the last years. GPs\nconstitute a solid Bayesian framework to formulate many function approximation\nproblems consistently. This paper reviews the main theoretical GP developments\nin the field. We review new algorithms that respect the signal and noise\ncharacteristics, that provide feature rankings automatically, and that allow\napplicability of associated uncertainty intervals to transport GP models in\nspace and time. All these developments are illustrated in the field of\ngeoscience and remote sensing at a local and global scales through a set of\nillustrative examples.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 10:46:37 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Mateo", "Fernando", ""], ["Munoz-Mari", "Jordi", ""], ["Laparra", "Valero", ""], ["Verrelst", "Jochem", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "2012.11978", "submitter": "Alessandro Rudi", "authors": "Alessandro Rudi and Ulysse Marteau-Ferey and Francis Bach", "title": "Finding Global Minima via Kernel Approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider the global minimization of smooth functions based solely on\nfunction evaluations. Algorithms that achieve the optimal number of function\nevaluations for a given precision level typically rely on explicitly\nconstructing an approximation of the function which is then minimized with\nalgorithms that have exponential running-time complexity. In this paper, we\nconsider an approach that jointly models the function to approximate and finds\na global minimum. This is done by using infinite sums of square smooth\nfunctions and has strong links with polynomial sum-of-squares hierarchies.\nLeveraging recent representation properties of reproducing kernel Hilbert\nspaces, the infinite-dimensional optimization problem can be solved by\nsubsampling in time polynomial in the number of function evaluations, and with\ntheoretical guarantees on the obtained minimum.\n  Given $n$ samples, the computational cost is $O(n^{3.5})$ in time, $O(n^2)$\nin space, and we achieve a convergence rate to the global optimum that is\n$O(n^{-m/d + 1/2 + 3/d})$ where $m$ is the degree of differentiability of the\nfunction and $d$ the number of dimensions. The rate is nearly optimal in the\ncase of Sobolev functions and more generally makes the proposed method\nparticularly suitable for functions that have a large number of derivatives.\nIndeed, when $m$ is in the order of $d$, the convergence rate to the global\noptimum does not suffer from the curse of dimensionality, which affects only\nthe worst-case constants (that we track explicitly through the paper).\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 12:59:30 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Rudi", "Alessandro", ""], ["Marteau-Ferey", "Ulysse", ""], ["Bach", "Francis", ""]]}, {"id": "2012.11987", "submitter": "Moritz Herrmann", "authors": "Moritz Herrmann and Fabian Scheipl", "title": "Unsupervised Functional Data Analysis via Nonlinear Dimension Reduction", "comments": "29 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In recent years, manifold methods have moved into focus as tools for\ndimension reduction. Assuming that the high-dimensional data actually lie on or\nclose to a low-dimensional nonlinear manifold, these methods have shown\nconvincing results in several settings. This manifold assumption is often\nreasonable for functional data, i.e., data representing continuously observed\nfunctions, as well. However, the performance of manifold methods recently\nproposed for tabular or image data has not been systematically assessed in the\ncase of functional data yet. Moreover, it is unclear how to evaluate the\nquality of learned embeddings that do not yield invertible mappings, since the\nreconstruction error cannot be used as a performance measure for such\nrepresentations. In this work, we describe and investigate the specific\nchallenges for nonlinear dimension reduction posed by the functional data\nsetting. The contributions of the paper are three-fold: First of all, we define\na theoretical framework which allows to systematically assess specific\nchallenges that arise in the functional data context, transfer several\nnonlinear dimension reduction methods for tabular and image data to functional\ndata, and show that manifold methods can be used successfully in this setting.\nSecondly, we subject performance assessment and tuning strategies to a thorough\nand systematic evaluation based on several different functional data settings\nand point out some previously undescribed weaknesses and pitfalls which can\njeopardize reliable judgment of embedding quality. Thirdly, we propose a\nnuanced approach to make trustworthy decisions for or against competing\nnonconforming embeddings more objectively.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 13:19:32 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Herrmann", "Moritz", ""], ["Scheipl", "Fabian", ""]]}, {"id": "2012.12056", "submitter": "Rossella Arcucci Dr", "authors": "Maddalena Amendola, Rossella Arcucci, Laetitia Mottet, Cesar Quilodran\n  Casas, Shiwei Fan, Christopher Pain, Paul Linden, Yi-Ke Guo", "title": "Data Assimilation in the Latent Space of a Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an urgent need to build models to tackle Indoor Air Quality issue.\nSince the model should be accurate and fast, Reduced Order Modelling technique\nis used to reduce the dimensionality of the problem. The accuracy of the model,\nthat represent a dynamic system, is improved integrating real data coming from\nsensors using Data Assimilation techniques. In this paper, we formulate a new\nmethodology called Latent Assimilation that combines Data Assimilation and\nMachine Learning. We use a Convolutional neural network to reduce the\ndimensionality of the problem, a Long-Short-Term-Memory to build a surrogate\nmodel of the dynamic system and an Optimal Interpolated Kalman Filter to\nincorporate real data. Experimental results are provided for CO2 concentration\nwithin an indoor space. This methodology can be used for example to predict in\nreal-time the load of virus, such as the SARS-COV-2, in the air by linking it\nto the concentration of CO2.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 14:43:50 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Amendola", "Maddalena", ""], ["Arcucci", "Rossella", ""], ["Mottet", "Laetitia", ""], ["Casas", "Cesar Quilodran", ""], ["Fan", "Shiwei", ""], ["Pain", "Christopher", ""], ["Linden", "Paul", ""], ["Guo", "Yi-Ke", ""]]}, {"id": "2012.12141", "submitter": "Kartik Ahuja", "authors": "Kartik Ahuja, Amit Dhurandhar, Kush R. Varshney", "title": "Learning to Initialize Gradient Descent Using Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-convex optimization problems are challenging to solve; the success and\ncomputational expense of a gradient descent algorithm or variant depend heavily\non the initialization strategy. Often, either random initialization is used or\ninitialization rules are carefully designed by exploiting the nature of the\nproblem class. As a simple alternative to hand-crafted initialization rules, we\npropose an approach for learning \"good\" initialization rules from previous\nsolutions. We provide theoretical guarantees that establish conditions that are\nsufficient in all cases and also necessary in some under which our approach\nperforms better than random initialization. We apply our methodology to various\nnon-convex problems such as generating adversarial examples, generating post\nhoc explanations for black-box machine learning models, and allocating\ncommunication spectrum, and show consistent gains over other initialization\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 16:23:36 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Ahuja", "Kartik", ""], ["Dhurandhar", "Amit", ""], ["Varshney", "Kush R.", ""]]}, {"id": "2012.12294", "submitter": "Jakob Drefs", "authors": "Jakob Drefs, Enrico Guiraud, J\\\"org L\\\"ucke", "title": "Evolutionary Variational Optimization of Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We combine two popular optimization approaches to derive learning algorithms\nfor generative models: variational optimization and evolutionary algorithms.\nThe combination is realized for generative models with discrete latents by\nusing truncated posteriors as the family of variational distributions. The\nvariational parameters of truncated posteriors are sets of latent states. By\ninterpreting these states as genomes of individuals and by using the\nvariational lower bound to define a fitness, we can apply evolutionary\nalgorithms to realize the variational loop. The used variational distributions\nare very flexible and we show that evolutionary algorithms can effectively and\nefficiently optimize the variational bound. Furthermore, the variational loop\nis generally applicable (\"black box\") with no analytical derivations required.\nTo show general applicability, we apply the approach to three generative models\n(we use noisy-OR Bayes Nets, Binary Sparse Coding, and Spike-and-Slab Sparse\nCoding). To demonstrate effectiveness and efficiency of the novel variational\napproach, we use the standard competitive benchmarks of image denoising and\ninpainting. The benchmarks allow quantitative comparisons to a wide range of\nmethods including probabilistic approaches, deep deterministic and generative\nnetworks, and non-local image processing methods. In the category of\n\"zero-shot\" learning (when only the corrupted image is used for training), we\nobserved the evolutionary variational algorithm to significantly improve the\nstate-of-the-art in many benchmark settings. For one well-known inpainting\nbenchmark, we also observed state-of-the-art performance across all categories\nof algorithms although we only train on the corrupted image. In general, our\ninvestigations highlight the importance of research on optimization methods for\ngenerative models to achieve performance improvements.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 19:06:33 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 19:44:49 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Drefs", "Jakob", ""], ["Guiraud", "Enrico", ""], ["L\u00fccke", "J\u00f6rg", ""]]}, {"id": "2012.12356", "submitter": "Weijun Xie", "authors": "Qing Ye and Weijun Xie", "title": "Unbiased Subdata Selection for Fair Classification: A Unified Framework\n  and Scalable Algorithms", "comments": "42 pages, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  As an important problem in modern data analytics, classification has\nwitnessed varieties of applications from different domains. Different from\nconventional classification approaches, fair classification concerns the issues\nof unintentional biases against the sensitive features (e.g., gender, race).\nDue to high nonconvexity of fairness measures, existing methods are often\nunable to model exact fairness, which can cause inferior fair classification\noutcomes. This paper fills the gap by developing a novel unified framework to\njointly optimize accuracy and fairness. The proposed framework is versatile and\ncan incorporate different fairness measures studied in literature precisely as\nwell as can be applicable to many classifiers including deep classification\nmodels. Specifically, in this paper, we first prove Fisher consistency of the\nproposed framework. We then show that many classification models within this\nframework can be recast as mixed-integer convex programs, which can be solved\neffectively by off-the-shelf solvers when the instance sizes are moderate and\ncan be used as benchmarks to compare the efficiency of approximation\nalgorithms. We prove that in the proposed framework, when the classification\noutcomes are known, the resulting problem, termed \"unbiased subdata selection,\"\nis strongly polynomial-solvable and can be used to enhance the classification\nfairness by selecting more representative data points. This motivates us to\ndevelop an iterative refining strategy (IRS) to solve the large-scale\ninstances, where we improve the classification accuracy and conduct the\nunbiased subdata selection in an alternating fashion. We study the convergence\nproperty of IRS and derive its approximation bound. More broadly, this\nframework can be leveraged to improve classification models with unbalanced\ndata by taking F1 score into consideration.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 21:09:38 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2020 05:20:18 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Ye", "Qing", ""], ["Xie", "Weijun", ""]]}, {"id": "2012.12367", "submitter": "Soumyadip Ghosh", "authors": "Soumyadip Ghosh and Mark Squillante", "title": "Unbiased Gradient Estimation for Distributionally Robust Learning", "comments": "ICML 2020, AISTATS 2021 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seeking to improve model generalization, we consider a new approach based on\ndistributionally robust learning (DRL) that applies stochastic gradient descent\nto the outer minimization problem. Our algorithm efficiently estimates the\ngradient of the inner maximization problem through multi-level Monte Carlo\nrandomization. Leveraging theoretical results that shed light on why standard\ngradient estimators fail, we establish the optimal parameterization of the\ngradient estimators of our approach that balances a fundamental tradeoff\nbetween computation time and statistical variance. Numerical experiments\ndemonstrate that our DRL approach yields significant benefits over previous\nwork.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 21:35:03 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Ghosh", "Soumyadip", ""], ["Squillante", "Mark", ""]]}, {"id": "2012.12384", "submitter": "Valeri Alexiev", "authors": "Valeri Alexiev", "title": "Fractal Dimension Generalization Measure", "comments": "4 pages, 2 figures, presented at the \"Predicting Generalization in\n  Deep Learning\" competition at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing a robust generalization measure for the performance of machine\nlearning models is an important and challenging task. A lot of recent research\nin the area focuses on the model decision boundary when predicting\ngeneralization. In this paper, as part of the \"Predicting Generalization in\nDeep Learning\" competition, we analyse the complexity of decision boundaries\nusing the concept of fractal dimension and develop a generalization measure\nbased on that technique.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 22:04:32 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Alexiev", "Valeri", ""]]}, {"id": "2012.12418", "submitter": "Xingyi Yang", "authors": "Xingyi Yang", "title": "Stochastic Gradient Variance Reduction by Solving a Filtering Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks (DNN) are typically optimized using stochastic gradient\ndescent (SGD). However, the estimation of the gradient using stochastic samples\ntends to be noisy and unreliable, resulting in large gradient variance and bad\nconvergence. In this paper, we propose \\textbf{Filter Gradient Decent}~(FGD),\nan efficient stochastic optimization algorithm that makes the consistent\nestimation of the local gradient by solving an adaptive filtering problem with\ndifferent design of filters. Our method reduces variance in stochastic gradient\ndescent by incorporating the historical states to enhance the current\nestimation. It is able to correct noisy gradient direction as well as to\naccelerate the convergence of learning. We demonstrate the effectiveness of the\nproposed Filter Gradient Descent on numerical optimization and training neural\nnetworks, where it achieves superior and robust performance compared with\ntraditional momentum-based methods. To the best of our knowledge, we are the\nfirst to provide a practical solution that integrates filtering into gradient\nestimation by making the analogy between gradient estimation and filtering\nproblems in signal processing. (The code is provided in\nhttps://github.com/Adamdad/Filter-Gradient-Decent)\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 23:48:42 GMT"}, {"version": "v2", "created": "Sat, 15 May 2021 01:53:12 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Yang", "Xingyi", ""]]}, {"id": "2012.12449", "submitter": "Roy Adams", "authors": "Noam Finkelstein, Roy Adams, Suchi Saria, Ilya Shpitser", "title": "Partial Identifiability in Discrete Data With Measurement Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When data contains measurement errors, it is necessary to make assumptions\nrelating the observed, erroneous data to the unobserved true phenomena of\ninterest. These assumptions should be justifiable on substantive grounds, but\nare often motivated by mathematical convenience, for the sake of exactly\nidentifying the target of inference. We adopt the view that it is preferable to\npresent bounds under justifiable assumptions than to pursue exact\nidentification under dubious ones. To that end, we demonstrate how a broad\nclass of modeling assumptions involving discrete variables, including common\nmeasurement error and conditional independence assumptions, can be expressed as\nlinear constraints on the parameters of the model. We then use linear\nprogramming techniques to produce sharp bounds for factual and counterfactual\ndistributions under measurement error in such models. We additionally propose a\nprocedure for obtaining outer bounds on non-linear models. Our method yields\nsharp bounds in a number of important settings -- such as the instrumental\nvariable scenario with measurement error -- for which no bounds were previously\nknown.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 02:11:08 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Finkelstein", "Noam", ""], ["Adams", "Roy", ""], ["Saria", "Suchi", ""], ["Shpitser", "Ilya", ""]]}, {"id": "2012.12450", "submitter": "Francesco Pinto", "authors": "Francesco Pinto, Giacomo Acciarini, Sascha Metz, Sarah Boufelja,\n  Sylvester Kaczmarek, Klaus Merz, Jos\\'e A. Martinez-Heras, Francesca Letizia,\n  Christopher Bridges, At{\\i}l{\\i}m G\\\"une\\c{s} Baydin", "title": "Towards Automated Satellite Conjunction Management with Bayesian Deep\n  Learning", "comments": "7 pages, 2 figures", "journal-ref": "AI for Earth Sciences Workshop at NeurIPS 2020, Vancouver, Canada", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After decades of space travel, low Earth orbit is a junkyard of discarded\nrocket bodies, dead satellites, and millions of pieces of debris from\ncollisions and explosions. Objects in high enough altitudes do not re-enter and\nburn up in the atmosphere, but stay in orbit around Earth for a long time. With\na speed of 28,000 km/h, collisions in these orbits can generate fragments and\npotentially trigger a cascade of more collisions known as the Kessler syndrome.\nThis could pose a planetary challenge, because the phenomenon could escalate to\nthe point of hindering future space operations and damaging satellite\ninfrastructure critical for space and Earth science applications. As commercial\nentities place mega-constellations of satellites in orbit, the burden on\noperators conducting collision avoidance manoeuvres will increase. For this\nreason, development of automated tools that predict potential collision events\n(conjunctions) is critical. We introduce a Bayesian deep learning approach to\nthis problem, and develop recurrent neural network architectures (LSTMs) that\nwork with time series of conjunction data messages (CDMs), a standard data\nformat used by the space community. We show that our method can be used to\nmodel all CDM features simultaneously, including the time of arrival of future\nCDMs, providing predictions of conjunction event evolution with associated\nuncertainties.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 02:16:54 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Pinto", "Francesco", ""], ["Acciarini", "Giacomo", ""], ["Metz", "Sascha", ""], ["Boufelja", "Sarah", ""], ["Kaczmarek", "Sylvester", ""], ["Merz", "Klaus", ""], ["Martinez-Heras", "Jos\u00e9 A.", ""], ["Letizia", "Francesca", ""], ["Bridges", "Christopher", ""], ["Baydin", "At\u0131l\u0131m G\u00fcne\u015f", ""]]}, {"id": "2012.12457", "submitter": "Mitas Ray", "authors": "Mitas Ray, Omid Sadeghi, Lillian J. Ratliff, Maryam Fazel", "title": "Function Design for Improved Competitive Ratio in Online Resource\n  Allocation with Procurement Costs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of online resource allocation, where multiple customers\narrive sequentially and the seller must irrevocably allocate resources to each\nincoming customer while also facing a procurement cost for the total\nallocation. Assuming resource procurement follows an a priori known marginally\nincreasing cost function, the objective is to maximize the reward obtained from\nfulfilling the customers' requests sans the cumulative procurement cost. We\nanalyze the competitive ratio of a primal-dual algorithm in this setting, and\ndevelop an optimization framework for synthesizing a surrogate function for the\nprocurement cost function to be used by the algorithm, in order to improve the\ncompetitive ratio of the primal-dual algorithm. Our first design method focuses\non polynomial procurement cost functions and uses the optimal surrogate\nfunction to provide a more refined bound than the state of the art. Our second\ndesign method uses quasiconvex optimization to find optimal design parameters\nfor a general class of procurement cost functions. Numerical examples are used\nto illustrate the design techniques. We conclude by extending the analysis to\ndevise a posted pricing mechanism in which the algorithm does not require the\ncustomers' preferences to be revealed.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 02:32:47 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Ray", "Mitas", ""], ["Sadeghi", "Omid", ""], ["Ratliff", "Lillian J.", ""], ["Fazel", "Maryam", ""]]}, {"id": "2012.12474", "submitter": "Hunter Lang", "authors": "Hunter Lang, Hoifung Poon", "title": "Self-supervised self-supervision by combining deep learning and\n  probabilistic logic", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Labeling training examples at scale is a perennial challenge in machine\nlearning. Self-supervision methods compensate for the lack of direct\nsupervision by leveraging prior knowledge to automatically generate noisy\nlabeled examples. Deep probabilistic logic (DPL) is a unifying framework for\nself-supervised learning that represents unknown labels as latent variables and\nincorporates diverse self-supervision using probabilistic logic to train a deep\nneural network end-to-end using variational EM. While DPL is successful at\ncombining pre-specified self-supervision, manually crafting self-supervision to\nattain high accuracy may still be tedious and challenging. In this paper, we\npropose Self-Supervised Self-Supervision (S4), which adds to DPL the capability\nto learn new self-supervision automatically. Starting from an initial \"seed,\"\nS4 iteratively uses the deep neural network to propose new self supervision.\nThese are either added directly (a form of structured self-training) or\nverified by a human expert (as in feature-based active learning). Experiments\nshow that S4 is able to automatically propose accurate self-supervision and can\noften nearly match the accuracy of supervised methods with a tiny fraction of\nthe human effort.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 04:06:41 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Lang", "Hunter", ""], ["Poon", "Hoifung", ""]]}, {"id": "2012.12485", "submitter": "Hansika Hewamalage", "authors": "Hansika Hewamalage, Christoph Bergmeir, Kasun Bandara", "title": "Global Models for Time Series Forecasting: A Simulation Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the current context of Big Data, the nature of many forecasting problems\nhas changed from predicting isolated time series to predicting many time series\nfrom similar sources. This has opened up the opportunity to develop competitive\nglobal forecasting models that simultaneously learn from many time series. But,\nit still remains unclear when global forecasting models can outperform the\nunivariate benchmarks, especially along the dimensions of the\nhomogeneity/heterogeneity of series, the complexity of patterns in the series,\nthe complexity of forecasting models, and the lengths/number of series. Our\nstudy attempts to address this problem through investigating the effect from\nthese factors, by simulating a number of datasets that have controllable time\nseries characteristics. Specifically, we simulate time series from simple data\ngenerating processes (DGP), such as Auto Regressive (AR) and Seasonal AR, to\ncomplex DGPs, such as Chaotic Logistic Map, Self-Exciting Threshold\nAuto-Regressive, and Mackey-Glass Equations. The data heterogeneity is\nintroduced by mixing time series generated from several DGPs into a single\ndataset. The lengths and the number of series in the dataset are varied in\ndifferent scenarios. We perform experiments on these datasets using global\nforecasting models including Recurrent Neural Networks (RNN), Feed-Forward\nNeural Networks, Pooled Regression (PR) models and Light Gradient Boosting\nModels (LGBM), and compare their performance against standard statistical\nunivariate forecasting techniques. Our experiments demonstrate that when\ntrained as global forecasting models, techniques such as RNNs and LGBMs, which\nhave complex non-linear modelling capabilities, are competitive methods in\ngeneral under challenging forecasting scenarios such as series having short\nlengths, datasets with heterogeneous series and having minimal prior knowledge\nof the patterns of the series.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 04:45:52 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 07:39:16 GMT"}, {"version": "v3", "created": "Mon, 22 Mar 2021 03:39:03 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Hewamalage", "Hansika", ""], ["Bergmeir", "Christoph", ""], ["Bandara", "Kasun", ""]]}, {"id": "2012.12581", "submitter": "Wei Qiu", "authors": "Wei Qiu, Yangsibo Huang, Quanzheng Li", "title": "IFGAN: Missing Value Imputation using Feature-specific Generative\n  Adversarial Networks", "comments": "Wei Qiu and Yangsibo Huang contribute equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing value imputation is a challenging and well-researched topic in data\nmining. In this paper, we propose IFGAN, a missing value imputation algorithm\nbased on Feature-specific Generative Adversarial Networks (GAN). Our idea is\nintuitive yet effective: a feature-specific generator is trained to impute\nmissing values, while a discriminator is expected to distinguish the imputed\nvalues from observed ones. The proposed architecture is capable of handling\ndifferent data types, data distributions, missing mechanisms, and missing\nrates. It also improves post-imputation analysis by preserving inter-feature\ncorrelations. We empirically show on several real-life datasets that IFGAN\noutperforms current state-of-the-art algorithm under various missing\nconditions.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 10:14:35 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Qiu", "Wei", ""], ["Huang", "Yangsibo", ""], ["Li", "Quanzheng", ""]]}, {"id": "2012.12687", "submitter": "Joachim Sicking", "authors": "Joachim Sicking, Maram Akila, Maximilian Pintz, Tim Wirtz, Asja\n  Fischer, Stefan Wrobel", "title": "Second-Moment Loss: A Novel Regression Objective for Improved\n  Uncertainties", "comments": "Code is available on:\n  https://github.com/fraunhofer-iais/second-moment-loss", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantification of uncertainty is one of the most promising approaches to\nestablish safe machine learning. Despite its importance, it is far from being\ngenerally solved, especially for neural networks. One of the most commonly used\napproaches so far is Monte Carlo dropout, which is computationally cheap and\neasy to apply in practice. However, it can underestimate the uncertainty. We\npropose a new objective, referred to as second-moment loss (SML), to address\nthis issue. While the full network is encouraged to model the mean, the dropout\nnetworks are explicitly used to optimize the model variance. We analyze the\nperformance of the new objective on various toy and UCI regression datasets.\nComparing to the state-of-the-art of deep ensembles, SML leads to comparable\nprediction accuracies and uncertainty estimates while only requiring a single\nmodel. Under distribution shift, we observe moderate improvements. From a\nsafety perspective also the study of worst-case uncertainties is crucial. In\nthis regard we improve considerably. Finally, we show that SML can be\nsuccessfully applied to SqueezeDet, a modern object detection network. We\nimprove on its uncertainty-related scores while not deteriorating regression\nquality. As a side result, we introduce an intuitive Wasserstein distance-based\nuncertainty measure that is non-saturating and thus allows to resolve quality\ndifferences between any two uncertainty estimates.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 14:17:33 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Sicking", "Joachim", ""], ["Akila", "Maram", ""], ["Pintz", "Maximilian", ""], ["Wirtz", "Tim", ""], ["Fischer", "Asja", ""], ["Wrobel", "Stefan", ""]]}, {"id": "2012.12738", "submitter": "Felix P. Kemeth", "authors": "Felix P. Kemeth, Tom Bertalan, Thomas Thiem, Felix Dietrich, Sung Joon\n  Moon, Carlo R. Laing and Ioannis G. Kevrekidis", "title": "Learning emergent PDEs in a learned emergent space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.AO cs.LG nlin.PS physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extract data-driven, intrinsic spatial coordinates from observations of\nthe dynamics of large systems of coupled heterogeneous agents. These\ncoordinates then serve as an emergent space in which to learn predictive models\nin the form of partial differential equations (PDEs) for the collective\ndescription of the coupled-agent system. They play the role of the independent\nspatial variables in this PDE (as opposed to the dependent, possibly also\ndata-driven, state variables). This leads to an alternative description of the\ndynamics, local in these emergent coordinates, thus facilitating an alternative\nmodeling path for complex coupled-agent systems. We illustrate this approach on\na system where each agent is a limit cycle oscillator (a so-called\nStuart-Landau oscillator); the agents are heterogeneous (they each have a\ndifferent intrinsic frequency $\\omega$) and are coupled through the ensemble\naverage of their respective variables. After fast initial transients, we show\nthat the collective dynamics on a slow manifold can be approximated through a\nlearned model based on local \"spatial\" partial derivatives in the emergent\ncoordinates. The model is then used for prediction in time, as well as to\ncapture collective bifurcations when system parameters vary. The proposed\napproach thus integrates the automatic, data-driven extraction of emergent\nspace coordinates parametrizing the agent dynamics, with machine-learning\nassisted identification of an \"emergent PDE\" description of the dynamics in\nthis parametrization.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 15:17:21 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Kemeth", "Felix P.", ""], ["Bertalan", "Tom", ""], ["Thiem", "Thomas", ""], ["Dietrich", "Felix", ""], ["Moon", "Sung Joon", ""], ["Laing", "Carlo R.", ""], ["Kevrekidis", "Ioannis G.", ""]]}, {"id": "2012.12772", "submitter": "Chao Ding", "authors": "Qian Zhang, Xinyuan Zhao, Chao Ding", "title": "Matrix optimization based Euclidean embedding with outliers", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Euclidean embedding from noisy observations containing outlier errors is an\nimportant and challenging problem in statistics and machine learning. Many\nexisting methods would struggle with outliers due to a lack of detection\nability. In this paper, we propose a matrix optimization based embedding model\nthat can produce reliable embeddings and identify the outliers jointly. We show\nthat the estimators obtained by the proposed method satisfy a non-asymptotic\nrisk bound, implying that the model provides a high accuracy estimator with\nhigh probability when the order of the sample size is roughly the degree of\nfreedom up to a logarithmic factor. Moreover, we show that under some mild\nconditions, the proposed model also can identify the outliers without any prior\ninformation with high probability. Finally, numerical experiments demonstrate\nthat the matrix optimization-based model can produce configurations of high\nquality and successfully identify outliers even for large networks.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 16:26:40 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Zhang", "Qian", ""], ["Zhao", "Xinyuan", ""], ["Ding", "Chao", ""]]}, {"id": "2012.12802", "submitter": "Marcelo Medeiros", "authors": "Ricardo P. Masini, Marcelo C. Medeiros and Eduardo F. Mendes", "title": "Machine Learning Advances for Time Series Forecasting", "comments": "42 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we survey the most recent advances in supervised machine\nlearning and high-dimensional models for time series forecasting. We consider\nboth linear and nonlinear alternatives. Among the linear methods we pay special\nattention to penalized regressions and ensemble of models. The nonlinear\nmethods considered in the paper include shallow and deep neural networks, in\ntheir feed-forward and recurrent versions, and tree-based methods, such as\nrandom forests and boosted trees. We also consider ensemble and hybrid models\nby combining ingredients from different alternatives. Tests for superior\npredictive ability are briefly reviewed. Finally, we discuss application of\nmachine learning in economics and finance and provide an illustration with\nhigh-frequency financial data.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 17:01:56 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 22:38:39 GMT"}, {"version": "v3", "created": "Fri, 9 Apr 2021 11:24:04 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Masini", "Ricardo P.", ""], ["Medeiros", "Marcelo C.", ""], ["Mendes", "Eduardo F.", ""]]}, {"id": "2012.12803", "submitter": "Vitaly Feldman", "authors": "Vitaly Feldman, Audra McMillan, Kunal Talwar", "title": "Hiding Among the Clones: A Simple and Nearly Optimal Analysis of Privacy\n  Amplification by Shuffling", "comments": "Minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work of Erlingsson, Feldman, Mironov, Raghunathan, Talwar, and\nThakurta [EFMRTT19] demonstrates that random shuffling amplifies differential\nprivacy guarantees of locally randomized data. Such amplification implies\nsubstantially stronger privacy guarantees for systems in which data is\ncontributed anonymously [BEMMRLRKTS17] and has lead to significant interest in\nthe shuffle model of privacy [CSUZZ19,EFMRTT19].\n  We show that random shuffling of $n$ data records that are input to\n$\\varepsilon_0$-differentially private local randomizers results in an\n$(O((1-e^{-\\varepsilon_0})\\sqrt{\\frac{e^{\\varepsilon_0}\\log(1/\\delta)}{n}}),\n\\delta)$-differentially private algorithm. This significantly improves over\nprevious work and achieves the asymptotically optimal dependence in\n$\\varepsilon_0$. Our result is based on a new approach that is simpler than\nprevious work and extends to approximate differential privacy with nearly the\nsame guarantees. Our work also yields an empirical method to derive tighter\nbounds the resulting $\\varepsilon$ and we show that it gets to within a small\nconstant factor of the optimal bound. As a direct corollary of our analysis, we\nderive a simple and asymptotically optimal algorithm for discrete distribution\nestimation in the shuffle model of privacy. We also observe that our result\nimplies the first asymptotically optimal privacy analysis of noisy stochastic\ngradient descent that applies to sampling without replacement.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 17:07:26 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2020 06:55:45 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Feldman", "Vitaly", ""], ["McMillan", "Audra", ""], ["Talwar", "Kunal", ""]]}, {"id": "2012.12843", "submitter": "Marius Arvinte", "authors": "Marius Arvinte, Ahmed H. Tewfik, and Sriram Vishwanath", "title": "EQ-Net: A Unified Deep Learning Framework for Log-Likelihood Ratio\n  Estimation and Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we introduce EQ-Net: the first holistic framework that solves\nboth the tasks of log-likelihood ratio (LLR) estimation and quantization using\na data-driven method. We motivate our approach with theoretical insights on two\npractical estimation algorithms at the ends of the complexity spectrum and\nreveal a connection between the complexity of an algorithm and the information\nbottleneck method: simpler algorithms admit smaller bottlenecks when\nrepresenting their solution. This motivates us to propose a two-stage algorithm\nthat uses LLR compression as a pretext task for estimation and is focused on\nlow-latency, high-performance implementations via deep neural networks. We\ncarry out extensive experimental evaluation and demonstrate that our single\narchitecture achieves state-of-the-art results on both tasks when compared to\nprevious methods, with gains in quantization efficiency as high as $20\\%$ and\nreduced estimation latency by up to $60\\%$ when measured on general purpose and\ngraphical processing units (GPU). In particular, our approach reduces the GPU\ninference latency by more than two times in several multiple-input\nmultiple-output (MIMO) configurations. Finally, we demonstrate that our scheme\nis robust to distributional shifts and retains a significant part of its\nperformance when evaluated on 5G channel models, as well as channel estimation\nerrors.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 18:11:30 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 17:27:05 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Arvinte", "Marius", ""], ["Tewfik", "Ahmed H.", ""], ["Vishwanath", "Sriram", ""]]}, {"id": "2012.12862", "submitter": "G\\\"okhan \\c{C}apan", "authors": "G\\\"okhan \\c{C}apan, \\\"Ozge Bozal, \\.Ilker G\\\"undo\\u{g}du, Ali Taylan\n  Cemgil", "title": "Towards Fair Personalization by Avoiding Feedback Loops", "comments": "NeurIPS 2019 Workshop on Human-Centric Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-reinforcing feedback loops are both cause and effect of over and/or\nunder-presentation of some content in interactive recommender systems. This\nleads to erroneous user preference estimates, namely, overestimation of\nover-presented content while violating the right to be presented of each\nalternative, contrary of which we define as a fair system. We consider two\nmodels that explicitly incorporate, or ignore the systematic and limited\nexposure to alternatives. By simulations, we demonstrate that ignoring the\nsystematic presentations overestimates promoted options and underestimates\ncensored alternatives. Simply conditioning on the limited exposure is a remedy\nfor these biases.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 19:28:57 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["\u00c7apan", "G\u00f6khan", ""], ["Bozal", "\u00d6zge", ""], ["G\u00fcndo\u011fdu", "\u0130lker", ""], ["Cemgil", "Ali Taylan", ""]]}, {"id": "2012.12896", "submitter": "Jingling Li", "authors": "Jingling Li, Mozhi Zhang, Keyulu Xu, John P. Dickerson, Jimmy Ba", "title": "Noisy Labels Can Induce Good Representations", "comments": "27 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current success of deep learning depends on large-scale labeled datasets.\nIn practice, high-quality annotations are expensive to collect, but noisy\nannotations are more affordable. Previous works report mixed empirical results\nwhen training with noisy labels: neural networks can easily memorize random\nlabels, but they can also generalize from noisy labels. To explain this puzzle,\nwe study how architecture affects learning with noisy labels. We observe that\nif an architecture \"suits\" the task, training with noisy labels can induce\nuseful hidden representations, even when the model generalizes poorly; i.e.,\nthe last few layers of the model are more negatively affected by noisy labels.\nThis finding leads to a simple method to improve models trained on noisy\nlabels: replacing the final dense layers with a linear model, whose weights are\nlearned from a small set of clean data. We empirically validate our findings\nacross three architectures (Convolutional Neural Networks, Graph Neural\nNetworks, and Multi-Layer Perceptrons) and two domains (graph algorithmic tasks\nand image classification). Furthermore, we achieve state-of-the-art results on\nimage classification benchmarks by combining our method with existing\napproaches on noisy label training.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 18:58:05 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Li", "Jingling", ""], ["Zhang", "Mozhi", ""], ["Xu", "Keyulu", ""], ["Dickerson", "John P.", ""], ["Ba", "Jimmy", ""]]}, {"id": "2012.12901", "submitter": "Daniel Schuh", "authors": "Matteo Favoni, Andreas Ipp, David I. M\\\"uller, Daniel Schuh", "title": "Lattice gauge equivariant convolutional neural networks", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-lat cs.LG hep-ph hep-th stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Lattice gauge equivariant Convolutional Neural Networks (L-CNNs)\nfor generic machine learning applications on lattice gauge theoretical\nproblems. At the heart of this network structure is a novel convolutional layer\nthat preserves gauge equivariance while forming arbitrarily shaped Wilson loops\nin successive bilinear layers. Together with topological information, for\nexample from Polyakov loops, such a network can in principle approximate any\ngauge covariant function on the lattice. We demonstrate that L-CNNs can learn\nand generalize gauge invariant quantities that traditional convolutional neural\nnetworks are incapable of finding.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 19:00:01 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Favoni", "Matteo", ""], ["Ipp", "Andreas", ""], ["M\u00fcller", "David I.", ""], ["Schuh", "Daniel", ""]]}, {"id": "2012.12917", "submitter": "Mattes Mollenhauer", "authors": "Mattes Mollenhauer and P\\'eter Koltai", "title": "Nonparametric approximation of conditional expectation operators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.NA math.NA stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given the joint distribution of two random variables $X,Y$ on some second\ncountable locally compact Hausdorff space, we investigate the statistical\napproximation of the $L^2$-operator defined by $[Pf](x) := \\mathbb{E}[ f(Y)\n\\mid X = x ]$ under minimal assumptions. By modifying its domain, we prove that\n$P$ can be arbitrarily well approximated in operator norm by Hilbert--Schmidt\noperators acting on a reproducing kernel Hilbert space. This fact allows to\nestimate $P$ uniformly by finite-rank operators over a dense subspace even when\n$P$ is not compact. In terms of modes of convergence, we thereby obtain the\nsuperiority of kernel-based techniques over classically used parametric\nprojection approaches such as Galerkin methods. This also provides a novel\nperspective on which limiting object the nonparametric estimate of $P$\nconverges to. As an application, we show that these results are particularly\nimportant for a large family of spectral analysis techniques for Markov\ntransition operators. Our investigation also gives a new asymptotic perspective\non the so-called kernel conditional mean embedding, which is the theoretical\nfoundation of a wide variety of techniques in kernel-based nonparametric\ninference.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 19:06:12 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 09:45:41 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Mollenhauer", "Mattes", ""], ["Koltai", "P\u00e9ter", ""]]}, {"id": "2012.12931", "submitter": "Lingxiao Zhao", "authors": "Lingxiao Zhao, Leman Akoglu", "title": "On Using Classification Datasets to Evaluate Graph-Level Outlier\n  Detection: Peculiar Observations and New Insights", "comments": "extensive revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  It is common practice of the outlier mining community to repurpose\nclassification datasets toward evaluating various detection models. To that\nend, often a binary classification dataset is used, where samples from one of\nthe classes is designated as the inlier samples, and the other class is\nsubstantially down-sampled to create the ground-truth outlier samples.\nGraph-level outlier detection (GLOD) is rarely studied but has many potentially\ninfluential real-world applications. In this study, we identify an intriguing\nissue with repurposing graph classification datasets for GLOD. We find that\nROC-AUC performance of the models changes significantly (flips from high to\nvery low, even worse than random) depending on which class is down-sampled.\nInterestingly, ROC-AUCs on these two variants approximately sum to 1 and their\nperformance gap is amplified with increasing propagations for a certain family\nof propagation based outlier detection models. We carefully study the graph\nembedding space produced by propagation based models and find two driving\nfactors: (1) disparity between within-class densities which is amplified by\npropagation, and (2)overlapping support (mixing of embeddings) across classes.\nWe also study other graph embedding methods and downstream outlier detectors,\nand find that the intriguing performance flip issue still widely exists but\nwhich version of the downsample achieves higher performance may vary.\nThoughtful analysis over comprehensive results further deeper our understanding\nof the established issue.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 19:38:21 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 18:34:03 GMT"}, {"version": "v3", "created": "Tue, 18 May 2021 23:07:33 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Zhao", "Lingxiao", ""], ["Akoglu", "Leman", ""]]}, {"id": "2012.13045", "submitter": "Aldo Pacchiano", "authors": "Aldo Pacchiano, Christoph Dann, Claudio Gentile, Peter Bartlett", "title": "Regret Bound Balancing and Elimination for Model Selection in Bandits\n  and RL", "comments": "57 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple model selection approach for algorithms in stochastic\nbandit and reinforcement learning problems. As opposed to prior work that\n(implicitly) assumes knowledge of the optimal regret, we only require that each\nbase algorithm comes with a candidate regret bound that may or may not hold\nduring all rounds. In each round, our approach plays a base algorithm to keep\nthe candidate regret bounds of all remaining base algorithms balanced, and\neliminates algorithms that violate their candidate bound. We prove that the\ntotal regret of this approach is bounded by the best valid candidate regret\nbound times a multiplicative factor. This factor is reasonably small in several\napplications, including linear bandits and MDPs with nested function classes,\nlinear bandits with unknown misspecification, and LinUCB applied to linear\nbandits with different confidence parameters. We further show that, under a\nsuitable gap-assumption, this factor only scales with the number of base\nalgorithms and not their complexity when the number of rounds is large enough.\nFinally, unlike recent efforts in model selection for linear stochastic\nbandits, our approach is versatile enough to also cover cases where the context\ninformation is generated by an adversarial environment, rather than a\nstochastic one.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 00:53:42 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Pacchiano", "Aldo", ""], ["Dann", "Christoph", ""], ["Gentile", "Claudio", ""], ["Bartlett", "Peter", ""]]}, {"id": "2012.13088", "submitter": "Eric Han", "authors": "Eric Han, Ishank Arora, Jonathan Scarlett", "title": "High-Dimensional Bayesian Optimization via Tree-Structured Additive\n  Models", "comments": "To appear in AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian Optimization (BO) has shown significant success in tackling\nexpensive low-dimensional black-box optimization problems. Many optimization\nproblems of interest are high-dimensional, and scaling BO to such settings\nremains an important challenge. In this paper, we consider generalized additive\nmodels in which low-dimensional functions with overlapping subsets of variables\nare composed to model a high-dimensional target function. Our goal is to lower\nthe computational resources required and facilitate faster model learning by\nreducing the model complexity while retaining the sample-efficiency of existing\nmethods. Specifically, we constrain the underlying dependency graphs to tree\nstructures in order to facilitate both the structure learning and optimization\nof the acquisition function. For the former, we propose a hybrid graph learning\nalgorithm based on Gibbs sampling and mutation. In addition, we propose a novel\nzooming-based algorithm that permits generalized additive models to be employed\nmore efficiently in the case of continuous domains. We demonstrate and discuss\nthe efficacy of our approach via a range of experiments on synthetic functions\nand real-world datasets.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 03:56:44 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Han", "Eric", ""], ["Arora", "Ishank", ""], ["Scarlett", "Jonathan", ""]]}, {"id": "2012.13112", "submitter": "David Walsh", "authors": "David Walsh, Alejandro Schuler, Diana Hall, Jon Walsh, Charles Fisher", "title": "Bayesian prognostic covariate adjustment", "comments": "28 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Historical data about disease outcomes can be integrated into the analysis of\nclinical trials in many ways. We build on existing literature that uses\nprognostic scores from a predictive model to increase the efficiency of\ntreatment effect estimates via covariate adjustment. Here we go further,\nutilizing a Bayesian framework that combines prognostic covariate adjustment\nwith an empirical prior distribution learned from the predictive performances\nof the prognostic model on past trials. The Bayesian approach interpolates\nbetween prognostic covariate adjustment with strict type I error control when\nthe prior is diffuse, and a single-arm trial when the prior is sharply peaked.\nThis method is shown theoretically to offer a substantial increase in\nstatistical power, while limiting the type I error rate under reasonable\nconditions. We demonstrate the utility of our method in simulations and with an\nanalysis of a past Alzheimer's disease clinical trial.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 05:19:03 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Walsh", "David", ""], ["Schuler", "Alejandro", ""], ["Hall", "Diana", ""], ["Walsh", "Jon", ""], ["Fisher", "Charles", ""]]}, {"id": "2012.13115", "submitter": "Ashok Cutkosky", "authors": "Ashok Cutkosky, Abhimanyu Das, Manish Purohit", "title": "Upper Confidence Bounds for Combining Stochastic Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a simple method to combine stochastic bandit algorithms. Our\napproach is based on a \"meta-UCB\" procedure that treats each of $N$ individual\nbandit algorithms as arms in a higher-level $N$-armed bandit problem that we\nsolve with a variant of the classic UCB algorithm. Our final regret depends\nonly on the regret of the base algorithm with the best regret in hindsight.\nThis approach provides an easy and intuitive alternative strategy to the CORRAL\nalgorithm for adversarial bandits, without requiring the stability conditions\nimposed by CORRAL on the base algorithms. Our results match lower bounds in\nseveral settings, and we provide empirical validation of our algorithm on\nmisspecified linear bandit and model selection problems.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 05:36:29 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Cutkosky", "Ashok", ""], ["Das", "Abhimanyu", ""], ["Purohit", "Manish", ""]]}, {"id": "2012.13189", "submitter": "Yves Rychener", "authors": "Yves Rychener, Xavier Renard, Djam\\'e Seddah, Pascal Frossard, Marcin\n  Detyniecki", "title": "Sentence-Based Model Agnostic NLP Interpretability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, interpretability of Black-Box Natural Language Processing (NLP) models\nbased on surrogates, like LIME or SHAP, uses word-based sampling to build the\nexplanations. In this paper we explore the use of sentences to tackle NLP\ninterpretability. While this choice may seem straight forward, we show that,\nwhen using complex classifiers like BERT, the word-based approach raises issues\nnot only of computational complexity, but also of an out of distribution\nsampling, eventually leading to non founded explanations. By using sentences,\nthe altered text remains in-distribution and the dimensionality of the problem\nis reduced for better fidelity to the black-box at comparable computational\ncomplexity.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 10:32:41 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2020 17:54:38 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Rychener", "Yves", ""], ["Renard", "Xavier", ""], ["Seddah", "Djam\u00e9", ""], ["Frossard", "Pascal", ""], ["Detyniecki", "Marcin", ""]]}, {"id": "2012.13190", "submitter": "Yves Rychener", "authors": "Yves Rychener, Xavier Renard, Djam\\'e Seddah, Pascal Frossard, Marcin\n  Detyniecki", "title": "QUACKIE: A NLP Classification Task With Ground Truth Explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NLP Interpretability aims to increase trust in model predictions. This makes\nevaluating interpretability approaches a pressing issue. There are multiple\ndatasets for evaluating NLP Interpretability, but their dependence on human\nprovided ground truths raises questions about their unbiasedness. In this work,\nwe take a different approach and formulate a specific classification task by\ndiverting question-answering datasets. For this custom classification task, the\ninterpretability ground-truth arises directly from the definition of the\nclassification problem. We use this method to propose a benchmark and lay the\ngroundwork for future research in NLP interpretability by evaluating a wide\nrange of current state of the art methods.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 10:43:20 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2020 18:04:17 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Rychener", "Yves", ""], ["Renard", "Xavier", ""], ["Seddah", "Djam\u00e9", ""], ["Frossard", "Pascal", ""], ["Detyniecki", "Marcin", ""]]}, {"id": "2012.13196", "submitter": "Daniel O'Connor", "authors": "Daniel O'Connor, Walter Vinci", "title": "RBM-Flow and D-Flow: Invertible Flows with Discrete Energy Base Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient sampling of complex data distributions can be achieved using\ntrained invertible flows (IF), where the model distribution is generated by\npushing a simple base distribution through multiple non-linear bijective\ntransformations. However, the iterative nature of the transformations in IFs\ncan limit the approximation to the target distribution. In this paper we seek\nto mitigate this by implementing RBM-Flow, an IF model whose base distribution\nis a Restricted Boltzmann Machine (RBM) with a continuous smoothing applied. We\nshow that by using RBM-Flow we are able to improve the quality of samples\ngenerated, quantified by the Inception Scores (IS) and Frechet Inception\nDistance (FID), over baseline models with the same IF transformations, but with\nless expressive base distributions. Furthermore, we also obtain D-Flow, an IF\nmodel with uncorrelated discrete latent variables. We show that D-Flow achieves\nsimilar likelihoods and FID/IS scores to those of a typical IF with Gaussian\nbase variables, but with the additional benefit that global features are\nmeaningfully encoded as discrete labels in the latent space.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 11:05:27 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 16:03:39 GMT"}, {"version": "v3", "created": "Mon, 12 Jul 2021 10:00:47 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["O'Connor", "Daniel", ""], ["Vinci", "Walter", ""]]}, {"id": "2012.13220", "submitter": "Jishnu Mukhoti", "authors": "Jishnu Mukhoti, Puneet K. Dokania, Philip H.S. Torr, Yarin Gal", "title": "On Batch Normalisation for Approximate Bayesian Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study batch normalisation in the context of variational inference methods\nin Bayesian neural networks, such as mean-field or MC Dropout. We show that\nbatch-normalisation does not affect the optimum of the evidence lower bound\n(ELBO). Furthermore, we study the Monte Carlo Batch Normalisation (MCBN)\nalgorithm, proposed as an approximate inference technique parallel to MC\nDropout, and show that for larger batch sizes, MCBN fails to capture epistemic\nuncertainty. Finally, we provide insights into what is required to fix this\nfailure, namely having to view the mini-batch size as a variational parameter\nin MCBN. We comment on the asymptotics of the ELBO with respect to this\nvariational parameter, showing that as dataset size increases towards infinity,\nthe batch-size must increase towards infinity as well for MCBN to be a valid\napproximate inference technique.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 12:40:11 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Mukhoti", "Jishnu", ""], ["Dokania", "Puneet K.", ""], ["Torr", "Philip H. S.", ""], ["Gal", "Yarin", ""]]}, {"id": "2012.13248", "submitter": "Ahmed Allam", "authors": "Kyriakos Schwarz, Ahmed Allam, Nicolas Andres Perez Gonzalez, Michael\n  Krauthammer", "title": "AttentionDDI: Siamese Attention-based Deep Learning method for drug-drug\n  interaction predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Drug-drug interactions (DDIs) refer to processes triggered by the\nadministration of two or more drugs leading to side effects beyond those\nobserved when drugs are administered by themselves. Due to the massive number\nof possible drug pairs, it is nearly impossible to experimentally test all\ncombinations and discover previously unobserved side effects. Therefore,\nmachine learning based methods are being used to address this issue.\n  Methods: We propose a Siamese self-attention multi-modal neural network for\nDDI prediction that integrates multiple drug similarity measures that have been\nderived from a comparison of drug characteristics including drug targets,\npathways and gene expression profiles.\n  Results: Our proposed DDI prediction model provides multiple advantages: 1)\nIt is trained end-to-end, overcoming limitations of models composed of multiple\nseparate steps, 2) it offers model explainability via an Attention mechanism\nfor identifying salient input features and 3) it achieves similar or better\nprediction performance (AUPR scores ranging from 0.77 to 0.92) compared to\nstate-of-the-art DDI models when tested on various benchmark datasets. Novel\nDDI predictions are further validated using independent data resources.\n  Conclusions: We find that a Siamese multi-modal neural network is able to\naccurately predict DDIs and that an Attention mechanism, typically used in the\nNatural Language Processing domain, can be beneficially applied to aid in DDI\nmodel explainability.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 13:33:07 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Schwarz", "Kyriakos", ""], ["Allam", "Ahmed", ""], ["Gonzalez", "Nicolas Andres Perez", ""], ["Krauthammer", "Michael", ""]]}, {"id": "2012.13311", "submitter": "Simon Passenheim", "authors": "Simon Passenheim and Emiel Hoogeboom", "title": "Variational Determinant Estimation with Spherical Normalizing Flows", "comments": "Accepted at 3rd Symposium on Advances in Approximate Bayesian\n  Inference (AABI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces the Variational Determinant Estimator (VDE), a\nvariational extension of the recently proposed determinant estimator discovered\nby arXiv:2005.06553v2. Our estimator significantly reduces the variance even\nfor low sample sizes by combining (importance-weighted) variational inference\nand a family of normalizing flows which allow density estimation on\nhyperspheres. In the ideal case of a tight variational bound, the VDE becomes a\nzero variance estimator, and a single sample is sufficient for an exact (log)\ndeterminant estimate.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 16:13:49 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 00:18:55 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2021 16:52:30 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Passenheim", "Simon", ""], ["Hoogeboom", "Emiel", ""]]}, {"id": "2012.13326", "submitter": "Qinghua Liu", "authors": "Qinghua Liu, Zhou Lu", "title": "A Tight Lower Bound for Uniformly Stable Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leveraging algorithmic stability to derive sharp generalization bounds is a\nclassic and powerful approach in learning theory. Since Vapnik and Chervonenkis\n[1974] first formalized the idea for analyzing SVMs, it has been utilized to\nstudy many fundamental learning algorithms (e.g., $k$-nearest neighbors [Rogers\nand Wagner, 1978], stochastic gradient method [Hardt et al., 2016], linear\nregression [Maurer, 2017], etc). In a recent line of great works by Feldman and\nVondrak [2018, 2019] as well as Bousquet et al. [2020b], they prove a high\nprobability generalization upper bound of order $\\tilde{\\mathcal{O}}(\\gamma\n+\\frac{L}{\\sqrt{n}})$ for any uniformly $\\gamma$-stable algorithm and\n$L$-bounded loss function. Although much progress was achieved in proving\ngeneralization upper bounds for stable algorithms, our knowledge of lower\nbounds is rather limited. In fact, there is no nontrivial lower bound known\never since the study of uniform stability [Bousquet and Elisseeff, 2002], to\nthe best of our knowledge. In this paper we fill the gap by proving a tight\ngeneralization lower bound of order $\\Omega(\\gamma+\\frac{L}{\\sqrt{n}})$, which\nmatches the best known upper bound up to logarithmic factors\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 17:01:18 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2021 22:46:24 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Liu", "Qinghua", ""], ["Lu", "Zhou", ""]]}, {"id": "2012.13329", "submitter": "Arda Sahiner", "authors": "Arda Sahiner, Tolga Ergen, John Pauly and Mert Pilanci", "title": "Vector-output ReLU Neural Network Problems are Copositive Programs:\n  Convex Analysis of Two Layer Networks and Polynomial-time Algorithms", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe the convex semi-infinite dual of the two-layer vector-output ReLU\nneural network training problem. This semi-infinite dual admits a finite\ndimensional representation, but its support is over a convex set which is\ndifficult to characterize. In particular, we demonstrate that the non-convex\nneural network training problem is equivalent to a finite-dimensional convex\ncopositive program. Our work is the first to identify this strong connection\nbetween the global optima of neural networks and those of copositive programs.\nWe thus demonstrate how neural networks implicitly attempt to solve copositive\nprograms via semi-nonnegative matrix factorization, and draw key insights from\nthis formulation. We describe the first algorithms for provably finding the\nglobal minimum of the vector output neural network training problem, which are\npolynomial in the number of samples for a fixed data rank, yet exponential in\nthe dimension. However, in the case of convolutional architectures, the\ncomputational complexity is exponential in only the filter size and polynomial\nin all other parameters. We describe the circumstances in which we can find the\nglobal optimum of this neural network training problem exactly with\nsoft-thresholded SVD, and provide a copositive relaxation which is guaranteed\nto be exact for certain classes of problems, and which corresponds with the\nsolution of Stochastic Gradient Descent in practice.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 17:03:30 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Sahiner", "Arda", ""], ["Ergen", "Tolga", ""], ["Pauly", "John", ""], ["Pilanci", "Mert", ""]]}, {"id": "2012.13435", "submitter": "Sree Ram Kamabattula", "authors": "Sree Ram Kamabattula, Venkat Devarajan, Babak Namazi, Ganesh\n  Sankaranarayanan", "title": "Identifying Training Stop Point with Noisy Labeled Data", "comments": "Published in: 2020 International Conference on Computational Science\n  and Computational Intelligence (CSCI)", "journal-ref": null, "doi": "10.1109/CSCI51800.2020.00084", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural networks (DNNs) with noisy labels is a challenging\nproblem due to over-parameterization. DNNs tend to essentially fit on clean\nsamples at a higher rate in the initial stages, and later fit on the noisy\nsamples at a relatively lower rate. Thus, with a noisy dataset, the test\naccuracy increases initially and drops in the later stages. To find an early\nstopping point at the maximum obtainable test accuracy (MOTA), recent studies\nassume either that i) a clean validation set is available or ii) the noise\nratio is known, or, both. However, often a clean validation set is unavailable,\nand the noise estimation can be inaccurate. To overcome these issues, we\nprovide a novel training solution, free of these conditions. We analyze the\nrate of change of the training accuracy for different noise ratios under\ndifferent conditions to identify a training stop region. We further develop a\nheuristic algorithm based on a small-learning assumption to find a training\nstop point (TSP) at or close to MOTA. To the best of our knowledge, our method\nis the first to rely solely on the \\textit{training behavior}, while utilizing\nthe entire training set, to automatically find a TSP. We validated the\nrobustness of our algorithm (AutoTSP) through several experiments on CIFAR-10,\nCIFAR-100, and a real-world noisy dataset for different noise ratios, noise\ntypes, and architectures.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 20:07:30 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 15:19:05 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Kamabattula", "Sree Ram", ""], ["Devarajan", "Venkat", ""], ["Namazi", "Babak", ""], ["Sankaranarayanan", "Ganesh", ""]]}, {"id": "2012.13453", "submitter": "Lukas Franken", "authors": "L. Franken, B. Georgiev, S. Muecke, M. Wolter, N. Piatkowski and C.\n  Bauckhage", "title": "Gradient-free quantum optimization on NISQ devices", "comments": "13 pages, 6 figures, comments welcome!", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Variational Quantum Eigensolvers (VQEs) have recently attracted considerable\nattention. Yet, in practice, they still suffer from the efforts for estimating\ncost function gradients for large parameter sets or resource-demanding\nreinforcement strategies. Here, we therefore consider recent advances in\nweight-agnostic learning and propose a strategy that addresses the trade-off\nbetween finding appropriate circuit architectures and parameter tuning. We\ninvestigate the use of NEAT-inspired algorithms which evaluate circuits via\ngenetic competition and thus circumvent issues due to exceeding numbers of\nparameters. Our methods are tested both via simulation and on real quantum\nhardware and are used to solve the transverse Ising Hamiltonian and the\nSherrington-Kirkpatrick spin model.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 10:24:54 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 13:58:13 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Franken", "L.", ""], ["Georgiev", "B.", ""], ["Muecke", "S.", ""], ["Wolter", "M.", ""], ["Piatkowski", "N.", ""], ["Bauckhage", "C.", ""]]}, {"id": "2012.13545", "submitter": "Ichiro Takeuchi Prof.", "authors": "Kazuya Sugiyama, Vo Nguyen Le Duy, Ichiro Takeuchi", "title": "More Powerful and General Selective Inference for Stepwise Feature\n  Selection using the Homotopy Continuation Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional selective inference (SI) has been actively studied as a new\nstatistical inference framework for data-driven hypotheses. The basic idea of\nconditional SI is to make inferences conditional on the selection event\ncharacterized by a set of linear and/or quadratic inequalities. Conditional SI\nhas been mainly studied in the context of feature selection such as stepwise\nfeature selection (SFS). The main limitation of the existing conditional SI\nmethods is the loss of power due to over-conditioning, which is required for\ncomputational tractability. In this study, we develop a more powerful and\ngeneral conditional SI method for SFS using the homotopy method which enables\nus to overcome this limitation. The homotopy-based SI is especially effective\nfor more complicated feature selection algorithms. As an example, we develop a\nconditional SI method for forward-backward SFS with AIC-based stopping criteria\nand show that it is not adversely affected by the increased complexity of the\nalgorithm. We conduct several experiments to demonstrate the effectiveness and\nefficiency of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 09:01:45 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 01:59:45 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Sugiyama", "Kazuya", ""], ["Duy", "Vo Nguyen Le", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "2012.13572", "submitter": "Elie Azeraf", "authors": "Elie Azeraf, Emmanuel Monfrini, Wojciech Pieczynski", "title": "Using the Naive Bayes as a discriminative classifier", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For classification tasks, probabilistic models can be categorized into two\ndisjoint classes: generative or discriminative. It depends on the posterior\nprobability computation of the label $x$ given the observation $y$, $p(x | y)$.\nOn the one hand, generative classifiers, like the Naive Bayes or the Hidden\nMarkov Model (HMM), need the computation of the joint probability p(x,y),\nbefore using the Bayes rule to compute $p(x | y)$. On the other hand,\ndiscriminative classifiers compute $p(x | y)$ directly, regardless of the\nobservations' law. They are intensively used nowadays, with models as Logistic\nRegression, Conditional Random Fields (CRF), and Artificial Neural Networks.\nHowever, the recent Entropic Forward-Backward algorithm shows that the HMM,\nconsidered as a generative model, can also match the discriminative one's\ndefinition. This example leads to question if it is the case for other\ngenerative models. In this paper, we show that the Naive Bayes classifier can\nalso match the discriminative classifier definition, so it can be used in\neither a generative or a discriminative way. Moreover, this observation also\ndiscusses the notion of Generative-Discriminative pairs, linking, for example,\nNaive Bayes and Logistic Regression, or HMM and CRF. Related to this point, we\nshow that the Logistic Regression can be viewed as a particular case of the\nNaive Bayes used in a discriminative way.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 13:32:23 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 20:03:51 GMT"}, {"version": "v3", "created": "Fri, 5 Mar 2021 16:15:28 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Azeraf", "Elie", ""], ["Monfrini", "Emmanuel", ""], ["Pieczynski", "Wojciech", ""]]}, {"id": "2012.13573", "submitter": "Fengxiang He", "authors": "Fengxiang He, Shaopeng Fu, Bohan Wang, Dacheng Tao", "title": "Robustness, Privacy, and Generalization of Adversarial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training can considerably robustify deep neural networks to\nresist adversarial attacks. However, some works suggested that adversarial\ntraining might comprise the privacy-preserving and generalization abilities.\nThis paper establishes and quantifies the privacy-robustness trade-off and\ngeneralization-robustness trade-off in adversarial training from both\ntheoretical and empirical aspects. We first define a notion, {\\it robustified\nintensity} to measure the robustness of an adversarial training algorithm. This\nmeasure can be approximate empirically by an asymptotically consistent\nempirical estimator, {\\it empirical robustified intensity}. Based on the\nrobustified intensity, we prove that (1) adversarial training is $(\\varepsilon,\n\\delta)$-differentially private, where the magnitude of the differential\nprivacy has a positive correlation with the robustified intensity; and (2) the\ngeneralization error of adversarial training can be upper bounded by an\n$\\mathcal O(\\sqrt{\\log N}/N)$ on-average bound and an $\\mathcal O(1/\\sqrt{N})$\nhigh-probability bound, both of which have positive correlations with the\nrobustified intensity. Additionally, our generalization bounds do not\nexplicitly rely on the parameter size which would be prohibitively large in\ndeep learning. Systematic experiments on standard datasets, CIFAR-10 and\nCIFAR-100, are in full agreement with our theories. The source code package is\navailable at \\url{https://github.com/fshp971/RPG}.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 13:35:02 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["He", "Fengxiang", ""], ["Fu", "Shaopeng", ""], ["Wang", "Bohan", ""], ["Tao", "Dacheng", ""]]}, {"id": "2012.13632", "submitter": "Huachuan Wang", "authors": "Huachuan Wang and James Ting-Ho Lo", "title": "Adaptively Solving the Local-Minimum Problem for Deep Neural Networks", "comments": "arXiv admin note: substantial text overlap with arXiv:1506.02690,\n  arXiv:1510.03826", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper aims to overcome a fundamental problem in the theory and\napplication of deep neural networks (DNNs). We propose a method to solve the\nlocal minimum problem in training DNNs directly. Our method is based on the\ncross-entropy loss criterion's convexification by transforming the\ncross-entropy loss into a risk averting error (RAE) criterion. To alleviate\nnumerical difficulties, a normalized RAE (NRAE) is employed. The convexity\nregion of the cross-entropy loss expands as its risk sensitivity index (RSI)\nincreases. Making the best use of the convexity region, our method starts\ntraining with an extensive RSI, gradually reduces it, and switches to the RAE\nas soon as the RAE is numerically feasible. After training converges, the\nresultant deep learning machine is expected to be inside the attraction basin\nof a global minimum of the cross-entropy loss. Numerical results are provided\nto show the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 21:51:48 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Wang", "Huachuan", ""], ["Lo", "James Ting-Ho", ""]]}, {"id": "2012.13717", "submitter": "Ahmad Kalhor", "authors": "Mostafa Kalhor, Ahmad Kalhor, and Mehdi Rahmani", "title": "Ranking and Rejecting of Pre-Trained Deep Neural Networks in Transfer\n  Learning based on Separation Index", "comments": "30 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automated ranking of pre-trained Deep Neural Networks (DNNs) reduces the\nrequired time for selecting optimal pre-trained DNN and boost the\nclassification performance in transfer learning. In this paper, we introduce a\nnovel algorithm to rank pre-trained DNNs by applying a straightforward\ndistance-based complexity measure named Separation Index (SI) to the target\ndataset. For this purpose, at first, a background about the SI is given and\nthen the automated ranking algorithm is explained. In this algorithm, the SI is\ncomputed for the target dataset which passes from the feature extracting parts\nof pre-trained DNNs. Then, by descending sort of the computed SIs, the\npre-trained DNNs are ranked, easily. In this ranking method, the best DNN makes\nmaximum SI on the target dataset and a few pre-trained DNNs may be rejected in\nthe case of their sufficiently low computed SIs. The efficiency of the proposed\nalgorithm is evaluated by using three challenging datasets including Linnaeus\n5, Breast Cancer Images, and COVID-CT. For the two first case studies, the\nresults of the proposed algorithm exactly match with the ranking of the trained\nDNNs by the accuracy on the target dataset. For the third case study, despite\nusing different preprocessing on the target data, the ranking of the algorithm\nhas a high correlation with the ranking resulted from classification accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 11:14:12 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Kalhor", "Mostafa", ""], ["Kalhor", "Ahmad", ""], ["Rahmani", "Mehdi", ""]]}, {"id": "2012.13760", "submitter": "Wenjie Li", "authors": "Wenjie Li, Zhanyu Wang, Yichen Zhang, Guang Cheng", "title": "Variance Reduction on Adaptive Stochastic Mirror Descent", "comments": "NeurIPS 2020 OPT workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the idea of variance reduction applied to adaptive\nstochastic mirror descent algorithms in the nonsmooth nonconvex finite-sum\noptimization problems. We propose a simple yet generalized adaptive mirror\ndescent algorithm with variance reduction named SVRAMD and provide its\nconvergence analysis in different settings. We prove that variance reduction\nreduces the SFO complexity of most adaptive mirror descent algorithms and\naccelerates their convergence. In particular, our general theory implies that\nvariance reduction can be applied to algorithms using time-varying step sizes\nand self-adaptive algorithms such as AdaGrad and RMSProp. Moreover, the\nconvergence rates of SVRAMD recover the best existing rates of non-adaptive\nvariance reduced mirror descent algorithms. We check the validity of our claims\nusing experiments in deep learning.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 15:15:51 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 17:19:41 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Li", "Wenjie", ""], ["Wang", "Zhanyu", ""], ["Zhang", "Yichen", ""], ["Cheng", "Guang", ""]]}, {"id": "2012.13779", "submitter": "Ismael Tito Freire Gonz\\'alez", "authors": "Ismael T. Freire, Adri\\'an F. Amil, Vasiliki Vouloutsi, Paul F.M.J.\n  Verschure", "title": "Towards sample-efficient episodic control with DAC-ML", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-bio.NC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The sample-inefficiency problem in Artificial Intelligence refers to the\ninability of current Deep Reinforcement Learning models to optimize action\npolicies within a small number of episodes. Recent studies have tried to\novercome this limitation by adding memory systems and architectural biases to\nimprove learning speed, such as in Episodic Reinforcement Learning. However,\ndespite achieving incremental improvements, their performance is still not\ncomparable to how humans learn behavioral policies. In this paper, we\ncapitalize on the design principles of the Distributed Adaptive Control (DAC)\ntheory of mind and brain to build a novel cognitive architecture (DAC-ML) that,\nby incorporating a hippocampus-inspired sequential memory system, can rapidly\nconverge to effective action policies that maximize reward acquisition in a\nchallenging foraging task.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 16:38:08 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Freire", "Ismael T.", ""], ["Amil", "Adri\u00e1n F.", ""], ["Vouloutsi", "Vasiliki", ""], ["Verschure", "Paul F. M. J.", ""]]}, {"id": "2012.13798", "submitter": "Manuele Leonelli", "authors": "Federico Carli, Manuele Leonelli, Gherardo Varando", "title": "A new class of generative classifiers based on staged tree models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative models for classification use the joint probability distribution\nof the class variable and the features to construct a decision rule. Among\ngenerative models, Bayesian networks and naive Bayes classifiers are the most\ncommonly used and provide a clear graphical representation of the relationship\namong all variables. However, these have the disadvantage of highly restricting\nthe type of relationships that could exist, by not allowing for\ncontext-specific independences. Here we introduce a new class of generative\nclassifiers, called staged tree classifiers, which formally account for\ncontext-specific independence. They are constructed by a partitioning of the\nvertices of an event tree from which conditional independence can be formally\nread. The naive staged tree classifier is also defined, which extends the\nclassic naive Bayes classifier whilst retaining the same complexity. An\nextensive simulation study shows that the classification accuracy of staged\ntree classifiers is competitive with those of state-of-the-art classifiers. An\napplied analysis to predict the fate of the passengers of the Titanic\nhighlights the insights that the new class of generative classifiers can give.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 19:30:35 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Carli", "Federico", ""], ["Leonelli", "Manuele", ""], ["Varando", "Gherardo", ""]]}, {"id": "2012.13805", "submitter": "Kunpeng Zhang", "authors": "Dongcheng Zhang, Kunpeng Zhang", "title": "Weighting-Based Treatment Effect Estimation via Distribution Learning", "comments": "33 pages, 16 tables, 7 figures, Github:\n  https://github.com/DLweighting/Distribution-Learning-based-weighting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG econ.EM stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing weighting methods for treatment effect estimation are often built\nupon the idea of propensity scores or covariate balance. They usually impose\nstrong assumptions on treatment assignment or outcome model to obtain unbiased\nestimation, such as linearity or specific functional forms, which easily leads\nto the major drawback of model mis-specification. In this paper, we aim to\nalleviate these issues by developing a distribution learning-based weighting\nmethod. We first learn the true underlying distribution of covariates\nconditioned on treatment assignment, then leverage the ratio of covariates'\ndensity in the treatment group to that of the control group as the weight for\nestimating treatment effects. Specifically, we propose to approximate the\ndistribution of covariates in both treatment and control groups through\ninvertible transformations via change of variables. To demonstrate the\nsuperiority, robustness, and generalizability of our method, we conduct\nextensive experiments using synthetic and real data. From the experiment\nresults, we find that our method for estimating average treatment effect on\ntreated (ATT) with observational data outperforms several cutting-edge\nweighting-only benchmarking methods, and it maintains its advantage under a\ndoubly-robust estimation framework that combines weighting with some advanced\noutcome modeling methods.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 20:15:44 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 18:36:16 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2021 04:23:34 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Zhang", "Dongcheng", ""], ["Zhang", "Kunpeng", ""]]}, {"id": "2012.13841", "submitter": "Johan Bjorck", "authors": "Johan Bjorck, Kilian Weinberger and Carla Gomes", "title": "Understanding Decoupled and Early Weight Decay", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Weight decay (WD) is a traditional regularization technique in deep learning,\nbut despite its ubiquity, its behavior is still an area of active research.\nGolatkar et al. have recently shown that WD only matters at the start of the\ntraining in computer vision, upending traditional wisdom. Loshchilov et al.\nshow that for adaptive optimizers, manually decaying weights can outperform\nadding an $l_2$ penalty to the loss. This technique has become increasingly\npopular and is referred to as decoupled WD. The goal of this paper is to\ninvestigate these two recent empirical observations. We demonstrate that by\napplying WD only at the start, the network norm stays small throughout\ntraining. This has a regularizing effect as the effective gradient updates\nbecome larger. However, traditional generalizations metrics fail to capture\nthis effect of WD, and we show how a simple scale-invariant metric can. We also\nshow how the growth of network weights is heavily influenced by the dataset and\nits generalization properties. For decoupled WD, we perform experiments in NLP\nand RL where adaptive optimizers are the norm. We demonstrate that the primary\nissue that decoupled WD alleviates is the mixing of gradients from the\nobjective function and the $l_2$ penalty in the buffers of Adam (which stores\nthe estimates of the first-order moment). Adaptivity itself is not problematic\nand decoupled WD ensures that the gradients from the $l_2$ term cannot \"drown\nout\" the true objective, facilitating easier hyperparameter tuning.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 00:59:30 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Bjorck", "Johan", ""], ["Weinberger", "Kilian", ""], ["Gomes", "Carla", ""]]}, {"id": "2012.13882", "submitter": "Wataru Kumagai", "authors": "Wataru Kumagai, Akiyoshi Sannai", "title": "Universal Approximation Theorem for Equivariant Maps by Group CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group symmetry is inherent in a wide variety of data distributions. Data\nprocessing that preserves symmetry is described as an equivariant map and often\neffective in achieving high performance. Convolutional neural networks (CNNs)\nhave been known as models with equivariance and shown to approximate\nequivariant maps for some specific groups. However, universal approximation\ntheorems for CNNs have been separately derived with individual techniques\naccording to each group and setting. This paper provides a unified method to\nobtain universal approximation theorems for equivariant maps by CNNs in various\nsettings. As its significant advantage, we can handle non-linear equivariant\nmaps between infinite-dimensional spaces for non-compact groups.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 07:09:06 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Kumagai", "Wataru", ""], ["Sannai", "Akiyoshi", ""]]}, {"id": "2012.13892", "submitter": "Yanyong Huang", "authors": "Yanyong Huang, Zongxin Shen, Fuxu Cai, Tianrui Li, Fengmao Lv", "title": "Adaptive Graph-based Generalized Regression Model for Unsupervised\n  Feature Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised feature selection is an important method to reduce dimensions of\nhigh dimensional data without labels, which is benefit to avoid ``curse of\ndimensionality'' and improve the performance of subsequent machine learning\ntasks, like clustering and retrieval. How to select the uncorrelated and\ndiscriminative features is the key problem of unsupervised feature selection.\nMany proposed methods select features with strong discriminant and high\nredundancy, or vice versa. However, they only satisfy one of these two\ncriteria. Other existing methods choose the discriminative features with low\nredundancy by constructing the graph matrix on the original feature space.\nSince the original feature space usually contains redundancy and noise, it will\ndegrade the performance of feature selection. In order to address these issues,\nwe first present a novel generalized regression model imposed by an\nuncorrelated constraint and the $\\ell_{2,1}$-norm regularization. It can\nsimultaneously select the uncorrelated and discriminative features as well as\nreduce the variance of these data points belonging to the same neighborhood,\nwhich is help for the clustering task. Furthermore, the local intrinsic\nstructure of data is constructed on the reduced dimensional space by learning\nthe similarity-induced graph adaptively. Then the learnings of the graph\nstructure and the indicator matrix based on the spectral analysis are\nintegrated into the generalized regression model. Finally, we develop an\nalternative iterative optimization algorithm to solve the objective function. A\nseries of experiments are carried out on nine real-world data sets to\ndemonstrate the effectiveness of the proposed method in comparison with other\ncompeting approaches.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 09:07:26 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Huang", "Yanyong", ""], ["Shen", "Zongxin", ""], ["Cai", "Fuxu", ""], ["Li", "Tianrui", ""], ["Lv", "Fengmao", ""]]}, {"id": "2012.13940", "submitter": "Zeyu Zheng", "authors": "Yufeng Zheng, Zeyu Zheng", "title": "Doubly Stochastic Generative Arrivals Modeling", "comments": "updated version with more explanatory figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework named DS-WGAN that integrates the doubly\nstochastic (DS) structure and the Wasserstein generative adversarial networks\n(WGAN) to model, estimate, and simulate a wide class of arrival processes with\ngeneral non-stationary and random arrival rates. Regarding statistical\nproperties, we prove consistency and convergence rate for the estimator solved\nby the DS-WGAN framework under a non-parametric smoothness condition. Regarding\ncomputational efficiency and tractability, we address a challenge in gradient\nevaluation and model estimation, arised from the discontinuity in the\nsimulator. We then show that the DS-WGAN framework can conveniently facilitate\nwhat-if simulation and predictive simulation for future scenarios that are\ndifferent from the history. Numerical experiments with synthetic and real data\nsets are implemented to demonstrate the performance of DS-WGAN. The performance\nis measured from both a statistical perspective and an operational performance\nevaluation perspective. Numerical experiments suggest that, in terms of\nperformance, the successful model estimation for DS-WGAN only requires a\nmoderate size of representative data, which can be appealing in many contexts\nof operational management.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 13:32:16 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 05:21:24 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Zheng", "Yufeng", ""], ["Zheng", "Zeyu", ""]]}, {"id": "2012.13962", "submitter": "Felix Leibfried", "authors": "Felix Leibfried, Vincent Dutordoir, ST John, Nicolas Durrande", "title": "A Tutorial on Sparse Gaussian Processes and Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs) provide a framework for Bayesian inference that can\noffer principled uncertainty estimates for a large range of problems. For\nexample, if we consider regression problems with Gaussian likelihoods, a GP\nmodel enjoys a posterior in closed form. However, identifying the posterior GP\nscales cubically with the number of training examples and requires to store all\nexamples in memory. In order to overcome these obstacles, sparse GPs have been\nproposed that approximate the true posterior GP with pseudo-training examples.\nImportantly, the number of pseudo-training examples is user-defined and enables\ncontrol over computational and memory complexity. In the general case, sparse\nGPs do not enjoy closed-form solutions and one has to resort to approximate\ninference. In this context, a convenient choice for approximate inference is\nvariational inference (VI), where the problem of Bayesian inference is cast as\nan optimization problem -- namely, to maximize a lower bound of the log\nmarginal likelihood. This paves the way for a powerful and versatile framework,\nwhere pseudo-training examples are treated as optimization arguments of the\napproximate posterior that are jointly identified together with hyperparameters\nof the generative model (i.e. prior and likelihood). The framework can\nnaturally handle a wide scope of supervised learning problems, ranging from\nregression with heteroscedastic and non-Gaussian likelihoods to classification\nproblems with discrete labels, but also multilabel problems. The purpose of\nthis tutorial is to provide access to the basic matter for readers without\nprior knowledge in both GPs and VI. A proper exposition to the subject enables\nalso access to more recent advances (like importance-weighted VI as well as\ninterdomain, multioutput and deep GPs) that can serve as an inspiration for new\nresearch ideas.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 15:25:13 GMT"}, {"version": "v10", "created": "Fri, 11 Jun 2021 23:39:42 GMT"}, {"version": "v11", "created": "Fri, 2 Jul 2021 23:06:11 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2020 12:25:12 GMT"}, {"version": "v3", "created": "Sun, 3 Jan 2021 17:18:26 GMT"}, {"version": "v4", "created": "Mon, 11 Jan 2021 08:44:30 GMT"}, {"version": "v5", "created": "Fri, 15 Jan 2021 09:58:34 GMT"}, {"version": "v6", "created": "Fri, 29 Jan 2021 11:51:16 GMT"}, {"version": "v7", "created": "Tue, 2 Feb 2021 17:02:43 GMT"}, {"version": "v8", "created": "Fri, 16 Apr 2021 11:21:38 GMT"}, {"version": "v9", "created": "Wed, 2 Jun 2021 19:29:18 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Leibfried", "Felix", ""], ["Dutordoir", "Vincent", ""], ["John", "ST", ""], ["Durrande", "Nicolas", ""]]}, {"id": "2012.13976", "submitter": "Raghavendra Addanki", "authors": "Raghavendra Addanki, Andrew McGregor, Cameron Musco", "title": "Intervention Efficient Algorithms for Approximate Learning of Causal\n  Graphs", "comments": "To appear, International Conference on Algorithmic Learning\n  Theory(ALT) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning the causal relationships between a set of\nobserved variables in the presence of latents, while minimizing the cost of\ninterventions on the observed variables. We assume access to an undirected\ngraph $G$ on the observed variables whose edges represent either all direct\ncausal relationships or, less restrictively, a superset of causal relationships\n(identified, e.g., via conditional independence tests or a domain expert). Our\ngoal is to recover the directions of all causal or ancestral relations in $G$,\nvia a minimum cost set of interventions. It is known that constructing an exact\nminimum cost intervention set for an arbitrary graph $G$ is NP-hard. We further\nargue that, conditioned on the hardness of approximate graph coloring, no\npolynomial time algorithm can achieve an approximation factor better than\n$\\Theta(\\log n)$, where $n$ is the number of observed variables in $G$. To\novercome this limitation, we introduce a bi-criteria approximation goal that\nlets us recover the directions of all but $\\epsilon n^2$ edges in $G$, for some\nspecified error parameter $\\epsilon > 0$. Under this relaxed goal, we give\npolynomial time algorithms that achieve intervention cost within a small\nconstant factor of the optimal. Our algorithms combine work on efficient\nintervention design and the design of low-cost separating set systems, with\nideas from the literature on graph property testing.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 17:08:46 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Addanki", "Raghavendra", ""], ["McGregor", "Andrew", ""], ["Musco", "Cameron", ""]]}, {"id": "2012.13982", "submitter": "Tong Zhang", "authors": "Cong Fang and Hanze Dong and Tong Zhang", "title": "Mathematical Models of Overparameterized Neural Networks", "comments": null, "journal-ref": "Proceedings of the IEEE, 2021", "doi": "10.1109/JPROC.2020.3048020", "report-no": null, "categories": "cs.LG cs.AI math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has received considerable empirical successes in recent years.\nHowever, while many ad hoc tricks have been discovered by practitioners, until\nrecently, there has been a lack of theoretical understanding for tricks\ninvented in the deep learning literature. Known by practitioners that\noverparameterized neural networks are easy to learn, in the past few years\nthere have been important theoretical developments in the analysis of\noverparameterized neural networks. In particular, it was shown that such\nsystems behave like convex systems under various restricted settings, such as\nfor two-layer NNs, and when learning is restricted locally in the so-called\nneural tangent kernel space around specialized initializations. This paper\ndiscusses some of these recent progresses leading to significant better\nunderstanding of neural networks. We will focus on the analysis of two-layer\nneural networks, and explain the key mathematical models, with their\nalgorithmic implications. We will then discuss challenges in understanding deep\nneural networks and some current research directions.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 17:48:31 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Fang", "Cong", ""], ["Dong", "Hanze", ""], ["Zhang", "Tong", ""]]}, {"id": "2012.14098", "submitter": "Ethan Fang", "authors": "Han Zhong, Ethan X. Fang, Zhuoran Yang, Zhaoran Wang", "title": "Risk-Sensitive Deep RL: Variance-Constrained Actor-Critic Provably Finds\n  Globally Optimal Policy", "comments": "45 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While deep reinforcement learning has achieved tremendous successes in\nvarious applications, most existing works only focus on maximizing the expected\nvalue of total return and thus ignore its inherent stochasticity. Such\nstochasticity is also known as the aleatoric uncertainty and is closely related\nto the notion of risk. In this work, we make the first attempt to study\nrisk-sensitive deep reinforcement learning under the average reward setting\nwith the variance risk criteria. In particular, we focus on a\nvariance-constrained policy optimization problem where the goal is to find a\npolicy that maximizes the expected value of the long-run average reward,\nsubject to a constraint that the long-run variance of the average reward is\nupper bounded by a threshold. Utilizing Lagrangian and Fenchel dualities, we\ntransform the original problem into an unconstrained saddle-point policy\noptimization problem, and propose an actor-critic algorithm that iteratively\nand efficiently updates the policy, the Lagrange multiplier, and the Fenchel\ndual variable. When both the value and policy functions are represented by\nmulti-layer overparameterized neural networks, we prove that our actor-critic\nalgorithm generates a sequence of policies that finds a globally optimal policy\nat a sublinear rate.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 05:02:26 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Zhong", "Han", ""], ["Fang", "Ethan X.", ""], ["Yang", "Zhuoran", ""], ["Wang", "Zhaoran", ""]]}, {"id": "2012.14100", "submitter": "Huangjie Zheng", "authors": "Huangjie Zheng and Mingyuan Zhou", "title": "Exploiting Chain Rule and Bayes' Theorem to Compare Probability\n  Distributions", "comments": "25 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To measure the difference between two probability distributions, referred to\nas the source and target, respectively, we exploit both the chain rule and\nBayes' theorem to construct conditional transport (CT), which is constituted by\nboth a forward component and a backward one. The forward CT is the expected\ncost of moving a source data point to a target one, with their joint\ndistribution defined by the product of the source probability density function\n(PDF) and a source-dependent conditional distribution, which is related to the\ntarget PDF via Bayes' theorem. The backward CT is defined by reversing the\ndirection. The CT cost can be approximated by replacing the source and target\nPDFs with their discrete empirical distributions supported on mini-batches,\nmaking it amenable to implicit distributions and stochastic gradient\ndescent-based optimization. When applied to train a generative model, CT is\nshown to strike a good balance between mode-covering and mode-seeking behaviors\nand strongly resist mode collapse. On a wide variety of benchmark datasets for\ngenerative modeling, substituting the default statistical distance of an\nexisting generative adversarial network with CT is shown to consistently\nimprove the performance. PyTorch-style code is provided.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 05:14:22 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 05:34:22 GMT"}, {"version": "v3", "created": "Fri, 9 Apr 2021 02:00:37 GMT"}, {"version": "v4", "created": "Tue, 29 Jun 2021 16:28:43 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Zheng", "Huangjie", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "2012.14117", "submitter": "Mundher Al-Shabi", "authors": "Mundher Al-Shabi, Kelvin Shak, Maxine Tan", "title": "3D Axial-Attention for Lung Nodule Classification", "comments": null, "journal-ref": "International Journal of Computer Assisted Radiology and Surgery,\n  1-6 (2021)", "doi": "10.1007/s11548-021-02415-z", "report-no": null, "categories": "eess.IV cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Purpose: In recent years, Non-Local based methods have been successfully\napplied to lung nodule classification. However, these methods offer 2D\nattention or limited 3D attention to low-resolution feature maps. Moreover,\nthey still depend on a convenient local filter such as convolution as full 3D\nattention is expensive to compute and requires a big dataset, which might not\nbe available.\n  Methods: We propose to use 3D Axial-Attention, which requires a fraction of\nthe computing power of a regular Non-Local network (i.e., self-attention).\nUnlike a regular Non-Local network, the 3D Axial-Attention network applies the\nattention operation to each axis separately. Additionally, we solve the\ninvariant position problem of the Non-Local network by proposing to add 3D\npositional encoding to shared embeddings.\n  Results: We validated the proposed method on 442 benign nodules and 406\nmalignant nodules, extracted from the public LIDC-IDRI dataset by following a\nrigorous experimental setup using only nodules annotated by at least three\nradiologists. Our results show that the 3D Axial-Attention model achieves\nstate-of-the-art performance on all evaluation metrics, including AUC and\nAccuracy.\n  Conclusions: The proposed model provides full 3D attention, whereby every\nelement (i.e., pixel) in the 3D volume space attends to every other element in\nthe nodule effectively. Thus, the 3D Axial-Attention network can be used in all\nlayers without the need for local filters. The experimental results show the\nimportance of full 3D attention for classifying lung nodules.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 06:49:09 GMT"}, {"version": "v2", "created": "Sat, 2 Jan 2021 06:52:30 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 10:43:26 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Al-Shabi", "Mundher", ""], ["Shak", "Kelvin", ""], ["Tan", "Maxine", ""]]}, {"id": "2012.14172", "submitter": "Joe Kileel", "authors": "Joe Kileel, Amit Moscovich, Nathan Zelesko, Amit Singer", "title": "Manifold learning with arbitrary norms", "comments": "53 pages, 8 figures, 3 tables, to appear in Journal of Fourier\n  Analysis and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manifold learning methods play a prominent role in nonlinear dimensionality\nreduction and other tasks involving high-dimensional data sets with low\nintrinsic dimensionality. Many of these methods are graph-based: they associate\na vertex with each data point and a weighted edge with each pair. Existing\ntheory shows that the Laplacian matrix of the graph converges to the\nLaplace-Beltrami operator of the data manifold, under the assumption that the\npairwise affinities are based on the Euclidean norm. In this paper, we\ndetermine the limiting differential operator for graph Laplacians constructed\nusing $\\textit{any}$ norm. Our proof involves an interplay between the second\nfundamental form of the manifold and the convex geometry of the given norm's\nunit ball. To demonstrate the potential benefits of non-Euclidean norms in\nmanifold learning, we consider the task of mapping the motion of large\nmolecules with continuous variability. In a numerical simulation we show that a\nmodified Laplacian eigenmaps algorithm, based on the Earthmover's distance,\noutperforms the classic Euclidean Laplacian eigenmaps, both in terms of\ncomputational cost and the sample size needed to recover the intrinsic\ngeometry.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 10:24:30 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 23:24:38 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Kileel", "Joe", ""], ["Moscovich", "Amit", ""], ["Zelesko", "Nathan", ""], ["Singer", "Amit", ""]]}, {"id": "2012.14193", "submitter": "Stanislaw Jastrzebski", "authors": "Stanislaw Jastrzebski, Devansh Arpit, Oliver Astrand, Giancarlo Kerg,\n  Huan Wang, Caiming Xiong, Richard Socher, Kyunghyun Cho, Krzysztof Geras", "title": "Catastrophic Fisher Explosion: Early Phase Fisher Matrix Impacts\n  Generalization", "comments": "The last two authors contributed equally. Accepted to the\n  International Conference on Machine Learning 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The early phase of training a deep neural network has a dramatic effect on\nthe local curvature of the loss function. For instance, using a small learning\nrate does not guarantee stable optimization because the optimization trajectory\nhas a tendency to steer towards regions of the loss surface with increasing\nlocal curvature. We ask whether this tendency is connected to the widely\nobserved phenomenon that the choice of the learning rate strongly influences\ngeneralization. We first show that stochastic gradient descent (SGD) implicitly\npenalizes the trace of the Fisher Information Matrix (FIM), a measure of the\nlocal curvature, from the start of training. We argue it is an implicit\nregularizer in SGD by showing that explicitly penalizing the trace of the FIM\ncan significantly improve generalization. We highlight that poor final\ngeneralization coincides with the trace of the FIM attaining a large value\nearly in training, to which we refer as catastrophic Fisher explosion. Finally,\nto gain insight into the regularization effect of penalizing the trace of the\nFIM, we show that it limits memorization by reducing the learning speed of\nexamples with noisy labels more than that of the examples with clean labels.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 11:17:46 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 19:47:02 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 07:06:03 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Jastrzebski", "Stanislaw", ""], ["Arpit", "Devansh", ""], ["Astrand", "Oliver", ""], ["Kerg", "Giancarlo", ""], ["Wang", "Huan", ""], ["Xiong", "Caiming", ""], ["Socher", "Richard", ""], ["Cho", "Kyunghyun", ""], ["Geras", "Krzysztof", ""]]}, {"id": "2012.14246", "submitter": "Vladimir Vovk", "authors": "Vladimir Vovk", "title": "Testing for concept shift online", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note continues study of exchangeability martingales, i.e., processes\nthat are martingales under any exchangeable distribution for the observations.\nSuch processes can be used for detecting violations of the IID assumption,\nwhich is commonly made in machine learning. Violations of the IID assumption\nare sometimes referred to as dataset shift, and dataset shift is sometimes\nsubdivided into concept shift, covariate shift, etc. Our primary interest is in\nconcept shift, but we will also discuss exchangeability martingales that\ndecompose perfectly into two components one of which detects concept shift and\nthe other detects what we call label shift. Our methods will be based on\ntechniques of conformal prediction.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 14:33:03 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Vovk", "Vladimir", ""]]}, {"id": "2012.14264", "submitter": "Matthieu Jedor", "authors": "Matthieu Jedor, Jonathan Lou\\\"edec, Vianney Perchet", "title": "Lifelong Learning in Multi-Armed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Continuously learning and leveraging the knowledge accumulated from prior\ntasks in order to improve future performance is a long standing machine\nlearning problem. In this paper, we study the problem in the multi-armed bandit\nframework with the objective to minimize the total regret incurred over a\nseries of tasks. While most bandit algorithms are designed to have a low\nworst-case regret, we examine here the average regret over bandit instances\ndrawn from some prior distribution which may change over time. We specifically\nfocus on confidence interval tuning of UCB algorithms. We propose a bandit over\nbandit approach with greedy algorithms and we perform extensive experimental\nevaluations in both stationary and non-stationary environments. We further\napply our solution to the mortal bandit problem, showing empirical improvement\nover previous work.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 15:13:31 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Jedor", "Matthieu", ""], ["Lou\u00ebdec", "Jonathan", ""], ["Perchet", "Vianney", ""]]}, {"id": "2012.14331", "submitter": "Abhranil Das", "authors": "Abhranil Das and Wilson S Geisler", "title": "A method to integrate and classify normal distributions", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Univariate and multivariate normal probability distributions are widely used\nwhen modeling decisions under uncertainty. Computing the performance of such\nmodels requires integrating these distributions over specific domains, which\ncan vary widely across models. Besides some special cases where these integrals\nare easy to calculate, there exist no general analytical expressions, standard\nnumerical methods or software for these integrals. Here we present mathematical\nresults and open-source software that provide (i) the probability in any domain\nof a normal in any dimensions with any parameters, (ii) the probability\ndensity, cumulative distribution, and inverse cumulative distribution of any\nfunction of a normal vector, (iii) the classification errors among any number\nof normal distributions, the Bayes-optimal discriminability index and relation\nto the operating characteristic, (iv) dimension reduction and visualizations\nfor such problems, and (v) tests for how reliably these methods may be used on\ngiven data. We demonstrate these tools with vision research applications of\ndetecting occluding objects in natural scenes, and detecting camouflage.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 05:45:41 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 20:23:39 GMT"}, {"version": "v3", "created": "Mon, 5 Apr 2021 23:00:37 GMT"}, {"version": "v4", "created": "Wed, 7 Apr 2021 18:49:56 GMT"}, {"version": "v5", "created": "Thu, 22 Apr 2021 20:40:22 GMT"}, {"version": "v6", "created": "Mon, 26 Apr 2021 23:11:55 GMT"}, {"version": "v7", "created": "Tue, 27 Jul 2021 23:02:36 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Das", "Abhranil", ""], ["Geisler", "Wilson S", ""]]}, {"id": "2012.14406", "submitter": "Przemyslaw Biecek", "authors": "Hubert Baniecki, Wojciech Kretowicz, Piotr Piatyszek, Jakub\n  Wisniewski, Przemyslaw Biecek", "title": "dalex: Responsible Machine Learning with Interactive Explainability and\n  Fairness in Python", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing amount of available data, computing power, and the constant\npursuit for higher performance results in the growing complexity of predictive\nmodels. Their black-box nature leads to opaqueness debt phenomenon inflicting\nincreased risks of discrimination, lack of reproducibility, and deflated\nperformance due to data drift. To manage these risks, good MLOps practices ask\nfor better validation of model performance and fairness, higher explainability,\nand continuous monitoring. The necessity of deeper model transparency appears\nnot only from scientific and social domains, but also emerging laws and\nregulations on artificial intelligence. To facilitate the development of\nresponsible machine learning models, we showcase dalex, a Python package which\nimplements the model-agnostic interface for interactive model exploration. It\nadopts the design crafted through the development of various tools for\nresponsible machine learning; thus, it aims at the unification of the existing\nsolutions. This library's source code and documentation are available under\nopen license at https://python.drwhy.ai/.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 18:39:59 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Baniecki", "Hubert", ""], ["Kretowicz", "Wojciech", ""], ["Piatyszek", "Piotr", ""], ["Wisniewski", "Jakub", ""], ["Biecek", "Przemyslaw", ""]]}, {"id": "2012.14409", "submitter": "Peter MacDonald", "authors": "Peter W. MacDonald, Elizaveta Levina, Ji Zhu", "title": "Latent space models for multiplex networks with shared structure", "comments": "41 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent space models are frequently used for modeling single-layer networks\nand include many popular special cases, such as the stochastic block model and\nthe random dot product graph. However, they are not well-developed for more\ncomplex network structures, which are becoming increasingly common in practice.\nHere we propose a new latent space model for multiplex networks: multiple,\nheterogeneous networks observed on a shared node set. Multiplex networks can\nrepresent a network sample with shared node labels, a network evolving over\ntime, or a network with multiple types of edges. The key feature of our model\nis that it learns from data how much of the network structure is shared between\nlayers and pools information across layers as appropriate. We establish\nidentifiability, develop a fitting procedure using convex optimization in\ncombination with a nuclear norm penalty, and prove a guarantee of recovery for\nthe latent positions as long as there is sufficient separation between the\nshared and the individual latent subspaces. We compare the model to competing\nmethods in the literature on simulated networks and on a multiplex network\ndescribing the worldwide trade of agricultural products.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 18:42:19 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 20:02:30 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["MacDonald", "Peter W.", ""], ["Levina", "Elizaveta", ""], ["Zhu", "Ji", ""]]}, {"id": "2012.14415", "submitter": "Junchi Li", "authors": "Chris Junchi Li, Michael I. Jordan", "title": "Stochastic Approximation for Online Tensorial Independent Component\n  Analysis", "comments": "To appear in Conference on Learning Theory (COLT), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent component analysis (ICA) has been a popular dimension reduction\ntool in statistical machine learning and signal processing. In this paper, we\npresent a convergence analysis for an online tensorial ICA algorithm, by\nviewing the problem as a nonconvex stochastic approximation problem. For\nestimating one component, we provide a dynamics-based analysis to prove that\nour online tensorial ICA algorithm with a specific choice of stepsize achieves\na sharp finite-sample error bound. In particular, under a mild assumption on\nthe data-generating distribution and a scaling condition such that $d^4/T$ is\nsufficiently small up to a polylogarithmic factor of data dimension $d$ and\nsample size $T$, a sharp finite-sample error bound of $\\tilde{O}(\\sqrt{d/T})$\ncan be obtained.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 18:52:37 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 17:50:51 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Li", "Chris Junchi", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2012.14453", "submitter": "Amirhossein Reisizadeh", "authors": "Amirhossein Reisizadeh, Isidoros Tziotis, Hamed Hassani, Aryan\n  Mokhtari, Ramtin Pedarsani", "title": "Straggler-Resilient Federated Learning: Leveraging the Interplay Between\n  Statistical Accuracy and System Heterogeneity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning is a novel paradigm that involves learning from data\nsamples distributed across a large network of clients while the data remains\nlocal. It is, however, known that federated learning is prone to multiple\nsystem challenges including system heterogeneity where clients have different\ncomputation and communication capabilities. Such heterogeneity in clients'\ncomputation speeds has a negative effect on the scalability of federated\nlearning algorithms and causes significant slow-down in their runtime due to\nthe existence of stragglers. In this paper, we propose a novel\nstraggler-resilient federated learning method that incorporates statistical\ncharacteristics of the clients' data to adaptively select the clients in order\nto speed up the learning procedure. The key idea of our algorithm is to start\nthe training procedure with faster nodes and gradually involve the slower nodes\nin the model training once the statistical accuracy of the data corresponding\nto the current participating nodes is reached. The proposed approach reduces\nthe overall runtime required to achieve the statistical accuracy of data of all\nnodes, as the solution for each stage is close to the solution of the\nsubsequent stage with more samples and can be used as a warm-start. Our\ntheoretical results characterize the speedup gain in comparison to standard\nfederated benchmarks for strongly convex objectives, and our numerical\nexperiments also demonstrate significant speedups in wall-clock time of our\nstraggler-resilient method compared to federated learning benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 19:21:14 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Reisizadeh", "Amirhossein", ""], ["Tziotis", "Isidoros", ""], ["Hassani", "Hamed", ""], ["Mokhtari", "Aryan", ""], ["Pedarsani", "Ramtin", ""]]}, {"id": "2012.14482", "submitter": "Nhat Ho", "authors": "Nhat Ho and Stephen G. Walker", "title": "Multivariate Smoothing via the Fourier Integral Theorem and Fourier\n  Kernel", "comments": "58 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Starting with the Fourier integral theorem, we present natural Monte Carlo\nestimators of multivariate functions including densities, mixing densities,\ntransition densities, regression functions, and the search for modes of\nmultivariate density functions (modal regression). Rates of convergence are\nestablished and, in many cases, provide superior rates to current standard\nestimators such as those based on kernels, including kernel density estimators\nand kernel regression functions. Numerical illustrations are presented.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 20:59:42 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Ho", "Nhat", ""], ["Walker", "Stephen G.", ""]]}, {"id": "2012.14540", "submitter": "Leonard Schulman", "authors": "Spencer L. Gordon, Bijan Mazaheri, Yuval Rabani, Leonard J. Schulman", "title": "Source Identification for Mixtures of Product Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS eess.SP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give an algorithm for source identification of a mixture of $k$ product\ndistributions on $n$ bits. This is a fundamental problem in machine learning\nwith many applications. Our algorithm identifies the source parameters of an\nidentifiable mixture, given, as input, approximate values of multilinear\nmoments (derived, for instance, from a sufficiently large sample), using\n$2^{O(k^2)} n^{O(k)}$ arithmetic operations. Our result is the first explicit\nbound on the computational complexity of source identification of such\nmixtures. The running time improves previous results by Feldman, O'Donnell, and\nServedio (FOCS 2005) and Chen and Moitra (STOC 2019) that guaranteed only\nlearning the mixture (without parametric identification of the source). Our\nanalysis gives a quantitative version of a qualitative characterization of\nidentifiable sources that is due to Tahmasebi, Motahari, and Maddah-Ali (ISIT\n2018).\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 00:21:11 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Gordon", "Spencer L.", ""], ["Mazaheri", "Bijan", ""], ["Rabani", "Yuval", ""], ["Schulman", "Leonard J.", ""]]}, {"id": "2012.14563", "submitter": "Munir Hiabu", "authors": "Munir Hiabu, Enno Mammen, Joseph T. Meyer", "title": "Random Planted Forest: a directly interpretable tree ensemble", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel interpretable and tree-based algorithm for prediction in\na regression setting in which each tree in a classical random forest is\nreplaced by a family of planted trees that grow simultaneously. The motivation\nfor our algorithm is to estimate the unknown regression function from a\nfunctional ANOVA decomposition perspective, where each tree corresponds to a\nfunction within that decomposition. Therefore, planted trees are limited in the\nnumber of interaction terms. The maximal order of approximation in the ANOVA\ndecomposition can be specified or left unlimited. If a first order\napproximation is chosen, the result is an additive model. In the other extreme\ncase, if the order of approximation is not limited, the resulting model puts no\nrestrictions on the form of the regression function. In a simulation study we\nfind encouraging prediction and visualisation properties of our random planted\nforest method. We also develop theory for an idealised version of random\nplanted forests in the case of an underlying additive model. We show that in\nthe additive case, the idealised version achieves up to a logarithmic factor\nasymptotically optimal one-dimensional convergence rates of order $n^{-2/5}$.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 01:51:59 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Hiabu", "Munir", ""], ["Mammen", "Enno", ""], ["Meyer", "Joseph T.", ""]]}, {"id": "2012.14595", "submitter": "Zhengxin Li", "authors": "Zhengxin Li, Feiping Nie, Jintang Bian, Xuelong Li", "title": "Sparse PCA via $l_{2,p}$-Norm Regularization for Unsupervised Feature\n  Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of data mining, how to deal with high-dimensional data is an\ninevitable problem. Unsupervised feature selection has attracted more and more\nattention because it does not rely on labels. The performance of spectral-based\nunsupervised methods depends on the quality of constructed similarity matrix,\nwhich is used to depict the intrinsic structure of data. However, real-world\ndata contain a large number of noise samples and features, making the\nsimilarity matrix constructed by original data cannot be completely reliable.\nWorse still, the size of similarity matrix expands rapidly as the number of\nsamples increases, making the computational cost increase significantly.\nInspired by principal component analysis, we propose a simple and efficient\nunsupervised feature selection method, by combining reconstruction error with\n$l_{2,p}$-norm regularization. The projection matrix, which is used for feature\nselection, is learned by minimizing the reconstruction error under the sparse\nconstraint. Then, we present an efficient optimization algorithm to solve the\nproposed unsupervised model, and analyse the convergence and computational\ncomplexity of the algorithm theoretically. Finally, extensive experiments on\nreal-world data sets demonstrate the effectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 04:08:38 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Li", "Zhengxin", ""], ["Nie", "Feiping", ""], ["Bian", "Jintang", ""], ["Li", "Xuelong", ""]]}, {"id": "2012.14657", "submitter": "Clement Dombry", "authors": "Cl\\'ement Dombry (UBFC, LMB), Youssef Esstafa (ENSAI)", "title": "Behavior of linear L2-boosting algorithms in the vanishing learning rate\n  asymptotic", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the asymptotic behaviour of gradient boosting algorithms when\nthe learning rate converges to zero and the number of iterations is rescaled\naccordingly. We mostly consider L2-boosting for regression with linear base\nlearner as studied in B{\\\"u}hlmann and Yu (2003) and analyze also a stochastic\nversion of the model where subsampling is used at each step (Friedman 2002). We\nprove a deterministic limit in the vanishing learning rate asymptotic and\ncharacterize the limit as the unique solution of a linear differential equation\nin an infinite dimensional function space. Besides, the training and test error\nof the limiting procedure are thoroughly analyzed. We finally illustrate and\ndiscuss our result on a simple numerical experiment where the linear\nL2-boosting operator is interpreted as a smoothed projection and time is\nrelated to its number of degrees of freedom.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 08:37:54 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Dombry", "Cl\u00e9ment", "", "UBFC, LMB"], ["Esstafa", "Youssef", "", "ENSAI"]]}, {"id": "2012.14670", "submitter": "Gersende Fort", "authors": "Gersende Fort (IMT), P. Gach (IMT), E. Moulines (CMAP, XPOP)", "title": "Fast Incremental Expectation Maximization for finite-sum optimization:\n  nonasymptotic convergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast Incremental Expectation Maximization (FIEM) is a version of the EM\nframework for large datasets. In this paper, we first recast FIEM and other\nincremental EM type algorithms in the {\\em Stochastic Approximation within EM}\nframework. Then, we provide nonasymptotic bounds for the convergence in\nexpectation as a function of the number of examples $n$ and of the maximal\nnumber of iterations $\\kmax$. We propose two strategies for achieving an\n$\\epsilon$-approximate stationary point, respectively with $\\kmax =\nO(n^{2/3}/\\epsilon)$ and $\\kmax = O(\\sqrt{n}/\\epsilon^{3/2})$, both strategies\nrelying on a random termination rule before $\\kmax$ and on a constant step size\nin the Stochastic Approximation step. Our bounds provide some improvements on\nthe literature. First, they allow $\\kmax$ to scale as $\\sqrt{n}$ which is\nbetter than $n^{2/3}$ which was the best rate obtained so far; it is at the\ncost of a larger dependence upon the tolerance $\\epsilon$, thus making this\ncontrol relevant for small to medium accuracy with respect to the number of\nexamples $n$. Second, for the $n^{2/3}$-rate, the numerical illustrations show\nthat thanks to an optimized choice of the step size and of the bounds in terms\nof quantities characterizing the optimization problem at hand, our results\ndesig a less conservative choice of the step size and provide a better control\nof the convergence in expectation.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 09:11:42 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Fort", "Gersende", "", "IMT"], ["Gach", "P.", "", "IMT"], ["Moulines", "E.", "", "CMAP, XPOP"]]}, {"id": "2012.14755", "submitter": "Jean Tarbouriech", "authors": "Jean Tarbouriech, Matteo Pirotta, Michal Valko, Alessandro Lazaric", "title": "Improved Sample Complexity for Incremental Autonomous Exploration in\n  MDPs", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the exploration of an unknown environment when no reward\nfunction is provided. Building on the incremental exploration setting\nintroduced by Lim and Auer [1], we define the objective of learning the set of\n$\\epsilon$-optimal goal-conditioned policies attaining all states that are\nincrementally reachable within $L$ steps (in expectation) from a reference\nstate $s_0$. In this paper, we introduce a novel model-based approach that\ninterleaves discovering new states from $s_0$ and improving the accuracy of a\nmodel estimate that is used to compute goal-conditioned policies to reach newly\ndiscovered states. The resulting algorithm, DisCo, achieves a sample complexity\nscaling as $\\tilde{O}(L^5 S_{L+\\epsilon} \\Gamma_{L+\\epsilon} A \\epsilon^{-2})$,\nwhere $A$ is the number of actions, $S_{L+\\epsilon}$ is the number of states\nthat are incrementally reachable from $s_0$ in $L+\\epsilon$ steps, and\n$\\Gamma_{L+\\epsilon}$ is the branching factor of the dynamics over such states.\nThis improves over the algorithm proposed in [1] in both $\\epsilon$ and $L$ at\nthe cost of an extra $\\Gamma_{L+\\epsilon}$ factor, which is small in most\nenvironments of interest. Furthermore, DisCo is the first algorithm that can\nreturn an $\\epsilon/c_{\\min}$-optimal policy for any cost-sensitive\nshortest-path problem defined on the $L$-reachable states with minimum cost\n$c_{\\min}$. Finally, we report preliminary empirical results confirming our\ntheoretical findings.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 14:06:09 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Tarbouriech", "Jean", ""], ["Pirotta", "Matteo", ""], ["Valko", "Michal", ""], ["Lazaric", "Alessandro", ""]]}, {"id": "2012.14844", "submitter": "Anru R. Zhang", "authors": "Dong Xia and Anru R. Zhang and Yuchen Zhou", "title": "Inference for Low-rank Tensors -- No Need to Debias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we consider the statistical inference for several low-rank\ntensor models. Specifically, in the Tucker low-rank tensor PCA or regression\nmodel, provided with any estimates achieving some attainable error rate, we\ndevelop the data-driven confidence regions for the singular subspace of the\nparameter tensor based on the asymptotic distribution of an updated estimate by\ntwo-iteration alternating minimization. The asymptotic distributions are\nestablished under some essential conditions on the signal-to-noise ratio (in\nPCA model) or sample size (in regression model). If the parameter tensor is\nfurther orthogonally decomposable, we develop the methods and theory for\ninference on each individual singular vector. For the rank-one tensor PCA\nmodel, we establish the asymptotic distribution for general linear forms of\nprincipal components and confidence interval for each entry of the parameter\ntensor. Finally, numerical simulations are presented to corroborate our\ntheoretical discoveries.\n  In all these models, we observe that different from many matrix/vector\nsettings in existing work, debiasing is not required to establish the\nasymptotic distribution of estimates or to make statistical inference on\nlow-rank tensors. In fact, due to the widely observed\nstatistical-computational-gap for low-rank tensor estimation, one usually\nrequires stronger conditions than the statistical (or information-theoretic)\nlimit to ensure the computationally feasible estimation is achievable.\nSurprisingly, such conditions ``incidentally\" render a feasible low-rank tensor\ninference without debiasing.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 16:48:02 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Xia", "Dong", ""], ["Zhang", "Anru R.", ""], ["Zhou", "Yuchen", ""]]}, {"id": "2012.14868", "submitter": "Aolin Xu", "authors": "Aolin Xu, Maxim Raginsky", "title": "Minimum Excess Risk in Bayesian Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We analyze the best achievable performance of Bayesian learning under\ngenerative models by defining and upper-bounding the minimum excess risk (MER):\nthe gap between the minimum expected loss attainable by learning from data and\nthe minimum expected loss that could be achieved if the model realization were\nknown. The definition of MER provides a principled way to define different\nnotions of uncertainties in Bayesian learning, including the aleatoric\nuncertainty and the minimum epistemic uncertainty. Two methods for deriving\nupper bounds for the MER are presented. The first method, generally suitable\nfor Bayesian learning with a parametric generative model, upper-bounds the MER\nby the conditional mutual information between the model parameters and the\nquantity being predicted given the observed data. It allows us to quantify the\nrate at which the MER decays to zero as more data becomes available. The second\nmethod, particularly suitable for Bayesian learning with a parametric\npredictive model, relates the MER to the deviation of the posterior predictive\ndistribution from the true predictive model, and further to the minimum\nestimation error of the model parameters from data. It explicitly shows how the\nuncertainty in model parameter estimation translates to the MER and to the\nfinal prediction uncertainty. We also extend the definition and analysis of MER\nto the setting with multiple parametric model families and the setting with\nnonparametric models. Along the discussions we draw some comparisons between\nthe MER in Bayesian learning and the excess risk in frequentist learning.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 17:41:30 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Xu", "Aolin", ""], ["Raginsky", "Maxim", ""]]}, {"id": "2012.14873", "submitter": "Sebastian Johann Wetzel", "authors": "Sebastian J. Wetzel, Kevin Ryczko, Roger G. Melko, Isaac Tamblyn", "title": "Twin Neural Network Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce twin neural network (TNN) regression. This method predicts\ndifferences between the target values of two different data points rather than\nthe targets themselves. The solution of a traditional regression problem is\nthen obtained by averaging over an ensemble of all predicted differences\nbetween the targets of an unseen data point and all training data points.\nWhereas ensembles are normally costly to produce, TNN regression intrinsically\ncreates an ensemble of predictions of twice the size of the training set while\nonly training a single neural network. Since ensembles have been shown to be\nmore accurate than single models this property naturally transfers to TNN\nregression. We show that TNNs are able to compete or yield more accurate\npredictions for different data sets, compared to other state-of-the-art\nmethods. Furthermore, TNN regression is constrained by self-consistency\nconditions. We find that the violation of these conditions provides an estimate\nfor the prediction uncertainty.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 17:52:31 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Wetzel", "Sebastian J.", ""], ["Ryczko", "Kevin", ""], ["Melko", "Roger G.", ""], ["Tamblyn", "Isaac", ""]]}, {"id": "2012.14878", "submitter": "Jianghao Shen", "authors": "Jianghao Shen, Sicheng Wang, Zhangyang Wang", "title": "Growing Deep Forests Efficiently with Soft Routing and Learned\n  Connectivity", "comments": "ICDM workshop 2018", "journal-ref": "ICDM Workshops 2018: 399-402", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the latest prevailing success of deep neural networks (DNNs), several\nconcerns have been raised against their usage, including the lack of\nintepretability the gap between DNNs and other well-established machine\nlearning models, and the growingly expensive computational costs. A number of\nrecent works [1], [2], [3] explored the alternative to sequentially stacking\ndecision tree/random forest building blocks in a purely feed-forward way, with\nno need of back propagation. Since decision trees enjoy inherent reasoning\ntransparency, such deep forest models can also facilitate the understanding of\nthe internaldecision making process. This paper further extends the deep forest\nidea in several important aspects. Firstly, we employ a probabilistic tree\nwhose nodes make probabilistic routing decisions, a.k.a., soft routing, rather\nthan hard binary decisions.Besides enhancing the flexibility, it also enables\nnon-greedy optimization for each tree. Second, we propose an innovative\ntopology learning strategy: every node in the ree now maintains a new learnable\nhyperparameter indicating the probability that it will be a leaf node. In that\nway, the tree will jointly optimize both its parameters and the tree topology\nduring training. Experiments on the MNIST dataset demonstrate that our\nempowered deep forests can achieve better or comparable performance than\n[1],[3] , with dramatically reduced model complexity. For example,our model\nwith only 1 layer of 15 trees can perform comparably with the model in [3] with\n2 layers of 2000 trees each.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 18:05:05 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Shen", "Jianghao", ""], ["Wang", "Sicheng", ""], ["Wang", "Zhangyang", ""]]}, {"id": "2012.14894", "submitter": "Wenxin Jiang", "authors": "Wenxin Jiang", "title": "Statistical Formulas for F Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide analytic formulas for the standard error and confidence intervals\nfor the F measures, based on a property of asymptotic normality in the large\nsample limit. The formula can be applied for sample size planning in order to\nachieve accurate enough estimation of these F measures.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 18:34:04 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Jiang", "Wenxin", ""]]}, {"id": "2012.14905", "submitter": "Louis Kirsch", "authors": "Louis Kirsch and J\\\"urgen Schmidhuber", "title": "Meta Learning Backpropagation And Improving It", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many concepts have been proposed for meta learning with neural networks\n(NNs), e.g., NNs that learn to control fast weights, hyper networks, learned\nlearning rules, and meta recurrent NNs. Our Variable Shared Meta Learning\n(VS-ML) unifies the above and demonstrates that simple weight-sharing and\nsparsity in an NN is sufficient to express powerful learning algorithms (LAs)\nin a reusable fashion. A simple implementation of VS-ML called VS-ML RNN allows\nfor implementing the backpropagation LA solely by running an RNN in\nforward-mode. It can even meta-learn new LAs that improve upon backpropagation\nand generalize to datasets outside of the meta training distribution without\nexplicit gradient calculation. Introspection reveals that our meta-learned LAs\nlearn qualitatively different from gradient descent through fast association.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 18:56:10 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 17:28:31 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Kirsch", "Louis", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "2012.14932", "submitter": "Mihai Cucuringu", "authors": "Mihai Cucuringu and Hemant Tyagi", "title": "An extension of the angular synchronization problem to the heterogeneous\n  setting", "comments": "45 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG cs.NA math.IT math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an undirected measurement graph $G = ([n], E)$, the classical angular\nsynchronization problem consists of recovering unknown angles\n$\\theta_1,\\dots,\\theta_n$ from a collection of noisy pairwise measurements of\nthe form $(\\theta_i - \\theta_j) \\mod 2\\pi$, for each $\\{i,j\\} \\in E$. This\nproblem arises in a variety of applications, including computer vision, time\nsynchronization of distributed networks, and ranking from preference\nrelationships. In this paper, we consider a generalization to the setting where\nthere exist $k$ unknown groups of angles $\\theta_{l,1}, \\dots,\\theta_{l,n}$,\nfor $l=1,\\dots,k$. For each $ \\{i,j\\} \\in E$, we are given noisy pairwise\nmeasurements of the form $\\theta_{\\ell,i} - \\theta_{\\ell,j}$ for an unknown\n$\\ell \\in \\{1,2,\\ldots,k\\}$. This can be thought of as a natural extension of\nthe angular synchronization problem to the heterogeneous setting of multiple\ngroups of angles, where the measurement graph has an unknown edge-disjoint\ndecomposition $G = G_1 \\cup G_2 \\ldots \\cup G_k$, where the $G_i$'s denote the\nsubgraphs of edges corresponding to each group. We propose a probabilistic\ngenerative model for this problem, along with a spectral algorithm for which we\nprovide a detailed theoretical analysis in terms of robustness against both\nsampling sparsity and noise. The theoretical findings are complemented by a\ncomprehensive set of numerical experiments, showcasing the efficacy of our\nalgorithm under various parameter regimes. Finally, we consider an application\nof bi-synchronization to the graph realization problem, and provide along the\nway an iterative graph disentangling procedure that uncovers the subgraphs\n$G_i$, $i=1,\\ldots,k$ which is of independent interest, as it is shown to\nimprove the final recovery accuracy across all the experiments considered.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 20:29:10 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 21:26:11 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Cucuringu", "Mihai", ""], ["Tyagi", "Hemant", ""]]}, {"id": "2012.14936", "submitter": "Jianwen Xie", "authors": "Jianwen Xie, Zilong Zheng, Ping Li", "title": "Learning Energy-Based Model with Variational Auto-Encoder as Amortized\n  Sampler", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the intractable partition function, training energy-based models\n(EBMs) by maximum likelihood requires Markov chain Monte Carlo (MCMC) sampling\nto approximate the gradient of the Kullback-Leibler divergence between data and\nmodel distributions. However, it is non-trivial to sample from an EBM because\nof the difficulty of mixing between modes. In this paper, we propose to learn a\nvariational auto-encoder (VAE) to initialize the finite-step MCMC, such as\nLangevin dynamics that is derived from the energy function, for efficient\namortized sampling of the EBM. With these amortized MCMC samples, the EBM can\nbe trained by maximum likelihood, which follows an \"analysis by synthesis\"\nscheme; while the variational auto-encoder learns from these MCMC samples via\nvariational Bayes. We call this joint training algorithm the variational MCMC\nteaching, in which the VAE chases the EBM toward data distribution. We\ninterpret the learning algorithm as a dynamic alternating projection in the\ncontext of information geometry. Our proposed models can generate samples\ncomparable to GANs and EBMs. Additionally, we demonstrate that our models can\nlearn effective probabilistic distribution toward supervised conditional\nlearning experiments.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 20:46:40 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Xie", "Jianwen", ""], ["Zheng", "Zilong", ""], ["Li", "Ping", ""]]}, {"id": "2012.14944", "submitter": "Tatiana Engel A", "authors": "Mikhail Genkin, Owen Hughes, and Tatiana A. Engel", "title": "Learning non-stationary Langevin dynamics from stochastic observations\n  of latent trajectories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.stat-mech cs.LG physics.bio-ph physics.data-an", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many complex systems operating far from the equilibrium exhibit stochastic\ndynamics that can be described by a Langevin equation. Inferring Langevin\nequations from data can reveal how transient dynamics of such systems give rise\nto their function. However, dynamics are often inaccessible directly and can be\nonly gleaned through a stochastic observation process, which makes the\ninference challenging. Here we present a non-parametric framework for inferring\nthe Langevin equation, which explicitly models the stochastic observation\nprocess and non-stationary latent dynamics. The framework accounts for the\nnon-equilibrium initial and final states of the observed system and for the\npossibility that the system's dynamics define the duration of observations.\nOmitting any of these non-stationary components results in incorrect inference,\nin which erroneous features arise in the dynamics due to non-stationary data\ndistribution. We illustrate the framework using models of neural dynamics\nunderlying decision making in the brain.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 21:22:21 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Genkin", "Mikhail", ""], ["Hughes", "Owen", ""], ["Engel", "Tatiana A.", ""]]}, {"id": "2012.14951", "submitter": "Jingyi Jessica Li", "authors": "Wei Vivian Li, Xin Tong, Jingyi Jessica Li", "title": "Bridging Cost-sensitive and Neyman-Pearson Paradigms for Asymmetric\n  Binary Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Asymmetric binary classification problems, in which the type I and II errors\nhave unequal severity, are ubiquitous in real-world applications. To handle\nsuch asymmetry, researchers have developed the cost-sensitive and\nNeyman-Pearson paradigms for training classifiers to control the more severe\ntype of classification error, say the type I error. The cost-sensitive paradigm\nis widely used and has straightforward implementations that do not require\nsample splitting; however, it demands an explicit specification of the costs of\nthe type I and II errors, and an open question is what specification can\nguarantee a high-probability control on the population type I error. In\ncontrast, the Neyman-Pearson paradigm can train classifiers to achieve a\nhigh-probability control of the population type I error, but it relies on\nsample splitting that reduces the effective training sample size. Since the two\nparadigms have complementary strengths, it is reasonable to combine their\nstrengths for classifier construction. In this work, we for the first time\nstudy the methodological connections between the two paradigms, and we develop\nthe TUBE-CS algorithm to bridge the two paradigms from the perspective of\ncontrolling the population type I error.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 21:40:52 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Li", "Wei Vivian", ""], ["Tong", "Xin", ""], ["Li", "Jingyi Jessica", ""]]}, {"id": "2012.14961", "submitter": "Hongjing Zhang", "authors": "Hongjing Zhang, Ian Davidson", "title": "Towards Fair Deep Anomaly Detection", "comments": "Accepted for publication at the ACM Conference on Fairness,\n  Accountability, and Transparency 2021 (ACM FAccT'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection aims to find instances that are considered unusual and is a\nfundamental problem of data science. Recently, deep anomaly detection methods\nwere shown to achieve superior results particularly in complex data such as\nimages. Our work focuses on deep one-class classification for anomaly detection\nwhich learns a mapping only from the normal samples. However, the non-linear\ntransformation performed by deep learning can potentially find patterns\nassociated with social bias. The challenge with adding fairness to deep anomaly\ndetection is to ensure both making fair and correct anomaly predictions\nsimultaneously. In this paper, we propose a new architecture for the fair\nanomaly detection approach (Deep Fair SVDD) and train it using an adversarial\nnetwork to de-correlate the relationships between the sensitive attributes and\nthe learned representations. This differs from how fairness is typically added\nnamely as a regularizer or a constraint. Further, we propose two effective\nfairness measures and empirically demonstrate that existing deep anomaly\ndetection methods are unfair. We show that our proposed approach can remove the\nunfairness largely with minimal loss on the anomaly detection performance.\nLastly, we conduct an in-depth analysis to show the strength and limitations of\nour proposed model, including parameter analysis, feature visualization, and\nrun-time analysis.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 22:34:45 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Zhang", "Hongjing", ""], ["Davidson", "Ian", ""]]}, {"id": "2012.14966", "submitter": "Tri Dao", "authors": "Tri Dao, Nimit S. Sohoni, Albert Gu, Matthew Eichhorn, Amit Blonder,\n  Megan Leszczynski, Atri Rudra, Christopher R\\'e", "title": "Kaleidoscope: An Efficient, Learnable Representation For All Structured\n  Linear Maps", "comments": "International Conference on Learning Representations (ICLR) 2020\n  spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern neural network architectures use structured linear transformations,\nsuch as low-rank matrices, sparse matrices, permutations, and the Fourier\ntransform, to improve inference speed and reduce memory usage compared to\ngeneral linear maps. However, choosing which of the myriad structured\ntransformations to use (and its associated parameterization) is a laborious\ntask that requires trading off speed, space, and accuracy. We consider a\ndifferent approach: we introduce a family of matrices called kaleidoscope\nmatrices (K-matrices) that provably capture any structured matrix with\nnear-optimal space (parameter) and time (arithmetic operation) complexity. We\nempirically validate that K-matrices can be automatically learned within\nend-to-end pipelines to replace hand-crafted procedures, in order to improve\nmodel quality. For example, replacing channel shuffles in ShuffleNet improves\nclassification accuracy on ImageNet by up to 5%. K-matrices can also simplify\nhand-engineered pipelines -- we replace filter bank feature computation in\nspeech data preprocessing with a learnable kaleidoscope layer, resulting in\nonly 0.4% loss in accuracy on the TIMIT speech recognition task. In addition,\nK-matrices can capture latent structure in models: for a challenging permuted\nimage classification task, a K-matrix based representation of permutations is\nable to learn the right latent structure and improves accuracy of a downstream\nconvolutional model by over 9%. We provide a practically efficient\nimplementation of our approach, and use K-matrices in a Transformer network to\nattain 36% faster end-to-end inference speed on a language translation task.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 22:51:29 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 07:29:16 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Dao", "Tri", ""], ["Sohoni", "Nimit S.", ""], ["Gu", "Albert", ""], ["Eichhorn", "Matthew", ""], ["Blonder", "Amit", ""], ["Leszczynski", "Megan", ""], ["Rudra", "Atri", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "2012.15000", "submitter": "Micha\\\"el Defferrard", "authors": "Micha\\\"el Defferrard, Martino Milani, Fr\\'ed\\'erick Gusset,\n  Nathana\\\"el Perraudin", "title": "DeepSphere: a graph-based spherical CNN", "comments": "published at ICLR'20, https://openreview.net/forum?id=B1e3OlStPB", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Designing a convolution for a spherical neural network requires a delicate\ntradeoff between efficiency and rotation equivariance. DeepSphere, a method\nbased on a graph representation of the sampled sphere, strikes a controllable\nbalance between these two desiderata. This contribution is twofold. First, we\nstudy both theoretically and empirically how equivariance is affected by the\nunderlying graph with respect to the number of vertices and neighbors. Second,\nwe evaluate DeepSphere on relevant problems. Experiments show state-of-the-art\nperformance and demonstrates the efficiency and flexibility of this\nformulation. Perhaps surprisingly, comparison with previous work suggests that\nanisotropic filters might be an unnecessary price to pay. Our code is available\nat https://github.com/deepsphere\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 01:35:27 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Defferrard", "Micha\u00ebl", ""], ["Milani", "Martino", ""], ["Gusset", "Fr\u00e9d\u00e9rick", ""], ["Perraudin", "Nathana\u00ebl", ""]]}, {"id": "2012.15005", "submitter": "Xiaoming Liu", "authors": "Yadong Zhou, Zhihao Ding, Xiaoming Liu, Chao Shen, Lingling Tong,\n  Xiaohong Guan", "title": "Infer-AVAE: An Attribute Inference Model Based on Adversarial\n  Variational Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  User attributes, such as gender and education, face severe incompleteness in\nsocial networks. In order to make this kind of valuable data usable for\ndownstream tasks like user profiling and personalized recommendation, attribute\ninference aims to infer users' missing attribute labels based on observed data.\nRecently, variational autoencoder (VAE), an end-to-end deep generative model,\nhas shown promising performance by handling the problem in a semi-supervised\nway. However, VAEs can easily suffer from over-fitting and over-smoothing when\napplied to attribute inference. To be specific, VAE implemented with\nmulti-layer perceptron (MLP) can only reconstruct input data but fail in\ninferring missing parts. While using the trending graph neural networks (GNNs)\nas encoder has the problem that GNNs aggregate redundant information from\nneighborhood and generate indistinguishable user representations, which is\nknown as over-smoothing. In this paper, we propose an attribute\n\\textbf{Infer}ence model based on \\textbf{A}dversarial \\textbf{VAE}\n(Infer-AVAE) to cope with these issues. Specifically, to overcome\nover-smoothing, Infer-AVAE unifies MLP and GNNs in encoder to learn positive\nand negative latent representations respectively. Meanwhile, an adversarial\nnetwork is trained to distinguish the two representations and GNNs are trained\nto aggregate less noise for more robust representations through adversarial\ntraining. Finally, to relieve over-fitting, mutual information constraint is\nintroduced as a regularizer for decoder, so that it can make better use of\nauxiliary information in representations and generate outputs not limited by\nobservations. We evaluate our model on 4 real-world social network datasets,\nexperimental results demonstrate that our model averagely outperforms baselines\nby 7.0$\\%$ in accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 02:03:25 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 01:31:39 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhou", "Yadong", ""], ["Ding", "Zhihao", ""], ["Liu", "Xiaoming", ""], ["Shen", "Chao", ""], ["Tong", "Lingling", ""], ["Guan", "Xiaohong", ""]]}, {"id": "2012.15036", "submitter": "Yazhen Wang", "authors": "Victor Luo, Yazhen Wang and Glenn Fung", "title": "SGD Distributional Dynamics of Three Layer Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With the rise of big data analytics, multi-layer neural networks have\nsurfaced as one of the most powerful machine learning methods. However, their\ntheoretical mathematical properties are still not fully understood. Training a\nneural network requires optimizing a non-convex objective function, typically\ndone using stochastic gradient descent (SGD). In this paper, we seek to extend\nthe mean field results of Mei et al. (2018) from two-layer neural networks with\none hidden layer to three-layer neural networks with two hidden layers. We will\nshow that the SGD dynamics is captured by a set of non-linear partial\ndifferential equations, and prove that the distributions of weights in the two\nhidden layers are independent. We will also detail exploratory work done based\non simulation and real-world data.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 04:37:09 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Luo", "Victor", ""], ["Wang", "Yazhen", ""], ["Fung", "Glenn", ""]]}, {"id": "2012.15046", "submitter": "Nam Ho-Nguyen", "authors": "Nam Ho-Nguyen and Fatma K{\\i}l{\\i}n\\c{c}-Karzan", "title": "Risk Guarantees for End-to-End Prediction and Optimization Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction models are often employed in estimating parameters of optimization\nmodels. Despite the fact that in an end-to-end view, the real goal is to\nachieve good optimization performance, the prediction performance is measured\non its own. While it is usually believed that good prediction performance in\nestimating the parameters will result in good subsequent optimization\nperformance, formal theoretical guarantees on this are notably lacking. In this\npaper, we explore conditions that allow us to explicitly describe how the\nprediction performance governs the optimization performance. Our weaker\ncondition allows for an asymptotic convergence result, while our stronger\ncondition allows for exact quantification of the optimization performance in\nterms of the prediction performance. In general, verification of these\nconditions is a non-trivial task. Nevertheless, we show that our weaker\ncondition is equivalent to the well-known Fisher consistency concept from the\nlearning theory literature. This then allows us to easily check our weaker\ncondition for several loss functions. We also establish that the squared error\nloss function satisfies our stronger condition. Consequently, we derive the\nexact theoretical relationship between prediction performance measured with the\nsquared loss, as well as a class of symmetric loss functions, and the\nsubsequent optimization performance. In a computational study on portfolio\noptimization, fractional knapsack and multiclass classification problems, we\ncompare the optimization performance of using of several prediction loss\nfunctions (some that are Fisher consistent and some that are not) and\ndemonstrate that lack of consistency of the loss function can indeed have a\ndetrimental effect on performance.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 05:20:26 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Ho-Nguyen", "Nam", ""], ["K\u0131l\u0131n\u00e7-Karzan", "Fatma", ""]]}, {"id": "2012.15047", "submitter": "Linfan Zhang", "authors": "Linfan Zhang and Arash A. Amini", "title": "Adjusted chi-square test for degree-corrected block models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SI stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a goodness-of-fit test for degree-corrected stochastic block\nmodels (DCSBM). The test is based on an adjusted chi-square statistic for\nmeasuring equality of means among groups of $n$ multinomial distributions with\n$d_1,\\dots,d_n$ observations. In the context of network models, the number of\nmultinomials, $n$, grows much faster than the number of observations, $d_i$,\nhence the setting deviates from classical asymptotics. We show that a simple\nadjustment allows the statistic to converge in distribution, under null, as\nlong as the harmonic mean of $\\{d_i\\}$ grows to infinity. This result applies\nto large sparse networks where the role of $d_i$ is played by the degree of\nnode $i$. Our distributional results are nonasymptotic, with explicit\nconstants, providing finite-sample bounds on the Kolmogorov-Smirnov distance to\nthe target distribution. When applied sequentially, the test can also be used\nto determine the number of communities. The test operates on a (row) compressed\nversion of the adjacency matrix, conditional on the degrees, and as a result is\nhighly scalable to large sparse networks. We incorporate a novel idea of\ncompressing the columns based on a $(K+1)$-community assignment when testing\nfor $K$ communities. This approach increases the power in sequential\napplications without sacrificing computational efficiency, and we prove its\nconsistency in recovering the number of communities. Since the test statistic\ndoes not rely on a specific alternative, its utility goes beyond sequential\ntesting and can be used to simultaneously test against a wide range of\nalternatives outside the DCSBM family. We show the effectiveness of the\napproach by extensive numerical experiments with simulated and real data. In\nparticular, applying the test to the Facebook-100 dataset, we find that a DCSBM\nwith a small number of communities is far from a good fit in almost all cases.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 05:20:59 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Zhang", "Linfan", ""], ["Amini", "Arash A.", ""]]}, {"id": "2012.15059", "submitter": "Rakshitha Godahewa", "authors": "Rakshitha Godahewa, Kasun Bandara, Geoffrey I. Webb, Slawek Smyl,\n  Christoph Bergmeir", "title": "Ensembles of Localised Models for Time Series Forecasting", "comments": "29 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With large quantities of data typically available nowadays, forecasting\nmodels that are trained across sets of time series, known as Global Forecasting\nModels (GFM), are regularly outperforming traditional univariate forecasting\nmodels that work on isolated series. As GFMs usually share the same set of\nparameters across all time series, they often have the problem of not being\nlocalised enough to a particular series, especially in situations where\ndatasets are heterogeneous. We study how ensembling techniques can be used with\ngeneric GFMs and univariate models to solve this issue. Our work systematises\nand compares relevant current approaches, namely clustering series and training\nseparate submodels per cluster, the so-called ensemble of specialists approach,\nand building heterogeneous ensembles of global and local models. We fill some\ngaps in the approaches and generalise them to different underlying GFM model\ntypes. We then propose a new methodology of clustered ensembles where we train\nmultiple GFMs on different clusters of series, obtained by changing the number\nof clusters and cluster seeds. Using Feed-forward Neural Networks, Recurrent\nNeural Networks, and Pooled Regression models as the underlying GFMs, in our\nevaluation on six publicly available datasets, the proposed models are able to\nachieve significantly higher accuracy than baseline GFM models and univariate\nforecasting methods.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 06:33:51 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Godahewa", "Rakshitha", ""], ["Bandara", "Kasun", ""], ["Webb", "Geoffrey I.", ""], ["Smyl", "Slawek", ""], ["Bergmeir", "Christoph", ""]]}, {"id": "2012.15085", "submitter": "Ying Jin", "authors": "Ying Jin, Zhuoran Yang, Zhaoran Wang", "title": "Is Pessimism Provably Efficient for Offline RL?", "comments": "60 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study offline reinforcement learning (RL), which aims to learn an optimal\npolicy based on a dataset collected a priori. Due to the lack of further\ninteractions with the environment, offline RL suffers from the insufficient\ncoverage of the dataset, which eludes most existing theoretical analysis. In\nthis paper, we propose a pessimistic variant of the value iteration algorithm\n(PEVI), which incorporates an uncertainty quantifier as the penalty function.\nSuch a penalty function simply flips the sign of the bonus function for\npromoting exploration in online RL, which makes it easily implementable and\ncompatible with general function approximators.\n  Without assuming the sufficient coverage of the dataset, we establish a\ndata-dependent upper bound on the suboptimality of PEVI for general Markov\ndecision processes (MDPs). When specialized to linear MDPs, it matches the\ninformation-theoretic lower bound up to multiplicative factors of the dimension\nand horizon. In other words, pessimism is not only provably efficient but also\nminimax optimal. In particular, given the dataset, the learned policy serves as\nthe \"best effort\" among all policies, as no other policies can do better. Our\ntheoretical analysis identifies the critical role of pessimism in eliminating a\nnotion of spurious correlation, which emerges from the \"irrelevant\"\ntrajectories that are less covered by the dataset and not informative for the\noptimal policy.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 09:06:57 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 15:05:39 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Jin", "Ying", ""], ["Yang", "Zhuoran", ""], ["Wang", "Zhaoran", ""]]}, {"id": "2012.15103", "submitter": "Giorgio Visani Mr", "authors": "Giorgio Visani, Federico Chesani, Enrico Bagli, Davide Capuzzo and\n  Alessandro Poluzzi", "title": "Explanations of Machine Learning predictions: a mandatory step for its\n  application to Operational Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the global economy, credit companies play a central role in economic\ndevelopment, through their activity as money lenders. This important task comes\nwith some drawbacks, mainly the risk of the debtors not being able to repay the\nprovided credit. Therefore, Credit Risk Modelling (CRM), namely the evaluation\nof the probability that a debtor will not repay the due amount, plays a\nparamount role. Statistical approaches have been successfully exploited since\nlong, becoming the most used methods for CRM. Recently, also machine and deep\nlearning techniques have been applied to the CRM task, showing an important\nincrease in prediction quality and performances. However, such techniques\nusually do not provide reliable explanations for the scores they come up with.\nAs a consequence, many machine and deep learning techniques fail to comply with\nwestern countries' regulations such as, for example, GDPR. In this paper we\nsuggest to use LIME (Local Interpretable Model-agnostic Explanations) technique\nto tackle the explainability problem in this field, we show its employment on a\nreal credit-risk dataset and eventually discuss its soundness and the necessary\nimprovements to guarantee its adoption and compliance with the task.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 10:27:59 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Visani", "Giorgio", ""], ["Chesani", "Federico", ""], ["Bagli", "Enrico", ""], ["Capuzzo", "Davide", ""], ["Poluzzi", "Alessandro", ""]]}, {"id": "2012.15115", "submitter": "Michael Sejr Schlichtkrull", "authors": "Michael Schlichtkrull, Vladimir Karpukhin, Barlas O\\u{g}uz, Mike\n  Lewis, Wen-tau Yih, Sebastian Riedel", "title": "Joint Verification and Reranking for Open Fact Checking Over Tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured information is an important knowledge source for automatic\nverification of factual claims. Nevertheless, the majority of existing research\ninto this task has focused on textual data, and the few recent inquiries into\nstructured data have been for the closed-domain setting where appropriate\nevidence for each claim is assumed to have already been retrieved. In this\npaper, we investigate verification over structured data in the open-domain\nsetting, introducing a joint reranking-and-verification model which fuses\nevidence documents in the verification component. Our open-domain model\nachieves performance comparable to the closed-domain state-of-the-art on the\nTabFact dataset, and demonstrates performance gains from the inclusion of\nmultiple tables as well as a significant improvement over a heuristic retrieval\nbaseline.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 11:22:31 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Schlichtkrull", "Michael", ""], ["Karpukhin", "Vladimir", ""], ["O\u011fuz", "Barlas", ""], ["Lewis", "Mike", ""], ["Yih", "Wen-tau", ""], ["Riedel", "Sebastian", ""]]}, {"id": "2012.15203", "submitter": "Manjesh Kumar Hanawal", "authors": "Debamita Ghosh and Manjesh K. Hanawal and Nikola Zlatanov", "title": "Learning to Optimize Energy Efficiency in Energy Harvesting Wireless\n  Sensor Networks", "comments": "5 pages, 4 figures. Under review at IEEE Wireless Communications\n  Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study wireless power transmission by an energy source to multiple energy\nharvesting nodes with the aim to maximize the energy efficiency. The source\ntransmits energy to the nodes using one of the available power levels in each\ntime slot and the nodes transmit information back to the energy source using\nthe harvested energy. The source does not have any channel state information\nand it only knows whether a received codeword from a given node was\nsuccessfully decoded or not. With this limited information, the source has to\nlearn the optimal power level that maximizes the energy efficiency of the\nnetwork. We model the problem as a stochastic Multi-Armed Bandits problem and\ndevelop an Upper Confidence Bound based algorithm, which learns the optimal\ntransmit power of the energy source that maximizes the energy efficiency.\nNumerical results validate the performance guarantees of the proposed algorithm\nand show significant gains compared to the benchmark schemes.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 15:51:39 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Ghosh", "Debamita", ""], ["Hanawal", "Manjesh K.", ""], ["Zlatanov", "Nikola", ""]]}, {"id": "2012.15231", "submitter": "Ivan Letteri", "authors": "Ivan Letteri, Antonio Di Cecco, Abeer Dyoub, Giuseppe Della Penna", "title": "A Novel Resampling Technique for Imbalanced Dataset Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the enormous amount of data, particular events of interest can still\nbe quite rare. Classification of rare events is a common problem in many\ndomains, such as fraudulent transactions, malware traffic analysis and network\nintrusion detection. Many studies have been developed for malware detection\nusing machine learning approaches on various datasets, but as far as we know\nonly the MTA-KDD'19 dataset has the peculiarity of updating the representative\nset of malicious traffic on a daily basis. This daily updating is the added\nvalue of the dataset, but it translates into a potential due to the class\nimbalance problem that the RRw-Optimized MTA-KDD'19 will occur. We capture\ndifficulties of class distribution in real datasets by considering four types\nof minority class examples: safe, borderline, rare and outliers. In this work,\nwe developed two versions of Generative Silhouette Resampling 1-Nearest\nNeighbour (G1Nos) oversampling algorithms for dealing with class imbalance\nproblem. The first module of G1Nos algorithms performs a coefficient-based\ninstance selection silhouette identifying the critical threshold of Imbalance\nDegree. (ID), the second module generates synthetic samples using a SMOTE-like\noversampling algorithm. The balancing of the classes is done by our G1Nos\nalgorithms to re-establish the proportions between the two classes of the used\ndataset. The experimental results show that our oversampling algorithm work\nbetter than the other two SOTA methodologies in all the metrics considered.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 17:17:08 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Letteri", "Ivan", ""], ["Di Cecco", "Antonio", ""], ["Dyoub", "Abeer", ""], ["Della Penna", "Giuseppe", ""]]}, {"id": "2012.15259", "submitter": "Joshua Lee", "authors": "Joshua Lee, Yuheng Bu, Prasanna Sattigeri, Rameswar Panda, Gregory\n  Wornell, Leonid Karlinsky, Rogerio Feris", "title": "A Maximal Correlation Approach to Imposing Fairness in Machine Learning", "comments": "9 Pages 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As machine learning algorithms grow in popularity and diversify to many\nindustries, ethical and legal concerns regarding their fairness have become\nincreasingly relevant. We explore the problem of algorithmic fairness, taking\nan information-theoretic view. The maximal correlation framework is introduced\nfor expressing fairness constraints and shown to be capable of being used to\nderive regularizers that enforce independence and separation-based fairness\ncriteria, which admit optimization algorithms for both discrete and continuous\nvariables which are more computationally efficient than existing algorithms. We\nshow that these algorithms provide smooth performance-fairness tradeoff curves\nand perform competitively with state-of-the-art methods on both discrete\ndatasets (COMPAS, Adult) and continuous datasets (Communities and Crimes).\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 18:15:05 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Lee", "Joshua", ""], ["Bu", "Yuheng", ""], ["Sattigeri", "Prasanna", ""], ["Panda", "Rameswar", ""], ["Wornell", "Gregory", ""], ["Karlinsky", "Leonid", ""], ["Feris", "Rogerio", ""]]}, {"id": "2012.15274", "submitter": "You-Lin Chen", "authors": "You-Lin Chen, Zhaoran Wang, Mladen Kolar", "title": "Provably Training Neural Network Classifiers under Fairness Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a classifier under fairness constraints has gotten increasing\nattention in the machine learning community thanks to moral, legal, and\nbusiness reasons. However, several recent works addressing algorithmic fairness\nhave only focused on simple models such as logistic regression or support\nvector machines due to non-convex and non-differentiable fairness criteria\nacross protected groups, such as race or gender. Neural networks, the most\nwidely used models for classification nowadays, are precluded and lack\ntheoretical guarantees. This paper aims to fill this missing but crucial part\nof the literature of algorithmic fairness for neural networks. In particular,\nwe show that overparametrized neural networks could meet the fairness\nconstraints. The key ingredient of building a fair neural network classifier is\nestablishing no-regret analysis for neural networks in the overparameterization\nregime, which may be of independent interest in the online learning of neural\nnetworks and related applications.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 18:46:50 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Chen", "You-Lin", ""], ["Wang", "Zhaoran", ""], ["Kolar", "Mladen", ""]]}, {"id": "2012.15278", "submitter": "Nora M. Villanueva", "authors": "Nora M. Villanueva and Marta Sestelo and Celestino Ord\\'o\\~nez and\n  Javier Roca-Pardi\\~nas", "title": "An automatic procedure to determine groups of nonparametric regression\n  curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In many situations it could be interesting to ascertain whether nonparametric\nregression curves can be grouped, especially when confronted with a\nconsiderable number of curves. The proposed testing procedure allows to\ndetermine groups with an automatic selection of their number. A simulation\nstudy is presented in order to investigate the finite sample properties of the\nproposed methods when compared to existing alternative procedures. Finally, the\napplicability of the procedure to study the geometry of a tunnel by analysing a\nset of cross-sections is demonstrated. The results obtained show the existence\nof some heterogeneity in the tunnel geometry.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 18:49:21 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 16:45:04 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Villanueva", "Nora M.", ""], ["Sestelo", "Marta", ""], ["Ord\u00f3\u00f1ez", "Celestino", ""], ["Roca-Pardi\u00f1as", "Javier", ""]]}, {"id": "2012.15301", "submitter": "Naz Gul Ms", "authors": "Zardad Khan, Naz Gul, Nosheen Faiz, Asma Gul, Werner Adler, Berthold\n  Lausen", "title": "Optimal trees selection for classification via out-of-bag assessment and\n  sub-bagging", "comments": "20 pages,4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The effect of training data size on machine learning methods has been well\ninvestigated over the past two decades. The predictive performance of tree\nbased machine learning methods, in general, improves with a decreasing rate as\nthe size of training data increases. We investigate this in optimal trees\nensemble (OTE) where the method fails to learn from some of the training\nobservations due to internal validation. Modified tree selection methods are\nthus proposed for OTE to cater for the loss of training observations in\ninternal validation. In the first method, corresponding out-of-bag (OOB)\nobservations are used in both individual and collective performance assessment\nfor each tree. Trees are ranked based on their individual performance on the\nOOB observations. A certain number of top ranked trees is selected and starting\nfrom the most accurate tree, subsequent trees are added one by one and their\nimpact is recorded by using the OOB observations left out from the bootstrap\nsample taken for the tree being added. A tree is selected if it improves\npredictive accuracy of the ensemble. In the second approach, trees are grown on\nrandom subsets, taken without replacement-known as sub-bagging, of the training\ndata instead of bootstrap samples (taken with replacement). The remaining\nobservations from each sample are used in both individual and collective\nassessments for each corresponding tree similar to the first method. Analysis\non 21 benchmark datasets and simulations studies show improved performance of\nthe modified methods in comparison to OTE and other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 19:44:11 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Khan", "Zardad", ""], ["Gul", "Naz", ""], ["Faiz", "Nosheen", ""], ["Gul", "Asma", ""], ["Adler", "Werner", ""], ["Lausen", "Berthold", ""]]}, {"id": "2012.15332", "submitter": "Ozan \\.Irsoy", "authors": "Ozan \\.Irsoy, Adrian Benton, Karl Stratos", "title": "k\\=oan: A Corrected CBOW Implementation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  It is a common belief in the NLP community that continuous bag-of-words\n(CBOW) word embeddings tend to underperform skip-gram (SG) embeddings. We find\nthat this belief is founded less on theoretical differences in their training\nobjectives but more on faulty CBOW implementations in standard software\nlibraries such as the official implementation word2vec.c and Gensim. We show\nthat our correct implementation of CBOW yields word embeddings that are fully\ncompetitive with SG on various intrinsic and extrinsic tasks while being more\nthan three times as fast to train. We release our implementation, k\\=oan, at\nhttps://github.com/bloomberg/koan.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 21:37:28 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["\u0130rsoy", "Ozan", ""], ["Benton", "Adrian", ""], ["Stratos", "Karl", ""]]}, {"id": "2012.15339", "submitter": "Florian Gerber", "authors": "Florian Gerber and Douglas W. Nychka", "title": "Fast covariance parameter estimation of spatial Gaussian process models\n  using neural networks", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gaussian processes (GPs) are a popular model for spatially referenced data\nand allow descriptive statements, predictions at new locations, and simulation\nof new fields. Often a few parameters are sufficient to parameterize the\ncovariance function, and maximum likelihood (ML) methods can be used to\nestimate these parameters from data. ML methods, however, are computationally\ndemanding. For example, in the case of local likelihood estimation, even\nfitting covariance models on modest size windows can overwhelm typical\ncomputational resources for data analysis. This limitation motivates the idea\nof using neural network (NN) methods to approximate ML estimates. We train NNs\nto take moderate size spatial fields or variograms as input and return the\nrange and noise-to-signal covariance parameters. Once trained, the NNs provide\nestimates with a similar accuracy compared to ML estimation and at a speedup by\na factor of 100 or more. Although we focus on a specific covariance estimation\nproblem motivated by a climate science application, this work can be easily\nextended to other, more complex, spatial problems and provides a\nproof-of-concept for this use of machine learning in computational statistics.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 22:06:26 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Gerber", "Florian", ""], ["Nychka", "Douglas W.", ""]]}, {"id": "2012.15411", "submitter": "Raghu Bollapragada", "authors": "Yuchen Xie, Raghu Bollapragada, Richard Byrd and Jorge Nocedal", "title": "Constrained and Composite Optimization via Adaptive Sampling Methods", "comments": "26 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The motivation for this paper stems from the desire to develop an adaptive\nsampling method for solving constrained optimization problems in which the\nobjective function is stochastic and the constraints are deterministic. The\nmethod proposed in this paper\n  is a proximal gradient method that can also be applied to the composite\noptimization problem min f(x) + h(x), where f is stochastic and h is convex\n(but not necessarily differentiable). Adaptive sampling methods employ a\nmechanism for gradually improving the quality of the gradient approximation so\nas to keep computational cost to a minimum. The mechanism commonly employed in\nunconstrained optimization is no longer reliable in the constrained or\ncomposite optimization settings because it is based on pointwise decisions that\ncannot correctly predict the quality of the proximal gradient step. The method\nproposed in this paper measures the result of a complete step to determine if\nthe gradient approximation is accurate enough; otherwise a more accurate\ngradient is generated and a new step is computed. Convergence results are\nestablished both for strongly convex and general convex f. Numerical\nexperiments are presented to illustrate the practical behavior of the method.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 02:50:39 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Xie", "Yuchen", ""], ["Bollapragada", "Raghu", ""], ["Byrd", "Richard", ""], ["Nocedal", "Jorge", ""]]}, {"id": "2012.15458", "submitter": "Vincent Roulet", "authors": "Vincent Roulet and Zaid Harchaoui", "title": "Differentiable Programming \\`a la Moreau", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of a Moreau envelope is central to the analysis of first-order\noptimization algorithms for machine learning. Yet, it has not been developed\nand extended to be applied to a deep network and, more broadly, to a machine\nlearning system with a differentiable programming implementation. We define a\ncompositional calculus adapted to Moreau envelopes and show how to integrate it\nwithin differentiable programming. The proposed framework casts in a\nmathematical optimization framework several variants of gradient\nback-propagation related to the idea of the propagation of virtual targets.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 05:56:51 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Roulet", "Vincent", ""], ["Harchaoui", "Zaid", ""]]}, {"id": "2012.15467", "submitter": "Zhenzhen Li", "authors": "Thomas Y. Hou, Zhenzhen Li, Ziyun Zhang", "title": "Fast Global Convergence for Low-rank Matrix Recovery via Riemannian\n  Gradient Descent with Random Initialization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.OC math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a new global analysis framework for a class of\nlow-rank matrix recovery problems on the Riemannian manifold. We analyze the\nglobal behavior for the Riemannian optimization with random initialization. We\nuse the Riemannian gradient descent algorithm to minimize a least squares loss\nfunction, and study the asymptotic behavior as well as the exact convergence\nrate. We reveal a previously unknown geometric property of the low-rank matrix\nmanifold, which is the existence of spurious critical points for the simple\nleast squares function on the manifold. We show that under some assumptions,\nthe Riemannian gradient descent starting from a random initialization with high\nprobability avoids these spurious critical points and only converges to the\nground truth in nearly linear convergence rate, i.e.\n$\\mathcal{O}(\\text{log}(\\frac{1}{\\epsilon})+ \\text{log}(n))$ iterations to\nreach an $\\epsilon$-accurate solution. We use two applications as examples for\nour global analysis. The first one is a rank-1 matrix recovery problem. The\nsecond one is a generalization of the Gaussian phase retrieval problem. It only\nsatisfies the weak isometry property, but has behavior similar to that of the\nfirst one except for an extra saddle set. Our convergence guarantee is nearly\noptimal and almost dimension-free, which fully explains the numerical\nobservations. The global analysis can be potentially extended to other data\nproblems with random measurement structures and empirical least squares loss\nfunctions.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 06:40:43 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 05:55:05 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Hou", "Thomas Y.", ""], ["Li", "Zhenzhen", ""], ["Zhang", "Ziyun", ""]]}, {"id": "2012.15477", "submitter": "Atsushi Nitanda", "authors": "Atsushi Nitanda, Denny Wu, Taiji Suzuki", "title": "Particle Dual Averaging: Optimization of Mean Field Neural Networks with\n  Global Convergence Rate Analysis", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the particle dual averaging (PDA) method, which generalizes the\ndual averaging method in convex optimization to the optimization over\nprobability distributions with quantitative runtime guarantee. The algorithm\nconsists of an inner loop and outer loop: the inner loop utilizes the Langevin\nalgorithm to approximately solve for a stationary distribution, which is then\noptimized in the outer loop. The method can thus be interpreted as an extension\nof the Langevin algorithm to naturally handle nonlinear functional on the\nprobability space. An important application of the proposed method is the\noptimization of neural network in the mean field regime, which is theoretically\nattractive due to the presence of nonlinear feature learning, but quantitative\nconvergence rate can be challenging to obtain. By adapting finite-dimensional\nconvex optimization theory into the space of distributions, we analyze PDA in\nregularized empirical / expected risk minimization, and establish quantitative\nglobal convergence in learning two-layer mean field neural networks under more\ngeneral settings. Our theoretical results are supported by numerical\nsimulations on neural networks with reasonable size.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 07:07:32 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 15:04:53 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Nitanda", "Atsushi", ""], ["Wu", "Denny", ""], ["Suzuki", "Taiji", ""]]}, {"id": "2012.15480", "submitter": "Rob Brekelmans", "authors": "Rob Brekelmans, Frank Nielsen, Alireza Makhzani, Aram Galstyan, Greg\n  Ver Steeg", "title": "Likelihood Ratio Exponential Families", "comments": "NeurIPS Workshop on Deep Learning through Information Geometry", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exponential family is well known in machine learning and statistical\nphysics as the maximum entropy distribution subject to a set of observed\nconstraints, while the geometric mixture path is common in MCMC methods such as\nannealed importance sampling. Linking these two ideas, recent work has\ninterpreted the geometric mixture path as an exponential family of\ndistributions to analyze the thermodynamic variational objective (TVO).\n  We extend these likelihood ratio exponential families to include solutions to\nrate-distortion (RD) optimization, the information bottleneck (IB) method, and\nrecent rate-distortion-classification approaches which combine RD and IB. This\nprovides a common mathematical framework for understanding these methods via\nthe conjugate duality of exponential families and hypothesis testing. Further,\nwe collect existing results to provide a variational representation of\nintermediate RD or TVO distributions as a minimizing an expectation of KL\ndivergences. This solution also corresponds to a size-power tradeoff using the\nlikelihood ratio test and the Neyman Pearson lemma. In thermodynamic\nintegration bounds such as the TVO, we identify the intermediate distribution\nwhose expected sufficient statistics match the log partition function.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 07:13:58 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 06:06:55 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Brekelmans", "Rob", ""], ["Nielsen", "Frank", ""], ["Makhzani", "Alireza", ""], ["Galstyan", "Aram", ""], ["Steeg", "Greg Ver", ""]]}, {"id": "2012.15483", "submitter": "Horia Mania", "authors": "Horia Mania, Suvrit Sra", "title": "Why do classifier accuracies show linear trends under distribution\n  shift?", "comments": "18 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies of generalization in deep learning have observed a puzzling\ntrend: accuracies of models on one data distribution are approximately linear\nfunctions of the accuracies on another distribution. We explain this trend\nunder an intuitive assumption on model similarity, which was verified\nempirically in prior work. More precisely, we assume the probability that two\nmodels agree in their predictions is higher than what we can infer from their\naccuracy levels alone. Then, we show that a linear trend must occur when\nevaluating models on two distributions unless the size of the distribution\nshift is large. This work emphasizes the value of understanding model\nsimilarity, which can have an impact on the generalization and robustness of\nclassification models.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 07:24:30 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 22:58:38 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Mania", "Horia", ""], ["Sra", "Suvrit", ""]]}, {"id": "2012.15550", "submitter": "Maxim Panov", "authors": "Achille Thin, Nikita Kotelevskii, Christophe Andrieu, Alain Durmus,\n  Eric Moulines, Maxim Panov", "title": "Nonreversible MCMC from conditional invertible transforms: a complete\n  recipe with convergence guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo (MCMC) is a class of algorithms to sample complex\nand high-dimensional probability distributions. The Metropolis-Hastings (MH)\nalgorithm, the workhorse of MCMC, provides a simple recipe to construct\nreversible Markov kernels. Reversibility is a tractable property that implies a\nless tractable but essential property here, invariance. Reversibility is\nhowever not necessarily desirable when considering performance. This has\nprompted recent interest in designing kernels breaking this property. At the\nsame time, an active stream of research has focused on the design of novel\nversions of the MH kernel, some nonreversible, relying on the use of complex\ninvertible deterministic transforms. While standard implementations of the MH\nkernel are well understood, the aforementioned developments have not received\nthe same systematic treatment to ensure their validity. This paper fills the\ngap by developing general tools to ensure that a class of nonreversible Markov\nkernels, possibly relying on complex transforms, has the desired invariance\nproperty and leads to convergent algorithms. This leads to a set of simple and\npractically verifiable conditions.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 11:22:22 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 12:30:47 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Thin", "Achille", ""], ["Kotelevskii", "Nikita", ""], ["Andrieu", "Christophe", ""], ["Durmus", "Alain", ""], ["Moulines", "Eric", ""], ["Panov", "Maxim", ""]]}, {"id": "2012.15566", "submitter": "Andrew Warrington", "authors": "Andrew Warrington and J. Wilder Lavington and Adam \\'Scibior and Mark\n  Schmidt and Frank Wood", "title": "Robust Asymmetric Learning in POMDPs", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Policies for partially observed Markov decision processes can be efficiently\nlearned by imitating policies for the corresponding fully observed Markov\ndecision processes. Unfortunately, existing approaches for this kind of\nimitation learning have a serious flaw: the expert does not know what the\ntrainee cannot see, and so may encourage actions that are sub-optimal, even\nunsafe, under partial information. We derive an objective to instead train the\nexpert to maximize the expected reward of the imitating agent policy, and use\nit to construct an efficient algorithm, adaptive asymmetric DAgger (A2D), that\njointly trains the expert and the agent. We show that A2D produces an expert\npolicy that the agent can safely imitate, in turn outperforming policies\nlearned by imitating a fixed expert.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 11:46:51 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 10:57:52 GMT"}, {"version": "v3", "created": "Thu, 1 Jul 2021 11:07:17 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Warrington", "Andrew", ""], ["Lavington", "J. Wilder", ""], ["\u015acibior", "Adam", ""], ["Schmidt", "Mark", ""], ["Wood", "Frank", ""]]}, {"id": "2012.15584", "submitter": "Yuko Kuroki", "authors": "Yuko Kuroki, Junya Honda, Masashi Sugiyama", "title": "Combinatorial Pure Exploration with Full-bandit Feedback and Beyond:\n  Solving Combinatorial Optimization under Uncertainty with Limited Observation", "comments": "Preprint of an Invited Review Article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DM cs.DS cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combinatorial optimization is one of the fundamental research fields that has\nbeen extensively studied in theoretical computer science and operations\nresearch. When developing an algorithm for combinatorial optimization, it is\ncommonly assumed that parameters such as edge weights are exactly known as\ninputs. However, this assumption may not be fulfilled since input parameters\nare often uncertain or initially unknown in many applications such as\nrecommender systems, crowdsourcing, communication networks, and online\nadvertisement. To resolve such uncertainty, the problem of combinatorial pure\nexploration of multi-armed bandits (CPE) and its variants have recieved\nincreasing attention. Earlier work on CPE has studied the semi-bandit feedback\nor assumed that the outcome from each individual edge is always accessible at\nall rounds. However, due to practical constraints such as a budget ceiling or\nprivacy concern, such strong feedback is not always available in recent\napplications. In this article, we review recently proposed techniques for\ncombinatorial pure exploration problems with limited feedback.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 12:40:52 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Kuroki", "Yuko", ""], ["Honda", "Junya", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "2012.15664", "submitter": "Snigdha Panigrahi", "authors": "Snigdha Panigrahi, Peter W. MacDonald, Daniel Kessler", "title": "Inference post Selection of Group-sparse Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a post-selective Bayesian framework to jointly and consistently\nestimate parameters within automatic group-sparse regression models. Selected\nthrough an indispensable class of learning algorithms, e.g. the Group LASSO,\nthe overlapping Group LASSO, the sparse Group LASSO etc., uncertainty estimates\nfor the matched parameters are unreliable in the absence of adjustments for\nselection bias. Limiting however the application of state of the art tools for\nthe group-sparse problem include estimation strictly tailored to (i)\nreal-valued projections onto very specific selected subspaces, (ii) selection\nevents admitting representations as linear inequalities in the data variables.\nOur Bayesian methods address these gaps by deriving an adjustment factor in an\neasily feasible analytic form that eliminates bias from the selection of\npromising groups. Paying a very nominal price for this adjustment, experiments\non simulated data and the Human Connectome Project demonstrate the efficacy of\nour methods at a joint estimation of group-sparse parameters learned from data.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 15:43:26 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 03:58:49 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Panigrahi", "Snigdha", ""], ["MacDonald", "Peter W.", ""], ["Kessler", "Daniel", ""]]}, {"id": "2012.15802", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas and Daniel M. Kane", "title": "The Sample Complexity of Robust Covariance Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of testing the covariance matrix of a high-dimensional\nGaussian in a robust setting, where the input distribution has been corrupted\nin Huber's contamination model. Specifically, we are given i.i.d. samples from\na distribution of the form $Z = (1-\\epsilon) X + \\epsilon B$, where $X$ is a\nzero-mean and unknown covariance Gaussian $\\mathcal{N}(0, \\Sigma)$, $B$ is a\nfixed but unknown noise distribution, and $\\epsilon>0$ is an arbitrarily small\nconstant representing the proportion of contamination. We want to distinguish\nbetween the cases that $\\Sigma$ is the identity matrix versus $\\gamma$-far from\nthe identity in Frobenius norm.\n  In the absence of contamination, prior work gave a simple tester for this\nhypothesis testing task that uses $O(d)$ samples. Moreover, this sample upper\nbound was shown to be best possible, within constant factors. Our main result\nis that the sample complexity of covariance testing dramatically increases in\nthe contaminated setting. In particular, we prove a sample complexity lower\nbound of $\\Omega(d^2)$ for $\\epsilon$ an arbitrarily small constant and $\\gamma\n= 1/2$. This lower bound is best possible, as $O(d^2)$ samples suffice to even\nrobustly {\\em learn} the covariance. The conceptual implication of our result\nis that, for the natural setting we consider, robust hypothesis testing is at\nleast as hard as robust estimation.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 18:24:41 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""]]}, {"id": "2012.15814", "submitter": "Jiayuan Mao", "authors": "Ruocheng Wang, Jiayuan Mao, Samuel J. Gershman, Jiajun Wu", "title": "Language-Mediated, Object-Centric Representation Learning", "comments": "ACL 2021 Findings. First two authors contributed equally; last two\n  authors contributed equally. Project page: https://lang-orl.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Language-mediated, Object-centric Representation Learning (LORL),\na paradigm for learning disentangled, object-centric scene representations from\nvision and language. LORL builds upon recent advances in unsupervised object\ndiscovery and segmentation, notably MONet and Slot Attention. While these\nalgorithms learn an object-centric representation just by reconstructing the\ninput image, LORL enables them to further learn to associate the learned\nrepresentations to concepts, i.e., words for object categories, properties, and\nspatial relationships, from language input. These object-centric concepts\nderived from language facilitate the learning of object-centric\nrepresentations. LORL can be integrated with various unsupervised object\ndiscovery algorithms that are language-agnostic. Experiments show that the\nintegration of LORL consistently improves the performance of unsupervised\nobject discovery methods on two datasets via the help of language. We also show\nthat concepts learned by LORL, in conjunction with object discovery methods,\naid downstream tasks such as referring expression comprehension.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 18:36:07 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 04:37:54 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Wang", "Ruocheng", ""], ["Mao", "Jiayuan", ""], ["Gershman", "Samuel J.", ""], ["Wu", "Jiajun", ""]]}, {"id": "2012.15816", "submitter": "Silvia Chiappa", "authors": "Luca Oneto, Silvia Chiappa", "title": "Fairness in Machine Learning", "comments": null, "journal-ref": "Recent Trends in Learning From Data. Studies in Computational\n  Intelligence, vol 896. Springer, Cham, 2020", "doi": "10.1007/978-3-030-43883-8_7", "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning based systems are reaching society at large and in many\naspects of everyday life. This phenomenon has been accompanied by concerns\nabout the ethical issues that may arise from the adoption of these\ntechnologies. ML fairness is a recently established area of machine learning\nthat studies how to ensure that biases in the data and model inaccuracies do\nnot lead to models that treat individuals unfavorably on the basis of\ncharacteristics such as e.g. race, gender, disabilities, and sexual or\npolitical orientation. In this manuscript, we discuss some of the limitations\npresent in the current reasoning about fairness and in methods that deal with\nit, and describe some work done by the authors to address them. More\nspecifically, we show how causal Bayesian networks can play an important role\nto reason about and deal with fairness, especially in complex unfairness\nscenarios. We describe how optimal transport theory can be used to develop\nmethods that impose constraints on the full shapes of distributions\ncorresponding to different sensitive attributes, overcoming the limitation of\nmost approaches that approximate fairness desiderata by imposing constraints on\nthe lower order moments or other functions of those distributions. We present a\nunified framework that encompasses methods that can deal with different\nsettings and fairness criteria, and that enjoys strong theoretical guarantees.\nWe introduce an approach to learn fair representations that can generalize to\nunseen tasks. Finally, we describe a technique that accounts for legal\nrestrictions about the use of sensitive attributes.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 18:38:58 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Oneto", "Luca", ""], ["Chiappa", "Silvia", ""]]}]