[{"id": "0712.0189", "submitter": "Jeffrey Picka", "authors": "Jeffrey Picka and Mingxia Deng", "title": "Summarization and Classification of Non-Poisson Point Processes", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": null, "abstract": "  Fitting models for non-Poisson point processes is complicated by the lack of\ntractable models for much of the data. By using large samples of independent\nand identically distributed realizations and statistical learning, it is\npossible to identify absence of fit through finding a classification rule that\ncan efficiently identify single realizations of each type. The method requires\na much wider range of descriptive statistics than are currently in use, and a\nnew concept of model fitting which is derive from how physical laws are judged\nto fit data.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2007 21:48:10 GMT"}], "update_date": "2007-12-04", "authors_parsed": [["Picka", "Jeffrey", ""], ["Deng", "Mingxia", ""]]}, {"id": "0712.0248", "submitter": "Olivier Catoni", "authors": "Olivier Catoni", "title": "Pac-Bayesian Supervised Classification: The Thermodynamics of\n  Statistical Learning", "comments": "Published in at http://dx.doi.org/10.1214/074921707000000391 the IMS\n  Lecture Notes Monograph Series\n  (http://www.imstat.org/publications/lecnotes.htm) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "IMS Lecture Notes Monograph Series 2007, Vol. 56, i-xii, 1-163", "doi": "10.1214/074921707000000391", "report-no": "IMS-LNMS56-LNMS5601", "categories": "stat.ML", "license": null, "abstract": "  This monograph deals with adaptive supervised classification, using tools\nborrowed from statistical mechanics and information theory, stemming from the\nPACBayesian approach pioneered by David McAllester and applied to a conception\nof statistical learning theory forged by Vladimir Vapnik. Using convex analysis\non the set of posterior probability measures, we show how to get local measures\nof the complexity of the classification model involving the relative entropy of\nposterior distributions with respect to Gibbs posterior measures. We then\ndiscuss relative bounds, comparing the generalization error of two\nclassification rules, showing how the margin assumption of Mammen and Tsybakov\ncan be replaced with some empirical measure of the covariance structure of the\nclassification model.We show how to associate to any posterior distribution an\neffective temperature relating it to the Gibbs prior distribution with the same\nlevel of expected error rate, and how to estimate this effective temperature\nfrom data, resulting in an estimator whose expected error rate converges\naccording to the best possible power of the sample size adaptively under any\nmargin and parametric complexity assumptions. We describe and study an\nalternative selection scheme based on relative bounds between estimators, and\npresent a two step localization technique which can handle the selection of a\nparametric model from a family of those. We show how to extend systematically\nall the results obtained in the inductive setting to transductive learning, and\nuse this to improve Vapnik's generalization bounds, extending them to the case\nwhen the sample is made of independent non-identically distributed pairs of\npatterns and labels. Finally we review briefly the construction of Support\nVector Machines and show how to derive generalization bounds for them,\nmeasuring the complexity either through the number of support vectors or\nthrough the value of the transductive or inductive margin.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2007 13:49:36 GMT"}], "update_date": "2007-12-04", "authors_parsed": [["Catoni", "Olivier", ""]]}, {"id": "0712.1027", "submitter": "Mu Zhu", "authors": "Mu Zhu", "title": "Kernels and Ensembles: Perspectives on Statistical Learning", "comments": "22 pages; 6 figures; sumitted to The American Statistician", "journal-ref": "The American Statistician, May 2008, Vol. 62, No. 2, Pages 97 -\n  109.", "doi": "10.1198/000313008X306367", "report-no": null, "categories": "stat.ME stat.ML", "license": null, "abstract": "  Since their emergence in the 1990's, the support vector machine and the\nAdaBoost algorithm have spawned a wave of research in statistical machine\nlearning. Much of this new research falls into one of two broad categories:\nkernel methods and ensemble methods. In this expository article, I discuss the\nmain ideas behind these two types of methods, namely how to transform linear\nalgorithms into nonlinear ones by using kernel functions, and how to make\npredictions with an ensemble or a collection of models rather than a single\nmodel. I also share my personal perspectives on how these ideas have influenced\nand shaped my own research. In particular, I present two recent algorithms that\nI have invented with my collaborators: LAGO, a fast kernel algorithm for\nunbalanced classification and rare target detection; and Darwinian evolution in\nparallel universes, an ensemble method for variable selection.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2007 20:31:29 GMT"}], "update_date": "2008-04-15", "authors_parsed": [["Zhu", "Mu", ""]]}, {"id": "0712.1219", "submitter": "Francois Meyer", "authors": "Francois G. Meyer and Greg J. Stephens", "title": "Locality and low-dimensions in the prediction of natural experience from\n  fMRI", "comments": "To appear in: Advances in Neural Information Processing Systems 20,\n  Scholkopf B., Platt J. and Hofmann T. (Editors), MIT Press, 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.ML", "license": null, "abstract": "  Functional Magnetic Resonance Imaging (fMRI) provides dynamical access into\nthe complex functioning of the human brain, detailing the hemodynamic activity\nof thousands of voxels during hundreds of sequential time points. One approach\ntowards illuminating the connection between fMRI and cognitive function is\nthrough decoding; how do the time series of voxel activities combine to provide\ninformation about internal and external experience? Here we seek models of fMRI\ndecoding which are balanced between the simplicity of their interpretation and\nthe effectiveness of their prediction. We use signals from a subject immersed\nin virtual reality to compare global and local methods of prediction applying\nboth linear and nonlinear techniques of dimensionality reduction. We find that\nthe prediction of complex stimuli is remarkably low-dimensional, saturating\nwith less than 100 features. In particular, we build effective models based on\nthe decorrelated components of cognitive activity in the classically-defined\nBrodmann areas. For some of the stimuli, the top predictive areas were\nsurprisingly transparent, including Wernicke's area for verbal instructions,\nvisual cortex for facial and body features, and visual-temporal regions for\nvelocity. Direct sensory experience resulted in the most robust predictions,\nwith the highest correlation ($c \\sim 0.8$) between the predicted and\nexperienced time series of verbal instructions. Techniques based on non-linear\ndimensionality reduction (Laplacian eigenmaps) performed similarly. The\ninterpretability and relative simplicity of our approach provides a conceptual\nbasis upon which to build more sophisticated techniques for fMRI decoding and\noffers a window into cognitive function during dynamic, natural experience.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2007 20:21:18 GMT"}, {"version": "v2", "created": "Sat, 12 Jan 2008 01:00:50 GMT"}], "update_date": "2008-01-16", "authors_parsed": [["Meyer", "Francois G.", ""], ["Stephens", "Greg J.", ""]]}, {"id": "0712.1698", "submitter": "Pierre Alquier", "authors": "Pierre Alquier (PMA, Crest)", "title": "PAC-Bayesian Bounds for Randomized Empirical Risk Minimizers", "comments": null, "journal-ref": "Mathematical Methods of Statistics 17, 4 (2008) 279-304", "doi": "10.3103/S1066530708040017", "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to generalize the PAC-Bayesian theorems proved by\nCatoni in the classification setting to more general problems of statistical\ninference. We show how to control the deviations of the risk of randomized\nestimators. A particular attention is paid to randomized estimators drawn in a\nsmall neighborhood of classical estimators, whose study leads to control the\nrisk of the latter. These results allow to bound the risk of very general\nestimation procedures, as well as to perform model selection.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2007 12:40:41 GMT"}, {"version": "v2", "created": "Tue, 4 Nov 2008 10:37:52 GMT"}, {"version": "v3", "created": "Fri, 9 Jan 2009 15:13:19 GMT"}], "update_date": "2009-01-09", "authors_parsed": [["Alquier", "Pierre", "", "PMA, Crest"]]}, {"id": "0712.2526", "submitter": "Jon McAuliffe", "authors": "Michael Braun and Jon McAuliffe", "title": "Variational inference for large-scale models of discrete choice", "comments": "29 pages, 2 tables, 2 figures", "journal-ref": "Journal of the American Statistical Association (2010) 105(489):\n  324-334", "doi": "10.1198/jasa.2009.tm08030", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": null, "abstract": "  Discrete choice models are commonly used by applied statisticians in numerous\nfields, such as marketing, economics, finance, and operations research. When\nagents in discrete choice models are assumed to have differing preferences,\nexact inference is often intractable. Markov chain Monte Carlo techniques make\napproximate inference possible, but the computational cost is prohibitive on\nthe large data sets now becoming routinely available. Variational methods\nprovide a deterministic alternative for approximation of the posterior\ndistribution. We derive variational procedures for empirical Bayes and fully\nBayesian inference in the mixed multinomial logit model of discrete choice. The\nalgorithms require only that we solve a sequence of unconstrained optimization\nproblems, which are shown to be convex. Extensive simulations demonstrate that\nvariational methods achieve accuracy competitive with Markov chain Monte Carlo,\nat a small fraction of the computational cost. Thus, variational methods permit\ninferences on data sets that otherwise could not be analyzed without\nbias-inducing modifications to the underlying model.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2007 16:16:18 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2007 18:46:25 GMT"}, {"version": "v3", "created": "Tue, 15 Jan 2008 18:03:40 GMT"}], "update_date": "2010-06-04", "authors_parsed": [["Braun", "Michael", ""], ["McAuliffe", "Jon", ""]]}, {"id": "0712.2682", "submitter": "Kai Puolamaki", "authors": "Kai Puolam\\\"aki, Sami Hanhij\\\"arvi, Gemma C. Garriga", "title": "An Approximation Ratio for Biclustering", "comments": "9 pages, 2 figures; presentation clarified, replaced to match the\n  version to be published in IPL", "journal-ref": "Information Processing Letters 108 (2008) 45-49", "doi": "10.1016/j.ipl.2008.03.013", "report-no": "Publications in Computer and Information Science E13", "categories": "cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of biclustering consists of the simultaneous clustering of rows\nand columns of a matrix such that each of the submatrices induced by a pair of\nrow and column clusters is as uniform as possible. In this paper we approximate\nthe optimal biclustering by applying one-way clustering algorithms\nindependently on the rows and on the columns of the input matrix. We show that\nsuch a solution yields a worst-case approximation ratio of 1+sqrt(2) under\nL1-norm for 0-1 valued matrices, and of 2 under L2-norm for real valued\nmatrices.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2007 11:45:42 GMT"}, {"version": "v2", "created": "Fri, 22 Aug 2008 07:01:26 GMT"}], "update_date": "2008-08-22", "authors_parsed": [["Puolam\u00e4ki", "Kai", ""], ["Hanhij\u00e4rvi", "Sami", ""], ["Garriga", "Gemma C.", ""]]}]