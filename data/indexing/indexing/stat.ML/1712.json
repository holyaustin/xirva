[{"id": "1712.00003", "submitter": "Matthew Toews", "authors": "Ahmad Chaddad, Behnaz Naisiri, Marco Pedersoli, Eric Granger,\n  Christian Desrosiers, Matthew Toews", "title": "Modeling Information Flow Through Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a principled information theoretic analysis of\nclassification for deep neural network structures, e.g. convolutional neural\nnetworks (CNN). The output of convolutional filters is modeled as a random\nvariable Y conditioned on the object class C and network filter bank F. The\nconditional entropy (CENT) H(Y |C,F) is shown in theory and experiments to be a\nhighly compact and class-informative code, that can be computed from the filter\noutputs throughout an existing CNN and used to obtain higher classification\nresults than the original CNN itself. Experiments demonstrate the effectiveness\nof CENT feature analysis in two separate CNN classification contexts. 1) In the\nclassification of neurodegeneration due to Alzheimer's disease (AD) and natural\naging from 3D magnetic resonance image (MRI) volumes, 3 CENT features result in\nan AUC=94.6% for whole-brain AD classification, the highest reported accuracy\non the public OASIS dataset used and 12% higher than the softmax output of the\noriginal CNN trained for the task. 2) In the context of visual object\nclassification from 2D photographs, transfer learning based on a small set of\nCENT features identified throughout an existing CNN leads to AUC values\ncomparable to the 1000-feature softmax output of the original network when\nclassifying previously unseen object categories. The general information\ntheoretical analysis explains various recent CNN design successes, e.g. densely\nconnected CNN architectures, and provides insights for future research\ndirections in deep learning.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 23:09:58 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Chaddad", "Ahmad", ""], ["Naisiri", "Behnaz", ""], ["Pedersoli", "Marco", ""], ["Granger", "Eric", ""], ["Desrosiers", "Christian", ""], ["Toews", "Matthew", ""]]}, {"id": "1712.00010", "submitter": "You Jin Kim", "authors": "You Jin Kim (1), Yun-Geun Lee (1), Jeong Whun Kim (2), Jin Joo Park\n  (2), Borim Ryu (2), Jung-Woo Ha (1) ((1) Clova AI Research, NAVER Corp., (2)\n  Seoul National University Bundang Hospital)", "title": "Highrisk Prediction from Electronic Medical Records via Deep Attention\n  Networks", "comments": "Accepted poster at NIPS 2017 Workshop on Machine Learning for Health\n  (https://ml4health.github.io/2017/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting highrisk vascular diseases is a significant issue in the medical\ndomain. Most predicting methods predict the prognosis of patients from\npathological and radiological measurements, which are expensive and require\nmuch time to be analyzed. Here we propose deep attention models that predict\nthe onset of the high risky vascular disease from symbolic medical histories\nsequence of hypertension patients such as ICD-10 and pharmacy codes only,\nMedical History-based Prediction using Attention Network (MeHPAN). We\ndemonstrate two types of attention models based on 1) bidirectional gated\nrecurrent unit (R-MeHPAN) and 2) 1D convolutional multilayer model (C-MeHPAN).\nTwo MeHPAN models are evaluated on approximately 50,000 hypertension patients\nwith respect to precision, recall, f1-measure and area under the curve (AUC).\nExperimental results show that our MeHPAN methods outperform standard\nclassification models. Comparing two MeHPANs, R-MeHPAN provides more better\ndiscriminative capability with respect to all metrics while C-MeHPAN presents\nmuch shorter training time with competitive accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 15:21:50 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Kim", "You Jin", ""], ["Lee", "Yun-Geun", ""], ["Kim", "Jeong Whun", ""], ["Park", "Jin Joo", ""], ["Ryu", "Borim", ""], ["Ha", "Jung-Woo", ""]]}, {"id": "1712.00028", "submitter": "Genevieve Flaspohler", "authors": "Genevieve Flaspohler, Nicholas Roy and Yogesh Girdhar", "title": "Feature discovery and visualization of robot mission data using\n  convolutional autoencoders and Bayesian nonparametric topic models", "comments": "8 pages", "journal-ref": null, "doi": "10.1109/IROS.2017.8202130", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The gap between our ability to collect interesting data and our ability to\nanalyze these data is growing at an unprecedented rate. Recent algorithmic\nattempts to fill this gap have employed unsupervised tools to discover\nstructure in data. Some of the most successful approaches have used\nprobabilistic models to uncover latent thematic structure in discrete data.\nDespite the success of these models on textual data, they have not generalized\nas well to image data, in part because of the spatial and temporal structure\nthat may exist in an image stream.\n  We introduce a novel unsupervised machine learning framework that\nincorporates the ability of convolutional autoencoders to discover features\nfrom images that directly encode spatial information, within a Bayesian\nnonparametric topic model that discovers meaningful latent patterns within\ndiscrete data. By using this hybrid framework, we overcome the fundamental\ndependency of traditional topic models on rigidly hand-coded data\nrepresentations, while simultaneously encoding spatial dependency in our topics\nwithout adding model complexity. We apply this model to the motivating\napplication of high-level scene understanding and mission summarization for\nexploratory marine robots. Our experiments on a seafloor dataset collected by a\nmarine robot show that the proposed hybrid framework outperforms current\nstate-of-the-art approaches on the task of unsupervised seafloor terrain\ncharacterization.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 19:02:34 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Flaspohler", "Genevieve", ""], ["Roy", "Nicholas", ""], ["Girdhar", "Yogesh", ""]]}, {"id": "1712.00032", "submitter": "Xavier Roynard", "authors": "Xavier Roynard and Jean-Emmanuel Deschaud and Fran\\c{c}ois Goulette", "title": "Paris-Lille-3D: a large and high-quality ground truth urban point cloud\n  dataset for automatic segmentation and classification", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new Urban Point Cloud Dataset for Automatic\nSegmentation and Classification acquired by Mobile Laser Scanning (MLS). We\ndescribe how the dataset is obtained from acquisition to post-processing and\nlabeling. This dataset can be used to learn classification algorithm, however,\ngiven that a great attention has been paid to the split between the different\nobjects, this dataset can also be used to learn the segmentation. The dataset\nconsists of around 2km of MLS point cloud acquired in two cities. The number of\npoints and range of classes make us consider that it can be used to train\nDeep-Learning methods. Besides we show some results of automatic segmentation\nand classification. The dataset is available at:\nhttp://caor-mines-paristech.fr/fr/paris-lille-3d-dataset/\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 19:08:52 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 15:53:58 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Roynard", "Xavier", ""], ["Deschaud", "Jean-Emmanuel", ""], ["Goulette", "Fran\u00e7ois", ""]]}, {"id": "1712.00117", "submitter": "I\\~nigo Urteaga", "authors": "I\\~nigo Urteaga, David J. Albers, Marija Vlajic Wheeler, Anna Druet,\n  Hans Raffauf and No\\'emie Elhadad", "title": "Towards Personalized Modeling of the Female Hormonal Cycle: Experiments\n  with Mechanistic Models and Gaussian Processes", "comments": "Accepted at NIPS 2017 Workshop on Machine Learning for Health\n  (https://ml4health.github.io/2017/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel task for machine learning in healthcare,\nnamely personalized modeling of the female hormonal cycle. The motivation for\nthis work is to model the hormonal cycle and predict its phases in time, both\nfor healthy individuals and for those with disorders of the reproductive\nsystem. Because there are individual differences in the menstrual cycle, we are\nparticularly interested in personalized models that can account for individual\nidiosyncracies, towards identifying phenotypes of menstrual cycles. As a first\nstep, we consider the hormonal cycle as a set of observations through time. We\nuse a previously validated mechanistic model to generate realistic hormonal\npatterns, and experiment with Gaussian process regression to estimate their\nvalues over time. Specifically, we are interested in the feasibility of\npredicting menstrual cycle phases under varying learning conditions: number of\ncycles used for training, hormonal measurement noise and sampling rates, and\ninformed vs. agnostic sampling of hormonal measurements. Our results indicate\nthat Gaussian processes can help model the female menstrual cycle. We discuss\nthe implications of our experiments in the context of modeling the female\nmenstrual cycle.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 23:24:08 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Urteaga", "I\u00f1igo", ""], ["Albers", "David J.", ""], ["Wheeler", "Marija Vlajic", ""], ["Druet", "Anna", ""], ["Raffauf", "Hans", ""], ["Elhadad", "No\u00e9mie", ""]]}, {"id": "1712.00123", "submitter": "Zelun Luo", "authors": "Zelun Luo, Yuliang Zou, Judy Hoffman, Li Fei-Fei", "title": "Label Efficient Learning of Transferable Representations across Domains\n  and Tasks", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework that learns a representation transferable across\ndifferent domains and tasks in a label efficient manner. Our approach battles\ndomain shift with a domain adversarial loss, and generalizes the embedding to\nnovel task using a metric learning-based approach. Our model is simultaneously\noptimized on labeled source data and unlabeled or sparsely labeled data in the\ntarget domain. Our method shows compelling results on novel classes within a\nnew domain even when only a few labeled examples per class are available,\noutperforming the prevalent fine-tuning approach. In addition, we demonstrate\nthe effectiveness of our framework on the transfer learning task from image\nobject recognition to video action recognition.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 23:31:28 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Luo", "Zelun", ""], ["Zou", "Yuliang", ""], ["Hoffman", "Judy", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1712.00126", "submitter": "Tammo Rukat", "authors": "Tammo Rukat, Dustin Lange, C\\'edric Archambeau", "title": "An interpretable latent variable model for attribute applicability in\n  the Amazon catalogue", "comments": "Presented at NIPS 2017 Symposium on Interpretable Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning attribute applicability of products in the Amazon catalog (e.g.,\npredicting that a shoe should have a value for size, but not for battery-type\nat scale is a challenge. The need for an interpretable model is contingent on\n(1) the lack of ground truth training data, (2) the need to utilise prior\ninformation about the underlying latent space and (3) the ability to understand\nthe quality of predictions on new, unseen data. To this end, we develop the\nMaxMachine, a probabilistic latent variable model that learns distributed\nbinary representations, associated to sets of features that are likely to\nco-occur in the data. Layers of MaxMachines can be stacked such that higher\nlayers encode more abstract information. Any set of variables can be clamped to\nencode prior information. We develop fast sampling based posterior inference.\nPreliminary results show that the model improves over the baseline in 17 out of\n19 product groups and provides qualitatively reasonable predictions.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 23:36:20 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 09:14:20 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Rukat", "Tammo", ""], ["Lange", "Dustin", ""], ["Archambeau", "C\u00e9dric", ""]]}, {"id": "1712.00155", "submitter": "Muneki Yasuda", "authors": "Muneki Yasuda and Kazuyuki Tanaka", "title": "Susceptibility Propagation by Using Diagonal Consistency", "comments": null, "journal-ref": "Phys. Rev. E, Vol.87, 012134, 2013", "doi": "10.1103/PhysRevE.87.012134", "report-no": null, "categories": "cond-mat.stat-mech math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A susceptibility propagation that is constructed by combining a belief\npropagation and a linear response method is used for approximate computation\nfor Markov random fields. Herein, we formulate a new, improved susceptibility\npropagation by using the concept of a diagonal matching method that is based on\nmean-field approaches to inverse Ising problems. The proposed susceptibility\npropagation is robust for various network structures, and it is reduced to the\nordinary susceptibility propagation and to the adaptive\nThouless-Anderson-Palmer equation in special cases.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 02:16:05 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Yasuda", "Muneki", ""], ["Tanaka", "Kazuyuki", ""]]}, {"id": "1712.00164", "submitter": "Alexandre Yahi", "authors": "Alexandre Yahi, Rami Vanguri, No\\'emie Elhadad, Nicholas P. Tatonetti", "title": "Generative Adversarial Networks for Electronic Health Records: A\n  Framework for Exploring and Evaluating Methods for Predicting Drug-Induced\n  Laboratory Test Trajectories", "comments": "NIPS ML4H 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) represent a promising class of\ngenerative networks that combine neural networks with game theory. From\ngenerating realistic images and videos to assisting musical creation, GANs are\ntransforming many fields of arts and sciences. However, their application to\nhealthcare has not been fully realized, more specifically in generating\nelectronic health records (EHR) data. In this paper, we propose a framework for\nexploring the value of GANs in the context of continuous laboratory time series\ndata. We devise an unsupervised evaluation method that measures the predictive\npower of synthetic laboratory test time series. Further, we show that when it\ncomes to predicting the impact of drug exposure on laboratory test data,\nincorporating representation learning of the training cohorts prior to training\nGAN models is beneficial.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 02:33:33 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Yahi", "Alexandre", ""], ["Vanguri", "Rami", ""], ["Elhadad", "No\u00e9mie", ""], ["Tatonetti", "Nicholas P.", ""]]}, {"id": "1712.00171", "submitter": "Wenbo Zhao", "authors": "Wenbo Zhao, Yang Gao, Rita Singh", "title": "Speaker identification from the sound of the human breath", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the speaker identification potential of breath sounds in\ncontinuous speech. Speech is largely produced during exhalation. In order to\nreplenish air in the lungs, speakers must periodically inhale. When inhalation\noccurs in the midst of continuous speech, it is generally through the mouth.\nIntra-speech breathing behavior has been the subject of much study, including\nthe patterns, cadence, and variations in energy levels. However, an often\nignored characteristic is the {\\em sound} produced during the inhalation phase\nof this cycle. Intra-speech inhalation is rapid and energetic, performed with\nopen mouth and glottis, effectively exposing the entire vocal tract to enable\nmaximum intake of air. This results in vocal tract resonances evoked by\nturbulence that are characteristic of the speaker's speech-producing apparatus.\nConsequently, the sounds of inhalation are expected to carry information about\nthe speaker's identity. Moreover, unlike other spoken sounds which are subject\nto active control, inhalation sounds are generally more natural and less\naffected by voluntary influences. The goal of this paper is to demonstrate that\nbreath sounds are indeed bio-signatures that can be used to identify speakers.\nWe show that these sounds by themselves can yield remarkably accurate speaker\nrecognition with appropriate feature representations and classification\nframeworks.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 03:16:23 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 17:30:42 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Zhao", "Wenbo", ""], ["Gao", "Yang", ""], ["Singh", "Rita", ""]]}, {"id": "1712.00174", "submitter": "Chris Wu", "authors": "Chris Wu, Tanay Tandon", "title": "Rapid point-of-care Hemoglobin measurement through low-cost optics and\n  Convolutional Neural Network based validation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A low-cost, robust, and simple mechanism to measure hemoglobin would play a\ncritical role in the modern health infrastructure. Consistent sample\nacquisition has been a long-standing technical hurdle for photometer-based\nportable hemoglobin detectors which rely on micro cuvettes and dry chemistry.\nAny particulates (e.g. intact red blood cells (RBCs), microbubbles, etc.) in a\ncuvette's sensing area drastically impact optical absorption profile, and\ncommercial hemoglobinometers lack the ability to automatically detect faulty\nsamples. We present the ground-up development of a portable, low-cost and open\nplatform with equivalent accuracy to medical-grade devices, with the addition\nof CNN-based image processing for rapid sample viability prechecks. The\ndeveloped platform has demonstrated precision to the nearest $0.18[g/dL]$ of\nhemoglobin, an R^2 = 0.945 correlation to hemoglobin absorption curves reported\nin literature, and a 97% detection accuracy of poorly-prepared samples. We see\nthe developed hemoglobin device/ML platform having massive implications in\nrural medicine, and consider it an excellent springboard for robust deep\nlearning optical spectroscopy: a currently untapped source of data for\ndetection of countless analytes.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 03:37:06 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Wu", "Chris", ""], ["Tandon", "Tanay", ""]]}, {"id": "1712.00181", "submitter": "Kelly Peterson", "authors": "Kelly Peterson, Ognjen Rudovic, Ricardo Guerrero, Rosalind W. Picard", "title": "Personalized Gaussian Processes for Future Prediction of Alzheimer's\n  Disease Progression", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the use of a personalized Gaussian Process model\n(pGP) to predict the key metrics of Alzheimer's Disease progression (MMSE,\nADAS-Cog13, CDRSB and CS) based on each patient's previous visits. We start by\nlearning a population-level model using multi-modal data from previously seen\npatients using the base Gaussian Process (GP) regression. Then, this model is\nadapted sequentially over time to a new patient using domain adaptive GPs to\nform the patient's pGP. We show that this new approach, together with an\nauto-regressive formulation, leads to significant improvements in forecasting\nfuture clinical status and cognitive scores for target patients when compared\nto modeling the population with traditional GPs.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 04:05:27 GMT"}, {"version": "v2", "created": "Mon, 5 Mar 2018 20:19:25 GMT"}, {"version": "v3", "created": "Mon, 12 Mar 2018 17:31:34 GMT"}, {"version": "v4", "created": "Fri, 4 May 2018 00:39:49 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Peterson", "Kelly", ""], ["Rudovic", "Ognjen", ""], ["Guerrero", "Ricardo", ""], ["Picard", "Rosalind W.", ""]]}, {"id": "1712.00205", "submitter": "Nikos Kargas", "authors": "Nikos Kargas, Nicholas D. Sidiropoulos, Xiao Fu", "title": "Tensors, Learning, and 'Kolmogorov Extension' for Finite-alphabet Random\n  Vectors", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2018.2862383", "report-no": null, "categories": "eess.SP cs.IT math.IT math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the joint probability mass function (PMF) of a set of random\nvariables lies at the heart of statistical learning and signal processing.\nWithout structural assumptions, such as modeling the variables as a Markov\nchain, tree, or other graphical model, joint PMF estimation is often considered\nmission impossible - the number of unknowns grows exponentially with the number\nof variables. But who gives us the structural model? Is there a generic,\n`non-parametric' way to control joint PMF complexity without relying on a\npriori structural assumptions regarding the underlying probability model? Is it\npossible to discover the operational structure without biasing the analysis up\nfront? What if we only observe random subsets of the variables, can we still\nreliably estimate the joint PMF of all? This paper shows, perhaps surprisingly,\nthat if the joint PMF of any three variables can be estimated, then the joint\nPMF of all the variables can be provably recovered under relatively mild\nconditions. The result is reminiscent of Kolmogorov's extension theorem -\nconsistent specification of lower-dimensional distributions induces a unique\nprobability measure for the entire process. The difference is that for\nprocesses of limited complexity (rank of the high-dimensional PMF) it is\npossible to obtain complete characterization from only three-dimensional\ndistributions. In fact not all three-dimensional PMFs are needed; and under\nmore stringent conditions even two-dimensional will do. Exploiting multilinear\nalgebra, this paper proves that such higher-dimensional PMF completion can be\nguaranteed - several pertinent identifiability results are derived. It also\nprovides a practical and efficient algorithm to carry out the recovery task.\nJudiciously designed simulations and real-data experiments on movie\nrecommendation and data classification are presented to showcase the\neffectiveness of the approach.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 06:19:33 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 16:08:09 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Kargas", "Nikos", ""], ["Sidiropoulos", "Nicholas D.", ""], ["Fu", "Xiao", ""]]}, {"id": "1712.00232", "submitter": "Cesar A. Uribe", "authors": "C\\'esar A. Uribe and Soomin Lee and Alexander Gasnikov and Angelia\n  Nedi\\'c", "title": "Optimal Algorithms for Distributed Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.MA cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the optimal convergence rate for distributed convex\noptimization problems in networks. We model the communication restrictions\nimposed by the network as a set of affine constraints and provide optimal\ncomplexity bounds for four different setups, namely: the function $F(\\xb)\n\\triangleq \\sum_{i=1}^{m}f_i(\\xb)$ is strongly convex and smooth, either\nstrongly convex or smooth or just convex. Our results show that Nesterov's\naccelerated gradient descent on the dual problem can be executed in a\ndistributed manner and obtains the same optimal rates as in the centralized\nversion of the problem (up to constant or logarithmic factors) with an\nadditional cost related to the spectral gap of the interaction matrix. Finally,\nwe discuss some extensions to the proposed setup such as proximal friendly\nfunctions, time-varying graphs, improvement of the condition numbers.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 08:41:28 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2018 15:55:45 GMT"}, {"version": "v3", "created": "Wed, 14 Nov 2018 20:17:30 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Uribe", "C\u00e9sar A.", ""], ["Lee", "Soomin", ""], ["Gasnikov", "Alexander", ""], ["Nedi\u0107", "Angelia", ""]]}, {"id": "1712.00254", "submitter": "Tycho Tax", "authors": "Tycho Max Sylvester Tax, Jose Luis Diez Antich, Hendrik Purwins, Lars\n  Maal{\\o}e", "title": "Utilizing Domain Knowledge in End-to-End Audio Processing", "comments": "Accepted at the ML4Audio workshop at the NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end neural network based approaches to audio modelling are generally\noutperformed by models trained on high-level data representations. In this\npaper we present preliminary work that shows the feasibility of training the\nfirst layers of a deep convolutional neural network (CNN) model to learn the\ncommonly-used log-scaled mel-spectrogram transformation. Secondly, we\ndemonstrate that upon initializing the first layers of an end-to-end CNN\nclassifier with the learned transformation, convergence and performance on the\nESC-50 environmental sound classification dataset are similar to a CNN-based\nmodel trained on the highly pre-processed log-scaled mel-spectrogram features.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 09:49:21 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Tax", "Tycho Max Sylvester", ""], ["Antich", "Jose Luis Diez", ""], ["Purwins", "Hendrik", ""], ["Maal\u00f8e", "Lars", ""]]}, {"id": "1712.00269", "submitter": "Nikolay Jetchev", "authors": "Nikolay Jetchev, Urs Bergmann, Calvin Seward", "title": "GANosaic: Mosaic Creation with Generative Texture Manifolds", "comments": "31st Conference on Neural Information Processing Systems (NIPS 2017),\n  Long Beach, CA, USA. Workshop on Machine Learning for Creativity and Design", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel framework for generating texture mosaics with\nconvolutional neural networks. Our method is called GANosaic and performs\noptimization in the latent noise space of a generative texture model, which\nallows the transformation of a content image into a mosaic exhibiting the\nvisual properties of the underlying texture manifold. To represent that\nmanifold, we use a state-of-the-art generative adversarial method for texture\nsynthesis, which can learn expressive texture representations from data and\nproduce mosaic images with very high resolution. This fully convolutional model\ngenerates smooth (without any visible borders) mosaic images which morph and\nblend different textures locally. In addition, we develop a new type of\ndifferentiable statistical regularization appropriate for optimization over the\nprior noise space of the PSGAN model.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 10:35:13 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Jetchev", "Nikolay", ""], ["Bergmann", "Urs", ""], ["Seward", "Calvin", ""]]}, {"id": "1712.00287", "submitter": "Stefan Webb", "authors": "Stefan Webb, Adam Golinski, Robert Zinkov, N. Siddharth, Tom\n  Rainforth, Yee Whye Teh, Frank Wood", "title": "Faithful Inversion of Generative Models for Effective Amortized\n  Inference", "comments": "To appear at the 32nd Conference on Neural Information Processing\n  Systems (NeurIPS 2018), Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference amortization methods share information across multiple\nposterior-inference problems, allowing each to be carried out more efficiently.\nGenerally, they require the inversion of the dependency structure in the\ngenerative model, as the modeller must learn a mapping from observations to\ndistributions approximating the posterior. Previous approaches have involved\ninverting the dependency structure in a heuristic way that fails to capture\nthese dependencies correctly, thereby limiting the achievable accuracy of the\nresulting approximations. We introduce an algorithm for faithfully, and\nminimally, inverting the graphical model structure of any generative model.\nSuch inverses have two crucial properties: (a) they do not encode any\nindependence assertions that are absent from the model and; (b) they are local\nmaxima for the number of true independencies encoded. We prove the correctness\nof our approach and empirically show that the resulting minimally faithful\ninverses lead to better inference amortization than existing heuristic\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 12:08:03 GMT"}, {"version": "v2", "created": "Sat, 10 Feb 2018 18:16:17 GMT"}, {"version": "v3", "created": "Tue, 29 May 2018 15:47:44 GMT"}, {"version": "v4", "created": "Wed, 24 Oct 2018 15:58:18 GMT"}, {"version": "v5", "created": "Thu, 29 Nov 2018 14:27:47 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Webb", "Stefan", ""], ["Golinski", "Adam", ""], ["Zinkov", "Robert", ""], ["Siddharth", "N.", ""], ["Rainforth", "Tom", ""], ["Teh", "Yee Whye", ""], ["Wood", "Frank", ""]]}, {"id": "1712.00288", "submitter": "Thomas Brouwer", "authors": "Thomas Brouwer, Pietro Lio'", "title": "Prior and Likelihood Choices for Bayesian Matrix Factorisation on Small\n  Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the effects of different prior and likelihood choices\nfor Bayesian matrix factorisation, focusing on small datasets. These choices\ncan greatly influence the predictive performance of the methods. We identify\nfour groups of approaches: Gaussian-likelihood with real-valued priors,\nnonnegative priors, semi-nonnegative models, and finally Poisson-likelihood\napproaches. For each group we review several models from the literature,\nconsidering sixteen in total, and discuss the relations between different\npriors and matrix norms. We extensively compare these methods on eight\nreal-world datasets across three application areas, giving both inter- and\nintra-group comparisons. We measure convergence runtime speed, cross-validation\nperformance, sparse and noisy prediction performance, and model selection\nrobustness. We offer several insights into the trade-offs between prior and\nlikelihood choices for Bayesian matrix factorisation on small datasets - such\nas that Poisson models give poor predictions, and that nonnegative models are\nmore constrained than real-valued ones.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 12:13:35 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Brouwer", "Thomas", ""], ["Lio'", "Pietro", ""]]}, {"id": "1712.00310", "submitter": "Jakub Tomczak Ph.D.", "authors": "Jakub M. Tomczak, Maximilian Ilse, Max Welling", "title": "Deep Learning with Permutation-invariant Operator for Multi-instance\n  Histopathology Classification", "comments": "Workshop on \"Medical Imaging meets NIPS\" at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computer-aided analysis of medical scans is a longstanding goal in the\nmedical imaging field. Currently, deep learning has became a dominant\nmethodology for supporting pathologists and radiologist. Deep learning\nalgorithms have been successfully applied to digital pathology and radiology,\nnevertheless, there are still practical issues that prevent these tools to be\nwidely used in practice. The main obstacles are low number of available cases\nand large size of images (a.k.a. the small n, large p problem in machine\nlearning), and a very limited access to annotation at a pixel level that can\nlead to severe overfitting and large computational requirements. We propose to\nhandle these issues by introducing a framework that processes a medical image\nas a collection of small patches using a single, shared neural network. The\nfinal diagnosis is provided by combining scores of individual patches using a\npermutation-invariant operator (combination). In machine learning community\nsuch approach is called a multi-instance learning (MIL).\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 13:30:36 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 11:29:27 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Tomczak", "Jakub M.", ""], ["Ilse", "Maximilian", ""], ["Welling", "Max", ""]]}, {"id": "1712.00311", "submitter": "Marc Oliu", "authors": "Marc Oliu, Javier Selva, Sergio Escalera", "title": "Folded Recurrent Neural Networks for Future Video Prediction", "comments": "Submitted to European Conference on Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future video prediction is an ill-posed Computer Vision problem that recently\nreceived much attention. Its main challenges are the high variability in video\ncontent, the propagation of errors through time, and the non-specificity of the\nfuture frames: given a sequence of past frames there is a continuous\ndistribution of possible futures. This work introduces bijective Gated\nRecurrent Units, a double mapping between the input and output of a GRU layer.\nThis allows for recurrent auto-encoders with state sharing between encoder and\ndecoder, stratifying the sequence representation and helping to prevent\ncapacity problems. We show how with this topology only the encoder or decoder\nneeds to be applied for input encoding and prediction, respectively. This\nreduces the computational cost and avoids re-encoding the predictions when\ngenerating a sequence of frames, mitigating the propagation of errors.\nFurthermore, it is possible to remove layers from an already trained model,\ngiving an insight to the role performed by each layer and making the model more\nexplainable. We evaluate our approach on three video datasets, outperforming\nstate of the art prediction results on MMNIST and UCF101, and obtaining\ncompetitive results on KTH with 2 and 3 times less memory usage and\ncomputational cost than the best scored approach.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 13:31:56 GMT"}, {"version": "v2", "created": "Fri, 16 Mar 2018 15:15:24 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Oliu", "Marc", ""], ["Selva", "Javier", ""], ["Escalera", "Sergio", ""]]}, {"id": "1712.00328", "submitter": "Hongbin Pei", "authors": "Hongbin Pei, Bo Yang, Jiming Liu, Lei Dong", "title": "Group Sparse Bayesian Learning for Active Surveillance on Epidemic\n  Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting epidemic dynamics is of great value in understanding and\ncontrolling diffusion processes, such as infectious disease spread and\ninformation propagation. This task is intractable, especially when surveillance\nresources are very limited. To address the challenge, we study the problem of\nactive surveillance, i.e., how to identify a small portion of system components\nas sentinels to effect monitoring, such that the epidemic dynamics of an entire\nsystem can be readily predicted from the partial data collected by such\nsentinels. We propose a novel measure, the gamma value, to identify the\nsentinels by modeling a sentinel network with row sparsity structure. We design\na flexible group sparse Bayesian learning algorithm to mine the sentinel\nnetwork suitable for handling both linear and non-linear dynamical systems by\nusing the expectation maximization method and variational approximation. The\nefficacy of the proposed algorithm is theoretically analyzed and empirically\nvalidated using both synthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 07:09:29 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Pei", "Hongbin", ""], ["Yang", "Bo", ""], ["Liu", "Jiming", ""], ["Dong", "Lei", ""]]}, {"id": "1712.00351", "submitter": "Wilfred Ndifon", "authors": "A. M. Degoot, Faraimunashe Chirove, and Wilfred Ndifon", "title": "Trans-allelic model for prediction of peptide:MHC-II interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.BM q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Major histocompatibility complex class two (MHC-II) molecules are\ntrans-membrane proteins and key components of the cellular immune system. Upon\nrecognition of foreign peptides expressed on the MHC-II binding groove, helper\nT cells mount an immune response against invading pathogens. Therefore,\nmechanistic identification and knowledge of physico-chemical features that\ngovern interactions between peptides and MHC-II molecules is useful for the\ndesign of effective epitope-based vaccines, as well as for understanding of\nimmune responses. In this paper, we present a comprehensive trans-allelic\nprediction model, a generalized version of our previous biophysical model, that\ncan predict peptide interactions for all three human MHC-II loci (HLA-DR,\nHLA-DP and HLA-DQ), using both peptide sequence data and structural information\nof MHC-II molecules. The advantage of this approach over other machine learning\nmodels is that it offers a simple and plausible physical explanation for\npeptide-MHC-II interactions. We train the model using a benchmark experimental\ndataset, and measure its predictive performance using novel data. Despite its\nrelative simplicity, we find that the model has comparable performance to the\nstate-of-the-art method. Focusing on the physical bases of peptide-MHC binding,\nwe find support for previous theoretical predictions about the contributions of\ncertain binding pockets to the binding energy. Additionally, we find that\nbinding pockets P 4 and P 5 of HLA-DP, which were not previously considered as\nprimary anchors, do make strong contributions to the binding energy. Together,\nthe results indicate that our model can serve as a useful complement to\nalternative approaches to predicting peptide-MHC interactions.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 15:03:40 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Degoot", "A. M.", ""], ["Chirove", "Faraimunashe", ""], ["Ndifon", "Wilfred", ""]]}, {"id": "1712.00368", "submitter": "Adrien Lagrange", "authors": "Adrien Lagrange, Mathieu Fauvel, St\\'ephane May and Nicolas Dobigeon", "title": "Hierarchical Bayesian image analysis: from low-level modeling to robust\n  supervised learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within a supervised classification framework, labeled data are used to learn\nclassifier parameters. Prior to that, it is generally required to perform\ndimensionality reduction via feature extraction. These preprocessing steps have\nmotivated numerous research works aiming at recovering latent variables in an\nunsupervised context. This paper proposes a unified framework to perform\nclassification and low-level modeling jointly. The main objective is to use the\nestimated latent variables as features for classification and to incorporate\nsimultaneously supervised information to help latent variable extraction. The\nproposed hierarchical Bayesian model is divided into three stages: a first\nlow-level modeling stage to estimate latent variables, a second stage\nclustering these features into statistically homogeneous groups and a last\nclassification stage exploiting the (possibly badly) labeled data. Performance\nof the model is assessed in the specific context of hyperspectral image\ninterpretation, unifying two standard analysis techniques, namely unmixing and\nclassification.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 15:32:58 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Lagrange", "Adrien", ""], ["Fauvel", "Mathieu", ""], ["May", "St\u00e9phane", ""], ["Dobigeon", "Nicolas", ""]]}, {"id": "1712.00409", "submitter": "Joel Hestness", "authors": "Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo\n  Jun, Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, Yanqi Zhou", "title": "Deep Learning Scaling is Predictable, Empirically", "comments": "19 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) creates impactful advances following a virtuous recipe:\nmodel architecture search, creating large training data sets, and scaling\ncomputation. It is widely believed that growing training sets and models should\nimprove accuracy and result in better products. As DL application domains grow,\nwe would like a deeper understanding of the relationships between training set\nsize, computational scale, and model accuracy improvements to advance the\nstate-of-the-art.\n  This paper presents a large scale empirical characterization of\ngeneralization error and model size growth as training sets grow. We introduce\na methodology for this measurement and test four machine learning domains:\nmachine translation, language modeling, image processing, and speech\nrecognition. Our empirical results show power-law generalization error scaling\nacross a breadth of factors, resulting in power-law exponents---the \"steepness\"\nof the learning curve---yet to be explained by theoretical work. Further, model\nimprovements only shift the error but do not appear to affect the power-law\nexponent. We also show that model size scales sublinearly with data size. These\nscaling relationships have significant implications on deep learning research,\npractice, and systems. They can assist model debugging, setting accuracy\ntargets, and decisions about data set growth. They can also guide computing\nsystem design and underscore the importance of continued computational scaling.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 17:13:14 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Hestness", "Joel", ""], ["Narang", "Sharan", ""], ["Ardalani", "Newsha", ""], ["Diamos", "Gregory", ""], ["Jun", "Heewoo", ""], ["Kianinejad", "Hassan", ""], ["Patwary", "Md. Mostofa Ali", ""], ["Yang", "Yang", ""], ["Zhou", "Yanqi", ""]]}, {"id": "1712.00424", "submitter": "James Wilson", "authors": "James T. Wilson, Riccardo Moriconi, Frank Hutter, Marc Peter\n  Deisenroth", "title": "The reparameterization trick for acquisition functions", "comments": "Accepted at the NIPS 2017 Workshop on Bayesian Optimization (BayesOpt\n  2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization is a sample-efficient approach to solving global\noptimization problems. Along with a surrogate model, this approach relies on\ntheoretically motivated value heuristics (acquisition functions) to guide the\nsearch process. Maximizing acquisition functions yields the best performance;\nunfortunately, this ideal is difficult to achieve since optimizing acquisition\nfunctions per se is frequently non-trivial. This statement is especially true\nin the parallel setting, where acquisition functions are routinely non-convex,\nhigh-dimensional, and intractable. Here, we demonstrate how many popular\nacquisition functions can be formulated as Gaussian integrals amenable to the\nreparameterization trick and, ensuingly, gradient-based optimization. Further,\nwe use this reparameterized representation to derive an efficient Monte Carlo\nestimator for the upper confidence bound acquisition function in the context of\nparallel selection.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 17:32:01 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Wilson", "James T.", ""], ["Moriconi", "Riccardo", ""], ["Hutter", "Frank", ""], ["Deisenroth", "Marc Peter", ""]]}, {"id": "1712.00443", "submitter": "Aly El Gamal", "authors": "Xiaoyu Liu, Diyu Yang, Aly El Gamal", "title": "Deep Neural Network Architectures for Modulation Classification", "comments": "5 pages, 10 figures, In proc. Asilomar Conference on Signals,\n  Systems, and Computers, Nov. 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate the value of employing deep learning for the\ntask of wireless signal modulation recognition. Recently in [1], a framework\nhas been introduced by generating a dataset using GNU radio that mimics the\nimperfections in a real wireless channel, and uses 10 different modulation\ntypes. Further, a convolutional neural network (CNN) architecture was developed\nand shown to deliver performance that exceeds that of expert-based approaches.\nHere, we follow the framework of [1] and find deep neural network architectures\nthat deliver higher accuracy than the state of the art. We tested the\narchitecture of [1] and found it to achieve an accuracy of approximately 75% of\ncorrectly recognizing the modulation type. We first tune the CNN architecture\nof [1] and find a design with four convolutional layers and two dense layers\nthat gives an accuracy of approximately 83.8% at high SNR. We then develop\narchitectures based on the recently introduced ideas of Residual Networks\n(ResNet [2]) and Densely Connected Networks (DenseNet [3]) to achieve high SNR\naccuracies of approximately 83.5% and 86.6%, respectively. Finally, we\nintroduce a Convolutional Long Short-term Deep Neural Network (CLDNN [4]) to\nachieve an accuracy of approximately 88.5% at high SNR.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 18:56:37 GMT"}, {"version": "v2", "created": "Thu, 21 Dec 2017 19:58:10 GMT"}, {"version": "v3", "created": "Fri, 5 Jan 2018 12:54:51 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Liu", "Xiaoyu", ""], ["Yang", "Diyu", ""], ["Gamal", "Aly El", ""]]}, {"id": "1712.00465", "submitter": "Samaneh Nasiri Ghosheh Bolagh", "authors": "Samaneh Nasiri Ghosheh Bolagh, Gari. D. Clifford", "title": "Subject Selection on a Riemannian Manifold for Unsupervised\n  Cross-subject Seizure Detection", "comments": "Cross-Subject Learning, Inter-Subject Variability, Unsupervised\n  Subject-Selection, Riemannian Manifold, Seizure Detection, EEG, NIPS ML4H\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inter-subject variability between individuals poses a challenge in\ninter-subject brain signal analysis problems. A new algorithm for\nsubject-selection based on clustering covariance matrices on a Riemannian\nmanifold is proposed. After unsupervised selection of the subsets of relevant\nsubjects, data in a cluster is mapped to a tangent space at the mean point of\ncovariance matrices in that cluster and an SVM classifier on labeled data from\nrelevant subjects is trained. Experiment on an EEG seizure database shows that\nthe proposed method increases the accuracy over state-of-the-art from 86.83% to\n89.84% and specificity from 87.38% to 89.64% while reducing the false positive\nrate/hour from 0.8/hour to 0.77/hour.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 19:12:37 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Bolagh", "Samaneh Nasiri Ghosheh", ""], ["Clifford", "Gari. D.", ""]]}, {"id": "1712.00481", "submitter": "Hasham Ul Haq", "authors": "Hasham Ul Haq and Rameel Ahmad and Sibt Ul Hussain", "title": "Intelligent EHRs: Predicting Procedure Codes From Diagnosis Codes", "comments": "Accepted poster at NIPS 2017 Workshop on Machine Learning for Health\n  (https://ml4health.github.io/2017/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to submit a claim to insurance companies, a doctor needs to code a\npatient encounter with both the diagnosis (ICDs) and procedures performed\n(CPTs) in an Electronic Health Record (EHR). Identifying and applying relevant\nprocedures code is a cumbersome and time-consuming task as a doctor has to\nchoose from around 13,000 procedure codes with no predefined one-to-one\nmapping. In this paper, we propose a state-of-the-art deep learning method for\nautomatic and intelligent coding of procedures (CPTs) from the diagnosis codes\n(ICDs) entered by the doctor. Precisely, we cast the learning problem as a\nmulti-label classification problem and use distributed representation to learn\nthe input mapping of high-dimensional sparse ICDs codes. Our final model\ntrained on 2.3 million claims is able to outperform existing rule-based\nprobabilistic and association-rule mining based methods and has a recall of\n90@3.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 20:29:16 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Haq", "Hasham Ul", ""], ["Ahmad", "Rameel", ""], ["Hussain", "Sibt Ul", ""]]}, {"id": "1712.00497", "submitter": "Onur Ozdemir", "authors": "Onur Ozdemir, Benjamin Woodward, Andrew A. Berlin", "title": "Propagating Uncertainty in Multi-Stage Bayesian Convolutional Neural\n  Networks with Application to Pulmonary Nodule Detection", "comments": "NIPS Workshop on Bayesian Deep Learning, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the problem of computer-aided detection (CAD) of pulmonary\nnodules, we introduce methods to propagate and fuse uncertainty information in\na multi-stage Bayesian convolutional neural network (CNN) architecture. The\nquestion we seek to answer is \"can we take advantage of the model uncertainty\nprovided by one deep learning model to improve the performance of the\nsubsequent deep learning models and ultimately of the overall performance in a\nmulti-stage Bayesian deep learning architecture?\". Our experiments show that\npropagating uncertainty through the pipeline enables us to improve the overall\nperformance in terms of both final prediction accuracy and model confidence.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 21:21:35 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Ozdemir", "Onur", ""], ["Woodward", "Benjamin", ""], ["Berlin", "Andrew A.", ""]]}, {"id": "1712.00499", "submitter": "Michael Hughes", "authors": "Michael C. Hughes, Gabriel Hope, Leah Weiner, Thomas H. McCoy, Roy H.\n  Perlis, Erik B. Sudderth, Finale Doshi-Velez", "title": "Prediction-Constrained Topic Models for Antidepressant Recommendation", "comments": "Accepted poster at NIPS 2017 Workshop on Machine Learning for Health\n  (https://ml4health.github.io/2017/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervisory signals can help topic models discover low-dimensional data\nrepresentations that are more interpretable for clinical tasks. We propose a\nframework for training supervised latent Dirichlet allocation that balances two\ngoals: faithful generative explanations of high-dimensional data and accurate\nprediction of associated class labels. Existing approaches fail to balance\nthese goals by not properly handling a fundamental asymmetry: the intended task\nis always predicting labels from data, not data from labels. Our new\nprediction-constrained objective trains models that predict labels from heldout\ndata well while also producing good generative likelihoods and interpretable\ntopic-word parameters. In a case study on predicting depression medications\nfrom electronic health records, we demonstrate improved recommendations\ncompared to previous supervised topic models and high- dimensional logistic\nregression from words alone.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 21:24:26 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Hughes", "Michael C.", ""], ["Hope", "Gabriel", ""], ["Weiner", "Leah", ""], ["McCoy", "Thomas H.", ""], ["Perlis", "Roy H.", ""], ["Sudderth", "Erik B.", ""], ["Doshi-Velez", "Finale", ""]]}, {"id": "1712.00504", "submitter": "Rui Luo", "authors": "Rui Luo, Weinan Zhang, Xiaojun Xu, and Jun Wang", "title": "A Neural Stochastic Volatility Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show that the recent integration of statistical models with\ndeep recurrent neural networks provides a new way of formulating volatility\n(the degree of variation of time series) models that have been widely used in\ntime series analysis and prediction in finance. The model comprises a pair of\ncomplementary stochastic recurrent neural networks: the generative network\nmodels the joint distribution of the stochastic volatility process; the\ninference network approximates the conditional distribution of the latent\nvariables given the observables. Our focus here is on the formulation of\ntemporal dynamics of volatility over time under a stochastic recurrent neural\nnetwork framework. Experiments on real-world stock price datasets demonstrate\nthat the proposed model generates a better volatility estimation and prediction\nthat outperforms mainstream methods, e.g., deterministic models such as GARCH\nand its variants, and stochastic models namely the MCMC-based model\n\\emph{stochvol} as well as the Gaussian process volatility model \\emph{GPVol},\non average negative log-likelihood.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 16:31:36 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 00:28:03 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Luo", "Rui", ""], ["Zhang", "Weinan", ""], ["Xu", "Xiaojun", ""], ["Wang", "Jun", ""]]}, {"id": "1712.00520", "submitter": "Sunho Park", "authors": "Sunho Park and Tae Hyun Hwang", "title": "Bayesian Semi-nonnegative Tri-matrix Factorization to Identify Pathways\n  Associated with Cancer Types", "comments": "This work has been accepted to \"ML4H: Machine Learning for Health\"\n  workshop at NIPS 2017 (https://ml4health.github.io/2017/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying altered pathways that are associated with specific cancer types\ncan potentially bring a significant impact on cancer patient treatment.\nAccurate identification of such key altered pathways information can be used to\ndevelop novel therapeutic agents as well as to understand the molecular\nmechanisms of various types of cancers better. Tri-matrix factorization is an\nefficient tool to learn associations between two different entities (e.g.,\ncancer types and pathways in our case) from data. To successfully apply\ntri-matrix factorization methods to biomedical problems, biological prior\nknowledge such as pathway databases or protein-protein interaction (PPI)\nnetworks, should be taken into account in the factorization model. However, it\nis not straightforward in the Bayesian setting even though Bayesian methods are\nmore appealing than point estimate methods, such as a maximum likelihood or a\nmaximum posterior method, in the sense that they calculate distributions over\nvariables and are robust against overfitting. We propose a Bayesian\n(semi-)nonnegative matrix factorization model for human cancer genomic data,\nwhere the biological prior knowledge represented by a pathway database and a\nPPI network is taken into account in the factorization model through a finite\ndependent Beta-Bernoulli prior. We tested our method on The Cancer Genome Atlas\n(TCGA) dataset and found that the pathways identified by our method can be used\nas a prognostic biomarkers for patient subgroup identification.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 23:33:21 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Park", "Sunho", ""], ["Hwang", "Tae Hyun", ""]]}, {"id": "1712.00535", "submitter": "George Chen", "authors": "George H. Chen, Jeremy C. Weiss", "title": "Survival-Supervised Topic Modeling with Anchor Words: Characterizing\n  Pancreatitis Outcomes", "comments": "NIPS Workshop on Machine Learning for Health 2017, fixed some\n  equation typos, some minor wording edits", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new approach for topic modeling that is supervised by survival\nanalysis. Specifically, we build on recent work on unsupervised topic modeling\nwith so-called anchor words by providing supervision through an elastic-net\nregularized Cox proportional hazards model. In short, an anchor word being\npresent in a document provides strong indication that the document is partially\nabout a specific topic. For example, by seeing \"gallstones\" in a document, we\nare fairly certain that the document is partially about medicine. Our proposed\nmethod alternates between learning a topic model and learning a survival model\nto find a local minimum of a block convex optimization problem. We apply our\nproposed approach to predicting how long patients with pancreatitis admitted to\nan intensive care unit (ICU) will stay in the ICU. Our approach is as accurate\nas the best of a variety of baselines while being more interpretable than any\nof the baselines.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 01:57:35 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 06:46:10 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Chen", "George H.", ""], ["Weiss", "Jeremy C.", ""]]}, {"id": "1712.00556", "submitter": "Chen Fang", "authors": "Chen Fang, Panuwat Janwattanapong, Chunfei Li, and Malek Adjouadi", "title": "A global feature extraction model for the effective computer aided\n  diagnosis of mild cognitive impairment using structural MRI images", "comments": "4 pages, NIPS ML4H 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple modalities of biomarkers have been proved to be very sensitive in\nassessing the progression of Alzheimer's disease (AD), and using these\nmodalities and machine learning algorithms, several approaches have been\nproposed to assist in the early diagnosis of AD. Among the recent investigated\nstate-of-the-art approaches, Gaussian discriminant analysis (GDA)-based\napproaches have been demonstrated to be more effective and accurate in the\nclassification of AD, especially for delineating its prodromal stage of mild\ncognitive impairment (MCI). Moreover, among those binary classification\ninvestigations, the local feature extraction methods were mostly used, which\nmade them hardly be applied to a practical computer aided diagnosis system.\nTherefore, this study presents a novel global feature extraction model taking\nadvantage of the recent proposed GDA-based dual high-dimensional decision\nspaces, which can significantly improve the early diagnosis performance\ncomparing to those local feature extraction methods. In the true test using 20%\nheld-out data, for discriminating the most challenging MCI group from the\ncognitively normal control (CN) group, an F1 score of 91.06%, an accuracy of\n88.78%, a sensitivity of 91.80%, and a specificity of 83.78% were achieved that\ncan be considered as the best performance obtained so far.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 05:57:10 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Fang", "Chen", ""], ["Janwattanapong", "Panuwat", ""], ["Li", "Chunfei", ""], ["Adjouadi", "Malek", ""]]}, {"id": "1712.00558", "submitter": "Chanh Nguyen", "authors": "Chanh Nguyen, Georgi Georgiev, Yujie Ji, Ting Wang", "title": "Where Classification Fails, Interpretation Rises", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An intriguing property of deep neural networks is their inherent\nvulnerability to adversarial inputs, which significantly hinders their\napplication in security-critical domains. Most existing detection methods\nattempt to use carefully engineered patterns to distinguish adversarial inputs\nfrom their genuine counterparts, which however can often be circumvented by\nadaptive adversaries. In this work, we take a completely different route by\nleveraging the definition of adversarial inputs: while deceiving for deep\nneural networks, they are barely discernible for human visions. Building upon\nrecent advances in interpretable models, we construct a new detection framework\nthat contrasts an input's interpretation against its classification. We\nvalidate the efficacy of this framework through extensive experiments using\nbenchmark datasets and attacks. We believe that this work opens a new direction\nfor designing adversarial input detection methods.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 06:18:49 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Nguyen", "Chanh", ""], ["Georgiev", "Georgi", ""], ["Ji", "Yujie", ""], ["Wang", "Ting", ""]]}, {"id": "1712.00559", "submitter": "Chenxi Liu", "authors": "Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua,\n  Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, Kevin Murphy", "title": "Progressive Neural Architecture Search", "comments": "To appear in ECCV 2018 as oral. The code and checkpoint for PNASNet-5\n  trained on ImageNet (both Mobile and Large) can now be downloaded from\n  https://github.com/tensorflow/models/tree/master/research/slim#Pretrained.\n  Also see https://github.com/chenxi116/PNASNet.TF for refactored and\n  simplified TensorFlow code; see https://github.com/chenxi116/PNASNet.pytorch\n  for exact conversion to PyTorch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for learning the structure of convolutional neural\nnetworks (CNNs) that is more efficient than recent state-of-the-art methods\nbased on reinforcement learning and evolutionary algorithms. Our approach uses\na sequential model-based optimization (SMBO) strategy, in which we search for\nstructures in order of increasing complexity, while simultaneously learning a\nsurrogate model to guide the search through structure space. Direct comparison\nunder the same search space shows that our method is up to 5 times more\nefficient than the RL method of Zoph et al. (2018) in terms of number of models\nevaluated, and 8 times faster in terms of total compute. The structures we\ndiscover in this way achieve state of the art classification accuracies on\nCIFAR-10 and ImageNet.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 06:23:16 GMT"}, {"version": "v2", "created": "Sat, 24 Mar 2018 00:39:27 GMT"}, {"version": "v3", "created": "Thu, 26 Jul 2018 19:51:26 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Liu", "Chenxi", ""], ["Zoph", "Barret", ""], ["Neumann", "Maxim", ""], ["Shlens", "Jonathon", ""], ["Hua", "Wei", ""], ["Li", "Li-Jia", ""], ["Fei-Fei", "Li", ""], ["Yuille", "Alan", ""], ["Huang", "Jonathan", ""], ["Murphy", "Kevin", ""]]}, {"id": "1712.00563", "submitter": "Gabriel Erion", "authors": "Gabriel Erion, Hugh Chen, Scott M. Lundberg, Su-In Lee", "title": "Anesthesiologist-level forecasting of hypoxemia with only SpO2 data\n  using deep learning", "comments": "To be presented at Machine Learning for Health Workshop: 31st\n  Conference on Neural Information Processing Systems (NIPS 2017), Long Beach,\n  CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use a deep learning model trained only on a patient's blood oxygenation\ndata (measurable with an inexpensive fingertip sensor) to predict impending\nhypoxemia (low blood oxygen) more accurately than trained anesthesiologists\nwith access to all the data recorded in a modern operating room. We also\nprovide a simple way to visualize the reason why a patient's risk is low or\nhigh by assigning weight to the patient's past blood oxygen values. This work\nhas the potential to provide cutting-edge clinical decision support in\nlow-resource settings, where rates of surgical complication and death are\nsubstantially greater than in high-resource areas.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 07:27:28 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Erion", "Gabriel", ""], ["Chen", "Hugh", ""], ["Lundberg", "Scott M.", ""], ["Lee", "Su-In", ""]]}, {"id": "1712.00573", "submitter": "Zihao Hu", "authors": "Zihao Hu, Xiyi Luo, Hongtao Lu, and Yong Yu", "title": "Supervised Hashing based on Energy Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, supervised hashing methods have attracted much attention since they\ncan optimize retrieval speed and storage cost while preserving semantic\ninformation. Because hashing codes learning is NP-hard, many methods resort to\nsome form of relaxation technique. But the performance of these methods can\neasily deteriorate due to the relaxation. Luckily, many supervised hashing\nformulations can be viewed as energy functions, hence solving hashing codes is\nequivalent to learning marginals in the corresponding conditional random field\n(CRF). By minimizing the KL divergence between a fully factorized distribution\nand the Gibbs distribution of this CRF, a set of consistency equations can be\nobtained, but updating them in parallel may not yield a local optimum since the\nvariational lower bound is not guaranteed to increase. In this paper, we use a\nlinear approximation of the sigmoid function to convert these consistency\nequations to linear systems, which have a closed-form solution. By applying\nthis novel technique to two classical hashing formulations KSH and SPLH, we\nobtain two new methods called EM (energy minimizing based)-KSH and EM-SPLH.\nExperimental results on three datasets show the superiority of our methods.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 08:43:23 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Hu", "Zihao", ""], ["Luo", "Xiyi", ""], ["Lu", "Hongtao", ""], ["Yu", "Yong", ""]]}, {"id": "1712.00644", "submitter": "Maggie Makar", "authors": "Maggie Makar, Marzyeh Ghassemi, David Cutler, Ziad Obermeyer", "title": "Short-term Mortality Prediction for Elderly Patients Using Medicare\n  Claims Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Risk prediction is central to both clinical medicine and public health. While\nmany machine learning models have been developed to predict mortality, they are\nrarely applied in the clinical literature, where classification tasks typically\nrely on logistic regression. One reason for this is that existing machine\nlearning models often seek to optimize predictions by incorporating features\nthat are not present in the databases readily available to providers and policy\nmakers, limiting generalizability and implementation. Here we tested a number\nof machine learning classifiers for prediction of six-month mortality in a\npopulation of elderly Medicare beneficiaries, using an administrative claims\ndatabase of the kind available to the majority of health care payers and\nproviders. We show that machine learning classifiers substantially outperform\ncurrent widely-used methods of risk prediction but only when used with an\nimproved feature set incorporating insights from clinical medicine, developed\nfor this study. Our work has applications to supporting patient and provider\ndecision making at the end of life, as well as population health-oriented\nefforts to identify patients at high risk of poor outcomes.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 17:35:40 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Makar", "Maggie", ""], ["Ghassemi", "Marzyeh", ""], ["Cutler", "David", ""], ["Obermeyer", "Ziad", ""]]}, {"id": "1712.00673", "submitter": "Xuanqing Liu", "authors": "Xuanqing Liu, Minhao Cheng, Huan Zhang, Cho-Jui Hsieh", "title": "Towards Robust Neural Networks via Random Self-ensemble", "comments": "ECCV 2018 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have revealed the vulnerability of deep neural networks: A\nsmall adversarial perturbation that is imperceptible to human can easily make a\nwell-trained deep neural network misclassify. This makes it unsafe to apply\nneural networks in security-critical applications. In this paper, we propose a\nnew defense algorithm called Random Self-Ensemble (RSE) by combining two\nimportant concepts: {\\bf randomness} and {\\bf ensemble}. To protect a targeted\nmodel, RSE adds random noise layers to the neural network to prevent the strong\ngradient-based attacks, and ensembles the prediction over random noises to\nstabilize the performance. We show that our algorithm is equivalent to ensemble\nan infinite number of noisy models $f_\\epsilon$ without any additional memory\noverhead, and the proposed training procedure based on noisy stochastic\ngradient descent can ensure the ensemble model has a good predictive\ncapability. Our algorithm significantly outperforms previous defense techniques\non real data sets. For instance, on CIFAR-10 with VGG network (which has 92\\%\naccuracy without any attack), under the strong C\\&W attack within a certain\ndistortion tolerance, the accuracy of unprotected model drops to less than\n10\\%, the best previous defense technique has $48\\%$ accuracy, while our method\nstill has $86\\%$ prediction accuracy under the same level of attack. Finally,\nour method is simple and easy to integrate into any neural network.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 22:26:12 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2018 00:44:31 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Liu", "Xuanqing", ""], ["Cheng", "Minhao", ""], ["Zhang", "Huan", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "1712.00679", "submitter": "Jose Daniel Gallego Posada", "authors": "Frans A. Oliehoek, Rahul Savani, Jose Gallego-Posada, Elise van der\n  Pol, Edwin D. de Jong and Roderich Gross", "title": "GANGs: Generative Adversarial Network Games", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GAN) have become one of the most successful\nframeworks for unsupervised generative modeling. As GANs are difficult to train\nmuch research has focused on this. However, very little of this research has\ndirectly exploited game-theoretic techniques. We introduce Generative\nAdversarial Network Games (GANGs), which explicitly model a finite zero-sum\ngame between a generator ($G$) and classifier ($C$) that use mixed strategies.\nThe size of these games precludes exact solution methods, therefore we define\nresource-bounded best responses (RBBRs), and a resource-bounded Nash\nEquilibrium (RB-NE) as a pair of mixed strategies such that neither $G$ or $C$\ncan find a better RBBR. The RB-NE solution concept is richer than the notion of\n`local Nash equilibria' in that it captures not only failures of escaping local\noptima of gradient descent, but applies to any approximate best response\ncomputations, including methods with random restarts. To validate our approach,\nwe solve GANGs with the Parallel Nash Memory algorithm, which provably\nmonotonically converges to an RB-NE. We compare our results to standard GAN\nsetups, and demonstrate that our method deals well with typical GAN problems\nsuch as mode collapse, partial mode coverage and forgetting.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 23:38:03 GMT"}, {"version": "v2", "created": "Sun, 17 Dec 2017 21:34:19 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Oliehoek", "Frans A.", ""], ["Savani", "Rahul", ""], ["Gallego-Posada", "Jose", ""], ["van der Pol", "Elise", ""], ["de Jong", "Edwin D.", ""], ["Gross", "Roderich", ""]]}, {"id": "1712.00699", "submitter": "Rajeev Ranjan", "authors": "Rajeev Ranjan, Swami Sankaranarayanan, Carlos D. Castillo and Rama\n  Chellappa", "title": "Improving Network Robustness against Adversarial Attacks with Compact\n  Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though Convolutional Neural Networks (CNNs) have surpassed human-level\nperformance on tasks such as object classification and face verification, they\ncan easily be fooled by adversarial attacks. These attacks add a small\nperturbation to the input image that causes the network to misclassify the\nsample. In this paper, we focus on neutralizing adversarial attacks by compact\nfeature learning. In particular, we show that learning features in a closed and\nbounded space improves the robustness of the network. We explore the effect of\nL2-Softmax Loss, that enforces compactness in the learned features, thus\nresulting in enhanced robustness to adversarial perturbations. Additionally, we\npropose compact convolution, a novel method of convolution that when\nincorporated in conventional CNNs improves their robustness. Compact\nconvolution ensures feature compactness at every layer such that they are\nbounded and close to each other. Extensive experiments show that Compact\nConvolutional Networks (CCNs) neutralize multiple types of attacks, and perform\nbetter than existing methods in defending adversarial attacks, without\nincurring any additional training overhead compared to CNNs.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 03:09:31 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2018 21:45:44 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Ranjan", "Rajeev", ""], ["Sankaranarayanan", "Swami", ""], ["Castillo", "Carlos D.", ""], ["Chellappa", "Rama", ""]]}, {"id": "1712.00716", "submitter": "Qing Qu", "authors": "Qing Qu, Yuqian Zhang, Yonina C. Eldar, and John Wright", "title": "Convolutional Phase Retrieval via Gradient Descent", "comments": "64 pages , 9 figures, appeared in NeurIPS 2017. Accepted at IEEE\n  Transactions on Information Theory. This is the final (minor) update: fixed\n  typos and grammar issues", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.IT cs.NA math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the convolutional phase retrieval problem, of recovering an unknown\nsignal $\\mathbf x \\in \\mathbb C^n $ from $m$ measurements consisting of the\nmagnitude of its cyclic convolution with a given kernel $\\mathbf a \\in \\mathbb\nC^m $. This model is motivated by applications such as channel estimation,\noptics, and underwater acoustic communication, where the signal of interest is\nacted on by a given channel/filter, and phase information is difficult or\nimpossible to acquire. We show that when $\\mathbf a$ is random and the number\nof observations $m$ is sufficiently large, with high probability $\\mathbf x$\ncan be efficiently recovered up to a global phase shift using a combination of\nspectral initialization and generalized gradient descent. The main challenge is\ncoping with dependencies in the measurement operator. We overcome this\nchallenge by using ideas from decoupling theory, suprema of chaos processes and\nthe restricted isometry property of random circulant matrices, and recent\nanalysis of alternating minimization methods.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 06:04:25 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2019 03:30:06 GMT"}, {"version": "v3", "created": "Sun, 6 Oct 2019 02:55:12 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Qu", "Qing", ""], ["Zhang", "Yuqian", ""], ["Eldar", "Yonina C.", ""], ["Wright", "John", ""]]}, {"id": "1712.00725", "submitter": "Abhinav Gupta", "authors": "Laura Graesser, Abhinav Gupta, Lakshay Sharma, Evelina Bakhturina", "title": "Sentiment Classification using Images and Label Embeddings", "comments": "13 pages, 3 figures, 9 tables. Technical report for Statistical\n  Natural Language Processing Project (NYU CS - Fall 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this project we analysed how much semantic information images carry, and\nhow much value image data can add to sentiment analysis of the text associated\nwith the images. To better understand the contribution from images, we compared\nmodels which only made use of image data, models which only made use of text\ndata, and models which combined both data types. We also analysed if this\napproach could help sentiment classifiers generalize to unknown sentiments.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 07:20:15 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Graesser", "Laura", ""], ["Gupta", "Abhinav", ""], ["Sharma", "Lakshay", ""], ["Bakhturina", "Evelina", ""]]}, {"id": "1712.00731", "submitter": "Hongwei Wang", "authors": "Hongwei Wang, Jia Wang, Miao Zhao, Jiannong Cao, Minyi Guo", "title": "Joint Topic-Semantic-aware Social Recommendation for Online Voting", "comments": "The 26th ACM International Conference on Information and Knowledge\n  Management (CIKM 2017)", "journal-ref": null, "doi": "10.1145/3132847.3132889", "report-no": null, "categories": "stat.ML cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online voting is an emerging feature in social networks, in which users can\nexpress their attitudes toward various issues and show their unique interest.\nOnline voting imposes new challenges on recommendation, because the propagation\nof votings heavily depends on the structure of social networks as well as the\ncontent of votings. In this paper, we investigate how to utilize these two\nfactors in a comprehensive manner when doing voting recommendation. First, due\nto the fact that existing text mining methods such as topic model and semantic\nmodel cannot well process the content of votings that is typically short and\nambiguous, we propose a novel Topic-Enhanced Word Embedding (TEWE) method to\nlearn word and document representation by jointly considering their topics and\nsemantics. Then we propose our Joint Topic-Semantic-aware social Matrix\nFactorization (JTS-MF) model for voting recommendation. JTS-MF model calculates\nsimilarity among users and votings by combining their TEWE representation and\nstructural information of social networks, and preserves this\ntopic-semantic-social similarity during matrix factorization. To evaluate the\nperformance of TEWE representation and JTS-MF model, we conduct extensive\nexperiments on real online voting dataset. The results prove the efficacy of\nour approach against several state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 08:11:43 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Wang", "Hongwei", ""], ["Wang", "Jia", ""], ["Zhao", "Miao", ""], ["Cao", "Jiannong", ""], ["Guo", "Minyi", ""]]}, {"id": "1712.00732", "submitter": "Hongwei Wang", "authors": "Hongwei Wang, Fuzheng Zhang, Min Hou, Xing Xie, Minyi Guo, Qi Liu", "title": "SHINE: Signed Heterogeneous Information Network Embedding for Sentiment\n  Link Prediction", "comments": "The 11th ACM International Conference on Web Search and Data Mining\n  (WSDM 2018)", "journal-ref": null, "doi": "10.1145/3159652.3159666", "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In online social networks people often express attitudes towards others,\nwhich forms massive sentiment links among users. Predicting the sign of\nsentiment links is a fundamental task in many areas such as personal\nadvertising and public opinion analysis. Previous works mainly focus on textual\nsentiment classification, however, text information can only disclose the \"tip\nof the iceberg\" about users' true opinions, of which the most are unobserved\nbut implied by other sources of information such as social relation and users'\nprofile. To address this problem, in this paper we investigate how to predict\npossibly existing sentiment links in the presence of heterogeneous information.\nFirst, due to the lack of explicit sentiment links in mainstream social\nnetworks, we establish a labeled heterogeneous sentiment dataset which consists\nof users' sentiment relation, social relation and profile knowledge by\nentity-level sentiment extraction method. Then we propose a novel and flexible\nend-to-end Signed Heterogeneous Information Network Embedding (SHINE) framework\nto extract users' latent representations from heterogeneous networks and\npredict the sign of unobserved sentiment links. SHINE utilizes multiple deep\nautoencoders to map each user into a low-dimension feature space while\npreserving the network structure. We demonstrate the superiority of SHINE over\nstate-of-the-art baselines on link prediction and node recommendation in two\nreal-world datasets. The experimental results also prove the efficacy of SHINE\nin cold start scenario.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 08:21:31 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Wang", "Hongwei", ""], ["Zhang", "Fuzheng", ""], ["Hou", "Min", ""], ["Xie", "Xing", ""], ["Guo", "Minyi", ""], ["Liu", "Qi", ""]]}, {"id": "1712.00779", "submitter": "Simon Du", "authors": "Simon S. Du, Jason D. Lee, Yuandong Tian, Barnabas Poczos, Aarti Singh", "title": "Gradient Descent Learns One-hidden-layer CNN: Don't be Afraid of\n  Spurious Local Minima", "comments": "Accepted by ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a one-hidden-layer neural network with\nnon-overlapping convolutional layer and ReLU activation, i.e., $f(\\mathbf{Z},\n\\mathbf{w}, \\mathbf{a}) = \\sum_j a_j\\sigma(\\mathbf{w}^T\\mathbf{Z}_j)$, in which\nboth the convolutional weights $\\mathbf{w}$ and the output weights $\\mathbf{a}$\nare parameters to be learned. When the labels are the outputs from a teacher\nnetwork of the same architecture with fixed weights $(\\mathbf{w}^*,\n\\mathbf{a}^*)$, we prove that with Gaussian input $\\mathbf{Z}$, there is a\nspurious local minimizer. Surprisingly, in the presence of the spurious local\nminimizer, gradient descent with weight normalization from randomly initialized\nweights can still be proven to recover the true parameters with constant\nprobability, which can be boosted to probability $1$ with multiple restarts. We\nalso show that with constant probability, the same procedure could also\nconverge to the spurious local minimum, showing that the local minimum plays a\nnon-trivial role in the dynamics of gradient descent. Furthermore, a\nquantitative analysis shows that the gradient descent dynamics has two phases:\nit starts off slow, but converges much faster after several iterations.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 15:00:35 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 00:41:03 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Du", "Simon S.", ""], ["Lee", "Jason D.", ""], ["Tian", "Yuandong", ""], ["Poczos", "Barnabas", ""], ["Singh", "Aarti", ""]]}, {"id": "1712.00828", "submitter": "Vaneet Aggarwal", "authors": "Wenqi Wang and Vaneet Aggarwal and Shuchin Aeron", "title": "Tensor Train Neighborhood Preserving Embedding", "comments": "Accepted to IEEE Transactions on Signal Processing, Mar 2018", "journal-ref": null, "doi": "10.1109/TSP.2018.2816568", "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a Tensor Train Neighborhood Preserving Embedding\n(TTNPE) to embed multi-dimensional tensor data into low dimensional tensor\nsubspace. Novel approaches to solve the optimization problem in TTNPE are\nproposed. For this embedding, we evaluate novel trade-off gain among\nclassification, computation, and dimensionality reduction (storage) for\nsupervised learning. It is shown that compared to the state-of-the-arts tensor\nembedding methods, TTNPE achieves superior trade-off in classification,\ncomputation, and dimensionality reduction in MNIST handwritten digits and\nWeizmann face datasets.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 20:09:48 GMT"}, {"version": "v2", "created": "Sat, 10 Mar 2018 01:11:03 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Wang", "Wenqi", ""], ["Aggarwal", "Vaneet", ""], ["Aeron", "Shuchin", ""]]}, {"id": "1712.00867", "submitter": "Matthew Kupilik", "authors": "Matthew Kupilik, Frank Witmer, Euan-Angus MacLeod, Caixia Wang, Tom\n  Ravens", "title": "Gaussian Process Regression for Arctic Coastal Erosion Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arctic coastal morphology is governed by multiple factors, many of which are\naffected by climatological changes. As the season length for shorefast ice\ndecreases and temperatures warm permafrost soils, coastlines are more\nsusceptible to erosion from storm waves. Such coastal erosion is a concern,\nsince the majority of the population centers and infrastructure in the Arctic\nare located near the coasts. Stakeholders and decision makers increasingly need\nmodels capable of scenario-based predictions to assess and mitigate the effects\nof coastal morphology on infrastructure and land use. Our research uses\nGaussian process models to forecast Arctic coastal erosion along the Beaufort\nSea near Drew Point, AK. Gaussian process regression is a data-driven modeling\nmethodology capable of extracting patterns and trends from data-sparse\nenvironments such as remote Arctic coastlines. To train our model, we use\nannual coastline positions and near-shore summer temperature averages from\nexisting datasets and extend these data by extracting additional coastlines\nfrom satellite imagery. We combine our calibrated models with future climate\nmodels to generate a range of plausible future erosion scenarios. Our results\nshow that the Gaussian process methodology substantially improves yearly\npredictions compared to linear and nonlinear least squares methods, and is\ncapable of generating detailed forecasts suitable for use by decision makers.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 01:01:39 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Kupilik", "Matthew", ""], ["Witmer", "Frank", ""], ["MacLeod", "Euan-Angus", ""], ["Wang", "Caixia", ""], ["Ravens", "Tom", ""]]}, {"id": "1712.00891", "submitter": "Mostafa Rahmani", "authors": "Mostafa Rahmani, George Atia", "title": "Data Dropout in Arbitrary Basis for Deep Network Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important problem in training deep networks with high capacity is to\nensure that the trained network works well when presented with new inputs\noutside the training dataset. Dropout is an effective regularization technique\nto boost the network generalization in which a random subset of the elements of\nthe given data and the extracted features are set to zero during the training\nprocess. In this paper, a new randomized regularization technique in which we\nwithhold a random part of the data without necessarily turning off the\nneurons/data-elements is proposed. In the proposed method, of which the\nconventional dropout is shown to be a special case, random data dropout is\nperformed in an arbitrary basis, hence the designation Generalized Dropout. We\nalso present a framework whereby the proposed technique can be applied\nefficiently to convolutional neural networks. The presented numerical\nexperiments demonstrate that the proposed technique yields notable performance\ngain. Generalized Dropout provides new insight into the idea of dropout, shows\nthat we can achieve different performance gains by using different bases\nmatrices, and opens up a new research question as of how to choose optimal\nbases matrices that achieve maximal performance gain.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 03:29:38 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 02:55:21 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Rahmani", "Mostafa", ""], ["Atia", "George", ""]]}, {"id": "1712.00912", "submitter": "Jong Chul Ye", "authors": "Jaejun Yoo, Sohail Sabir, Duchang Heo, Kee Hyun Kim, Abdul Wahab,\n  Yoonseok Choi, Seul-I Lee, Eun Young Chae, Hak Hee Kim, Young Min Bae,\n  Young-wook Choi, Seungryong Cho, and Jong Chul Ye", "title": "Deep Learning Diffuse Optical Tomography", "comments": "Accepted for IEEE Trans. on Medical Imaging", "journal-ref": null, "doi": "10.1109/TMI.2019.2936522", "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffuse optical tomography (DOT) has been investigated as an alternative\nimaging modality for breast cancer detection thanks to its excellent contrast\nto hemoglobin oxidization level. However, due to the complicated non-linear\nphoton scattering physics and ill-posedness, the conventional reconstruction\nalgorithms are sensitive to imaging parameters such as boundary conditions. To\naddress this, here we propose a novel deep learning approach that learns\nnon-linear photon scattering physics and obtains an accurate three dimensional\n(3D) distribution of optical anomalies. In contrast to the traditional\nblack-box deep learning approaches, our deep network is designed to invert the\nLippman-Schwinger integral equation using the recent mathematical theory of\ndeep convolutional framelets. As an example of clinical relevance, we applied\nthe method to our prototype DOT system. We show that our deep neural network,\ntrained with only simulation data, can accurately recover the location of\nanomalies within biomimetic phantoms and live animals without the use of an\nexogenous contrast agent.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 05:47:10 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 03:46:33 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Yoo", "Jaejun", ""], ["Sabir", "Sohail", ""], ["Heo", "Duchang", ""], ["Kim", "Kee Hyun", ""], ["Wahab", "Abdul", ""], ["Choi", "Yoonseok", ""], ["Lee", "Seul-I", ""], ["Chae", "Eun Young", ""], ["Kim", "Hak Hee", ""], ["Bae", "Young Min", ""], ["Choi", "Young-wook", ""], ["Cho", "Seungryong", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1712.00961", "submitter": "Niki Kilbertus", "authors": "Giambattista Parascandolo, Niki Kilbertus, Mateo Rojas-Carulla,\n  Bernhard Sch\\\"olkopf", "title": "Learning Independent Causal Mechanisms", "comments": "ICML 2018", "journal-ref": "Proceedings of the 35th International Conference on Machine\n  Learning, PMLR 80:4036-4044, 2018", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical learning relies upon data sampled from a distribution, and we\nusually do not care what actually generated it in the first place. From the\npoint of view of causal modeling, the structure of each distribution is induced\nby physical mechanisms that give rise to dependences between observables.\nMechanisms, however, can be meaningful autonomous modules of generative models\nthat make sense beyond a particular entailed data distribution, lending\nthemselves to transfer between problems. We develop an algorithm to recover a\nset of independent (inverse) mechanisms from a set of transformed data points.\nThe approach is unsupervised and based on a set of experts that compete for\ndata generated by the mechanisms, driving specialization. We analyze the\nproposed method in a series of experiments on image data. Each expert learns to\nmap a subset of the transformed data back to a reference distribution. The\nlearned mechanisms generalize to novel domains. We discuss implications for\ntransfer learning and links to recent trends in generative modeling.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 09:06:15 GMT"}, {"version": "v2", "created": "Sat, 9 Dec 2017 00:34:00 GMT"}, {"version": "v3", "created": "Sun, 21 Jan 2018 15:44:45 GMT"}, {"version": "v4", "created": "Mon, 19 Feb 2018 18:28:56 GMT"}, {"version": "v5", "created": "Sat, 8 Sep 2018 08:17:44 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Parascandolo", "Giambattista", ""], ["Kilbertus", "Niki", ""], ["Rojas-Carulla", "Mateo", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1712.00996", "submitter": "Giovanni Montana", "authors": "Emanuele Pesce, Petros-Pavlos Ypsilantis, Samuel Withey, Robert\n  Bakewell, Vicky Goh, Giovanni Montana", "title": "Learning to detect chest radiographs containing lung nodules using\n  visual attention networks", "comments": null, "journal-ref": "Medical Image Analysis, Vol. 53, pag. 26-38, 2019", "doi": "10.1016/j.media.2018.12.007", "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning approaches hold great potential for the automated detection\nof lung nodules in chest radiographs, but training the algorithms requires vary\nlarge amounts of manually annotated images, which are difficult to obtain. Weak\nlabels indicating whether a radiograph is likely to contain pulmonary nodules\nare typically easier to obtain at scale by parsing historical free-text\nradiological reports associated to the radiographs. Using a repositotory of\nover 700,000 chest radiographs, in this study we demonstrate that promising\nnodule detection performance can be achieved using weak labels through\nconvolutional neural networks for radiograph classification. We propose two\nnetwork architectures for the classification of images likely to contain\npulmonary nodules using both weak labels and manually-delineated bounding\nboxes, when these are available. Annotated nodules are used at training time to\ndeliver a visual attention mechanism informing the model about its localisation\nperformance. The first architecture extracts saliency maps from high-level\nconvolutional layers and compares the estimated position of a nodule against\nthe ground truth, when this is available. A corresponding localisation error is\nthen back-propagated along with the softmax classification error. The second\napproach consists of a recurrent attention model that learns to observe a short\nsequence of smaller image portions through reinforcement learning. When a\nnodule annotation is available at training time, the reward function is\nmodified accordingly so that exploring portions of the radiographs away from a\nnodule incurs a larger penalty. Our empirical results demonstrate the potential\nadvantages of these architectures in comparison to competing methodologies.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 10:44:32 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 19:48:02 GMT"}, {"version": "v3", "created": "Thu, 7 Feb 2019 10:52:39 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Pesce", "Emanuele", ""], ["Ypsilantis", "Petros-Pavlos", ""], ["Withey", "Samuel", ""], ["Bakewell", "Robert", ""], ["Goh", "Vicky", ""], ["Montana", "Giovanni", ""]]}, {"id": "1712.01033", "submitter": "Tianbao Yang", "authors": "Yi Xu, Rong Jin, Tianbao Yang", "title": "NEON+: Accelerated Gradient Methods for Extracting Negative Curvature\n  for Non-Convex Optimization", "comments": "The main result is merged into our manuscript \"First-order Stochastic\n  Algorithms for Escaping From Saddle Points in Almost Linear Time\"\n  (arXiv:1711.01944)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerated gradient (AG) methods are breakthroughs in convex optimization,\nimproving the convergence rate of the gradient descent method for optimization\nwith smooth functions. However, the analysis of AG methods for non-convex\noptimization is still limited. It remains an open question whether AG methods\nfrom convex optimization can accelerate the convergence of the gradient descent\nmethod for finding local minimum of non-convex optimization problems. This\npaper provides an affirmative answer to this question. In particular, we\nanalyze two renowned variants of AG methods (namely Polyak's Heavy Ball method\nand Nesterov's Accelerated Gradient method) for extracting the negative\ncurvature from random noise, which is central to escaping from saddle points.\nBy leveraging the proposed AG methods for extracting the negative curvature, we\npresent a new AG algorithm with double loops for non-convex\noptimization~\\footnote{this is in contrast to a single-loop AG algorithm\nproposed in a recent manuscript~\\citep{AGNON}, which directly analyzed the\nNesterov's AG method for non-convex optimization and appeared online on\nNovember 29, 2017. However, we emphasize that our work is an independent work,\nwhich is inspired by our earlier work~\\citep{NEON17} and is based on a\ndifferent novel analysis.}, which converges to second-order stationary point\n$\\x$ such that $\\|\\nabla f(\\x)\\|\\leq \\epsilon$ and $\\nabla^2 f(\\x)\\geq\n-\\sqrt{\\epsilon} I$ with $\\widetilde O(1/\\epsilon^{1.75})$ iteration\ncomplexity, improving that of gradient descent method by a factor of\n$\\epsilon^{-0.25}$ and matching the best iteration complexity of second-order\nHessian-free methods for non-convex optimization.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 12:17:19 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 22:46:43 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Xu", "Yi", ""], ["Jin", "Rong", ""], ["Yang", "Tianbao", ""]]}, {"id": "1712.01038", "submitter": "Mohammad Emtiyaz Khan", "authors": "Mohammad Emtiyaz Khan, Zuozhu Liu, Voot Tangkaratt, Yarin Gal", "title": "Vprop: Variational Inference using RMSprop", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computationally-efficient methods for Bayesian deep learning rely on\ncontinuous optimization algorithms, but the implementation of these methods\nrequires significant changes to existing code-bases. In this paper, we propose\nVprop, a method for Gaussian variational inference that can be implemented with\ntwo minor changes to the off-the-shelf RMSprop optimizer. Vprop also reduces\nthe memory requirements of Black-Box Variational Inference by half. We derive\nVprop using the conjugate-computation variational inference method, and\nestablish its connections to Newton's method, natural-gradient methods, and\nextended Kalman filters. Overall, this paper presents Vprop as a principled,\ncomputationally-efficient, and easy-to-implement method for Bayesian deep\nlearning.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 12:31:36 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Khan", "Mohammad Emtiyaz", ""], ["Liu", "Zuozhu", ""], ["Tangkaratt", "Voot", ""], ["Gal", "Yarin", ""]]}, {"id": "1712.01048", "submitter": "Yiren Zhou", "authors": "Yiren Zhou, Seyed-Mohsen Moosavi-Dezfooli, Ngai-Man Cheung, Pascal\n  Frossard", "title": "Adaptive Quantization for Deep Neural Network", "comments": "9 pages main paper + 5 pages supplementary, 8 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years Deep Neural Networks (DNNs) have been rapidly developed in\nvarious applications, together with increasingly complex architectures. The\nperformance gain of these DNNs generally comes with high computational costs\nand large memory consumption, which may not be affordable for mobile platforms.\nDeep model quantization can be used for reducing the computation and memory\ncosts of DNNs, and deploying complex DNNs on mobile equipment. In this work, we\npropose an optimization framework for deep model quantization. First, we\npropose a measurement to estimate the effect of parameter quantization errors\nin individual layers on the overall model prediction accuracy. Then, we propose\nan optimization process based on this measurement for finding optimal\nquantization bit-width for each layer. This is the first work that\ntheoretically analyse the relationship between parameter quantization errors of\nindividual layers and model accuracy. Our new quantization algorithm\noutperforms previous quantization optimization methods, and achieves 20-40%\nhigher compression rate compared to equal bit-width quantization at the same\nmodel prediction accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 12:56:33 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Zhou", "Yiren", ""], ["Moosavi-Dezfooli", "Seyed-Mohsen", ""], ["Cheung", "Ngai-Man", ""], ["Frossard", "Pascal", ""]]}, {"id": "1712.01076", "submitter": "Yann Ollivier", "authors": "Ga\\'etan Marceau-Caron and Yann Ollivier", "title": "Natural Langevin Dynamics for Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One way to avoid overfitting in machine learning is to use model parameters\ndistributed according to a Bayesian posterior given the data, rather than the\nmaximum likelihood estimator. Stochastic gradient Langevin dynamics (SGLD) is\none algorithm to approximate such Bayesian posteriors for large models and\ndatasets. SGLD is a standard stochastic gradient descent to which is added a\ncontrolled amount of noise, specifically scaled so that the parameter converges\nin law to the posterior distribution [WT11, TTV16]. The posterior predictive\ndistribution can be approximated by an ensemble of samples from the trajectory.\n  Choice of the variance of the noise is known to impact the practical behavior\nof SGLD: for instance, noise should be smaller for sensitive parameter\ndirections. Theoretically, it has been suggested to use the inverse Fisher\ninformation matrix of the model as the variance of the noise, since it is also\nthe variance of the Bayesian posterior [PT13, AKW12, GC11]. But the Fisher\nmatrix is costly to compute for large- dimensional models.\n  Here we use the easily computed Fisher matrix approximations for deep neural\nnetworks from [MO16, Oll15]. The resulting natural Langevin dynamics combines\nthe advantages of Amari's natural gradient descent and Fisher-preconditioned\nLangevin dynamics for large neural networks.\n  Small-scale experiments on MNIST show that Fisher matrix preconditioning\nbrings SGLD close to dropout as a regularizing technique.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 13:54:45 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Marceau-Caron", "Ga\u00e9tan", ""], ["Ollivier", "Yann", ""]]}, {"id": "1712.01081", "submitter": "Muhammad Khan", "authors": "Muhammad Raza Khan, Joshua Blumenstock", "title": "Determinants of Mobile Money Adoption in Pakistan", "comments": "Presented at NIPS 2017 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we analyze the problem of adoption of mobile money in Pakistan\nby using the call detail records of a major telecom company as our input. Our\nresults highlight the fact that different sections of the society have\ndifferent patterns of adoption of digital financial services but user mobility\nrelated features are the most important one when it comes to adopting and using\nmobile money services.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 23:02:03 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Khan", "Muhammad Raza", ""], ["Blumenstock", "Joshua", ""]]}, {"id": "1712.01137", "submitter": "Dieter Hendricks", "authors": "Dieter Hendricks, Adam Cobb, Richard Everett, Jonathan Downing and\n  Stephen J. Roberts", "title": "Inferring agent objectives at different scales of a complex adaptive\n  system", "comments": "6 pages, 3 figures, NIPS 2017 Workshop on Learning in the Presence of\n  Strategic Behaviour (MLStrat)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework to study the effective objectives at different time\nscales of financial market microstructure. The financial market can be regarded\nas a complex adaptive system, where purposeful agents collectively and\nsimultaneously create and perceive their environment as they interact with it.\nIt has been suggested that multiple agent classes operate in this system, with\na non-trivial hierarchy of top-down and bottom-up causation classes with\ndifferent effective models governing each level. We conjecture that agent\nclasses may in fact operate at different time scales and thus act differently\nin response to the same perceived market state. Given scale-specific temporal\nstate trajectories and action sequences estimated from aggregate market\nbehaviour, we use Inverse Reinforcement Learning to compute the effective\nreward function for the aggregate agent class at each scale, allowing us to\nassess the relative attractiveness of feature vectors across different scales.\nDifferences in reward functions for feature vectors may indicate different\nobjectives of market participants, which could assist in finding the scale\nboundary for agent classes. This has implications for learning algorithms\noperating in this domain.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 15:06:15 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Hendricks", "Dieter", ""], ["Cobb", "Adam", ""], ["Everett", "Richard", ""], ["Downing", "Jonathan", ""], ["Roberts", "Stephen J.", ""]]}, {"id": "1712.01141", "submitter": "Abdul-Saboor Sheikh", "authors": "Abdul-Saboor Sheikh, Kashif Rasul, Andreas Merentitis, Urs Bergmann", "title": "Stochastic Maximum Likelihood Optimization via Hypernetworks", "comments": "To appear at NIPS 2017 Workshop on Bayesian Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work explores maximum likelihood optimization of neural networks through\nhypernetworks. A hypernetwork initializes the weights of another network, which\nin turn can be employed for typical functional tasks such as regression and\nclassification. We optimize hypernetworks to directly maximize the conditional\nlikelihood of target variables given input. Using this approach we obtain\ncompetitive empirical results on regression and classification benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 15:17:27 GMT"}, {"version": "v2", "created": "Fri, 12 Jan 2018 14:25:08 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Sheikh", "Abdul-Saboor", ""], ["Rasul", "Kashif", ""], ["Merentitis", "Andreas", ""], ["Bergmann", "Urs", ""]]}, {"id": "1712.01145", "submitter": "Xiaoyong Yuan", "authors": "Ruimin Sun, Xiaoyong Yuan, Pan He, Qile Zhu, Aokun Chen, Andre Gregio,\n  Daniela Oliveira, Xiaolin Li", "title": "Learning Fast and Slow: PROPEDEUTICA for Real-time Malware Detection", "comments": "17 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce and evaluate PROPEDEUTICA, a novel methodology\nand framework for efficient and effective real-time malware detection,\nleveraging the best of conventional machine learning (ML) and deep learning\n(DL) algorithms. In PROPEDEUTICA, all software processes in the system start\nexecution subjected to a conventional ML detector for fast classification. If a\npiece of software receives a borderline classification, it is subjected to\nfurther analysis via more performance expensive and more accurate DL methods,\nvia our newly proposed DL algorithm DEEPMALWARE. Further, we introduce delays\nto the execution of software subjected to deep learning analysis as a way to\n\"buy time\" for DL analysis and to rate-limit the impact of possible malware in\nthe system. We evaluated PROPEDEUTICA with a set of 9,115 malware samples and\n877 commonly used benign software samples from various categories for the\nWindows OS. Our results show that the false positive rate for conventional ML\nmethods can reach 20%, and for modern DL methods it is usually below 6%.\nHowever, the classification time for DL can be 100X longer than conventional ML\nmethods. PROPEDEUTICA improved the detection F1-score from 77.54% (conventional\nML method) to 90.25%, and reduced the detection time by 54.86%. Further, the\npercentage of software subjected to DL analysis was approximately 40% on\naverage. Further, the application of delays in software subjected to ML reduced\nthe detection time by approximately 10%. Finally, we found and discussed a\ndiscrepancy between the detection accuracy offline (analysis after all traces\nare collected) and on-the-fly (analysis in tandem with trace collection). Our\ninsights show that conventional ML and modern DL-based malware detectors in\nisolation cannot meet the needs of efficient and effective malware detection:\nhigh accuracy, low false positive rate, and short classification time.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 15:30:03 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Sun", "Ruimin", ""], ["Yuan", "Xiaoyong", ""], ["He", "Pan", ""], ["Zhu", "Qile", ""], ["Chen", "Aokun", ""], ["Gregio", "Andre", ""], ["Oliveira", "Daniela", ""], ["Li", "Xiaolin", ""]]}, {"id": "1712.01158", "submitter": "Mohsen Ahmadi Fahandar", "authors": "Mohsen Ahmadi Fahandar, Eyke H\\\"ullermeier, In\\'es Couso", "title": "Statistical Inference for Incomplete Ranking Data: The Case of\n  Rank-Dependent Coarsening", "comments": "Proceedings of the 34th International Conference on Machine Learning\n  (ICML 2017), 10 pages", "journal-ref": "Proceedings of the 34th International Conference on Machine\n  Learning (ICML 2017), Sydney, Australia, PMLR 70:1078-1087, 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of statistical inference for ranking data,\nspecifically rank aggregation, under the assumption that samples are incomplete\nin the sense of not comprising all choice alternatives. In contrast to most\nexisting methods, we explicitly model the process of turning a full ranking\ninto an incomplete one, which we call the coarsening process. To this end, we\npropose the concept of rank-dependent coarsening, which assumes that incomplete\nrankings are produced by projecting a full ranking to a random subset of ranks.\nFor a concrete instantiation of our model, in which full rankings are drawn\nfrom a Plackett-Luce distribution and observations take the form of pairwise\npreferences, we study the performance of various rank aggregation methods. In\naddition to predictive accuracy in the finite sample setting, we address the\ntheoretical question of consistency, by which we mean the ability to recover a\ntarget ranking when the sample size goes to infinity, despite a potential bias\nin the observations caused by the (unknown) coarsening.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 15:49:57 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Fahandar", "Mohsen Ahmadi", ""], ["H\u00fcllermeier", "Eyke", ""], ["Couso", "In\u00e9s", ""]]}, {"id": "1712.01169", "submitter": "David Nagy", "authors": "David G. Nagy, Gerg\\H{o} Orb\\'an", "title": "Episodic memory for continual model learning", "comments": "CLDL at NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both the human brain and artificial learning agents operating in real-world\nor comparably complex environments are faced with the challenge of online model\nselection. In principle this challenge can be overcome: hierarchical Bayesian\ninference provides a principled method for model selection and it converges on\nthe same posterior for both off-line (i.e. batch) and online learning. However,\nmaintaining a parameter posterior for each model in parallel has in general an\neven higher memory cost than storing the entire data set and is consequently\nclearly unfeasible. Alternatively, maintaining only a limited set of models in\nmemory could limit memory requirements. However, sufficient statistics for one\nmodel will usually be insufficient for fitting a different kind of model,\nmeaning that the agent loses information with each model change. We propose\nthat episodic memory can circumvent the challenge of limited memory-capacity\nonline model selection by retaining a selected subset of data points. We design\na method to compute the quantities necessary for model selection even when the\ndata is discarded and only statistics of one (or few) learnt models are\navailable. We demonstrate on a simple model that a limited-sized episodic\nmemory buffer, when the content is optimised to retain data with statistics not\nmatching the current representation, can resolve the fundamental challenge of\nonline model selection.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 16:02:36 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Nagy", "David G.", ""], ["Orb\u00e1n", "Gerg\u0151", ""]]}, {"id": "1712.01193", "submitter": "Madhav Nimishakavi Mr", "authors": "Madhav Nimishakavi, Pratik Jawanpuria, and Bamdev Mishra", "title": "A dual framework for low-rank tensor completion", "comments": "Aceepted to appear in Advances of Nueral Information Processing\n  Systems (NIPS), 2018. A shorter version appeared in the NIPS workshop on\n  Synergies in Geometric Data Analysis 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the popular approaches for low-rank tensor completion is to use the\nlatent trace norm regularization. However, most existing works in this\ndirection learn a sparse combination of tensors. In this work, we fill this gap\nby proposing a variant of the latent trace norm that helps in learning a\nnon-sparse combination of tensors. We develop a dual framework for solving the\nlow-rank tensor completion problem. We first show a novel characterization of\nthe dual solution space with an interesting factorization of the optimal\nsolution. Overall, the optimal solution is shown to lie on a Cartesian product\nof Riemannian manifolds. Furthermore, we exploit the versatile Riemannian\noptimization framework for proposing computationally efficient trust region\nalgorithm. The experiments illustrate the efficacy of the proposed algorithm on\nseveral real-world datasets across applications.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 16:55:52 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 07:24:04 GMT"}, {"version": "v3", "created": "Fri, 25 May 2018 08:55:14 GMT"}, {"version": "v4", "created": "Sat, 10 Nov 2018 06:20:42 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Nimishakavi", "Madhav", ""], ["Jawanpuria", "Pratik", ""], ["Mishra", "Bamdev", ""]]}, {"id": "1712.01252", "submitter": "Jun Lu", "authors": "Wei Ma, Jun Lu", "title": "An Equivalence of Fully Connected Layer and Convolutional Layer", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article demonstrates that convolutional operation can be converted to\nmatrix multiplication, which has the same calculation way with fully connected\nlayer. The article is helpful for the beginners of the neural network to\nunderstand how fully connected layer and the convolutional layer work in the\nbackend. To be concise and to make the article more readable, we only consider\nthe linear case. It can be extended to the non-linear case easily through\nplugging in a non-linear encapsulation to the values like this $\\sigma(x)$\ndenoted as $x^{\\prime}$.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 18:53:01 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Ma", "Wei", ""], ["Lu", "Jun", ""]]}, {"id": "1712.01312", "submitter": "Christos Louizos", "authors": "Christos Louizos, Max Welling and Diederik P. Kingma", "title": "Learning Sparse Neural Networks through $L_0$ Regularization", "comments": "Published as a conference paper at the International Conference on\n  Learning Representations (ICLR) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a practical method for $L_0$ norm regularization for neural\nnetworks: pruning the network during training by encouraging weights to become\nexactly zero. Such regularization is interesting since (1) it can greatly speed\nup training and inference, and (2) it can improve generalization. AIC and BIC,\nwell-known model selection criteria, are special cases of $L_0$ regularization.\nHowever, since the $L_0$ norm of weights is non-differentiable, we cannot\nincorporate it directly as a regularization term in the objective function. We\npropose a solution through the inclusion of a collection of non-negative\nstochastic gates, which collectively determine which weights to set to zero. We\nshow that, somewhat surprisingly, for certain distributions over the gates, the\nexpected $L_0$ norm of the resulting gated weights is differentiable with\nrespect to the distribution parameters. We further propose the \\emph{hard\nconcrete} distribution for the gates, which is obtained by \"stretching\" a\nbinary concrete distribution and then transforming its samples with a\nhard-sigmoid. The parameters of the distribution over the gates can then be\njointly optimized with the original network parameters. As a result our method\nallows for straightforward and efficient learning of model structures with\nstochastic gradient descent and allows for conditional computation in a\nprincipled way. We perform various experiments to demonstrate the effectiveness\nof the resulting approach and regularizer.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 19:20:27 GMT"}, {"version": "v2", "created": "Fri, 22 Jun 2018 14:54:59 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Louizos", "Christos", ""], ["Welling", "Max", ""], ["Kingma", "Diederik P.", ""]]}, {"id": "1712.01334", "submitter": "Katja Ried", "authors": "Katja Ried and Thomas M\\\"uller and Hans J. Briegel", "title": "Modelling collective motion based on the principle of agency", "comments": "13 pages plus 6 page appendix, 6 figures", "journal-ref": "PLoS ONE 14(2): e0212044 (2019)", "doi": "10.1371/journal.pone.0212044", "report-no": null, "categories": "q-bio.PE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collective motion is an intriguing phenomenon, especially considering that it\narises from a set of simple rules governing local interactions between\nindividuals. In theoretical models, these rules are normally \\emph{assumed} to\ntake a particular form, possibly constrained by heuristic arguments. We propose\na new class of models, which describe the individuals as \\emph{agents}, capable\nof deciding for themselves how to act and learning from their experiences. The\nlocal interaction rules do not need to be postulated in this model, since they\n\\emph{emerge} from the learning process. We apply this ansatz to a concrete\nscenario involving marching locusts, in order to model the phenomenon of\ndensity-dependent alignment. We show that our learning agent-based model can\naccount for a Fokker-Planck equation that describes the collective motion and,\nmost notably, that the agents can learn the appropriate local interactions,\nrequiring no strong previous assumptions on their form. These results suggest\nthat learning agent-based models are a powerful tool for studying a broader\nclass of problems involving collective motion and animal agency in general.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 20:20:59 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Ried", "Katja", ""], ["M\u00fcller", "Thomas", ""], ["Briegel", "Hans J.", ""]]}, {"id": "1712.01378", "submitter": "Samuel Otto", "authors": "Samuel E. Otto and Clarence W. Rowley", "title": "Linearly-Recurrent Autoencoder Networks for Learning Dynamics", "comments": "37 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a method for learning low-dimensional approximations of\nnonlinear dynamical systems, based on neural-network approximations of the\nunderlying Koopman operator. Extended Dynamic Mode Decomposition (EDMD)\nprovides a useful data-driven approximation of the Koopman operator for\nanalyzing dynamical systems. This paper addresses a fundamental problem\nassociated with EDMD: a trade-off between representational capacity of the\ndictionary and over-fitting due to insufficient data. A new neural network\narchitecture combining an autoencoder with linear recurrent dynamics in the\nencoded state is used to learn a low-dimensional and highly informative\nKoopman-invariant subspace of observables. A method is also presented for\nbalanced model reduction of over-specified EDMD systems in feature space.\nNonlinear reconstruction using partially linear multi-kernel regression aims to\nimprove reconstruction accuracy from the low-dimensional state when the data\nhas complex but intrinsically low-dimensional structure. The techniques\ndemonstrate the ability to identify Koopman eigenfunctions of the unforced\nDuffing equation, create accurate low-dimensional models of an unstable\ncylinder wake flow, and make short-time predictions of the chaotic\nKuramoto-Sivashinsky equation.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 21:29:43 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2019 19:15:19 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Otto", "Samuel E.", ""], ["Rowley", "Clarence W.", ""]]}, {"id": "1712.01447", "submitter": "Shubhanshu Shekhar", "authors": "Shubhanshu Shekhar, Tara Javidi", "title": "Gaussian Process bandits with adaptive discretization", "comments": "34 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the problem of maximizing a black-box function $f:\\mathcal{X}\n\\to \\mathbb{R}$ is studied in the Bayesian framework with a Gaussian Process\n(GP) prior. In particular, a new algorithm for this problem is proposed, and\nhigh probability bounds on its simple and cumulative regret are established.\nThe query point selection rule in most existing methods involves an exhaustive\nsearch over an increasingly fine sequence of uniform discretizations of\n$\\mathcal{X}$. The proposed algorithm, in contrast, adaptively refines\n$\\mathcal{X}$ which leads to a lower computational complexity, particularly\nwhen $\\mathcal{X}$ is a subset of a high dimensional Euclidean space. In\naddition to the computational gains, sufficient conditions are identified under\nwhich the regret bounds of the new algorithm improve upon the known results.\nFinally an extension of the algorithm to the case of contextual bandits is\nproposed, and high probability bounds on the contextual regret are presented.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 02:22:45 GMT"}, {"version": "v2", "created": "Fri, 5 Jan 2018 21:36:14 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Shekhar", "Shubhanshu", ""], ["Javidi", "Tara", ""]]}, {"id": "1712.01473", "submitter": "Thomas Laurent", "authors": "Thomas Laurent and James von Brecht", "title": "Deep linear neural networks with arbitrary loss: All local minima are\n  global", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider deep linear networks with arbitrary convex differentiable loss.\nWe provide a short and elementary proof of the fact that all local minima are\nglobal minima if the hidden layers are either 1) at least as wide as the input\nlayer, or 2) at least as wide as the output layer. This result is the strongest\npossible in the following sense: If the loss is convex and Lipschitz but not\ndifferentiable then deep linear networks can have sub-optimal local minima.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 04:12:09 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 13:42:03 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Laurent", "Thomas", ""], ["von Brecht", "James", ""]]}, {"id": "1712.01496", "submitter": "Zhanli Chen", "authors": "Zhanli Chen, Rashid Ansari, Diana J. Wilkie", "title": "Learning Pain from Action Unit Combinations: A Weakly Supervised\n  Approach via Multiple Instance Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Patient pain can be detected highly reliably from facial expressions using a\nset of facial muscle-based action units (AUs) defined by the Facial Action\nCoding System (FACS). A key characteristic of facial expression of pain is the\nsimultaneous occurrence of pain-related AU combinations, whose automated\ndetection would be highly beneficial for efficient and practical pain\nmonitoring. Existing general Automated Facial Expression Recognition (AFER)\nsystems prove inadequate when applied specifically for detecting pain as they\neither focus on detecting individual pain-related AUs but not on combinations\nor they seek to bypass AU detection by training a binary pain classifier\ndirectly on pain intensity data but are limited by lack of enough labeled data\nfor satisfactory training. In this paper, we propose a new approach that mimics\nthe strategy of human coders of decoupling pain detection into two consecutive\ntasks: one performed at the individual video-frame level and the other at\nvideo-sequence level. Using state-of-the-art AFER tools to detect single AUs at\nthe frame level, we propose two novel data structures to encode AU combinations\nfrom single AU scores. Two weakly supervised learning frameworks namely\nmultiple instance learning (MIL) and multiple clustered instance learning\n(MCIL) are employed corresponding to each data structure to learn pain from\nvideo sequences. Experimental results show an 87% pain recognition accuracy\nwith 0.94 AUC (Area Under Curve) on the UNBC-McMaster Shoulder Pain Expression\ndataset. Tests on long videos in a lung cancer patient video dataset\ndemonstrates the potential value of the proposed system for pain monitoring in\nclinical settings.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 06:27:22 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 04:32:48 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Chen", "Zhanli", ""], ["Ansari", "Rashid", ""], ["Wilkie", "Diana J.", ""]]}, {"id": "1712.01521", "submitter": "Wei Xiao", "authors": "Wei Xiao", "title": "An Online Algorithm for Nonparametric Correlations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric correlations such as Spearman's rank correlation and Kendall's\ntau correlation are widely applied in scientific and engineering fields. This\npaper investigates the problem of computing nonparametric correlations on the\nfly for streaming data. Standard batch algorithms are generally too slow to\nhandle real-world big data applications. They also require too much memory\nbecause all the data need to be stored in the memory before processing. This\npaper proposes a novel online algorithm for computing nonparametric\ncorrelations. The algorithm has O(1) time complexity and O(1) memory cost and\nis quite suitable for edge devices, where only limited memory and processing\npower are available. You can seek a balance between speed and accuracy by\nchanging the number of cutpoints specified in the algorithm. The online\nalgorithm can compute the nonparametric correlations 10 to 1,000 times faster\nthan the corresponding batch algorithm, and it can compute them based either on\nall past observations or on fixed-size sliding windows.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 08:19:11 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Xiao", "Wei", ""]]}, {"id": "1712.01551", "submitter": "Zhiwu Huang", "authors": "Zhiwu Huang, Jiqing Wu, Luc Van Gool", "title": "Manifold-valued Image Generation with Wasserstein Generative Adversarial\n  Nets", "comments": "Accepted by AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative modeling over natural images is one of the most fundamental\nmachine learning problems. However, few modern generative models, including\nWasserstein Generative Adversarial Nets (WGANs), are studied on manifold-valued\nimages that are frequently encountered in real-world applications. To fill the\ngap, this paper first formulates the problem of generating manifold-valued\nimages and exploits three typical instances: hue-saturation-value (HSV) color\nimage generation, chromaticity-brightness (CB) color image generation, and\ndiffusion-tensor (DT) image generation. For the proposed generative modeling\nproblem, we then introduce a theorem of optimal transport to derive a new\nWasserstein distance of data distributions on complete manifolds, enabling us\nto achieve a tractable objective under the WGAN framework. In addition, we\nrecommend three benchmark datasets that are CIFAR-10 HSV/CB color images,\nImageNet HSV/CB color images, UCL DT image datasets. On the three datasets, we\nexperimentally demonstrate the proposed manifold-aware WGAN model can generate\nmore plausible manifold-valued images than its competitors.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 10:02:05 GMT"}, {"version": "v2", "created": "Thu, 3 Jan 2019 16:09:47 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Huang", "Zhiwu", ""], ["Wu", "Jiqing", ""], ["Van Gool", "Luc", ""]]}, {"id": "1712.01572", "submitter": "Stefan Klus", "authors": "Stefan Klus, Ingmar Schuster, Krikamol Muandet", "title": "Eigendecompositions of Transfer Operators in Reproducing Kernel Hilbert\n  Spaces", "comments": null, "journal-ref": "Journal of Nonlinear Science, 2019", "doi": "10.1007/s00332-019-09574-z", "report-no": null, "categories": "math.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer operators such as the Perron--Frobenius or Koopman operator play an\nimportant role in the global analysis of complex dynamical systems. The\neigenfunctions of these operators can be used to detect metastable sets, to\nproject the dynamics onto the dominant slow processes, or to separate\nsuperimposed signals. We extend transfer operator theory to reproducing kernel\nHilbert spaces and show that these operators are related to Hilbert space\nrepresentations of conditional distributions, known as conditional mean\nembeddings in the machine learning community. Moreover, numerical methods to\ncompute empirical estimates of these embeddings are akin to data-driven methods\nfor the approximation of transfer operators such as extended dynamic mode\ndecomposition and its variants. One main benefit of the presented kernel-based\napproaches is that these methods can be applied to any domain where a\nsimilarity measure given by a kernel is available. We illustrate the results\nwith the aid of guiding examples and highlight potential applications in\nmolecular dynamics as well as video and text data analysis.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 11:17:11 GMT"}, {"version": "v2", "created": "Wed, 16 May 2018 12:36:56 GMT"}, {"version": "v3", "created": "Tue, 13 Aug 2019 07:07:02 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Klus", "Stefan", ""], ["Schuster", "Ingmar", ""], ["Muandet", "Krikamol", ""]]}, {"id": "1712.01664", "submitter": "Jos van der Westhuizen", "authors": "David Janz, Jos van der Westhuizen, Brooks Paige, Matt J. Kusner,\n  Jos\\'e Miguel Hern\\'andez-Lobato", "title": "Learning a Generative Model for Validity in Complex Discrete Structures", "comments": "Conference paper at ICLR 2018. Code available online", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models have been successfully used to learn representations\nfor high-dimensional discrete spaces by representing discrete objects as\nsequences and employing powerful sequence-based deep models. Unfortunately,\nthese sequence-based models often produce invalid sequences: sequences which do\nnot represent any underlying discrete structure; invalid sequences hinder the\nutility of such models. As a step towards solving this problem, we propose to\nlearn a deep recurrent validator model, which can estimate whether a partial\nsequence can function as the beginning of a full, valid sequence. This\nvalidator provides insight as to how individual sequence elements influence the\nvalidity of the overall sequence, and can be used to constrain sequence based\nmodels to generate valid sequences -- and thus faithfully model discrete\nobjects. Our approach is inspired by reinforcement learning, where an oracle\nwhich can evaluate validity of complete sequences provides a sparse reward\nsignal. We demonstrate its effectiveness as a generative model of Python 3\nsource code for mathematical expressions, and in improving the ability of a\nvariational autoencoder trained on SMILES strings to decode valid molecular\nstructures.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 14:36:23 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 19:19:59 GMT"}, {"version": "v3", "created": "Mon, 16 Apr 2018 17:55:48 GMT"}, {"version": "v4", "created": "Fri, 2 Nov 2018 02:19:00 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Janz", "David", ""], ["van der Westhuizen", "Jos", ""], ["Paige", "Brooks", ""], ["Kusner", "Matt J.", ""], ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""]]}, {"id": "1712.01665", "submitter": "Beyza Ermis Ms", "authors": "Beyza Ermis, Ali Taylan Cemgil", "title": "Differentially Private Dropout", "comments": "arXiv admin note: text overlap with arXiv:1611.00340 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large data collections required for the training of neural networks often\ncontain sensitive information such as the medical histories of patients, and\nthe privacy of the training data must be preserved. In this paper, we introduce\na dropout technique that provides an elegant Bayesian interpretation to\ndropout, and show that the intrinsic noise added, with the primary goal of\nregularization, can be exploited to obtain a degree of differential privacy.\nThe iterative nature of training neural networks presents a challenge for\nprivacy-preserving estimation since multiple iterations increase the amount of\nnoise added. We overcome this by using a relaxed notion of differential\nprivacy, called concentrated differential privacy, which provides tighter\nestimates on the overall privacy loss. We demonstrate the accuracy of our\nprivacy-preserving dropout algorithm on benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 21:15:01 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Ermis", "Beyza", ""], ["Cemgil", "Ali Taylan", ""]]}, {"id": "1712.01727", "submitter": "Jos\\'e Lezama", "authors": "Jos\\'e Lezama, Qiang Qiu, Pablo Mus\\'e, Guillermo Sapiro", "title": "OL\\'E: Orthogonal Low-rank Embedding, A Plug and Play Geometric Loss for\n  Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks trained using a softmax layer at the top and the\ncross-entropy loss are ubiquitous tools for image classification. Yet, this\ndoes not naturally enforce intra-class similarity nor inter-class margin of the\nlearned deep representations. To simultaneously achieve these two goals,\ndifferent solutions have been proposed in the literature, such as the pairwise\nor triplet losses. However, such solutions carry the extra task of selecting\npairs or triplets, and the extra computational burden of computing and learning\nfor many combinations of them. In this paper, we propose a plug-and-play loss\nterm for deep networks that explicitly reduces intra-class variance and\nenforces inter-class margin simultaneously, in a simple and elegant geometric\nmanner. For each class, the deep features are collapsed into a learned linear\nsubspace, or union of them, and inter-class subspaces are pushed to be as\northogonal as possible. Our proposed Orthogonal Low-rank Embedding (OL\\'E) does\nnot require carefully crafting pairs or triplets of samples for training, and\nworks standalone as a classification loss, being the first reported deep metric\nlearning framework of its kind. Because of the improved margin between features\nof different classes, the resulting deep networks generalize better, are more\ndiscriminative, and more robust. We demonstrate improved classification\nperformance in general object recognition, plugging the proposed loss term into\nexisting off-the-shelf architectures. In particular, we show the advantage of\nthe proposed loss in the small data/model scenario, and we significantly\nadvance the state-of-the-art on the Stanford STL-10 benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 16:03:56 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Lezama", "Jos\u00e9", ""], ["Qiu", "Qiang", ""], ["Mus\u00e9", "Pablo", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1712.01769", "submitter": "Chung-Cheng Chiu", "authors": "Chung-Cheng Chiu, Tara N. Sainath, Yonghui Wu, Rohit Prabhavalkar,\n  Patrick Nguyen, Zhifeng Chen, Anjuli Kannan, Ron J. Weiss, Kanishka Rao,\n  Ekaterina Gonina, Navdeep Jaitly, Bo Li, Jan Chorowski, Michiel Bacchiani", "title": "State-of-the-art Speech Recognition With Sequence-to-Sequence Models", "comments": "ICASSP camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention-based encoder-decoder architectures such as Listen, Attend, and\nSpell (LAS), subsume the acoustic, pronunciation and language model components\nof a traditional automatic speech recognition (ASR) system into a single neural\nnetwork. In previous work, we have shown that such architectures are comparable\nto state-of-theart ASR systems on dictation tasks, but it was not clear if such\narchitectures would be practical for more challenging tasks such as voice\nsearch. In this work, we explore a variety of structural and optimization\nimprovements to our LAS model which significantly improve performance. On the\nstructural side, we show that word piece models can be used instead of\ngraphemes. We also introduce a multi-head attention architecture, which offers\nimprovements over the commonly-used single-head attention. On the optimization\nside, we explore synchronous training, scheduled sampling, label smoothing, and\nminimum word error rate optimization, which are all shown to improve accuracy.\nWe present results with a unidirectional LSTM encoder for streaming\nrecognition. On a 12, 500 hour voice search task, we find that the proposed\nchanges improve the WER from 9.2% to 5.6%, while the best conventional system\nachieves 6.7%; on a dictation task our model achieves a WER of 4.1% compared to\n5% for the conventional system.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 17:24:05 GMT"}, {"version": "v2", "created": "Wed, 13 Dec 2017 23:25:23 GMT"}, {"version": "v3", "created": "Fri, 15 Dec 2017 18:41:01 GMT"}, {"version": "v4", "created": "Fri, 22 Dec 2017 22:55:44 GMT"}, {"version": "v5", "created": "Thu, 18 Jan 2018 18:25:33 GMT"}, {"version": "v6", "created": "Fri, 23 Feb 2018 18:44:30 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Chiu", "Chung-Cheng", ""], ["Sainath", "Tara N.", ""], ["Wu", "Yonghui", ""], ["Prabhavalkar", "Rohit", ""], ["Nguyen", "Patrick", ""], ["Chen", "Zhifeng", ""], ["Kannan", "Anjuli", ""], ["Weiss", "Ron J.", ""], ["Rao", "Kanishka", ""], ["Gonina", "Ekaterina", ""], ["Jaitly", "Navdeep", ""], ["Li", "Bo", ""], ["Chorowski", "Jan", ""], ["Bacchiani", "Michiel", ""]]}, {"id": "1712.01807", "submitter": "Chung-Cheng Chiu", "authors": "Tara N. Sainath, Chung-Cheng Chiu, Rohit Prabhavalkar, Anjuli Kannan,\n  Yonghui Wu, Patrick Nguyen, Zhifeng Chen", "title": "Improving the Performance of Online Neural Transducer Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Having a sequence-to-sequence model which can operate in an online fashion is\nimportant for streaming applications such as Voice Search. Neural transducer is\na streaming sequence-to-sequence model, but has shown a significant degradation\nin performance compared to non-streaming models such as Listen, Attend and\nSpell (LAS). In this paper, we present various improvements to NT.\nSpecifically, we look at increasing the window over which NT computes\nattention, mainly by looking backwards in time so the model still remains\nonline. In addition, we explore initializing a NT model from a LAS-trained\nmodel so that it is guided with a better alignment. Finally, we explore\nincluding stronger language models such as using wordpiece models, and applying\nan external LM during the beam search. On a Voice Search task, we find with\nthese improvements we can get NT to match the performance of LAS.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 18:34:56 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Sainath", "Tara N.", ""], ["Chiu", "Chung-Cheng", ""], ["Prabhavalkar", "Rohit", ""], ["Kannan", "Anjuli", ""], ["Wu", "Yonghui", ""], ["Nguyen", "Patrick", ""], ["Chen", "Zhifeng", ""]]}, {"id": "1712.01818", "submitter": "Chung-Cheng Chiu", "authors": "Rohit Prabhavalkar, Tara N. Sainath, Yonghui Wu, Patrick Nguyen,\n  Zhifeng Chen, Chung-Cheng Chiu, Anjuli Kannan", "title": "Minimum Word Error Rate Training for Attention-based\n  Sequence-to-Sequence Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-to-sequence models, such as attention-based models in automatic\nspeech recognition (ASR), are typically trained to optimize the cross-entropy\ncriterion which corresponds to improving the log-likelihood of the data.\nHowever, system performance is usually measured in terms of word error rate\n(WER), not log-likelihood. Traditional ASR systems benefit from discriminative\nsequence training which optimizes criteria such as the state-level minimum\nBayes risk (sMBR) which are more closely related to WER. In the present work,\nwe explore techniques to train attention-based models to directly minimize\nexpected word error rate. We consider two loss functions which approximate the\nexpected number of word errors: either by sampling from the model, or by using\nN-best lists of decoded hypotheses, which we find to be more effective than the\nsampling-based method. In experimental evaluations, we find that the proposed\ntraining procedure improves performance by up to 8.2% relative to the baseline\nsystem. This allows us to train grapheme-based, uni-directional attention-based\nmodels which match the performance of a traditional, state-of-the-art,\ndiscriminative sequence-trained system on a mobile voice-search task.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 18:52:18 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Prabhavalkar", "Rohit", ""], ["Sainath", "Tara N.", ""], ["Wu", "Yonghui", ""], ["Nguyen", "Patrick", ""], ["Chen", "Zhifeng", ""], ["Chiu", "Chung-Cheng", ""], ["Kannan", "Anjuli", ""]]}, {"id": "1712.01856", "submitter": "Behzad Tabibian", "authors": "Behzad Tabibian, Utkarsh Upadhyay, Abir De, Ali Zarezade, Bernhard\n  Schoelkopf, Manuel Gomez-Rodriguez", "title": "Optimizing Human Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spaced repetition is a technique for efficient memorization which uses\nrepeated, spaced review of content to improve long-term retention. Can we find\nthe optimal reviewing schedule to maximize the benefits of spaced repetition?\nIn this paper, we introduce a novel, flexible representation of spaced\nrepetition using the framework of marked temporal point processes and then\naddress the above question as an optimal control problem for stochastic\ndifferential equations with jumps. For two well-known human memory models, we\nshow that the optimal reviewing schedule is given by the recall probability of\nthe content to be learned. As a result, we can then develop a simple, scalable\nonline algorithm, Memorize, to sample the optimal reviewing times. Experiments\non both synthetic and real data gathered from Duolingo, a popular\nlanguage-learning online platform, show that our algorithm may be able to help\nlearners memorize more effectively than alternatives.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 19:00:11 GMT"}, {"version": "v2", "created": "Sat, 10 Mar 2018 19:32:38 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Tabibian", "Behzad", ""], ["Upadhyay", "Utkarsh", ""], ["De", "Abir", ""], ["Zarezade", "Ali", ""], ["Schoelkopf", "Bernhard", ""], ["Gomez-Rodriguez", "Manuel", ""]]}, {"id": "1712.01864", "submitter": "Chung-Cheng Chiu", "authors": "Tara N. Sainath, Rohit Prabhavalkar, Shankar Kumar, Seungji Lee,\n  Anjuli Kannan, David Rybach, Vlad Schogol, Patrick Nguyen, Bo Li, Yonghui Wu,\n  Zhifeng Chen, Chung-Cheng Chiu", "title": "No Need for a Lexicon? Evaluating the Value of the Pronunciation Lexica\n  in End-to-End Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For decades, context-dependent phonemes have been the dominant sub-word unit\nfor conventional acoustic modeling systems. This status quo has begun to be\nchallenged recently by end-to-end models which seek to combine acoustic,\npronunciation, and language model components into a single neural network. Such\nsystems, which typically predict graphemes or words, simplify the recognition\nprocess since they remove the need for a separate expert-curated pronunciation\nlexicon to map from phoneme-based units to words. However, there has been\nlittle previous work comparing phoneme-based versus grapheme-based sub-word\nunits in the end-to-end modeling framework, to determine whether the gains from\nsuch approaches are primarily due to the new probabilistic model, or from the\njoint learning of the various components with grapheme-based units.\n  In this work, we conduct detailed experiments which are aimed at quantifying\nthe value of phoneme-based pronunciation lexica in the context of end-to-end\nmodels. We examine phoneme-based end-to-end models, which are contrasted\nagainst grapheme-based ones on a large vocabulary English Voice-search task,\nwhere we find that graphemes do indeed outperform phonemes. We also compare\ngrapheme and phoneme-based approaches on a multi-dialect English task, which\nonce again confirm the superiority of graphemes, greatly simplifying the system\nfor recognizing multiple dialects.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 19:02:28 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Sainath", "Tara N.", ""], ["Prabhavalkar", "Rohit", ""], ["Kumar", "Shankar", ""], ["Lee", "Seungji", ""], ["Kannan", "Anjuli", ""], ["Rybach", "David", ""], ["Schogol", "Vlad", ""], ["Nguyen", "Patrick", ""], ["Li", "Bo", ""], ["Wu", "Yonghui", ""], ["Chen", "Zhifeng", ""], ["Chiu", "Chung-Cheng", ""]]}, {"id": "1712.01887", "submitter": "Song Han", "authors": "Yujun Lin, Song Han, Huizi Mao, Yu Wang, William J. Dally", "title": "Deep Gradient Compression: Reducing the Communication Bandwidth for\n  Distributed Training", "comments": "we find 99.9% of the gradient exchange in distributed SGD is\n  redundant; we reduce the communication bandwidth by two orders of magnitude\n  without losing accuracy. Code is available at:\n  https://github.com/synxlin/deep-gradient-compression", "journal-ref": "ICLR 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale distributed training requires significant communication bandwidth\nfor gradient exchange that limits the scalability of multi-node training, and\nrequires expensive high-bandwidth network infrastructure. The situation gets\neven worse with distributed training on mobile devices (federated learning),\nwhich suffers from higher latency, lower throughput, and intermittent poor\nconnections. In this paper, we find 99.9% of the gradient exchange in\ndistributed SGD is redundant, and propose Deep Gradient Compression (DGC) to\ngreatly reduce the communication bandwidth. To preserve accuracy during\ncompression, DGC employs four methods: momentum correction, local gradient\nclipping, momentum factor masking, and warm-up training. We have applied Deep\nGradient Compression to image classification, speech recognition, and language\nmodeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and\nLibrispeech Corpus. On these scenarios, Deep Gradient Compression achieves a\ngradient compression ratio from 270x to 600x without losing accuracy, cutting\nthe gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from\n488MB to 0.74MB. Deep gradient compression enables large-scale distributed\ntraining on inexpensive commodity 1Gbps Ethernet and facilitates distributed\ntraining on mobile. Code is available at:\nhttps://github.com/synxlin/deep-gradient-compression.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 19:48:11 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 19:38:39 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2020 03:28:30 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Lin", "Yujun", ""], ["Han", "Song", ""], ["Mao", "Huizi", ""], ["Wang", "Yu", ""], ["Dally", "William J.", ""]]}, {"id": "1712.01934", "submitter": "Oleksandr Zadorozhnyi", "authors": "Gilles Blanchard and Oleksandr Zadorozhnyi", "title": "Concentration of weakly dependent Banach-valued sums and applications to\n  statistical learning methods", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain a Bernstein-type inequality for sums of Banach-valued random\nvariables satisfying a weak dependence assumption of general type and under\ncertain smoothness assumptions of the underlying Banach norm. We use this\ninequality in order to investigate in the asymptotical regime the error upper\nbounds for the broad family of spectral regularization methods for reproducing\nkernel decision rules, when trained on a sample coming from a $\\tau-$mixing\nprocess.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 21:24:52 GMT"}, {"version": "v2", "created": "Sun, 9 Dec 2018 14:34:57 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Blanchard", "Gilles", ""], ["Zadorozhnyi", "Oleksandr", ""]]}, {"id": "1712.01977", "submitter": "Nand Sharma", "authors": "Nand Sharma", "title": "Single-trial P300 Classification using PCA with LDA, QDA and Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The P300 event-related potential (ERP), evoked in scalp-recorded\nelectroencephalography (EEG) by external stimuli, has proven to be a reliable\nresponse for controlling a BCI. The P300 component of an event related\npotential is thus widely used in brain-computer interfaces to translate the\nsubjects' intent by mere thoughts into commands to control artificial devices.\nThe main challenge in the classification of P300 trials in\nelectroencephalographic (EEG) data is the low signal-to-noise ratio (SNR) of\nthe P300 response. To overcome the low SNR of individual trials, it is common\npractice to average together many consecutive trials, which effectively\ndiminishes the random noise. Unfortunately, when more repeated trials are\nrequired for applications such as the P300 speller, the communication rate is\ngreatly reduced. This has resulted in a need for better methods to improve\nsingle-trial classification accuracy of P300 response. In this work, we use\nPrincipal Component Analysis (PCA) as a preprocessing method and use Linear\nDiscriminant Analysis (LDA)and neural networks for classification. The results\nshow that a combination of PCA with these methods provided as high as 13\\%\naccuracy gain for single-trial classification while using only 3 to 4 principal\ncomponents.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 00:21:07 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Sharma", "Nand", ""]]}, {"id": "1712.01990", "submitter": "Kyeong Soo (Joseph) Kim", "authors": "Kyeong Soo Kim, Sanghyuk Lee, Kaizhu Huang", "title": "A Scalable Deep Neural Network Architecture for Multi-Building and\n  Multi-Floor Indoor Localization Based on Wi-Fi Fingerprinting", "comments": "9 pages, 6 figures", "journal-ref": "Big Data Analytics, vol. 3, no. 4, pp. 1-17, Apr. 19, 2018", "doi": "10.1186/s41044-018-0031-2", "report-no": null, "categories": "cs.NI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key technologies for future large-scale location-aware services\ncovering a complex of multi-story buildings --- e.g., a big shopping mall and a\nuniversity campus --- is a scalable indoor localization technique. In this\npaper, we report the current status of our investigation on the use of deep\nneural networks (DNNs) for scalable building/floor classification and\nfloor-level position estimation based on Wi-Fi fingerprinting. Exploiting the\nhierarchical nature of the building/floor estimation and floor-level\ncoordinates estimation of a location, we propose a new DNN architecture\nconsisting of a stacked autoencoder for the reduction of feature space\ndimension and a feed-forward classifier for multi-label classification of\nbuilding/floor/location, on which the multi-building and multi-floor indoor\nlocalization system based on Wi-Fi fingerprinting is built. Experimental\nresults for the performance of building/floor estimation and floor-level\ncoordinates estimation of a given location demonstrate the feasibility of the\nproposed DNN-based indoor localization system, which can provide near\nstate-of-the-art performance using a single DNN, for the implementation with\nlower complexity and energy consumption at mobile devices.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 01:06:01 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Kim", "Kyeong Soo", ""], ["Lee", "Sanghyuk", ""], ["Huang", "Kaizhu", ""]]}, {"id": "1712.02009", "submitter": "Adityanand Guntuboyina", "authors": "Sujayam Saha and Adityanand Guntuboyina", "title": "On the nonparametric maximum likelihood estimator for Gaussian location\n  mixture densities with application to Gaussian denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Nonparametric Maximum Likelihood Estimator (NPMLE) for\nestimating Gaussian location mixture densities in $d$-dimensions from\nindependent observations. Unlike usual likelihood-based methods for fitting\nmixtures, NPMLEs are based on convex optimization. We prove finite sample\nresults on the Hellinger accuracy of every NPMLE. Our results imply, in\nparticular, that every NPMLE achieves near parametric risk (up to logarithmic\nmultiplicative factors) when the true density is a discrete Gaussian mixture\nwithout any prior information on the number of mixture components. NPMLEs can\nnaturally be used to yield empirical Bayes estimates of the Oracle Bayes\nestimator in the Gaussian denoising problem. We prove bounds for the accuracy\nof the empirical Bayes estimate as an approximation to the Oracle Bayes\nestimator. Here our results imply that the empirical Bayes estimator performs\nat nearly the optimal level (up to logarithmic multiplicative factors) for\ndenoising in clustering situations without any prior knowledge of the number of\nclusters.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 02:30:08 GMT"}, {"version": "v2", "created": "Sun, 7 Jul 2019 23:36:07 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Saha", "Sujayam", ""], ["Guntuboyina", "Adityanand", ""]]}, {"id": "1712.02029", "submitter": "Aditya Devarakonda", "authors": "Aditya Devarakonda, Maxim Naumov, Michael Garland", "title": "AdaBatch: Adaptive Batch Sizes for Training Deep Neural Networks", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural networks with Stochastic Gradient Descent, or its\nvariants, requires careful choice of both learning rate and batch size. While\nsmaller batch sizes generally converge in fewer training epochs, larger batch\nsizes offer more parallelism and hence better computational efficiency. We have\ndeveloped a new training approach that, rather than statically choosing a\nsingle batch size for all epochs, adaptively increases the batch size during\nthe training process. Our method delivers the convergence rate of small batch\nsizes while achieving performance similar to large batch sizes. We analyse our\napproach using the standard AlexNet, ResNet, and VGG networks operating on the\npopular CIFAR-10, CIFAR-100, and ImageNet datasets. Our results demonstrate\nthat learning with adaptive batch sizes can improve performance by factors of\nup to 6.25 on 4 NVIDIA Tesla P100 GPUs while changing accuracy by less than 1%\nrelative to training with fixed batch sizes.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 04:19:14 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 04:26:45 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Devarakonda", "Aditya", ""], ["Naumov", "Maxim", ""], ["Garland", "Michael", ""]]}, {"id": "1712.02034", "submitter": "Garrett Goh", "authors": "Garrett B. Goh, Nathan O. Hodas, Charles Siegel, Abhinav Vishnu", "title": "SMILES2Vec: An Interpretable General-Purpose Deep Neural Network for\n  Predicting Chemical Properties", "comments": "Submitted to SIGKDD 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chemical databases store information in text representations, and the SMILES\nformat is a universal standard used in many cheminformatics software. Encoded\nin each SMILES string is structural information that can be used to predict\ncomplex chemical properties. In this work, we develop SMILES2vec, a deep RNN\nthat automatically learns features from SMILES to predict chemical properties,\nwithout the need for additional explicit feature engineering. Using Bayesian\noptimization methods to tune the network architecture, we show that an\noptimized SMILES2vec model can serve as a general-purpose neural network for\npredicting distinct chemical properties including toxicity, activity,\nsolubility and solvation energy, while also outperforming contemporary MLP\nneural networks that uses engineered features. Furthermore, we demonstrate\nproof-of-concept of interpretability by developing an explanation mask that\nlocalizes on the most important characters used in making a prediction. When\ntested on the solubility dataset, it identified specific parts of a chemical\nthat is consistent with established first-principles knowledge with an accuracy\nof 88%. Our work demonstrates that neural networks can learn technically\naccurate chemical concept and provide state-of-the-art accuracy, making\ninterpretable deep neural networks a useful tool of relevance to the chemical\nindustry.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 04:29:28 GMT"}, {"version": "v2", "created": "Sun, 18 Mar 2018 13:50:32 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Goh", "Garrett B.", ""], ["Hodas", "Nathan O.", ""], ["Siegel", "Charles", ""], ["Vishnu", "Abhinav", ""]]}, {"id": "1712.02046", "submitter": "Borui Wang", "authors": "Borui Wang, Geoffrey Gordon", "title": "Learning General Latent-Variable Graphical Models with Predictive Belief\n  Propagation", "comments": "In AAAI 2020 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning general latent-variable probabilistic graphical models is a key\ntheoretical challenge in machine learning and artificial intelligence. All\nprevious methods, including the EM algorithm and the spectral algorithms, face\nsevere limitations that largely restrict their applicability and affect their\nperformance. In order to overcome these limitations, in this paper we introduce\na novel formulation of message-passing inference over junction trees named\npredictive belief propagation, and propose a new learning and inference\nalgorithm for general latent-variable graphical models based on this\nformulation. Our proposed algorithm reduces the hard parameter learning problem\ninto a sequence of supervised learning problems, and unifies the learning of\ndifferent kinds of latent graphical models into a single learning framework,\nwhich is local-optima-free and statistically consistent. We then give a proof\nof the correctness of our algorithm and show in experiments on both synthetic\nand real datasets that our algorithm significantly outperforms both the EM\nalgorithm and the spectral algorithm while also being orders of magnitude\nfaster to compute.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 05:38:25 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 14:50:06 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Wang", "Borui", ""], ["Gordon", "Geoffrey", ""]]}, {"id": "1712.02083", "submitter": "David Barmherzig", "authors": "David Barmherzig and Ju Sun", "title": "A Local Analysis of Block Coordinate Descent for Gaussian Phase\n  Retrieval", "comments": "10th NIPS Workshop on Optimization for Machine Learning (NIPS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While convergence of the Alternating Direction Method of Multipliers (ADMM)\non convex problems is well studied, convergence on nonconvex problems is only\npartially understood. In this paper, we consider the Gaussian phase retrieval\nproblem, formulated as a linear constrained optimization problem with a\nbiconvex objective. The particular structure allows for a novel application of\nthe ADMM. It can be shown that the dual variable is zero at the global\nminimizer. This motivates the analysis of a block coordinate descent algorithm,\nwhich is equivalent to the ADMM with the dual variable fixed to be zero. We\nshow that the block coordinate descent algorithm converges to the global\nminimizer at a linear rate, when starting from a deterministically achievable\ninitialization point.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 08:38:24 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Barmherzig", "David", ""], ["Sun", "Ju", ""]]}, {"id": "1712.02154", "submitter": "Sebastian Stabinger MSc", "authors": "Sebastian Stabinger, Antonio Rodriguez-Sanchez", "title": "Guided Labeling using Convolutional Neural Networks", "comments": "Under review for CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last couple of years, deep learning and especially convolutional\nneural networks have become one of the work horses of computer vision. One\nlimiting factor for the applicability of supervised deep learning to more areas\nis the need for large, manually labeled datasets. In this paper we propose an\neasy to implement method we call guided labeling, which automatically\ndetermines which samples from an unlabeled dataset should be labeled. We show\nthat using this procedure, the amount of samples that need to be labeled is\nreduced considerably in comparison to labeling images arbitrarily.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 12:18:24 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Stabinger", "Sebastian", ""], ["Rodriguez-Sanchez", "Antonio", ""]]}, {"id": "1712.02162", "submitter": "Chaopeng Shen", "authors": "Chaopeng Shen", "title": "A trans-disciplinary review of deep learning research for water\n  resources scientists", "comments": null, "journal-ref": "Water Resources Research, 2018", "doi": "10.1029/2018WR022643", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL), a new-generation of artificial neural network research,\nhas transformed industries, daily lives and various scientific disciplines in\nrecent years. DL represents significant progress in the ability of neural\nnetworks to automatically engineer problem-relevant features and capture highly\ncomplex data distributions. I argue that DL can help address several major new\nand old challenges facing research in water sciences such as\ninter-disciplinarity, data discoverability, hydrologic scaling, equifinality,\nand needs for parameter regionalization. This review paper is intended to\nprovide water resources scientists and hydrologists in particular with a simple\ntechnical overview, trans-disciplinary progress update, and a source of\ninspiration about the relevance of DL to water. The review reveals that various\nphysical and geoscientific disciplines have utilized DL to address data\nchallenges, improve efficiency, and gain scientific insights. DL is especially\nsuited for information extraction from image-like data and sequential data.\nTechniques and experiences presented in other disciplines are of high relevance\nto water research. Meanwhile, less noticed is that DL may also serve as a\nscientific exploratory tool. A new area termed 'AI neuroscience,' where\nscientists interpret the decision process of deep networks and derive insights,\nhas been born. This budding sub-discipline has demonstrated methods including\ncorrelation-based analysis, inversion of network-extracted features,\nreduced-order approximations by interpretable models, and attribution of\nnetwork decisions to inputs. Moreover, DL can also use data to condition\nneurons that mimic problem-specific fundamental organizing units, thus\nrevealing emergent behaviors of these units. Vast opportunities exist for DL to\npropel advances in water sciences.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 12:44:27 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 14:15:13 GMT"}, {"version": "v3", "created": "Fri, 24 Aug 2018 15:12:46 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Shen", "Chaopeng", ""]]}, {"id": "1712.02195", "submitter": "Ranjan Maitra", "authors": "Alejandro Murua and Ranjan Maitra", "title": "Fast spatial inference in the homogeneous Ising model", "comments": "18 pages, 1 figure, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ising model is important in statistical modeling and inference in many\napplications, however its normalizing constant, mean number of active vertices\nand mean spin interaction are intractable. We provide accurate approximations\nthat make it possible to calculate these quantities numerically. Simulation\nstudies indicate good performance when compared to Markov Chain Monte Carlo\nmethods and at a tiny fraction of the time. The methodology is also used to\nperform Bayesian inference in a functional Magnetic Resonance Imaging\nactivation detection experiment.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 14:24:34 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 00:10:33 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Murua", "Alejandro", ""], ["Maitra", "Ranjan", ""]]}, {"id": "1712.02225", "submitter": "Xuelin Qian", "authors": "Xuelin Qian, Yanwei Fu, Tao Xiang, Wenxuan Wang, Jie Qiu, Yang Wu,\n  Yu-Gang Jiang, Xiangyang Xue", "title": "Pose-Normalized Image Generation for Person Re-identification", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person Re-identification (re-id) faces two major challenges: the lack of\ncross-view paired training data and learning discriminative identity-sensitive\nand view-invariant features in the presence of large pose variations. In this\nwork, we address both problems by proposing a novel deep person image\ngeneration model for synthesizing realistic person images conditional on the\npose. The model is based on a generative adversarial network (GAN) designed\nspecifically for pose normalization in re-id, thus termed pose-normalization\nGAN (PN-GAN). With the synthesized images, we can learn a new type of deep\nre-id feature free of the influence of pose variations. We show that this\nfeature is strong on its own and complementary to features learned with the\noriginal images. Importantly, under the transfer learning setting, we show that\nour model generalizes well to any new re-id dataset without the need for\ncollecting any training data for model fine-tuning. The model thus has the\npotential to make re-id model truly scalable.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 15:18:53 GMT"}, {"version": "v2", "created": "Wed, 10 Jan 2018 04:55:01 GMT"}, {"version": "v3", "created": "Thu, 18 Jan 2018 00:28:00 GMT"}, {"version": "v4", "created": "Fri, 2 Feb 2018 06:59:45 GMT"}, {"version": "v5", "created": "Tue, 13 Feb 2018 06:22:12 GMT"}, {"version": "v6", "created": "Wed, 25 Apr 2018 05:57:05 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Qian", "Xuelin", ""], ["Fu", "Yanwei", ""], ["Xiang", "Tao", ""], ["Wang", "Wenxuan", ""], ["Qiu", "Jie", ""], ["Wu", "Yang", ""], ["Jiang", "Yu-Gang", ""], ["Xue", "Xiangyang", ""]]}, {"id": "1712.02259", "submitter": "Nicolas Bousquet", "authors": "Nicolas Thiebaut, Antoine Simoulin, Karl Neuberger, Issam Ibnouhsein,\n  Nicolas Bousquet, Nathalie Reix, S\\'ebastien Moli\\`ere, Carole Mathelin", "title": "An innovative solution for breast cancer textual big data analysis", "comments": "In submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The digitalization of stored information in hospitals now allows for the\nexploitation of medical data in text format, as electronic health records\n(EHRs), initially gathered for other purposes than epidemiology. Manual search\nand analysis operations on such data become tedious. In recent years, the use\nof natural language processing (NLP) tools was highlighted to automatize the\nextraction of information contained in EHRs, structure it and perform\nstatistical analysis on this structured information. The main difficulties with\nthe existing approaches is the requirement of synonyms or ontology\ndictionaries, that are mostly available in English only and do not include\nlocal or custom notations. In this work, a team composed of oncologists as\ndomain experts and data scientists develop a custom NLP-based system to process\nand structure textual clinical reports of patients suffering from breast\ncancer. The tool relies on the combination of standard text mining techniques\nand an advanced synonym detection method. It allows for a global analysis by\nretrieval of indicators such as medical history, tumor characteristics,\ntherapeutic responses, recurrences and prognosis. The versatility of the method\nallows to obtain easily new indicators, thus opening up the way for\nretrospective studies with a substantial reduction of the amount of manual\nwork. With no need for biomedical annotators or pre-defined ontologies, this\nlanguage-agnostic method reached an good extraction accuracy for several\nconcepts of interest, according to a comparison with a manually structured\nfile, without requiring any existing corpus with local or new notations.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 16:18:31 GMT"}], "update_date": "2021-01-04", "authors_parsed": [["Thiebaut", "Nicolas", ""], ["Simoulin", "Antoine", ""], ["Neuberger", "Karl", ""], ["Ibnouhsein", "Issam", ""], ["Bousquet", "Nicolas", ""], ["Reix", "Nathalie", ""], ["Moli\u00e8re", "S\u00e9bastien", ""], ["Mathelin", "Carole", ""]]}, {"id": "1712.02270", "submitter": "Xiaoyong Pan", "authors": "Xiaoyong Pan and Junchi Yan", "title": "Attention based convolutional neural network for predicting RNA-protein\n  binding sites", "comments": null, "journal-ref": "NIPS 2017 Computational Biology Workshop", "doi": null, "report-no": null, "categories": "q-bio.GN cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  RNA-binding proteins (RBPs) play crucial roles in many biological processes,\ne.g. gene regulation. Computational identification of RBP binding sites on RNAs\nare urgently needed. In particular, RBPs bind to RNAs by recognizing sequence\nmotifs. Thus, fast locating those motifs on RNA sequences is crucial and\ntime-efficient for determining whether the RNAs interact with the RBPs or not.\nIn this study, we present an attention based convolutional neural network,\niDeepA, to predict RNA-protein binding sites from raw RNA sequences. We first\nencode RNA sequences into one-hot encoding. Next, we design a deep learning\nmodel with a convolutional neural network (CNN) and an attention mechanism,\nwhich automatically search for important positions, e.g. binding motifs, to\nlearn discriminant high-level features for predicting RBP binding sites. We\nevaluate iDeepA on publicly gold-standard RBP binding sites derived from\nCLIP-seq data. The results demonstrate iDeepA achieves comparable performance\nwith other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 16:33:29 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Pan", "Xiaoyong", ""], ["Yan", "Junchi", ""]]}, {"id": "1712.02311", "submitter": "Victor Veitch", "authors": "Victor Veitch, Ekansh Sharma, Zacharie Naulet, and Daniel M. Roy", "title": "Exchangeable modelling of relational data: checking sparsity, train-test\n  splitting, and sparse exchangeable Poisson matrix factorization", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variety of machine learning tasks---e.g., matrix factorization, topic\nmodelling, and feature allocation---can be viewed as learning the parameters of\na probability distribution over bipartite graphs. Recently, a new class of\nmodels for networks, the sparse exchangeable graphs, have been introduced to\nresolve some important pathologies of traditional approaches to statistical\nnetwork modelling; most notably, the inability to model sparsity (in the\nasymptotic sense). The present paper explains some practical insights arising\nfrom this work. We first show how to check if sparsity is relevant for\nmodelling a given (fixed size) dataset by using network subsampling to identify\na simple signature of sparsity. We discuss the implications of the (sparse)\nexchangeable subsampling theory for test-train dataset splitting; we argue\ncommon approaches can lead to biased results, and we propose a principled\nalternative. Finally, we study sparse exchangeable Poisson matrix factorization\nas a worked example. In particular, we show how to adapt mean field variational\ninference to the sparse exchangeable setting, allowing us to scale inference to\nhuge datasets.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 18:20:14 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Veitch", "Victor", ""], ["Sharma", "Ekansh", ""], ["Naulet", "Zacharie", ""], ["Roy", "Daniel M.", ""]]}, {"id": "1712.02328", "submitter": "Omid Poursaeed", "authors": "Omid Poursaeed, Isay Katsman, Bicheng Gao, Serge Belongie", "title": "Generative Adversarial Perturbations", "comments": "CVPR 2018, camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose novel generative models for creating adversarial\nexamples, slightly perturbed images resembling natural images but maliciously\ncrafted to fool pre-trained models. We present trainable deep neural networks\nfor transforming images to adversarial perturbations. Our proposed models can\nproduce image-agnostic and image-dependent perturbations for both targeted and\nnon-targeted attacks. We also demonstrate that similar architectures can\nachieve impressive results in fooling classification and semantic segmentation\nmodels, obviating the need for hand-crafting attack methods for each task.\nUsing extensive experiments on challenging high-resolution datasets such as\nImageNet and Cityscapes, we show that our perturbations achieve high fooling\nrates with small perturbation norms. Moreover, our attacks are considerably\nfaster than current iterative methods at inference time.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 18:52:12 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2018 01:18:08 GMT"}, {"version": "v3", "created": "Fri, 6 Jul 2018 06:50:03 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Poursaeed", "Omid", ""], ["Katsman", "Isay", ""], ["Gao", "Bicheng", ""], ["Belongie", "Serge", ""]]}, {"id": "1712.02330", "submitter": "Tatjana Chavdarova", "authors": "Tatjana Chavdarova, Fran\\c{c}ois Fleuret", "title": "SGAN: An Alternative Training of Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Generative Adversarial Networks (GANs) have demonstrated impressive\nperformance for data synthesis, and are now used in a wide range of computer\nvision tasks. In spite of this success, they gained a reputation for being\ndifficult to train, what results in a time-consuming and human-involved\ndevelopment process to use them.\n  We consider an alternative training process, named SGAN, in which several\nadversarial \"local\" pairs of networks are trained independently so that a\n\"global\" supervising pair of networks can be trained against them. The goal is\nto train the global pair with the corresponding ensemble opponent for improved\nperformances in terms of mode coverage. This approach aims at increasing the\nchances that learning will not stop for the global pair, preventing both to be\ntrapped in an unsatisfactory local minimum, or to face oscillations often\nobserved in practice. To guarantee the latter, the global pair never affects\nthe local ones.\n  The rules of SGAN training are thus as follows: the global generator and\ndiscriminator are trained using the local discriminators and generators,\nrespectively, whereas the local networks are trained with their fixed local\nopponent.\n  Experimental results on both toy and real-world problems demonstrate that\nthis approach outperforms standard training in terms of better mitigating mode\ncollapse, stability while converging and that it surprisingly, increases the\nconvergence speed as well.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 18:52:21 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Chavdarova", "Tatjana", ""], ["Fleuret", "Fran\u00e7ois", ""]]}, {"id": "1712.02369", "submitter": "Lirong Xue", "authors": "Lirong Xue, Samory Kpotufe", "title": "Achieving the time of $1$-NN, but the accuracy of $k$-NN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple approach which, given distributed computing resources,\ncan nearly achieve the accuracy of $k$-NN prediction, while matching (or\nimproving) the faster prediction time of $1$-NN. The approach consists of\naggregating denoised $1$-NN predictors over a small number of distributed\nsubsamples. We show, both theoretically and experimentally, that small\nsubsample sizes suffice to attain similar performance as $k$-NN, without\nsacrificing the computational efficiency of $1$-NN.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 19:02:00 GMT"}, {"version": "v2", "created": "Fri, 22 Dec 2017 01:47:29 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Xue", "Lirong", ""], ["Kpotufe", "Samory", ""]]}, {"id": "1712.02390", "submitter": "Guodong Zhang", "authors": "Guodong Zhang and Shengyang Sun and David Duvenaud and Roger Grosse", "title": "Noisy Natural Gradient as Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Bayesian neural nets combine the flexibility of deep learning\nwith Bayesian uncertainty estimation. Unfortunately, there is a tradeoff\nbetween cheap but simple variational families (e.g.~fully factorized) or\nexpensive and complicated inference procedures. We show that natural gradient\nascent with adaptive weight noise implicitly fits a variational posterior to\nmaximize the evidence lower bound (ELBO). This insight allows us to train\nfull-covariance, fully factorized, or matrix-variate Gaussian variational\nposteriors using noisy versions of natural gradient, Adam, and K-FAC,\nrespectively, making it possible to scale up to modern-size ConvNets. On\nstandard regression benchmarks, our noisy K-FAC algorithm makes better\npredictions and matches Hamiltonian Monte Carlo's predictive variances better\nthan existing methods. Its improved uncertainty estimates lead to more\nefficient exploration in active learning, and intrinsic motivation for\nreinforcement learning.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 19:43:47 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 06:48:49 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Zhang", "Guodong", ""], ["Sun", "Shengyang", ""], ["Duvenaud", "David", ""], ["Grosse", "Roger", ""]]}, {"id": "1712.02412", "submitter": "Guo Yu", "authors": "Guo Yu and Jacob Bien", "title": "Estimating the error variance in a high-dimensional linear model", "comments": "Biometrika(2019)", "journal-ref": null, "doi": "10.1093/biomet/asz017", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lasso has been studied extensively as a tool for estimating the\ncoefficient vector in the high-dimensional linear model; however, considerably\nless is known about estimating the error variance in this context. In this\npaper, we propose the natural lasso estimator for the error variance, which\nmaximizes a penalized likelihood objective. A key aspect of the natural lasso\nis that the likelihood is expressed in terms of the natural parameterization of\nthe multiparameter exponential family of a Gaussian with unknown mean and\nvariance. The result is a remarkably simple estimator of the error variance\nwith provably good performance in terms of mean squared error. These\ntheoretical results do not require placing any assumptions on the design matrix\nor the true regression coefficients. We also propose a companion estimator,\ncalled the organic lasso, which theoretically does not require tuning of the\nregularization parameter. Both estimators do well empirically compared to\npreexisting methods, especially in settings where successful recovery of the\ntrue support of the coefficient vector is hard. Finally, we show that existing\nmethods can do well under fewer assumptions than previously known, thus\nproviding a fuller story about the problem of estimating the error variance in\nhigh-dimensional linear models.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 21:16:35 GMT"}, {"version": "v2", "created": "Wed, 17 Jan 2018 18:06:46 GMT"}, {"version": "v3", "created": "Fri, 19 Jul 2019 09:15:05 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Yu", "Guo", ""], ["Bien", "Jacob", ""]]}, {"id": "1712.02432", "submitter": "Cecilia Clementi", "authors": "Lorenzo Boninsegna, Feliks N\\\"uske, Cecilia Clementi", "title": "Sparse learning of stochastic dynamic equations", "comments": null, "journal-ref": null, "doi": "10.1063/1.5018409", "report-no": null, "categories": "stat.ML cond-mat.stat-mech", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid increase of available data for complex systems, there is great\ninterest in the extraction of physically relevant information from massive\ndatasets. Recently, a framework called Sparse Identification of Nonlinear\nDynamics (SINDy) has been introduced to identify the governing equations of\ndynamical systems from simulation data. In this study, we extend SINDy to\nstochastic dynamical systems, which are frequently used to model biophysical\nprocesses. We prove the asymptotic correctness of stochastics SINDy in the\ninfinite data limit, both in the original and projected variables. We discuss\nalgorithms to solve the sparse regression problem arising from the practical\nimplementation of SINDy, and show that cross validation is an essential tool to\ndetermine the right level of sparsity. We demonstrate the proposed methodology\non two test systems, namely, the diffusion in a one-dimensional potential, and\nthe projected dynamics of a two-dimensional diffusion process.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 22:51:54 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Boninsegna", "Lorenzo", ""], ["N\u00fcske", "Feliks", ""], ["Clementi", "Cecilia", ""]]}, {"id": "1712.02441", "submitter": "Farzaneh S. Fard", "authors": "Farzaneh S. Fard, and Thomas P. Trappenberg", "title": "A Novel Model for Arbitration between Planning and Habitual Control\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well established that humans decision making and instrumental control\nuses multiple systems, some which use habitual action selection and some which\nrequire deliberate planning. Deliberate planning systems use predictions of\naction-outcomes using an internal model of the agent's environment, while\nhabitual action selection systems learn to automate by repeating previously\nrewarded actions. Habitual control is computationally efficient but may be\ninflexible in changing environments. Conversely, deliberate planning may be\ncomputationally expensive, but flexible in dynamic environments. This paper\nproposes a general architecture comprising both control paradigms by\nintroducing an arbitrator that controls which subsystem is used at any time.\nThis system is implemented for a target-reaching task with a simulated\ntwo-joint robotic arm that comprises a supervised internal model and deep\nreinforcement learning. Through permutation of target-reaching conditions, we\ndemonstrate that the proposed is capable of rapidly learning kinematics of the\nsystem without a priori knowledge, and is robust to (A) changing environmental\nreward and kinematics, and (B) occluded vision. The arbitrator model is\ncompared to exclusive deliberate planning with the internal model and exclusive\nhabitual control instances of the model. The results show how such a model can\nharness the benefits of both systems, using fast decisions in reliable\ncircumstances while optimizing performance in changing environments. In\naddition, the proposed model learns very fast. Finally, the system which\nincludes internal models is able to reach the target under the visual\nocclusion, while the pure habitual system is unable to operate sufficiently\nunder such conditions.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 23:33:40 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Fard", "Farzaneh S.", ""], ["Trappenberg", "Thomas P.", ""]]}, {"id": "1712.02488", "submitter": "Yunpeng Li", "authors": "Yunpeng Li, Ivan Kiskin, Davide Zilli, Marianne Sinka, Henry Chan,\n  Kathy Willis, Stephen Roberts", "title": "Cost-sensitive detection with variational autoencoders for environmental\n  acoustic sensing", "comments": "Presented at the NIPS 2017 Workshop on Machine Learning for Audio\n  Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Environmental acoustic sensing involves the retrieval and processing of audio\nsignals to better understand our surroundings. While large-scale acoustic data\nmake manual analysis infeasible, they provide a suitable playground for machine\nlearning approaches. Most existing machine learning techniques developed for\nenvironmental acoustic sensing do not provide flexible control of the trade-off\nbetween the false positive rate and the false negative rate. This paper\npresents a cost-sensitive classification paradigm, in which the\nhyper-parameters of classifiers and the structure of variational autoencoders\nare selected in a principled Neyman-Pearson framework. We examine the\nperformance of the proposed approach using a dataset from the HumBug project\nwhich aims to detect the presence of mosquitoes using sound collected by simple\nembedded devices.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 04:35:39 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Li", "Yunpeng", ""], ["Kiskin", "Ivan", ""], ["Zilli", "Davide", ""], ["Sinka", "Marianne", ""], ["Chan", "Henry", ""], ["Willis", "Kathy", ""], ["Roberts", "Stephen", ""]]}, {"id": "1712.02512", "submitter": "Lucas Roberts", "authors": "Lucas Roberts, Leo Razoumov, Lin Su, Yuyang Wang", "title": "Gini-regularized Optimal Transport with an Application to\n  Spatio-Temporal Forecasting", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapidly growing product lines and services require a finer-granularity\nforecast that considers geographic locales. However the open question remains,\nhow to assess the quality of a spatio-temporal forecast? In this manuscript we\nintroduce a metric to evaluate spatio-temporal forecasts. This metric is based\non an Opti- mal Transport (OT) problem. The metric we propose is a constrained\nOT objec- tive function using the Gini impurity function as a regularizer. We\ndemonstrate through computer experiments both the qualitative and the\nquantitative charac- teristics of the Gini regularized OT problem. Moreover, we\nshow that the Gini regularized OT problem converges to the classical OT\nproblem, when the Gini regularized problem is considered as a function of\n{\\lambda}, the regularization parame-ter. The convergence to the classical OT\nsolution is faster than the state-of-the-art Entropic-regularized OT[Cuturi,\n2013] and results in a numerically more stable algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 06:58:45 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Roberts", "Lucas", ""], ["Razoumov", "Leo", ""], ["Su", "Lin", ""], ["Wang", "Yuyang", ""]]}, {"id": "1712.02519", "submitter": "Chao Gao", "authors": "Fengshuo Zhang and Chao Gao", "title": "Convergence Rates of Variational Posterior Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study convergence rates of variational posterior distributions for\nnonparametric and high-dimensional inference. We formulate general conditions\non prior, likelihood, and variational class that characterize the convergence\nrates. Under similar \"prior mass and testing\" conditions considered in the\nliterature, the rate is found to be the sum of two terms. The first term stands\nfor the convergence rate of the true posterior distribution, and the second\nterm is contributed by the variational approximation error. For a class of\npriors that admit the structure of a mixture of product measures, we propose a\nnovel prior mass condition, under which the variational approximation error of\nthe mean-field class is dominated by convergence rate of the true posterior. We\ndemonstrate the applicability of our general results for various models, prior\ndistributions and variational classes by deriving convergence rates of the\ncorresponding variational posteriors.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 07:30:16 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 02:15:27 GMT"}, {"version": "v3", "created": "Fri, 2 Feb 2018 17:38:55 GMT"}, {"version": "v4", "created": "Mon, 17 Jun 2019 04:44:38 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Zhang", "Fengshuo", ""], ["Gao", "Chao", ""]]}, {"id": "1712.02527", "submitter": "Jianqiao Wangni", "authors": "Jianqiao Wangni, Jingwei Zhuo, Jun Zhu", "title": "Learning Random Fourier Features by Hybrid Constrained Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The kernel embedding algorithm is an important component for adapting kernel\nmethods to large datasets. Since the algorithm consumes a major computation\ncost in the testing phase, we propose a novel teacher-learner framework of\nlearning computation-efficient kernel embeddings from specific data. In the\nframework, the high-precision embeddings (teacher) transfer the data\ninformation to the computation-efficient kernel embeddings (learner). We\njointly select informative embedding functions and pursue an orthogonal\ntransformation between two embeddings. We propose a novel approach of\nconstrained variational expectation maximization (CVEM), where the alternate\ndirection method of multiplier (ADMM) is applied over a nonconvex domain in the\nmaximization step. We also propose two specific formulations based on the\nprevalent Random Fourier Feature (RFF), the masked and blocked version of\nComputation-Efficient RFF (CERF), by imposing a random binary mask or a block\nstructure on the transformation matrix. By empirical studies of several\napplications on different real-world datasets, we demonstrate that the CERF\nsignificantly improves the performance of kernel methods upon the RFF, under\ncertain arithmetic operation requirements, and suitable for structured matrix\nmultiplication in Fastfood type algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 08:07:26 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Wangni", "Jianqiao", ""], ["Zhuo", "Jingwei", ""], ["Zhu", "Jun", ""]]}, {"id": "1712.02609", "submitter": "Carles Riera Molina", "authors": "Carles Roger Riera Molina and Oriol Pujol Vila", "title": "Solving internal covariate shift in deep learning with linked neurons", "comments": "Submitted to CVPR 2018. Code available at\n  https://github.com/blauigris/linked_neurons", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a novel solution to the problem of internal covariate\nshift and dying neurons using the concept of linked neurons. We define the\nneuron linkage in terms of two constraints: first, all neuron activations in\nthe linkage must have the same operating point. That is to say, all of them\nshare input weights. Secondly, a set of neurons is linked if and only if there\nis at least one member of the linkage that has a non-zero gradient in regard to\nthe input of the activation function. This means that for any input in the\nactivation function, there is at least one member of the linkage that operates\nin a non-flat and non-zero area. This simple change has profound implications\nin the network learning dynamics. In this article we explore the consequences\nof this proposal and show that by using this kind of units, internal covariate\nshift is implicitly solved. As a result of this, the use of linked neurons\nallows to train arbitrarily large networks without any architectural or\nalgorithmic trick, effectively removing the need of using re-normalization\nschemes such as Batch Normalization, which leads to halving the required\ntraining time. It also solves the problem of the need for standarized input\ndata. Results show that the units using the linkage not only do effectively\nsolve the aforementioned problems, but are also a competitive alternative with\nrespect to state-of-the-art with very promising results.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 13:26:26 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Molina", "Carles Roger Riera", ""], ["Vila", "Oriol Pujol", ""]]}, {"id": "1712.02629", "submitter": "Beyza Ermis Ms", "authors": "Beyza Ermis, Ali Taylan Cemgil", "title": "Differentially Private Variational Dropout", "comments": "arXiv admin note: substantial text overlap with arXiv:1712.01665", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks with their large number of parameters are highly\nflexible learning systems. The high flexibility in such networks brings with\nsome serious problems such as overfitting, and regularization is used to\naddress this problem. A currently popular and effective regularization\ntechnique for controlling the overfitting is dropout. Often, large data\ncollections required for neural networks contain sensitive information such as\nthe medical histories of patients, and the privacy of the training data should\nbe protected. In this paper, we modify the recently proposed variational\ndropout technique which provided an elegant Bayesian interpretation to dropout,\nand show that the intrinsic noise in the variational dropout can be exploited\nto obtain a degree of differential privacy. The iterative nature of training\nneural networks presents a challenge for privacy-preserving estimation since\nmultiple iterations increase the amount of noise added. We overcome this by\nusing a relaxed notion of differential privacy, called concentrated\ndifferential privacy, which provides tighter estimates on the overall privacy\nloss. We demonstrate the accuracy of our privacy-preserving variational dropout\nalgorithm on benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 21:32:27 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 20:05:27 GMT"}, {"version": "v3", "created": "Sat, 16 Dec 2017 10:41:02 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Ermis", "Beyza", ""], ["Cemgil", "Ali Taylan", ""]]}, {"id": "1712.02640", "submitter": "St\\'ephane Ga\\\"iffas", "authors": "Alain Virouleau, Agathe Guilloux, St\\'ephane Ga\\\"iffas, Malgorzata\n  Bogdan", "title": "High-dimensional robust regression and outliers detection with SLOPE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problems of outliers detection and robust regression in a\nhigh-dimensional setting are fundamental in statistics, and have numerous\napplications. Following a recent set of works providing methods for\nsimultaneous robust regression and outliers detection, we consider in this\npaper a model of linear regression with individual intercepts, in a\nhigh-dimensional setting. We introduce a new procedure for simultaneous\nestimation of the linear regression coefficients and intercepts, using two\ndedicated sorted-$\\ell_1$ penalizations, also called SLOPE. We develop a\ncomplete theory for this problem: first, we provide sharp upper bounds on the\nstatistical estimation error of both the vector of individual intercepts and\nregression coefficients. Second, we give an asymptotic control on the False\nDiscovery Rate (FDR) and statistical power for support selection of the\nindividual intercepts. As a consequence, this paper is the first to introduce a\nprocedure with guaranteed FDR and statistical power control for outliers\ndetection under the mean-shift model. Numerical illustrations, with a\ncomparison to recent alternative approaches, are provided on both simulated and\nseveral real-world datasets. Experiments are conducted using an open-source\nsoftware written in Python and C++.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 14:25:02 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Virouleau", "Alain", ""], ["Guilloux", "Agathe", ""], ["Ga\u00efffas", "St\u00e9phane", ""], ["Bogdan", "Malgorzata", ""]]}, {"id": "1712.02658", "submitter": "Ga\\\"elle Bonnet Loosli", "authors": "Ga\\\"elle Loosli and Hattoibe Aboubacar", "title": "Using SVDD in SimpleMKL for 3D-Shapes Filtering", "comments": "9 pages, 6 figures, conference : https://cap2014.sciencesconf.org/", "journal-ref": "CAp, conference d'apprentissage, July 2014, Saint-Etienne, France,\n  pp.84-92", "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the adaptation of Support Vector Data Description (SVDD)\nto the multiple kernel case (MK-SVDD), based on SimpleMKL. It also introduces a\nvariant called Slim-MK-SVDD that is able to produce a tighter frontier around\nthe data. For the sake of comparison, the equivalent methods are also developed\nfor One-Class SVM, known to be very similar to SVDD for certain shapes of\nkernels.\n  Those algorithms are illustrated in the context of 3D-shapes filtering and\noutliers detection. For the 3D-shapes problem, the objective is to be able to\nselect a sub-category of 3D-shapes, each sub-category being learned with our\nalgorithm in order to create a filter. For outliers detection, we apply the\nproposed algorithms for unsupervised outliers detection as well as for the\nsupervised case.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 14:56:53 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Loosli", "Ga\u00eblle", ""], ["Aboubacar", "Hattoibe", ""]]}, {"id": "1712.02675", "submitter": "Andreas Svensson", "authors": "Andreas Svensson, Dave Zachariah, Thomas B. Sch\\\"on", "title": "How consistent is my model with the data? Information-Theoretic Model\n  Check", "comments": "The title has been updated, but no other significant changes have\n  been made from the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The choice of model class is fundamental in statistical learning and system\nidentification, no matter whether the class is derived from physical principles\nor is a generic black-box. We develop a method to evaluate the specified model\nclass by assessing its capability of reproducing data that is similar to the\nobserved data record. This model check is based on the information-theoretic\nproperties of models viewed as data generators and is applicable to e.g.\nsequential data and nonlinear dynamical models. The method can be understood as\na specific two-sided posterior predictive test. We apply the\ninformation-theoretic model check to both synthetic and real data and compare\nit with a classical whiteness test.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 15:40:17 GMT"}, {"version": "v2", "created": "Tue, 19 Dec 2017 08:55:47 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Svensson", "Andreas", ""], ["Zachariah", "Dave", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1712.02679", "submitter": "Chia-Yu Chen", "authors": "Chia-Yu Chen, Jungwook Choi, Daniel Brand, Ankur Agrawal, Wei Zhang,\n  Kailash Gopalakrishnan", "title": "AdaComp : Adaptive Residual Gradient Compression for Data-Parallel\n  Distributed Training", "comments": "IBM Research AI, 9 pages, 7 figures, AAAI18 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highly distributed training of Deep Neural Networks (DNNs) on future compute\nplatforms (offering 100 of TeraOps/s of computational capacity) is expected to\nbe severely communication constrained. To overcome this limitation, new\ngradient compression techniques are needed that are computationally friendly,\napplicable to a wide variety of layers seen in Deep Neural Networks and\nadaptable to variations in network architectures as well as their\nhyper-parameters. In this paper we introduce a novel technique - the Adaptive\nResidual Gradient Compression (AdaComp) scheme. AdaComp is based on localized\nselection of gradient residues and automatically tunes the compression rate\ndepending on local activity. We show excellent results on a wide spectrum of\nstate of the art Deep Learning models in multiple domains (vision, speech,\nlanguage), datasets (MNIST, CIFAR10, ImageNet, BN50, Shakespeare), optimizers\n(SGD with momentum, Adam) and network parameters (number of learners,\nminibatch-size etc.). Exploiting both sparsity and quantization, we demonstrate\nend-to-end compression rates of ~200X for fully-connected and recurrent layers,\nand ~40X for convolutional layers, without any noticeable degradation in model\naccuracies.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 15:44:19 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Chen", "Chia-Yu", ""], ["Choi", "Jungwook", ""], ["Brand", "Daniel", ""], ["Agrawal", "Ankur", ""], ["Zhang", "Wei", ""], ["Gopalakrishnan", "Kailash", ""]]}, {"id": "1712.02734", "submitter": "Garrett Goh", "authors": "Garrett B. Goh, Charles Siegel, Abhinav Vishnu, Nathan O. Hodas", "title": "Using Rule-Based Labels for Weak Supervised Learning: A ChemNet for\n  Transferable Chemical Property Prediction", "comments": "Submitted to SIGKDD 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With access to large datasets, deep neural networks (DNN) have achieved\nhuman-level accuracy in image and speech recognition tasks. However, in\nchemistry, data is inherently small and fragmented. In this work, we develop an\napproach of using rule-based knowledge for training ChemNet, a transferable and\ngeneralizable deep neural network for chemical property prediction that learns\nin a weak-supervised manner from large unlabeled chemical databases. When\ncoupled with transfer learning approaches to predict other smaller datasets for\nchemical properties that it was not originally trained on, we show that\nChemNet's accuracy outperforms contemporary DNN models that were trained using\nconventional supervised learning. Furthermore, we demonstrate that the ChemNet\npre-training approach is equally effective on both CNN (Chemception) and RNN\n(SMILES2vec) models, indicating that this approach is network architecture\nagnostic and is effective across multiple data modalities. Our results indicate\na pre-trained ChemNet that incorporates chemistry domain knowledge, enables the\ndevelopment of generalizable neural networks for more accurate prediction of\nnovel chemical properties.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 17:25:48 GMT"}, {"version": "v2", "created": "Sun, 18 Mar 2018 13:50:02 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Goh", "Garrett B.", ""], ["Siegel", "Charles", ""], ["Vishnu", "Abhinav", ""], ["Hodas", "Nathan O.", ""]]}, {"id": "1712.02743", "submitter": "Thomas Hehn", "authors": "Thomas Hehn and Fred A. Hamprecht", "title": "End-to-end Learning of Deterministic Decision Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional decision trees have a number of favorable properties, including\ninterpretability, a small computational footprint and the ability to learn from\nlittle training data. However, they lack a key quality that has helped fuel the\ndeep learning revolution: that of being end-to-end trainable, and to learn from\nscratch those features that best allow to solve a given supervised learning\nproblem. Recent work (Kontschieder 2015) has addressed this deficit, but at the\ncost of losing a main attractive trait of decision trees: the fact that each\nsample is routed along a small subset of tree nodes only. We here propose a\nmodel and Expectation-Maximization training scheme for decision trees that are\nfully probabilistic at train time, but after a deterministic annealing process\nbecome deterministic at test time. We also analyze the learned oblique split\nparameters on image datasets and show that Neural Networks can be trained at\neach split node. In summary, we present the first end-to-end learning scheme\nfor deterministic decision trees and present results on par with or superior to\npublished standard oblique decision tree algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 17:40:25 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Hehn", "Thomas", ""], ["Hamprecht", "Fred A.", ""]]}, {"id": "1712.02779", "submitter": "Dimitris Tsipras", "authors": "Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt,\n  Aleksander Madry", "title": "Exploring the Landscape of Spatial Robustness", "comments": "ICML 2019. Presented in NIPS 2017 Workshop on Machine Learning and\n  Computer Security as \"A Rotation and a Translation Suffice: Fooling CNNs with\n  Simple Transformations.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of adversarial robustness has so far largely focused on\nperturbations bound in p-norms. However, state-of-the-art models turn out to be\nalso vulnerable to other, more natural classes of perturbations such as\ntranslations and rotations. In this work, we thoroughly investigate the\nvulnerability of neural network--based classifiers to rotations and\ntranslations. While data augmentation offers relatively small robustness, we\nuse ideas from robust optimization and test-time input aggregation to\nsignificantly improve robustness. Finally we find that, in contrast to the\np-norm case, first-order methods cannot reliably find worst-case perturbations.\nThis highlights spatial robustness as a fundamentally different setting\nrequiring additional study. Code available at\nhttps://github.com/MadryLab/adversarial_spatial and\nhttps://github.com/MadryLab/spatial-pytorch.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 18:53:52 GMT"}, {"version": "v2", "created": "Mon, 11 Dec 2017 12:00:50 GMT"}, {"version": "v3", "created": "Tue, 13 Feb 2018 18:33:22 GMT"}, {"version": "v4", "created": "Mon, 16 Sep 2019 04:38:13 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Engstrom", "Logan", ""], ["Tran", "Brandon", ""], ["Tsipras", "Dimitris", ""], ["Schmidt", "Ludwig", ""], ["Madry", "Aleksander", ""]]}, {"id": "1712.02831", "submitter": "Seyed Mehran Kazemi", "authors": "Seyed Mehran Kazemi and David Poole", "title": "RelNN: A Deep Neural Model for Relational Learning", "comments": "9 pages, 8 figures, accepted at AAAI-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Statistical relational AI (StarAI) aims at reasoning and learning in noisy\ndomains described in terms of objects and relationships by combining\nprobability with first-order logic. With huge advances in deep learning in the\ncurrent years, combining deep networks with first-order logic has been the\nfocus of several recent studies. Many of the existing attempts, however, only\nfocus on relations and ignore object properties. The attempts that do consider\nobject properties are limited in terms of modelling power or scalability. In\nthis paper, we develop relational neural networks (RelNNs) by adding hidden\nlayers to relational logistic regression (the relational counterpart of\nlogistic regression). We learn latent properties for objects both directly and\nthrough general rules. Back-propagation is used for training these models. A\nmodular, layer-wise architecture facilitates utilizing the techniques developed\nwithin deep learning community to our architecture. Initial experiments on\neight tasks over three real-world datasets show that RelNNs are promising\nmodels for relational learning.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 19:40:01 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Kazemi", "Seyed Mehran", ""], ["Poole", "David", ""]]}, {"id": "1712.02854", "submitter": "Lukas Mosser", "authors": "Lukas Mosser, Olivier Dubrule, Martin J. Blunt", "title": "Stochastic reconstruction of an oolitic limestone by generative\n  adversarial networks", "comments": "22 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.geo-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic image reconstruction is a key part of modern digital rock physics\nand materials analysis that aims to create numerous representative samples of\nmaterial micro-structures for upscaling, numerical computation of effective\nproperties and uncertainty quantification. We present a method of\nthree-dimensional stochastic image reconstruction based on generative\nadversarial neural networks (GANs). GANs represent a framework of unsupervised\nlearning methods that require no a priori inference of the probability\ndistribution associated with the training data. Using a fully convolutional\nneural network allows fast sampling of large volumetric images.We apply a GAN\nbased workflow of network training and image generation to an oolitic Ketton\nlimestone micro-CT dataset. Minkowski functionals, effective permeability as\nwell as velocity distributions of simulated flow within the acquired images are\ncompared with the synthetic reconstructions generated by the deep neural\nnetwork. While our results show that GANs allow a fast and accurate\nreconstruction of the evaluated image dataset, we address a number of open\nquestions and challenges involved in the evaluation of generative network-based\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 20:21:01 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Mosser", "Lukas", ""], ["Dubrule", "Olivier", ""], ["Blunt", "Martin J.", ""]]}, {"id": "1712.02902", "submitter": "Valerio Perrone", "authors": "Valerio Perrone, Rodolphe Jenatton, Matthias Seeger, Cedric Archambeau", "title": "Multiple Adaptive Bayesian Linear Regression for Scalable Bayesian\n  Optimization with Warm Start", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization (BO) is a model-based approach for gradient-free\nblack-box function optimization. Typically, BO is powered by a Gaussian process\n(GP), whose algorithmic complexity is cubic in the number of evaluations.\nHence, GP-based BO cannot leverage large amounts of past or related function\nevaluations, for example, to warm start the BO procedure. We develop a multiple\nadaptive Bayesian linear regression model as a scalable alternative whose\ncomplexity is linear in the number of observations. The multiple Bayesian\nlinear regression models are coupled through a shared feedforward neural\nnetwork, which learns a joint representation and transfers knowledge across\nmachine learning problems.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 01:00:19 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Perrone", "Valerio", ""], ["Jenatton", "Rodolphe", ""], ["Seeger", "Matthias", ""], ["Archambeau", "Cedric", ""]]}, {"id": "1712.02903", "submitter": "Panagiotis Traganitis", "authors": "Panagiotis A. Traganitis, Alba Pag\\`es-Zamora, Georgios B. Giannakis", "title": "Blind Multiclass Ensemble Classification", "comments": "To appear in IEEE Transactions in Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2018.2860562", "report-no": null, "categories": "stat.ML cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rising interest in pattern recognition and data analytics has spurred the\ndevelopment of innovative machine learning algorithms and tools. However, as\neach algorithm has its strengths and limitations, one is motivated to\njudiciously fuse multiple algorithms in order to find the \"best\" performing\none, for a given dataset. Ensemble learning aims at such high-performance\nmeta-algorithm, by combining the outputs from multiple algorithms. The present\nwork introduces a blind scheme for learning from ensembles of classifiers,\nusing a moment matching method that leverages joint tensor and matrix\nfactorization. Blind refers to the combiner who has no knowledge of the\nground-truth labels that each classifier has been trained on. A rigorous\nperformance analysis is derived and the proposed scheme is evaluated on\nsynthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 01:01:37 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 21:20:08 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Traganitis", "Panagiotis A.", ""], ["Pag\u00e8s-Zamora", "Alba", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1712.02950", "submitter": "Casey Chu", "authors": "Casey Chu, Andrey Zhmoginov, Mark Sandler", "title": "CycleGAN, a Master of Steganography", "comments": "NIPS 2017, workshop on Machine Deception", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CycleGAN (Zhu et al. 2017) is one recent successful approach to learn a\ntransformation between two image distributions. In a series of experiments, we\ndemonstrate an intriguing property of the model: CycleGAN learns to \"hide\"\ninformation about a source image into the images it generates in a nearly\nimperceptible, high-frequency signal. This trick ensures that the generator can\nrecover the original sample and thus satisfy the cyclic consistency\nrequirement, while the generated image remains realistic. We connect this\nphenomenon with adversarial attacks by viewing CycleGAN's training procedure as\ntraining a generator of adversarial examples and demonstrate that the cyclic\nconsistency loss causes CycleGAN to be especially vulnerable to adversarial\nattacks.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 06:07:52 GMT"}, {"version": "v2", "created": "Sat, 16 Dec 2017 07:42:46 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Chu", "Casey", ""], ["Zhmoginov", "Andrey", ""], ["Sandler", "Mark", ""]]}, {"id": "1712.03010", "submitter": "Farnood Salehi", "authors": "Farnood Salehi, Patrick Thiran, L. Elisa Celis", "title": "Coordinate Descent with Bandit Sampling", "comments": "appearing at NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coordinate descent methods usually minimize a cost function by updating a\nrandom decision variable (corresponding to one coordinate) at a time. Ideally,\nwe would update the decision variable that yields the largest decrease in the\ncost function. However, finding this coordinate would require checking all of\nthem, which would effectively negate the improvement in computational\ntractability that coordinate descent is intended to afford. To address this, we\npropose a new adaptive method for selecting a coordinate. First, we find a\nlower bound on the amount the cost function decreases when a coordinate is\nupdated. We then use a multi-armed bandit algorithm to learn which coordinates\nresult in the largest lower bound by interleaving this learning with\nconventional coordinate descent updates except that the coordinate is selected\nproportionately to the expected decrease. We show that our approach improves\nthe convergence of coordinate descent methods both theoretically and\nexperimentally.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 10:23:30 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 15:25:14 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Salehi", "Farnood", ""], ["Thiran", "Patrick", ""], ["Celis", "L. Elisa", ""]]}, {"id": "1712.03133", "submitter": "Kartik Audhkhasi", "authors": "Kartik Audhkhasi, Brian Kingsbury, Bhuvana Ramabhadran, George Saon,\n  Michael Picheny", "title": "Building competitive direct acoustics-to-word models for English\n  conversational speech recognition", "comments": "Submitted to IEEE International Conference on Acoustics, Speech and\n  Signal Processing (ICASSP), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct acoustics-to-word (A2W) models in the end-to-end paradigm have\nreceived increasing attention compared to conventional sub-word based automatic\nspeech recognition models using phones, characters, or context-dependent hidden\nMarkov model states. This is because A2W models recognize words from speech\nwithout any decoder, pronunciation lexicon, or externally-trained language\nmodel, making training and decoding with such models simple. Prior work has\nshown that A2W models require orders of magnitude more training data in order\nto perform comparably to conventional models. Our work also showed this\naccuracy gap when using the English Switchboard-Fisher data set. This paper\ndescribes a recipe to train an A2W model that closes this gap and is at-par\nwith state-of-the-art sub-word based models. We achieve a word error rate of\n8.8%/13.9% on the Hub5-2000 Switchboard/CallHome test sets without any decoder\nor language model. We find that model initialization, training data order, and\nregularization have the most impact on the A2W model performance. Next, we\npresent a joint word-character A2W model that learns to first spell the word\nand then recognize it. This model provides a rich output to the user instead of\nsimple word hypotheses, making it especially useful in the case of words unseen\nor rarely-seen during training.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 15:43:21 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Audhkhasi", "Kartik", ""], ["Kingsbury", "Brian", ""], ["Ramabhadran", "Bhuvana", ""], ["Saon", "George", ""], ["Picheny", "Michael", ""]]}, {"id": "1712.03134", "submitter": "Xue Lu", "authors": "Xue Lu, Niall Adams, Nikolas Kantas", "title": "On Adaptive Estimation for Dynamic Bernoulli Bandits", "comments": "Added another AFF version of the standard UCB algorithm; modified the\n  AFF-TS algorithm; in the numerical studies, added comparisons to SW-UCB and\n  D-UCB", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multi-armed bandit (MAB) problem is a classic example of the\nexploration-exploitation dilemma. It is concerned with maximising the total\nrewards for a gambler by sequentially pulling an arm from a multi-armed slot\nmachine where each arm is associated with a reward distribution. In static\nMABs, the reward distributions do not change over time, while in dynamic MABs,\neach arm's reward distribution can change, and the optimal arm can switch over\ntime. Motivated by many real applications where rewards are binary, we focus on\ndynamic Bernoulli bandits. Standard methods like $\\epsilon$-Greedy and Upper\nConfidence Bound (UCB), which rely on the sample mean estimator, often fail to\ntrack changes in the underlying reward for dynamic problems. In this paper, we\novercome the shortcoming of slow response to change by deploying adaptive\nestimation in the standard methods and propose a new family of algorithms,\nwhich are adaptive versions of $\\epsilon$-Greedy, UCB, and Thompson sampling.\nThese new methods are simple and easy to implement. Moreover, they do not\nrequire any prior knowledge about the dynamic reward process, which is\nimportant for real applications. We examine the new algorithms numerically in\ndifferent scenarios and the results show solid improvements of our algorithms\nin dynamic environments.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 15:45:28 GMT"}, {"version": "v2", "created": "Tue, 15 May 2018 01:34:57 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Lu", "Xue", ""], ["Adams", "Niall", ""], ["Kantas", "Nikolas", ""]]}, {"id": "1712.03281", "submitter": "Mohammadreza Soltani", "authors": "Mohammadreza Soltani and Chinmay Hegde", "title": "Fast Low-Rank Matrix Estimation without the Condition Number", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the general problem of optimizing a convex function\n$F(L)$ over the set of $p \\times p$ matrices, subject to rank constraints on\n$L$. However, existing first-order methods for solving such problems either are\ntoo slow to converge, or require multiple invocations of singular value\ndecompositions. On the other hand, factorization-based non-convex algorithms,\nwhile being much faster, require stringent assumptions on the \\emph{condition\nnumber} of the optimum. In this paper, we provide a novel algorithmic framework\nthat achieves the best of both worlds: asymptotically as fast as factorization\nmethods, while requiring no dependency on the condition number.\n  We instantiate our general framework for three important matrix estimation\nproblems that impact several practical applications; (i) a \\emph{nonlinear}\nvariant of affine rank minimization, (ii) logistic PCA, and (iii) precision\nmatrix estimation in probabilistic graphical model learning. We then derive\nexplicit bounds on the sample complexity as well as the running time of our\napproach, and show that it achieves the best possible bounds for both cases. We\nalso provide an extensive range of experimental results, and demonstrate that\nour algorithm provides a very attractive tradeoff between estimation accuracy\nand running time.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 21:10:42 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Soltani", "Mohammadreza", ""], ["Hegde", "Chinmay", ""]]}, {"id": "1712.03298", "submitter": "Ying Xiao", "authors": "Shankar Krishnan, Ying Xiao, Rif A. Saurous", "title": "Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progress in deep learning is slowed by the days or weeks it takes to train\nlarge models. The natural solution of using more hardware is limited by\ndiminishing returns, and leads to inefficient use of additional resources. In\nthis paper, we present a large batch, stochastic optimization algorithm that is\nboth faster than widely used algorithms for fixed amounts of computation, and\nalso scales up substantially better as more computational resources become\navailable. Our algorithm implicitly computes the inverse Hessian of each\nmini-batch to produce descent directions; we do so without either an explicit\napproximation to the Hessian or Hessian-vector products. We demonstrate the\neffectiveness of our algorithm by successfully training large ImageNet models\n(Inception-V3, Resnet-50, Resnet-101 and Inception-Resnet-V2) with mini-batch\nsizes of up to 32000 with no loss in validation error relative to current\nbaselines, and no increase in the total number of steps. At smaller mini-batch\nsizes, our optimizer improves the validation error in these models by 0.8-0.9%.\nAlternatively, we can trade off this accuracy to reduce the number of training\nsteps needed by roughly 10-30%. Our work is practical and easily usable by\nothers -- only one hyperparameter (learning rate) needs tuning, and\nfurthermore, the algorithm is as computationally cheap as the commonly used\nAdam optimizer.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 22:26:58 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Krishnan", "Shankar", ""], ["Xiao", "Ying", ""], ["Saurous", "Rif A.", ""]]}, {"id": "1712.03337", "submitter": "Shihua Zhang", "authors": "Chihao Zhang, Shihua Zhang", "title": "Bayesian Joint Matrix Decomposition for Data Integration with\n  Heterogeneous Noise", "comments": "14 pages, 7 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix decomposition is a popular and fundamental approach in machine\nlearning and data mining. It has been successfully applied into various fields.\nMost matrix decomposition methods focus on decomposing a data matrix from one\nsingle source. However, it is common that data are from different sources with\nheterogeneous noise. A few of matrix decomposition methods have been extended\nfor such multi-view data integration and pattern discovery. While only few\nmethods were designed to consider the heterogeneity of noise in such multi-view\ndata for data integration explicitly. To this end, we propose a joint matrix\ndecomposition framework (BJMD), which models the heterogeneity of noise by\nGaussian distribution in a Bayesian framework. We develop two algorithms to\nsolve this model: one is a variational Bayesian inference algorithm, which\nmakes full use of the posterior distribution; and another is a maximum a\nposterior algorithm, which is more scalable and can be easily paralleled.\nExtensive experiments on synthetic and real-world datasets demonstrate that\nBJMD considering the heterogeneity of noise is superior or competitive to the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 9 Dec 2017 02:54:17 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Zhang", "Chihao", ""], ["Zhang", "Shihua", ""]]}, {"id": "1712.03351", "submitter": "Boyang Deng", "authors": "Boyang Deng, Junjie Yan, Dahua Lin", "title": "Peephole: Predicting Network Performance Before Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quest for performant networks has been a significant force that drives\nthe advancements of deep learning in recent years. While rewarding, improving\nnetwork design has never been an easy journey. The large design space combined\nwith the tremendous cost required for network training poses a major obstacle\nto this endeavor. In this work, we propose a new approach to this problem,\nnamely, predicting the performance of a network before training, based on its\narchitecture. Specifically, we develop a unified way to encode individual\nlayers into vectors and bring them together to form an integrated description\nvia LSTM. Taking advantage of the recurrent network's strong expressive power,\nthis method can reliably predict the performances of various network\narchitectures. Our empirical studies showed that it not only achieved accurate\npredictions but also produced consistent rankings across datasets -- a key\ndesideratum in performance prediction.\n", "versions": [{"version": "v1", "created": "Sat, 9 Dec 2017 07:50:27 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Deng", "Boyang", ""], ["Yan", "Junjie", ""], ["Lin", "Dahua", ""]]}, {"id": "1712.03353", "submitter": "Adam McCarthy", "authors": "Adam McCarthy, Blanca Rodriguez, and Ana Minchole", "title": "Variational Inference over Non-differentiable Cardiac Simulators using\n  Bayesian Optimization", "comments": "Workshops on Deep Learning for Physical Sciences and Machine Learning\n  4 Health, NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing inference over simulators is generally intractable as their\nruntime means we cannot compute a marginal likelihood. We develop a\nlikelihood-free inference method to infer parameters for a cardiac simulator,\nwhich replicates electrical flow through the heart to the body surface. We\nimprove the fit of a state-of-the-art simulator to an electrocardiogram (ECG)\nrecorded from a real patient.\n", "versions": [{"version": "v1", "created": "Sat, 9 Dec 2017 08:11:17 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["McCarthy", "Adam", ""], ["Rodriguez", "Blanca", ""], ["Minchole", "Ana", ""]]}, {"id": "1712.03412", "submitter": "Huiming Zhang", "authors": "Huiming Zhang, Jinzhu Jia", "title": "Elastic-net Regularized High-dimensional Negative Binomial Regression:\n  Consistency and Weak Signals Detection", "comments": "27 pages", "journal-ref": "Statistica Sinica,32(1),1-27 (2022)", "doi": "10.5705/ss.202019.0315", "report-no": null, "categories": "stat.ML math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a sparse negative binomial regression (NBR) for count data by\nshowing the non-asymptotic advantages of using the elastic-net estimator. Two\ntypes of oracle inequalities are derived for the NBR's elastic-net estimates by\nusing the Compatibility Factor Condition and the Stabil Condition. The second\ntype of oracle inequality is for the random design and can be extended to many\n$\\ell_1 + \\ell_2$ regularized M-estimations, with the corresponding empirical\nprocess having stochastic Lipschitz properties. We derive the concentration\ninequality for the suprema empirical processes for the weighted sum of negative\nbinomial variables to show some high--probability events. We apply the method\nby showing the sign consistency, provided that the nonzero components in the\ntrue sparse vector are larger than a proper choice of the weakest signal\ndetection threshold. In the second application, we show the grouping effect\ninequality with high probability. Third, under some assumptions for a design\nmatrix, we can recover the true variable set with a high probability if the\nweakest signal detection threshold is large than the turning parameter up to a\nknown constant. Lastly, we briefly discuss the de-biased elastic-net estimator,\nand numerical studies are given to support the proposal.\n", "versions": [{"version": "v1", "created": "Sat, 9 Dec 2017 17:08:08 GMT"}, {"version": "v2", "created": "Fri, 26 Jan 2018 17:19:01 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2020 17:58:50 GMT"}, {"version": "v4", "created": "Fri, 11 Jun 2021 02:07:41 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Zhang", "Huiming", ""], ["Jia", "Jinzhu", ""]]}, {"id": "1712.03428", "submitter": "Matteo Pirotta", "authors": "Matteo Pirotta and Marcello Restelli", "title": "Cost-Sensitive Approach to Batch Size Adaptation for Gradient Descent", "comments": "Presented at the NIPS workshop on Optimizing the Optimizers.\n  Barcelona, Spain, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel approach to automatically determine the\nbatch size in stochastic gradient descent methods. The choice of the batch size\ninduces a trade-off between the accuracy of the gradient estimate and the cost\nin terms of samples of each update. We propose to determine the batch size by\noptimizing the ratio between a lower bound to a linear or quadratic Taylor\napproximation of the expected improvement and the number of samples used to\nestimate the gradient. The performance of the proposed approach is empirically\ncompared with related methods on popular classification tasks.\n  The work was presented at the NIPS workshop on Optimizing the Optimizers.\nBarcelona, Spain, 2016.\n", "versions": [{"version": "v1", "created": "Sat, 9 Dec 2017 19:59:25 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Pirotta", "Matteo", ""], ["Restelli", "Marcello", ""]]}, {"id": "1712.03471", "submitter": "Zahra Shakeri", "authors": "Zahra Shakeri, Anand D. Sarwate, and Waheed U. Bajwa", "title": "Identifiability of Kronecker-structured Dictionaries for Tensor Data", "comments": "16 pages, to appear in IEEE Journal of Special Topics in Signal\n  Processing", "journal-ref": "IEEE J. Sel. Topics Signal Processing, vol. 12, no. 5, pp.\n  1047-1062, Oct. 2018", "doi": "10.1109/JSTSP.2018.2838092", "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper derives sufficient conditions for local recovery of coordinate\ndictionaries comprising a Kronecker-structured dictionary that is used for\nrepresenting $K$th-order tensor data. Tensor observations are assumed to be\ngenerated from a Kronecker-structured dictionary multiplied by sparse\ncoefficient tensors that follow the separable sparsity model. This work\nprovides sufficient conditions on the underlying coordinate dictionaries,\ncoefficient and noise distributions, and number of samples that guarantee\nrecovery of the individual coordinate dictionaries up to a specified error, as\na local minimum of the objective function, with high probability. In\nparticular, the sample complexity to recover $K$ coordinate dictionaries with\ndimensions $m_k \\times p_k$ up to estimation error $\\varepsilon_k$ is shown to\nbe $\\max_{k \\in [K]}\\mathcal{O}(m_kp_k^3\\varepsilon_k^{-2})$.\n", "versions": [{"version": "v1", "created": "Sun, 10 Dec 2017 05:28:48 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 13:58:35 GMT"}, {"version": "v3", "created": "Fri, 25 May 2018 13:47:49 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Shakeri", "Zahra", ""], ["Sarwate", "Anand D.", ""], ["Bajwa", "Waheed U.", ""]]}, {"id": "1712.03480", "submitter": "Edgar Xi", "authors": "Edgar Xi, Selina Bing, Yang Jin", "title": "Capsule Network Performance on Complex Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, convolutional neural networks (CNN) have played an important\nrole in the field of deep learning. Variants of CNN's have proven to be very\nsuccessful in classification tasks across different domains. However, there are\ntwo big drawbacks to CNN's: their failure to take into account of important\nspatial hierarchies between features, and their lack of rotational invariance.\nAs long as certain key features of an object are present in the test data,\nCNN's classify the test data as the object, disregarding features' relative\nspatial orientation to each other. This causes false positives. The lack of\nrotational invariance in CNN's would cause the network to incorrectly assign\nthe object another label, causing false negatives. To address this concern,\nHinton et al. propose a novel type of neural network using the concept of\ncapsules in a recent paper. With the use of dynamic routing and reconstruction\nregularization, the capsule network model would be both rotation invariant and\nspatially aware. The capsule network has shown its potential by achieving a\nstate-of-the-art result of 0.25% test error on MNIST without data augmentation\nsuch as rotation and scaling, better than the previous baseline of 0.39%. To\nfurther test out the application of capsule networks on data with higher\ndimensionality, we attempt to find the best set of configurations that yield\nthe optimal test error on CIFAR10 dataset.\n", "versions": [{"version": "v1", "created": "Sun, 10 Dec 2017 07:50:04 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Xi", "Edgar", ""], ["Bing", "Selina", ""], ["Jin", "Yang", ""]]}, {"id": "1712.03483", "submitter": "Pedro Silva", "authors": "Pedro Silva, Sepehr Akhavan-Masouleh and Li Li", "title": "Improving Malware Detection Accuracy by Extracting Icon Information", "comments": "Full version. IEEE MIPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting PE malware files is now commonly approached using statistical and\nmachine learning models. While these models commonly use features extracted\nfrom the structure of PE files, we propose that icons from these files can also\nhelp better predict malware. We propose an innovative machine learning approach\nto extract information from icons. Our proposed approach consists of two steps:\n1) extracting icon features using summary statics, histogram of gradients\n(HOG), and a convolutional autoencoder, 2) clustering icons based on the\nextracted icon features. Using publicly available data and by using machine\nlearning experiments, we show our proposed icon clusters significantly boost\nthe efficacy of malware prediction models. In particular, our experiments show\nan average accuracy increase of 10% when icon clusters are used in the\nprediction model.\n", "versions": [{"version": "v1", "created": "Sun, 10 Dec 2017 08:15:12 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Silva", "Pedro", ""], ["Akhavan-Masouleh", "Sepehr", ""], ["Li", "Li", ""]]}, {"id": "1712.03541", "submitter": "Abien Fred Agarap", "authors": "Abien Fred Agarap", "title": "An Architecture Combining Convolutional Neural Network (CNN) and Support\n  Vector Machine (SVM) for Image Classification", "comments": "4 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Convolutional neural networks (CNNs) are similar to \"ordinary\" neural\nnetworks in the sense that they are made up of hidden layers consisting of\nneurons with \"learnable\" parameters. These neurons receive inputs, performs a\ndot product, and then follows it with a non-linearity. The whole network\nexpresses the mapping between raw image pixels and their class scores.\nConventionally, the Softmax function is the classifier used at the last layer\nof this network. However, there have been studies (Alalshekmubarak and Smith,\n2013; Agarap, 2017; Tang, 2013) conducted to challenge this norm. The cited\nstudies introduce the usage of linear support vector machine (SVM) in an\nartificial neural network architecture. This project is yet another take on the\nsubject, and is inspired by (Tang, 2013). Empirical data has shown that the\nCNN-SVM model was able to achieve a test accuracy of ~99.04% using the MNIST\ndataset (LeCun, Cortes, and Burges, 2010). On the other hand, the CNN-Softmax\nwas able to achieve a test accuracy of ~99.23% using the same dataset. Both\nmodels were also tested on the recently-published Fashion-MNIST dataset (Xiao,\nRasul, and Vollgraf, 2017), which is suppose to be a more difficult image\nclassification dataset than MNIST (Zalandoresearch, 2017). This proved to be\nthe case as CNN-SVM reached a test accuracy of ~90.72%, while the CNN-Softmax\nreached a test accuracy of ~91.86%. The said results may be improved if data\npreprocessing techniques were employed on the datasets, and if the base CNN\nmodel was a relatively more sophisticated than the one used in this study.\n", "versions": [{"version": "v1", "created": "Sun, 10 Dec 2017 14:50:28 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 06:25:08 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Agarap", "Abien Fred", ""]]}, {"id": "1712.03553", "submitter": "Jason Poulos", "authors": "Jason Poulos, Shuxi Zeng", "title": "RNN-based counterfactual prediction, with an application to homestead\n  policy and public schooling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method for estimating the effect of a policy\nintervention on an outcome over time. We train recurrent neural networks (RNNs)\non the history of control unit outcomes to learn a useful representation for\npredicting future outcomes. The learned representation of control units is then\napplied to the treated units for predicting counterfactual outcomes. RNNs are\nspecifically structured to exploit temporal dependencies in panel data, and are\nable to learn negative and nonlinear interactions between control unit\noutcomes. We apply the method to the problem of estimating the long-run impact\nof U.S. homestead policy on public school spending.\n", "versions": [{"version": "v1", "created": "Sun, 10 Dec 2017 16:00:18 GMT"}, {"version": "v2", "created": "Thu, 4 Jan 2018 23:14:22 GMT"}, {"version": "v3", "created": "Mon, 26 Mar 2018 18:47:57 GMT"}, {"version": "v4", "created": "Fri, 11 May 2018 17:51:34 GMT"}, {"version": "v5", "created": "Sat, 13 Apr 2019 00:30:08 GMT"}, {"version": "v6", "created": "Wed, 30 Sep 2020 20:14:51 GMT"}, {"version": "v7", "created": "Mon, 17 May 2021 13:56:51 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Poulos", "Jason", ""], ["Zeng", "Shuxi", ""]]}, {"id": "1712.03563", "submitter": "Bo Wu", "authors": "Bo Wu, Yang Liu, Bo Lang, Lei Huang", "title": "DGCNN: Disordered Graph Convolutional Neural Network Based on the\n  Gaussian Mixture Model", "comments": "16 pages,8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) can be applied to graph similarity\nmatching, in which case they are called graph CNNs. Graph CNNs are attracting\nincreasing attention due to their effectiveness and efficiency. However, the\nexisting convolution approaches focus only on regular data forms and require\nthe transfer of the graph or key node neighborhoods of the graph into the same\nfixed form. During this transfer process, structural information of the graph\ncan be lost, and some redundant information can be incorporated. To overcome\nthis problem, we propose the disordered graph convolutional neural network\n(DGCNN) based on the mixed Gaussian model, which extends the CNN by adding a\npreprocessing layer called the disordered graph convolutional layer (DGCL). The\nDGCL uses a mixed Gaussian function to realize the mapping between the\nconvolution kernel and the nodes in the neighborhood of the graph. The output\nof the DGCL is the input of the CNN. We further implement a\nbackward-propagation optimization process of the convolutional layer by which\nwe incorporate the feature-learning model of the irregular node neighborhood\nstructure into the network. Thereafter, the optimization of the convolution\nkernel becomes part of the neural network learning process. The DGCNN can\naccept arbitrary scaled and disordered neighborhood graph structures as the\nreceptive fields of CNNs, which reduces information loss during graph\ntransformation. Finally, we perform experiments on multiple standard graph\ndatasets. The results show that the proposed method outperforms the\nstate-of-the-art methods in graph classification and retrieval.\n", "versions": [{"version": "v1", "created": "Sun, 10 Dec 2017 17:38:25 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Wu", "Bo", ""], ["Liu", "Yang", ""], ["Lang", "Bo", ""], ["Huang", "Lei", ""]]}, {"id": "1712.03605", "submitter": "Stefan Depeweg", "authors": "Stefan Depeweg, Jos\\'e Miguel Hern\\'andez-Lobato, Steffen Udluft,\n  Thomas Runkler", "title": "Sensitivity Analysis for Predictive Uncertainty in Bayesian Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a novel sensitivity analysis of input variables for predictive\nepistemic and aleatoric uncertainty. We use Bayesian neural networks with\nlatent variables as a model class and illustrate the usefulness of our\nsensitivity analysis on real-world datasets. Our method increases the\ninterpretability of complex black-box probabilistic models.\n", "versions": [{"version": "v1", "created": "Sun, 10 Dec 2017 22:57:42 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Depeweg", "Stefan", ""], ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""], ["Udluft", "Steffen", ""], ["Runkler", "Thomas", ""]]}, {"id": "1712.03607", "submitter": "Robert Kwiatkowski", "authors": "Robert Kwiatkowski, Oscar Chang", "title": "Gradient Normalization & Depth Based Decay For Deep Learning", "comments": "Results seemed more promising at the time", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a novel method of gradient normalization and decay\nwith respect to depth. Our method leverages the simple concept of normalizing\nall gradients in a deep neural network, and then decaying said gradients with\nrespect to their depth in the network. Our proposed normalization and decay\ntechniques can be used in conjunction with most current state of the art\noptimizers and are a very simple addition to any network. This method, although\nsimple, showed improvements in convergence time on state of the art networks\nsuch as DenseNet and ResNet on image classification tasks, as well as on an\nLSTM for natural language processing tasks.\n", "versions": [{"version": "v1", "created": "Sun, 10 Dec 2017 23:01:13 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 15:56:52 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Kwiatkowski", "Robert", ""], ["Chang", "Oscar", ""]]}, {"id": "1712.03638", "submitter": "Christos Thrampoulidis", "authors": "Christos Thrampoulidis and Ankit Singh Rawat", "title": "Lifting high-dimensional nonlinear models with Gaussian regressors", "comments": "Improved the algorithm and expanded on its motivation; added\n  simulation results", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of recovering a structured signal $\\mathbf{x}_0$ from\nhigh-dimensional data $\\mathbf{y}_i=f(\\mathbf{a}_i^T\\mathbf{x}_0)$ for some\nnonlinear (and potentially unknown) link function $f$, when the regressors\n$\\mathbf{a}_i$ are iid Gaussian. Brillinger (1982) showed that ordinary\nleast-squares estimates $\\mathbf{x}_0$ up to a constant of proportionality\n$\\mu_\\ell$, which depends on $f$. Recently, Plan & Vershynin (2015) extended\nthis result to the high-dimensional setting deriving sharp error bounds for the\ngeneralized Lasso. Unfortunately, both least-squares and the Lasso fail to\nrecover $\\mathbf{x}_0$ when $\\mu_\\ell=0$. For example, this includes all even\nlink functions. We resolve this issue by proposing and analyzing an alternative\nconvex recovery method. In a nutshell, our method treats such link functions as\nif they were linear in a lifted space of higher-dimension. Interestingly, our\nerror analysis captures the effect of both the nonlinearity and the problem's\ngeometry in a few simple summary parameters.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 03:59:03 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 18:57:19 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Thrampoulidis", "Christos", ""], ["Rawat", "Ankit Singh", ""]]}, {"id": "1712.03660", "submitter": "Mustafa Hajij", "authors": "Mustafa Hajij, Basem Assiri, Paul Rosen", "title": "Parallel Mapper", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The construction of Mapper has emerged in the last decade as a powerful and\neffective topological data analysis tool that approximates and generalizes\nother topological summaries, such as the Reeb graph, the contour tree, split,\nand joint trees. In this paper, we study the parallel analysis of the\nconstruction of Mapper. We give a provably correct parallel algorithm to\nexecute Mapper on multiple processors and discuss the performance results that\ncompare our approach to a reference sequential Mapper implementation. We report\nthe performance experiments that demonstrate the efficiency of our method.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 07:02:06 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 02:32:46 GMT"}, {"version": "v3", "created": "Tue, 12 May 2020 01:56:37 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Hajij", "Mustafa", ""], ["Assiri", "Basem", ""], ["Rosen", "Paul", ""]]}, {"id": "1712.03779", "submitter": "Karl Kumbier", "authors": "Bin Yu and Karl Kumbier", "title": "Artificial Intelligence and Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence (AI) is intrinsically data-driven. It calls for the\napplication of statistical concepts through human-machine collaboration during\ngeneration of data, development of algorithms, and evaluation of results. This\npaper discusses how such human-machine collaboration can be approached through\nthe statistical concepts of population, question of interest,\nrepresentativeness of training data, and scrutiny of results (PQRS). The PQRS\nworkflow provides a conceptual framework for integrating statistical ideas with\nhuman input into AI products and research. These ideas include experimental\ndesign principles of randomization and local control as well as the principle\nof stability to gain reproducibility and interpretability of algorithms and\ndata results. We discuss the use of these principles in the contexts of\nself-driving cars, automated medical diagnoses, and examples from the authors'\ncollaborative research.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 02:18:43 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Yu", "Bin", ""], ["Kumbier", "Karl", ""]]}, {"id": "1712.03834", "submitter": "Luiz Gustavo De Andrade Alves", "authors": "Luiz G A Alves, Haroldo V Ribeiro, Francisco A Rodrigues", "title": "Crime prediction through urban metrics and statistical learning", "comments": "Accepted for publication in Physica A", "journal-ref": "Physica A 505, 435-443 (2018)", "doi": "10.1016/j.physa.2018.03.084", "report-no": null, "categories": "physics.soc-ph stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the causes of crime is a longstanding issue in researcher's\nagenda. While it is a hard task to extract causality from data, several linear\nmodels have been proposed to predict crime through the existing correlations\nbetween crime and urban metrics. However, because of non-Gaussian distributions\nand multicollinearity in urban indicators, it is common to find controversial\nconclusions about the influence of some urban indicators on crime. Machine\nlearning ensemble-based algorithms can handle well such problems. Here, we use\na random forest regressor to predict crime and quantify the influence of urban\nindicators on homicides. Our approach can have up to 97% of accuracy on crime\nprediction, and the importance of urban indicators is ranked and clustered in\ngroups of equal influence, which are robust under slightly changes in the data\nsample analyzed. Our results determine the rank of importance of urban\nindicators to predict crime, unveiling that unemployment and illiteracy are the\nmost important variables for describing homicides in Brazilian cities. We\nfurther believe that our approach helps in producing more robust conclusions\nregarding the effects of urban indicators on crime, having potential\napplications for guiding public policies for crime control.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 13:38:23 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2018 12:47:52 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Alves", "Luiz G A", ""], ["Ribeiro", "Haroldo V", ""], ["Rodrigues", "Francisco A", ""]]}, {"id": "1712.03847", "submitter": "Ferenc Husz\\'ar", "authors": "Ferenc Husz\\'ar", "title": "On Quadratic Penalties in Elastic Weight Consolidation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Elastic weight consolidation (EWC, Kirkpatrick et al, 2017) is a novel\nalgorithm designed to safeguard against catastrophic forgetting in neural\nnetworks. EWC can be seen as an approximation to Laplace propagation (Eskin et\nal, 2004), and this view is consistent with the motivation given by Kirkpatrick\net al (2017). In this note, I present an extended derivation that covers the\ncase when there are more than two tasks. I show that the quadratic penalties in\nEWC are inconsistent with this derivation and might lead to double-counting\ndata from earlier tasks.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 15:49:02 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Husz\u00e1r", "Ferenc", ""]]}, {"id": "1712.03878", "submitter": "Vinay Verma Kumar", "authors": "Vinay Kumar Verma, Gundeep Arora, Ashish Mishra, Piyush Rai", "title": "Generalized Zero-Shot Learning via Synthesized Examples", "comments": "Accepted in CVPR'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generative framework for generalized zero-shot learning where\nthe training and test classes are not necessarily disjoint. Built upon a\nvariational autoencoder based architecture, consisting of a probabilistic\nencoder and a probabilistic conditional decoder, our model can generate novel\nexemplars from seen/unseen classes, given their respective class attributes.\nThese exemplars can subsequently be used to train any off-the-shelf\nclassification model. One of the key aspects of our encoder-decoder\narchitecture is a feedback-driven mechanism in which a discriminator (a\nmultivariate regressor) learns to map the generated exemplars to the\ncorresponding class attribute vectors, leading to an improved generator. Our\nmodel's ability to generate and leverage examples from unseen classes to train\nthe classification model naturally helps to mitigate the bias towards\npredicting seen classes in generalized zero-shot learning settings. Through a\ncomprehensive set of experiments, we show that our model outperforms several\nstate-of-the-art methods, on several benchmark datasets, for both standard as\nwell as generalized zero-shot learning.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 16:44:12 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 09:19:01 GMT"}, {"version": "v3", "created": "Mon, 23 Apr 2018 11:20:15 GMT"}, {"version": "v4", "created": "Fri, 8 Jun 2018 16:55:14 GMT"}, {"version": "v5", "created": "Tue, 12 Jun 2018 00:13:53 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Verma", "Vinay Kumar", ""], ["Arora", "Gundeep", ""], ["Mishra", "Ashish", ""], ["Rai", "Piyush", ""]]}, {"id": "1712.03893", "submitter": "Akinori Tanaka", "authors": "Akinori Tanaka, Akio Tomiya", "title": "Towards reduction of autocorrelation in HMC by machine learning", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": "RIKEN-iTHEMS-Report-17", "categories": "hep-lat cond-mat.dis-nn stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose new algorithm to reduce autocorrelation in Markov\nchain Monte-Carlo algorithms for euclidean field theories on the lattice. Our\nproposing algorithm is the Hybrid Monte-Carlo algorithm (HMC) with restricted\nBoltzmann machine. We examine the validity of the algorithm by employing the\nphi-fourth theory in three dimension. We observe reduction of the\nautocorrelation both in symmetric and broken phase as well. Our proposing\nalgorithm provides consistent central values of expectation values of the\naction density and one-point Green's function with ones from the original HMC\nin both the symmetric phase and broken phase within the statistical error. On\nthe other hand, two-point Green's functions have slight difference between one\ncalculated by the HMC and one by our proposing algorithm in the symmetric\nphase. Furthermore, near the criticality, the distribution of the one-point\nGreen's function differs from the one from HMC. We discuss the origin of\ndiscrepancies and its improvement.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 17:12:11 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Tanaka", "Akinori", ""], ["Tomiya", "Akio", ""]]}, {"id": "1712.03999", "submitter": "Brian Dolhansky", "authors": "Brian Dolhansky, Cristian Canton Ferrer", "title": "Eye In-Painting with Exemplar Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel approach to in-painting where the identity of\nthe object to remove or change is preserved and accounted for at inference\ntime: Exemplar GANs (ExGANs). ExGANs are a type of conditional GAN that utilize\nexemplar information to produce high-quality, personalized in painting results.\nWe propose using exemplar information in the form of a reference image of the\nregion to in-paint, or a perceptual code describing that object. Unlike\nprevious conditional GAN formulations, this extra information can be inserted\nat multiple points within the adversarial network, thus increasing its\ndescriptive power. We show that ExGANs can produce photo-realistic personalized\nin-painting results that are both perceptually and semantically plausible by\napplying them to the task of closed to-open eye in-painting in natural\npictures. A new benchmark dataset is also introduced for the task of eye\nin-painting for future comparisons.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 19:40:55 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Dolhansky", "Brian", ""], ["Ferrer", "Cristian Canton", ""]]}, {"id": "1712.04046", "submitter": "Jason Poulos", "authors": "Jason Poulos and Rafael Valle", "title": "Character-Based Handwritten Text Transcription with Attention Networks", "comments": null, "journal-ref": null, "doi": "10.1007/s00521-021-05813-1", "report-no": null, "categories": "cs.CV cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper approaches the task of handwritten text recognition (HTR) with\nattentional encoder-decoder networks trained on sequences of characters, rather\nthan words. We experiment on lines of text from popular handwriting datasets\nand compare different activation functions for the attention mechanism used for\naligning image pixels and target characters. We find that softmax attention\nfocuses heavily on individual characters, while sigmoid attention focuses on\nmultiple characters at each step of the decoding. When the sequence alignment\nis one-to-one, softmax attention is able to learn a more precise alignment at\neach step of the decoding, whereas the alignment generated by sigmoid attention\nis much less precise. When a linear function is used to obtain attention\nweights, the model predicts a character by looking at the entire sequence of\ncharacters and performs poorly because it lacks a precise alignment between the\nsource and target. Future research may explore HTR in natural scene images,\nsince the model is capable of transcribing handwritten text without the need\nfor producing segmentations or bounding boxes of text in images.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 21:57:03 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2019 19:33:31 GMT"}, {"version": "v3", "created": "Wed, 24 Feb 2021 17:00:03 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Poulos", "Jason", ""], ["Valle", "Rafael", ""]]}, {"id": "1712.04086", "submitter": "Sewoong Oh", "authors": "Zinan Lin, Ashish Khetan, Giulia Fanti, Sewoong Oh", "title": "PacGAN: The power of two samples in generative adversarial networks", "comments": "49 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are innovative techniques for learning\ngenerative models of complex data distributions from samples. Despite\nremarkable recent improvements in generating realistic images, one of their\nmajor shortcomings is the fact that in practice, they tend to produce samples\nwith little diversity, even when trained on diverse datasets. This phenomenon,\nknown as mode collapse, has been the main focus of several recent advances in\nGANs. Yet there is little understanding of why mode collapse happens and why\nexisting approaches are able to mitigate mode collapse. We propose a principled\napproach to handling mode collapse, which we call packing. The main idea is to\nmodify the discriminator to make decisions based on multiple samples from the\nsame class, either real or artificially generated. We borrow analysis tools\nfrom binary hypothesis testing---in particular the seminal result of Blackwell\n[Bla53]---to prove a fundamental connection between packing and mode collapse.\nWe show that packing naturally penalizes generators with mode collapse, thereby\nfavoring generator distributions with less mode collapse during the training\nprocess. Numerical experiments on benchmark datasets suggests that packing\nprovides significant improvements in practice as well.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 00:57:52 GMT"}, {"version": "v2", "created": "Sun, 15 Apr 2018 22:45:01 GMT"}, {"version": "v3", "created": "Fri, 2 Nov 2018 04:15:06 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Lin", "Zinan", ""], ["Khetan", "Ashish", ""], ["Fanti", "Giulia", ""], ["Oh", "Sewoong", ""]]}, {"id": "1712.04118", "submitter": "Haitao Zhao", "authors": "Haitao Zhao", "title": "Neural Component Analysis for Fault Detection", "comments": "10 pages,11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is largely adopted for chemical process\nmonitoring and numerous PCA-based systems have been developed to solve various\nfault detection and diagnosis problems. Since PCA-based methods assume that the\nmonitored process is linear, nonlinear PCA models, such as autoencoder models\nand kernel principal component analysis (KPCA), has been proposed and applied\nto nonlinear process monitoring. However, KPCA-based methods need to perform\neigen-decomposition (ED) on the kernel Gram matrix whose dimensions depend on\nthe number of training data. Moreover, prefixed kernel parameters cannot be\nmost effective for different faults which may need different parameters to\nmaximize their respective detection performances. Autoencoder models lack the\nconsideration of orthogonal constraints which is crucial for PCA-based\nalgorithms. To address these problems, this paper proposes a novel nonlinear\nmethod, called neural component analysis (NCA), which intends to train a\nfeedforward neural work with orthogonal constraints such as those used in PCA.\nNCA can adaptively learn its parameters through backpropagation and the\ndimensionality of the nonlinear features has no relationship with the number of\ntraining samples. Extensive experimental results on the Tennessee Eastman (TE)\nbenchmark process show the superiority of NCA in terms of missed detection rate\n(MDR) and false alarm rate (FAR). The source code of NCA can be found in\nhttps://github.com/haitaozhao/Neural-Component-Analysis.git.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 04:11:37 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Zhao", "Haitao", ""]]}, {"id": "1712.04120", "submitter": "Alex Lamb", "authors": "Alex Lamb, Devon Hjelm, Yaroslav Ganin, Joseph Paul Cohen, Aaron\n  Courville, Yoshua Bengio", "title": "GibbsNet: Iterative Adversarial Inference for Deep Graphical Models", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directed latent variable models that formulate the joint distribution as\n$p(x,z) = p(z) p(x \\mid z)$ have the advantage of fast and exact sampling.\nHowever, these models have the weakness of needing to specify $p(z)$, often\nwith a simple fixed prior that limits the expressiveness of the model.\nUndirected latent variable models discard the requirement that $p(z)$ be\nspecified with a prior, yet sampling from them generally requires an iterative\nprocedure such as blocked Gibbs-sampling that may require many steps to draw\nsamples from the joint distribution $p(x, z)$. We propose a novel approach to\nlearning the joint distribution between the data and a latent code which uses\nan adversarially learned iterative procedure to gradually refine the joint\ndistribution, $p(x, z)$, to better match with the data distribution on each\nstep. GibbsNet is the best of both worlds both in theory and in practice.\nAchieving the speed and simplicity of a directed latent variable model, it is\nguaranteed (assuming the adversarial game reaches the virtual training criteria\nglobal minimum) to produce samples from $p(x, z)$ with only a few sampling\niterations. Achieving the expressiveness and flexibility of an undirected\nlatent variable model, GibbsNet does away with the need for an explicit $p(z)$\nand has the ability to do attribute prediction, class-conditional generation,\nand joint image-attribute modeling in a single model which is not trained for\nany of these specific tasks. We show empirically that GibbsNet is able to learn\na more complex $p(z)$ and show that this leads to improved inpainting and\niterative refinement of $p(x, z)$ for dozens of steps and stable generation\nwithout collapse for thousands of steps, despite being trained on only a few\nsteps.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 04:16:52 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Lamb", "Alex", ""], ["Hjelm", "Devon", ""], ["Ganin", "Yaroslav", ""], ["Cohen", "Joseph Paul", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1712.04129", "submitter": "Utkarsh Porwal", "authors": "Utkarsh Porwal, Smruthi Mukund", "title": "Outlier Detection by Consistent Data Selection Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often the challenge associated with tasks like fraud and spam detection[1] is\nthe lack of all likely patterns needed to train suitable supervised learning\nmodels. In order to overcome this limitation, such tasks are attempted as\noutlier or anomaly detection tasks. We also hypothesize that out- liers have\nbehavioral patterns that change over time. Limited data and continuously\nchanging patterns makes learning significantly difficult. In this work we are\nproposing an approach that detects outliers in large data sets by relying on\ndata points that are consistent. The primary contribution of this work is that\nit will quickly help retrieve samples for both consistent and non-outlier data\nsets and is also mindful of new outlier patterns. No prior knowledge of each\nset is required to extract the samples. The method consists of two phases, in\nthe first phase, consistent data points (non- outliers) are retrieved by an\nensemble method of unsupervised clustering techniques and in the second phase a\none class classifier trained on the consistent data point set is ap- plied on\nthe remaining sample set to identify the outliers. The approach is tested on\nthree publicly available data sets and the performance scores are competitive.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 04:57:40 GMT"}, {"version": "v2", "created": "Tue, 21 Aug 2018 04:37:56 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Porwal", "Utkarsh", ""], ["Mukund", "Smruthi", ""]]}, {"id": "1712.04135", "submitter": "Aidin Ferdowsi", "authors": "Aidin Ferdowsi, Ursula Challita, Walid Saad", "title": "Deep Learning for Reliable Mobile Edge Analytics in Intelligent\n  Transportation Systems", "comments": "5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent transportation systems (ITSs) will be a major component of\ntomorrow's smart cities. However, realizing the true potential of ITSs requires\nultra-low latency and reliable data analytics solutions that can combine, in\nreal-time, a heterogeneous mix of data stemming from the ITS network and its\nenvironment. Such data analytics capabilities cannot be provided by\nconventional cloud-centric data processing techniques whose communication and\ncomputing latency can be high. Instead, edge-centric solutions that are\ntailored to the unique ITS environment must be developed. In this paper, an\nedge analytics architecture for ITSs is introduced in which data is processed\nat the vehicle or roadside smart sensor level in order to overcome the ITS\nlatency and reliability challenges. With a higher capability of passengers'\nmobile devices and intra-vehicle processors, such a distributed edge computing\narchitecture can leverage deep learning techniques for reliable mobile sensing\nin ITSs. In this context, the ITS mobile edge analytics challenges pertaining\nto heterogeneous data, autonomous control, vehicular platoon control, and\ncyber-physical security are investigated. Then, different deep learning\nsolutions for such challenges are proposed. The proposed deep learning\nsolutions will enable ITS edge analytics by endowing the ITS devices with\npowerful computer vision and signal processing functions. Preliminary results\nshow that the proposed edge analytics architecture, coupled with the power of\ndeep learning algorithms, can provide a reliable, secure, and truly smart\ntransportation environment.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 05:12:44 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Ferdowsi", "Aidin", ""], ["Challita", "Ursula", ""], ["Saad", "Walid", ""]]}, {"id": "1712.04144", "submitter": "Song Cheng", "authors": "Song Cheng, Jing Chen, Lei Wang", "title": "Information Perspective to Probabilistic Modeling: Boltzmann Machines\n  versus Born Machines", "comments": "7 pages, 4 figures", "journal-ref": "Entropy 2018, 20(8), 583", "doi": "10.3390/e20080583", "report-no": null, "categories": "physics.data-an cond-mat.stat-mech quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare and contrast the statistical physics and quantum physics inspired\napproaches for unsupervised generative modeling of classical data. The two\napproaches represent probabilities of observed data using energy-based models\nand quantum states respectively.Classical and quantum information patterns of\nthe target datasets therefore provide principled guidelines for structural\ndesign and learning in these two approaches. Taking the restricted Boltzmann\nmachines (RBM) as an example, we analyze the information theoretical bounds of\nthe two approaches. We verify our reasonings by comparing the performance of\nRBMs of various architectures on the standard MNIST datasets.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 06:34:10 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Cheng", "Song", ""], ["Chen", "Jing", ""], ["Wang", "Lei", ""]]}, {"id": "1712.04145", "submitter": "Sho Sonoda", "authors": "Sho Sonoda, Noboru Murata", "title": "Transportation analysis of denoising autoencoders: a novel method for\n  analyzing deep neural networks", "comments": "Accepted at NIPS 2017 workshop on Optimal Transport & Machine\n  Learning (OTML2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The feature map obtained from the denoising autoencoder (DAE) is investigated\nby determining transportation dynamics of the DAE, which is a cornerstone for\ndeep learning. Despite the rapid development in its application, deep neural\nnetworks remain analytically unexplained, because the feature maps are nested\nand parameters are not faithful. In this paper, we address the problem of the\nformulation of nested complex of parameters by regarding the feature map as a\ntransport map. Even when a feature map has different dimensions between input\nand output, we can regard it as a transportation map by considering that both\nthe input and output spaces are embedded in a common high-dimensional space. In\naddition, the trajectory is a geometric object and thus, is independent of\nparameterization. In this manner, transportation can be regarded as a universal\ncharacter of deep neural networks. By determining and analyzing the\ntransportation dynamics, we can understand the behavior of a deep neural\nnetwork. In this paper, we investigate a fundamental case of deep neural\nnetworks: the DAE. We derive the transport map of the DAE, and reveal that the\ninfinitely deep DAE transports mass to decrease a certain quantity, such as\nentropy, of the data distribution. These results though analytically simple,\nshed light on the correspondence between deep neural networks and the\nWasserstein gradient flows.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 06:37:28 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Sonoda", "Sho", ""], ["Murata", "Noboru", ""]]}, {"id": "1712.04146", "submitter": "Salman Salloum", "authors": "Salman Salloum and Yulin He and Joshua Zhexue Huang and Xiaoliang\n  Zhang and Tamer Z. Emara and Chenghao Wei and Heping He", "title": "A Random Sample Partition Data Model for Big Data Analysis", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": "10.1109/TII.2019.2912723", "report-no": null, "categories": "cs.DC cs.DS physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data sets must be carefully partitioned into statistically similar data\nsubsets that can be used as representative samples for big data analysis tasks.\nIn this paper, we propose the random sample partition (RSP) data model to\nrepresent a big data set as a set of non-overlapping data subsets, called RSP\ndata blocks, where each RSP data block has a probability distribution similar\nto the whole big data set. Under this data model, efficient block level\nsampling is used to randomly select RSP data blocks, replacing expensive record\nlevel sampling to select sample data from a big distributed data set on a\ncomputing cluster. We show how RSP data blocks can be employed to estimate\nstatistics of a big data set and build models which are equivalent to those\nbuilt from the whole big data set. In this approach, analysis of a big data set\nbecomes analysis of few RSP data blocks which have been generated in advance on\nthe computing cluster. Therefore, the new method for data analysis based on RSP\ndata blocks is scalable to big data.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 06:49:28 GMT"}, {"version": "v2", "created": "Sat, 20 Jan 2018 10:59:15 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Salloum", "Salman", ""], ["He", "Yulin", ""], ["Huang", "Joshua Zhexue", ""], ["Zhang", "Xiaoliang", ""], ["Emara", "Tamer Z.", ""], ["Wei", "Chenghao", ""], ["He", "Heping", ""]]}, {"id": "1712.04165", "submitter": "Irene Teinemaa", "authors": "Irene Teinemaa, Marlon Dumas, Anna Leontjeva, Fabrizio Maria Maggi", "title": "Temporal Stability in Predictive Process Monitoring", "comments": null, "journal-ref": "Data Min Knowl Disc (2018) 32: 1306", "doi": "10.1007/s10618-018-0575-9", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive process monitoring is concerned with the analysis of events\nproduced during the execution of a business process in order to predict as\nearly as possible the final outcome of an ongoing case. Traditionally,\npredictive process monitoring methods are optimized with respect to accuracy.\nHowever, in environments where users make decisions and take actions in\nresponse to the predictions they receive, it is equally important to optimize\nthe stability of the successive predictions made for each case. To this end,\nthis paper defines a notion of temporal stability for binary classification\ntasks in predictive process monitoring and evaluates existing methods with\nrespect to both temporal stability and accuracy. We find that methods based on\nXGBoost and LSTM neural networks exhibit the highest temporal stability. We\nthen show that temporal stability can be enhanced by hyperparameter-optimizing\nrandom forests and XGBoost classifiers with respect to inter-run stability.\nFinally, we show that time series smoothing techniques can further enhance\ntemporal stability at the expense of slightly lower accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 08:20:56 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 06:30:37 GMT"}, {"version": "v3", "created": "Fri, 15 Jun 2018 06:42:42 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Teinemaa", "Irene", ""], ["Dumas", "Marlon", ""], ["Leontjeva", "Anna", ""], ["Maggi", "Fabrizio Maria", ""]]}, {"id": "1712.04195", "submitter": "Yoshihiro Nagano", "authors": "Yoshihiro Nagano, Ryo Karakida and Masato Okada", "title": "Concept Formation and Dynamics of Repeated Inference in Deep Generative\n  Models", "comments": "20 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models are reported to be useful in broad applications\nincluding image generation. Repeated inference between data space and latent\nspace in these models can denoise cluttered images and improve the quality of\ninferred results. However, previous studies only qualitatively evaluated image\noutputs in data space, and the mechanism behind the inference has not been\ninvestigated. The purpose of the current study is to numerically analyze\nchanges in activity patterns of neurons in the latent space of a deep\ngenerative model called a \"variational auto-encoder\" (VAE). What kinds of\ninference dynamics the VAE demonstrates when noise is added to the input data\nare identified. The VAE embeds a dataset with clear cluster structures in the\nlatent space and the center of each cluster of multiple correlated data points\n(memories) is referred as the concept. Our study demonstrated that transient\ndynamics of inference first approaches a concept, and then moves close to a\nmemory. Moreover, the VAE revealed that the inference dynamics approaches a\nmore abstract concept to the extent that the uncertainty of input data\nincreases due to noise. It was demonstrated that by increasing the number of\nthe latent variables, the trend of the inference dynamics to approach a concept\ncan be enhanced, and the generalization ability of the VAE can be improved.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 09:55:11 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Nagano", "Yoshihiro", ""], ["Karakida", "Ryo", ""], ["Okada", "Masato", ""]]}, {"id": "1712.04221", "submitter": "Hiroki Mori", "authors": "Hiroki Mori and Keisuke Kawano and Hiroki Yokoyama", "title": "Causal Patterns: Extraction of multiple causal relationships by Mixture\n  of Probabilistic Partial Canonical Correlation Analysis", "comments": "DSAA2017 - The 4th IEEE International Conference on Data Science and\n  Advanced Analytics", "journal-ref": "Proceedings of the 4th IEEE International Conference on Data\n  Science and Advanced Analytics, pp.744-754, 2017", "doi": "10.1109/DSAA.2017.60", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a mixture of probabilistic partial canonical\ncorrelation analysis (MPPCCA) that extracts the Causal Patterns from two\nmultivariate time series. Causal patterns refer to the signal patterns within\ninteractions of two elements having multiple types of mutually causal\nrelationships, rather than a mixture of simultaneous correlations or the\nabsence of presence of a causal relationship between the elements. In\nmultivariate statistics, partial canonical correlation analysis (PCCA)\nevaluates the correlation between two multivariates after subtracting the\neffect of the third multivariate. PCCA can calculate the Granger Causal- ity\nIndex (which tests whether a time-series can be predicted from an- other\ntime-series), but is not applicable to data containing multiple partial\ncanonical correlations. After introducing the MPPCCA, we propose an\nexpectation-maxmization (EM) algorithm that estimates the parameters and latent\nvariables of the MPPCCA. The MPPCCA is expected to ex- tract multiple partial\ncanonical correlations from data series without any supervised signals to split\nthe data as clusters. The method was then eval- uated in synthetic data\nexperiments. In the synthetic dataset, our method estimated the multiple\npartial canonical correlations more accurately than the existing method. To\ndetermine the types of patterns detectable by the method, experiments were also\nconducted on real datasets. The method estimated the communication patterns In\nmotion-capture data. The MP- PCCA is applicable to various type of signals such\nas brain signals, human communication and nonlinear complex multibody systems.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 10:46:27 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Mori", "Hiroki", ""], ["Kawano", "Keisuke", ""], ["Yokoyama", "Hiroki", ""]]}, {"id": "1712.04248", "submitter": "Jonas Rauber", "authors": "Wieland Brendel, Jonas Rauber, Matthias Bethge", "title": "Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box\n  Machine Learning Models", "comments": "Published as a conference paper at the Sixth International Conference\n  on Learning Representations (ICLR 2018)\n  https://openreview.net/forum?id=SyZI0GWCZ", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning algorithms are vulnerable to almost imperceptible\nperturbations of their inputs. So far it was unclear how much risk adversarial\nperturbations carry for the safety of real-world machine learning applications\nbecause most methods used to generate such perturbations rely either on\ndetailed model information (gradient-based attacks) or on confidence scores\nsuch as class probabilities (score-based attacks), neither of which are\navailable in most real-world scenarios. In many such cases one currently needs\nto retreat to transfer-based attacks which rely on cumbersome substitute\nmodels, need access to the training data and can be defended against. Here we\nemphasise the importance of attacks which solely rely on the final model\ndecision. Such decision-based attacks are (1) applicable to real-world\nblack-box models such as autonomous cars, (2) need less knowledge and are\neasier to apply than transfer-based attacks and (3) are more robust to simple\ndefences than gradient- or score-based attacks. Previous attacks in this\ncategory were limited to simple models or simple datasets. Here we introduce\nthe Boundary Attack, a decision-based attack that starts from a large\nadversarial perturbation and then seeks to reduce the perturbation while\nstaying adversarial. The attack is conceptually simple, requires close to no\nhyperparameter tuning, does not rely on substitute models and is competitive\nwith the best gradient-based attacks in standard computer vision tasks like\nImageNet. We apply the attack on two black-box algorithms from Clarifai.com.\nThe Boundary Attack in particular and the class of decision-based attacks in\ngeneral open new avenues to study the robustness of machine learning models and\nraise new questions regarding the safety of deployed machine learning systems.\nAn implementation of the attack is available as part of Foolbox at\nhttps://github.com/bethgelab/foolbox .\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 11:36:26 GMT"}, {"version": "v2", "created": "Fri, 16 Feb 2018 14:40:42 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Brendel", "Wieland", ""], ["Rauber", "Jonas", ""], ["Bethge", "Matthias", ""]]}, {"id": "1712.04276", "submitter": "Soumitro Chakrabarty", "authors": "Soumitro Chakrabarty and Emanu\\\"el A. P. Habets", "title": "Multi-Speaker Localization Using Convolutional Neural Network Trained\n  with Noise", "comments": "Presented at Machine Learning for Audio Processing (ML4Audio)\n  Workshop at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of multi-speaker localization is formulated as a multi-class\nmulti-label classification problem, which is solved using a convolutional\nneural network (CNN) based source localization method. Utilizing the common\nassumption of disjoint speaker activities, we propose a novel method to train\nthe CNN using synthesized noise signals. The proposed localization method is\nevaluated for two speakers and compared to a well-known steered response power\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 13:17:30 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Chakrabarty", "Soumitro", ""], ["Habets", "Emanu\u00ebl A. P.", ""]]}, {"id": "1712.04323", "submitter": "Claudio Gallicchio", "authors": "Claudio Gallicchio and Alessio Micheli", "title": "Deep Echo State Network (DeepESN): A Brief Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of deep recurrent neural networks (RNNs) and, in particular, of\ndeep Reservoir Computing (RC) is gaining an increasing research attention in\nthe neural networks community. The recently introduced Deep Echo State Network\n(DeepESN) model opened the way to an extremely efficient approach for designing\ndeep neural networks for temporal data. At the same time, the study of DeepESNs\nallowed to shed light on the intrinsic properties of state dynamics developed\nby hierarchical compositions of recurrent layers, i.e. on the bias of depth in\nRNNs architectural design. In this paper, we summarize the advancements in the\ndevelopment, analysis and applications of DeepESNs.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 14:50:51 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 14:01:34 GMT"}, {"version": "v3", "created": "Mon, 25 Feb 2019 19:59:13 GMT"}, {"version": "v4", "created": "Fri, 25 Sep 2020 17:53:57 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Gallicchio", "Claudio", ""], ["Micheli", "Alessio", ""]]}, {"id": "1712.04332", "submitter": "Yue Lu", "authors": "Chuang Wang, Jonathan Mattingly, and Yue M. Lu", "title": "Scaling Limit: Exact and Tractable Analysis of Online Learning\n  Algorithms with Applications to Regularized Regression and PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for analyzing the exact dynamics of a class of online\nlearning algorithms in the high-dimensional scaling limit. Our results are\napplied to two concrete examples: online regularized linear regression and\nprincipal component analysis. As the ambient dimension tends to infinity, and\nwith proper time scaling, we show that the time-varying joint empirical\nmeasures of the target feature vector and its estimates provided by the\nalgorithms will converge weakly to a deterministic measured-valued process that\ncan be characterized as the unique solution of a nonlinear PDE. Numerical\nsolutions of this PDE can be efficiently obtained. These solutions lead to\nprecise predictions of the performance of the algorithms, as many practical\nperformance metrics are linear functionals of the joint empirical measures. In\naddition to characterizing the dynamic performance of online learning\nalgorithms, our asymptotic analysis also provides useful insights. In\nparticular, in the high-dimensional limit, and due to exchangeability, the\noriginal coupled dynamics associated with the algorithms will be asymptotically\n\"decoupled\", with each coordinate independently solving a 1-D effective\nminimization problem via stochastic gradient descent. Exploiting this insight\nfor nonconvex optimization problems may prove an interesting line of future\nresearch.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 00:20:18 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Wang", "Chuang", ""], ["Mattingly", "Jonathan", ""], ["Lu", "Yue M.", ""]]}, {"id": "1712.04350", "submitter": "Luis Perez", "authors": "Luis Perez", "title": "Predicting Yelp Star Reviews Based on Network Structure with Deep\n  Learning", "comments": "10 pages, 17 figures, manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the real-world problem of predicting Yelp\nstar-review rating based on business features (such as images, descriptions),\nuser features (average previous ratings), and, of particular interest, network\nproperties (which businesses has a user rated before). We compare multiple\nmodels on different sets of features -- from simple linear regression on\nnetwork features only to deep learning models on network and item features.\n  In recent years, breakthroughs in deep learning have led to increased\naccuracy in common supervised learning tasks, such as image classification,\ncaptioning, and language understanding. However, the idea of combining deep\nlearning with network feature and structure appears to be novel. While the\nproblem of predicting future interactions in a network has been studied at\nlength, these approaches have often ignored either node-specific data or global\nstructure.\n  We demonstrate that taking a mixed approach combining both node-level\nfeatures and network information can effectively be used to predict Yelp-review\nstar ratings. We evaluate on the Yelp dataset by splitting our data along the\ntime dimension (as would naturally occur in the real-world) and comparing our\nmodel against others which do no take advantage of the network structure and/or\ndeep learning.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 18:54:23 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Perez", "Luis", ""]]}, {"id": "1712.04356", "submitter": "Farshid Rayhan", "authors": "Farshid Rayhan, Sajid Ahmed, Asif Mahbub, Md. Rafsan Jani, Swakkhar\n  Shatabda, and Dewan Md. Farid", "title": "CUSBoost: Cluster-based Under-sampling with Boosting for Imbalanced\n  Classification", "comments": "CSITSS-2017", "journal-ref": null, "doi": "10.1109/CSITSS.2017.8447534", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Class imbalance classification is a challenging research problem in data\nmining and machine learning, as most of the real-life datasets are often\nimbalanced in nature. Existing learning algorithms maximise the classification\naccuracy by correctly classifying the majority class, but misclassify the\nminority class. However, the minority class instances are representing the\nconcept with greater interest than the majority class instances in real-life\napplications. Recently, several techniques based on sampling methods\n(under-sampling of the majority class and over-sampling the minority class),\ncost-sensitive learning methods, and ensemble learning have been used in the\nliterature for classifying imbalanced datasets. In this paper, we introduce a\nnew clustering-based under-sampling approach with boosting (AdaBoost)\nalgorithm, called CUSBoost, for effective imbalanced classification. The\nproposed algorithm provides an alternative to RUSBoost (random under-sampling\nwith AdaBoost) and SMOTEBoost (synthetic minority over-sampling with AdaBoost)\nalgorithms. We evaluated the performance of CUSBoost algorithm with the\nstate-of-the-art methods based on ensemble learning like AdaBoost, RUSBoost,\nSMOTEBoost on 13 imbalance binary and multi-class datasets with various\nimbalance ratios. The experimental results show that the CUSBoost is a\npromising and effective approach for dealing with highly imbalanced datasets.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 15:33:26 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Rayhan", "Farshid", ""], ["Ahmed", "Sajid", ""], ["Mahbub", "Asif", ""], ["Jani", "Md. Rafsan", ""], ["Shatabda", "Swakkhar", ""], ["Farid", "Dewan Md.", ""]]}, {"id": "1712.04407", "submitter": "Alexander Sage", "authors": "Alexander Sage, Eirikur Agustsson, Radu Timofte, Luc Van Gool", "title": "Logo Synthesis and Manipulation with Clustered Generative Adversarial\n  Networks", "comments": null, "journal-ref": null, "doi": "10.1109/CVPR.2018.00616", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing a logo for a new brand is a lengthy and tedious back-and-forth\nprocess between a designer and a client. In this paper we explore to what\nextent machine learning can solve the creative task of the designer. For this,\nwe build a dataset -- LLD -- of 600k+ logos crawled from the world wide web.\nTraining Generative Adversarial Networks (GANs) for logo synthesis on such\nmulti-modal data is not straightforward and results in mode collapse for some\nstate-of-the-art methods. We propose the use of synthetic labels obtained\nthrough clustering to disentangle and stabilize GAN training. We are able to\ngenerate a high diversity of plausible logos and we demonstrate latent space\nexploration techniques to ease the logo design task in an interactive manner.\nMoreover, we validate the proposed clustered GAN training on CIFAR 10,\nachieving state-of-the-art Inception scores when using synthetic labels\nobtained via clustering the features of an ImageNet classifier. GANs can cope\nwith multi-modal data by means of synthetic labels achieved through clustering,\nand our results show the creative potential of such techniques for logo\nsynthesis and manipulation. Our dataset and models will be made publicly\navailable at https://data.vision.ee.ethz.ch/cvl/lld/.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 17:51:23 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Sage", "Alexander", ""], ["Agustsson", "Eirikur", ""], ["Timofte", "Radu", ""], ["Van Gool", "Luc", ""]]}, {"id": "1712.04432", "submitter": "Aydin Buluc", "authors": "Amir Gholami, Ariful Azad, Peter Jin, Kurt Keutzer, Aydin Buluc", "title": "Integrated Model, Batch and Domain Parallelism in Training Neural\n  Networks", "comments": "11 pages", "journal-ref": "30th ACM Symposium on Parallelism in Algorithms and Architectures\n  (SPAA), 2018", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new integrated method of exploiting model, batch and domain\nparallelism for the training of deep neural networks (DNNs) on large\ndistributed-memory computers using minibatch stochastic gradient descent (SGD).\nOur goal is to find an efficient parallelization strategy for a fixed batch\nsize using $P$ processes. Our method is inspired by the communication-avoiding\nalgorithms in numerical linear algebra. We see $P$ processes as logically\ndivided into a $P_r \\times P_c$ grid where the $P_r$ dimension is implicitly\nresponsible for model/domain parallelism and the $P_c$ dimension is implicitly\nresponsible for batch parallelism. In practice, the integrated matrix-based\nparallel algorithm encapsulates these types of parallelism automatically. We\nanalyze the communication complexity and analytically demonstrate that the\nlowest communication costs are often achieved neither with pure model nor with\npure data parallelism. We also show how the domain parallel approach can help\nin extending the theoretical scaling limit of the typical batch parallel\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 18:42:07 GMT"}, {"version": "v2", "created": "Thu, 4 Jan 2018 22:32:40 GMT"}, {"version": "v3", "created": "Wed, 14 Feb 2018 17:52:25 GMT"}, {"version": "v4", "created": "Wed, 16 May 2018 04:38:31 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Gholami", "Amir", ""], ["Azad", "Ariful", ""], ["Jin", "Peter", ""], ["Keutzer", "Kurt", ""], ["Buluc", "Aydin", ""]]}, {"id": "1712.04542", "submitter": "Arun Venkitaraman", "authors": "Arun Venkitaraman and Dave Zachariah", "title": "Learning Sparse Graphs for Prediction and Filtering of Multivariate Data\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of prediction of multivariate data process using an\nunderlying graph model. We develop a method that learns a sparse partial\ncorrelation graph in a tuning-free and computationally efficient manner.\nSpecifically, the graph structure is learned recursively without the need for\ncross-validation or parameter tuning by building upon a hyperparameter-free\nframework. Our approach does not require the graph to be undirected and also\naccommodates varying noise levels across different nodes.Experiments using\nreal-world datasets show that the proposed method offers significant\nperformance gains in prediction, in comparison with the graphs frequently\nassociated with these datasets.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 21:39:18 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 19:17:23 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Venkitaraman", "Arun", ""], ["Zachariah", "Dave", ""]]}, {"id": "1712.04543", "submitter": "Young Woong Park", "authors": "Seokhyun Chung, Young Woong Park, Taesu Cheong", "title": "A Mathematical Programming Approach for Integrated Multiple Linear\n  Regression Subset Selection and Validation", "comments": null, "journal-ref": "Pattern Recognition 108(2020): 107565", "doi": "10.1016/j.patcog.2020.107565", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subset selection for multiple linear regression aims to construct a\nregression model that minimizes errors by selecting a small number of\nexplanatory variables. Once a model is built, various statistical tests and\ndiagnostics are conducted to validate the model and to determine whether the\nregression assumptions are met. Most traditional approaches require human\ndecisions at this step. For example, the user adding or removing a variable\nuntil a satisfactory model is obtained. However, this trial-and-error strategy\ncannot guarantee that a subset that minimizes the errors while satisfying all\nregression assumptions will be found. In this paper, we propose a fully\nautomated model building procedure for multiple linear regression subset\nselection that integrates model building and validation based on mathematical\nprogramming. The proposed model minimizes mean squared errors while ensuring\nthat the majority of the important regression assumptions are met. We also\npropose an efficient constraint to approximate the constraint for the\ncoefficient t-test. When no subset satisfies all of the considered regression\nassumptions, our model provides an alternative subset that satisfies most of\nthese assumptions. Computational results show that our model yields better\nsolutions (i.e., satisfying more regression assumptions) compared to the\nstate-of-the-art benchmark models while maintaining similar explanatory power.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 21:42:01 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 19:02:25 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Chung", "Seokhyun", ""], ["Park", "Young Woong", ""], ["Cheong", "Taesu", ""]]}, {"id": "1712.04567", "submitter": "Michael McCourt", "authors": "Ruben Martinez-Cantin, Kevin Tee, Michael McCourt", "title": "Practical Bayesian optimization in the presence of outliers", "comments": "10 pages (2 of references), 6 figures, 1 algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference in the presence of outliers is an important field of research as\noutliers are ubiquitous and may arise across a variety of problems and domains.\nBayesian optimization is method that heavily relies on probabilistic inference.\nThis allows outstanding sample efficiency because the probabilistic machinery\nprovides a memory of the whole optimization process. However, that virtue\nbecomes a disadvantage when the memory is populated with outliers, inducing\nbias in the estimation. In this paper, we present an empirical evaluation of\nBayesian optimization methods in the presence of outliers. The empirical\nevidence shows that Bayesian optimization with robust regression often produces\nsuboptimal results. We then propose a new algorithm which combines robust\nregression (a Gaussian process with Student-t likelihood) with outlier\ndiagnostics to classify data points as outliers or inliers. By using an\nscheduler for the classification of outliers, our method is more efficient and\nhas better convergence over the standard robust regression. Furthermore, we\nshow that even in controlled situations with no expected outliers, our method\nis able to produce better results.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 23:31:45 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Martinez-Cantin", "Ruben", ""], ["Tee", "Kevin", ""], ["McCourt", "Michael", ""]]}, {"id": "1712.04603", "submitter": "Jinyoung Choi", "authors": "Jinyoung Choi, Beom-Jin Lee, and Byoung-Tak Zhang", "title": "Multi-focus Attention Network for Efficient Deep Reinforcement Learning", "comments": "AAAI 2017 Workshop on What's next for AI in games (WNAIG 2017), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning (DRL) has shown incredible performance in\nlearning various tasks to the human level. However, unlike human perception,\ncurrent DRL models connect the entire low-level sensory input to the\nstate-action values rather than exploiting the relationship between and among\nentities that constitute the sensory input. Because of this difference, DRL\nneeds vast amount of experience samples to learn. In this paper, we propose a\nMulti-focus Attention Network (MANet) which mimics human ability to spatially\nabstract the low-level sensory input into multiple entities and attend to them\nsimultaneously. The proposed method first divides the low-level input into\nseveral segments which we refer to as partial states. After this segmentation,\nparallel attention layers attend to the partial states relevant to solving the\ntask. Our model estimates state-action values using these attended partial\nstates. In our experiments, MANet attains highest scores with significantly\nless experience samples. Additionally, the model shows higher performance\ncompared to the Deep Q-network and the single attention model as benchmarks.\nFurthermore, we extend our model to attentive communication model for\nperforming multi-agent cooperative tasks. In multi-agent cooperative task\nexperiments, our model shows 20% faster learning than existing state-of-the-art\nmodel.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 04:04:29 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Choi", "Jinyoung", ""], ["Lee", "Beom-Jin", ""], ["Zhang", "Byoung-Tak", ""]]}, {"id": "1712.04644", "submitter": "Branislav Kveton", "authors": "Branislav Kveton, Csaba Szepesvari, Anup Rao, Zheng Wen, Yasin\n  Abbasi-Yadkori, and S. Muthukrishnan", "title": "Stochastic Low-Rank Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in computer vision and recommender systems involve low-rank\nmatrices. In this work, we study the problem of finding the maximum entry of a\nstochastic low-rank matrix from sequential observations. At each step, a\nlearning agent chooses pairs of row and column arms, and receives the noisy\nproduct of their latent values as a reward. The main challenge is that the\nlatent values are unobserved. We identify a class of non-negative matrices\nwhose maximum entry can be found statistically efficiently and propose an\nalgorithm for finding them, which we call LowRankElim. We derive a\n$\\DeclareMathOperator{\\poly}{poly} O((K + L) \\poly(d) \\Delta^{-1} \\log n)$\nupper bound on its $n$-step regret, where $K$ is the number of rows, $L$ is the\nnumber of columns, $d$ is the rank of the matrix, and $\\Delta$ is the minimum\ngap. The bound depends on other problem-specific constants that clearly do not\ndepend $K L$. To the best of our knowledge, this is the first such result in\nthe literature.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 07:59:48 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Kveton", "Branislav", ""], ["Szepesvari", "Csaba", ""], ["Rao", "Anup", ""], ["Wen", "Zheng", ""], ["Abbasi-Yadkori", "Yasin", ""], ["Muthukrishnan", "S.", ""]]}, {"id": "1712.04667", "submitter": "Nikita Zhivotovskiy", "authors": "D. Belomestny, L. Iosipoi, Q. Paris, N. Zhivotovskiy", "title": "Empirical Variance Minimization with Applications in Variance Reduction\n  and Optimal Control", "comments": "31 pages, Quentin Paris added as an author, the paper is\n  significantly reworked and the title is changed", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of empirical minimization for variance-type functionals\nover functional classes. Sharp non-asymptotic bounds for the excess variance\nare derived under mild conditions. In particular, it is shown that under some\nrestrictions imposed on the functional class fast convergence rates can be\nachieved including the optimal non-parametric rates for expressive classes in\nthe non-Donsker regime under some additional assumptions. Our main applications\ninclude variance reduction and optimal control.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 09:09:09 GMT"}, {"version": "v2", "created": "Mon, 2 Apr 2018 12:59:51 GMT"}, {"version": "v3", "created": "Sat, 8 Dec 2018 09:55:24 GMT"}, {"version": "v4", "created": "Tue, 8 Sep 2020 17:32:56 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Belomestny", "D.", ""], ["Iosipoi", "L.", ""], ["Paris", "Q.", ""], ["Zhivotovskiy", "N.", ""]]}, {"id": "1712.04688", "submitter": "George Philipp", "authors": "George Philipp, Seunghak Lee, Eric P. Xing", "title": "Stability Selection for Structured Variable Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In variable or graph selection problems, finding a right-sized model or\ncontrolling the number of false positives is notoriously difficult. Recently, a\nmeta-algorithm called Stability Selection was proposed that can provide\nreliable finite-sample control of the number of false positives. Its benefits\nwere demonstrated when used in conjunction with the lasso and orthogonal\nmatching pursuit algorithms.\n  In this paper, we investigate the applicability of stability selection to\nstructured selection algorithms: the group lasso and the structured\ninput-output lasso. We find that using stability selection often increases the\npower of both algorithms, but that the presence of complex structure reduces\nthe reliability of error control under stability selection. We give strategies\nfor setting tuning parameters to obtain a good model size under stability\nselection, and highlight its strengths and weaknesses compared to competing\nmethods screen and clean and cross-validation. We give guidelines about when to\nuse which error control method.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 10:20:15 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Philipp", "George", ""], ["Lee", "Seunghak", ""], ["Xing", "Eric P.", ""]]}, {"id": "1712.04709", "submitter": "Hideyuki Miyahara", "authors": "Hideyuki Miyahara, Yuki Sughiyama", "title": "A Quantum Extension of Variational Bayes Inference", "comments": "7 pages, 4 figures", "journal-ref": "Phys. Rev. A 98, 022330 (2018)", "doi": "10.1103/PhysRevA.98.022330", "report-no": "Phys. Rev. A 98, 022330 -- Published 29 August 2018", "categories": "stat.ML cond-mat.stat-mech quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Bayes (VB) inference is one of the most important algorithms in\nmachine learning and widely used in engineering and industry. However, VB is\nknown to suffer from the problem of local optima. In this Letter, we generalize\nVB by using quantum mechanics, and propose a new algorithm, which we call\nquantum annealing variational Bayes (QAVB) inference. We then show that QAVB\ndrastically improve the performance of VB by applying them to a clustering\nproblem described by a Gaussian mixture model. Finally, we discuss an intuitive\nunderstanding on how QAVB works well.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 11:18:22 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Miyahara", "Hideyuki", ""], ["Sughiyama", "Yuki", ""]]}, {"id": "1712.04755", "submitter": "Loucas Pillaud-Vivien", "authors": "Loucas Pillaud-Vivien (SIERRA), Alessandro Rudi (SIERRA), Francis Bach\n  (SIERRA)", "title": "Exponential convergence of testing error for stochastic gradient methods", "comments": null, "journal-ref": "Conference on Learning Theory (COLT), Jul 2018, Stockholm, Sweden", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider binary classification problems with positive definite kernels and\nsquare loss, and study the convergence rates of stochastic gradient methods. We\nshow that while the excess testing loss (squared loss) converges slowly to zero\nas the number of observations (and thus iterations) goes to infinity, the\ntesting error (classification error) converges exponentially fast if low-noise\nconditions are assumed.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 13:35:27 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2018 14:16:39 GMT"}, {"version": "v3", "created": "Fri, 29 Jun 2018 08:09:44 GMT"}, {"version": "v4", "created": "Tue, 20 Nov 2018 11:49:03 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Pillaud-Vivien", "Loucas", "", "SIERRA"], ["Rudi", "Alessandro", "", "SIERRA"], ["Bach", "Francis", "", "SIERRA"]]}, {"id": "1712.04775", "submitter": "Clementine Barreyre", "authors": "Cl\\'ementine Barreyre, B\\'eatrice Laurent (IMT), Jean-Michel Loubes\n  (IMT), Bertrand Cabon, Lo\\\"ic Boussouf", "title": "Multiple testing for outlier detection in functional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel procedure for outlier detection in functional data, in a\nsemi-supervised framework. As the data is functional, we consider the\ncoefficients obtained after projecting the observations onto orthonormal bases\n(wavelet, PCA). A multiple testing procedure based on the two-sample test is\ndefined in order to highlight the levels of the coefficients on which the\noutliers appear as significantly different to the normal data. The selected\ncoefficients are then called features for the outlier detection, on which we\ncompute the Local Outlier Factor to highlight the outliers. This procedure to\nselect the features is applied on simulated data that mimic the behaviour of\nspace telemetries, and compared with existing dimension reduction techniques.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 14:07:55 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Barreyre", "Cl\u00e9mentine", "", "IMT"], ["Laurent", "B\u00e9atrice", "", "IMT"], ["Loubes", "Jean-Michel", "", "IMT"], ["Cabon", "Bertrand", ""], ["Boussouf", "Lo\u00efc", ""]]}, {"id": "1712.04802", "submitter": "Ivan Fernandez-Val", "authors": "Victor Chernozhukov, Mert Demirer, Esther Duflo, and Iv\\'an\n  Fern\\'andez-Val", "title": "Generic Machine Learning Inference on Heterogenous Treatment Effects in\n  Randomized Experiments", "comments": "52 pages, 4 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose strategies to estimate and make inference on key features of\nheterogeneous effects in randomized experiments. These key features include\nbest linear predictors of the effects on machine learning proxies, average\neffects sorted by impact groups, and average characteristics of most and least\nimpacted units. The approach is valid in high dimensional settings, where the\neffects are proxied by machine learning methods. We post-process these proxies\ninto the estimates of the key features. Our approach is generic, it can be used\nin conjunction with penalized methods, deep and shallow neural networks,\ncanonical and new random forests, boosted trees, and ensemble methods.\nEstimation and inference are based on repeated data splitting to avoid\noverfitting and achieve validity. For inference, we take medians of p-values\nand medians of confidence intervals, resulting from many different data splits,\nand then adjust their nominal level to guarantee uniform validity. This\nvariational inference method, which quantifies the uncertainty coming from both\nparameter estimation and data splitting, is shown to be uniformly valid for a\nlarge class of data generating processes. We illustrate the use of the approach\nwith a randomized field experiment that evaluated a combination of nudges to\nstimulate demand for immunization in India.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 14:47:57 GMT"}, {"version": "v2", "created": "Sun, 31 Dec 2017 18:23:16 GMT"}, {"version": "v3", "created": "Fri, 23 Feb 2018 01:40:23 GMT"}, {"version": "v4", "created": "Tue, 3 Sep 2019 13:34:05 GMT"}, {"version": "v5", "created": "Mon, 28 Dec 2020 09:05:55 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Demirer", "Mert", ""], ["Duflo", "Esther", ""], ["Fern\u00e1ndez-Val", "Iv\u00e1n", ""]]}, {"id": "1712.04828", "submitter": "Tom Hope", "authors": "Tom Hope, Dafna Shahaf", "title": "Ballpark Crowdsourcing: The Wisdom of Rough Group Comparisons", "comments": null, "journal-ref": "WSDM 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing has become a popular method for collecting labeled training\ndata. However, in many practical scenarios traditional labeling can be\ndifficult for crowdworkers (for example, if the data is high-dimensional or\nunintuitive, or the labels are continuous).\n  In this work, we develop a novel model for crowdsourcing that can complement\nstandard practices by exploiting people's intuitions about groups and relations\nbetween them. We employ a recent machine learning setting, called Ballpark\nLearning, that can estimate individual labels given only coarse, aggregated\nsignal over groups of data points. To address the important case of continuous\nlabels, we extend the Ballpark setting (which focused on classification) to\nregression problems. We formulate the problem as a convex optimization problem\nand propose fast, simple methods with an innate robustness to outliers.\n  We evaluate our methods on real-world datasets, demonstrating how useful\nconstraints about groups can be harnessed from a crowd of non-experts. Our\nmethods can rival supervised models trained on many true labels, and can obtain\nconsiderably better results from the crowd than a standard label-collection\nprocess (for a lower price). By collecting rough guesses on groups of instances\nand using machine learning to infer the individual labels, our lightweight\nframework is able to address core crowdsourcing challenges and train machine\nlearning models in a cost-effective way.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 15:56:27 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Hope", "Tom", ""], ["Shahaf", "Dafna", ""]]}, {"id": "1712.04910", "submitter": "Mahdi Nazemi", "authors": "Sheng Lin, Ning Liu, Mahdi Nazemi, Hongjia Li, Caiwen Ding, Yanzhi\n  Wang, Massoud Pedram", "title": "FFT-Based Deep Learning Deployment in Embedded Systems", "comments": "Design, Automation, and Test in Europe (DATE) For source code, please\n  contact Mahdi Nazemi at <mnazemi@usc.edu>", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has delivered its powerfulness in many application domains,\nespecially in image and speech recognition. As the backbone of deep learning,\ndeep neural networks (DNNs) consist of multiple layers of various types with\nhundreds to thousands of neurons. Embedded platforms are now becoming essential\nfor deep learning deployment due to their portability, versatility, and energy\nefficiency. The large model size of DNNs, while providing excellent accuracy,\nalso burdens the embedded platforms with intensive computation and storage.\nResearchers have investigated on reducing DNN model size with negligible\naccuracy loss. This work proposes a Fast Fourier Transform (FFT)-based DNN\ntraining and inference model suitable for embedded platforms with reduced\nasymptotic complexity of both computation and storage, making our approach\ndistinguished from existing approaches. We develop the training and inference\nalgorithms based on FFT as the computing kernel and deploy the FFT-based\ninference model on embedded platforms achieving extraordinary processing speed.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 18:26:17 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Lin", "Sheng", ""], ["Liu", "Ning", ""], ["Nazemi", "Mahdi", ""], ["Li", "Hongjia", ""], ["Ding", "Caiwen", ""], ["Wang", "Yanzhi", ""], ["Pedram", "Massoud", ""]]}, {"id": "1712.04912", "submitter": "Xinkun Nie", "authors": "Xinkun Nie, Stefan Wager", "title": "Quasi-Oracle Estimation of Heterogeneous Treatment Effects", "comments": "Biometrika, forthcoming", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flexible estimation of heterogeneous treatment effects lies at the heart of\nmany statistical challenges, such as personalized medicine and optimal resource\nallocation. In this paper, we develop a general class of two-step algorithms\nfor heterogeneous treatment effect estimation in observational studies. We\nfirst estimate marginal effects and treatment propensities in order to form an\nobjective function that isolates the causal component of the signal. Then, we\noptimize this data-adaptive objective function. Our approach has several\nadvantages over existing methods. From a practical perspective, our method is\nflexible and easy to use: In both steps, we can use any loss-minimization\nmethod, e.g., penalized regression, deep neural networks, or boosting;\nmoreover, these methods can be fine-tuned by cross validation. Meanwhile, in\nthe case of penalized kernel regression, we show that our method has a\nquasi-oracle property: Even if the pilot estimates for marginal effects and\ntreatment propensities are not particularly accurate, we achieve the same error\nbounds as an oracle who has a priori knowledge of these two nuisance\ncomponents. We implement variants of our approach based on penalized\nregression, kernel ridge regression, and boosting in a variety of simulation\nsetups, and find promising performance relative to existing baselines.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 18:32:13 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 17:11:54 GMT"}, {"version": "v3", "created": "Mon, 4 Feb 2019 16:51:33 GMT"}, {"version": "v4", "created": "Thu, 6 Aug 2020 06:31:20 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Nie", "Xinkun", ""], ["Wager", "Stefan", ""]]}, {"id": "1712.04997", "submitter": "Lei Lin", "authors": "Lei Lin, Zhengbing He, Srinivas Peeta", "title": "Predicting Station-level Hourly Demands in a Large-scale Bike-sharing\n  Network: A Graph Convolutional Neural Network Approach", "comments": "9 figures, 3 tables, accepted by Transportation Research Part C:\n  Emerging Technologies", "journal-ref": "Transportation Research Part C: Emerging Technologies 97 (2018):\n  258-276", "doi": "10.1016/j.trc.2018.10.011", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study proposes a novel Graph Convolutional Neural Network with\nData-driven Graph Filter (GCNN-DDGF) model that can learn hidden heterogeneous\npairwise correlations between stations to predict station-level hourly demand\nin a large-scale bike-sharing network. Two architectures of the GCNN-DDGF model\nare explored; GCNNreg-DDGF is a regular GCNN-DDGF model which contains the\nconvolution and feedforward blocks, and GCNNrec-DDGF additionally contains a\nrecurrent block from the Long Short-term Memory neural network architecture to\ncapture temporal dependencies in the bike-sharing demand series. Furthermore,\nfour types of GCNN models are proposed whose adjacency matrices are based on\nvarious bike-sharing system data, including Spatial Distance matrix (SD),\nDemand matrix (DE), Average Trip Duration matrix (ATD), and Demand Correlation\nmatrix (DC). These six types of GCNN models and seven other benchmark models\nare built and compared on a Citi Bike dataset from New York City which includes\n272 stations and over 28 million transactions from 2013 to 2016. Results show\nthat the GCNNrec-DDGF performs the best in terms of the Root Mean Square Error,\nthe Mean Absolute Error and the coefficient of determination (R2), followed by\nthe GCNNreg-DDGF. They outperform the other models. Through a more detailed\ngraph network analysis based on the learned DDGF, insights are obtained on the\nblack box of the GCNN-DDGF model. It is found to capture some information\nsimilar to details embedded in the SD, DE and DC matrices. More importantly, it\nalso uncovers hidden heterogeneous pairwise correlations between stations that\nare not revealed by any of those matrices.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 20:26:50 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 18:58:23 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Lin", "Lei", ""], ["He", "Zhengbing", ""], ["Peeta", "Srinivas", ""]]}, {"id": "1712.05016", "submitter": "Alexandre Lacoste", "authors": "Alexandre Lacoste, Thomas Boquet, Negar Rostamzadeh, Boris Oreshkin,\n  Wonchang Chung, David Krueger", "title": "Deep Prior", "comments": "Workshop paper, Accepted at Bayesian Deep Learning workshop, NIPS\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent literature on deep learning offers new tools to learn a rich\nprobability distribution over high dimensional data such as images or sounds.\nIn this work we investigate the possibility of learning the prior distribution\nover neural network parameters using such tools. Our resulting variational\nBayes algorithm generalizes well to new tasks, even when very few training\nexamples are provided. Furthermore, this learned prior allows the model to\nextrapolate correctly far from a given task's training data on a meta-dataset\nof periodic signals.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 21:41:56 GMT"}, {"version": "v2", "created": "Sat, 16 Dec 2017 02:52:55 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Lacoste", "Alexandre", ""], ["Boquet", "Thomas", ""], ["Rostamzadeh", "Negar", ""], ["Oreshkin", "Boris", ""], ["Chung", "Wonchang", ""], ["Krueger", "David", ""]]}, {"id": "1712.05134", "submitter": "Jinmian Ye", "authors": "Jinmian Ye, Linnan Wang, Guangxi Li, Di Chen, Shandian Zhe, Xinqi Chu,\n  Zenglin Xu", "title": "Learning Compact Recurrent Neural Networks with Block-Term Tensor\n  Decomposition", "comments": "CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) are powerful sequence modeling tools.\nHowever, when dealing with high dimensional inputs, the training of RNNs\nbecomes computational expensive due to the large number of model parameters.\nThis hinders RNNs from solving many important computer vision tasks, such as\nAction Recognition in Videos and Image Captioning. To overcome this problem, we\npropose a compact and flexible structure, namely Block-Term tensor\ndecomposition, which greatly reduces the parameters of RNNs and improves their\ntraining efficiency. Compared with alternative low-rank approximations, such as\ntensor-train RNN (TT-RNN), our method, Block-Term RNN (BT-RNN), is not only\nmore concise (when using the same rank), but also able to attain a better\napproximation to the original RNNs with much fewer parameters. On three\nchallenging tasks, including Action Recognition in Videos, Image Captioning and\nImage Generation, BT-RNN outperforms TT-RNN and the standard RNN in terms of\nboth prediction accuracy and convergence rate. Specifically, BT-LSTM utilizes\n17,388 times fewer parameters than the standard LSTM to achieve an accuracy\nimprovement over 15.6\\% in the Action Recognition task on the UCF11 dataset.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 09:24:27 GMT"}, {"version": "v2", "created": "Fri, 11 May 2018 07:33:47 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Ye", "Jinmian", ""], ["Wang", "Linnan", ""], ["Li", "Guangxi", ""], ["Chen", "Di", ""], ["Zhe", "Shandian", ""], ["Chu", "Xinqi", ""], ["Xu", "Zenglin", ""]]}, {"id": "1712.05279", "submitter": "Johanna F. Ziegel", "authors": "Ingo Steinwart, Johanna F. Ziegel", "title": "Strictly proper kernel scores and characteristic kernels on compact\n  spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strictly proper kernel scores are well-known tool in probabilistic\nforecasting, while characteristic kernels have been extensively investigated in\nthe machine learning literature. We first show that both notions coincide, so\nthat insights from one part of the literature can be used in the other. We then\nshow that the metric induced by a characteristic kernel cannot reliably\ndistinguish between distributions that are far apart in the total variation\nnorm as soon as the underlying space of measures is infinite dimensional. In\naddition, we provide a characterization of characteristic kernels in terms of\neigenvalues and -functions and apply this characterization to the case of\ncontinuous kernels on (locally) compact spaces. In the compact case we further\nshow that characteristic kernels exist if and only if the space is metrizable.\nAs special cases of our general theory we investigate translation-invariant\nkernels on compact Abelian groups and isotropic kernels on spheres. The latter\nare of particular interest for forecast evaluation of probabilistic predictions\non spherical domains as frequently encountered in meteorology and climatology.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 15:18:29 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Steinwart", "Ingo", ""], ["Ziegel", "Johanna F.", ""]]}, {"id": "1712.05382", "submitter": "Chung-Cheng Chiu", "authors": "Chung-Cheng Chiu and Colin Raffel", "title": "Monotonic Chunkwise Attention", "comments": "ICLR camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-to-sequence models with soft attention have been successfully\napplied to a wide variety of problems, but their decoding process incurs a\nquadratic time and space cost and is inapplicable to real-time sequence\ntransduction. To address these issues, we propose Monotonic Chunkwise Attention\n(MoChA), which adaptively splits the input sequence into small chunks over\nwhich soft attention is computed. We show that models utilizing MoChA can be\ntrained efficiently with standard backpropagation while allowing online and\nlinear-time decoding at test time. When applied to online speech recognition,\nwe obtain state-of-the-art results and match the performance of a model using\nan offline soft attention mechanism. In document summarization experiments\nwhere we do not expect monotonic alignments, we show significantly improved\nperformance compared to a baseline monotonic attention-based model.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 18:29:42 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 01:35:36 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Chiu", "Chung-Cheng", ""], ["Raffel", "Colin", ""]]}, {"id": "1712.05438", "submitter": "Atsushi Nitanda", "authors": "Atsushi Nitanda and Taiji Suzuki", "title": "Stochastic Particle Gradient Descent for Infinite Ensembles", "comments": "33 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The superior performance of ensemble methods with infinite models are well\nknown. Most of these methods are based on optimization problems in\ninfinite-dimensional spaces with some regularization, for instance, boosting\nmethods and convex neural networks use $L^1$-regularization with the\nnon-negative constraint. However, due to the difficulty of handling\n$L^1$-regularization, these problems require early stopping or a rough\napproximation to solve it inexactly. In this paper, we propose a new ensemble\nlearning method that performs in a space of probability measures, that is, our\nmethod can handle the $L^1$-constraint and the non-negative constraint in a\nrigorous way. Such an optimization is realized by proposing a general purpose\nstochastic optimization method for learning probability measures via\nparameterization using transport maps on base models. As a result of running\nthe method, a transport map to output an infinite ensemble is obtained, which\nforms a residual-type network. From the perspective of functional gradient\nmethods, we give a convergence rate as fast as that of a stochastic\noptimization method for finite dimensional nonconvex problems. Moreover, we\nshow an interior optimality property of a local optimality condition used in\nour analysis.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 20:12:02 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Nitanda", "Atsushi", ""], ["Suzuki", "Taiji", ""]]}, {"id": "1712.05510", "submitter": "Alexander LeNail", "authors": "Alexander LeNail, Ludwig Schmidt, Johnathan Li, Tobias Ehrenberger,\n  Karen Sachs, Stefanie Jegelka, Ernest Fraenkel", "title": "Graph-Sparse Logistic Regression", "comments": "7 pages, 2 figures, NIPS DISCML workshop paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Graph-Sparse Logistic Regression, a new algorithm for\nclassification for the case in which the support should be sparse but connected\non a graph. We val- idate this algorithm against synthetic data and benchmark\nit against L1-regularized Logistic Regression. We then explore our technique in\nthe bioinformatics context of proteomics data on the interactome graph. We make\nall our experimental code public and provide GSLR as an open source package.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 02:17:06 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["LeNail", "Alexander", ""], ["Schmidt", "Ludwig", ""], ["Li", "Johnathan", ""], ["Ehrenberger", "Tobias", ""], ["Sachs", "Karen", ""], ["Jegelka", "Stefanie", ""], ["Fraenkel", "Ernest", ""]]}, {"id": "1712.05556", "submitter": "Kyriakos Polymenakos", "authors": "Kyriakos Polymenakos, Alessandro Abate, Stephen Roberts", "title": "Safe Policy Search with Gaussian Process Models", "comments": "9 pages, 2 figures, extended version of the paper that was presented\n  in AAMAS 2019, Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to optimise the parameters of a policy which will be used\nto safely perform a given task in a data-efficient manner. We train a Gaussian\nprocess model to capture the system dynamics, based on the PILCO framework. Our\nmodel has useful analytic properties, which allow closed form computation of\nerror gradients and estimating the probability of violating given state space\nconstraints. During training, as well as operation, only policies that are\ndeemed safe are implemented on the real system, minimising the risk of failure.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 06:34:53 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 16:17:36 GMT"}, {"version": "v3", "created": "Fri, 29 Nov 2019 20:46:15 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Polymenakos", "Kyriakos", ""], ["Abate", "Alessandro", ""], ["Roberts", "Stephen", ""]]}, {"id": "1712.05630", "submitter": "Richard Samworth", "authors": "Milana Gataric, Tengyao Wang and Richard J. Samworth", "title": "Sparse principal component analysis via axis-aligned random projections", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new method for sparse principal component analysis, based on\nthe aggregation of eigenvector information from carefully-selected axis-aligned\nrandom projections of the sample covariance matrix. Unlike most alternative\napproaches, our algorithm is non-iterative, so is not vulnerable to a bad\nchoice of initialisation. We provide theoretical guarantees under which our\nprincipal subspace estimator can attain the minimax optimal rate of convergence\nin polynomial time. In addition, our theory provides a more refined\nunderstanding of the statistical and computational trade-off in the problem of\nsparse principal component estimation, revealing a subtle interplay between the\neffective sample size and the number of random projections that are required to\nachieve the minimax optimal rate. Numerical studies provide further insight\ninto the procedure and confirm its highly competitive finite-sample\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 11:55:39 GMT"}, {"version": "v2", "created": "Sat, 13 Jan 2018 17:35:54 GMT"}, {"version": "v3", "created": "Tue, 24 Jul 2018 10:12:09 GMT"}, {"version": "v4", "created": "Mon, 6 May 2019 16:18:07 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Gataric", "Milana", ""], ["Wang", "Tengyao", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1712.05654", "submitter": "Julien Mairal", "authors": "Hongzhou Lin, Julien Mairal and Zaid Harchaoui", "title": "Catalyst Acceleration for First-order Convex Optimization: from Theory\n  to Practice", "comments": "link to publisher website:\n  http://jmlr.org/papers/volume18/17-748/17-748.pdf", "journal-ref": "Journal of Machine Learning Research (JMLR), 18(212):1--54, 2018", "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a generic scheme for accelerating gradient-based optimization\nmethods in the sense of Nesterov. The approach, called Catalyst, builds upon\nthe inexact accelerated proximal point algorithm for minimizing a convex\nobjective function, and consists of approximately solving a sequence of\nwell-chosen auxiliary problems, leading to faster convergence. One of the keys\nto achieve acceleration in theory and in practice is to solve these\nsub-problems with appropriate accuracy by using the right stopping criterion\nand the right warm-start strategy. We give practical guidelines to use Catalyst\nand present a comprehensive analysis of its global complexity. We show that\nCatalyst applies to a large class of algorithms, including gradient descent,\nblock coordinate descent, incremental algorithms such as SAG, SAGA, SDCA, SVRG,\nMISO/Finito, and their proximal variants. For all of these methods, we\nestablish faster rates using the Catalyst acceleration, for strongly convex and\nnon-strongly convex objectives. We conclude with extensive experiments showing\nthat acceleration is useful in practice, especially for ill-conditioned\nproblems.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 13:05:18 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 07:05:12 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Lin", "Hongzhou", ""], ["Mairal", "Julien", ""], ["Harchaoui", "Zaid", ""]]}, {"id": "1712.05689", "submitter": "Guangxi Li", "authors": "Guangxi Li, Jinmian Ye, Haiqin Yang, Di Chen, Shuicheng Yan and\n  Zenglin Xu", "title": "BT-Nets: Simplifying Deep Neural Networks via Block Term Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep neural networks (DNNs) have been regarded as the\nstate-of-the-art classification methods in a wide range of applications,\nespecially in image classification. Despite the success, the huge number of\nparameters blocks its deployment to situations with light computing resources.\nResearchers resort to the redundancy in the weights of DNNs and attempt to find\nhow fewer parameters can be chosen while preserving the accuracy at the same\ntime. Although several promising results have been shown along this research\nline, most existing methods either fail to significantly compress a\nwell-trained deep network or require a heavy fine-tuning process for the\ncompressed network to regain the original performance. In this paper, we\npropose the \\textit{Block Term} networks (BT-nets) in which the commonly used\nfully-connected layers (FC-layers) are replaced with block term layers\n(BT-layers). In BT-layers, the inputs and the outputs are reshaped into two\nlow-dimensional high-order tensors, then block-term decomposition is applied as\ntensor operators to connect them. We conduct extensive experiments on benchmark\ndatasets to demonstrate that BT-layers can achieve a very large compression\nratio on the number of parameters while preserving the representation power of\nthe original FC-layers as much as possible. Specifically, we can get a higher\nperformance while requiring fewer parameters compared with the tensor train\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 14:42:32 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Li", "Guangxi", ""], ["Ye", "Jinmian", ""], ["Yang", "Haiqin", ""], ["Chen", "Di", ""], ["Yan", "Shuicheng", ""], ["Xu", "Zenglin", ""]]}, {"id": "1712.05690", "submitter": "Tobias Domhan", "authors": "Felix Hieber, Tobias Domhan, Michael Denkowski, David Vilar, Artem\n  Sokolov, Ann Clifton, Matt Post", "title": "Sockeye: A Toolkit for Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe Sockeye (version 1.12), an open-source sequence-to-sequence\ntoolkit for Neural Machine Translation (NMT). Sockeye is a production-ready\nframework for training and applying models as well as an experimental platform\nfor researchers. Written in Python and built on MXNet, the toolkit offers\nscalable training and inference for the three most prominent encoder-decoder\narchitectures: attentional recurrent neural networks, self-attentional\ntransformers, and fully convolutional networks. Sockeye also supports a wide\nrange of optimizers, normalization and regularization techniques, and inference\nimprovements from current NMT literature. Users can easily run standard\ntraining recipes, explore different model settings, and incorporate new ideas.\nIn this paper, we highlight Sockeye's features and benchmark it against other\nNMT toolkits on two language arcs from the 2017 Conference on Machine\nTranslation (WMT): English-German and Latvian-English. We report competitive\nBLEU scores across all three architectures, including an overall best score for\nSockeye's transformer implementation. To facilitate further comparison, we\nrelease all system outputs and training scripts used in our experiments. The\nSockeye toolkit is free software released under the Apache 2.0 license.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 14:44:28 GMT"}, {"version": "v2", "created": "Fri, 1 Jun 2018 13:29:31 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Hieber", "Felix", ""], ["Domhan", "Tobias", ""], ["Denkowski", "Michael", ""], ["Vilar", "David", ""], ["Sokolov", "Artem", ""], ["Clifton", "Ann", ""], ["Post", "Matt", ""]]}, {"id": "1712.05754", "submitter": "Jonathan Hollenbeck", "authors": "Brian Bierig, Jonathan Hollenbeck, Alexander Stroud", "title": "Understanding Career Progression in Baseball Through Machine Learning", "comments": "5 pages, class project for CS229 Fall 2017 at Stanford", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Professional baseball players are increasingly guaranteed expensive long-term\ncontracts, with over 70 deals signed in excess of \\$90 million, mostly in the\nlast decade. These are substantial sums compared to a typical franchise\nvaluation of \\$1-2 billion. Hence, the players to whom a team chooses to give\nsuch a contract can have an enormous impact on both competitiveness and profit.\nDespite this, most published approaches examining career progression in\nbaseball are fairly simplistic. We applied four machine learning algorithms to\nthe problem and soundly improved upon existing approaches, particularly for\nbatting data.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 17:02:03 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Bierig", "Brian", ""], ["Hollenbeck", "Jonathan", ""], ["Stroud", "Alexander", ""]]}, {"id": "1712.05790", "submitter": "Cl\\'ement Godard", "authors": "Cl\\'ement Godard, Kevin Matzen, Matt Uyttendaele", "title": "Deep Burst Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noise is an inherent issue of low-light image capture, one which is\nexacerbated on mobile devices due to their narrow apertures and small sensors.\nOne strategy for mitigating noise in a low-light situation is to increase the\nshutter time of the camera, thus allowing each photosite to integrate more\nlight and decrease noise variance. However, there are two downsides of long\nexposures: (a) bright regions can exceed the sensor range, and (b) camera and\nscene motion will result in blurred images. Another way of gathering more light\nis to capture multiple short (thus noisy) frames in a \"burst\" and intelligently\nintegrate the content, thus avoiding the above downsides. In this paper, we use\nthe burst-capture strategy and implement the intelligent integration via a\nrecurrent fully convolutional deep neural net (CNN). We build our novel,\nmultiframe architecture to be a simple addition to any single frame denoising\nmodel, and design to handle an arbitrary number of noisy input frames. We show\nthat it achieves state of the art denoising results on our burst dataset,\nimproving on the best published multi-frame techniques, such as VBM4D and\nFlexISP. Finally, we explore other applications of image enhancement by\nintegrating content from multiple frames and demonstrate that our DNN\narchitecture generalizes well to image super-resolution.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 18:55:16 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Godard", "Cl\u00e9ment", ""], ["Matzen", "Kevin", ""], ["Uyttendaele", "Matt", ""]]}, {"id": "1712.05813", "submitter": "Kyle Brown", "authors": "Kyle Brown and Derek Doran", "title": "Realistic Traffic Generation for Web Robots", "comments": "8 pages", "journal-ref": null, "doi": "10.1109/ICMLA.2017.0-161", "report-no": null, "categories": "cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Critical to evaluating the capacity, scalability, and availability of web\nsystems are realistic web traffic generators. Web traffic generation is a\nclassic research problem, no generator accounts for the characteristics of web\nrobots or crawlers that are now the dominant source of traffic to a web server.\nAdministrators are thus unable to test, stress, and evaluate how their systems\nperform in the face of ever increasing levels of web robot traffic. To resolve\nthis problem, this paper introduces a novel approach to generate synthetic web\nrobot traffic with high fidelity. It generates traffic that accounts for both\nthe temporal and behavioral qualities of robot traffic by statistical and\nBayesian models that are fitted to the properties of robot traffic seen in web\nlogs from North America and Europe. We evaluate our traffic generator by\ncomparing the characteristics of generated traffic to those of the original\ndata. We look at session arrival rates, inter-arrival times and session\nlengths, comparing and contrasting them between generated and real traffic.\nFinally, we show that our generated traffic affects cache performance similarly\nto actual traffic, using the common LRU and LFU eviction policies.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 19:16:17 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Brown", "Kyle", ""], ["Doran", "Derek", ""]]}, {"id": "1712.05861", "submitter": "Philipp Marquetand", "authors": "Michael Gastegger, Ludwig Schwiedrzik, Marius Bittermann, Florian\n  Berzsenyi, Philipp Marquetand", "title": "WACSF - Weighted Atom-Centered Symmetry Functions as Descriptors in\n  Machine Learning Potentials", "comments": null, "journal-ref": "J. Chem. Phys., 148, 241709 (2018)", "doi": "10.1063/1.5019667", "report-no": null, "categories": "physics.chem-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce weighted atom-centered symmetry functions (wACSFs) as\ndescriptors of a chemical system's geometry for use in the prediction of\nchemical properties such as enthalpies or potential energies via machine\nlearning. The wACSFs are based on conventional atom-centered symmetry functions\n(ACSFs) but overcome the undesirable scaling of the latter with increasing\nnumber of different elements in a chemical system. The performance of these two\ndescriptors is compared using them as inputs in high-dimensional neural network\npotentials (HDNNPs), employing the molecular structures and associated\nenthalpies of the 133855 molecules containing up to five different elements\nreported in the QM9 database as reference data. A substantially smaller number\nof wACSFs than ACSFs is needed to obtain a comparable spatial resolution of the\nmolecular structures. At the same time, this smaller set of wACSFs leads to\nsignificantly better generalization performance in the machine learning\npotential than the large set of conventional ACSFs. Furthermore, we show that\nthe intrinsic parameters of the descriptors can in principle be optimized with\na genetic algorithm in a highly automated manner. For the wACSFs employed here,\nwe find however that using a simple empirical parametrization scheme is\nsufficient in order to obtain HDNNPs with high accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 22:15:27 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Gastegger", "Michael", ""], ["Schwiedrzik", "Ludwig", ""], ["Bittermann", "Marius", ""], ["Berzsenyi", "Florian", ""], ["Marquetand", "Philipp", ""]]}, {"id": "1712.05877", "submitter": "Bo Chen", "authors": "Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang,\n  Andrew Howard, Hartwig Adam, Dmitry Kalenichenko", "title": "Quantization and Training of Neural Networks for Efficient\n  Integer-Arithmetic-Only Inference", "comments": "14 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rising popularity of intelligent mobile devices and the daunting\ncomputational cost of deep learning-based models call for efficient and\naccurate on-device inference schemes. We propose a quantization scheme that\nallows inference to be carried out using integer-only arithmetic, which can be\nimplemented more efficiently than floating point inference on commonly\navailable integer-only hardware. We also co-design a training procedure to\npreserve end-to-end model accuracy post quantization. As a result, the proposed\nquantization scheme improves the tradeoff between accuracy and on-device\nlatency. The improvements are significant even on MobileNets, a model family\nknown for run-time efficiency, and are demonstrated in ImageNet classification\nand COCO detection on popular CPUs.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 23:56:52 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Jacob", "Benoit", ""], ["Kligys", "Skirmantas", ""], ["Chen", "Bo", ""], ["Zhu", "Menglong", ""], ["Tang", "Matthew", ""], ["Howard", "Andrew", ""], ["Adam", "Hartwig", ""], ["Kalenichenko", "Dmitry", ""]]}, {"id": "1712.05878", "submitter": "M. Spiropulu", "authors": "Dustin Anderson, Jean-Roch Vlimant and Maria Spiropulu", "title": "An MPI-Based Python Framework for Distributed Training with Keras", "comments": "4 pages, 4 figures, 1 table, DS@HEP, SC17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a lightweight Python framework for distributed training of neural\nnetworks on multiple GPUs or CPUs. The framework is built on the popular Keras\nmachine learning library. The Message Passing Interface (MPI) protocol is used\nto coordinate the training process, and the system is well suited for job\nsubmission at supercomputing sites. We detail the software's features, describe\nits use, and demonstrate its performance on systems of varying sizes on a\nbenchmark problem drawn from high-energy physics research.\n", "versions": [{"version": "v1", "created": "Sat, 16 Dec 2017 00:01:27 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Anderson", "Dustin", ""], ["Vlimant", "Jean-Roch", ""], ["Spiropulu", "Maria", ""]]}, {"id": "1712.05882", "submitter": "Taegyun Jeon", "authors": "Junghoon Seo and Taegyun Jeon", "title": "On reproduction of On the regularization of Wasserstein GANs", "comments": "9 pages, 9 figures, ICLR 2018 reproducibility challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report has several purposes. First, our report is written to investigate\nthe reproducibility of the submitted paper On the regularization of Wasserstein\nGANs (2018). Second, among the experiments performed in the submitted paper,\nfive aspects were emphasized and reproduced: learning speed, stability,\nrobustness against hyperparameter, estimating the Wasserstein distance, and\nvarious sampling method. Finally, we identify which parts of the contribution\ncan be reproduced, and at what cost in terms of resources. All source code for\nreproduction is open to the public.\n", "versions": [{"version": "v1", "created": "Sat, 16 Dec 2017 00:37:20 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Seo", "Junghoon", ""], ["Jeon", "Taegyun", ""]]}, {"id": "1712.05889", "submitter": "Robert Nishihara", "authors": "Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov,\n  Richard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael\n  I. Jordan, Ion Stoica", "title": "Ray: A Distributed Framework for Emerging AI Applications", "comments": "17 pages, 14 figures, 13th USENIX Symposium on Operating Systems\n  Design and Implementation, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The next generation of AI applications will continuously interact with the\nenvironment and learn from these interactions. These applications impose new\nand demanding systems requirements, both in terms of performance and\nflexibility. In this paper, we consider these requirements and present Ray---a\ndistributed system to address them. Ray implements a unified interface that can\nexpress both task-parallel and actor-based computations, supported by a single\ndynamic execution engine. To meet the performance requirements, Ray employs a\ndistributed scheduler and a distributed and fault-tolerant store to manage the\nsystem's control state. In our experiments, we demonstrate scaling beyond 1.8\nmillion tasks per second and better performance than existing specialized\nsystems for several challenging reinforcement learning applications.\n", "versions": [{"version": "v1", "created": "Sat, 16 Dec 2017 01:29:49 GMT"}, {"version": "v2", "created": "Sun, 30 Sep 2018 03:14:16 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Moritz", "Philipp", ""], ["Nishihara", "Robert", ""], ["Wang", "Stephanie", ""], ["Tumanov", "Alexey", ""], ["Liaw", "Richard", ""], ["Liang", "Eric", ""], ["Elibol", "Melih", ""], ["Yang", "Zongheng", ""], ["Paul", "William", ""], ["Jordan", "Michael I.", ""], ["Stoica", "Ion", ""]]}, {"id": "1712.05901", "submitter": "Jung Woo Ha", "authors": "Jung-Woo Ha, Adrian Kim, Chanju Kim, Jangyeon Park, Sunghun Kim", "title": "Automatic Music Highlight Extraction using Convolutional Recurrent\n  Attention Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MM cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music highlights are valuable contents for music services. Most methods\nfocused on low-level signal features. We propose a method for extracting\nhighlights using high-level features from convolutional recurrent attention\nnetworks (CRAN). CRAN utilizes convolution and recurrent layers for sequential\nlearning with an attention mechanism. The attention allows CRAN to capture\nsignificant snippets for distinguishing between genres, thus being used as a\nhigh-level feature. CRAN was evaluated on over 32,000 popular tracks in Korea\nfor two months. Experimental results show our method outperforms three baseline\nmethods through quantitative and qualitative evaluations. Also, we analyze the\neffects of attention and sequence information on performance.\n", "versions": [{"version": "v1", "created": "Sat, 16 Dec 2017 04:27:36 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Ha", "Jung-Woo", ""], ["Kim", "Adrian", ""], ["Kim", "Chanju", ""], ["Park", "Jangyeon", ""], ["Kim", "Sunghun", ""]]}, {"id": "1712.05907", "submitter": "Erin Conlon", "authors": "Zheng Wei and Erin M. Conlon", "title": "Parallel Markov Chain Monte Carlo for Bayesian Hierarchical Models with\n  Big Data, in Two Stages", "comments": "30 pages, 2 figures. New simulation example for logistic regression.\n  MCMC efficiency measure added. Details of convergence diagnostics added. One\n  additional table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the escalating growth of big data sets in recent years, new Bayesian\nMarkov chain Monte Carlo (MCMC) parallel computing methods have been developed.\nThese methods partition large data sets by observations into subsets. However,\nfor Bayesian nested hierarchical models, typically only a few parameters are\ncommon for the full data set, with most parameters being group-specific. Thus,\nparallel Bayesian MCMC methods that take into account the structure of the\nmodel and split the full data set by groups rather than by observations are a\nmore natural approach for analysis. Here, we adapt and extend a recently\nintroduced two-stage Bayesian hierarchical modeling approach, and we partition\ncomplete data sets by groups. In stage 1, the group-specific parameters are\nestimated independently in parallel. The stage 1 posteriors are used as\nproposal distributions in stage 2, where the target distribution is the full\nmodel. Using three-level and four-level models, we show in both simulation and\nreal data studies that results of our method agree closely with the full data\nanalysis, with greatly increased MCMC efficiency and greatly reduced\ncomputation times. The advantages of our method versus existing parallel MCMC\ncomputing methods are also described.\n", "versions": [{"version": "v1", "created": "Sat, 16 Dec 2017 06:14:18 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2019 22:07:54 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Wei", "Zheng", ""], ["Conlon", "Erin M.", ""]]}, {"id": "1712.05997", "submitter": "Amir Karami", "authors": "Amir Karami", "title": "Taming Wild High Dimensional Text Data with a Fuzzy Lash", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.IR cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bag of words (BOW) represents a corpus in a matrix whose elements are the\nfrequency of words. However, each row in the matrix is a very high-dimensional\nsparse vector. Dimension reduction (DR) is a popular method to address sparsity\nand high-dimensionality issues. Among different strategies to develop DR\nmethod, Unsupervised Feature Transformation (UFT) is a popular strategy to map\nall words on a new basis to represent BOW. The recent increase of text data and\nits challenges imply that DR area still needs new perspectives. Although a wide\nrange of methods based on the UFT strategy has been developed, the fuzzy\napproach has not been considered for DR based on this strategy. This research\ninvestigates the application of fuzzy clustering as a DR method based on the\nUFT strategy to collapse BOW matrix to provide a lower-dimensional\nrepresentation of documents instead of the words in a corpus. The quantitative\nevaluation shows that fuzzy clustering produces superior performance and\nfeatures to Principal Components Analysis (PCA) and Singular Value\nDecomposition (SVD), two popular DR methods based on the UFT strategy.\n", "versions": [{"version": "v1", "created": "Sat, 16 Dec 2017 17:57:57 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Karami", "Amir", ""]]}, {"id": "1712.05999", "submitter": "Miguel Molina-Solana", "authors": "Julio Amador and Axel Oehmichen and Miguel Molina-Solana", "title": "Characterizing Political Fake News in Twitter by its Meta-Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a preliminary approach towards characterizing political\nfake news on Twitter through the analysis of their meta-data. In particular, we\nfocus on more than 1.5M tweets collected on the day of the election of Donald\nTrump as 45th president of the United States of America. We use the meta-data\nembedded within those tweets in order to look for differences between tweets\ncontaining fake news and tweets not containing them. Specifically, we perform\nour analysis only on tweets that went viral, by studying proxies for users'\nexposure to the tweets, by characterizing accounts spreading fake news, and by\nlooking at their polarization. We found significant differences on the\ndistribution of followers, the number of URLs on tweets, and the verification\nof the users.\n", "versions": [{"version": "v1", "created": "Sat, 16 Dec 2017 18:07:58 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Amador", "Julio", ""], ["Oehmichen", "Axel", ""], ["Molina-Solana", "Miguel", ""]]}, {"id": "1712.06006", "submitter": "Ryan Turner", "authors": "Ryan Turner, Brady Neal", "title": "How well does your sampler really work?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new data-driven benchmark system to evaluate the performance of\nnew MCMC samplers. Taking inspiration from the COCO benchmark in optimization,\nwe view this task as having critical importance to machine learning and\nstatistics given the rate at which new samplers are proposed. The common\nhand-crafted examples to test new samplers are unsatisfactory; we take a\nmeta-learning-like approach to generate benchmark examples from a large corpus\nof data sets and models. Surrogates of posteriors found in real problems are\ncreated using highly flexible density models including modern neural network\nbased approaches. We provide new insights into the real effective sample size\nof various samplers per unit time and the estimation efficiency of the samplers\nper sample. Additionally, we provide a meta-analysis to assess the predictive\nutility of various MCMC diagnostics and perform a nonparametric regression to\ncombine them.\n", "versions": [{"version": "v1", "created": "Sat, 16 Dec 2017 18:56:20 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Turner", "Ryan", ""], ["Neal", "Brady", ""]]}, {"id": "1712.06028", "submitter": "Bhavya Kailkhura", "authors": "Bhavya Kailkhura, Jayaraman J. Thiagarajan, Charvi Rastogi, Pramod K.\n  Varshney, Peer-Timo Bremer", "title": "A Spectral Approach for the Design of Experiments: Design, Analysis and\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new approach to construct high quality space-filling\nsample designs. First, we propose a novel technique to quantify the\nspace-filling property and optimally trade-off uniformity and randomness in\nsample designs in arbitrary dimensions. Second, we connect the proposed metric\n(defined in the spatial domain) to the objective measure of the design\nperformance (defined in the spectral domain). This connection serves as an\nanalytic framework for evaluating the qualitative properties of space-filling\ndesigns in general. Using the theoretical insights provided by this\nspatial-spectral analysis, we derive the notion of optimal space-filling\ndesigns, which we refer to as space-filling spectral designs. Third, we propose\nan efficient estimator to evaluate the space-filling properties of sample\ndesigns in arbitrary dimensions and use it to develop an optimization framework\nto generate high quality space-filling designs. Finally, we carry out a\ndetailed performance comparison on two different applications in 2 to 6\ndimensions: a) image reconstruction and b) surrogate modeling on several\nbenchmark optimization functions and an inertial confinement fusion (ICF)\nsimulation code. We demonstrate that the propose spectral designs significantly\noutperform existing approaches especially in high dimensions.\n", "versions": [{"version": "v1", "created": "Sat, 16 Dec 2017 22:31:52 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Kailkhura", "Bhavya", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Rastogi", "Charvi", ""], ["Varshney", "Pramod K.", ""], ["Bremer", "Peer-Timo", ""]]}, {"id": "1712.06047", "submitter": "Aditya Devarakonda", "authors": "Aditya Devarakonda, Kimon Fountoulakis, James Demmel, Michael W.\n  Mahoney", "title": "Avoiding Synchronization in First-Order Methods for Sparse Convex\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel computing has played an important role in speeding up convex\noptimization methods for big data analytics and large-scale machine learning\n(ML). However, the scalability of these optimization methods is inhibited by\nthe cost of communicating and synchronizing processors in a parallel setting.\nIterative ML methods are particularly sensitive to communication cost since\nthey often require communication every iteration. In this work, we extend\nwell-known techniques from Communication-Avoiding Krylov subspace methods to\nfirst-order, block coordinate descent methods for Support Vector Machines and\nProximal Least-Squares problems. Our Synchronization-Avoiding (SA) variants\nreduce the latency cost by a tunable factor of $s$ at the expense of a factor\nof $s$ increase in flops and bandwidth costs. We show that the SA-variants are\nnumerically stable and can attain large speedups of up to $5.1\\times$ on a Cray\nXC30 supercomputer.\n", "versions": [{"version": "v1", "created": "Sun, 17 Dec 2017 02:15:15 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Devarakonda", "Aditya", ""], ["Fountoulakis", "Kimon", ""], ["Demmel", "James", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1712.06050", "submitter": "Rui Gao", "authors": "Rui Gao, Xi Chen, Anton J. Kleywegt", "title": "Wasserstein Distributionally Robust Optimization and Variation\n  Regularization", "comments": "The paper is previously titled \"Wasserstein Distributional Robustness\n  and Regularization in Statistical Learning\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wasserstein distributionally robust optimization (DRO) has recently achieved\nempirical success for various applications in operations research and machine\nlearning, owing partly to its regularization effect. Although connection\nbetween Wasserstein DRO and regularization has been established in several\nsettings, existing results often require restrictive assumptions, such as\nsmoothness or convexity, that are not satisfied for many problems. In this\npaper, we develop a general theory on the variation regularization effect of\nthe Wasserstein DRO - a new form of regularization that generalizes\ntotal-variation regularization, Lipschitz regularization and gradient\nregularization. Our results cover possibly non-convex and non-smooth losses and\nlosses on non-Euclidean spaces. Examples include multi-item newsvendor,\nportfolio selection, linear prediction, neural networks, manifold learning, and\nintensity estimation for Poisson processes, etc. As an application of our\ntheory of variation regularization, we derive new generalization guarantees for\nadversarial robust learning.\n", "versions": [{"version": "v1", "created": "Sun, 17 Dec 2017 02:47:14 GMT"}, {"version": "v2", "created": "Tue, 26 Dec 2017 15:50:30 GMT"}, {"version": "v3", "created": "Fri, 30 Oct 2020 17:56:21 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Gao", "Rui", ""], ["Chen", "Xi", ""], ["Kleywegt", "Anton J.", ""]]}, {"id": "1712.06061", "submitter": "Praneeth Narayanamurthy", "authors": "Praneeth Narayanamurthy and Namrata Vaswani", "title": "Nearly Optimal Robust Subspace Tracking", "comments": "A [short\n  version](http://proceedings.mlr.press/v80/narayanamurthy18a.html) will be\n  presented at ICML 2018 (Long Talk). arXiv admin note: text overlap with\n  arXiv:1803.00651", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the robust subspace tracking (RST) problem and obtain\none of the first two provable guarantees for it. The goal of RST is to track\nsequentially arriving data vectors that lie in a slowly changing\nlow-dimensional subspace, while being robust to corruption by additive sparse\noutliers. It can also be interpreted as a dynamic (time-varying) extension of\nrobust PCA (RPCA), with the minor difference that RST also requires a short\ntracking delay. We develop a recursive projected compressive sensing algorithm\nthat we call Nearly Optimal RST via ReProCS (ReProCS-NORST) because its\ntracking delay is nearly optimal. We prove that NORST solves both the RST and\nthe dynamic RPCA problems under weakened standard RPCA assumptions, two simple\nextra assumptions (slow subspace change and most outlier magnitudes lower\nbounded), and a few minor assumptions.\n  Our guarantee shows that NORST enjoys a near optimal tracking delay of $O(r\n\\log n \\log(1/\\epsilon))$. Its required delay between subspace change times is\nthe same, and its memory complexity is $n$ times this value. Thus both these\nare also nearly optimal. Here $n$ is the ambient space dimension, $r$ is the\nsubspaces' dimension, and $\\epsilon$ is the tracking accuracy. NORST also has\nthe best outlier tolerance compared with all previous RPCA or RST methods, both\ntheoretically and empirically (including for real videos), without requiring\nany model on how the outlier support is generated. This is possible because of\nthe extra assumptions it uses.\n", "versions": [{"version": "v1", "created": "Sun, 17 Dec 2017 06:14:58 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 00:29:16 GMT"}, {"version": "v3", "created": "Sun, 25 Mar 2018 23:03:55 GMT"}, {"version": "v4", "created": "Fri, 6 Jul 2018 17:41:33 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Narayanamurthy", "Praneeth", ""], ["Vaswani", "Namrata", ""]]}, {"id": "1712.06096", "submitter": "Jong Chul Ye", "authors": "Yeo Hun Yoon, Shujaat Khan, Jaeyoung Huh, and Jong Chul Ye", "title": "Efficient B-mode Ultrasound Image Reconstruction from Sub-sampled RF\n  Data using Deep Learning", "comments": "The title has been changed. This version will appear in IEEE Trans.\n  on Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In portable, three dimensional, and ultra-fast ultrasound imaging systems,\nthere is an increasing demand for the reconstruction of high quality images\nfrom a limited number of radio-frequency (RF) measurements due to receiver (Rx)\nor transmit (Xmit) event sub-sampling. However, due to the presence of side\nlobe artifacts from RF sub-sampling, the standard beamformer often produces\nblurry images with less contrast, which are unsuitable for diagnostic purposes.\nExisting compressed sensing approaches often require either hardware changes or\ncomputationally expensive algorithms, but their quality improvements are\nlimited. To address this problem, here we propose a novel deep learning\napproach that directly interpolates the missing RF data by utilizing redundancy\nin the Rx-Xmit plane. Our extensive experimental results using sub-sampled RF\ndata from a multi-line acquisition B-mode system confirm that the proposed\nmethod can effectively reduce the data rate without sacrificing image quality.\n", "versions": [{"version": "v1", "created": "Sun, 17 Dec 2017 12:15:08 GMT"}, {"version": "v2", "created": "Thu, 21 Dec 2017 03:58:18 GMT"}, {"version": "v3", "created": "Tue, 7 Aug 2018 09:19:13 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Yoon", "Yeo Hun", ""], ["Khan", "Shujaat", ""], ["Huh", "Jaeyoung", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1712.06120", "submitter": "Sivaraman Balakrishnan", "authors": "Sivaraman Balakrishnan and Larry Wasserman", "title": "Hypothesis Testing for High-Dimensional Multinomials: A Selective Review", "comments": "19 pages, 6 figures. Written in memory of Stephen E. Fienberg", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical analysis of discrete data has been the subject of extensive\nstatistical research dating back to the work of Pearson. In this survey we\nreview some recently developed methods for testing hypotheses about\nhigh-dimensional multinomials. Traditional tests like the $\\chi^2$ test and the\nlikelihood ratio test can have poor power in the high-dimensional setting. Much\nof the research in this area has focused on finding tests with asymptotically\nNormal limits and developing (stringent) conditions under which tests have\nNormal limits. We argue that this perspective suffers from a significant\ndeficiency: it can exclude many high-dimensional cases when - despite having\nnon Normal null distributions - carefully designed tests can have high power.\nFinally, we illustrate that taking a minimax perspective and considering\nrefinements of this perspective can lead naturally to powerful and practical\ntests.\n", "versions": [{"version": "v1", "created": "Sun, 17 Dec 2017 14:21:46 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Balakrishnan", "Sivaraman", ""], ["Wasserman", "Larry", ""]]}, {"id": "1712.06132", "submitter": "Sakyasingha Dasgupta", "authors": "Rudy Raymond, Takayuki Osogami and Sakyasingha Dasgupta", "title": "Dynamic Boltzmann Machines for Second Order Moments and Generalized\n  Gaussian Distributions", "comments": "7 pages, 3 figures. Accepted and presented in NIPS 2017 (time-series\n  workshop) at Long Beach, California", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic Boltzmann Machine (DyBM) has been shown highly efficient to predict\ntime-series data. Gaussian DyBM is a DyBM that assumes the predicted data is\ngenerated by a Gaussian distribution whose first-order moment (mean)\ndynamically changes over time but its second-order moment (variance) is fixed.\nHowever, in many financial applications, the assumption is quite limiting in\ntwo aspects. First, even when the data follows a Gaussian distribution, its\nvariance may change over time. Such variance is also related to important\ntemporal economic indicators such as the market volatility. Second, financial\ntime-series data often requires learning datasets generated by the generalized\nGaussian distribution with an additional shape parameter that is important to\napproximate heavy-tailed distributions. Addressing those aspects, we show how\nto extend DyBM that results in significant performance improvement in\npredicting financial time-series data.\n", "versions": [{"version": "v1", "created": "Sun, 17 Dec 2017 16:08:53 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Raymond", "Rudy", ""], ["Osogami", "Takayuki", ""], ["Dasgupta", "Sakyasingha", ""]]}, {"id": "1712.06148", "submitter": "Nathan Killoran", "authors": "Nathan Killoran, Leo J. Lee, Andrew Delong, David Duvenaud, Brendan J.\n  Frey", "title": "Generating and designing DNA with deep generative models", "comments": "NIPS 2017 Computational Biology Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose generative neural network methods to generate DNA sequences and\ntune them to have desired properties. We present three approaches: creating\nsynthetic DNA sequences using a generative adversarial network; a DNA-based\nvariant of the activation maximization (\"deep dream\") design method; and a\njoint procedure which combines these two approaches together. We show that\nthese tools capture important structures of the data and, when applied to\ndesigning probes for protein binding microarrays, allow us to generate new\nsequences whose properties are estimated to be superior to those found in the\ntraining data. We believe that these results open the door for applying deep\ngenerative models to advance genomics research.\n", "versions": [{"version": "v1", "created": "Sun, 17 Dec 2017 17:23:10 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Killoran", "Nathan", ""], ["Lee", "Leo J.", ""], ["Delong", "Andrew", ""], ["Duvenaud", "David", ""], ["Frey", "Brendan J.", ""]]}, {"id": "1712.06199", "submitter": "David Alvarez-Melis", "authors": "David Alvarez-Melis, Tommi S. Jaakkola, Stefanie Jegelka", "title": "Structured Optimal Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal Transport has recently gained interest in machine learning for\napplications ranging from domain adaptation, sentence similarities to deep\nlearning. Yet, its ability to capture frequently occurring structure beyond the\n\"ground metric\" is limited. In this work, we develop a nonlinear generalization\nof (discrete) optimal transport that is able to reflect much additional\nstructure. We demonstrate how to leverage the geometry of this new model for\nfast algorithms, and explore connections and properties. Illustrative\nexperiments highlight the benefit of the induced structured couplings for tasks\nin domain adaptation and natural language processing.\n", "versions": [{"version": "v1", "created": "Sun, 17 Dec 2017 22:51:40 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Alvarez-Melis", "David", ""], ["Jaakkola", "Tommi S.", ""], ["Jegelka", "Stefanie", ""]]}, {"id": "1712.06203", "submitter": "Kuang Gong", "authors": "Kuang Gong, Jaewon Yang, Kyungsang Kim, Georges El Fakhri, Youngho\n  Seo, Quanzheng Li", "title": "Attenuation correction for brain PET imaging using deep neural network\n  based on dixon and ZTE MR images", "comments": "15 pages, 12 figures", "journal-ref": null, "doi": "10.1088/1361-6560/aac763", "report-no": null, "categories": "physics.med-ph cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Positron Emission Tomography (PET) is a functional imaging modality widely\nused in neuroscience studies. To obtain meaningful quantitative results from\nPET images, attenuation correction is necessary during image reconstruction.\nFor PET/MR hybrid systems, PET attenuation is challenging as Magnetic Resonance\n(MR) images do not reflect attenuation coefficients directly. To address this\nissue, we present deep neural network methods to derive the continuous\nattenuation coefficients for brain PET imaging from MR images. With only Dixon\nMR images as the network input, the existing U-net structure was adopted and\nanalysis using forty patient data sets shows it is superior than other Dixon\nbased methods. When both Dixon and zero echo time (ZTE) images are available,\nwe have proposed a modified U-net structure, named GroupU-net, to efficiently\nmake use of both Dixon and ZTE information through group convolution modules\nwhen the network goes deeper. Quantitative analysis based on fourteen real\npatient data sets demonstrates that both network approaches can perform better\nthan the standard methods, and the proposed network structure can further\nreduce the PET quantification error compared to the U-net structure.\n", "versions": [{"version": "v1", "created": "Sun, 17 Dec 2017 23:07:35 GMT"}, {"version": "v2", "created": "Thu, 24 May 2018 20:19:24 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Gong", "Kuang", ""], ["Yang", "Jaewon", ""], ["Kim", "Kyungsang", ""], ["Fakhri", "Georges El", ""], ["Seo", "Youngho", ""], ["Li", "Quanzheng", ""]]}, {"id": "1712.06206", "submitter": "James Murphy", "authors": "Anna Little, Mauro Maggioni, James M. Murphy", "title": "Path-Based Spectral Clustering: Guarantees, Robustness to Outliers, and\n  Fast Algorithms", "comments": "59 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of clustering with the longest-leg path distance\n(LLPD) metric, which is informative for elongated and irregularly shaped\nclusters. We prove finite-sample guarantees on the performance of clustering\nwith respect to this metric when random samples are drawn from multiple\nintrinsically low-dimensional clusters in high-dimensional space, in the\npresence of a large number of high-dimensional outliers. By combining these\nresults with spectral clustering with respect to LLPD, we provide conditions\nunder which the Laplacian eigengap statistic correctly determines the number of\nclusters for a large class of data sets, and prove guarantees on the labeling\naccuracy of the proposed algorithm. Our methods are quite general and provide\nperformance guarantees for spectral clustering with any ultrametric. We also\nintroduce an efficient, easy to implement approximation algorithm for the LLPD\nbased on a multiscale analysis of adjacency graphs, which allows for the\nruntime of LLPD spectral clustering to be quasilinear in the number of data\npoints.\n", "versions": [{"version": "v1", "created": "Sun, 17 Dec 2017 23:16:49 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 13:42:00 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Little", "Anna", ""], ["Maggioni", "Mauro", ""], ["Murphy", "James M.", ""]]}, {"id": "1712.06214", "submitter": "David Ledbetter", "authors": "Cameron Carlin, Long Van Ho, David Ledbetter, Melissa Aczon, Randall\n  Wetzel", "title": "Predicting Individual Physiologically Acceptable States for Discharge\n  from a Pediatric Intensive Care Unit", "comments": "8 pages with appendix, 6 figures (4 of which comprise 1 meta figure),\n  4 tables in main section, 10 tables including appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Predict patient-specific vitals deemed medically acceptable for\ndischarge from a pediatric intensive care unit (ICU). Design: The means of each\npatient's hr, sbp and dbp measurements between their medical and physical\ndischarge from the ICU were computed as a proxy for their physiologically\nacceptable state space (PASS) for successful ICU discharge. These individual\nPASS values were compared via root mean squared error (rMSE) to population\nage-normal vitals, a polynomial regression through the PASS values of a\nPediatric ICU (PICU) population and predictions from two recurrent neural\nnetwork models designed to predict personalized PASS within the first twelve\nhours following ICU admission. Setting: PICU at Children's Hospital Los Angeles\n(CHLA). Patients: 6,899 PICU episodes (5,464 patients) collected between 2009\nand 2016. Interventions: None. Measurements: Each episode data contained 375\nvariables representing vitals, labs, interventions, and drugs. They also\nincluded a time indicator for PICU medical discharge and physical discharge.\nMain Results: The rMSEs between individual PASS values and population\nage-normals (hr: 25.9 bpm, sbp: 13.4 mmHg, dbp: 13.0 mmHg) were larger than the\nrMSEs corresponding to the polynomial regression (hr: 19.1 bpm, sbp: 12.3 mmHg,\ndbp: 10.8 mmHg). The rMSEs from the best performing RNN model were the lowest\n(hr: 16.4 bpm; sbp: 9.9 mmHg, dbp: 9.0 mmHg). Conclusion: PICU patients are a\nunique subset of the general population, and general age-normal vitals may not\nbe suitable as target values indicating physiologic stability at discharge.\nAge-normal vitals that were specifically derived from the medical-to-physical\ndischarge window of ICU patients may be more appropriate targets for\n'acceptable' physiologic state for critical care patients. Going beyond simple\nage bins, an RNN model can provide more personalized target values.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 01:13:53 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Carlin", "Cameron", ""], ["Van Ho", "Long", ""], ["Ledbetter", "David", ""], ["Aczon", "Melissa", ""], ["Wetzel", "Randall", ""]]}, {"id": "1712.06229", "submitter": "Chen Gao", "authors": "Brian E. Moore, Chen Gao, Raj Rao Nadakuditi", "title": "Panoramic Robust PCA for Foreground-Background Separation on Noisy,\n  Free-Motion Camera Video", "comments": "IEEE TCI. Project webpage: https://gaochen315.github.io/pRPCA/ Code:\n  https://github.com/gaochen315/panoramicRPCA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a new robust PCA method for foreground-background\nseparation on freely moving camera video with possible dense and sparse\ncorruptions. Our proposed method registers the frames of the corrupted video\nand then encodes the varying perspective arising from camera motion as missing\ndata in a global model. This formulation allows our algorithm to produce a\npanoramic background component that automatically stitches together corrupted\ndata from partially overlapping frames to reconstruct the full field of view.\nWe model the registered video as the sum of a low-rank component that captures\nthe background, a smooth component that captures the dynamic foreground of the\nscene, and a sparse component that isolates possible outliers and other sparse\ncorruptions in the video. The low-rank portion of our model is based on a\nrecent low-rank matrix estimator (OptShrink) that has been shown to yield\nsuperior low-rank subspace estimates in practice. To estimate the smooth\nforeground component of our model, we use a weighted total variation framework\nthat enables our method to reliably decouple the true foreground of the video\nfrom sparse corruptions. We perform extensive numerical experiments on both\nstatic and moving camera video subject to a variety of dense and sparse\ncorruptions. Our experiments demonstrate the state-of-the-art performance of\nour proposed method compared to existing methods both in terms of foreground\nand background estimation accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 02:45:54 GMT"}, {"version": "v2", "created": "Thu, 3 Jan 2019 18:59:20 GMT"}, {"version": "v3", "created": "Fri, 4 Jan 2019 03:42:51 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Moore", "Brian E.", ""], ["Gao", "Chen", ""], ["Nadakuditi", "Raj Rao", ""]]}, {"id": "1712.06245", "submitter": "Zhuoran Yang", "authors": "Zhuoran Yang, Lin F. Yang, Ethan X. Fang, Tuo Zhao, Zhaoran Wang,\n  Matey Neykov", "title": "Misspecified Nonconvex Statistical Optimization for Phase Retrieval", "comments": "56 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing nonconvex statistical optimization theory and methods crucially rely\non the correct specification of the underlying \"true\" statistical models. To\naddress this issue, we take a first step towards taming model misspecification\nby studying the high-dimensional sparse phase retrieval problem with\nmisspecified link functions. In particular, we propose a simple variant of the\nthresholded Wirtinger flow algorithm that, given a proper initialization,\nlinearly converges to an estimator with optimal statistical accuracy for a\nbroad family of unknown link functions. We further provide extensive numerical\nexperiments to support our theoretical findings.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 04:06:08 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Yang", "Zhuoran", ""], ["Yang", "Lin F.", ""], ["Fang", "Ethan X.", ""], ["Zhao", "Tuo", ""], ["Wang", "Zhaoran", ""], ["Neykov", "Matey", ""]]}, {"id": "1712.06246", "submitter": "Guoqing Chao", "authors": "Guoqing Chao, Shiliang Sun, Jinbo Bi", "title": "A Survey on Multi-View Clustering", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With advances in information acquisition technologies, multi-view data become\nubiquitous. Multi-view learning has thus become more and more popular in\nmachine learning and data mining fields. Multi-view unsupervised or\nsemi-supervised learning, such as co-training, co-regularization has gained\nconsiderable attention. Although recently, multi-view clustering (MVC) methods\nhave been developed rapidly, there has not been a survey to summarize and\nanalyze the current progress. Therefore, this paper reviews the common\nstrategies for combining multiple views of data and based on this summary we\npropose a novel taxonomy of the MVC approaches. We further discuss the\nrelationships between MVC and multi-view representation, ensemble clustering,\nmulti-task clustering, multi-view supervised and semi-supervised learning.\nSeveral representative real-world applications are elaborated. To promote\nfuture development of MVC, we envision several open problems that may require\nfurther investigation and thorough examination.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 04:07:42 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 04:20:51 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Chao", "Guoqing", ""], ["Sun", "Shiliang", ""], ["Bi", "Jinbo", ""]]}, {"id": "1712.06260", "submitter": "Takashi Matsubara", "authors": "Takashi Matsubara, Tetsuo Tashiro, Kuniaki Uehara", "title": "Deep Neural Generative Model of Functional MRI Images for Psychiatric\n  Disorder Diagnosis", "comments": "accepted version, 12 pages", "journal-ref": "IEEE Transactions on Biomedical Engineering, 2019", "doi": "10.1109/TBME.2019.2895663", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate diagnosis of psychiatric disorders plays a critical role in\nimproving the quality of life for patients and potentially supports the\ndevelopment of new treatments. Many studies have been conducted on machine\nlearning techniques that seek brain imaging data for specific biomarkers of\ndisorders. These studies have encountered the following dilemma: A direct\nclassification overfits to a small number of high-dimensional samples but\nunsupervised feature-extraction has the risk of extracting a signal of no\ninterest. In addition, such studies often provided only diagnoses for patients\nwithout presenting the reasons for these diagnoses. This study proposed a deep\nneural generative model of resting-state functional magnetic resonance imaging\n(fMRI) data. The proposed model is conditioned by the assumption of the\nsubject's state and estimates the posterior probability of the subject's state\ngiven the imaging data, using Bayes' rule. This study applied the proposed\nmodel to diagnose schizophrenia and bipolar disorders. Diagnostic accuracy was\nimproved by a large margin over competitive approaches, namely classifications\nof functional connectivity, discriminative/generative models of region-wise\nsignals, and those with unsupervised feature-extractors. The proposed model\nvisualizes brain regions largely related to the disorders, thus motivating\nfurther biological investigation.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 06:16:18 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 02:34:36 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Matsubara", "Takashi", ""], ["Tashiro", "Tetsuo", ""], ["Uehara", "Kuniaki", ""]]}, {"id": "1712.06283", "submitter": "Luca Franceschi", "authors": "Luca Franceschi, Michele Donini, Paolo Frasconi, Massimiliano Pontil", "title": "A Bridge Between Hyperparameter Optimization and Learning-to-learn", "comments": "NIPS 2017 workshop on Meta-learning (http://metalearning.ml/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a class of a nested optimization problems involving inner and\nouter objectives. We observe that by taking into explicit account the\noptimization dynamics for the inner objective it is possible to derive a\ngeneral framework that unifies gradient-based hyperparameter optimization and\nmeta-learning (or learning-to-learn). Depending on the specific setting, the\nvariables of the outer objective take either the meaning of hyperparameters in\na supervised learning problem or parameters of a meta-learner. We show that\nsome recently proposed methods in the latter setting can be instantiated in our\nframework and tackled with the same gradient-based algorithms. Finally, we\ndiscuss possible design patterns for learning-to-learn and present encouraging\npreliminary experiments for few-shot learning.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 08:02:50 GMT"}, {"version": "v2", "created": "Sun, 4 Feb 2018 11:58:30 GMT"}, {"version": "v3", "created": "Wed, 21 Aug 2019 10:40:54 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Franceschi", "Luca", ""], ["Donini", "Michele", ""], ["Frasconi", "Paolo", ""], ["Pontil", "Massimiliano", ""]]}, {"id": "1712.06302", "submitter": "Jose Oramas", "authors": "Jose Oramas, Kaili Wang, Tinne Tuytelaars", "title": "Visual Explanation by Interpretation: Improving Visual Feedback\n  Capabilities of Deep Neural Networks", "comments": "Accepted at International Conference on Learning Representations\n  (ICLR) 2019. Project website:\n  http://homes.esat.kuleuven.be/~joramas/projects/visualExplanationByInterpretation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretation and explanation of deep models is critical towards wide\nadoption of systems that rely on them. In this paper, we propose a novel scheme\nfor both interpretation as well as explanation in which, given a pretrained\nmodel, we automatically identify internal features relevant for the set of\nclasses considered by the model, without relying on additional annotations. We\ninterpret the model through average visualizations of this reduced set of\nfeatures. Then, at test time, we explain the network prediction by accompanying\nthe predicted class label with supporting visualizations derived from the\nidentified features. In addition, we propose a method to address the artifacts\nintroduced by stridded operations in deconvNet-based visualizations. Moreover,\nwe introduce an8Flower, a dataset specifically designed for objective\nquantitative evaluation of methods for visual explanation.Experiments on the\nMNIST,ILSVRC12,Fashion144k and an8Flower datasets show that our method produces\ndetailed explanations with good coverage of relevant features of the classes of\ninterest\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 09:17:44 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 15:04:25 GMT"}, {"version": "v3", "created": "Fri, 8 Mar 2019 12:11:15 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Oramas", "Jose", ""], ["Wang", "Kaili", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1712.06391", "submitter": "Xudong Mao", "authors": "Xudong Mao, Qing Li, Haoran Xie, Raymond Y.K. Lau, Zhen Wang, Stephen\n  Paul Smolley", "title": "On the Effectiveness of Least Squares Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning with generative adversarial networks (GANs) has proven\nto be hugely successful. Regular GANs hypothesize the discriminator as a\nclassifier with the sigmoid cross entropy loss function. However, we found that\nthis loss function may lead to the vanishing gradients problem during the\nlearning process. To overcome such a problem, we propose in this paper the\nLeast Squares Generative Adversarial Networks (LSGANs) which adopt the least\nsquares loss for both the discriminator and the generator. We show that\nminimizing the objective function of LSGAN yields minimizing the Pearson\n$\\chi^2$ divergence. We also show that the derived objective function that\nyields minimizing the Pearson $\\chi^2$ divergence performs better than the\nclassical one of using least squares for classification. There are two benefits\nof LSGANs over regular GANs. First, LSGANs are able to generate higher quality\nimages than regular GANs. Second, LSGANs perform more stably during the\nlearning process. For evaluating the image quality, we conduct both qualitative\nand quantitative experiments, and the experimental results show that LSGANs can\ngenerate higher quality images than regular GANs. Furthermore, we evaluate the\nstability of LSGANs in two groups. One is to compare between LSGANs and regular\nGANs without gradient penalty. We conduct three experiments, including Gaussian\nmixture distribution, difficult architectures, and a newly proposed method ---\ndatasets with small variability, to illustrate the stability of LSGANs. The\nother one is to compare between LSGANs with gradient penalty (LSGANs-GP) and\nWGANs with gradient penalty (WGANs-GP). The experimental results show that\nLSGANs-GP succeed in training for all the difficult architectures used in\nWGANs-GP, including 101-layer ResNet.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 13:36:09 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2018 07:48:53 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Mao", "Xudong", ""], ["Li", "Qing", ""], ["Xie", "Haoran", ""], ["Lau", "Raymond Y. K.", ""], ["Wang", "Zhen", ""], ["Smolley", "Stephen Paul", ""]]}, {"id": "1712.06424", "submitter": "Danyang Sun", "authors": "Danyang Sun, Tongzheng Ren, Chongxun Li, Hang Su, Jun Zhu", "title": "Learning to Write Stylized Chinese Characters by Reading a Handful of\n  Examples", "comments": "Accepted by IJCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically writing stylized Chinese characters is an attractive yet\nchallenging task due to its wide applicabilities. In this paper, we propose a\nnovel framework named Style-Aware Variational Auto-Encoder (SA-VAE) to flexibly\ngenerate Chinese characters. Specifically, we propose to capture the different\ncharacteristics of a Chinese character by disentangling the latent features\ninto content-related and style-related components. Considering of the complex\nshapes and structures, we incorporate the structure information as prior\nknowledge into our framework to guide the generation. Our framework shows a\npowerful one-shot/low-shot generalization ability by inferring the style\ncomponent given a character with unseen style. To the best of our knowledge,\nthis is the first attempt to learn to write new-style Chinese characters by\nobserving only one or a few examples. Extensive experiments demonstrate its\neffectiveness in generating different stylized Chinese characters by fusing the\nfeature vectors corresponding to different contents and styles, which is of\nsignificant importance in real-world applications.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 13:33:51 GMT"}, {"version": "v2", "created": "Thu, 21 Dec 2017 09:18:22 GMT"}, {"version": "v3", "created": "Mon, 18 Jun 2018 11:17:09 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Sun", "Danyang", ""], ["Ren", "Tongzheng", ""], ["Li", "Chongxun", ""], ["Su", "Hang", ""], ["Zhu", "Jun", ""]]}, {"id": "1712.06527", "submitter": "Debora Marks", "authors": "Adam J. Riesselman, John B. Ingraham, Debora S. Marks", "title": "Deep generative models of genetic variation capture mutation effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cond-mat.dis-nn physics.bio-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The functions of proteins and RNAs are determined by a myriad of interactions\nbetween their constituent residues, but most quantitative models of how\nmolecular phenotype depends on genotype must approximate this by simple\nadditive effects. While recent models have relaxed this constraint to also\naccount for pairwise interactions, these approaches do not provide a tractable\npath towards modeling higher-order dependencies. Here, we show how latent\nvariable models with nonlinear dependencies can be applied to capture\nbeyond-pairwise constraints in biomolecules. We present a new probabilistic\nmodel for sequence families, DeepSequence, that can predict the effects of\nmutations across a variety of deep mutational scanning experiments\nsignificantly better than site independent or pairwise models that are based on\nthe same evolutionary data. The model, learned in an unsupervised manner solely\nfrom sequence information, is grounded with biologically motivated priors,\nreveals latent organization of sequence families, and can be used to\nextrapolate to new parts of sequence space\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 17:13:08 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Riesselman", "Adam J.", ""], ["Ingraham", "John B.", ""], ["Marks", "Debora S.", ""]]}, {"id": "1712.06536", "submitter": "Erik Bodin", "authors": "Erik Bodin, Iman Malik, Carl Henrik Ek, Neill D. F. Campbell", "title": "Nonparametric Inference for Auto-Encoding Variational Bayes", "comments": "Presented at NIPS 2017 Workshop on Advances in Approximate Bayesian\n  Inference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We would like to learn latent representations that are low-dimensional and\nhighly interpretable. A model that has these characteristics is the Gaussian\nProcess Latent Variable Model. The benefits and negative of the GP-LVM are\ncomplementary to the Variational Autoencoder, the former provides interpretable\nlow-dimensional latent representations while the latter is able to handle large\namounts of data and can use non-Gaussian likelihoods. Our inspiration for this\npaper is to marry these two approaches and reap the benefits of both. In order\nto do so we will introduce a novel approximate inference scheme inspired by the\nGP-LVM and the VAE. We show experimentally that the approximation allows the\ncapacity of the generative bottle-neck (Z) of the VAE to be arbitrarily large\nwithout losing a highly interpretable representation, allowing reconstruction\nquality to be unlimited by Z at the same time as a low-dimensional space can be\nused to perform ancestral sampling from as well as a means to reason about the\nembedded data.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 17:22:41 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Bodin", "Erik", ""], ["Malik", "Iman", ""], ["Ek", "Carl Henrik", ""], ["Campbell", "Neill D. F.", ""]]}, {"id": "1712.06541", "submitter": "Ohad Shamir", "authors": "Noah Golowich, Alexander Rakhlin and Ohad Shamir", "title": "Size-Independent Sample Complexity of Neural Networks", "comments": "Fixed a bug in the proof of theorem 7 (not affecting theorem\n  statement), by slightly changing the construction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the sample complexity of learning neural networks, by providing new\nbounds on their Rademacher complexity assuming norm constraints on the\nparameter matrix of each layer. Compared to previous work, these complexity\nbounds have improved dependence on the network depth, and under some additional\nassumptions, are fully independent of the network size (both depth and width).\nThese results are derived using some novel techniques, which may be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 17:26:15 GMT"}, {"version": "v2", "created": "Wed, 10 Jan 2018 14:36:45 GMT"}, {"version": "v3", "created": "Wed, 30 May 2018 13:58:23 GMT"}, {"version": "v4", "created": "Wed, 6 Jun 2018 15:08:35 GMT"}, {"version": "v5", "created": "Sun, 17 Nov 2019 07:41:30 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Golowich", "Noah", ""], ["Rakhlin", "Alexander", ""], ["Shamir", "Ohad", ""]]}, {"id": "1712.06559", "submitter": "Siyuan Ma", "authors": "Siyuan Ma, Raef Bassily and Mikhail Belkin", "title": "The Power of Interpolation: Understanding the Effectiveness of SGD in\n  Modern Over-parametrized Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we aim to formally explain the phenomenon of fast convergence\nof SGD observed in modern machine learning. The key observation is that most\nmodern learning architectures are over-parametrized and are trained to\ninterpolate the data by driving the empirical loss (classification and\nregression) close to zero. While it is still unclear why these interpolated\nsolutions perform well on test data, we show that these regimes allow for fast\nconvergence of SGD, comparable in number of iterations to full gradient\ndescent.\n  For convex loss functions we obtain an exponential convergence bound for {\\it\nmini-batch} SGD parallel to that for full gradient descent. We show that there\nis a critical batch size $m^*$ such that: (a) SGD iteration with mini-batch\nsize $m\\leq m^*$ is nearly equivalent to $m$ iterations of mini-batch size $1$\n(\\emph{linear scaling regime}). (b) SGD iteration with mini-batch $m> m^*$ is\nnearly equivalent to a full gradient descent iteration (\\emph{saturation\nregime}).\n  Moreover, for the quadratic loss, we derive explicit expressions for the\noptimal mini-batch and step size and explicitly characterize the two regimes\nabove. The critical mini-batch size can be viewed as the limit for effective\nmini-batch parallelization. It is also nearly independent of the data size,\nimplying $O(n)$ acceleration over GD per unit of computation. We give\nexperimental evidence on real data which closely follows our theoretical\nanalyses.\n  Finally, we show how our results fit in the recent developments in training\ndeep neural networks and discuss connections to adaptive rates for SGD and\nvariance reduction.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 18:10:39 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 21:35:54 GMT"}, {"version": "v3", "created": "Thu, 14 Jun 2018 18:07:01 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Ma", "Siyuan", ""], ["Bassily", "Raef", ""], ["Belkin", "Mikhail", ""]]}, {"id": "1712.06657", "submitter": "Andreas Holzinger", "authors": "Andreas Holzinger, Bernd Malle, Peter Kieseberg, Peter M. Roth, Heimo\n  M\\\"uller, Robert Reihs, Kurt Zatloukal", "title": "Towards the Augmented Pathologist: Challenges of Explainable-AI in\n  Digital Pathology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital pathology is not only one of the most promising fields of diagnostic\nmedicine, but at the same time a hot topic for fundamental research. Digital\npathology is not just the transfer of histopathological slides into digital\nrepresentations. The combination of different data sources (images, patient\nrecords, and *omics data) together with current advances in artificial\nintelligence/machine learning enable to make novel information accessible and\nquantifiable to a human expert, which is not yet available and not exploited in\ncurrent medical settings. The grand goal is to reach a level of usable\nintelligence to understand the data in the context of an application task,\nthereby making machine decisions transparent, interpretable and explainable.\nThe foundation of such an \"augmented pathologist\" needs an integrated approach:\nWhile machine learning algorithms require many thousands of training examples,\na human expert is often confronted with only a few data points. Interestingly,\nhumans can learn from such few examples and are able to instantly interpret\ncomplex patterns. Consequently, the grand goal is to combine the possibilities\nof artificial intelligence with human intelligence and to find a well-suited\nbalance between them to enable what neither of them could do on their own. This\ncan raise the quality of education, diagnosis, prognosis and prediction of\ncancer and other diseases. In this paper we describe some (incomplete) research\nissues which we believe should be addressed in an integrated and concerted\neffort for paving the way towards the augmented pathologist.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 20:15:02 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Holzinger", "Andreas", ""], ["Malle", "Bernd", ""], ["Kieseberg", "Peter", ""], ["Roth", "Peter M.", ""], ["M\u00fcller", "Heimo", ""], ["Reihs", "Robert", ""], ["Zatloukal", "Kurt", ""]]}, {"id": "1712.06658", "submitter": "Farshid Rayhan", "authors": "Farshid Rayhan, Sajid Ahmed, Asif Mahbub, Md. Rafsan Jani, Swakkhar\n  Shatabda, Dewan Md. Farid and Chowdhury Mofizur Rahman", "title": "MEBoost: Mixing Estimators with Boosting for Imbalanced Data\n  Classification", "comments": "SKIMA-2017", "journal-ref": null, "doi": "10.1109/SKIMA.2017.8294128", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Class imbalance problem has been a challenging research problem in the fields\nof machine learning and data mining as most real life datasets are imbalanced.\nSeveral existing machine learning algorithms try to maximize the accuracy\nclassification by correctly identifying majority class samples while ignoring\nthe minority class. However, the concept of the minority class instances\nusually represents a higher interest than the majority class. Recently, several\ncost sensitive methods, ensemble models and sampling techniques have been used\nin literature in order to classify imbalance datasets. In this paper, we\npropose MEBoost, a new boosting algorithm for imbalanced datasets. MEBoost\nmixes two different weak learners with boosting to improve the performance on\nimbalanced datasets. MEBoost is an alternative to the existing techniques such\nas SMOTEBoost, RUSBoost, Adaboost, etc. The performance of MEBoost has been\nevaluated on 12 benchmark imbalanced datasets with state of the art ensemble\nmethods like SMOTEBoost, RUSBoost, Easy Ensemble, EUSBoost, DataBoost.\nExperimental results show significant improvement over the other methods and it\ncan be concluded that MEBoost is an effective and promising algorithm to deal\nwith imbalance datasets. The python version of the code is available here:\nhttps://github.com/farshidrayhanuiu/\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 20:18:30 GMT"}, {"version": "v2", "created": "Sat, 13 Jan 2018 19:21:33 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Rayhan", "Farshid", ""], ["Ahmed", "Sajid", ""], ["Mahbub", "Asif", ""], ["Jani", "Md. Rafsan", ""], ["Shatabda", "Swakkhar", ""], ["Farid", "Dewan Md.", ""], ["Rahman", "Chowdhury Mofizur", ""]]}, {"id": "1712.06695", "submitter": "Yash Deshpande", "authors": "Yash Deshpande, Lester Mackey, Vasilis Syrgkanis, Matt Taddy", "title": "Accurate Inference for Adaptive Linear Models", "comments": "Typos fixed for clarification", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimators computed from adaptively collected data do not behave like their\nnon-adaptive brethren. Rather, the sequential dependence of the collection\npolicy can lead to severe distributional biases that persist even in the\ninfinite data limit. We develop a general method -- $\\mathbf{W}$-decorrelation\n-- for transforming the bias of adaptive linear regression estimators into\nvariance. The method uses only coarse-grained information about the data\ncollection policy and does not need access to propensity scores or exact\nknowledge of the policy. We bound the finite-sample bias and variance of the\n$\\mathbf{W}$-estimator and develop asymptotically correct confidence intervals\nbased on a novel martingale central limit theorem. We then demonstrate the\nempirical benefits of the generic $\\mathbf{W}$-decorrelation procedure in two\ndifferent adaptive data settings: the multi-armed bandit and the autoregressive\ntime series.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 22:07:29 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 21:30:24 GMT"}, {"version": "v3", "created": "Wed, 14 Aug 2019 14:11:49 GMT"}, {"version": "v4", "created": "Thu, 5 Sep 2019 20:04:37 GMT"}, {"version": "v5", "created": "Thu, 2 Jan 2020 22:33:42 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Deshpande", "Yash", ""], ["Mackey", "Lester", ""], ["Syrgkanis", "Vasilis", ""], ["Taddy", "Matt", ""]]}, {"id": "1712.06704", "submitter": "Kriste Krstovski", "authors": "Kriste Krstovski, Michael J. Kurtz, David A. Smith and Alberto\n  Accomazzi", "title": "Multilingual Topic Models", "comments": "18 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific publications have evolved several features for mitigating\nvocabulary mismatch when indexing, retrieving, and computing similarity between\narticles. These mitigation strategies range from simply focusing on high-value\narticle sections, such as titles and abstracts, to assigning keywords, often\nfrom controlled vocabularies, either manually or through automatic annotation.\nVarious document representation schemes possess different cost-benefit\ntradeoffs. In this paper, we propose to model different representations of the\nsame article as translations of each other, all generated from a common latent\nrepresentation in a multilingual topic model. We start with a methodological\noverview on latent variable models for parallel document representations that\ncould be used across many information science tasks. We then show how solving\nthe inference problem of mapping diverse representations into a shared topic\nspace allows us to evaluate representations based on how topically similar they\nare to the original article. In addition, our proposed approach provides means\nto discover where different concept vocabularies require improvement.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 22:45:20 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Krstovski", "Kriste", ""], ["Kurtz", "Michael J.", ""], ["Smith", "David A.", ""], ["Accomazzi", "Alberto", ""]]}, {"id": "1712.06715", "submitter": "Jiajun Shen", "authors": "Jiajun Shen, Yali Amit", "title": "Deformable Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometric variations of objects, which do not modify the object class, pose a\nmajor challenge for object recognition. These variations could be rigid as well\nas non-rigid transformations. In this paper, we design a framework for training\ndeformable classifiers, where latent transformation variables are introduced,\nand a transformation of the object image to a reference instantiation is\ncomputed in terms of the classifier output, separately for each class. The\nclassifier outputs for each class, after transformation, are compared to yield\nthe final decision. As a by-product of the classification this yields a\ntransformation of the input object to a reference pose, which can be used for\ndownstream tasks such as the computation of object support. We apply a two-step\ntraining mechanism for our framework, which alternates between optimizing over\nthe latent transformation variables and the classifier parameters to minimize\nthe loss function. We show that multilayer perceptrons, also known as deep\nnetworks, are well suited for this approach and achieve state of the art\nresults on the rotated MNIST and the Google Earth dataset, and produce\ncompetitive results on MNIST and CIFAR-10 when training on smaller subsets of\ntraining data.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 23:12:06 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Shen", "Jiajun", ""], ["Amit", "Yali", ""]]}, {"id": "1712.06745", "submitter": "Jun Kitazono", "authors": "Jun Kitazono, Ryota Kanai and Masafumi Oizumi", "title": "Efficient Algorithms for Searching the Minimum Information Partition in\n  Integrated Information Theory", "comments": null, "journal-ref": null, "doi": "10.3390/e20030173", "report-no": null, "categories": "q-bio.NC cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to integrate information in the brain is considered to be an\nessential property for cognition and consciousness. Integrated Information\nTheory (IIT) hypothesizes that the amount of integrated information ($\\Phi$) in\nthe brain is related to the level of consciousness. IIT proposes that to\nquantify information integration in a system as a whole, integrated information\nshould be measured across the partition of the system at which information loss\ncaused by partitioning is minimized, called the Minimum Information Partition\n(MIP). The computational cost for exhaustively searching for the MIP grows\nexponentially with system size, making it difficult to apply IIT to real neural\ndata. It has been previously shown that if a measure of $\\Phi$ satisfies a\nmathematical property, submodularity, the MIP can be found in a polynomial\norder by an optimization algorithm. However, although the first version of\n$\\Phi$ is submodular, the later versions are not. In this study, we empirically\nexplore to what extent the algorithm can be applied to the non-submodular\nmeasures of $\\Phi$ by evaluating the accuracy of the algorithm in simulated\ndata and real neural data. We find that the algorithm identifies the MIP in a\nnearly perfect manner even for the non-submodular measures. Our results show\nthat the algorithm allows us to measure $\\Phi$ in large systems within a\npractical amount of time.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 01:44:58 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 09:38:30 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Kitazono", "Jun", ""], ["Kanai", "Ryota", ""], ["Oizumi", "Masafumi", ""]]}, {"id": "1712.06924", "submitter": "Romain Laroche", "authors": "Romain Laroche, Paul Trichelair, R\\'emi Tachet des Combes", "title": "Safe Policy Improvement with Baseline Bootstrapping", "comments": "accepted as a long oral at ICML2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers Safe Policy Improvement (SPI) in Batch Reinforcement\nLearning (Batch RL): from a fixed dataset and without direct access to the true\nenvironment, train a policy that is guaranteed to perform at least as well as\nthe baseline policy used to collect the data. Our approach, called SPI with\nBaseline Bootstrapping (SPIBB), is inspired by the knows-what-it-knows\nparadigm: it bootstraps the trained policy with the baseline when the\nuncertainty is high. Our first algorithm, $\\Pi_b$-SPIBB, comes with SPI\ntheoretical guarantees. We also implement a variant, $\\Pi_{\\leq b}$-SPIBB, that\nis even more efficient in practice. We apply our algorithms to a motivational\nstochastic gridworld domain and further demonstrate on randomly generated MDPs\nthe superiority of SPIBB with respect to existing algorithms, not only in\nsafety but also in mean performance. Finally, we implement a model-free version\nof SPIBB and show its benefits on a navigation task with deep RL implementation\ncalled SPIBB-DQN, which is, to the best of our knowledge, the first RL\nalgorithm relying on a neural network representation able to train efficiently\nand reliably from batch data, without any interaction with the environment.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 13:43:41 GMT"}, {"version": "v2", "created": "Wed, 20 Dec 2017 19:52:03 GMT"}, {"version": "v3", "created": "Thu, 18 Jan 2018 21:37:53 GMT"}, {"version": "v4", "created": "Thu, 14 Jun 2018 19:54:34 GMT"}, {"version": "v5", "created": "Fri, 7 Jun 2019 17:45:54 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Laroche", "Romain", ""], ["Trichelair", "Paul", ""], ["Combes", "R\u00e9mi Tachet des", ""]]}, {"id": "1712.07004", "submitter": "Rasoul Kaljahi", "authors": "Rasoul Kaljahi, Jennifer Foster", "title": "Any-gram Kernels for Sentence Classification: A Sentiment Analysis Case\n  Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Any-gram kernels are a flexible and efficient way to employ bag-of-n-gram\nfeatures when learning from textual data. They are also compatible with the use\nof word embeddings so that word similarities can be accounted for. While the\noriginal any-gram kernels are implemented on top of tree kernels, we propose a\nnew approach which is independent of tree kernels and is more efficient. We\nalso propose a more effective way to make use of word embeddings than the\noriginal any-gram formulation. When applied to the task of sentiment\nclassification, our new formulation achieves significantly better performance.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 15:47:00 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Kaljahi", "Rasoul", ""], ["Foster", "Jennifer", ""]]}, {"id": "1712.07008", "submitter": "Ardhendu Shekhar Tripathy", "authors": "Ardhendu Tripathy and Ye Wang and Prakash Ishwar", "title": "Privacy-Preserving Adversarial Networks", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.GT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a data-driven framework for optimizing privacy-preserving data\nrelease mechanisms to attain the information-theoretically optimal tradeoff\nbetween minimizing distortion of useful data and concealing specific sensitive\ninformation. Our approach employs adversarially-trained neural networks to\nimplement randomized mechanisms and to perform a variational approximation of\nmutual information privacy. We validate our Privacy-Preserving Adversarial\nNetworks (PPAN) framework via proof-of-concept experiments on discrete and\ncontinuous synthetic data, as well as the MNIST handwritten digits dataset. For\nsynthetic data, our model-agnostic PPAN approach achieves tradeoff points very\nclose to the optimal tradeoffs that are analytically-derived from model\nknowledge. In experiments with the MNIST data, we visually demonstrate a\nlearned tradeoff between minimizing the pixel-level distortion versus\nconcealing the written digit.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 15:53:45 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2019 13:49:31 GMT"}, {"version": "v3", "created": "Wed, 12 Jun 2019 14:42:37 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Tripathy", "Ardhendu", ""], ["Wang", "Ye", ""], ["Ishwar", "Prakash", ""]]}, {"id": "1712.07022", "submitter": "Marzieh Haghighi", "authors": "Marzieh Haghighi, Simon K. Warfield, Sila Kurugol", "title": "Automatic Renal Segmentation in DCE-MRI using Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kidney function evaluation using dynamic contrast-enhanced MRI (DCE-MRI)\nimages could help in diagnosis and treatment of kidney diseases of children.\nAutomatic segmentation of renal parenchyma is an important step in this\nprocess. In this paper, we propose a time and memory efficient fully automated\nsegmentation method which achieves high segmentation accuracy with running time\nin the order of seconds in both normal kidneys and kidneys with hydronephrosis.\nThe proposed method is based on a cascaded application of two 3D convolutional\nneural networks that employs spatial and temporal information at the same time\nin order to learn the tasks of localization and segmentation of kidneys,\nrespectively. Segmentation performance is evaluated on both normal and abnormal\nkidneys with varying levels of hydronephrosis. We achieved a mean dice\ncoefficient of 91.4 and 83.6 for normal and abnormal kidneys of pediatric\npatients, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 16:13:01 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Haghighi", "Marzieh", ""], ["Warfield", "Simon K.", ""], ["Kurugol", "Sila", ""]]}, {"id": "1712.07027", "submitter": "Adil Salim", "authors": "Adil Salim, Pascal Bianchi and Walid Hachem", "title": "Snake: a Stochastic Proximal Gradient Algorithm for Regularized Problems\n  over Large Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A regularized optimization problem over a large unstructured graph is\nstudied, where the regularization term is tied to the graph geometry. Typical\nregularization examples include the total variation and the Laplacian\nregularizations over the graph. When applying the proximal gradient algorithm\nto solve this problem, there exist quite affordable methods to implement the\nproximity operator (backward step) in the special case where the graph is a\nsimple path without loops. In this paper, an algorithm, referred to as \"Snake\",\nis proposed to solve such regularized problems over general graphs, by taking\nbenefit of these fast methods. The algorithm consists in properly selecting\nrandom simple paths in the graph and performing the proximal gradient algorithm\nover these simple paths. This algorithm is an instance of a new general\nstochastic proximal gradient algorithm, whose convergence is proven.\nApplications to trend filtering and graph inpainting are provided among others.\nNumerical experiments are conducted over large graphs.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 16:28:17 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Salim", "Adil", ""], ["Bianchi", "Pascal", ""], ["Hachem", "Walid", ""]]}, {"id": "1712.07042", "submitter": "Marta Stepniewska-Dziubinska", "authors": "Marta M. Stepniewska-Dziubinska, Piotr Zielenkiewicz, and Pawel\n  Siedlecki", "title": "Development and evaluation of a deep learning model for protein-ligand\n  binding affinity prediction", "comments": null, "journal-ref": null, "doi": "10.1093/bioinformatics/bty374/4994792", "report-no": null, "categories": "stat.ML cs.LG q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structure based ligand discovery is one of the most successful approaches for\naugmenting the drug discovery process. Currently, there is a notable shift\ntowards machine learning (ML) methodologies to aid such procedures. Deep\nlearning has recently gained considerable attention as it allows the model to\n\"learn\" to extract features that are relevant for the task at hand. We have\ndeveloped a novel deep neural network estimating the binding affinity of\nligand-receptor complexes. The complex is represented with a 3D grid, and the\nmodel utilizes a 3D convolution to produce a feature map of this\nrepresentation, treating the atoms of both proteins and ligands in the same\nmanner. Our network was tested on the CASF \"scoring power\" benchmark and Astex\nDiverse Set and outperformed classical scoring functions. The model, together\nwith usage instructions and examples, is available as a git repository at\nhttp://gitlab.com/cheminfIBB/pafnucy\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 16:49:01 GMT"}, {"version": "v2", "created": "Wed, 3 Jan 2018 16:15:15 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Stepniewska-Dziubinska", "Marta M.", ""], ["Zielenkiewicz", "Piotr", ""], ["Siedlecki", "Pawel", ""]]}, {"id": "1712.07101", "submitter": "Yingbo Zhou", "authors": "Yingbo Zhou, Caiming Xiong, Richard Socher", "title": "Improving End-to-End Speech Recognition with Policy Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connectionist temporal classification (CTC) is widely used for maximum\nlikelihood learning in end-to-end speech recognition models. However, there is\nusually a disparity between the negative maximum likelihood and the performance\nmetric used in speech recognition, e.g., word error rate (WER). This results in\na mismatch between the objective function and metric during training. We show\nthat the above problem can be mitigated by jointly training with maximum\nlikelihood and policy gradient. In particular, with policy learning we are able\nto directly optimize on the (otherwise non-differentiable) performance metric.\nWe show that joint training improves relative performance by 4% to 13% for our\nend-to-end model as compared to the same model learned through maximum\nlikelihood. The model achieves 5.53% WER on Wall Street Journal dataset, and\n5.42% and 14.70% on Librispeech test-clean and test-other set, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 18:39:50 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Zhou", "Yingbo", ""], ["Xiong", "Caiming", ""], ["Socher", "Richard", ""]]}, {"id": "1712.07102", "submitter": "Shahin Shahrampour", "authors": "Shahin Shahrampour, Ahmad Beirami, Vahid Tarokh", "title": "On Data-Dependent Random Features for Improved Generalization in\n  Supervised Learning", "comments": "12 pages; (pages 1-8) to appear in Proc. of AAAI Conference on\n  Artificial Intelligence (AAAI), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The randomized-feature approach has been successfully employed in large-scale\nkernel approximation and supervised learning. The distribution from which the\nrandom features are drawn impacts the number of features required to\nefficiently perform a learning task. Recently, it has been shown that employing\ndata-dependent randomization improves the performance in terms of the required\nnumber of random features. In this paper, we are concerned with the\nrandomized-feature approach in supervised learning for good generalizability.\nWe propose the Energy-based Exploration of Random Features (EERF) algorithm\nbased on a data-dependent score function that explores the set of possible\nfeatures and exploits the promising regions. We prove that the proposed score\nfunction with high probability recovers the spectrum of the best fit within the\nmodel class. Our empirical results on several benchmark datasets further verify\nthat our method requires smaller number of random features to achieve a certain\ngeneralization error compared to the state-of-the-art while introducing\nnegligible pre-processing overhead. EERF can be implemented in a few lines of\ncode and requires no additional tuning parameters.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 18:40:36 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Shahrampour", "Shahin", ""], ["Beirami", "Ahmad", ""], ["Tarokh", "Vahid", ""]]}, {"id": "1712.07106", "submitter": "Jayaraman J. Thiagarajan", "authors": "Jayaraman J. Thiagarajan, Shusen Liu, Karthikeyan Natesan Ramamurthy,\n  Peer-Timo Bremer", "title": "Exploring High-Dimensional Structure via Axis-Aligned Decomposition of\n  Linear Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-dimensional embeddings remain the dominant approach to visualize high\ndimensional data. The choice of embeddings ranges from highly non-linear ones,\nwhich can capture complex relationships but are difficult to interpret\nquantitatively, to axis-aligned projections, which are easy to interpret but\nare limited to bivariate relationships. Linear project can be considered as a\ncompromise between complexity and interpretability, as they allow explicit axes\nlabels, yet provide significantly more degrees of freedom compared to\naxis-aligned projections. Nevertheless, interpreting the axes directions, which\nare linear combinations often with many non-trivial components, remains\ndifficult. To address this problem we introduce a structure aware decomposition\nof (multiple) linear projections into sparse sets of axis aligned projections,\nwhich jointly capture all information of the original linear ones. In\nparticular, we use tools from Dempster-Shafer theory to formally define how\nrelevant a given axis aligned project is to explain the neighborhood relations\ndisplayed in some linear projection. Furthermore, we introduce a new approach\nto discover a diverse set of high quality linear projections and show that in\npractice the information of $k$ linear projections is often jointly encoded in\n$\\sim k$ axis aligned plots. We have integrated these ideas into an interactive\nvisualization system that allows users to jointly browse both linear\nprojections and their axis aligned representatives. Using a number of case\nstudies we show how the resulting plots lead to more intuitive visualizations\nand new insight.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 18:43:20 GMT"}, {"version": "v2", "created": "Wed, 20 Dec 2017 04:00:03 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Thiagarajan", "Jayaraman J.", ""], ["Liu", "Shusen", ""], ["Ramamurthy", "Karthikeyan Natesan", ""], ["Bremer", "Peer-Timo", ""]]}, {"id": "1712.07107", "submitter": "Xiaoyong Yuan", "authors": "Xiaoyong Yuan, Pan He, Qile Zhu, Xiaolin Li", "title": "Adversarial Examples: Attacks and Defenses for Deep Learning", "comments": "Github: https://github.com/chbrian/awesome-adversarial-examples-dl", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With rapid progress and significant successes in a wide spectrum of\napplications, deep learning is being applied in many safety-critical\nenvironments. However, deep neural networks have been recently found vulnerable\nto well-designed input samples, called adversarial examples. Adversarial\nexamples are imperceptible to human but can easily fool deep neural networks in\nthe testing/deploying stage. The vulnerability to adversarial examples becomes\none of the major risks for applying deep neural networks in safety-critical\nenvironments. Therefore, attacks and defenses on adversarial examples draw\ngreat attention. In this paper, we review recent findings on adversarial\nexamples for deep neural networks, summarize the methods for generating\nadversarial examples, and propose a taxonomy of these methods. Under the\ntaxonomy, applications for adversarial examples are investigated. We further\nelaborate on countermeasures for adversarial examples and explore the\nchallenges and the potential solutions.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 18:44:07 GMT"}, {"version": "v2", "created": "Fri, 5 Jan 2018 15:51:54 GMT"}, {"version": "v3", "created": "Sat, 7 Jul 2018 02:32:57 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Yuan", "Xiaoyong", ""], ["He", "Pan", ""], ["Zhu", "Qile", ""], ["Li", "Xiaolin", ""]]}, {"id": "1712.07108", "submitter": "Yingbo Zhou", "authors": "Yingbo Zhou, Caiming Xiong, Richard Socher", "title": "Improved Regularization Techniques for End-to-End Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization is important for end-to-end speech models, since the models\nare highly flexible and easy to overfit. Data augmentation and dropout has been\nimportant for improving end-to-end models in other domains. However, they are\nrelatively under explored for end-to-end speech models. Therefore, we\ninvestigate the effectiveness of both methods for end-to-end trainable, deep\nspeech recognition models. We augment audio data through random perturbations\nof tempo, pitch, volume, temporal alignment, and adding random noise.We further\ninvestigate the effect of dropout when applied to the inputs of all layers of\nthe network. We show that the combination of data augmentation and dropout give\na relative performance improvement on both Wall Street Journal (WSJ) and\nLibriSpeech dataset of over 20%. Our model performance is also competitive with\nother end-to-end speech models on both datasets.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 18:45:25 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Zhou", "Yingbo", ""], ["Xiong", "Caiming", ""], ["Socher", "Richard", ""]]}, {"id": "1712.07113", "submitter": "Andrew Ilyas", "authors": "Andrew Ilyas, Logan Engstrom, Anish Athalye, Jessy Lin", "title": "Query-Efficient Black-box Adversarial Examples (superceded)", "comments": "Superceded by \"Black-Box Adversarial Attacks with Limited Queries and\n  Information.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Note that this paper is superceded by \"Black-Box Adversarial Attacks with\nLimited Queries and Information.\"\n  Current neural network-based image classifiers are susceptible to adversarial\nexamples, even in the black-box setting, where the attacker is limited to query\naccess without access to gradients. Previous methods --- substitute networks\nand coordinate-based finite-difference methods --- are either unreliable or\nquery-inefficient, making these methods impractical for certain problems.\n  We introduce a new method for reliably generating adversarial examples under\nmore restricted, practical black-box threat models. First, we apply natural\nevolution strategies to perform black-box attacks using two to three orders of\nmagnitude fewer queries than previous methods. Second, we introduce a new\nalgorithm to perform targeted adversarial attacks in the partial-information\nsetting, where the attacker only has access to a limited number of target\nclasses. Using these techniques, we successfully perform the first targeted\nadversarial attack against a commercially deployed machine learning system, the\nGoogle Cloud Vision API, in the partial information setting.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 18:58:10 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 17:20:27 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Ilyas", "Andrew", ""], ["Engstrom", "Logan", ""], ["Athalye", "Anish", ""], ["Lin", "Jessy", ""]]}, {"id": "1712.07177", "submitter": "Dmitri Pavlichin", "authors": "Dmitri S. Pavlichin, Jiantao Jiao, Tsachy Weissman", "title": "Approximate Profile Maximum Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient algorithm for approximate computation of the profile\nmaximum likelihood (PML), a variant of maximum likelihood maximizing the\nprobability of observing a sufficient statistic rather than the empirical\nsample. The PML has appealing theoretical properties, but is difficult to\ncompute exactly. Inspired by observations gleaned from exactly solvable cases,\nwe look for an approximate PML solution, which, intuitively, clumps comparably\nfrequent symbols into one symbol. This amounts to lower-bounding a certain\nmatrix permanent by summing over a subgroup of the symmetric group rather than\nthe whole group during the computation. We extensively experiment with the\napproximate solution, and find the empirical performance of our approach is\ncompetitive and sometimes significantly better than state-of-the-art\nperformance for various estimation problems.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 19:50:07 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Pavlichin", "Dmitri S.", ""], ["Jiao", "Jiantao", ""], ["Weissman", "Tsachy", ""]]}, {"id": "1712.07203", "submitter": "Xiaowei Jia", "authors": "Xiaowei Jia, Ankush Khandelwal, Anuj Karpatne, Vipin Kumar", "title": "Discovery of Shifting Patterns in Sequence Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the multi-variate sequence classification\nproblem from a multi-instance learning perspective. Real-world sequential data\ncommonly show discriminative patterns only at specific time periods. For\ninstance, we can identify a cropland during its growing season, but it looks\nsimilar to a barren land after harvest or before planting. Besides, even within\nthe same class, the discriminative patterns can appear in different periods of\nsequential data. Due to such property, these discriminative patterns are also\nreferred to as shifting patterns. The shifting patterns in sequential data\nseverely degrade the performance of traditional classification methods without\nsufficient training data.\n  We propose a novel sequence classification method by automatically mining\nshifting patterns from multi-variate sequence. The method employs a\nmulti-instance learning approach to detect shifting patterns while also\nmodeling temporal relationships within each multi-instance bag by an LSTM model\nto further improve the classification performance. We extensively evaluate our\nmethod on two real-world applications - cropland mapping and affective state\nrecognition. The experiments demonstrate the superiority of our proposed method\nin sequence classification performance and in detecting discriminative shifting\npatterns.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 20:51:32 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Jia", "Xiaowei", ""], ["Khandelwal", "Ankush", ""], ["Karpatne", "Anuj", ""], ["Kumar", "Vipin", ""]]}, {"id": "1712.07230", "submitter": "Yehezkel Resheff", "authors": "Yehezkel S. Resheff and Moni Shahar", "title": "Fusing Multifaceted Transaction Data for User Modeling and Demographic\n  Prediction", "comments": "IFUP 2018 (WSDM workshop)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring user characteristics such as demographic attributes is of the\nutmost importance in many user-centric applications. Demographic data is an\nenabler of personalization, identity security, and other applications. Despite\nthat, this data is sensitive and often hard to obtain. Previous work has shown\nthat purchase history can be used for multi-task prediction of many demographic\nfields such as gender and marital status. Here we present an embedding based\nmethod to integrate multifaceted sequences of transaction data, together with\nauxiliary relational tables, for better user modeling and demographic\nprediction.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 21:45:06 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Resheff", "Yehezkel S.", ""], ["Shahar", "Moni", ""]]}, {"id": "1712.07233", "submitter": "Pushparaja Murugan", "authors": "Pushparaja Murugan", "title": "Hyperparameters Optimization in Deep Convolutional Neural Network /\n  Bayesian Approach with Gaussian Process Prior", "comments": "10 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Convolutional Neural Network is known as ConvNet have been extensively used\nin many complex machine learning tasks. However, hyperparameters optimization\nis one of a crucial step in developing ConvNet architectures, since the\naccuracy and performance are reliant on the hyperparameters. This multilayered\narchitecture parameterized by a set of hyperparameters such as the number of\nconvolutional layers, number of fully connected dense layers & neurons, the\nprobability of dropout implementation, learning rate. Hence the searching the\nhyperparameter over the hyperparameter space are highly difficult to build such\ncomplex hierarchical architecture. Many methods have been proposed over the\ndecade to explore the hyperparameter space and find the optimum set of\nhyperparameter values. Reportedly, Gird search and Random search are said to be\ninefficient and extremely expensive, due to a large number of hyperparameters\nof the architecture. Hence, Sequential model-based Bayesian Optimization is a\npromising alternative technique to address the extreme of the unknown cost\nfunction. The recent study on Bayesian Optimization by Snoek in nine\nconvolutional network parameters is achieved the lowerest error report in the\nCIFAR-10 benchmark. This article is intended to provide the overview of the\nmathematical concept behind the Bayesian Optimization over a Gaussian prior.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 21:48:56 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Murugan", "Pushparaja", ""]]}, {"id": "1712.07240", "submitter": "John Herr", "authors": "John E. Herr, Kun Yao, Ryker McIntyre, David Toth, and John Parkhill", "title": "Metadynamics for Training Neural Network Model Chemistries: a\n  Competitive Assessment", "comments": null, "journal-ref": null, "doi": "10.1063/1.5020067", "report-no": null, "categories": "physics.chem-ph physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network (NN) model chemistries (MCs) promise to facilitate the\naccurate exploration of chemical space and simulation of large reactive\nsystems. One important path to improving these models is to add layers of\nphysical detail, especially long-range forces. At short range, however, these\nmodels are data driven and data limited. Little is systematically known about\nhow data should be sampled, and `test data' chosen randomly from some sampling\ntechniques can provide poor information about generality. If the sampling\nmethod is narrow `test error' can appear encouragingly tiny while the model\nfails catastrophically elsewhere. In this manuscript we competitively evaluate\ntwo common sampling methods: molecular dynamics (MD), normal-mode sampling\n(NMS) and one uncommon alternative, Metadynamics (MetaMD), for preparing\ntraining geometries. We show that MD is an inefficient sampling method in the\nsense that additional samples do not improve generality. We also show MetaMD is\neasily implemented in any NNMC software package with cost that scales linearly\nwith the number of atoms in a sample molecule. MetaMD is a black-box way to\nensure samples always reach out to new regions of chemical space, while\nremaining relevant to chemistry near $k_bT$. It is one cheap tool to address\nthe issue of generalization.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 22:17:15 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Herr", "John E.", ""], ["Yao", "Kun", ""], ["McIntyre", "Ryker", ""], ["Toth", "David", ""], ["Parkhill", "John", ""]]}, {"id": "1712.07296", "submitter": "Caiming Xiong Mr", "authors": "Huishuai Zhang, Caiming Xiong, James Bradbury, Richard Socher", "title": "Block-diagonal Hessian-free Optimization for Training Neural Networks", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Second-order methods for neural network optimization have several advantages\nover methods based on first-order gradient descent, including better scaling to\nlarge mini-batch sizes and fewer updates needed for convergence. But they are\nrarely applied to deep learning in practice because of high computational cost\nand the need for model-dependent algorithmic variations. We introduce a variant\nof the Hessian-free method that leverages a block-diagonal approximation of the\ngeneralized Gauss-Newton matrix. Our method computes the curvature\napproximation matrix only for pairs of parameters from the same layer or block\nof the neural network and performs conjugate gradient updates independently for\neach block. Experiments on deep autoencoders, deep convolutional networks, and\nmultilayer LSTMs demonstrate better convergence and generalization compared to\nthe original Hessian-free approach and the Adam method.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 02:52:35 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Zhang", "Huishuai", ""], ["Xiong", "Caiming", ""], ["Bradbury", "James", ""], ["Socher", "Richard", ""]]}, {"id": "1712.07316", "submitter": "Stephen Merity", "authors": "Martin Schrimpf, Stephen Merity, James Bradbury, Richard Socher", "title": "A Flexible Approach to Automated RNN Architecture Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of designing neural architectures requires expert knowledge and\nextensive trial and error. While automated architecture search may simplify\nthese requirements, the recurrent neural network (RNN) architectures generated\nby existing methods are limited in both flexibility and components. We propose\na domain-specific language (DSL) for use in automated architecture search which\ncan produce novel RNNs of arbitrary depth and width. The DSL is flexible enough\nto define standard architectures such as the Gated Recurrent Unit and Long\nShort Term Memory and allows the introduction of non-standard RNN components\nsuch as trigonometric curves and layer normalization. Using two different\ncandidate generation techniques, random search with a ranking function and\nreinforcement learning, we explore the novel architectures produced by the RNN\nDSL for language modeling and machine translation domains. The resulting\narchitectures do not follow human intuition yet perform well on their targeted\ntasks, suggesting the space of usable RNN architectures is far larger than\npreviously assumed.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 04:20:40 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Schrimpf", "Martin", ""], ["Merity", "Stephen", ""], ["Bradbury", "James", ""], ["Socher", "Richard", ""]]}, {"id": "1712.07325", "submitter": "Lingzhou Xue", "authors": "Kevin H. Lee, Lingzhou Xue and David R. Hunter", "title": "Model-Based Clustering of Time-Evolving Networks through Temporal\n  Exponential-Family Random Graph Models", "comments": "30 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic networks are a general language for describing time-evolving complex\nsystems, and discrete time network models provide an emerging statistical\ntechnique for various applications. It is a fundamental research question to\ndetect the community structure in time-evolving networks. However, due to\nsignificant computational challenges and difficulties in modeling communities\nof time-evolving networks, there is little progress in the current literature\nto effectively find communities in time-evolving networks. In this work, we\npropose a novel model-based clustering framework for time-evolving networks\nbased on discrete time exponential-family random graph models. To choose the\nnumber of communities, we use conditional likelihood to construct an effective\nmodel selection criterion. Furthermore, we propose an efficient variational\nexpectation-maximization (EM) algorithm to find approximate maximum likelihood\nestimates of network parameters and mixing proportions. By using variational\nmethods and minorization-maximization (MM) techniques, our method has appealing\nscalability for large-scale time-evolving networks. The power of our method is\ndemonstrated in simulation studies and empirical applications to international\ntrade networks and the collaboration networks of a large American research\nuniversity.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 05:36:00 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Lee", "Kevin H.", ""], ["Xue", "Lingzhou", ""], ["Hunter", "David R.", ""]]}, {"id": "1712.07364", "submitter": "Martin Spindler", "authors": "Sven Klaassen and Jannis Kueck and Martin Spindler", "title": "Transformation Models in High-Dimensions", "comments": "63 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformation models are a very important tool for applied statisticians and\neconometricians. In many applications, the dependent variable is transformed so\nthat homogeneity or normal distribution of the error holds. In this paper, we\nanalyze transformation models in a high-dimensional setting, where the set of\npotential covariates is large. We propose an estimator for the transformation\nparameter and we show that it is asymptotically normally distributed using an\northogonalized moment condition where the nuisance functions depend on the\ntarget parameter. In a simulation study, we show that the proposed estimator\nworks well in small samples. A common practice in labor economics is to\ntransform wage with the log-function. In this study, we test if this\ntransformation holds in CPS data from the United States.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 08:58:41 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Klaassen", "Sven", ""], ["Kueck", "Jannis", ""], ["Spindler", "Martin", ""]]}, {"id": "1712.07374", "submitter": "Ashkan Rezaei", "authors": "Hong Wang, Ashkan Rezaei, Brian D. Ziebart", "title": "Adversarial Structured Prediction for Multivariate Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many predicted structured objects (e.g., sequences, matchings, trees) are\nevaluated using the F-score, alignment error rate (AER), or other multivariate\nperformance measures. Since inductively optimizing these measures using\ntraining data is typically computationally difficult, empirical risk\nminimization of surrogate losses is employed, using, e.g., the hinge loss for\n(structured) support vector machines. These approximations often introduce a\nmismatch between the learner's objective and the desired application\nperformance, leading to inconsistency. We take a different approach:\nadversarially approximate training data while optimizing the exact F-score or\nAER. Structured predictions under this formulation result from solving zero-sum\ngames between a predictor seeking the best performance and an adversary seeking\nthe worst while required to (approximately) match certain structured properties\nof the training data. We explore this approach for word alignment (AER\nevaluation) and named entity recognition (F-score evaluation) with linear-chain\nconstraints.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 09:31:06 GMT"}, {"version": "v2", "created": "Thu, 21 Dec 2017 04:22:52 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Wang", "Hong", ""], ["Rezaei", "Ashkan", ""], ["Ziebart", "Brian D.", ""]]}, {"id": "1712.07420", "submitter": "Martin Wistuba", "authors": "Martin Wistuba", "title": "Finding Competitive Network Architectures Within a Day Using UCT", "comments": null, "journal-ref": "Proceedings of the 5th IEEE International Conference on Data\n  Science and Advanced Analytics, pages 263-272, 2018", "doi": "10.1109/DSAA.2018.00037", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of neural network architectures for a new data set is a laborious\ntask which requires human deep learning expertise. In order to make deep\nlearning available for a broader audience, automated methods for finding a\nneural network architecture are vital. Recently proposed methods can already\nachieve human expert level performances. However, these methods have run times\nof months or even years of GPU computing time, ignoring hardware constraints as\nfaced by many researchers and companies. We propose the use of Monte Carlo\nplanning in combination with two different UCT (upper confidence bound applied\nto trees) derivations to search for network architectures. We adapt the UCT\nalgorithm to the needs of network architecture search by proposing two ways of\nsharing information between different branches of the search tree. In an\nempirical study we are able to demonstrate that this method is able to find\ncompetitive networks for MNIST, SVHN and CIFAR-10 in just a single GPU day.\nExtending the search time to five GPU days, we are able to outperform human\narchitectures and our competitors which consider the same types of layers.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 11:24:50 GMT"}, {"version": "v2", "created": "Mon, 23 Jul 2018 13:57:50 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Wistuba", "Martin", ""]]}, {"id": "1712.07424", "submitter": "Vishwak Srinivasan", "authors": "Vishwak Srinivasan, Adepu Ravi Sankar and Vineeth N Balasubramanian", "title": "ADINE: An Adaptive Momentum Method for Stochastic Gradient Descent", "comments": "8 + 1 pages, 12 figures, accepted at CoDS-COMAD 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two major momentum-based techniques that have achieved tremendous success in\noptimization are Polyak's heavy ball method and Nesterov's accelerated\ngradient. A crucial step in all momentum-based methods is the choice of the\nmomentum parameter $m$ which is always suggested to be set to less than $1$.\nAlthough the choice of $m < 1$ is justified only under very strong theoretical\nassumptions, it works well in practice even when the assumptions do not\nnecessarily hold. In this paper, we propose a new momentum based method\n$\\textit{ADINE}$, which relaxes the constraint of $m < 1$ and allows the\nlearning algorithm to use adaptive higher momentum. We motivate our hypothesis\non $m$ by experimentally verifying that a higher momentum ($\\ge 1$) can help\nescape saddles much faster. Using this motivation, we propose our method\n$\\textit{ADINE}$ that helps weigh the previous updates more (by setting the\nmomentum parameter $> 1$), evaluate our proposed algorithm on deep neural\nnetworks and show that $\\textit{ADINE}$ helps the learning algorithm to\nconverge much faster without compromising on the generalization error.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 11:30:16 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Srinivasan", "Vishwak", ""], ["Sankar", "Adepu Ravi", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "1712.07436", "submitter": "Markus Wulfmeier", "authors": "Markus Wulfmeier, Alex Bewley and Ingmar Posner", "title": "Incremental Adversarial Domain Adaptation for Continually Changing\n  Environments", "comments": "International Conference on Robotics and Automation 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous appearance shifts such as changes in weather and lighting\nconditions can impact the performance of deployed machine learning models.\nWhile unsupervised domain adaptation aims to address this challenge, current\napproaches do not utilise the continuity of the occurring shifts. In\nparticular, many robotics applications exhibit these conditions and thus\nfacilitate the potential to incrementally adapt a learnt model over minor\nshifts which integrate to massive differences over time. Our work presents an\nadversarial approach for lifelong, incremental domain adaptation which benefits\nfrom unsupervised alignment to a series of intermediate domains which\nsuccessively diverge from the labelled source domain. We empirically\ndemonstrate that our incremental approach improves handling of large appearance\nchanges, e.g. day to night, on a traversable-path segmentation task compared\nwith a direct, single alignment step approach. Furthermore, by approximating\nthe feature distribution for the source domain with a generative adversarial\nnetwork, the deployment module can be rendered fully independent of retaining\npotentially large amounts of the related source training data for only a minor\nreduction in performance.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 12:08:04 GMT"}, {"version": "v2", "created": "Sat, 24 Feb 2018 16:05:02 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Wulfmeier", "Markus", ""], ["Bewley", "Alex", ""], ["Posner", "Ingmar", ""]]}, {"id": "1712.07454", "submitter": "Robert Duin", "authors": "Robert P.W. Duin and Sergey Verzakov", "title": "Fast kNN mode seeking clustering applied to active learning", "comments": "23 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significantly faster algorithm is presented for the original kNN mode\nseeking procedure. It has the advantages over the well-known mean shift\nalgorithm that it is feasible in high-dimensional vector spaces and results in\nuniquely, well defined modes. Moreover, without any additional computational\neffort it may yield a multi-scale hierarchy of clusterings. The time complexity\nis just O(n^1.5). resulting computing times range from seconds for 10^4 objects\nto minutes for 10^5 objects and to less than an hour for 10^6 objects. The\nspace complexity is just O(n). The procedure is well suited for finding large\nsets of small clusters and is thereby a candidate to analyze thousands of\nclusters in millions of objects.\n  The kNN mode seeking procedure can be used for active learning by assigning\nthe clusters to the class of the modal objects of the clusters. Its feasibility\nis shown by some examples with up to 1.5 million handwritten digits. The\nobtained classification results based on the clusterings are compared with\nthose obtained by the nearest neighbor rule and the support vector classifier\nbased on the same labeled objects for training. It can be concluded that using\nthe clustering structure for classification can be significantly better than\nusing the trained classifiers. A drawback of using the clustering for\nclassification, however, is that no classifier is obtained that may be used for\nout-of-sample objects.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 12:51:21 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Duin", "Robert P. W.", ""], ["Verzakov", "Sergey", ""]]}, {"id": "1712.07495", "submitter": "Wenjie Zheng", "authors": "Wenjie Zheng, Aur\\'elien Bellet, Patrick Gallinari", "title": "A Distributed Frank-Wolfe Framework for Learning Low-Rank Matrices with\n  the Trace Norm", "comments": null, "journal-ref": null, "doi": "10.1007/s10994-018-5713-5", "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a high-dimensional but low-rank matrix\nfrom a large-scale dataset distributed over several machines, where\nlow-rankness is enforced by a convex trace norm constraint. We propose\nDFW-Trace, a distributed Frank-Wolfe algorithm which leverages the low-rank\nstructure of its updates to achieve efficiency in time, memory and\ncommunication usage. The step at the heart of DFW-Trace is solved approximately\nusing a distributed version of the power method. We provide a theoretical\nanalysis of the convergence of DFW-Trace, showing that we can ensure sublinear\nconvergence in expectation to an optimal solution with few power iterations per\nepoch. We implement DFW-Trace in the Apache Spark distributed programming\nframework and validate the usefulness of our approach on synthetic and real\ndata, including the ImageNet dataset with high-dimensional features extracted\nfrom a deep neural network.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 14:28:21 GMT"}, {"version": "v2", "created": "Fri, 11 May 2018 12:09:11 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Zheng", "Wenjie", ""], ["Bellet", "Aur\u00e9lien", ""], ["Gallinari", "Patrick", ""]]}, {"id": "1712.07519", "submitter": "Tengyuan Liang", "authors": "Tengyuan Liang and Weijie Su", "title": "Statistical Inference for the Population Landscape via Moment Adjusted\n  Stochastic Gradients", "comments": "Journal of the Royal Statistical Society: Series B (Statistical\n  Methodology) 2019, to appear", "journal-ref": "Journal of the Royal Statistical Society: Series B (Statistical\n  Methodology) 81 (2019) 431-456", "doi": "10.1111/rssb.12313", "report-no": null, "categories": "stat.ML math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern statistical inference tasks often require iterative optimization\nmethods to compute the solution. Convergence analysis from an optimization\nviewpoint only informs us how well the solution is approximated numerically but\noverlooks the sampling nature of the data. In contrast, recognizing the\nrandomness in the data, statisticians are keen to provide uncertainty\nquantification, or confidence, for the solution obtained using iterative\noptimization methods. This paper makes progress along this direction by\nintroducing the moment-adjusted stochastic gradient descents, a new stochastic\noptimization method for statistical inference. We establish non-asymptotic\ntheory that characterizes the statistical distribution for certain iterative\nmethods with optimization guarantees. On the statistical front, the theory\nallows for model mis-specification, with very mild conditions on the data. For\noptimization, the theory is flexible for both convex and non-convex cases.\nRemarkably, the moment-adjusting idea motivated from \"error standardization\" in\nstatistics achieves a similar effect as acceleration in first-order\noptimization methods used to fit generalized linear models. We also demonstrate\nthis acceleration effect in the non-convex setting through numerical\nexperiments.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 15:16:53 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 23:31:12 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Liang", "Tengyuan", ""], ["Su", "Weijie", ""]]}, {"id": "1712.07557", "submitter": "Robin Geyer", "authors": "Robin C. Geyer, Tassilo Klein, Moin Nabi", "title": "Differentially Private Federated Learning: A Client Level Perspective", "comments": "NIPS 2017 Workshop: Machine Learning on the Phone and other Consumer\n  Devices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is a recent advance in privacy protection. In this\ncontext, a trusted curator aggregates parameters optimized in decentralized\nfashion by multiple clients. The resulting model is then distributed back to\nall clients, ultimately converging to a joint representative model without\nexplicitly having to share the data. However, the protocol is vulnerable to\ndifferential attacks, which could originate from any party contributing during\nfederated optimization. In such an attack, a client's contribution during\ntraining and information about their data set is revealed through analyzing the\ndistributed model. We tackle this problem and propose an algorithm for client\nsided differential privacy preserving federated optimization. The aim is to\nhide clients' contributions during training, balancing the trade-off between\nprivacy loss and model performance. Empirical studies suggest that given a\nsufficiently large number of participating clients, our proposed procedure can\nmaintain client-level differential privacy at only a minor cost in model\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 16:28:37 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 10:12:27 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Geyer", "Robin C.", ""], ["Klein", "Tassilo", ""], ["Nabi", "Moin", ""]]}, {"id": "1712.07581", "submitter": "Stefano Carrazza", "authors": "Daniel Krefl, Stefano Carrazza, Babak Haghighat, Jens Kahlen", "title": "Riemann-Theta Boltzmann Machine", "comments": "29 pages, 11 figures, final version published in Neurocomputing", "journal-ref": null, "doi": "10.1016/j.neucom.2020.01.011", "report-no": "CERN-TH-2017-275", "categories": "stat.ML cs.LG hep-ph hep-th math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general Boltzmann machine with continuous visible and discrete integer\nvalued hidden states is introduced. Under mild assumptions about the connection\nmatrices, the probability density function of the visible units can be solved\nfor analytically, yielding a novel parametric density function involving a\nratio of Riemann-Theta functions. The conditional expectation of a hidden state\nfor given visible states can also be calculated analytically, yielding a\nderivative of the logarithmic Riemann-Theta function. The conditional\nexpectation can be used as activation function in a feedforward neural network,\nthereby increasing the modelling capacity of the network. Both the Boltzmann\nmachine and the derived feedforward neural network can be successfully trained\nvia standard gradient- and non-gradient-based optimization techniques.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 17:01:42 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 09:32:24 GMT"}, {"version": "v3", "created": "Tue, 28 Jan 2020 09:28:05 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Krefl", "Daniel", ""], ["Carrazza", "Stefano", ""], ["Haghighat", "Babak", ""], ["Kahlen", "Jens", ""]]}, {"id": "1712.07639", "submitter": "R. Lily Hu", "authors": "R. Lily Hu, Jeremy Karnowski, Ross Fadely, Jean-Patrick Pommier", "title": "Image Segmentation to Distinguish Between Overlapping Human Chromosomes", "comments": "Presented at NIPS 2017 Machine Learning for Health", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medicine, visualizing chromosomes is important for medical diagnostics,\ndrug development, and biomedical research. Unfortunately, chromosomes often\noverlap and it is necessary to identify and distinguish between the overlapping\nchromosomes. A segmentation solution that is fast and automated will enable\nscaling of cost effective medicine and biomedical research. We apply neural\nnetwork-based image segmentation to the problem of distinguishing between\npartially overlapping DNA chromosomes. A convolutional neural network is\ncustomized for this problem. The results achieved intersection over union (IOU)\nscores of 94.7% for the overlapping region and 88-94% on the non-overlapping\nchromosome regions.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 18:48:41 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Hu", "R. Lily", ""], ["Karnowski", "Jeremy", ""], ["Fadely", "Ross", ""], ["Pommier", "Jean-Patrick", ""]]}, {"id": "1712.07682", "submitter": "Giovanni Montana", "authors": "Mauro Annarumma, Giovanni Montana", "title": "Deep metric learning for multi-labelled radiographs", "comments": "SAC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many radiological studies can reveal the presence of several co-existing\nabnormalities, each one represented by a distinct visual pattern. In this\narticle we address the problem of learning a distance metric for plain\nradiographs that captures a notion of \"radiological similarity\": two chest\nradiographs are considered to be similar if they share similar abnormalities.\nDeep convolutional neural networks (DCNs) are used to learn a low-dimensional\nembedding for the radiographs that is equipped with the desired metric. Two\nloss functions are proposed to deal with multi-labelled images and potentially\nnoisy labels. We report on a large-scale study involving over 745,000 chest\nradiographs whose labels were automatically extracted from free-text\nradiological reports through a natural language processing system. Using 4,500\nvalidated exams, we demonstrate that the methodology performs satisfactorily on\nclustering and image retrieval tasks. Remarkably, the learned metric separates\nnormal exams from those having radiological abnormalities.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 20:55:08 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Annarumma", "Mauro", ""], ["Montana", "Giovanni", ""]]}, {"id": "1712.07704", "submitter": "Brooke Husic", "authors": "Brooke E. Husic and Vijay S. Pande", "title": "Unsupervised learning of dynamical and molecular similarity using\n  variance minimization", "comments": "NIPS 2017 Workshop on Machine Learning for Molecules and Materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.bio-ph q-bio.BM q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, we present an unsupervised machine learning method for\ndetermining groups of molecular systems according to similarity in their\ndynamics or structures using Ward's minimum variance objective function. We\nfirst apply the minimum variance clustering to a set of simulated tripeptides\nusing the information theoretic Jensen-Shannon divergence between Markovian\ntransition matrices in order to gain insight into how point mutations affect\nprotein dynamics. Then, we extend the method to partition two chemoinformatic\ndatasets according to structural similarity to motivate a train/validation/test\nsplit for supervised learning that avoids overfitting.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 20:56:12 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Husic", "Brooke E.", ""], ["Pande", "Vijay S.", ""]]}, {"id": "1712.07788", "submitter": "Dejiao Zhang", "authors": "Dejiao Zhang, Yifan Sun, Brian Eriksson, Laura Balzano", "title": "Deep Unsupervised Clustering Using Mixture of Autoencoders", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised clustering is one of the most fundamental challenges in machine\nlearning. A popular hypothesis is that data are generated from a union of\nlow-dimensional nonlinear manifolds; thus an approach to clustering is\nidentifying and separating these manifolds. In this paper, we present a novel\napproach to solve this problem by using a mixture of autoencoders. Our model\nconsists of two parts: 1) a collection of autoencoders where each autoencoder\nlearns the underlying manifold of a group of similar objects, and 2) a mixture\nassignment neural network, which takes the concatenated latent vectors from the\nautoencoders as input and infers the distribution over clusters. By jointly\noptimizing the two parts, we simultaneously assign data to clusters and learn\nthe underlying manifolds of each cluster.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 04:27:35 GMT"}, {"version": "v2", "created": "Tue, 26 Dec 2017 16:45:12 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Zhang", "Dejiao", ""], ["Sun", "Yifan", ""], ["Eriksson", "Brian", ""], ["Balzano", "Laura", ""]]}, {"id": "1712.07800", "submitter": "Lingzhou Xue", "authors": "Amal Agarwal and Lingzhou Xue", "title": "Model-Based Clustering of Nonparametric Weighted Networks with\n  Application to Water Pollution Analysis", "comments": "29 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Water pollution is a major global environmental problem, and it poses a great\nenvironmental risk to public health and biological diversity. This work is\nmotivated by assessing the potential environmental threat of coal mining\nthrough increased sulfate concentrations in river networks, which do not belong\nto any simple parametric distribution. However, existing network models mainly\nfocus on binary or discrete networks and weighted networks with known\nparametric weight distributions. We propose a principled nonparametric weighted\nnetwork model based on exponential-family random graph models and local\nlikelihood estimation and study its model-based clustering with application to\nlarge-scale water pollution network analysis. We do not require any parametric\ndistribution assumption on network weights. The proposed method greatly extends\nthe methodology and applicability of statistical network models. Furthermore,\nit is scalable to large and complex networks in large-scale environmental\nstudies. The power of our proposed methods is demonstrated in simulation\nstudies and a real application to sulfate pollution network analysis in Ohio\nwatershed located in Pennsylvania, United States.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 05:36:10 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 03:31:53 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Agarwal", "Amal", ""], ["Xue", "Lingzhou", ""]]}, {"id": "1712.07811", "submitter": "Takashi Kurokawa", "authors": "Takashi Kurokawa, Taihei Oki, Hiromichi Nagao", "title": "Multi-dimensional Graph Fourier Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many signals on Cartesian product graphs appear in the real world, such as\ndigital images, sensor observation time series, and movie ratings on Netflix.\nThese signals are \"multi-dimensional\" and have directional characteristics\nalong each factor graph. However, the existing graph Fourier transform does not\ndistinguish these directions, and assigns 1-D spectra to signals on product\ngraphs. Further, these spectra are often multi-valued at some frequencies. Our\nmain result is a multi-dimensional graph Fourier transform that solves such\nproblems associated with the conventional GFT. Using algebraic properties of\nCartesian products, the proposed transform rearranges 1-D spectra obtained by\nthe conventional GFT into the multi-dimensional frequency domain, of which each\ndimension represents a directional frequency along each factor graph. Thus, the\nmulti-dimensional graph Fourier transform enables directional frequency\nanalysis, in addition to frequency analysis with the conventional GFT.\nMoreover, this rearrangement resolves the multi-valuedness of spectra in some\ncases. The multi-dimensional graph Fourier transform is a foundation of novel\nfilterings and stationarities that utilize dimensional information of graph\nsignals, which are also discussed in this study. The proposed methods are\napplicable to a wide variety of data that can be regarded as signals on\nCartesian product graphs. This study also notes that multivariate graph signals\ncan be regarded as 2-D univariate graph signals. This correspondence provides\nnatural definitions of the multivariate graph Fourier transform and the\nmultivariate stationarity based on their 2-D univariate versions.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 06:57:55 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Kurokawa", "Takashi", ""], ["Oki", "Taihei", ""], ["Nagao", "Hiromichi", ""]]}, {"id": "1712.07822", "submitter": "L\\'eon Bottou", "authors": "Leon Bottou and Martin Arjovsky and David Lopez-Paz and Maxime Oquab", "title": "Geometrical Insights for Implicit Generative Modeling", "comments": "this version fixes a typo in a definition", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning algorithms for implicit generative models can optimize a variety of\ncriteria that measure how the data distribution differs from the implicit model\ndistribution, including the Wasserstein distance, the Energy distance, and the\nMaximum Mean Discrepancy criterion. A careful look at the geometries induced by\nthese distances on the space of probability measures reveals interesting\ndifferences. In particular, we can establish surprising approximate global\nconvergence guarantees for the $1$-Wasserstein distance,even when the\nparametric generator has a nonconvex parametrization.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 08:11:44 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2018 20:21:04 GMT"}, {"version": "v3", "created": "Wed, 21 Aug 2019 21:09:56 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Bottou", "Leon", ""], ["Arjovsky", "Martin", ""], ["Lopez-Paz", "David", ""], ["Oquab", "Maxime", ""]]}, {"id": "1712.07897", "submitter": "Purushottam Kar", "authors": "Prateek Jain and Purushottam Kar", "title": "Non-convex Optimization for Machine Learning", "comments": "The official publication is available from now publishers via\n  http://dx.doi.org/10.1561/2200000058", "journal-ref": "Foundations and Trends in Machine Learning: Vol. 10: No. 3-4, pp\n  142-336 (2017)", "doi": "10.1561/2200000058", "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A vast majority of machine learning algorithms train their models and perform\ninference by solving optimization problems. In order to capture the learning\nand prediction problems accurately, structural constraints such as sparsity or\nlow rank are frequently imposed or else the objective itself is designed to be\na non-convex function. This is especially true of algorithms that operate in\nhigh-dimensional spaces or that train non-linear models such as tensor models\nand deep networks.\n  The freedom to express the learning problem as a non-convex optimization\nproblem gives immense modeling power to the algorithm designer, but often such\nproblems are NP-hard to solve. A popular workaround to this has been to relax\nnon-convex problems to convex ones and use traditional methods to solve the\n(convex) relaxed optimization problems. However this approach may be lossy and\nnevertheless presents significant challenges for large scale optimization.\n  On the other hand, direct approaches to non-convex optimization have met with\nresounding success in several domains and remain the methods of choice for the\npractitioner, as they frequently outperform relaxation-based techniques -\npopular heuristics include projected gradient descent and alternating\nminimization. However, these are often poorly understood in terms of their\nconvergence and other properties.\n  This monograph presents a selection of recent advances that bridge a\nlong-standing gap in our understanding of these heuristics. The monograph will\nlead the reader through several widely used non-convex optimization techniques,\nas well as applications thereof. The goal of this monograph is to both,\nintroduce the rich literature in this area, as well as equip the reader with\nthe tools and techniques needed to analyze these simple procedures for\nnon-convex problems.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 12:05:40 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Jain", "Prateek", ""], ["Kar", "Purushottam", ""]]}, {"id": "1712.07924", "submitter": "Emil Wiedemann", "authors": "Meike Zehlike and Philipp Hacker and Emil Wiedemann", "title": "Matching Code and Law: Achieving Algorithmic Fairness with Optimal\n  Transport", "comments": "Vastly extended new version, now including computational experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasingly, discrimination by algorithms is perceived as a societal and\nlegal problem. As a response, a number of criteria for implementing algorithmic\nfairness in machine learning have been developed in the literature. This paper\nproposes the Continuous Fairness Algorithm (CFA$\\theta$) which enables a\ncontinuous interpolation between different fairness definitions. More\nspecifically, we make three main contributions to the existing literature.\nFirst, our approach allows the decision maker to continuously vary between\nspecific concepts of individual and group fairness. As a consequence, the\nalgorithm enables the decision maker to adopt intermediate ``worldviews'' on\nthe degree of discrimination encoded in algorithmic processes, adding nuance to\nthe extreme cases of ``we're all equal'' (WAE) and ``what you see is what you\nget'' (WYSIWYG) proposed so far in the literature. Second, we use optimal\ntransport theory, and specifically the concept of the barycenter, to maximize\ndecision maker utility under the chosen fairness constraints. Third, the\nalgorithm is able to handle cases of intersectionality, i.e., of\nmulti-dimensional discrimination of certain groups on grounds of several\ncriteria. We discuss three main examples (credit applications; college\nadmissions; insurance contracts) and map out the legal and policy implications\nof our approach. The explicit formalization of the trade-off between individual\nand group fairness allows this post-processing approach to be tailored to\ndifferent situational contexts in which one or the other fairness criterion may\ntake precedence. Finally, we evaluate our model experimentally.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 13:08:03 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 10:25:54 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Zehlike", "Meike", ""], ["Hacker", "Philipp", ""], ["Wiedemann", "Emil", ""]]}, {"id": "1712.08041", "submitter": "Payel Das", "authors": "Ravi Tejwani, Adam Liska, Hongyuan You, Jenna Reinen, and Payel Das", "title": "Autism Classification Using Brain Functional Connectivity Dynamics and\n  Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of the present study is to identify autism using machine learning\ntechniques and resting-state brain imaging data, leveraging the temporal\nvariability of the functional connections (FC) as the only information. We\nestimated and compared the FC variability across brain regions between typical,\nhealthy subjects and autistic population by analyzing brain imaging data from a\nworld-wide multi-site database known as ABIDE (Autism Brain Imaging Data\nExchange). Our analysis revealed that patients diagnosed with autism spectrum\ndisorder (ASD) show increased FC variability in several brain regions that are\nassociated with low FC variability in the typical brain. We then used the\nenhanced FC variability of brain regions as features for training machine\nlearning models for ASD classification and achieved 65% accuracy in\nidentification of ASD versus control subjects within the dataset. We also used\nnode strength estimated from number of functional connections per node averaged\nover the whole scan as features for ASD classification.The results reveal that\nthe dynamic FC measures outperform or are comparable with the static FC\nmeasures in predicting ASD.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 16:08:16 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Tejwani", "Ravi", ""], ["Liska", "Adam", ""], ["You", "Hongyuan", ""], ["Reinen", "Jenna", ""], ["Das", "Payel", ""]]}, {"id": "1712.08048", "submitter": "Topi Paananen", "authors": "Topi Paananen, Juho Piironen, Michael Riis Andersen, Aki Vehtari", "title": "Variable selection for Gaussian processes via sensitivity analysis of\n  the posterior predictive distribution", "comments": "Minor changes to text, additions to supplementary material", "journal-ref": "Proceedings of the 22nd International Conference on Artificial\n  Intelligence and Statistics (AISTATS), PMLR 89: 1743-1752, 2019", "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection for Gaussian process models is often done using automatic\nrelevance determination, which uses the inverse length-scale parameter of each\ninput variable as a proxy for variable relevance. This implicitly determined\nrelevance has several drawbacks that prevent the selection of optimal input\nvariables in terms of predictive performance. To improve on this, we propose\ntwo novel variable selection methods for Gaussian process models that utilize\nthe predictions of a full model in the vicinity of the training points and\nthereby rank the variables based on their predictive relevance. Our empirical\nresults on synthetic and real world data sets demonstrate improved variable\nselection compared to automatic relevance determination in terms of variability\nand predictive performance.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 16:15:34 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 08:03:12 GMT"}, {"version": "v3", "created": "Wed, 6 Mar 2019 09:19:03 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Paananen", "Topi", ""], ["Piironen", "Juho", ""], ["Andersen", "Michael Riis", ""], ["Vehtari", "Aki", ""]]}, {"id": "1712.08058", "submitter": "Christian Dansereau", "authors": "Christian Dansereau, Angela Tam, AmanPreet Badhwar, Sebastian Urchs,\n  Pierre Orban, Pedro Rosa-Neto, Pierre Bellec", "title": "A brain signature highly predictive of future progression to Alzheimer's\n  dementia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early prognosis of Alzheimer's dementia is hard. Mild cognitive impairment\n(MCI) typically precedes Alzheimer's dementia, yet only a fraction of MCI\nindividuals will progress to dementia, even when screened using biomarkers. We\npropose here to identify a subset of individuals who share a common brain\nsignature highly predictive of oncoming dementia. This signature was composed\nof brain atrophy and functional dysconnectivity and discovered using a machine\nlearning model in patients suffering from dementia. The model recognized the\nsame brain signature in MCI individuals, 90% of which progressed to dementia\nwithin three years. This result is a marked improvement on the state-of-the-art\nin prognostic precision, while the brain signature still identified 47% of all\nMCI progressors. We thus discovered a sizable MCI subpopulation which\nrepresents an excellent recruitment target for clinical trials at the prodromal\nstage of Alzheimer's disease.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 16:26:48 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 17:37:11 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Dansereau", "Christian", ""], ["Tam", "Angela", ""], ["Badhwar", "AmanPreet", ""], ["Urchs", "Sebastian", ""], ["Orban", "Pierre", ""], ["Rosa-Neto", "Pedro", ""], ["Bellec", "Pierre", ""]]}, {"id": "1712.08091", "submitter": "Tien Do Huu", "authors": "Tien Huu Do, Duc Minh Nguyen, Evaggelia Tsiligianni, Bruno Cornelis\n  and Nikos Deligiannis", "title": "Multiview Deep Learning for Predicting Twitter Users' Location", "comments": "Submitted to the IEEE Transactions on Big Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of predicting the location of users on large social networks like\nTwitter has emerged from real-life applications such as social unrest detection\nand online marketing. Twitter user geolocation is a difficult and active\nresearch topic with a vast literature. Most of the proposed methods follow\neither a content-based or a network-based approach. The former exploits\nuser-generated content while the latter utilizes the connection or interaction\nbetween Twitter users. In this paper, we introduce a novel method combining the\nstrength of both approaches. Concretely, we propose a multi-entry neural\nnetwork architecture named MENET leveraging the advances in deep learning and\nmultiview learning. The generalizability of MENET enables the integration of\nmultiple data representations. In the context of Twitter user geolocation, we\nrealize MENET with textual, network, and metadata features. Considering the\nnatural distribution of Twitter users across the concerned geographical area,\nwe subdivide the surface of the earth into multi-scale cells and train MENET\nwith the labels of the cells. We show that our method outperforms the state of\nthe art by a large margin on three benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 17:15:41 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Do", "Tien Huu", ""], ["Nguyen", "Duc Minh", ""], ["Tsiligianni", "Evaggelia", ""], ["Cornelis", "Bruno", ""], ["Deligiannis", "Nikos", ""]]}, {"id": "1712.08101", "submitter": "Sebastiaan H\\\"oppner", "authors": "Sebastiaan H\\\"oppner, Eugen Stripling, Bart Baesens, Seppe vanden\n  Broucke and Tim Verdonck", "title": "Profit Driven Decision Trees for Churn Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Customer retention campaigns increasingly rely on predictive models to detect\npotential churners in a vast customer base. From the perspective of machine\nlearning, the task of predicting customer churn can be presented as a binary\nclassification problem. Using data on historic behavior, classification\nalgorithms are built with the purpose of accurately predicting the probability\nof a customer defecting. The predictive churn models are then commonly selected\nbased on accuracy related performance measures such as the area under the ROC\ncurve (AUC). However, these models are often not well aligned with the core\nbusiness requirement of profit maximization, in the sense that, the models fail\nto take into account not only misclassification costs, but also the benefits\noriginating from a correct classification. Therefore, the aim is to construct\nchurn prediction models that are profitable and preferably interpretable too.\nThe recently developed expected maximum profit measure for customer churn\n(EMPC) has been proposed in order to select the most profitable churn model. We\npresent a new classifier that integrates the EMPC metric directly into the\nmodel construction. Our technique, called ProfTree, uses an evolutionary\nalgorithm for learning profit driven decision trees. In a benchmark study with\nreal-life data sets from various telecommunication service providers, we show\nthat ProfTree achieves significant profit improvements compared to classic\naccuracy driven tree-based methods.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 17:31:58 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["H\u00f6ppner", "Sebastiaan", ""], ["Stripling", "Eugen", ""], ["Baesens", "Bart", ""], ["Broucke", "Seppe vanden", ""], ["Verdonck", "Tim", ""]]}, {"id": "1712.08104", "submitter": "J\\\"org L\\\"ucke", "authors": "J\\\"org L\\\"ucke, Zhenwen Dai, Georgios Exarchakis", "title": "Truncated Variational Sampling for \"Black Box\" Optimization of\n  Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the optimization of two probabilistic generative models with\nbinary latent variables using a novel variational EM approach. The approach\ndistinguishes itself from previous variational approaches by using latent\nstates as variational parameters. Here we use efficient and general purpose\nsampling procedures to vary the latent states, and investigate the \"black box\"\napplicability of the resulting optimization procedure. For general purpose\napplicability, samples are drawn from approximate marginal distributions of the\nconsidered generative model as well as from the model's prior distribution. As\nsuch, variational sampling is defined in a generic form, and is directly\nexecutable for a given model. As a proof of concept, we then apply the novel\nprocedure (A) to Binary Sparse Coding (a model with continuous observables),\nand (B) to basic Sigmoid Belief Networks (which are models with binary\nobservables). Numerical experiments verify that the investigated approach\nefficiently as well as effectively increases a variational free energy\nobjective without requiring any additional analytical steps.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 17:38:34 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 20:38:07 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["L\u00fccke", "J\u00f6rg", ""], ["Dai", "Zhenwen", ""], ["Exarchakis", "Georgios", ""]]}, {"id": "1712.08107", "submitter": "Jordi De La Torre", "authors": "Jordi de la Torre and Aida Valls and Domenec Puig", "title": "A Deep Learning Interpretable Classifier for Diabetic Retinopathy\n  Disease Grading", "comments": "Submitted to Elsevier", "journal-ref": null, "doi": "10.1016/j.neucom.2018.07.102", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network models have been proven to be very successful in image\nclassification tasks, also for medical diagnosis, but their main concern is its\nlack of interpretability. They use to work as intuition machines with high\nstatistical confidence but unable to give interpretable explanations about the\nreported results. The vast amount of parameters of these models make difficult\nto infer a rationale interpretation from them. In this paper we present a\ndiabetic retinopathy interpretable classifier able to classify retine images\ninto the different levels of disease severity and of explaining its results by\nassigning a score for every point in the hidden and input space, evaluating its\ncontribution to the final classification in a linear way. The generated visual\nmaps can be interpreted by an expert in order to compare its own knowledge with\nthe interpretation given by the model.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 17:40:32 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["de la Torre", "Jordi", ""], ["Valls", "Aida", ""], ["Puig", "Domenec", ""]]}, {"id": "1712.08160", "submitter": "Ilya Kuzovkin", "authors": "Anna Leontjeva, Ilya Kuzovkin", "title": "Combining Static and Dynamic Features for Multivariate Sequence\n  Classification", "comments": "Presented at IEEE DSAA 2016", "journal-ref": null, "doi": "10.1109/DSAA.2016.10", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Model precision in a classification task is highly dependent on the feature\nspace that is used to train the model. Moreover, whether the features are\nsequential or static will dictate which classification method can be applied as\nmost of the machine learning algorithms are designed to deal with either one or\nanother type of data. In real-life scenarios, however, it is often the case\nthat both static and dynamic features are present, or can be extracted from the\ndata. In this work, we demonstrate how generative models such as Hidden Markov\nModels (HMM) and Long Short-Term Memory (LSTM) artificial neural networks can\nbe used to extract temporal information from the dynamic data. We explore how\nthe extracted information can be combined with the static features in order to\nimprove the classification performance. We evaluate the existing techniques and\nsuggest a hybrid approach, which outperforms other methods on several public\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 23:59:13 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Leontjeva", "Anna", ""], ["Kuzovkin", "Ilya", ""]]}, {"id": "1712.08197", "submitter": "Edward Raff", "authors": "Edward Raff, Jared Sylvester, Steven Mills", "title": "Fair Forests: Regularized Tree Induction to Minimize Model Bias", "comments": "To appear in the first AAAI / ACM conference on Artificial\n  Intelligence, Ethics, and Society (AIES) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The potential lack of fairness in the outputs of machine learning algorithms\nhas recently gained attention both within the research community as well as in\nsociety more broadly. Surprisingly, there is no prior work developing\ntree-induction algorithms for building fair decision trees or fair random\nforests. These methods have widespread popularity as they are one of the few to\nbe simultaneously interpretable, non-linear, and easy-to-use. In this paper we\ndevelop, to our knowledge, the first technique for the induction of fair\ndecision trees. We show that our \"Fair Forest\" retains the benefits of the\ntree-based approach, while providing both greater accuracy and fairness than\nother alternatives, for both \"group fairness\" and \"individual fairness.'\" We\nalso introduce new measures for fairness which are able to handle multinomial\nand continues attributes as well as regression problems, as opposed to binary\nattributes and labels only. Finally, we demonstrate a new, more robust\nevaluation procedure for algorithms that considers the dataset in its entirety\nrather than only a specific protected attribute.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 20:19:48 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Raff", "Edward", ""], ["Sylvester", "Jared", ""], ["Mills", "Steven", ""]]}, {"id": "1712.08211", "submitter": "Eric Tramel", "authors": "Baptiste Goujaud, Eric W. Tramel, Pierre Courtiol, Mikhail Zaslavskiy,\n  Gilles Wainrib", "title": "Robust Detection of Covariate-Treatment Interactions in Clinical Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of interactions between treatment effects and patient descriptors\nin clinical trials is critical for optimizing the drug development process. The\nincreasing volume of data accumulated in clinical trials provides a unique\nopportunity to discover new biomarkers and further the goal of personalized\nmedicine, but it also requires innovative robust biomarker detection methods\ncapable of detecting non-linear, and sometimes weak, signals. We propose a set\nof novel univariate statistical tests, based on the theory of random walks,\nwhich are able to capture non-linear and non-monotonic covariate-treatment\ninteractions. We also propose a novel combined test, which leverages the power\nof all of our proposed univariate tests into a single general-case tool. We\npresent results for both synthetic trials as well as real-world clinical\ntrials, where we compare our method with state-of-the-art techniques and\ndemonstrate the utility and robustness of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 21:09:13 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Goujaud", "Baptiste", ""], ["Tramel", "Eric W.", ""], ["Courtiol", "Pierre", ""], ["Zaslavskiy", "Mikhail", ""], ["Wainrib", "Gilles", ""]]}, {"id": "1712.08235", "submitter": "Nigul Olspert", "authors": "N. Olspert, J. Pelt, M. J. K\\\"apyl\\\"a, J. Lehtinen", "title": "Estimating activity cycles with probabilistic methods I. Bayesian\n  Generalised Lomb-Scargle Periodogram with Trend", "comments": null, "journal-ref": "A&A 615, A111 (2018)", "doi": "10.1051/0004-6361/201732524", "report-no": null, "categories": "astro-ph.SR astro-ph.IM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Period estimation is one of the central topics in astronomical time series\nanalysis, where data is often unevenly sampled. Especially challenging are\nstudies of stellar magnetic cycles, as there the periods looked for are of the\norder of the same length than the datasets themselves. The datasets often\ncontain trends, the origin of which is either a real long-term cycle or an\ninstrumental effect, but these effects cannot be reliably separated, while they\ncan lead to erroneous period determinations if not properly handled. In this\nstudy we aim at developing a method that can handle the trends properly, and by\nperforming extensive set of testing, we show that this is the optimal procedure\nwhen contrasted with methods that do not include the trend directly to the\nmodel. The effect of the form of the noise (whether constant or\nheteroscedastic) on the results is also investigated. We introduce a Bayesian\nGeneralised Lomb-Scargle Periodogram with Trend (BGLST), which is a\nprobabilistic linear regression model using Gaussian priors for the\ncoefficients and uniform prior for the frequency parameter. We show, using\nsynthetic data, that when there is no prior information on whether and to what\nextent the true model of the data contains a linear trend, the introduced BGLST\nmethod is preferable to the methods which either detrend the data or leave the\ndata untrended before fitting the periodic model. Whether to use noise with\ndifferent than constant variance in the model depends on the density of the\ndata sampling as well as on the true noise type of the process.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 22:16:14 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 13:12:16 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Olspert", "N.", ""], ["Pelt", "J.", ""], ["K\u00e4pyl\u00e4", "M. J.", ""], ["Lehtinen", "J.", ""]]}, {"id": "1712.08240", "submitter": "Nigul Olspert", "authors": "N. Olspert, J. Lehtinen, M. J. K\\\"apyl\\\"a, J. Pelt, A. Grigorievskiy", "title": "Estimating activity cycles with probabilistic methods II. The Mount\n  Wilson Ca H&K data", "comments": null, "journal-ref": null, "doi": "10.1051/0004-6361/201732525", "report-no": null, "categories": "astro-ph.SR stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Debate over the existence of branches in the stellar activity-rotation\ndiagrams continues. Application of modern time series analysis tools to study\nthe mean cycle periods in chromospheric activity index is lacking. We develop\nsuch models, based on Gaussian processes, for one-dimensional time series and\napply it to the extended Mount Wilson Ca H&K sample. Our main aim is to study\nhow the previously commonly used assumption of strict harmonicity of the\nstellar cycles as well as handling of the linear trends affects the results. We\nintroduce three methods of different complexity, starting with the simple\nBayesian harmonic model and followed by Gaussian Process models with periodic\nand quasi-periodic covariance functions. We confirm the existence of two\npopulations in the activity-period diagram. We find only one significant trend\nin the inactive population, namely that the cycle periods get shorter with\nincreasing rotation. This is in contrast with earlier studies, that postulate\nthe existence of trends in both of the populations. In terms of rotation to\ncycle period ratio, our data is consistent with only two activity branches such\nthat the active branch merges together with the transitional one. The retrieved\nstellar cycles are uniformly distributed over the R'HK activity index,\nindicating that the operation of stellar large-scale dynamos carries smoothly\nover the Vaughan-Preston gap. At around the solar activity index, however,\nindications of a disruption in the cyclic dynamo action are seen. Our study\nshows that stellar cycle estimates depend significantly on the model applied.\nSuch model-dependent aspects include the improper treatment of linear trends,\nwhile the assumption of strict harmonicity can result in the appearance of\ndouble cyclicities that seem more likely to be explained by the\nquasi-periodicity of the cycles.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 22:40:45 GMT"}, {"version": "v2", "created": "Sat, 2 Jun 2018 08:20:50 GMT"}, {"version": "v3", "created": "Fri, 29 Jun 2018 07:25:26 GMT"}, {"version": "v4", "created": "Thu, 12 Jul 2018 06:53:50 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Olspert", "N.", ""], ["Lehtinen", "J.", ""], ["K\u00e4pyl\u00e4", "M. J.", ""], ["Pelt", "J.", ""], ["Grigorievskiy", "A.", ""]]}, {"id": "1712.08243", "submitter": "St\\'ephane Ga\\\"iffas", "authors": "Maryan Morel, Emmanuel Bacry, St\\'ephane Ga\\\"iffas, Agathe Guilloux,\n  Fanny Leroy", "title": "ConvSCCS: convolutional self-controlled case series model for lagged\n  adverse event detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increased availability of large databases of electronic health\nrecords (EHRs) comes the chance of enhancing health risks screening. Most\npost-marketing detections of adverse drug reaction (ADR) rely on physicians'\nspontaneous reports, leading to under reporting. To take up this challenge, we\ndevelop a scalable model to estimate the effect of multiple longitudinal\nfeatures (drug exposures) on a rare longitudinal outcome. Our procedure is\nbased on a conditional Poisson model also known as self-controlled case series\n(SCCS). We model the intensity of outcomes using a convolution between\nexposures and step functions, that are penalized using a combination of\ngroup-Lasso and total-variation. This approach does not require the\nspecification of precise risk periods, and allows to study in the same model\nseveral exposures at the same time. We illustrate the fact that this approach\nimproves the state-of-the-art for the estimation of the relative risks both on\nsimulations and on a cohort of diabetic patients, extracted from the large\nFrench national health insurance database (SNIIRAM), a SQL database built\naround medical reimbursements of more than 65 million people. This work has\nbeen done in the context of a research partnership between Ecole Polytechnique\nand CNAMTS (in charge of SNIIRAM).\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 23:06:22 GMT"}, {"version": "v2", "created": "Thu, 25 Jan 2018 23:27:15 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Morel", "Maryan", ""], ["Bacry", "Emmanuel", ""], ["Ga\u00efffas", "St\u00e9phane", ""], ["Guilloux", "Agathe", ""], ["Leroy", "Fanny", ""]]}, {"id": "1712.08244", "submitter": "Tengyuan Liang", "authors": "Tengyuan Liang", "title": "How Well Can Generative Adversarial Networks Learn Densities: A\n  Nonparametric View", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study in this paper the rate of convergence for learning densities under\nthe Generative Adversarial Networks (GAN) framework, borrowing insights from\nnonparametric statistics. We introduce an improved GAN estimator that achieves\na faster rate, through simultaneously leveraging the level of smoothness in the\ntarget density and the evaluation metric, which in theory remedies the mode\ncollapse problem reported in the literature. A minimax lower bound is\nconstructed to show that when the dimension is large, the exponent in the rate\nfor the new GAN estimator is near optimal. One can view our results as\nanswering in a quantitative way how well GAN learns a wide range of densities\nwith different smoothness properties, under a hierarchy of evaluation metrics.\nAs a byproduct, we also obtain improved generalization bounds for GAN with\ndeeper ReLU discriminator network.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 23:13:27 GMT"}, {"version": "v2", "created": "Fri, 16 Feb 2018 21:19:40 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Liang", "Tengyuan", ""]]}, {"id": "1712.08259", "submitter": "Reza Bonyadi", "authors": "Mohammad Reza Bonyadi, Viktor Vegh, David C. Reutens", "title": "Linear centralization classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classification algorithm, called the Linear Centralization Classifier\n(LCC), is introduced. The algorithm seeks to find a transformation that best\nmaps instances from the feature space to a space where they concentrate towards\nthe center of their own classes, while maximimizing the distance between class\ncenters. We formulate the classifier as a quadratic program with quadratic\nconstraints. We then simplify this formulation to a linear program that can be\nsolved effectively using a linear programming solver (e.g., simplex-dual). We\nextend the formulation for LCC to enable the use of kernel functions for\nnon-linear classification applications. We compare our method with two standard\nclassification methods (support vector machine and linear discriminant\nanalysis) and four state-of-the-art classification methods when they are\napplied to eight standard classification datasets. Our experimental results\nshow that LCC is able to classify instances more accurately (based on the area\nunder the receiver operating characteristic) in comparison to other tested\nmethods on the chosen datasets. We also report the results for LCC with a\nparticular kernel to solve for synthetic non-linear classification problems.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 00:31:46 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Bonyadi", "Mohammad Reza", ""], ["Vegh", "Viktor", ""], ["Reutens", "David C.", ""]]}, {"id": "1712.08289", "submitter": "Kui Zhao", "authors": "Kui Zhao, Yuechuan Li, Zhaoqian Shuai, Cheng Yang", "title": "Learning and Transferring IDs Representation in E-commerce", "comments": "KDD'18, 9 pages", "journal-ref": null, "doi": "10.1145/3219819.3219855", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many machine intelligence techniques are developed in E-commerce and one of\nthe most essential components is the representation of IDs, including user ID,\nitem ID, product ID, store ID, brand ID, category ID etc. The classical\nencoding based methods (like one-hot encoding) are inefficient in that it\nsuffers sparsity problems due to its high dimension, and it cannot reflect the\nrelationships among IDs, either homogeneous or heterogeneous ones. In this\npaper, we propose an embedding based framework to learn and transfer the\nrepresentation of IDs. As the implicit feedbacks of users, a tremendous amount\nof item ID sequences can be easily collected from the interactive sessions. By\njointly using these informative sequences and the structural connections among\nIDs, all types of IDs can be embedded into one low-dimensional semantic space.\nSubsequently, the learned representations are utilized and transferred in four\nscenarios: (i) measuring the similarity between items, (ii) transferring from\nseen items to unseen items, (iii) transferring across different domains, (iv)\ntransferring across different tasks. We deploy and evaluate the proposed\napproach in Hema App and the results validate its effectiveness.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 02:53:50 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 03:40:19 GMT"}, {"version": "v3", "created": "Mon, 7 May 2018 01:50:57 GMT"}, {"version": "v4", "created": "Tue, 22 May 2018 04:26:46 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Zhao", "Kui", ""], ["Li", "Yuechuan", ""], ["Shuai", "Zhaoqian", ""], ["Yang", "Cheng", ""]]}, {"id": "1712.08314", "submitter": "Ekaba Bisong", "authors": "Ekaba Bisong", "title": "Benchmarking Decoupled Neural Interfaces with Synthetic Gradients", "comments": "Serious issues with the content and not appropriate for high-level\n  academic distribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Artifical Neural Networks are a particular class of learning systems modeled\nafter biological neural functions with an interesting penchant for Hebbian\nlearning, that is \"neurons that wire together, fire together\". However, unlike\ntheir natural counterparts, artificial neural networks have a close and\nstringent coupling between the modules of neurons in the network. This coupling\nor locking imposes upon the network a strict and inflexible structure that\nprevent layers in the network from updating their weights until a full\nfeed-forward and backward pass has occurred. Such a constraint though may have\nsufficed for a while, is now no longer feasible in the era of very-large-scale\nmachine learning, coupled with the increased desire for parallelization of the\nlearning process across multiple computing infrastructures. To solve this\nproblem, synthetic gradients (SG) with decoupled neural interfaces (DNI) are\nintroduced as a viable alternative to the backpropagation algorithm. This paper\nperforms a speed benchmark to compare the speed and accuracy capabilities of\nSG-DNI as opposed to a standard neural interface using multilayer perceptron\nMLP. SG-DNI shows good promise, in that it not only captures the learning\nproblem, it is also over 3-fold faster due to it asynchronous learning\ncapabilities.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 06:28:28 GMT"}, {"version": "v2", "created": "Sat, 6 Jan 2018 23:16:33 GMT"}, {"version": "v3", "created": "Mon, 21 May 2018 14:06:52 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Bisong", "Ekaba", ""]]}, {"id": "1712.08324", "submitter": "Greg Stephens", "authors": "Katarzyna Bozek, Laetitia Hebert, Alexander S Mikheyev, Greg J\n  Stephens", "title": "Towards dense object tracking in a 2D honeybee hive", "comments": "15 pages, including supplementary figures. 1 supplemental movie\n  available as an ancillary file", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From human crowds to cells in tissue, the detection and efficient tracking of\nmultiple objects in dense configurations is an important and unsolved problem.\nIn the past, limitations of image analysis have restricted studies of dense\ngroups to tracking a single or subset of marked individuals, or to\ncoarse-grained group-level dynamics, all of which yield incomplete information.\nHere, we combine convolutional neural networks (CNNs) with the model\nenvironment of a honeybee hive to automatically recognize all individuals in a\ndense group from raw image data. We create new, adapted individual labeling and\nuse the segmentation architecture U-Net with a loss function dependent on both\nobject identity and orientation. We additionally exploit temporal regularities\nof the video recording in a recurrent manner and achieve near human-level\nperformance while reducing the network size by 94% compared to the original\nU-Net architecture. Given our novel application of CNNs, we generate extensive\nproblem-specific image data in which labeled examples are produced through a\ncustom interface with Amazon Mechanical Turk. This dataset contains over\n375,000 labeled bee instances across 720 video frames at 2 FPS, representing an\nextensive resource for the development and testing of tracking methods. We\ncorrectly detect 96% of individuals with a location error of ~7% of a typical\nbody dimension, and orientation error of 12 degrees, approximating the\nvariability of human raters. Our results provide an important step towards\nefficient image-based dense object tracking by allowing for the accurate\ndetermination of object location and orientation across time-series image data\nefficiently within one network architecture.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 07:20:57 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Bozek", "Katarzyna", ""], ["Hebert", "Laetitia", ""], ["Mikheyev", "Alexander S", ""], ["Stephens", "Greg J", ""]]}, {"id": "1712.08363", "submitter": "Jan Chorowski", "authors": "Jan Chorowski, Ron J. Weiss, Rif A. Saurous, Samy Bengio", "title": "On Using Backpropagation for Speech Texture Generation and Voice\n  Conversion", "comments": "Accepted to ICASSP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by recent work on neural network image generation which rely on\nbackpropagation towards the network inputs, we present a proof-of-concept\nsystem for speech texture synthesis and voice conversion based on two\nmechanisms: approximate inversion of the representation learned by a speech\nrecognition neural network, and on matching statistics of neuron activations\nbetween different source and target utterances. Similar to image texture\nsynthesis and neural style transfer, the system works by optimizing a cost\nfunction with respect to the input waveform samples. To this end we use a\ndifferentiable mel-filterbank feature extraction pipeline and train a\nconvolutional CTC speech recognition network. Our system is able to extract\nspeaker characteristics from very limited amounts of target speaker data, as\nlittle as a few seconds, and can be used to generate realistic speech babble or\nreconstruct an utterance in a different voice.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 09:19:23 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 09:17:27 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Chorowski", "Jan", ""], ["Weiss", "Ron J.", ""], ["Saurous", "Rif A.", ""], ["Bengio", "Samy", ""]]}, {"id": "1712.08443", "submitter": "Thibault Laugel", "authors": "Thibault Laugel, Marie-Jeanne Lesot, Christophe Marsala, Xavier\n  Renard, Marcin Detyniecki", "title": "Inverse Classification for Comparison-based Interpretability in Machine\n  Learning", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of post-hoc interpretability, this paper addresses the task of\nexplaining the prediction of a classifier, considering the case where no\ninformation is available, neither on the classifier itself, nor on the\nprocessed data (neither the training nor the test data). It proposes an\ninstance-based approach whose principle consists in determining the minimal\nchanges needed to alter a prediction: given a data point whose classification\nmust be explained, the proposed method consists in identifying a close\nneighbour classified differently, where the closeness definition integrates a\nsparsity constraint. This principle is implemented using observation generation\nin the Growing Spheres algorithm. Experimental results on two datasets\nillustrate the relevance of the proposed approach that can be used to gain\nknowledge about the classifier.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 13:51:21 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Laugel", "Thibault", ""], ["Lesot", "Marie-Jeanne", ""], ["Marsala", "Christophe", ""], ["Renard", "Xavier", ""], ["Detyniecki", "Marcin", ""]]}, {"id": "1712.08449", "submitter": "Yann Ollivier", "authors": "Yann Ollivier", "title": "True Asymptotic Natural Gradient Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a simple algorithm, True Asymptotic Natural Gradient\nOptimization (TANGO), that converges to a true natural gradient descent in the\nlimit of small learning rates, without explicit Fisher matrix estimation.\n  For quadratic models the algorithm is also an instance of averaged stochastic\ngradient, where the parameter is a moving average of a \"fast\", constant-rate\ngradient descent. TANGO appears as a particular de-linearization of averaged\nSGD, and is sometimes quite different on non-quadratic models. This further\nconnects averaged SGD and natural gradient, both of which are arguably optimal\nasymptotically.\n  In large dimension, small learning rates will be required to approximate the\nnatural gradient well. Still, this shows it is possible to get arbitrarily\nclose to exact natural gradient descent with a lightweight algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 14:04:04 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Ollivier", "Yann", ""]]}, {"id": "1712.08493", "submitter": "Shounak Datta", "authors": "Shounak Datta, Sayak Nag, Sankha Subhra Mullick, Swagatam Das", "title": "Diversifying Support Vector Machines for Boosting using Kernel\n  Perturbation: Applications to Class Imbalance and Small Disjuncts", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The diversification (generating slightly varying separating discriminators)\nof Support Vector Machines (SVMs) for boosting has proven to be a challenge due\nto the strong learning nature of SVMs. Based on the insight that perturbing the\nSVM kernel may help in diversifying SVMs, we propose two kernel perturbation\nbased boosting schemes where the kernel is modified in each round so as to\nincrease the resolution of the kernel-induced Reimannian metric in the vicinity\nof the datapoints misclassified in the previous round. We propose a method for\nidentifying the disjuncts in a dataset, dispelling the dependence on rule-based\nlearning methods for identifying the disjuncts. We also present a new\nperformance measure called Geometric Small Disjunct Index (GSDI) to quantify\nthe performance on small disjuncts for balanced as well as class imbalanced\ndatasets. Experimental comparison with a variety of state-of-the-art algorithms\nis carried out using the best classifiers of each type selected by a new\napproach inspired by multi-criteria decision making. The proposed method is\nfound to outperform the contending state-of-the-art methods on different\ndatasets (ranging from mildly imbalanced to highly imbalanced and characterized\nby varying number of disjuncts) in terms of three different performance indices\n(including the proposed GSDI).\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 15:09:16 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Datta", "Shounak", ""], ["Nag", "Sayak", ""], ["Mullick", "Sankha Subhra", ""], ["Das", "Swagatam", ""]]}, {"id": "1712.08577", "submitter": "R\\'emi Le Priol", "authors": "R\\'emi Le Priol, Alexandre Pich\\'e and Simon Lacoste-Julien", "title": "Adaptive Stochastic Dual Coordinate Ascent for Conditional Random Fields", "comments": "Published as a conference paper at UAI 2018. 22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work investigates the training of conditional random fields (CRFs) via\nthe stochastic dual coordinate ascent (SDCA) algorithm of Shalev-Shwartz and\nZhang (2016). SDCA enjoys a linear convergence rate and a strong empirical\nperformance for binary classification problems. However, it has never been used\nto train CRFs. Yet it benefits from an `exact' line search with a single\nmarginalization oracle call, unlike previous approaches. In this paper, we\nadapt SDCA to train CRFs, and we enhance it with an adaptive non-uniform\nsampling strategy based on block duality gaps. We perform experiments on four\nstandard sequence prediction tasks. SDCA demonstrates performances on par with\nthe state of the art, and improves over it on three of the four datasets, which\nhave in common the use of sparse features.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 17:05:24 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2018 00:19:07 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Priol", "R\u00e9mi Le", ""], ["Pich\u00e9", "Alexandre", ""], ["Lacoste-Julien", "Simon", ""]]}, {"id": "1712.08626", "submitter": "Fattaneh Jabbari", "authors": "Fattaneh Jabbari, Mahdi Pakdaman Naeini, Gregory F. Cooper", "title": "Obtaining Accurate Probabilistic Causal Inference by Post-Processing\n  Calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovery of an accurate causal Bayesian network structure from observational\ndata can be useful in many areas of science. Often the discoveries are made\nunder uncertainty, which can be expressed as probabilities. To guide the use of\nsuch discoveries, including directing further investigation, it is important\nthat those probabilities be well-calibrated. In this paper, we introduce a\nnovel framework to derive calibrated probabilities of causal relationships from\nobservational data. The framework consists of three components: (1) an\napproximate method for generating initial probability estimates of the edge\ntypes for each pair of variables, (2) the availability of a relatively small\nnumber of the causal relationships in the network for which the truth status is\nknown, which we call a calibration training set, and (3) a calibration method\nfor using the approximate probability estimates and the calibration training\nset to generate calibrated probabilities for the many remaining pairs of\nvariables. We also introduce a new calibration method based on a shallow neural\nnetwork. Our experiments on simulated data support that the proposed approach\nimproves the calibration of causal edge predictions. The results also support\nthat the approach often improves the precision and recall of predictions.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 19:15:15 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Jabbari", "Fattaneh", ""], ["Naeini", "Mahdi Pakdaman", ""], ["Cooper", "Gregory F.", ""]]}, {"id": "1712.08642", "submitter": "Stephen Tu", "authors": "Stephen Tu and Benjamin Recht", "title": "Least-Squares Temporal Difference Learning for the Linear Quadratic\n  Regulator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) has been successfully used to solve many\ncontinuous control tasks. Despite its impressive results however, fundamental\nquestions regarding the sample complexity of RL on continuous problems remain\nopen. We study the performance of RL in this setting by considering the\nbehavior of the Least-Squares Temporal Difference (LSTD) estimator on the\nclassic Linear Quadratic Regulator (LQR) problem from optimal control. We give\nthe first finite-time analysis of the number of samples needed to estimate the\nvalue function for a fixed static state-feedback policy to within\n$\\varepsilon$-relative error. In the process of deriving our result, we give a\ngeneral characterization for when the minimum eigenvalue of the empirical\ncovariance matrix formed along the sample path of a fast-mixing stochastic\nprocess concentrates above zero, extending a result by Koltchinskii and\nMendelson in the independent covariates setting. Finally, we provide\nexperimental evidence indicating that our analysis correctly captures the\nqualitative behavior of LSTD on several LQR instances.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 20:12:07 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Tu", "Stephen", ""], ["Recht", "Benjamin", ""]]}, {"id": "1712.08645", "submitter": "Chun-Hao Chang", "authors": "Chun-Hao Chang, Ladislav Rampasek, Anna Goldenberg", "title": "Dropout Feature Ranking for Deep Learning Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) achieve state-of-the-art results in a variety of\ndomains. Unfortunately, DNNs are notorious for their non-interpretability, and\nthus limit their applicability in hypothesis-driven domains such as biology and\nhealthcare. Moreover, in the resource-constraint setting, it is critical to\ndesign tests relying on fewer more informative features leading to high\naccuracy performance within reasonable budget. We aim to close this gap by\nproposing a new general feature ranking method for deep learning. We show that\nour simple yet effective method performs on par or compares favorably to eight\nstrawman, classical and deep-learning feature ranking methods in two\nsimulations and five very different datasets on tasks ranging from\nclassification to regression, in both static and time series scenarios. We also\nillustrate the use of our method on a drug response dataset and show that it\nidentifies genes relevant to the drug-response.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 20:25:31 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2018 16:36:04 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Chang", "Chun-Hao", ""], ["Rampasek", "Ladislav", ""], ["Goldenberg", "Anna", ""]]}, {"id": "1712.08655", "submitter": "Michael Bianco", "authors": "Michael Bianco and Peter Gerstoft", "title": "Travel time tomography with adaptive dictionaries", "comments": "Submitted to IEEE Transactions on Computational Imaging (1st\n  revision)", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a 2D travel time tomography method which regularizes the inversion\nby modeling groups of slowness pixels from discrete slowness maps, called\npatches, as sparse linear combinations of atoms from a dictionary. We propose\nto use dictionary learning during the inversion to adapt dictionaries to\nspecific slowness maps. This patch regularization, called the local model, is\nintegrated into the overall slowness map, called the global model. The local\nmodel considers small-scale variations using a sparsity constraint and the\nglobal model considers larger-scale features constrained using $\\ell_2$\nregularization. This strategy in a locally-sparse travel time tomography (LST)\napproach enables simultaneous modeling of smooth and discontinuous slowness\nfeatures. This is in contrast to conventional tomography methods, which\nconstrain models to be exclusively smooth or discontinuous. We develop a\n$\\textit{maximum a posteriori}$ formulation for LST and exploit the sparsity of\nslowness patches using dictionary learning. The LST approach compares favorably\nwith smoothness and total variation regularization methods on densely, but\nirregularly sampled synthetic slowness maps.\n", "versions": [{"version": "v1", "created": "Sat, 16 Dec 2017 02:28:40 GMT"}, {"version": "v2", "created": "Sun, 14 Jan 2018 22:23:14 GMT"}, {"version": "v3", "created": "Wed, 16 May 2018 04:09:57 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Bianco", "Michael", ""], ["Gerstoft", "Peter", ""]]}, {"id": "1712.08664", "submitter": "Paul McNicholas", "authors": "Michael P.B. Gallaugher and Paul D. McNicholas", "title": "A Mixture of Matrix Variate Bilinear Factor Analyzers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the years data has become increasingly higher dimensional, which has\nprompted an increased need for dimension reduction techniques. This is perhaps\nespecially true for clustering (unsupervised classification) as well as\nsemi-supervised and supervised classification. Although dimension reduction in\nthe area of clustering for multivariate data has been quite thoroughly\ndiscussed within the literature, there is relatively little work in the area of\nthree-way, or matrix variate, data. Herein, we develop a mixture of matrix\nvariate bilinear factor analyzers (MMVBFA) model for use in clustering\nhigh-dimensional matrix variate data. This work can be considered both the\nfirst matrix variate bilinear factor analysis model as well as the first MMVBFA\nmodel. Parameter estimation is discussed, and the MMVBFA model is illustrated\nusing simulated and real data.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 21:26:09 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 16:45:03 GMT"}, {"version": "v3", "created": "Sat, 29 Sep 2018 23:41:55 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Gallaugher", "Michael P. B.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1712.08708", "submitter": "Siddique Latif", "authors": "Siddique Latif, Rajib Rana, Junaid Qadir, Julien Epps", "title": "Variational Autoencoders for Learning Latent Representations of Speech\n  Emotion: A Preliminary Study", "comments": "Proc. Interspeech 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the latent representation of data in unsupervised fashion is a very\ninteresting process that provides relevant features for enhancing the\nperformance of a classifier. For speech emotion recognition tasks, generating\neffective features is crucial. Currently, handcrafted features are mostly used\nfor speech emotion recognition, however, features learned automatically using\ndeep learning have shown strong success in many problems, especially in image\nprocessing. In particular, deep generative models such as Variational\nAutoencoders (VAEs) have gained enormous success for generating features for\nnatural images. Inspired by this, we propose VAEs for deriving the latent\nrepresentation of speech signals and use this representation to classify\nemotions. To the best of our knowledge, we are the first to propose VAEs for\nspeech emotion classification. Evaluations on the IEMOCAP dataset demonstrate\nthat features learned by VAEs can produce state-of-the-art results for speech\nemotion classification.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 03:54:00 GMT"}, {"version": "v2", "created": "Mon, 26 Mar 2018 07:47:54 GMT"}, {"version": "v3", "created": "Tue, 28 Jul 2020 01:35:27 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Latif", "Siddique", ""], ["Rana", "Rajib", ""], ["Qadir", "Junaid", ""], ["Epps", "Julien", ""]]}, {"id": "1712.08713", "submitter": "Fnu Suya", "authors": "Fnu Suya, Yuan Tian, David Evans, Paolo Papotti", "title": "Query-limited Black-box Attacks to Classifiers", "comments": "5 Pages, 2017 NIPS workshop on machine learning and computer security\n  (12/08/2017-12/09/2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study black-box attacks on machine learning classifiers where each query\nto the model incurs some cost or risk of detection to the adversary. We focus\nexplicitly on minimizing the number of queries as a major objective.\nSpecifically, we consider the problem of attacking machine learning classifiers\nsubject to a budget of feature modification cost while minimizing the number of\nqueries, where each query returns only a class and confidence score. We\ndescribe an approach that uses Bayesian optimization to minimize the number of\nqueries, and find that the number of queries can be reduced to approximately\none tenth of the number needed through a random strategy for scenarios where\nthe feature modification cost budget is low.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 04:40:47 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Suya", "Fnu", ""], ["Tian", "Yuan", ""], ["Evans", "David", ""], ["Papotti", "Paolo", ""]]}, {"id": "1712.08754", "submitter": "Hirofumi Ohta", "authors": "Hirofumi Ohta, Satoshi Hara", "title": "On Estimation of Conditional Modes Using Multiple Quantile Regressions", "comments": "26 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an estimation method for the conditional mode when the\nconditioning variable is high-dimensional. In the proposed method, we first\nestimate the conditional density by solving quantile regressions multiple\ntimes. We then estimate the conditional mode by finding the maximum of the\nestimated conditional density. The proposed method has two advantages in that\nit is computationally stable because it has no initial parameter dependencies,\nand it is statistically efficient with a fast convergence rate. Synthetic and\nreal-world data experiments demonstrate the better performance of the proposed\nmethod compared to other existing ones.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 11:02:20 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Ohta", "Hirofumi", ""], ["Hara", "Satoshi", ""]]}, {"id": "1712.08773", "submitter": "Gabriel Terejanu", "authors": "Chao Chen, Xiao Lin, Gabriel Terejanu", "title": "An Approximate Bayesian Long Short-Term Memory Algorithm for Outlier\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long Short-Term Memory networks trained with gradient descent and\nback-propagation have received great success in various applications. However,\npoint estimation of the weights of the networks is prone to over-fitting\nproblems and lacks important uncertainty information associated with the\nestimation. However, exact Bayesian neural network methods are intractable and\nnon-applicable for real-world applications. In this study, we propose an\napproximate estimation of the weights uncertainty using Ensemble Kalman Filter,\nwhich is easily scalable to a large number of weights. Furthermore, we optimize\nthe covariance of the noise distribution in the ensemble update step using\nmaximum likelihood estimation. To assess the proposed algorithm, we apply it to\noutlier detection in five real-world events retrieved from the Twitter\nplatform.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 13:23:26 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 02:11:33 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Chen", "Chao", ""], ["Lin", "Xiao", ""], ["Terejanu", "Gabriel", ""]]}, {"id": "1712.08786", "submitter": "Ranjan Maitra", "authors": "Anna D. Peterson and Arka P. Ghosh and Ranjan Maitra", "title": "Merging $K$-means with hierarchical clustering for identifying\n  general-shaped groups", "comments": "16 pages, 1 table, 9 figures; accepted for publication in Stat", "journal-ref": null, "doi": "10.1002/sta4.172", "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering partitions a dataset such that observations placed together in a\ngroup are similar but different from those in other groups. Hierarchical and\n$K$-means clustering are two approaches but have different strengths and\nweaknesses. For instance, hierarchical clustering identifies groups in a\ntree-like structure but suffers from computational complexity in large datasets\nwhile $K$-means clustering is efficient but designed to identify homogeneous\nspherically-shaped clusters. We present a hybrid non-parametric clustering\napproach that amalgamates the two methods to identify general-shaped clusters\nand that can be applied to larger datasets. Specifically, we first partition\nthe dataset into spherical groups using $K$-means. We next merge these groups\nusing hierarchical methods with a data-driven distance measure as a stopping\ncriterion. Our proposal has the potential to reveal groups with general shapes\nand structure in a dataset. We demonstrate good performance on several\nsimulated and real datasets.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 15:07:00 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Peterson", "Anna D.", ""], ["Ghosh", "Arka P.", ""], ["Maitra", "Ranjan", ""]]}, {"id": "1712.08837", "submitter": "Ze Jin", "authors": "Ze Jin, Benjamin B. Risk, David S. Matteson", "title": "Optimization and Testing in Linear Non-Gaussian Component Analysis", "comments": "33 pages, 3 tables, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent component analysis (ICA) decomposes multivariate data into\nmutually independent components (ICs). The ICA model is subject to a constraint\nthat at most one of these components is Gaussian, which is required for model\nidentifiability. Linear non-Gaussian component analysis (LNGCA) generalizes the\nICA model to a linear latent factor model with any number of both non-Gaussian\ncomponents (signals) and Gaussian components (noise), where observations are\nlinear combinations of independent components. Although the individual Gaussian\ncomponents are not identifiable, the Gaussian subspace is identifiable. We\nintroduce an estimator along with its optimization approach in which\nnon-Gaussian and Gaussian components are estimated simultaneously, maximizing\nthe discrepancy of each non-Gaussian component from Gaussianity while\nminimizing the discrepancy of each Gaussian component from Gaussianity. When\nthe number of non-Gaussian components is unknown, we develop a statistical test\nto determine it based on resampling and the discrepancy of estimated\ncomponents. Through a variety of simulation studies, we demonstrate the\nimprovements of our estimator over competing estimators, and we illustrate the\neffectiveness of the test to determine the number of non-Gaussian components.\nFurther, we apply our method to real data examples and demonstrate its\npractical value.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 20:37:26 GMT"}, {"version": "v2", "created": "Fri, 29 Dec 2017 20:23:30 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Jin", "Ze", ""], ["Risk", "Benjamin B.", ""], ["Matteson", "David S.", ""]]}, {"id": "1712.08880", "submitter": "Michael Mahoney", "authors": "Petros Drineas and Michael W. Mahoney", "title": "Lectures on Randomized Numerical Linear Algebra", "comments": "To appear in the edited volume of lectures from the 2016 PCMI summer\n  school", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter is based on lectures on Randomized Numerical Linear Algebra from\nthe 2016 Park City Mathematics Institute summer school on The Mathematics of\nData.\n", "versions": [{"version": "v1", "created": "Sun, 24 Dec 2017 06:05:57 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Drineas", "Petros", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1712.08885", "submitter": "Shiliang Sun", "authors": "Qingjiu Zhang, Shiliang Sun", "title": "Weighted Data Normalization Based on Eigenvalues for Artificial Neural\n  Network Classification", "comments": null, "journal-ref": "The 16th International Conference on Neural Information Processing\n  (ICONIP), Lecture Notes in Computer Science, 2009, 5863: 349-356", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural network (ANN) is a very useful tool in solving learning\nproblems. Boosting the performances of ANN can be mainly concluded from two\naspects: optimizing the architecture of ANN and normalizing the raw data for\nANN. In this paper, a novel method which improves the effects of ANN by\npreprocessing the raw data is proposed. It totally leverages the fact that\ndifferent features should play different roles. The raw data set is firstly\npreprocessed by principle component analysis (PCA), and then its principle\ncomponents are weighted by their corresponding eigenvalues. Several aspects of\nanalysis are carried out to analyze its theory and the applicable occasions.\nThree classification problems are launched by an active learning algorithm to\nverify the proposed method. From the empirical results, conclusion comes to the\nfact that the proposed method can significantly improve the performance of ANN.\n", "versions": [{"version": "v1", "created": "Sun, 24 Dec 2017 07:47:13 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Zhang", "Qingjiu", ""], ["Sun", "Shiliang", ""]]}, {"id": "1712.08914", "submitter": "Ahmed Alaa", "authors": "Ahmed M. Alaa and Mihaela van der Schaar", "title": "Bayesian Nonparametric Causal Inference: Information Rates and Learning\n  Algorithms", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2018.2848230", "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of estimating the causal effect of a treatment on\nindividual subjects from observational data, this is a central problem in\nvarious application domains, including healthcare, social sciences, and online\nadvertising. Within the Neyman Rubin potential outcomes model, we use the\nKullback Leibler (KL) divergence between the estimated and true distributions\nas a measure of accuracy of the estimate, and we define the information rate of\nthe Bayesian causal inference procedure as the (asymptotic equivalence class of\nthe) expected value of the KL divergence between the estimated and true\ndistributions as a function of the number of samples. Using Fano method, we\nestablish a fundamental limit on the information rate that can be achieved by\nany Bayesian estimator, and show that this fundamental limit is independent of\nthe selection bias in the observational data. We characterize the Bayesian\npriors on the potential (factual and counterfactual) outcomes that achieve the\noptimal information rate. As a consequence, we show that a particular class of\npriors that have been widely used in the causal inference literature cannot\nachieve the optimal information rate. On the other hand, a broader class of\npriors can achieve the optimal information rate. We go on to propose a prior\nadaptation procedure (which we call the information based empirical Bayes\nprocedure) that optimizes the Bayesian prior by maximizing an information\ntheoretic criterion on the recovered causal effects rather than maximizing the\nmarginal likelihood of the observed (factual) data. Building on our analysis,\nwe construct an information optimal Bayesian causal inference algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 24 Dec 2017 12:36:23 GMT"}, {"version": "v2", "created": "Sun, 21 Jan 2018 23:16:54 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Alaa", "Ahmed M.", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1712.08968", "submitter": "Itay Safran", "authors": "Itay Safran, Ohad Shamir", "title": "Spurious Local Minima are Common in Two-Layer ReLU Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the optimization problem associated with training simple ReLU\nneural networks of the form $\\mathbf{x}\\mapsto\n\\sum_{i=1}^{k}\\max\\{0,\\mathbf{w}_i^\\top \\mathbf{x}\\}$ with respect to the\nsquared loss. We provide a computer-assisted proof that even if the input\ndistribution is standard Gaussian, even if the dimension is arbitrarily large,\nand even if the target values are generated by such a network, with orthonormal\nparameter vectors, the problem can still have spurious local minima once $6\\le\nk\\le 20$. By a concentration of measure argument, this implies that in high\ninput dimensions, \\emph{nearly all} target networks of the relevant sizes lead\nto spurious local minima. Moreover, we conduct experiments which show that the\nprobability of hitting such local minima is quite high, and increasing with the\nnetwork size. On the positive side, mild over-parameterization appears to\ndrastically reduce such local minima, indicating that an over-parameterization\nassumption is necessary to get a positive result in this setting.\n", "versions": [{"version": "v1", "created": "Sun, 24 Dec 2017 21:00:10 GMT"}, {"version": "v2", "created": "Thu, 28 Dec 2017 16:16:48 GMT"}, {"version": "v3", "created": "Thu, 9 Aug 2018 14:17:45 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Safran", "Itay", ""], ["Shamir", "Ohad", ""]]}, {"id": "1712.08983", "submitter": "Debdeep Pati", "authors": "Debdeep Pati, Anirban Bhattacharya, Yun Yang", "title": "On Statistical Optimality of Variational Bayes", "comments": "Accepted at AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article addresses a long-standing open problem on the justification of\nusing variational Bayes methods for parameter estimation. We provide general\nconditions for obtaining optimal risk bounds for point estimates acquired from\nmean-field variational Bayesian inference. The conditions pertain to the\nexistence of certain test functions for the distance metric on the parameter\nspace and minimal assumptions on the prior. A general recipe for verification\nof the conditions is outlined which is broadly applicable to existing Bayesian\nmodels with or without latent variables. As illustrations, specific\napplications to Latent Dirichlet Allocation and Gaussian mixture models are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Mon, 25 Dec 2017 01:29:09 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Pati", "Debdeep", ""], ["Bhattacharya", "Anirban", ""], ["Yang", "Yun", ""]]}, {"id": "1712.09001", "submitter": "Shiliang Sun", "authors": "Rongqing Huang, Shiliang Sun", "title": "Kernel Regression with Sparse Metric Learning", "comments": null, "journal-ref": "Journal of Intelligent and Fuzzy Systems, 2013, 24(4): 775-787", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel regression is a popular non-parametric fitting technique. It aims at\nlearning a function which estimates the targets for test inputs as precise as\npossible. Generally, the function value for a test input is estimated by a\nweighted average of the surrounding training examples. The weights are\ntypically computed by a distance-based kernel function and they strongly depend\non the distances between examples. In this paper, we first review the latest\ndevelopments of sparse metric learning and kernel regression. Then a novel\nkernel regression method involving sparse metric learning, which is called\nkernel regression with sparse metric learning (KR$\\_$SML), is proposed. The\nsparse kernel regression model is established by enforcing a mixed $(2,1)$-norm\nregularization over the metric matrix. It learns a Mahalanobis distance metric\nby a gradient descent procedure, which can simultaneously conduct\ndimensionality reduction and lead to good prediction results. Our work is the\nfirst to combine kernel regression with sparse metric learning. To verify the\neffectiveness of the proposed method, it is evaluated on 19 data sets for\nregression. Furthermore, the new method is also applied to solving practical\nproblems of forecasting short-term traffic flows. In the end, we compare the\nproposed method with other three related kernel regression methods on all test\ndata sets under two criterions. Experimental results show that the proposed\nmethod is much more competitive.\n", "versions": [{"version": "v1", "created": "Mon, 25 Dec 2017 04:00:23 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Huang", "Rongqing", ""], ["Sun", "Shiliang", ""]]}, {"id": "1712.09005", "submitter": "George Linderman", "authors": "George C. Linderman, Manas Rachh, Jeremy G. Hoskins, Stefan\n  Steinerberger, Yuval Kluger", "title": "Efficient Algorithms for t-distributed Stochastic Neighborhood Embedding", "comments": null, "journal-ref": null, "doi": "10.1038/s41592-018-0308-4", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  t-distributed Stochastic Neighborhood Embedding (t-SNE) is a method for\ndimensionality reduction and visualization that has become widely popular in\nrecent years. Efficient implementations of t-SNE are available, but they scale\npoorly to datasets with hundreds of thousands to millions of high dimensional\ndata-points. We present Fast Fourier Transform-accelerated Interpolation-based\nt-SNE (FIt-SNE), which dramatically accelerates the computation of t-SNE. The\nmost time-consuming step of t-SNE is a convolution that we accelerate by\ninterpolating onto an equispaced grid and subsequently using the fast Fourier\ntransform to perform the convolution. We also optimize the computation of input\nsimilarities in high dimensions using multi-threaded approximate nearest\nneighbors. We further present a modification to t-SNE called \"late\nexaggeration,\" which allows for easier identification of clusters in t-SNE\nembeddings. Finally, for datasets that cannot be loaded into the memory, we\npresent out-of-core randomized principal component analysis (oocPCA), so that\nthe top principal components of a dataset can be computed without ever fully\nloading the matrix, hence allowing for t-SNE of large datasets to be computed\non resource-limited machines.\n", "versions": [{"version": "v1", "created": "Mon, 25 Dec 2017 04:51:25 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Linderman", "George C.", ""], ["Rachh", "Manas", ""], ["Hoskins", "Jeremy G.", ""], ["Steinerberger", "Stefan", ""], ["Kluger", "Yuval", ""]]}, {"id": "1712.09007", "submitter": "Ger Yang", "authors": "David Liau, Eric Price, Zhao Song, Ger Yang", "title": "Stochastic Multi-armed Bandits in Constant Space", "comments": "AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the stochastic bandit problem in the sublinear space setting,\nwhere one cannot record the win-loss record for all $K$ arms. We give an\nalgorithm using $O(1)$ words of space with regret \\[\n  \\sum_{i=1}^{K}\\frac{1}{\\Delta_i}\\log \\frac{\\Delta_i}{\\Delta}\\log T \\] where\n$\\Delta_i$ is the gap between the best arm and arm $i$ and $\\Delta$ is the gap\nbetween the best and the second-best arms. If the rewards are bounded away from\n$0$ and $1$, this is within an $O(\\log 1/\\Delta)$ factor of the optimum regret\npossible without space constraints.\n", "versions": [{"version": "v1", "created": "Mon, 25 Dec 2017 05:04:35 GMT"}, {"version": "v2", "created": "Wed, 16 May 2018 17:06:53 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Liau", "David", ""], ["Price", "Eric", ""], ["Song", "Zhao", ""], ["Yang", "Ger", ""]]}, {"id": "1712.09043", "submitter": "Qibing Li", "authors": "Qibing Li, Xiaolin Zheng and Xinyue Wu", "title": "Neural Collaborative Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep neural networks have yielded state-of-the-art\nperformance on several tasks. Although some recent works have focused on\ncombining deep learning with recommendation, we highlight three issues of\nexisting models. First, these models cannot work on both explicit and implicit\nfeedback, since the network structures are specially designed for one\nparticular case. Second, due to the difficulty on training deep neural\nnetworks, existing explicit models do not fully exploit the expressive\npotential of deep learning. Third, neural network models are easier to overfit\non the implicit setting than shallow models. To tackle these issues, we present\na generic recommender framework called Neural Collaborative Autoencoder (NCAE)\nto perform collaborative filtering, which works well for both explicit feedback\nand implicit feedback. NCAE can effectively capture the subtle hidden\nrelationships between interactions via a non-linear matrix factorization\nprocess. To optimize the deep architecture of NCAE, we develop a three-stage\npre-training mechanism that combines supervised and unsupervised feature\nlearning. Moreover, to prevent overfitting on the implicit setting, we propose\nan error reweighting module and a sparsity-aware data-augmentation strategy.\nExtensive experiments on three real-world datasets demonstrate that NCAE can\nsignificantly advance the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 25 Dec 2017 08:48:43 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 04:39:05 GMT"}, {"version": "v3", "created": "Wed, 19 Dec 2018 06:40:14 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Li", "Qibing", ""], ["Zheng", "Xiaolin", ""], ["Wu", "Xinyue", ""]]}, {"id": "1712.09097", "submitter": "Bai Li", "authors": "Bai Li, Changyou Chen, Hao Liu, Lawrence Carin", "title": "On Connecting Stochastic Gradient MCMC and Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Significant success has been realized recently on applying machine learning\nto real-world applications. There have also been corresponding concerns on the\nprivacy of training data, which relates to data security and confidentiality\nissues. Differential privacy provides a principled and rigorous privacy\nguarantee on machine learning models. While it is common to design a model\nsatisfying a required differential-privacy property by injecting noise, it is\ngenerally hard to balance the trade-off between privacy and utility. We show\nthat stochastic gradient Markov chain Monte Carlo (SG-MCMC) -- a class of\nscalable Bayesian posterior sampling algorithms proposed recently -- satisfies\nstrong differential privacy with carefully chosen step sizes. We develop theory\non the performance of the proposed differentially-private SG-MCMC method. We\nconduct experiments to support our analysis and show that a standard SG-MCMC\nsampler without any modification (under a default setting) can reach\nstate-of-the-art performance in terms of both privacy and utility on Bayesian\nlearning.\n", "versions": [{"version": "v1", "created": "Mon, 25 Dec 2017 16:29:44 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Li", "Bai", ""], ["Chen", "Changyou", ""], ["Liu", "Hao", ""], ["Carin", "Lawrence", ""]]}, {"id": "1712.09117", "submitter": "Romain Cosentino Mr", "authors": "Romain Cosentino, Randall Balestriero, Richard Baraniuk, Ankit Patel", "title": "Overcomplete Frame Thresholding for Acoustic Scene Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we derive a generic overcomplete frame thresholding scheme\nbased on risk minimization. Overcomplete frames being favored for analysis\ntasks such as classification, regression or anomaly detection, we provide a way\nto leverage those optimal representations in real-world applications through\nthe use of thresholding. We validate the method on a large scale bird activity\ndetection task via the scattering network architecture performed by means of\ncontinuous wavelets, known for being an adequate dictionary in audio\nenvironments.\n", "versions": [{"version": "v1", "created": "Mon, 25 Dec 2017 19:13:24 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Cosentino", "Romain", ""], ["Balestriero", "Randall", ""], ["Baraniuk", "Richard", ""], ["Patel", "Ankit", ""]]}, {"id": "1712.09123", "submitter": "Shameem Puthiya Parambath Mr.", "authors": "Shameem A Puthiya Parambath, Nishant Vijayakumar, Sanjay Chawla", "title": "SAGA: A Submodular Greedy Algorithm For Group Recommendation", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a unified framework and an algorithm for the\nproblem of group recommendation where a fixed number of items or alternatives\ncan be recommended to a group of users. The problem of group recommendation\narises naturally in many real world contexts, and is closely related to the\nbudgeted social choice problem studied in economics. We frame the group\nrecommendation problem as choosing a subgraph with the largest group consensus\nscore in a completely connected graph defined over the item affinity matrix. We\npropose a fast greedy algorithm with strong theoretical guarantees, and show\nthat the proposed algorithm compares favorably to the state-of-the-art group\nrecommendation algorithms according to commonly used relevance and coverage\nperformance measures on benchmark dataset.\n", "versions": [{"version": "v1", "created": "Mon, 25 Dec 2017 20:03:46 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Parambath", "Shameem A Puthiya", ""], ["Vijayakumar", "Nishant", ""], ["Chawla", "Sanjay", ""]]}, {"id": "1712.09150", "submitter": "Michael Smith", "authors": "Ruben Loaiza-Maya and Michael Stanley Smith", "title": "Variational Bayes Estimation of Discrete-Margined Copula Models with\n  Application to Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new variational Bayes estimator for high-dimensional copulas\nwith discrete, or a combination of discrete and continuous, margins. The method\nis based on a variational approximation to a tractable augmented posterior, and\nis faster than previous likelihood-based approaches. We use it to estimate\ndrawable vine copulas for univariate and multivariate Markov ordinal and mixed\ntime series. These have dimension $rT$, where $T$ is the number of observations\nand $r$ is the number of series, and are difficult to estimate using previous\nmethods. The vine pair-copulas are carefully selected to allow for\nheteroskedasticity, which is a feature of most ordinal time series data. When\ncombined with flexible margins, the resulting time series models also allow for\nother common features of ordinal data, such as zero inflation, multiple modes\nand under- or over-dispersion. Using six example series, we illustrate both the\nflexibility of the time series copula models, and the efficacy of the\nvariational Bayes estimator for copulas of up to 792 dimensions and 60\nparameters. This far exceeds the size and complexity of copula models for\ndiscrete data that can be estimated using previous methods.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 00:38:39 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 05:18:15 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Loaiza-Maya", "Ruben", ""], ["Smith", "Michael Stanley", ""]]}, {"id": "1712.09196", "submitter": "Ajil Jalal", "authors": "Ajil Jalal, Andrew Ilyas, Constantinos Daskalakis, Alexandros G.\n  Dimakis", "title": "The Robust Manifold Defense: Adversarial Training using Generative\n  Models", "comments": "Added pseudo code for defense-gan break", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new type of attack for finding adversarial examples for image\nclassifiers. Our method exploits spanners, i.e. deep neural networks whose\ninput space is low-dimensional and whose output range approximates the set of\nimages of interest. Spanners may be generators of GANs or decoders of VAEs. The\nkey idea in our attack is to search over latent code pairs to find ones that\ngenerate nearby images with different classifier outputs. We argue that our\nattack is stronger than searching over perturbations of real images. Moreover,\nwe show that our stronger attack can be used to reduce the accuracy of\nDefense-GAN to 3\\%, resolving an open problem from the well-known paper by\nAthalye et al. We combine our attack with normal adversarial training to obtain\nthe most robust known MNIST classifier, significantly improving the state of\nthe art against PGD attacks. Our formulation involves solving a min-max\nproblem, where the min player sets the parameters of the classifier and the max\nplayer is running our attack, and is thus searching for adversarial examples in\nthe {\\em low-dimensional} input space of the spanner.\n  All code and models are available at\n\\url{https://github.com/ajiljalal/manifold-defense.git}\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 07:28:14 GMT"}, {"version": "v2", "created": "Fri, 31 May 2019 14:42:03 GMT"}, {"version": "v3", "created": "Tue, 4 Jun 2019 13:23:51 GMT"}, {"version": "v4", "created": "Thu, 4 Jul 2019 15:26:38 GMT"}, {"version": "v5", "created": "Wed, 10 Jul 2019 03:51:45 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Jalal", "Ajil", ""], ["Ilyas", "Andrew", ""], ["Daskalakis", "Constantinos", ""], ["Dimakis", "Alexandros G.", ""]]}, {"id": "1712.09203", "submitter": "Hongyang Zhang", "authors": "Yuanzhi Li, Tengyu Ma, Hongyang Zhang", "title": "Algorithmic Regularization in Over-parameterized Matrix Sensing and\n  Neural Networks with Quadratic Activations", "comments": "COLT 2018 best paper; fixed minor missing steps in the previous\n  version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the gradient descent algorithm provides an implicit\nregularization effect in the learning of over-parameterized matrix\nfactorization models and one-hidden-layer neural networks with quadratic\nactivations. Concretely, we show that given $\\tilde{O}(dr^{2})$ random linear\nmeasurements of a rank $r$ positive semidefinite matrix $X^{\\star}$, we can\nrecover $X^{\\star}$ by parameterizing it by $UU^\\top$ with $U\\in \\mathbb\nR^{d\\times d}$ and minimizing the squared loss, even if $r \\ll d$. We prove\nthat starting from a small initialization, gradient descent recovers\n$X^{\\star}$ in $\\tilde{O}(\\sqrt{r})$ iterations approximately. The results\nsolve the conjecture of Gunasekar et al.'17 under the restricted isometry\nproperty. The technique can be applied to analyzing neural networks with\none-hidden-layer quadratic activations with some technical modifications.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 08:04:43 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2018 08:21:52 GMT"}, {"version": "v3", "created": "Thu, 22 Feb 2018 03:50:08 GMT"}, {"version": "v4", "created": "Mon, 19 Mar 2018 07:59:32 GMT"}, {"version": "v5", "created": "Thu, 14 Feb 2019 00:24:05 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Li", "Yuanzhi", ""], ["Ma", "Tengyu", ""], ["Zhang", "Hongyang", ""]]}, {"id": "1712.09227", "submitter": "Murat Ozbayoglu", "authors": "A. Murat Ozbayoglu, Gokhan Kucukayan, Erdogan Dogdu", "title": "A Real-Time Autonomous Highway Accident Detection Model Based on Big\n  Data Processing and Computational Intelligence", "comments": null, "journal-ref": "IEEE International Conference on Big Data, (2016), pp.1807-1813,\n  Washington D.C. 5-8 December, 2016", "doi": "10.1109/BigData.2016.7840798", "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to increasing urban population and growing number of motor vehicles,\ntraffic congestion is becoming a major problem of the 21st century. One of the\nmain reasons behind traffic congestion is accidents which can not only result\nin casualties and losses for the participants, but also in wasted and lost time\nfor the others that are stuck behind the wheels. Early detection of an accident\ncan save lives, provides quicker road openings, hence decreases wasted time and\nresources, and increases efficiency. In this study, we propose a preliminary\nreal-time autonomous accident-detection system based on computational\nintelligence techniques. Istanbul City traffic-flow data for the year 2015 from\nvarious sensor locations are populated using big data processing methodologies.\nThe extracted features are then fed into a nearest neighbor model, a regression\ntree, and a feed-forward neural network model. For the output, the possibility\nof an occurrence of an accident is predicted. The results indicate that even\nthough the number of false alarms dominates the real accident cases, the system\ncan still provide useful information that can be used for status verification\nand early reaction to possible accidents.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 10:17:13 GMT"}], "update_date": "2018-01-02", "authors_parsed": [["Ozbayoglu", "A. Murat", ""], ["Kucukayan", "Gokhan", ""], ["Dogdu", "Erdogan", ""]]}, {"id": "1712.09277", "submitter": "Yenisel Plasencia-Cala\\~na Dr.", "authors": "Yenisel Plasencia-Cala\\~na, Mauricio Orozco-Alzate, Heydi\n  M\\'endez-V\\'azquez, Edel Garc\\'ia-Reyes, Robert P.W. Duin", "title": "Scalable Prototype Selection by Genetic Algorithms and Hashing", "comments": "26 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification in the dissimilarity space has become a very active research\narea since it provides a possibility to learn from data given in the form of\npairwise non-metric dissimilarities, which otherwise would be difficult to cope\nwith. The selection of prototypes is a key step for the further creation of the\nspace. However, despite previous efforts to find good prototypes, how to select\nthe best representation set remains an open issue. In this paper we proposed\nscalable methods to select the set of prototypes out of very large datasets.\nThe methods are based on genetic algorithms, dissimilarity-based hashing, and\ntwo different unsupervised and supervised scalable criteria. The unsupervised\ncriterion is based on the Minimum Spanning Tree of the graph created by the\nprototypes as nodes and the dissimilarities as edges. The supervised criterion\nis based on counting matching labels of objects and their closest prototypes.\nThe suitability of these type of algorithms is analyzed for the specific case\nof dissimilarity representations. The experimental results showed that the\nmethods select good prototypes taking advantage of the large datasets, and they\ndo so at low runtimes.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 14:59:09 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Plasencia-Cala\u00f1a", "Yenisel", ""], ["Orozco-Alzate", "Mauricio", ""], ["M\u00e9ndez-V\u00e1zquez", "Heydi", ""], ["Garc\u00eda-Reyes", "Edel", ""], ["Duin", "Robert P. W.", ""]]}, {"id": "1712.09327", "submitter": "Yousef Fadila", "authors": "Arkar Min Aung, Yousef Fadila, Radian Gondokaryono, Luis Gonzalez", "title": "Building Robust Deep Neural Networks for Road Sign Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks are built to generalize outside of training set in mind\nby using techniques such as regularization, early stopping and dropout. But\nconsiderations to make them more resilient to adversarial examples are rarely\ntaken. As deep neural networks become more prevalent in mission-critical and\nreal-time systems, miscreants start to attack them by intentionally making deep\nneural networks to misclassify an object of one type to be seen as another\ntype. This can be catastrophic in some scenarios where the classification of a\ndeep neural network can lead to a fatal decision by a machine. In this work, we\nused GTSRB dataset to craft adversarial samples by Fast Gradient Sign Method\nand Jacobian Saliency Method, used those crafted adversarial samples to attack\nanother Deep Convolutional Neural Network and built the attacked network to be\nmore resilient against adversarial attacks by making it more robust by\nDefensive Distillation and Adversarial Training\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 18:52:41 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Aung", "Arkar Min", ""], ["Fadila", "Yousef", ""], ["Gondokaryono", "Radian", ""], ["Gonzalez", "Luis", ""]]}, {"id": "1712.09376", "submitter": "Daniel Roy", "authors": "Gintare Karolina Dziugaite, Daniel M. Roy", "title": "Entropy-SGD optimizes the prior of a PAC-Bayes bound: Generalization\n  properties of Entropy-SGD and data-dependent priors", "comments": "18 pages, 6 figures; combines ICML camera ready with supplementary\n  materials", "journal-ref": "Proceedings of the 35th International Conference on Machine\n  Learning, PMLR 80:1377-1386, 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that Entropy-SGD (Chaudhari et al., 2017), when viewed as a learning\nalgorithm, optimizes a PAC-Bayes bound on the risk of a Gibbs (posterior)\nclassifier, i.e., a randomized classifier obtained by a risk-sensitive\nperturbation of the weights of a learned classifier. Entropy-SGD works by\noptimizing the bound's prior, violating the hypothesis of the PAC-Bayes theorem\nthat the prior is chosen independently of the data. Indeed, available\nimplementations of Entropy-SGD rapidly obtain zero training error on random\nlabels and the same holds of the Gibbs posterior. In order to obtain a valid\ngeneralization bound, we rely on a result showing that data-dependent priors\nobtained by stochastic gradient Langevin dynamics (SGLD) yield valid PAC-Bayes\nbounds provided the target distribution of SGLD is {\\epsilon}-differentially\nprivate. We observe that test error on MNIST and CIFAR10 falls within the\n(empirically nonvacuous) risk bounds computed under the assumption that SGLD\nreaches stationarity. In particular, Entropy-SGLD can be configured to yield\nrelatively tight generalization bounds and still fit real labels, although\nthese same settings do not obtain state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 19:20:55 GMT"}, {"version": "v2", "created": "Sat, 3 Mar 2018 15:51:32 GMT"}, {"version": "v3", "created": "Fri, 19 Apr 2019 19:19:21 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Dziugaite", "Gintare Karolina", ""], ["Roy", "Daniel M.", ""]]}, {"id": "1712.09379", "submitter": "Anastasios Kyrillidis", "authors": "Rajiv Khanna and Anastasios Kyrillidis", "title": "IHT dies hard: Provable accelerated Iterative Hard Thresholding", "comments": "accepted to AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study --both in theory and practice-- the use of momentum motions in\nclassic iterative hard thresholding (IHT) methods. By simply modifying plain\nIHT, we investigate its convergence behavior on convex optimization criteria\nwith non-convex constraints, under standard assumptions. In diverse scenaria,\nwe observe that acceleration in IHT leads to significant improvements, compared\nto state of the art projected gradient descent and Frank-Wolfe variants. As a\nbyproduct of our inspection, we study the impact of selecting the momentum\nparameter: similar to convex settings, two modes of behavior are observed\n--\"rippling\" and linear-- depending on the level of momentum.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 19:40:47 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 18:01:30 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Khanna", "Rajiv", ""], ["Kyrillidis", "Anastasios", ""]]}, {"id": "1712.09473", "submitter": "Zhao Song", "authors": "Huaian Diao, Zhao Song, Wen Sun, David P. Woodruff", "title": "Sketching for Kronecker Product Regression and P-splines", "comments": "AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  TensorSketch is an oblivious linear sketch introduced in Pagh'13 and later\nused in Pham, Pagh'13 in the context of SVMs for polynomial kernels. It was\nshown in Avron, Nguyen, Woodruff'14 that TensorSketch provides a subspace\nembedding, and therefore can be used for canonical correlation analysis, low\nrank approximation, and principal component regression for the polynomial\nkernel. We take TensorSketch outside of the context of polynomials kernels, and\nshow its utility in applications in which the underlying design matrix is a\nKronecker product of smaller matrices. This allows us to solve Kronecker\nproduct regression and non-negative Kronecker product regression, as well as\nregularized spline regression. Our main technical result is then in extending\nTensorSketch to other norms. That is, TensorSketch only provides input sparsity\ntime for Kronecker product regression with respect to the $2$-norm. We show how\nto solve Kronecker product regression with respect to the $1$-norm in time\nsublinear in the time required for computing the Kronecker product, as well as\nfor more general $p$-norms.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 01:26:52 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Diao", "Huaian", ""], ["Song", "Zhao", ""], ["Sun", "Wen", ""], ["Woodruff", "David P.", ""]]}, {"id": "1712.09482", "submitter": "Aritra Ghosh", "authors": "Aritra Ghosh, Himanshu Kumar, P.S. Sastry", "title": "Robust Loss Functions under Label Noise for Deep Neural Networks", "comments": "Appeared in AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications of classifier learning, training data suffers from label\nnoise. Deep networks are learned using huge training data where the problem of\nnoisy labels is particularly relevant. The current techniques proposed for\nlearning deep networks under label noise focus on modifying the network\narchitecture and on algorithms for estimating true labels from noisy labels. An\nalternate approach would be to look for loss functions that are inherently\nnoise-tolerant. For binary classification there exist theoretical results on\nloss functions that are robust to label noise. In this paper, we provide some\nsufficient conditions on a loss function so that risk minimization under that\nloss function would be inherently tolerant to label noise for multiclass\nclassification problems. These results generalize the existing results on\nnoise-tolerant loss functions for binary classification. We study some of the\nwidely used loss functions in deep networks and show that the loss function\nbased on mean absolute value of error is inherently robust to label noise. Thus\nstandard back propagation is enough to learn the true classifier even under\nlabel noise. Through experiments, we illustrate the robustness of risk\nminimization with such loss functions for learning neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 03:07:57 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Ghosh", "Aritra", ""], ["Kumar", "Himanshu", ""], ["Sastry", "P. S.", ""]]}, {"id": "1712.09520", "submitter": "Guillaume Rabusseau", "authors": "Xingwei Cao, Guillaume Rabusseau", "title": "Tensor Regression Networks with various Low-Rank Tensor Approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor regression networks achieve high compression rate of neural networks\nwhile having slight impact on performances. They do so by imposing low tensor\nrank structure on the weight matrices of fully connected layers. In recent\nyears, tensor regression networks have been investigated from the perspective\nof their compressive power, however, the regularization effect of enforcing\nlow-rank tensor structure has not been investigated enough. We study tensor\nregression networks using various low-rank tensor approximations, aiming to\ncompare the compressive and regularization power of different low-rank\nconstraints. We evaluate the compressive and regularization performances of the\nproposed model with both deep and shallow convolutional neural networks. The\noutcome of our experiment suggests the superiority of Global Average Pooling\nLayer over Tensor Regression Layer when applied to deep convolutional neural\nnetwork with CIFAR-10 dataset. On the contrary, shallow convolutional neural\nnetworks with tensor regression layer and dropout achieved lower test error\nthan both Global Average Pooling and fully-connected layer with dropout\nfunction when trained with a small number of samples.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 08:04:34 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 02:10:55 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Cao", "Xingwei", ""], ["Rabusseau", "Guillaume", ""]]}, {"id": "1712.09592", "submitter": "Murat Ozbayoglu", "authors": "O.B. Sezer, M. Ozbayoglu, E. Dogdu", "title": "An Artificial Neural Network-based Stock Trading System Using Technical\n  Analysis and Big Data Framework", "comments": "ACM Southeast Conference, ACMSE 2017, Kennesaw State University, GA,\n  U.S.A., 13-15 April, 2017", "journal-ref": "ACM Southeast Conference, ACMSE 2017, Kennesaw State University,\n  GA, U.S.A., 13-15 April, 2017", "doi": "10.1145/3077286.3077294", "report-no": null, "categories": "cs.CE q-fin.TR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a neural network-based stock price prediction and trading\nsystem using technical analysis indicators is presented. The model developed\nfirst converts the financial time series data into a series of buy-sell-hold\ntrigger signals using the most commonly preferred technical analysis\nindicators. Then, a Multilayer Perceptron (MLP) artificial neural network (ANN)\nmodel is trained in the learning stage on the daily stock prices between 1997\nand 2007 for all of the Dow30 stocks. Apache Spark big data framework is used\nin the training stage. The trained model is then tested with data from 2007 to\n2017. The results indicate that by choosing the most appropriate technical\nindicators, the neural network model can achieve comparable results against the\nBuy and Hold strategy in most of the cases. Furthermore, fine tuning the\ntechnical indicators and/or optimization strategy can enhance the overall\ntrading performance.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 14:45:40 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Sezer", "O. B.", ""], ["Ozbayoglu", "M.", ""], ["Dogdu", "E.", ""]]}, {"id": "1712.09641", "submitter": "Kostas Hatalis", "authors": "Kostas Hatalis, Shalinee Kishore", "title": "A Composite Quantile Fourier Neural Network for Multi-Step Probabilistic\n  Forecasting of Nonstationary Univariate Time Series", "comments": "59 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point forecasting of univariate time series is a challenging problem with\nextensive work having been conducted. However, nonparametric probabilistic\nforecasting of time series, such as in the form of quantiles or prediction\nintervals is an even more challenging problem. In an effort to expand the\npossible forecasting paradigms we devise and explore an extrapolation-based\napproach that has not been applied before for probabilistic forecasting. We\npresent a novel quantile Fourier neural network is for nonparametric\nprobabilistic forecasting of univariate time series. Multi-step predictions are\nprovided in the form of composite quantiles using time as the only input to the\nmodel. This effectively is a form of extrapolation based nonlinear quantile\nregression applied for forecasting. Experiments are conducted on eight real\nworld datasets that demonstrate a variety of periodic and aperiodic patterns.\nNine naive and advanced methods are used as benchmarks including quantile\nregression neural network, support vector quantile regression, SARIMA, and\nexponential smoothing. The obtained empirical results validate the\neffectiveness of the proposed method in providing high quality and accurate\nprobabilistic predictions.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 17:33:51 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 03:04:14 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Hatalis", "Kostas", ""], ["Kishore", "Shalinee", ""]]}, {"id": "1712.09657", "submitter": "Dj Strouse", "authors": "DJ Strouse, David J Schwab", "title": "The information bottleneck and geometric clustering", "comments": "Updated to final published version with more detailed relationship to\n  GMMs/k-means", "journal-ref": "Neural Computation 31 (2019) 596-612", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The information bottleneck (IB) approach to clustering takes a joint\ndistribution $P\\!\\left(X,Y\\right)$ and maps the data $X$ to cluster labels $T$\nwhich retain maximal information about $Y$ (Tishby et al., 1999). This\nobjective results in an algorithm that clusters data points based upon the\nsimilarity of their conditional distributions $P\\!\\left(Y\\mid X\\right)$. This\nis in contrast to classic \"geometric clustering'' algorithms such as $k$-means\nand gaussian mixture models (GMMs) which take a set of observed data points\n$\\left\\{ \\mathbf{x}_{i}\\right\\} _{i=1:N}$ and cluster them based upon their\ngeometric (typically Euclidean) distance from one another. Here, we show how to\nuse the deterministic information bottleneck (DIB) (Strouse and Schwab, 2017),\na variant of IB, to perform geometric clustering, by choosing cluster labels\nthat preserve information about data point location on a smoothed dataset. We\nalso introduce a novel method to choose the number of clusters, based on\nidentifying solutions where the tradeoff between number of clusters used and\nspatial information preserved is strongest. We apply this approach to a variety\nof simple clustering problems, showing that DIB with our model selection\nprocedure recovers the generative cluster labels. We also show that, in\nparticular limits of our model parameters, clustering with DIB and IB is\nequivalent to $k$-means and EM fitting of a GMM with hard and soft assignments,\nrespectively. Thus, clustering with (D)IB generalizes and provides an\ninformation-theoretic perspective on these classic algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 19:04:49 GMT"}, {"version": "v2", "created": "Sun, 31 May 2020 15:55:36 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Strouse", "DJ", ""], ["Schwab", "David J", ""]]}, {"id": "1712.09677", "submitter": "Nicolas Loizou", "authors": "Nicolas Loizou, Peter Richt\\'arik", "title": "Momentum and Stochastic Momentum for Stochastic Gradient, Newton,\n  Proximal Point and Subspace Descent Methods", "comments": "47 pages, 7 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study several classes of stochastic optimization algorithms\nenriched with heavy ball momentum. Among the methods studied are: stochastic\ngradient descent, stochastic Newton, stochastic proximal point and stochastic\ndual subspace ascent. This is the first time momentum variants of several of\nthese methods are studied. We choose to perform our analysis in a setting in\nwhich all of the above methods are equivalent. We prove global nonassymptotic\nlinear convergence rates for all methods and various measures of success,\nincluding primal function values, primal iterates (in L2 sense), and dual\nfunction values. We also show that the primal iterates converge at an\naccelerated linear rate in the L1 sense. This is the first time a linear rate\nis shown for the stochastic heavy ball method (i.e., stochastic gradient\ndescent method with momentum). Under somewhat weaker conditions, we establish a\nsublinear convergence rate for Cesaro averages of primal iterates. Moreover, we\npropose a novel concept, which we call stochastic momentum, aimed at decreasing\nthe cost of performing the momentum step. We prove linear convergence of\nseveral stochastic methods with stochastic momentum, and show that in some\nsparse data regimes and for sufficiently small momentum parameters, these\nmethods enjoy better overall complexity than methods with deterministic\nmomentum. Finally, we perform extensive numerical testing on artificial and\nreal datasets, including data coming from average consensus problems.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 20:40:24 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 18:14:11 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Loizou", "Nicolas", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1712.09685", "submitter": "Jens Berg", "authors": "Jens Berg and Kaj Nystr\\\"om", "title": "Neural network augmented inverse problems for PDEs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show how to augment classical methods for inverse problems\nwith artificial neural networks. The neural network acts as a prior for the\ncoefficient to be estimated from noisy data. Neural networks are global, smooth\nfunction approximators and as such they do not require explicit regularization\nof the error functional to recover smooth solutions and coefficients. We give\ndetailed examples using the Poisson equation in 1, 2, and 3 space dimensions\nand show that the neural network augmentation is robust with respect to noisy\nand incomplete data, mesh, and geometry.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 21:05:42 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 09:17:14 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Berg", "Jens", ""], ["Nystr\u00f6m", "Kaj", ""]]}, {"id": "1712.09694", "submitter": "Haolei Weng", "authors": "Haolei Weng and Yang Feng", "title": "On the estimation of correlation in a binary sequence model", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a binary sequence generated by thresholding a hidden continuous\nsequence. The hidden variables are assumed to have a compound symmetry\ncovariance structure with a single parameter characterizing the common\ncorrelation. We study the parameter estimation problem under such one-parameter\nmodels. We demonstrate that maximizing the likelihood function does not yield\nconsistent estimates for the correlation. We then formally prove the\nnonestimability of the parameter by deriving a non-vanishing minimax lower\nbound. This counter-intuitive phenomenon provides an interesting insight that\none-bit information of each latent variable is not sufficient to consistently\nrecover their common correlation. On the other hand, we further show that\ntrinary data generated from the hidden variables can consistently estimate the\ncorrelation with parametric convergence rate. Thus we reveal a phase transition\nphenomenon regarding the discretization of latent continuous variables while\npreserving the estimability of the correlation. Numerical experiments are\nperformed to validate the conclusions.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 22:19:19 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2019 19:27:54 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Weng", "Haolei", ""], ["Feng", "Yang", ""]]}, {"id": "1712.09707", "submitter": "Bethany Lusch", "authors": "Bethany Lusch, J. Nathan Kutz, Steven L. Brunton", "title": "Deep learning for universal linear embeddings of nonlinear dynamics", "comments": "v2: added another example and further details (increase from 9 pages\n  to 14 pages and increase from 4 figures to 16 figures)", "journal-ref": null, "doi": "10.1038/s41467-018-07210-0", "report-no": null, "categories": "math.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying coordinate transformations that make strongly nonlinear dynamics\napproximately linear is a central challenge in modern dynamical systems. These\ntransformations have the potential to enable prediction, estimation, and\ncontrol of nonlinear systems using standard linear theory. The Koopman operator\nhas emerged as a leading data-driven embedding, as eigenfunctions of this\noperator provide intrinsic coordinates that globally linearize the dynamics.\nHowever, identifying and representing these eigenfunctions has proven to be\nmathematically and computationally challenging. This work leverages the power\nof deep learning to discover representations of Koopman eigenfunctions from\ntrajectory data of dynamical systems. Our network is parsimonious and\ninterpretable by construction, embedding the dynamics on a low-dimensional\nmanifold that is of the intrinsic rank of the dynamics and parameterized by the\nKoopman eigenfunctions. In particular, we identify nonlinear coordinates on\nwhich the dynamics are globally linear using a modified auto-encoder. We also\ngeneralize Koopman representations to include a ubiquitous class of systems\nthat exhibit continuous spectra, ranging from the simple pendulum to nonlinear\noptics and broadband turbulence. Our framework parametrizes the continuous\nfrequency using an auxiliary network, enabling a compact and efficient\nembedding at the intrinsic rank, while connecting our models to half a century\nof asymptotics. In this way, we benefit from the power and generality of deep\nlearning, while retaining the physical interpretability of Koopman embeddings.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 23:10:35 GMT"}, {"version": "v2", "created": "Fri, 13 Apr 2018 06:17:30 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Lusch", "Bethany", ""], ["Kutz", "J. Nathan", ""], ["Brunton", "Steven L.", ""]]}, {"id": "1712.09713", "submitter": "Charles Zheng", "authors": "Charles Zheng, Rakesh Achanta, Yuval Benjamini", "title": "Extrapolating Expected Accuracies for Large Multi-Class Problems", "comments": "Submitted to JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The difficulty of multi-class classification generally increases with the\nnumber of classes. Using data from a subset of the classes, can we predict how\nwell a classifier will scale with an increased number of classes? Under the\nassumptions that the classes are sampled identically and independently from a\npopulation, and that the classifier is based on independently learned scoring\nfunctions, we show that the expected accuracy when the classifier is trained on\nk classes is the (k-1)st moment of a certain distribution that can be estimated\nfrom data. We present an unbiased estimation method based on the theory, and\ndemonstrate its application on a facial recognition example.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 23:49:39 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Zheng", "Charles", ""], ["Achanta", "Rakesh", ""], ["Benjamini", "Yuval", ""]]}, {"id": "1712.09763", "submitter": "Xi Chen", "authors": "Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, Pieter Abbeel", "title": "PixelSNAIL: An Improved Autoregressive Generative Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoregressive generative models consistently achieve the best results in\ndensity estimation tasks involving high dimensional data, such as images or\naudio. They pose density estimation as a sequence modeling task, where a\nrecurrent neural network (RNN) models the conditional distribution over the\nnext element conditioned on all previous elements. In this paradigm, the\nbottleneck is the extent to which the RNN can model long-range dependencies,\nand the most successful approaches rely on causal convolutions, which offer\nbetter access to earlier parts of the sequence than conventional RNNs. Taking\ninspiration from recent work in meta reinforcement learning, where dealing with\nlong-range dependencies is also essential, we introduce a new generative model\narchitecture that combines causal convolutions with self attention. In this\nnote, we describe the resulting model and present state-of-the-art\nlog-likelihood results on CIFAR-10 (2.85 bits per dim) and $32 \\times 32$\nImageNet (3.80 bits per dim). Our implementation is available at\nhttps://github.com/neocxi/pixelsnail-public\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 05:54:44 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Chen", "Xi", ""], ["Mishra", "Nikhil", ""], ["Rohaninejad", "Mostafa", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1712.09771", "submitter": "Meysam Golmohammadi", "authors": "Meysam Golmohammadi, Amir Hossein Harati Nejad Torbati, Silvia Lopez\n  de Diego, Iyad Obeid, and Joseph Picone", "title": "Automatic Analysis of EEGs Using Big Data and Hybrid Deep Learning\n  Architectures", "comments": "Under review in Journal of Clinical Neurophysiology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: A clinical decision support tool that automatically interprets\nEEGs can reduce time to diagnosis and enhance real-time applications such as\nICU monitoring. Clinicians have indicated that a sensitivity of 95% with a\nspecificity below 5% was the minimum requirement for clinical acceptance. We\npropose a highperformance classification system based on principles of big data\nand machine learning. Methods: A hybrid machine learning system that uses\nhidden Markov models (HMM) for sequential decoding and deep learning networks\nfor postprocessing is proposed. These algorithms were trained and evaluated\nusing the TUH EEG Corpus, which is the world's largest publicly available\ndatabase of clinical EEG data. Results: Our approach delivers a sensitivity\nabove 90% while maintaining a specificity below 5%. This system detects three\nevents of clinical interest: (1) spike and/or sharp waves, (2) periodic\nlateralized epileptiform discharges, (3) generalized periodic epileptiform\ndischarges. It also detects three events used to model background noise: (1)\nartifacts, (2) eye movement (3) background. Conclusions: A hybrid HMM/deep\nlearning system can deliver a low false alarm rate on EEG event detection,\nmaking automated analysis a viable option for clinicians. Significance: The TUH\nEEG Corpus enables application of highly data consumptive machine learning\nalgorithms to EEG analysis. Performance is approaching clinical acceptance for\nreal-time applications.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 06:22:28 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Golmohammadi", "Meysam", ""], ["Torbati", "Amir Hossein Harati Nejad", ""], ["de Diego", "Silvia Lopez", ""], ["Obeid", "Iyad", ""], ["Picone", "Joseph", ""]]}, {"id": "1712.09776", "submitter": "Meysam Golmohammadi", "authors": "Meysam Golmohammadi, Saeedeh Ziyabari, Vinit Shah, Silvia Lopez de\n  Diego, Iyad Obeid, and Joseph Picone", "title": "Deep Architectures for Automated Seizure Detection in Scalp EEGs", "comments": "nder review in International Conference on Machine Learning,\n  Stockholm, Sweden", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated seizure detection using clinical electroencephalograms is a\nchallenging machine learning problem because the multichannel signal often has\nan extremely low signal to noise ratio. Events of interest such as seizures are\neasily confused with signal artifacts (e.g, eye movements) or benign variants\n(e.g., slowing). Commercially available systems suffer from unacceptably high\nfalse alarm rates. Deep learning algorithms that employ high dimensional models\nhave not previously been effective due to the lack of big data resources. In\nthis paper, we use the TUH EEG Seizure Corpus to evaluate a variety of hybrid\ndeep structures including Convolutional Neural Networks and Long Short-Term\nMemory Networks. We introduce a novel recurrent convolutional architecture that\ndelivers 30% sensitivity at 7 false alarms per 24 hours. We have also evaluated\nour system on a held-out evaluation set based on the Duke University Seizure\nCorpus and demonstrate that performance trends are similar to the TUH EEG\nSeizure Corpus. This is a significant finding because the Duke corpus was\ncollected with different instrumentation and at different hospitals. Our work\nshows that deep learning architectures that integrate spatial and temporal\ncontexts are critical to achieving state of the art performance and will enable\na new generation of clinically-acceptable technology.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 06:31:22 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Golmohammadi", "Meysam", ""], ["Ziyabari", "Saeedeh", ""], ["Shah", "Vinit", ""], ["de Diego", "Silvia Lopez", ""], ["Obeid", "Iyad", ""], ["Picone", "Joseph", ""]]}, {"id": "1712.09913", "submitter": "Hao Li", "authors": "Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, Tom Goldstein", "title": "Visualizing the Loss Landscape of Neural Nets", "comments": "NIPS 2018 (extended version, 10.5 pages), code is available at\n  https://github.com/tomgoldstein/loss-landscape", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network training relies on our ability to find \"good\" minimizers of\nhighly non-convex loss functions. It is well-known that certain network\narchitecture designs (e.g., skip connections) produce loss functions that train\neasier, and well-chosen training parameters (batch size, learning rate,\noptimizer) produce minimizers that generalize better. However, the reasons for\nthese differences, and their effects on the underlying loss landscape, are not\nwell understood. In this paper, we explore the structure of neural loss\nfunctions, and the effect of loss landscapes on generalization, using a range\nof visualization methods. First, we introduce a simple \"filter normalization\"\nmethod that helps us visualize loss function curvature and make meaningful\nside-by-side comparisons between loss functions. Then, using a variety of\nvisualizations, we explore how network architecture affects the loss landscape,\nand how training parameters affect the shape of minimizers.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 16:15:42 GMT"}, {"version": "v2", "created": "Mon, 5 Mar 2018 18:23:03 GMT"}, {"version": "v3", "created": "Wed, 7 Nov 2018 06:25:20 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Li", "Hao", ""], ["Xu", "Zheng", ""], ["Taylor", "Gavin", ""], ["Studer", "Christoph", ""], ["Goldstein", "Tom", ""]]}, {"id": "1712.09923", "submitter": "Andreas Holzinger", "authors": "Andreas Holzinger, Chris Biemann, Constantinos S. Pattichis, Douglas\n  B. Kell", "title": "What do we need to build explainable AI systems for the medical domain?", "comments": "This is a survey article and section 3.1. draws heavily from\n  arXiv:1706.07979", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence (AI) generally and machine learning (ML) specifically\ndemonstrate impressive practical success in many different application domains,\ne.g. in autonomous driving, speech recognition, or recommender systems. Deep\nlearning approaches, trained on extremely large data sets or using\nreinforcement learning methods have even exceeded human performance in visual\ntasks, particularly on playing games such as Atari, or mastering the game of\nGo. Even in the medical domain there are remarkable results. The central\nproblem of such models is that they are regarded as black-box models and even\nif we understand the underlying mathematical principles, they lack an explicit\ndeclarative knowledge representation, hence have difficulty in generating the\nunderlying explanatory structures. This calls for systems enabling to make\ndecisions transparent, understandable and explainable. A huge motivation for\nour approach are rising legal and privacy aspects. The new European General\nData Protection Regulation entering into force on May 25th 2018, will make\nblack-box approaches difficult to use in business. This does not imply a ban on\nautomatic learning approaches or an obligation to explain everything all the\ntime, however, there must be a possibility to make the results re-traceable on\ndemand. In this paper we outline some of our research topics in the context of\nthe relatively new area of explainable-AI with a focus on the application in\nmedicine, which is a very special domain. This is due to the fact that medical\nprofessionals are working mostly with distributed heterogeneous and complex\nsources of data. In this paper we concentrate on three sources: images, *omics\ndata and text. We argue that research in explainable-AI would generally help to\nfacilitate the implementation of AI/ML in the medical domain, and specifically\nhelp to facilitate transparency and trust.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 16:46:05 GMT"}], "update_date": "2018-01-02", "authors_parsed": [["Holzinger", "Andreas", ""], ["Biemann", "Chris", ""], ["Pattichis", "Constantinos S.", ""], ["Kell", "Douglas B.", ""]]}, {"id": "1712.09926", "submitter": "Tsendsuren Munkhdalai", "authors": "Tsendsuren Munkhdalai, Xingdi Yuan, Soroush Mehri, and Adam Trischler", "title": "Rapid Adaptation with Conditionally Shifted Neurons", "comments": "ICML 2018; Added: additional ablation and speed comparison with\n  MetaNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a mechanism by which artificial neural networks can learn rapid\nadaptation - the ability to adapt on the fly, with little data, to new tasks -\nthat we call conditionally shifted neurons. We apply this mechanism in the\nframework of metalearning, where the aim is to replicate some of the\nflexibility of human learning in machines. Conditionally shifted neurons modify\ntheir activation values with task-specific shifts retrieved from a memory\nmodule, which is populated rapidly based on limited task experience. On\nmetalearning benchmarks from the vision and language domains, models augmented\nwith conditionally shifted neurons achieve state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 16:47:13 GMT"}, {"version": "v2", "created": "Wed, 7 Mar 2018 15:27:42 GMT"}, {"version": "v3", "created": "Tue, 3 Jul 2018 18:04:34 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Munkhdalai", "Tsendsuren", ""], ["Yuan", "Xingdi", ""], ["Mehri", "Soroush", ""], ["Trischler", "Adam", ""]]}, {"id": "1712.09983", "submitter": "Yanning Shen", "authors": "Yanning Shen and Tianyi Chen and Georgios B. Giannakis", "title": "Random Feature-based Online Multi-kernel Learning in Environments with\n  Unknown Dynamics", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel-based methods exhibit well-documented performance in various nonlinear\nlearning tasks. Most of them rely on a preselected kernel, whose prudent choice\npresumes task-specific prior information. Especially when the latter is not\navailable, multi-kernel learning has gained popularity thanks to its\nflexibility in choosing kernels from a prescribed kernel dictionary. Leveraging\nthe random feature approximation and its recent orthogonality-promoting\nvariant, the present contribution develops a scalable multi-kernel learning\nscheme (termed Raker) to obtain the sought nonlinear learning function `on the\nfly,' first for static environments. To further boost performance in dynamic\nenvironments, an adaptive multi-kernel learning scheme (termed AdaRaker) is\ndeveloped. AdaRaker accounts not only for data-driven learning of kernel\ncombination, but also for the unknown dynamics. Performance is analyzed in\nterms of both static and dynamic regrets. AdaRaker is uniquely capable of\ntracking nonlinear learning functions in environments with unknown dynamics,\nand with with analytic performance guarantees. Tests with synthetic and real\ndatasets are carried out to showcase the effectiveness of the novel algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 18:42:38 GMT"}, {"version": "v2", "created": "Wed, 17 Jan 2018 05:11:34 GMT"}, {"version": "v3", "created": "Fri, 28 Dec 2018 06:33:22 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Shen", "Yanning", ""], ["Chen", "Tianyi", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1712.09988", "submitter": "Vira Semenova", "authors": "Vira Semenova, Matt Goldman, Victor Chernozhukov, Matt Taddy", "title": "Estimation and Inference on Heterogeneous Treatment Effects in\n  High-Dimensional Dynamic Panels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides estimation and inference methods for a large number of\nheterogeneous treatment effects in the presence of an even larger number of\ncontrols and unobserved unit heterogeneity. In our main example, the vector of\nheterogeneous treatments is generated by interacting the base treatment\nvariable with a subset of controls. We first estimate the unit-specific\nexpectation functions of the outcome and each treatment interaction conditional\non controls and take the residuals. Second, we report the Lasso (L1-regularized\nleast squares) estimate of the heterogeneous treatment effect parameter,\nregressing the outcome residual on the vector of treatment ones. We debias the\nLasso estimator to conduct simultaneous inference on the target parameter by\nGaussian bootstrap. We account for the unobserved unit heterogeneity by\nprojecting it onto the time-invariant covariates, following the correlated\nrandom effects approach of Mundlak (1978) and Chamberlain (1982). We\ndemonstrate our method by estimating price elasticities of groceries based on\nscanner data.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 18:49:44 GMT"}, {"version": "v2", "created": "Wed, 10 Jan 2018 17:13:31 GMT"}, {"version": "v3", "created": "Sat, 27 Jun 2020 21:36:44 GMT"}, {"version": "v4", "created": "Tue, 16 Mar 2021 19:48:03 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Semenova", "Vira", ""], ["Goldman", "Matt", ""], ["Chernozhukov", "Victor", ""], ["Taddy", "Matt", ""]]}, {"id": "1712.10024", "submitter": "Vira Semenova", "authors": "Vira Semenova", "title": "Machine Learning for Set-Identified Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides estimation and inference methods for an identified set\nwhere the selection among a very large number of covariates is based on modern\nmachine learning tools. I characterize the boundary of the identified set\n(i.e., support function) using a semiparametric moment condition. Combining\nNeyman-orthogonality and sample splitting ideas, I construct a root-N\nconsistent, uniformly asymptotically Gaussian estimator of the support function\nand propose a weighted bootstrap procedure to conduct inference about the\nidentified set. I provide a general method to construct a Neyman-orthogonal\nmoment condition for the support function. Applying my method to Lee (2008)'s\nendogenous selection model, I provide the asymptotic theory for the sharp\n(i.e., the tightest possible) bounds on the Average Treatment Effect in the\npresence of high-dimensional covariates. Furthermore, I relax the conventional\nmonotonicity assumption and allow the sign of the treatment effect on the\nselection (e.g., employment) to be determined by covariates. Using JobCorps\ndata set with very rich baseline characteristics, I substantially tighten the\nbounds on the JobCorps effect on wages under weakened monotonicity assumption.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 19:04:28 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2018 02:12:00 GMT"}, {"version": "v3", "created": "Fri, 6 Dec 2019 21:48:51 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Semenova", "Vira", ""]]}, {"id": "1712.10043", "submitter": "Anqi Liu", "authors": "Anqi Liu and Brian D. Ziebart", "title": "Robust Covariate Shift Prediction with General Losses and Feature Views", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covariate shift relaxes the widely-employed independent and identically\ndistributed (IID) assumption by allowing different training and testing input\ndistributions. Unfortunately, common methods for addressing covariate shift by\ntrying to remove the bias between training and testing distributions using\nimportance weighting often provide poor performance guarantees in theory and\nunreliable predictions with high variance in practice. Recently developed\nmethods that construct a predictor that is inherently robust to the\ndifficulties of learning under covariate shift are restricted to minimizing\nlogloss and can be too conservative when faced with high-dimensional learning\ntasks. We address these limitations in two ways: by robustly minimizing various\nloss functions, including non-convex ones, under the testing distribution; and\nby separately shaping the influence of covariate shift according to different\nfeature-based views of the relationship between input variables and example\nlabels. These generalizations make robust covariate shift prediction applicable\nto more task scenarios. We demonstrate the benefits on classification under\ncovariate shift tasks.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 20:04:43 GMT"}], "update_date": "2018-01-02", "authors_parsed": [["Liu", "Anqi", ""], ["Ziebart", "Brian D.", ""]]}, {"id": "1712.10050", "submitter": "Anqi Liu", "authors": "Anqi Liu, Rizal Fathony, Brian D. Ziebart", "title": "Kernel Robust Bias-Aware Prediction under Covariate Shift", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under covariate shift, training (source) data and testing (target) data\ndiffer in input space distribution, but share the same conditional label\ndistribution. This poses a challenging machine learning task. Robust Bias-Aware\n(RBA) prediction provides the conditional label distribution that is robust to\nthe worstcase logarithmic loss for the target distribution while matching\nfeature expectation constraints from the source distribution. However,\nemploying RBA with insufficient feature constraints may result in high\ncertainty predictions for much of the source data, while leaving too much\nuncertainty for target data predictions. To overcome this issue, we extend the\nrepresenter theorem to the RBA setting, enabling minimization of regularized\nexpected target risk by a reweighted kernel expectation under the source\ndistribution. By applying kernel methods, we establish consistency guarantees\nand demonstrate better performance of the RBA classifier than competing methods\non synthetically biased UCI datasets as well as datasets that have natural\ncovariate shift.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 20:23:18 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Liu", "Anqi", ""], ["Fathony", "Rizal", ""], ["Ziebart", "Brian D.", ""]]}, {"id": "1712.10062", "submitter": "Aditya Gilra", "authors": "Marco Martinolli, Wulfram Gerstner and Aditya Gilra", "title": "Multi-timescale memory dynamics in a reinforcement learning network with\n  attention-gated memory", "comments": null, "journal-ref": "Frontiers in Computational Neuroscience, 12 July 2018 |\n  https://doi.org/10.3389/fncom.2018.00050", "doi": "10.3389/fncom.2018.00050", "report-no": null, "categories": "q-bio.NC cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning and memory are intertwined in our brain and their relationship is at\nthe core of several recent neural network models. In particular, the\nAttention-Gated MEmory Tagging model (AuGMEnT) is a reinforcement learning\nnetwork with an emphasis on biological plausibility of memory dynamics and\nlearning. We find that the AuGMEnT network does not solve some hierarchical\ntasks, where higher-level stimuli have to be maintained over a long time, while\nlower-level stimuli need to be remembered and forgotten over a shorter\ntimescale. To overcome this limitation, we introduce hybrid AuGMEnT, with leaky\nor short-timescale and non-leaky or long-timescale units in memory, that allow\nto exchange lower-level information while maintaining higher-level one, thus\nsolving both hierarchical and distractor tasks.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 21:26:43 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Martinolli", "Marco", ""], ["Gerstner", "Wulfram", ""], ["Gilra", "Aditya", ""]]}, {"id": "1712.10082", "submitter": "Yao Zhang", "authors": "Yao Zhang, Woong-Je Sung, Dimitri Mavris", "title": "Application of Convolutional Neural Network to Predict Airfoil Lift\n  Coefficient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adaptability of the convolutional neural network (CNN) technique for\naerodynamic meta-modeling tasks is probed in this work. The primary objective\nis to develop suitable CNN architecture for variable flow conditions and object\ngeometry, in addition to identifying a sufficient data preparation process.\nMultiple CNN structures were trained to learn the lift coefficients of the\nairfoils with a variety of shapes in multiple flow Mach numbers, Reynolds\nnumbers, and diverse angles of attack. This is conducted to illustrate the\nconcept of the technique. A multi-layered perceptron (MLP) is also used for the\ntraining sets. The MLP results are compared with that of the CNN results. The\nnewly proposed meta-modeling concept has been found to be comparable with the\nMLP in learning capability; and more importantly, our CNN model exhibits a\ncompetitive prediction accuracy with minimal constraints in a geometric\nrepresentation.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 00:05:31 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 21:30:11 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Zhang", "Yao", ""], ["Sung", "Woong-Je", ""], ["Mavris", "Dimitri", ""]]}, {"id": "1712.10087", "submitter": "Jason Klusowski M", "authors": "W. D. Brinda and Jason M. Klusowski", "title": "Finite-sample risk bounds for maximum likelihood estimation with\n  arbitrary penalties", "comments": "To appear in IEEE Transactions on Information Theory, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The MDL two-part coding $ \\textit{index of resolvability} $ provides a\nfinite-sample upper bound on the statistical risk of penalized likelihood\nestimators over countable models. However, the bound does not apply to\nunpenalized maximum likelihood estimation or procedures with exceedingly small\npenalties. In this paper, we point out a more general inequality that holds for\narbitrary penalties. In addition, this approach makes it possible to derive\nexact risk bounds of order $1/n$ for iid parametric models, which improves on\nthe order $(\\log n)/n$ resolvability bounds. We conclude by discussing\nimplications for adaptive estimation.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 01:15:49 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Brinda", "W. D.", ""], ["Klusowski", "Jason M.", ""]]}, {"id": "1712.10107", "submitter": "Saeedeh Ziyabari", "authors": "Saeedeh Ziyabari, Vinit Shah, Meysam Golmohammadi, Iyad Obeid and\n  Joseph Picone", "title": "Objective evaluation metrics for automatic classification of EEG events", "comments": "22 pages, 11 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evaluation of machine learning algorithms in biomedical fields for\napplications involving sequential data lacks standardization. Common\nquantitative scalar evaluation metrics such as sensitivity and specificity can\noften be misleading depending on the requirements of the application.\nEvaluation metrics must ultimately reflect the needs of users yet be\nsufficiently sensitive to guide algorithm development. Feedback from critical\ncare clinicians who use automated event detection software in clinical\napplications has been overwhelmingly emphatic that a low false alarm rate,\ntypically measured in units of the number of errors per 24 hours, is the single\nmost important criterion for user acceptance. Though using a single metric is\nnot often as insightful as examining performance over a range of operating\nconditions, there is a need for a single scalar figure of merit. In this paper,\nwe discuss the deficiencies of existing metrics for a seizure detection task\nand propose several new metrics that offer a more balanced view of performance.\nWe demonstrate these metrics on a seizure detection task based on the TUH EEG\nCorpus. We show that two promising metrics are a measure based on a concept\nborrowed from the spoken term detection literature, Actual Term-Weighted Value\n(ATWV), and a new metric, Time-Aligned Event Scoring (TAES), that accounts for\nthe temporal alignment of the hypothesis to the reference annotation. We also\ndemonstrate that state of the art technology based on deep learning, though\nimpressive in its performance, still needs significant improvement before it\nmeets very strict user acceptance criteria.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 03:36:46 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 07:03:06 GMT"}, {"version": "v3", "created": "Mon, 2 Dec 2019 17:27:35 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Ziyabari", "Saeedeh", ""], ["Shah", "Vinit", ""], ["Golmohammadi", "Meysam", ""], ["Obeid", "Iyad", ""], ["Picone", "Joseph", ""]]}, {"id": "1712.10110", "submitter": "Su Yan", "authors": "Su Yan, Wei Lin, Tianshu Wu, Daorui Xiao, Xu Zheng, Bo Wu, Kaipeng Liu", "title": "Beyond Keywords and Relevance: A Personalized Ad Retrieval Framework in\n  E-Commerce Sponsored Search", "comments": null, "journal-ref": "Proceedings of the 2018 World Wide Web Conference Pages 1919-1928", "doi": "10.1145/3178876.3186172", "report-no": null, "categories": "cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  On most sponsored search platforms, advertisers bid on some keywords for\ntheir advertisements (ads). Given a search request, ad retrieval module\nrewrites the query into bidding keywords, and uses these keywords as keys to\nselect Top N ads through inverted indexes. In this way, an ad will not be\nretrieved even if queries are related when the advertiser does not bid on\ncorresponding keywords. Moreover, most ad retrieval approaches regard rewriting\nand ad-selecting as two separated tasks, and focus on boosting relevance\nbetween search queries and ads. Recently, in e-commerce sponsored search more\nand more personalized information has been introduced, such as user profiles,\nlong-time and real-time clicks. Personalized information makes ad retrieval\nable to employ more elements (e.g. real-time clicks) as search signals and\nretrieval keys, however it makes ad retrieval more difficult to measure ads\nretrieved through different signals. To address these problems, we propose a\nnovel ad retrieval framework beyond keywords and relevance in e-commerce\nsponsored search. Firstly, we employ historical ad click data to initialize a\nhierarchical network representing signals, keys and ads, in which personalized\ninformation is introduced. Then we train a model on top of the hierarchical\nnetwork by learning the weights of edges. Finally we select the best edges\naccording to the model, boosting RPM/CTR. Experimental results on our\ne-commerce platform demonstrate that our ad retrieval framework achieves good\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 03:48:25 GMT"}, {"version": "v2", "created": "Mon, 1 Jan 2018 13:10:17 GMT"}, {"version": "v3", "created": "Thu, 8 Mar 2018 08:10:56 GMT"}, {"version": "v4", "created": "Mon, 9 Apr 2018 02:56:34 GMT"}, {"version": "v5", "created": "Mon, 23 Apr 2018 18:32:58 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Yan", "Su", ""], ["Lin", "Wei", ""], ["Wu", "Tianshu", ""], ["Xiao", "Daorui", ""], ["Zheng", "Xu", ""], ["Wu", "Bo", ""], ["Liu", "Kaipeng", ""]]}, {"id": "1712.10158", "submitter": "Aditya Gilra", "authors": "Aditya Gilra and Wulfram Gerstner", "title": "Non-linear motor control by local learning in spiking neural networks", "comments": null, "journal-ref": "Proceedings of the 35th International Conference on Machine\n  Learning, PMLR 80:1773-1782, 2018", "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG cs.NE cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning weights in a spiking neural network with hidden neurons, using\nlocal, stable and online rules, to control non-linear body dynamics is an open\nproblem. Here, we employ a supervised scheme, Feedback-based Online Local\nLearning Of Weights (FOLLOW), to train a network of heterogeneous spiking\nneurons with hidden layers, to control a two-link arm so as to reproduce a\ndesired state trajectory. The network first learns an inverse model of the\nnon-linear dynamics, i.e. from state trajectory as input to the network, it\nlearns to infer the continuous-time command that produced the trajectory.\nConnection weights are adjusted via a local plasticity rule that involves\npre-synaptic firing and post-synaptic feedback of the error in the inferred\ncommand. We choose a network architecture, termed differential feedforward,\nthat gives the lowest test error from different feedforward and recurrent\narchitectures. The learned inverse model is then used to generate a\ncontinuous-time motor command to control the arm, given a desired trajectory.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 09:21:34 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Gilra", "Aditya", ""], ["Gerstner", "Wulfram", ""]]}, {"id": "1712.10248", "submitter": "Jong Chul Ye", "authors": "Yoseob Han, Jawook Gu, Jong Chul Ye", "title": "Deep Learning Interior Tomography for Region-of-Interest Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interior tomography for the region-of-interest (ROI) imaging has advantages\nof using a small detector and reducing X-ray radiation dose. However, standard\nanalytic reconstruction suffers from severe cupping artifacts due to existence\nof null space in the truncated Radon transform. Existing penalized\nreconstruction methods may address this problem but they require extensive\ncomputations due to the iterative reconstruction. Inspired by the recent deep\nlearning approaches to low-dose and sparse view CT, here we propose a deep\nlearning architecture that removes null space signals from the FBP\nreconstruction. Experimental results have shown that the proposed method\nprovides near-perfect reconstruction with about 7-10 dB improvement in PSNR\nover existing methods in spite of significantly reduced run-time complexity.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 14:59:41 GMT"}, {"version": "v2", "created": "Wed, 3 Jan 2018 15:54:24 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Han", "Yoseob", ""], ["Gu", "Jawook", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1712.10277", "submitter": "Frank E. Curtis", "authors": "Frank E. Curtis, Katya Scheinberg, Rui Shi", "title": "A Stochastic Trust Region Algorithm Based on Careful Step Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": "Lehigh ISE/COR@L Technical Report 17T-017", "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm is proposed for solving stochastic and finite sum minimization\nproblems. Based on a trust region methodology, the algorithm employs normalized\nsteps, at least as long as the norms of the stochastic gradient estimates are\nwithin a specified interval. The complete algorithm---which dynamically chooses\nwhether or not to employ normalized steps---is proved to have convergence\nguarantees that are similar to those possessed by a traditional stochastic\ngradient approach under various sets of conditions related to the accuracy of\nthe stochastic gradient estimates and choice of stepsize sequence. The results\nof numerical experiments are presented when the method is employed to minimize\nconvex and nonconvex machine learning test problems. These results illustrate\nthat the method can outperform a traditional stochastic gradient approach.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 16:59:25 GMT"}, {"version": "v2", "created": "Sat, 27 Jan 2018 16:19:08 GMT"}, {"version": "v3", "created": "Tue, 26 Jun 2018 16:14:56 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Curtis", "Frank E.", ""], ["Scheinberg", "Katya", ""], ["Shi", "Rui", ""]]}, {"id": "1712.10321", "submitter": "Michela Paganini", "authors": "Michela Paganini, Luke de Oliveira, Benjamin Nachman", "title": "CaloGAN: Simulating 3D High Energy Particle Showers in Multi-Layer\n  Electromagnetic Calorimeters with Generative Adversarial Networks", "comments": "14 pages, 4 tables, 13 figures; version accepted by Physical Review D\n  (PRD)", "journal-ref": "Phys. Rev. D 97, 014021 (2018)", "doi": "10.1103/PhysRevD.97.014021", "report-no": null, "categories": "hep-ex cs.LG hep-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The precise modeling of subatomic particle interactions and propagation\nthrough matter is paramount for the advancement of nuclear and particle physics\nsearches and precision measurements. The most computationally expensive step in\nthe simulation pipeline of a typical experiment at the Large Hadron Collider\n(LHC) is the detailed modeling of the full complexity of physics processes that\ngovern the motion and evolution of particle showers inside calorimeters. We\nintroduce \\textsc{CaloGAN}, a new fast simulation technique based on generative\nadversarial networks (GANs). We apply these neural networks to the modeling of\nelectromagnetic showers in a longitudinally segmented calorimeter, and achieve\nspeedup factors comparable to or better than existing full simulation\ntechniques on CPU ($100\\times$-$1000\\times$) and even faster on GPU (up to\n$\\sim10^5\\times$). There are still challenges for achieving precision across\nthe entire phase space, but our solution can reproduce a variety of geometric\nshower shape properties of photons, positrons and charged pions. This\nrepresents a significant stepping stone toward a full neural network-based\ndetector simulation that could save significant computing time and enable many\nanalyses now and in the future.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 22:28:53 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Paganini", "Michela", ""], ["de Oliveira", "Luke", ""], ["Nachman", "Benjamin", ""]]}]