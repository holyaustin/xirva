[{"id": "0710.0508", "submitter": "Hui Zou", "authors": "Seongho Wu, Hui Zou, Ming Yuan", "title": "Structured variable selection in support vector machines", "comments": "Published in at http://dx.doi.org/10.1214/07-EJS125 the Electronic\n  Journal of Statistics (http://www.i-journals.org/ejs/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Electronic Journal of Statistics 2008, Vol. 2, 103-117", "doi": "10.1214/07-EJS125", "report-no": "IMS-EJS-EJS_2007_125", "categories": "stat.ML math.ST stat.TH", "license": null, "abstract": "  When applying the support vector machine (SVM) to high-dimensional\nclassification problems, we often impose a sparse structure in the SVM to\neliminate the influences of the irrelevant predictors. The lasso and other\nvariable selection techniques have been successfully used in the SVM to perform\nautomatic variable selection. In some problems, there is a natural hierarchical\nstructure among the variables. Thus, in order to have an interpretable SVM\nclassifier, it is important to respect the heredity principle when enforcing\nthe sparsity in the SVM. Many variable selection methods, however, do not\nrespect the heredity principle. In this paper we enforce both sparsity and the\nheredity principle in the SVM by using the so-called structured variable\nselection (SVS) framework originally proposed in Yuan, Joseph and Zou (2007).\nWe minimize the empirical hinge loss under a set of linear inequality\nconstraints and a lasso-type penalty. The solution always obeys the desired\nheredity principle and enjoys sparsity. The new SVM classifier can be\nefficiently fitted, because the optimization problem is a linear program.\nAnother contribution of this work is to present a nonparametric extension of\nthe SVS framework, and we propose nonparametric heredity SVMs. Simulated and\nreal data are used to illustrate the merits of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2007 11:58:55 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2008 13:34:37 GMT"}], "update_date": "2008-02-22", "authors_parsed": [["Wu", "Seongho", ""], ["Zou", "Hui", ""], ["Yuan", "Ming", ""]]}, {"id": "0710.0845", "submitter": "David Blei", "authors": "David M. Blei, Thomas L. Griffiths, Michael I. Jordan", "title": "The nested Chinese restaurant process and Bayesian nonparametric\n  inference of topic hierarchies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the nested Chinese restaurant process (nCRP), a stochastic process\nwhich assigns probability distributions to infinitely-deep,\ninfinitely-branching trees. We show how this stochastic process can be used as\na prior distribution in a Bayesian nonparametric model of document collections.\nSpecifically, we present an application to information retrieval in which\ndocuments are modeled as paths down a random tree, and the preferential\nattachment dynamics of the nCRP leads to clustering of documents according to\nsharing of topics at multiple levels of abstraction. Given a corpus of\ndocuments, a posterior inference algorithm finds an approximation to a\nposterior distribution over trees, topics and allocations of words to levels of\nthe tree. We demonstrate this algorithm on collections of scientific abstracts\nfrom several journals. This model exemplifies a recent trend in statistical\nmachine learning--the use of Bayesian nonparametric methods to infer\ndistributions on flexible data structures.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2007 17:32:21 GMT"}, {"version": "v2", "created": "Wed, 19 Mar 2008 20:04:56 GMT"}, {"version": "v3", "created": "Thu, 27 Aug 2009 16:32:49 GMT"}], "update_date": "2009-08-27", "authors_parsed": [["Blei", "David M.", ""], ["Griffiths", "Thomas L.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "0710.1203", "submitter": "Dimitri Petritis", "authors": "Thomas Sierocinski (IRMAR), Anthony Le B\\'echec, Nathalie Th\\'eret,\n  Dimitri Petritis (IRMAR)", "title": "Semantic distillation: a method for clustering objects by their\n  contextual specificity", "comments": "Accepted for publication in Studies in Computational Intelligence,\n  Springer-Verlag", "journal-ref": null, "doi": null, "report-no": "2007-58", "categories": "math.PR cs.DB math.ST q-bio.QM stat.ML stat.TH", "license": null, "abstract": "  Techniques for data-mining, latent semantic analysis, contextual search of\ndatabases, etc. have long ago been developed by computer scientists working on\ninformation retrieval (IR). Experimental scientists, from all disciplines,\nhaving to analyse large collections of raw experimental data (astronomical,\nphysical, biological, etc.) have developed powerful methods for their\nstatistical analysis and for clustering, categorising, and classifying objects.\nFinally, physicists have developed a theory of quantum measurement, unifying\nthe logical, algebraic, and probabilistic aspects of queries into a single\nformalism. The purpose of this paper is twofold: first to show that when\nformulated at an abstract level, problems from IR, from statistical data\nanalysis, and from physical measurement theories are very similar and hence can\nprofitably be cross-fertilised, and, secondly, to propose a novel method of\nfuzzy hierarchical clustering, termed \\textit{semantic distillation} --\nstrongly inspired from the theory of quantum measurement --, we developed to\nanalyse raw data coming from various types of experiments on DNA arrays. We\nillustrate the method by analysing DNA arrays experiments and clustering the\ngenes of the array according to their specificity.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2007 12:30:43 GMT"}, {"version": "v2", "created": "Sat, 6 Oct 2007 09:57:46 GMT"}], "update_date": "2007-10-09", "authors_parsed": [["Sierocinski", "Thomas", "", "IRMAR"], ["B\u00e9chec", "Anthony Le", "", "IRMAR"], ["Th\u00e9ret", "Nathalie", "", "IRMAR"], ["Petritis", "Dimitri", "", "IRMAR"]]}, {"id": "0710.3183", "submitter": "Robert Seiringer", "authors": "Joel Predd, Robert Seiringer, Elliott H. Lieb, Daniel Osherson,\n  Vincent Poor, Sanjeev Kulkarni", "title": "Probabilistic coherence and proper scoring rules", "comments": "LaTeX2, 15 pages", "journal-ref": "IEEE T. Inform. Theory 55, 4786 (2009)", "doi": "10.1109/TIT.2009.2027573", "report-no": null, "categories": "stat.ML", "license": null, "abstract": "  We provide self-contained proof of a theorem relating probabilistic coherence\nof forecasts to their non-domination by rival forecasts with respect to any\nproper scoring rule. The theorem appears to be new but is closely related to\nresults achieved by other investigators.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2007 21:16:29 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Predd", "Joel", ""], ["Seiringer", "Robert", ""], ["Lieb", "Elliott H.", ""], ["Osherson", "Daniel", ""], ["Poor", "Vincent", ""], ["Kulkarni", "Sanjeev", ""]]}, {"id": "0710.3742", "submitter": "Ryan Adams", "authors": "Ryan Prescott Adams, David J.C. MacKay", "title": "Bayesian Online Changepoint Detection", "comments": "7 pages, 4 figures, latex", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": null, "abstract": "  Changepoints are abrupt variations in the generative parameters of a data\nsequence. Online detection of changepoints is useful in modelling and\nprediction of time series in application areas such as finance, biometrics, and\nrobotics. While frequentist methods have yielded online filtering and\nprediction techniques, most Bayesian papers have focused on the retrospective\nsegmentation problem. Here we examine the case where the model parameters\nbefore and after the changepoint are independent and we derive an online\nalgorithm for exact inference of the most recent changepoint. We compute the\nprobability distribution of the length of the current ``run,'' or time since\nthe last changepoint, using a simple message-passing algorithm. Our\nimplementation is highly modular so that the algorithm may be applied to a\nvariety of types of data. We illustrate this modularity by demonstrating the\nalgorithm on three different real-world data sets.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2007 17:18:30 GMT"}], "update_date": "2007-10-22", "authors_parsed": [["Adams", "Ryan Prescott", ""], ["MacKay", "David J. C.", ""]]}, {"id": "0710.4516", "submitter": "Thomas Sch\\\"urmann", "authors": "Thomas Sch\\\"urmann and Peter Grassberger", "title": "The predictability of letters in written english", "comments": "3 pages, 4 figures", "journal-ref": "Fractals, Vol. 4, No. 1 (1996) 1-5", "doi": "10.1142/S0218348X96000029", "report-no": null, "categories": "physics.soc-ph cs.CL stat.ML", "license": null, "abstract": "  We show that the predictability of letters in written English texts depends\nstrongly on their position in the word. The first letters are usually the least\neasy to predict. This agrees with the intuitive notion that words are well\ndefined subunits in written languages, with much weaker correlations across\nthese units than within them. It implies that the average entropy of a letter\ndeep inside a word is roughly 4 times smaller than the entropy of the first\nletter.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2007 17:23:13 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2007 16:46:30 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Sch\u00fcrmann", "Thomas", ""], ["Grassberger", "Peter", ""]]}, {"id": "0710.5896", "submitter": "Chien-Yu Chen", "authors": "Yen-Jen Oyang, Chien-Yu Chen, Darby Tien-Hao Chang, and Chih-Peng Wu", "title": "Supervised Machine Learning with a Novel Pointwise Density Estimator", "comments": "Inclusion of a new \"Remarks\" section", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": null, "abstract": "  This article proposes a novel density estimation based algorithm for carrying\nout supervised machine learning. The proposed algorithm features O(n) time\ncomplexity for generating a classifier, where n is the number of sampling\ninstances in the training dataset. This feature is highly desirable in\ncontemporary applications that involve large and still growing databases. In\ncomparison with the kernel density estimation based approaches, the\nmathe-matical fundamental behind the proposed algorithm is not based on the\nassump-tion that the number of training instances approaches infinite. As a\nresult, a classifier generated with the proposed algorithm may deliver higher\nprediction accuracy than the kernel density estimation based classifier in some\ncases.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2007 16:00:08 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2007 12:06:48 GMT"}], "update_date": "2007-11-06", "authors_parsed": [["Oyang", "Yen-Jen", ""], ["Chen", "Chien-Yu", ""], ["Chang", "Darby Tien-Hao", ""], ["Wu", "Chih-Peng", ""]]}]