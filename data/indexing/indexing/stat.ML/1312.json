[{"id": "1312.0162", "submitter": "Dmitry Ignatov", "authors": "Anastasia Bezzubtseva and Dmitry I. Ignatov", "title": "A Typology of Collaboration Platform Users", "comments": null, "journal-ref": "R. Tagiew et al. (Eds.) Proc. of Int. Workshop on Experimental\n  Economics in Machine Learning 2012. Published by KU-Leuven, ISBN\n  978-9-08-140992-6, pp. 9-19", "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a review of the existing typologies of Internet\nservice users. We zoom in on social networking services including blogs and\ncrowdsourcing websites. Based on the results of the analysis of the considered\ntypologies obtained by means of FCA we developed a new user typology of a\ncertain class of Internet services, namely a collaboration innovation platform.\nCluster analysis of data extracted from the collaboration platform Witology was\nused to divide more than 500 participants into six groups based on three\nactivity indicators: idea generation, commenting, and evaluation (assigning\nmarks) The obtained groups and their percentages appear to follow the \"90 - 9 -\n1\" rule.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2013 23:58:32 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Bezzubtseva", "Anastasia", ""], ["Ignatov", "Dmitry I.", ""]]}, {"id": "1312.0232", "submitter": "Hemant Tyagi", "authors": "Hemant Tyagi, Sebastian Stich and Bernd G\\\"artner", "title": "Stochastic continuum armed bandit problem of few linear parameters in\n  high dimensions", "comments": "Changes from previous version: (a) Corrected typos throughout. (b) In\n  earlier version, regret was defined as a conditional expectation (and hence\n  bounded w.h.p); this is changed to an expectation now resulting in minor\n  changes in statements of Lemma 1, Theorems 1,2 and Corollary 1. See Remark 1.\n  (c) Added Remark 3, and corrected statement of Proposition 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a stochastic continuum armed bandit problem where the arms are\nindexed by the $\\ell_2$ ball $B_{d}(1+\\nu)$ of radius $1+\\nu$ in\n$\\mathbb{R}^d$. The reward functions $r :B_{d}(1+\\nu) \\rightarrow \\mathbb{R}$\nare considered to intrinsically depend on $k \\ll d$ unknown linear parameters\nso that $r(\\mathbf{x}) = g(\\mathbf{A} \\mathbf{x})$ where $\\mathbf{A}$ is a full\nrank $k \\times d$ matrix. Assuming the mean reward function to be smooth we\nmake use of results from low-rank matrix recovery literature and derive an\nefficient randomized algorithm which achieves a regret bound of $O(C(k,d)\nn^{\\frac{1+k}{2+k}} (\\log n)^{\\frac{1}{2+k}})$ with high probability. Here\n$C(k,d)$ is at most polynomial in $d$ and $k$ and $n$ is the number of rounds\nor the sampling budget which is assumed to be known beforehand.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2013 15:16:25 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2014 16:17:59 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2015 12:14:55 GMT"}, {"version": "v4", "created": "Tue, 30 May 2017 13:19:17 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Tyagi", "Hemant", ""], ["Stich", "Sebastian", ""], ["G\u00e4rtner", "Bernd", ""]]}, {"id": "1312.0286", "submitter": "William L Hamilton", "authors": "William L. Hamilton, Mahdi Milani Fard, and Joelle Pineau", "title": "Efficient Learning and Planning with Compressed Predictive States", "comments": "45 pages, 10 figures, submitted to the Journal of Machine Learning\n  Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive state representations (PSRs) offer an expressive framework for\nmodelling partially observable systems. By compactly representing systems as\nfunctions of observable quantities, the PSR learning approach avoids using\nlocal-minima prone expectation-maximization and instead employs a globally\noptimal moment-based algorithm. Moreover, since PSRs do not require a\npredetermined latent state structure as an input, they offer an attractive\nframework for model-based reinforcement learning when agents must plan without\na priori access to a system model. Unfortunately, the expressiveness of PSRs\ncomes with significant computational cost, and this cost is a major factor\ninhibiting the use of PSRs in applications. In order to alleviate this\nshortcoming, we introduce the notion of compressed PSRs (CPSRs). The CPSR\nlearning approach combines recent advancements in dimensionality reduction,\nincremental matrix decomposition, and compressed sensing. We show how this\napproach provides a principled avenue for learning accurate approximations of\nPSRs, drastically reducing the computational costs associated with learning\nwhile also providing effective regularization. Going further, we propose a\nplanning framework which exploits these learned models. And we show that this\napproach facilitates model-learning and planning in large complex partially\nobservable domains, a task that is infeasible without the principled use of\ncompression.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2013 23:17:06 GMT"}, {"version": "v2", "created": "Sun, 20 Jul 2014 20:16:44 GMT"}], "update_date": "2014-07-22", "authors_parsed": [["Hamilton", "William L.", ""], ["Fard", "Mahdi Milani", ""], ["Pineau", "Joelle", ""]]}, {"id": "1312.0335", "submitter": "Ali Shojaie", "authors": "Ali Shojaie, Alexandra Jauhiainen, Michael Kallitsis and George\n  Michailidis", "title": "Inferring Regulatory Networks by Combining Perturbation Screens and\n  Steady State Gene Expression Profiles", "comments": "24 pages, 4 figures, 6 tables", "journal-ref": null, "doi": "10.1371/journal.pone.0082393", "report-no": null, "categories": "stat.ML q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing transcriptional regulatory networks is an important task in\nfunctional genomics. Data obtained from experiments that perturb genes by\nknockouts or RNA interference contain useful information for addressing this\nreconstruction problem. However, such data can be limited in size and/or are\nexpensive to acquire. On the other hand, observational data of the organism in\nsteady state (e.g. wild-type) are more readily available, but their\ninformational content is inadequate for the task at hand. We develop a\ncomputational approach to appropriately utilize both data sources for\nestimating a regulatory network. The proposed approach is based on a three-step\nalgorithm to estimate the underlying directed but cyclic network, that uses as\ninput both perturbation screens and steady state gene expression data. In the\nfirst step, the algorithm determines causal orderings of the genes that are\nconsistent with the perturbation data, by combining an exhaustive search method\nwith a fast heuristic that in turn couples a Monte Carlo technique with a fast\nsearch algorithm. In the second step, for each obtained causal ordering, a\nregulatory network is estimated using a penalized likelihood based method,\nwhile in the third step a consensus network is constructed from the highest\nscored ones. Extensive computational experiments show that the algorithm\nperforms well in reconstructing the underlying network and clearly outperforms\ncompeting approaches that rely only on a single data source. Further, it is\nestablished that the algorithm produces a consistent estimate of the regulatory\nnetwork.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 06:00:55 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Shojaie", "Ali", ""], ["Jauhiainen", "Alexandra", ""], ["Kallitsis", "Michael", ""], ["Michailidis", "George", ""]]}, {"id": "1312.0365", "submitter": "Dirk Tasche", "authors": "Dirk Tasche", "title": "The Law of Total Odds", "comments": "12 pages, 1 figure, new references", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The law of total probability may be deployed in binary classification\nexercises to estimate the unconditional class probabilities if the class\nproportions in the training set are not representative of the population class\nproportions. We argue that this is not a conceptually sound approach and\nsuggest an alternative based on the new law of total odds. We quantify the bias\nof the total probability estimator of the unconditional class probabilities and\nshow that the total odds estimator is unbiased. The sample version of the total\nodds estimator is shown to coincide with a maximum-likelihood estimator known\nfrom the literature. The law of total odds can also be used for transforming\nthe conditional class probabilities if independent estimates of the\nunconditional class probabilities of the population are available.\n  Keywords: Total probability, likelihood ratio, Bayes' formula, binary\nclassification, relative odds, unbiased estimator, supervised learning, dataset\nshift.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 07:54:15 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2013 19:10:11 GMT"}, {"version": "v3", "created": "Sun, 29 Dec 2013 19:01:11 GMT"}, {"version": "v4", "created": "Sat, 18 Jan 2014 22:36:12 GMT"}, {"version": "v5", "created": "Fri, 14 Feb 2014 17:54:41 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Tasche", "Dirk", ""]]}, {"id": "1312.0451", "submitter": "Aryeh Kontorovich", "authors": "Daniel Berend and Aryeh Kontorovich", "title": "Consistency of weighted majority votes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the classical decision-theoretic problem of weighted expert voting\nfrom a statistical learning perspective. In particular, we examine the\nconsistency (both asymptotic and finitary) of the optimal Nitzan-Paroush\nweighted majority and related rules. In the case of known expert competence\nlevels, we give sharp error estimates for the optimal rule. When the competence\nlevels are unknown, they must be empirically estimated. We provide frequentist\nand Bayesian analyses for this situation. Some of our proof techniques are\nnon-standard and may be of independent interest. The bounds we derive are\nnearly optimal, and several challenging open problems are posed. Experimental\nresults are provided to illustrate the theory.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 13:41:44 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2013 13:02:17 GMT"}, {"version": "v3", "created": "Mon, 9 Dec 2013 17:13:01 GMT"}, {"version": "v4", "created": "Sun, 22 Dec 2013 11:23:48 GMT"}, {"version": "v5", "created": "Tue, 21 Jan 2014 08:24:07 GMT"}], "update_date": "2014-01-22", "authors_parsed": [["Berend", "Daniel", ""], ["Kontorovich", "Aryeh", ""]]}, {"id": "1312.0485", "submitter": "Weiyu Xu", "authors": "Weiyu Xu, Jian-Feng Cai, Kumar Vijay Mishra, Myung Cho, and Anton\n  Kruger", "title": "Precise Semidefinite Programming Formulation of Atomic Norm Minimization\n  for Recovering d-Dimensional ($d\\geq 2$) Off-the-Grid Frequencies", "comments": "4 pages, double-column,1 Figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research in off-the-grid compressed sensing (CS) has demonstrated\nthat, under certain conditions, one can successfully recover a spectrally\nsparse signal from a few time-domain samples even though the dictionary is\ncontinuous. In particular, atomic norm minimization was proposed in\n\\cite{tang2012csotg} to recover $1$-dimensional spectrally sparse signal.\nHowever, in spite of existing research efforts \\cite{chi2013compressive}, it\nwas still an open problem how to formulate an equivalent positive semidefinite\nprogram for atomic norm minimization in recovering signals with $d$-dimensional\n($d\\geq 2$) off-the-grid frequencies. In this paper, we settle this problem by\nproposing equivalent semidefinite programming formulations of atomic norm\nminimization to recover signals with $d$-dimensional ($d\\geq 2$) off-the-grid\nfrequencies.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 15:14:57 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Xu", "Weiyu", ""], ["Cai", "Jian-Feng", ""], ["Mishra", "Kumar Vijay", ""], ["Cho", "Myung", ""], ["Kruger", "Anton", ""]]}, {"id": "1312.0493", "submitter": "Ozan \\.Irsoy", "authors": "Ozan \\.Irsoy, Claire Cardie", "title": "Bidirectional Recursive Neural Networks for Token-Level Labeling with\n  Structure", "comments": "9 pages, 5 figures, NIPS Deep Learning Workshop 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep architectures, such as recurrent and recursive neural networks\nhave been successfully applied to various natural language processing tasks.\nInspired by bidirectional recurrent neural networks which use representations\nthat summarize the past and future around an instance, we propose a novel\narchitecture that aims to capture the structural information around an input,\nand use it to label instances. We apply our method to the task of opinion\nexpression extraction, where we employ the binary parse tree of a sentence as\nthe structure, and word vector representations as the initial representation of\na single token. We conduct preliminary experiments to investigate its\nperformance and compare it to the sequential approach.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 15:54:40 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["\u0130rsoy", "Ozan", ""], ["Cardie", "Claire", ""]]}, {"id": "1312.0516", "submitter": "Vassilis Kekatos", "authors": "Vassilis Kekatos, Georgios B. Giannakis, Ross Baldick", "title": "Grid Topology Identification using Electricity Prices", "comments": "PES General Meeting 2014 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The potential of recovering the topology of a grid using solely publicly\navailable market data is explored here. In contemporary whole-sale electricity\nmarkets, real-time prices are typically determined by solving the\nnetwork-constrained economic dispatch problem. Under a linear DC model,\nlocational marginal prices (LMPs) correspond to the Lagrange multipliers of the\nlinear program involved. The interesting observation here is that the matrix of\nspatiotemporally varying LMPs exhibits the following property: Once\npremultiplied by the weighted grid Laplacian, it yields a low-rank and sparse\nmatrix. Leveraging this rich structure, a regularized maximum likelihood\nestimator (MLE) is developed to recover the grid Laplacian from the LMPs. The\nconvex optimization problem formulated includes low rank- and\nsparsity-promoting regularizers, and it is solved using a scalable algorithm.\nNumerical tests on prices generated for the IEEE 14-bus benchmark provide\nencouraging topology recovery results.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 16:58:10 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2014 00:35:43 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Kekatos", "Vassilis", ""], ["Giannakis", "Georgios B.", ""], ["Baldick", "Ross", ""]]}, {"id": "1312.0518", "submitter": "Utkarsh Dang", "authors": "Utkarsh J. Dang and Paul D. McNicholas", "title": "Families of Parsimonious Finite Mixtures of Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite mixtures of regression models offer a flexible framework for\ninvestigating heterogeneity in data with functional dependencies. These models\ncan be conveniently used for unsupervised learning on data with clear\nregression relationships. We extend such models by imposing an\neigen-decomposition on the multivariate error covariance matrix. By\nconstraining parts of this decomposition, we obtain families of parsimonious\nmixtures of regressions and mixtures of regressions with concomitant variables.\nThese families of models account for correlations between multiple responses.\nAn expectation-maximization algorithm is presented for parameter estimation and\nperformance is illustrated on simulated and real data.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 17:08:03 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Dang", "Utkarsh J.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1312.0624", "submitter": "Uri Shalit", "authors": "Uri Shalit and Gal Chechik", "title": "Efficient coordinate-descent for orthogonal matrices through Givens\n  rotations", "comments": "A shorter version of this paper will appear in the proceedings of the\n  31st International Conference for Machine Learning (ICML 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing over the set of orthogonal matrices is a central component in\nproblems like sparse-PCA or tensor decomposition. Unfortunately, such\noptimization is hard since simple operations on orthogonal matrices easily\nbreak orthogonality, and correcting orthogonality usually costs a large amount\nof computation. Here we propose a framework for optimizing orthogonal matrices,\nthat is the parallel of coordinate-descent in Euclidean spaces. It is based on\n{\\em Givens-rotations}, a fast-to-compute operation that affects a small number\nof entries in the learned matrix, and preserves orthogonality. We show two\napplications of this approach: an algorithm for tensor decomposition that is\nused in learning mixture models, and an algorithm for sparse-PCA. We study the\nparameter regime where a Givens rotation approach converges faster and achieves\na superior model on a genome-wide brain-wide mRNA expression dataset.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 21:09:40 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2013 18:47:20 GMT"}], "update_date": "2013-12-16", "authors_parsed": [["Shalit", "Uri", ""], ["Chechik", "Gal", ""]]}, {"id": "1312.0631", "submitter": "Aram Galstyan", "authors": "Greg Ver Steeg, Cristopher Moore, Aram Galstyan, and Armen E.\n  Allahverdyan", "title": "Phase Transitions in Community Detection: A Solvable Toy Model", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": "10.1209/0295-5075/106/48004", "report-no": null, "categories": "cs.SI cond-mat.stat-mech physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, it was shown that there is a phase transition in the community\ndetection problem. This transition was first computed using the cavity method,\nand has been proved rigorously in the case of $q=2$ groups. However, analytic\ncalculations using the cavity method are challenging since they require us to\nunderstand probability distributions of messages. We study analogous\ntransitions in so-called \"zero-temperature inference\" model, where this\ndistribution is supported only on the most-likely messages. Furthermore,\nwhenever several messages are equally likely, we break the tie by choosing\namong them with equal probability. While the resulting analysis does not give\nthe correct values of the thresholds, it does reproduce some of the qualitative\nfeatures of the system. It predicts a first-order detectability transition\nwhenever $q > 2$, while the finite-temperature cavity method shows that this is\nthe case only when $q > 4$. It also has a regime analogous to the \"hard but\ndetectable\" phase, where the community structure can be partially recovered,\nbut only when the initial messages are sufficiently accurate. Finally, we study\na semisupervised setting where we are given the correct labels for a fraction\n$\\rho$ of the nodes. For $q > 2$, we find a regime where the accuracy jumps\ndiscontinuously at a critical value of $\\rho$.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 21:18:53 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Steeg", "Greg Ver", ""], ["Moore", "Cristopher", ""], ["Galstyan", "Aram", ""], ["Allahverdyan", "Armen E.", ""]]}, {"id": "1312.0712", "submitter": "Xiao-Feng Gong", "authors": "Xiao-Feng Gong, Xiu-Lin Wang, Qiu-Hua Lin", "title": "Generalized Non-orthogonal Joint Diagonalization with LU Decomposition\n  and Successive Rotations", "comments": "Signal Processing, IEEE Transactions on (Volume:63 , Issue: 5 )", "journal-ref": null, "doi": "10.1109/TSP.2015.2391074", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-orthogonal joint diagonalization (NJD) free of prewhitening has been\nwidely studied in the context of blind source separation (BSS) and array signal\nprocessing, etc. However, NJD is used to retrieve the jointly diagonalizable\nstructure for a single set of target matrices which are mostly formulized with\na single dataset, and thus is insufficient to handle multiple datasets with\ninter-set dependences, a scenario often encountered in joint BSS (J-BSS)\napplications. As such, we present a generalized NJD (GNJD) algorithm to\nsimultaneously perform asymmetric NJD upon multiple sets of target matrices\nwith mutually linked loading matrices, by using LU decomposition and successive\nrotations, to enable J-BSS over multiple datasets with indication/exploitation\nof their mutual dependences. Experiments with synthetic and real-world datasets\nare provided to illustrate the performance of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2013 06:44:52 GMT"}, {"version": "v2", "created": "Mon, 1 Sep 2014 07:57:26 GMT"}, {"version": "v3", "created": "Thu, 12 Feb 2015 16:23:23 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Gong", "Xiao-Feng", ""], ["Wang", "Xiu-Lin", ""], ["Lin", "Qiu-Hua", ""]]}, {"id": "1312.0790", "submitter": "Sneha Chaudhari", "authors": "Sneha Chaudhari, Pankaj Dayama, Vinayaka Pandit, Indrajit Bhattacharya", "title": "Test Set Selection using Active Information Acquisition for Predictive\n  Models", "comments": "The paper has been withdrawn by the authors. The current version is\n  incomplete and the work is still on going. The algorithm gives poor results\n  for a particular setting and we are working on it. However, we are not\n  planning to submit a revision of the paper. This work is going to take some\n  time and we want to withdraw the current version since it is not in a good\n  shape and needs a lot more work to be in publishable condition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider active information acquisition when the prediction\nmodel is meant to be applied on a targeted subset of the population. The goal\nis to label a pre-specified fraction of customers in the target or test set by\niteratively querying for information from the non-target or training set. The\nnumber of queries is limited by an overall budget. Arising in the context of\ntwo rather disparate applications- banking and medical diagnosis, we pose the\nactive information acquisition problem as a constrained optimization problem.\nWe propose two greedy iterative algorithms for solving the above problem. We\nconduct experiments with synthetic data and compare results of our proposed\nalgorithms with few other baseline approaches. The experimental results show\nthat our proposed approaches perform better than the baseline schemes.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2013 12:12:23 GMT"}, {"version": "v2", "created": "Fri, 14 Mar 2014 16:36:36 GMT"}], "update_date": "2014-03-17", "authors_parsed": [["Chaudhari", "Sneha", ""], ["Dayama", "Pankaj", ""], ["Pandit", "Vinayaka", ""], ["Bhattacharya", "Indrajit", ""]]}, {"id": "1312.0925", "submitter": "Moritz Hardt", "authors": "Moritz Hardt", "title": "Understanding Alternating Minimization for Matrix Completion", "comments": "Slightly improved main theorem and a correction: The tail bound\n  stated in Lemma A.5 of the previous version is incorrect. See manuscript for\n  fix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alternating Minimization is a widely used and empirically successful\nheuristic for matrix completion and related low-rank optimization problems.\nTheoretical guarantees for Alternating Minimization have been hard to come by\nand are still poorly understood. This is in part because the heuristic is\niterative and non-convex in nature. We give a new algorithm based on\nAlternating Minimization that provably recovers an unknown low-rank matrix from\na random subsample of its entries under a standard incoherence assumption. Our\nresults reduce the sample size requirements of the Alternating Minimization\napproach by at least a quartic factor in the rank and the condition number of\nthe unknown matrix. These improvements apply even if the matrix is only close\nto low-rank in the Frobenius norm. Our algorithm runs in nearly linear time in\nthe dimension of the matrix and, in a broad range of parameters, gives the\nstrongest sample bounds among all subquadratic time algorithms that we are\naware of.\n  Underlying our work is a new robust convergence analysis of the well-known\nPower Method for computing the dominant singular vectors of a matrix. This\nviewpoint leads to a conceptually simple understanding of Alternating\nMinimization. In addition, we contribute a new technique for controlling the\ncoherence of intermediate solutions arising in iterative algorithms based on a\nsmoothed analysis of the QR factorization. These techniques may be of interest\nbeyond their application here.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2013 20:37:28 GMT"}, {"version": "v2", "created": "Wed, 9 Apr 2014 21:28:27 GMT"}, {"version": "v3", "created": "Wed, 14 May 2014 19:54:58 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Hardt", "Moritz", ""]]}, {"id": "1312.1099", "submitter": "Francesca Petralia", "authors": "Francesca Petralia, Joshua Vogelstein and David B. Dunson", "title": "Multiscale Dictionary Learning for Estimating Conditional Distributions", "comments": null, "journal-ref": "Proceeding of Neural Information Processing Systems, Lake Tahoe,\n  Nevada December 2013", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric estimation of the conditional distribution of a response given\nhigh-dimensional features is a challenging problem. It is important to allow\nnot only the mean but also the variance and shape of the response density to\nchange flexibly with features, which are massive-dimensional. We propose a\nmultiscale dictionary learning model, which expresses the conditional response\ndensity as a convex combination of dictionary densities, with the densities\nused and their weights dependent on the path through a tree decomposition of\nthe feature space. A fast graph partitioning algorithm is applied to obtain the\ntree decomposition, with Bayesian methods then used to adaptively prune and\naverage over different sub-trees in a soft probabilistic manner. The algorithm\nscales efficiently to approximately one million features. State of the art\npredictive performance is demonstrated for toy examples and two neuroscience\napplications including up to a million features.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2013 10:44:01 GMT"}], "update_date": "2013-12-05", "authors_parsed": [["Petralia", "Francesca", ""], ["Vogelstein", "Joshua", ""], ["Dunson", "David B.", ""]]}, {"id": "1312.1244", "submitter": "Vladimir Temlyakov", "authors": "Vladimir Temlyakov", "title": "Chebushev Greedy Algorithm in convex optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chebyshev Greedy Algorithm is a generalization of the well known Orthogonal\nMatching Pursuit defined in a Hilbert space to the case of Banach spaces. We\napply this algorithm for constructing sparse approximate solutions (with\nrespect to a given dictionary) to convex optimization problems. Rate of\nconvergence results in a style of the Lebesgue-type inequalities are proved.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2013 17:03:30 GMT"}], "update_date": "2013-12-05", "authors_parsed": [["Temlyakov", "Vladimir", ""]]}, {"id": "1312.1349", "submitter": "Torsten Ensslin", "authors": "Torsten A. En{\\ss}lin, Henrik Junklewitz, Lars Winderling, Maksim\n  Greiner, Marco Selig", "title": "Improving self-calibration", "comments": "17 pages, 3 figures, revised version, title changed", "journal-ref": null, "doi": "10.1103/PhysRevE.90.043301", "report-no": null, "categories": "astro-ph.IM cs.IT math.IT physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Response calibration is the process of inferring how much the measured data\ndepend on the signal one is interested in. It is essential for any quantitative\nsignal estimation on the basis of the data. Here, we investigate\nself-calibration methods for linear signal measurements and linear dependence\nof the response on the calibration parameters. The common practice is to\naugment an external calibration solution using a known reference signal with an\ninternal calibration on the unknown measurement signal itself. Contemporary\nself-calibration schemes try to find a self-consistent solution for signal and\ncalibration by exploiting redundancies in the measurements. This can be\nunderstood in terms of maximizing the joint probability of signal and\ncalibration. However, the full uncertainty structure of this joint probability\naround its maximum is thereby not taken into account by these schemes.\nTherefore better schemes -- in sense of minimal square error -- can be designed\nby accounting for asymmetries in the uncertainty of signal and calibration. We\nargue that at least a systematic correction of the common self-calibration\nscheme should be applied in many measurement situations in order to properly\ntreat uncertainties of the signal on which one calibrates. Otherwise the\ncalibration solutions suffer from a systematic bias, which consequently\ndistorts the signal reconstruction. Furthermore, we argue that non-parametric,\nsignal-to-noise filtered calibration should provide more accurate\nreconstructions than the common bin averages and provide a new, improved\nself-calibration scheme. We illustrate our findings with a simplistic numerical\nexample.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2013 21:00:22 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2013 14:51:16 GMT"}, {"version": "v3", "created": "Sat, 6 Sep 2014 15:00:28 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["En\u00dflin", "Torsten A.", ""], ["Junklewitz", "Henrik", ""], ["Winderling", "Lars", ""], ["Greiner", "Maksim", ""], ["Selig", "Marco", ""]]}, {"id": "1312.1522", "submitter": "Aleksandr Aravkin", "authors": "Dmitry Malioutov, Aleksandr Aravkin", "title": "Iterative Log Thresholding", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse reconstruction approaches using the re-weighted l1-penalty have been\nshown, both empirically and theoretically, to provide a significant improvement\nin recovering sparse signals in comparison to the l1-relaxation. However,\nnumerical optimization of such penalties involves solving problems with\nl1-norms in the objective many times. Using the direct link of reweighted\nl1-penalties to the concave log-regularizer for sparsity, we derive a simple\nprox-like algorithm for the log-regularized formulation. The proximal splitting\nstep of the algorithm has a closed form solution, and we call the algorithm\n'log-thresholding' in analogy to soft thresholding for the l1-penalty.\n  We establish convergence results, and demonstrate that log-thresholding\nprovides more accurate sparse reconstructions compared to both soft and hard\nthresholding. Furthermore, the approach can be directly extended to\noptimization over matrices with penalty for rank (i.e. the nuclear norm penalty\nand its re-weigthed version), where we suggest a singular-value\nlog-thresholding approach.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2013 12:40:06 GMT"}], "update_date": "2013-12-06", "authors_parsed": [["Malioutov", "Dmitry", ""], ["Aravkin", "Aleksandr", ""]]}, {"id": "1312.1613", "submitter": "Jim Wang J-Y", "authors": "Jim Jing-Yan Wang", "title": "Max-Min Distance Nonnegative Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative Matrix Factorization (NMF) has been a popular representation\nmethod for pattern classification problem. It tries to decompose a nonnegative\nmatrix of data samples as the product of a nonnegative basic matrix and a\nnonnegative coefficient matrix, and the coefficient matrix is used as the new\nrepresentation. However, traditional NMF methods ignore the class labels of the\ndata samples. In this paper, we proposed a supervised novel NMF algorithm to\nimprove the discriminative ability of the new representation. Using the class\nlabels, we separate all the data sample pairs into within-class pairs and\nbetween-class pairs. To improve the discriminate ability of the new NMF\nrepresentations, we hope that the maximum distance of the within-class pairs in\nthe new NMF space could be minimized, while the minimum distance of the\nbetween-class pairs pairs could be maximized. With this criterion, we construct\nan objective function and optimize it with regard to basic and coefficient\nmatrices and slack variables alternatively, resulting in a iterative algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2013 16:49:05 GMT"}], "update_date": "2013-12-06", "authors_parsed": [["Wang", "Jim Jing-Yan", ""]]}, {"id": "1312.1666", "submitter": "Jakub Kone\\v{c}n\\'y", "authors": "Jakub Kone\\v{c}n\\'y and Peter Richt\\'arik", "title": "Semi-Stochastic Gradient Descent Methods", "comments": "19 pages, 3 figures, 2 algorithms, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of minimizing the average of a large\nnumber ($n$) of smooth convex loss functions. We propose a new method, S2GD\n(Semi-Stochastic Gradient Descent), which runs for one or several epochs in\neach of which a single full gradient and a random number of stochastic\ngradients is computed, following a geometric law. The total work needed for the\nmethod to output an $\\varepsilon$-accurate solution in expectation, measured in\nthe number of passes over data, or equivalently, in units equivalent to the\ncomputation of a single gradient of the loss, is\n$O((\\kappa/n)\\log(1/\\varepsilon))$, where $\\kappa$ is the condition number.\nThis is achieved by running the method for $O(\\log(1/\\varepsilon))$ epochs,\nwith a single gradient evaluation and $O(\\kappa)$ stochastic gradient\nevaluations in each. The SVRG method of Johnson and Zhang arises as a special\ncase. If our method is limited to a single epoch only, it needs to evaluate at\nmost $O((\\kappa/\\varepsilon)\\log(1/\\varepsilon))$ stochastic gradients. In\ncontrast, SVRG requires $O(\\kappa/\\varepsilon^2)$ stochastic gradients. To\nillustrate our theoretical results, S2GD only needs the workload equivalent to\nabout 2.1 full gradient evaluations to find an $10^{-6}$-accurate solution for\na problem with $n=10^9$ and $\\kappa=10^3$.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2013 20:04:52 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2015 05:05:40 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Kone\u010dn\u00fd", "Jakub", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1312.1706", "submitter": "Divyanshu Vats", "authors": "Divyanshu Vats and Richard G. Baraniuk", "title": "Swapping Variables for High-Dimensional Sparse Regression with\n  Correlated Measurements", "comments": "Parts of this paper have appeared in NIPS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the high-dimensional sparse linear regression problem of\naccurately estimating a sparse vector using a small number of linear\nmeasurements that are contaminated by noise. It is well known that the standard\ncadre of computationally tractable sparse regression algorithms---such as the\nLasso, Orthogonal Matching Pursuit (OMP), and their extensions---perform poorly\nwhen the measurement matrix contains highly correlated columns. To address this\nshortcoming, we develop a simple greedy algorithm, called SWAP, that\niteratively swaps variables until convergence. SWAP is surprisingly effective\nin handling measurement matrices with high correlations. In fact, we prove that\nSWAP outputs the true support, the locations of the non-zero entries in the\nsparse vector, under a relatively mild condition on the measurement matrix.\nFurthermore, we show that SWAP can be used to boost the performance of any\nsparse regression algorithm. We empirically demonstrate the advantages of SWAP\nby comparing it with several state-of-the-art sparse regression algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2013 21:27:40 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2014 19:38:13 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Vats", "Divyanshu", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1312.1733", "submitter": "Antony Joseph", "authors": "Antony Joseph, Bin Yu", "title": "Impact of regularization on Spectral Clustering", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of spectral clustering can be considerably improved via\nregularization, as demonstrated empirically in Amini et. al (2012). Here, we\nprovide an attempt at quantifying this improvement through theoretical\nanalysis. Under the stochastic block model (SBM), and its extensions, previous\nresults on spectral clustering relied on the minimum degree of the graph being\nsufficiently large for its good performance. By examining the scenario where\nthe regularization parameter $\\tau$ is large we show that the minimum degree\nassumption can potentially be removed. As a special case, for an SBM with two\nblocks, the results require the maximum degree to be large (grow faster than\n$\\log n$) as opposed to the minimum degree.\n  More importantly, we show the usefulness of regularization in situations\nwhere not all nodes belong to well-defined clusters. Our results rely on a\n`bias-variance'-like trade-off that arises from understanding the concentration\nof the sample Laplacian and the eigen gap as a function of the regularization\nparameter. As a byproduct of our bounds, we propose a data-driven technique\n\\textit{DKest} (standing for estimated Davis-Kahan bounds) for choosing the\nregularization parameter. This technique is shown to work well through\nsimulations and on a real data set.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2013 23:31:20 GMT"}, {"version": "v2", "created": "Mon, 21 Jul 2014 04:43:03 GMT"}], "update_date": "2014-07-22", "authors_parsed": [["Joseph", "Antony", ""], ["Yu", "Bin", ""]]}, {"id": "1312.1903", "submitter": "Helen Ogden", "authors": "Helen Ogden", "title": "A sequential reduction method for inference in generalized linear mixed\n  models", "comments": "17 pages, 3 Figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The likelihood for the parameters of a generalized linear mixed model\ninvolves an integral which may be of very high dimension. Because of this\nintractability, many approximations to the likelihood have been proposed, but\nall can fail when the model is sparse, in that there is only a small amount of\ninformation available on each random effect. The sequential reduction method\ndescribed in this paper exploits the dependence structure of the posterior\ndistribution of the random effects to reduce substantially the cost of finding\nan accurate approximation to the likelihood in models with sparse structure.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2013 16:06:38 GMT"}, {"version": "v2", "created": "Fri, 29 Aug 2014 08:38:58 GMT"}], "update_date": "2014-09-01", "authors_parsed": [["Ogden", "Helen", ""]]}, {"id": "1312.1909", "submitter": "Qi Wang", "authors": "Qi Wang and Joseph JaJa", "title": "From Maxout to Channel-Out: Encoding Information on Sparse Pathways", "comments": "10 pages including the appendix, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by an important insight from neural science, we propose a new\nframework for understanding the success of the recently proposed \"maxout\"\nnetworks. The framework is based on encoding information on sparse pathways and\nrecognizing the correct pathway at inference time. Elaborating further on this\ninsight, we propose a novel deep network architecture, called \"channel-out\"\nnetwork, which takes a much better advantage of sparse pathway encoding. In\nchannel-out networks, pathways are not only formed a posteriori, but they are\nalso actively selected according to the inference outputs from the lower\nlayers. From a mathematical perspective, channel-out networks can represent a\nwider class of piece-wise continuous functions, thereby endowing the network\nwith more expressive power than that of maxout networks. We test our\nchannel-out networks on several well-known image classification benchmarks,\nsetting new state-of-the-art performance on CIFAR-100 and STL-10, which\nrepresent some of the \"harder\" image classification benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 17:56:11 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Wang", "Qi", ""], ["JaJa", "Joseph", ""]]}, {"id": "1312.1970", "submitter": "Hoyt Koepke", "authors": "Hoyt Koepke, Marina Meila", "title": "An Algorithmic Theory of Dependent Regularizers, Part 1: Submodular\n  Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an exploration of the rich theoretical connections between several\nclasses of regularized models, network flows, and recent results in submodular\nfunction theory. This work unifies key aspects of these problems under a common\ntheory, leading to novel methods for working with several important models of\ninterest in statistics, machine learning and computer vision.\n  In Part 1, we review the concepts of network flows and submodular function\noptimization theory foundational to our results. We then examine the\nconnections between network flows and the minimum-norm algorithm from\nsubmodular optimization, extending and improving several current results. This\nleads to a concise representation of the structure of a large class of pairwise\nregularized models important in machine learning, statistics and computer\nvision.\n  In Part 2, we describe the full regularization path of a class of penalized\nregression problems with dependent variables that includes the graph-guided\nLASSO and total variation constrained models. This description also motivates a\npractical algorithm. This allows us to efficiently find the regularization path\nof the discretized version of TV penalized models. Ultimately, our new\nalgorithms scale up to high-dimensional problems with millions of variables.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2013 19:25:16 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Koepke", "Hoyt", ""], ["Meila", "Marina", ""]]}, {"id": "1312.2050", "submitter": "Jing Lei", "authors": "Jing Lei, Alessandro Rinaldo", "title": "Consistency of spectral clustering in stochastic block models", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1274 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 1, 215-237", "doi": "10.1214/14-AOS1274", "report-no": "IMS-AOS-AOS1274", "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the performance of spectral clustering for community extraction in\nstochastic block models. We show that, under mild conditions, spectral\nclustering applied to the adjacency matrix of the network can consistently\nrecover hidden communities even when the order of the maximum expected degree\nis as small as $\\log n$, with $n$ the number of nodes. This result applies to\nsome popular polynomial time spectral clustering algorithms and is further\nextended to degree corrected stochastic block models using a spherical\n$k$-median spectral clustering method. A key component of our analysis is a\ncombinatorial bound on the spectrum of binary random matrices, which is sharper\nthan the conventional matrix Bernstein inequality and may be of independent\ninterest.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2013 01:54:41 GMT"}, {"version": "v2", "created": "Fri, 19 Sep 2014 21:57:03 GMT"}, {"version": "v3", "created": "Tue, 30 Dec 2014 07:00:31 GMT"}], "update_date": "2014-12-31", "authors_parsed": [["Lei", "Jing", ""], ["Rinaldo", "Alessandro", ""]]}, {"id": "1312.2132", "submitter": "Dorsa Sadigh", "authors": "Dorsa Sadigh, Henrik Ohlsson, S. Shankar Sastry, Sanjit A. Seshia", "title": "Robust Subspace System Identification via Weighted Nuclear Norm\n  Optimization", "comments": "Submitted to the IFAC World Congress 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace identification is a classical and very well studied problem in\nsystem identification. The problem was recently posed as a convex optimization\nproblem via the nuclear norm relaxation. Inspired by robust PCA, we extend this\nframework to handle outliers. The proposed framework takes the form of a convex\noptimization problem with an objective that trades off fit, rank and sparsity.\nAs in robust PCA, it can be problematic to find a suitable regularization\nparameter. We show how the space in which a suitable parameter should be sought\ncan be limited to a bounded open set of the two dimensional parameter space. In\npractice, this is very useful since it restricts the parameter space that is\nneeded to be surveyed.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2013 19:19:03 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Sadigh", "Dorsa", ""], ["Ohlsson", "Henrik", ""], ["Sastry", "S. Shankar", ""], ["Seshia", "Sanjit A.", ""]]}, {"id": "1312.2139", "submitter": "John Duchi", "authors": "John C. Duchi and Michael I. Jordan and Martin J. Wainwright and Andre\n  Wibisono", "title": "Optimal rates for zero-order convex optimization: the power of two\n  function evaluations", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider derivative-free algorithms for stochastic and non-stochastic\nconvex optimization problems that use only function values rather than\ngradients. Focusing on non-asymptotic bounds on convergence rates, we show that\nif pairs of function values are available, algorithms for $d$-dimensional\noptimization that use gradient estimates based on random perturbations suffer a\nfactor of at most $\\sqrt{d}$ in convergence rate over traditional stochastic\ngradient methods. We establish such results for both smooth and non-smooth\ncases, sharpening previous analyses that suggested a worse dimension\ndependence, and extend our results to the case of multiple ($m \\ge 2$)\nevaluations. We complement our algorithmic development with\ninformation-theoretic lower bounds on the minimax convergence rate of such\nproblems, establishing the sharpness of our achievable results up to constant\n(sometimes logarithmic) factors.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2013 20:24:14 GMT"}, {"version": "v2", "created": "Wed, 20 Aug 2014 06:41:12 GMT"}], "update_date": "2014-08-21", "authors_parsed": [["Duchi", "John C.", ""], ["Jordan", "Michael I.", ""], ["Wainwright", "Martin J.", ""], ["Wibisono", "Andre", ""]]}, {"id": "1312.2154", "submitter": "Koji Eguchi", "authors": "Tomoki Kobayashi, Koji Eguchi", "title": "Sequential Monte Carlo Inference of Mixed Membership Stochastic\n  Blockmodels for Dynamic Social Networks", "comments": "NIPS 2013 Workshop on Frontiers of Network Analysis: Methods, Models,\n  and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many kinds of data can be represented as a network or graph. It is crucial to\ninfer the latent structure underlying such a network and to predict unobserved\nlinks in the network. Mixed Membership Stochastic Blockmodel (MMSB) is a\npromising model for network data. Latent variables and unknown parameters in\nMMSB have been estimated through Bayesian inference with the entire network;\nhowever, it is important to estimate them online for evolving networks. In this\npaper, we first develop online inference methods for MMSB through sequential\nMonte Carlo methods, also known as particle filters. We then extend them for\ntime-evolving networks, taking into account the temporal dependency of the\nnetwork structure. We demonstrate through experiments that the time-dependent\nparticle filter outperformed several baselines in terms of prediction\nperformance in an online condition.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2013 23:42:55 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Kobayashi", "Tomoki", ""], ["Eguchi", "Koji", ""]]}, {"id": "1312.2164", "submitter": "Le Song", "authors": "Nan Du, Yingyu Liang, Maria Florina Balcan, Le Song", "title": "Budgeted Influence Maximization for Multiple Products", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The typical algorithmic problem in viral marketing aims to identify a set of\ninfluential users in a social network, who, when convinced to adopt a product,\nshall influence other users in the network and trigger a large cascade of\nadoptions. However, the host (the owner of an online social platform) often\nfaces more constraints than a single product, endless user attentions,\nunlimited budget and unbounded time; in reality, multiple products need to be\nadvertised, each user can tolerate only a small number of recommendations,\ninfluencing user has a cost and advertisers have only limited budgets, and the\nadoptions need to be maximized within a short time window.\n  Given theses myriads of user, monetary, and timing constraints, it is\nextremely challenging for the host to design principled and efficient viral\nmarket algorithms with provable guarantees. In this paper, we provide a novel\nsolution by formulating the problem as a submodular maximization in a\ncontinuous-time diffusion model under an intersection of a matroid and multiple\nknapsack constraints. We also propose an adaptive threshold greedy algorithm\nwhich can be faster than the traditional greedy algorithm with lazy evaluation,\nand scalable to networks with million of nodes. Furthermore, our mathematical\nformulation allows us to prove that the algorithm can achieve an approximation\nfactor of $k_a/(2+2 k)$ when $k_a$ out of the $k$ knapsack constraints are\nactive, which also improves over previous guarantees from combinatorial\noptimization literature. In the case when influencing each user has uniform\ncost, the approximation becomes even better to a factor of $1/3$. Extensive\nsynthetic and real world experiments demonstrate that our budgeted influence\nmaximization algorithm achieves the-state-of-the-art in terms of both\neffectiveness and scalability, often beating the next best by significant\nmargins.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2013 01:58:39 GMT"}, {"version": "v2", "created": "Wed, 16 Apr 2014 03:53:04 GMT"}], "update_date": "2014-04-17", "authors_parsed": [["Du", "Nan", ""], ["Liang", "Yingyu", ""], ["Balcan", "Maria Florina", ""], ["Song", "Le", ""]]}, {"id": "1312.2171", "submitter": "Adam Kapelner", "authors": "Adam Kapelner and Justin Bleich", "title": "bartMachine: Machine Learning with Bayesian Additive Regression Trees", "comments": "39 pages, 13 figures, 4 tables, 2 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new package in R implementing Bayesian additive regression trees\n(BART). The package introduces many new features for data analysis using BART\nsuch as variable selection, interaction detection, model diagnostic plots,\nincorporation of missing data and the ability to save trees for future\nprediction. It is significantly faster than the current R implementation,\nparallelized, and capable of handling both large sample sizes and\nhigh-dimensional data.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2013 03:40:47 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2014 18:38:50 GMT"}, {"version": "v3", "created": "Mon, 24 Nov 2014 19:21:22 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Kapelner", "Adam", ""], ["Bleich", "Justin", ""]]}, {"id": "1312.2249", "submitter": "Dumitru Erhan", "authors": "Dumitru Erhan, Christian Szegedy, Alexander Toshev, Dragomir Anguelov", "title": "Scalable Object Detection using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have recently achieved state-of-the-art\nperformance on a number of image recognition benchmarks, including the ImageNet\nLarge-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on\nthe localization sub-task was a network that predicts a single bounding box and\na confidence score for each object category in the image. Such a model captures\nthe whole-image context around the objects but cannot handle multiple instances\nof the same object in the image without naively replicating the number of\noutputs for each instance. In this work, we propose a saliency-inspired neural\nnetwork model for detection, which predicts a set of class-agnostic bounding\nboxes along with a single score for each box, corresponding to its likelihood\nof containing any object of interest. The model naturally handles a variable\nnumber of instances for each class and allows for cross-class generalization at\nthe highest levels of the network. We are able to obtain competitive\nrecognition performance on VOC2007 and ILSVRC2012, while using only the top few\npredicted locations in each image and a small number of neural network\nevaluations.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2013 19:40:51 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Erhan", "Dumitru", ""], ["Szegedy", "Christian", ""], ["Toshev", "Alexander", ""], ["Anguelov", "Dragomir", ""]]}, {"id": "1312.2298", "submitter": "Shohei Hidaka", "authors": "Shohei Hidaka, Neeraj Kashyap", "title": "On the Estimation of Pointwise Dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an math.DS nlin.CD physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal in this paper is to develop an effective estimator of fractal\ndimension. We survey existing ideas in dimension estimation, with a focus on\nthe currently popular method of Grassberger and Procaccia for the estimation of\ncorrelation dimension. There are two major difficulties in estimation based on\nthis method. The first is the insensitivity of correlation dimension itself to\ndifferences in dimensionality over data, which we term \"dimension blindness\".\nThe second comes from the reliance of the method on the inference of limiting\nbehavior from finite data.\n  We propose pointwise dimension as an object for estimation in response to the\ndimension blindness of correlation dimension. Pointwise dimension is a local\nquantity, and the distribution of pointwise dimensions over the data contains\nthe information to which correlation dimension is blind. We use a \"limit-free\"\ndescription of pointwise dimension to develop a new estimator. We conclude by\ndiscussing potential applications of our estimator as well as some challenges\nit raises.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2013 03:09:25 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2013 02:21:13 GMT"}, {"version": "v3", "created": "Thu, 16 Jan 2014 04:04:13 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Hidaka", "Shohei", ""], ["Kashyap", "Neeraj", ""]]}, {"id": "1312.2565", "submitter": "Charanpal Dhanjal", "authors": "Charanpal Dhanjal (LTCI), St\\'ephan Cl\\'emen\\c{c}on (LTCI)", "title": "An SIR Graph Growth Model for the Epidemics of Communicable Diseases", "comments": "A few minor corrections and tidy of references. arXiv admin note:\n  text overlap with arXiv:0810.0896 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is the main purpose of this paper to introduce a graph-valued stochastic\nprocess in order to model the spread of a communicable infectious disease. The\nmajor novelty of the SIR model we promote lies in the fact that the social\nnetwork on which the epidemics is taking place is not specified in advance but\nevolves through time, accounting for the temporal evolution of the interactions\ninvolving infective individuals. Without assuming the existence of a fixed\nunderlying network model, the stochastic process introduced describes, in a\nflexible and realistic manner, epidemic spread in non-uniformly mixing and\npossibly heterogeneous populations. It is shown how to fit such a\n(parametrised) model by means of Approximate Bayesian Computation methods based\non graph-valued statistics. The concepts and statistical methods described in\nthis paper are finally applied to a real epidemic dataset, related to the\nspread of HIV in Cuba in presence of a contact tracing system, which permits\none to reconstruct partly the evolution of the graph of sexual partners\ndiagnosed HIV positive between 1986 and 2006.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2013 20:19:36 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2014 14:27:44 GMT"}], "update_date": "2014-02-06", "authors_parsed": [["Dhanjal", "Charanpal", "", "LTCI"], ["Cl\u00e9men\u00e7on", "St\u00e9phan", "", "LTCI"]]}, {"id": "1312.2638", "submitter": "C. E. Priebe", "authors": "D. E. Fishkind, V. Lyzinski, H. Pao, L. Chen, C. E. Priebe", "title": "Vertex nomination schemes for membership prediction", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS834 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 3, 1510-1532", "doi": "10.1214/15-AOAS834", "report-no": "IMS-AOAS-AOAS834", "categories": "stat.ML math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that a graph is realized from a stochastic block model where one of\nthe blocks is of interest, but many or all of the vertices' block labels are\nunobserved. The task is to order the vertices with unobserved block labels into\na ``nomination list'' such that, with high probability, vertices from the\ninteresting block are concentrated near the list's beginning. We propose\nseveral vertex nomination schemes. Our basic - but principled - setting and\ndevelopment yields a best nomination scheme (which is a Bayes-Optimal\nanalogue), and also a likelihood maximization nomination scheme that is\npractical to implement when there are a thousand vertices, and which is\nempirically near-optimal when the number of vertices is small enough to allow\ncomparison to the best nomination scheme. We then illustrate the robustness of\nthe likelihood maximization nomination scheme to the modeling challenges\ninherent in real data, using examples which include a social network involving\nhuman trafficking, the Enron Graph, a worm brain connectome and a political\nblog network.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 01:04:59 GMT"}, {"version": "v2", "created": "Mon, 7 Jul 2014 15:09:43 GMT"}, {"version": "v3", "created": "Thu, 7 Aug 2014 14:27:12 GMT"}, {"version": "v4", "created": "Fri, 17 Apr 2015 14:09:53 GMT"}, {"version": "v5", "created": "Tue, 17 Nov 2015 11:58:17 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Fishkind", "D. E.", ""], ["Lyzinski", "V.", ""], ["Pao", "H.", ""], ["Chen", "L.", ""], ["Priebe", "C. E.", ""]]}, {"id": "1312.2646", "submitter": "E.D. Guti\\'errez", "authors": "E.D. Guti\\'errez", "title": "Guaranteed Model Order Estimation and Sample Complexity Bounds for LDA", "comments": "Preliminary, unsubmitted draft; Incomplete", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question of how to determine the number of independent latent factors\n(topics) in mixture models such as Latent Dirichlet Allocation (LDA) is of\ngreat practical importance. In most applications, the exact number of topics is\nunknown, and depends on the application and the size of the data set. Bayesian\nnonparametric methods can avoid the problem of topic number selection, but they\ncan be impracticably slow for large sample sizes and are subject to local\noptima. We develop a guaranteed procedure for topic number recovery that does\nnot necessitate learning the model's latent parameters beforehand. Our\nprocedure relies on adapting results from random matrix theory. Performance of\nour topic number recovery procedure is superior to hLDA, a nonparametric\nmethod. We also discuss some implications of our results on the sample\ncomplexity and accuracy of popular spectral learning algorithms for LDA. Our\nresults and procedure can be extended to spectral learning algorithms for other\nexchangeable mixture models as well as Hidden Markov Models.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 02:38:52 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2013 16:52:45 GMT"}, {"version": "v3", "created": "Sun, 19 Jan 2014 03:09:11 GMT"}, {"version": "v4", "created": "Tue, 21 Jan 2014 23:51:01 GMT"}], "update_date": "2014-01-23", "authors_parsed": [["Guti\u00e9rrez", "E. D.", ""]]}, {"id": "1312.2967", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "Every LWF and AMP chain graph originates from a set of causal models", "comments": "Changes from v1 to v2: Major reorganization and correction of some\n  errors. Changes from v2 to v3: Negligible changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at justifying LWF and AMP chain graphs by showing that they\ndo not represent arbitrary independence models. Specifically, we show that\nevery chain graph is inclusion optimal wrt the intersection of the independence\nmodels represented by a set of directed and acyclic graphs under conditioning.\nThis implies that the independence model represented by the chain graph can be\naccounted for by a set of causal models that are subject to selection bias,\nwhich in turn can be accounted for by a system that switches between different\nregimes or configurations.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 21:12:34 GMT"}, {"version": "v2", "created": "Thu, 3 Jul 2014 22:47:49 GMT"}, {"version": "v3", "created": "Mon, 26 Jan 2015 13:50:04 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}, {"id": "1312.2988", "submitter": "Jinbo Xu", "authors": "Jianzhu Ma, Sheng Wang, Zhiyong Wang and Jinbo Xu", "title": "Protein Contact Prediction by Integrating Joint Evolutionary Coupling\n  Analysis and Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG math.OC q-bio.BM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protein contacts contain important information for protein structure and\nfunctional study, but contact prediction from sequence remains very\nchallenging. Both evolutionary coupling (EC) analysis and supervised machine\nlearning methods are developed to predict contacts, making use of different\ntypes of information, respectively. This paper presents a group graphical lasso\n(GGL) method for contact prediction that integrates joint multi-family EC\nanalysis and supervised learning. Different from existing single-family EC\nanalysis that uses residue co-evolution information in only the target protein\nfamily, our joint EC analysis uses residue co-evolution in both the target\nfamily and its related families, which may have divergent sequences but similar\nfolds. To implement joint EC analysis, we model a set of related protein\nfamilies using Gaussian graphical models (GGM) and then co-estimate their\nprecision matrices by maximum-likelihood, subject to the constraint that the\nprecision matrices shall share similar residue co-evolution patterns. To\nfurther improve the accuracy of the estimated precision matrices, we employ a\nsupervised learning method to predict contact probability from a variety of\nevolutionary and non-evolutionary information and then incorporate the\npredicted probability as prior into our GGL framework. Experiments show that\nour method can predict contacts much more accurately than existing methods, and\nthat our method performs better on both conserved and family-specific contacts.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 22:45:06 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2013 15:12:55 GMT"}, {"version": "v3", "created": "Tue, 14 Jan 2014 03:39:26 GMT"}, {"version": "v4", "created": "Wed, 15 Jan 2014 01:47:21 GMT"}, {"version": "v5", "created": "Wed, 8 Apr 2015 14:21:09 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Ma", "Jianzhu", ""], ["Wang", "Sheng", ""], ["Wang", "Zhiyong", ""], ["Xu", "Jinbo", ""]]}, {"id": "1312.3291", "submitter": "James Sharpnack", "authors": "James Sharpnack, Akshay Krishnamurthy, Aarti Singh", "title": "Near-optimal Anomaly Detection in Graphs using Lovasz Extended Scan\n  Statistic", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 26. MIT Press.\n  Cambridge, MA. 2013", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of anomalous activity in graphs is a statistical problem that\narises in many applications, such as network surveillance, disease outbreak\ndetection, and activity monitoring in social networks. Beyond its wide\napplicability, graph structured anomaly detection serves as a case study in the\ndifficulty of balancing computational complexity with statistical power. In\nthis work, we develop from first principles the generalized likelihood ratio\ntest for determining if there is a well connected region of activation over the\nvertices in the graph in Gaussian noise. Because this test is computationally\ninfeasible, we provide a relaxation, called the Lovasz extended scan statistic\n(LESS) that uses submodularity to approximate the intractable generalized\nlikelihood ratio. We demonstrate a connection between LESS and maximum\na-posteriori inference in Markov random fields, which provides us with a\npoly-time algorithm for LESS. Using electrical network theory, we are able to\ncontrol type 1 error for LESS and prove conditions under which LESS is risk\nconsistent. Finally, we consider specific graph models, the torus, k-nearest\nneighbor graphs, and epsilon-random graphs. We show that on these graphs our\nresults provide near-optimal performance by matching our results to known lower\nbounds.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2013 19:43:38 GMT"}], "update_date": "2013-12-12", "authors_parsed": [["Sharpnack", "James", ""], ["Krishnamurthy", "Akshay", ""], ["Singh", "Aarti", ""]]}, {"id": "1312.3386", "submitter": "Yoshikazu Terada", "authors": "Yoshikazu Terada", "title": "Clustering for high-dimension, low-sample size data using distance\n  vectors", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high-dimension, low-sample size (HDLSS) data, it is not always true that\ncloseness of two objects reflects a hidden cluster structure. We point out the\nimportant fact that it is not the closeness, but the \"values\" of distance that\ncontain information of the cluster structure in high-dimensional space. Based\non this fact, we propose an efficient and simple clustering approach, called\ndistance vector clustering, for HDLSS data. Under the assumptions given in the\nwork of Hall et al. (2005), we show the proposed approach provides a true\ncluster label under milder conditions when the dimension tends to infinity with\nthe sample size fixed. The effectiveness of the distance vector clustering\napproach is illustrated through a numerical experiment and real data analysis.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2013 02:37:36 GMT"}, {"version": "v2", "created": "Wed, 25 Dec 2013 13:10:44 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Terada", "Yoshikazu", ""]]}, {"id": "1312.3429", "submitter": "Kishore Konda", "authors": "Kishore Konda, Roland Memisevic", "title": "Unsupervised learning of depth and motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a model for the joint estimation of disparity and motion. The\nmodel is based on learning about the interrelations between images from\nmultiple cameras, multiple frames in a video, or the combination of both. We\nshow that learning depth and motion cues, as well as their combinations, from\ndata is possible within a single type of architecture and a single type of\nlearning algorithm, by using biologically inspired \"complex cell\" like units,\nwhich encode correlations between the pixels across image pairs. Our\nexperimental results show that the learning of depth and motion makes it\npossible to achieve state-of-the-art performance in 3-D activity analysis, and\nto outperform existing hand-engineered 3-D motion features by a very large\nmargin.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2013 10:03:47 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2013 16:11:52 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Konda", "Kishore", ""], ["Memisevic", "Roland", ""]]}, {"id": "1312.3516", "submitter": "Bharath Sriperumbudur", "authors": "Bharath Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Aapo\n  Hyv\\\"arinen and Revant Kumar", "title": "Density Estimation in Infinite Dimensional Exponential Families", "comments": "58 pages, 8 figures; Fixed some errors and typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider an infinite dimensional exponential family,\n$\\mathcal{P}$ of probability densities, which are parametrized by functions in\na reproducing kernel Hilbert space, $H$ and show it to be quite rich in the\nsense that a broad class of densities on $\\mathbb{R}^d$ can be approximated\narbitrarily well in Kullback-Leibler (KL) divergence by elements in\n$\\mathcal{P}$. The main goal of the paper is to estimate an unknown density,\n$p_0$ through an element in $\\mathcal{P}$. Standard techniques like maximum\nlikelihood estimation (MLE) or pseudo MLE (based on the method of sieves),\nwhich are based on minimizing the KL divergence between $p_0$ and\n$\\mathcal{P}$, do not yield practically useful estimators because of their\ninability to efficiently handle the log-partition function. Instead, we propose\nan estimator, $\\hat{p}_n$ based on minimizing the \\emph{Fisher divergence},\n$J(p_0\\Vert p)$ between $p_0$ and $p\\in \\mathcal{P}$, which involves solving a\nsimple finite-dimensional linear system. When $p_0\\in\\mathcal{P}$, we show that\nthe proposed estimator is consistent, and provide a convergence rate of\n$n^{-\\min\\left\\{\\frac{2}{3},\\frac{2\\beta+1}{2\\beta+2}\\right\\}}$ in Fisher\ndivergence under the smoothness assumption that $\\log\np_0\\in\\mathcal{R}(C^\\beta)$ for some $\\beta\\ge 0$, where $C$ is a certain\nHilbert-Schmidt operator on $H$ and $\\mathcal{R}(C^\\beta)$ denotes the image of\n$C^\\beta$. We also investigate the misspecified case of $p_0\\notin\\mathcal{P}$\nand show that $J(p_0\\Vert\\hat{p}_n)\\rightarrow \\inf_{p\\in\\mathcal{P}}J(p_0\\Vert\np)$ as $n\\rightarrow\\infty$, and provide a rate for this convergence under a\nsimilar smoothness condition as above. Through numerical simulations we\ndemonstrate that the proposed estimator outperforms the non-parametric kernel\ndensity estimator, and that the advantage with the proposed estimator grows as\n$d$ increases.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2013 15:09:25 GMT"}, {"version": "v2", "created": "Sat, 26 Apr 2014 16:26:29 GMT"}, {"version": "v3", "created": "Sun, 2 Nov 2014 03:51:53 GMT"}, {"version": "v4", "created": "Fri, 26 May 2017 14:42:48 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Sriperumbudur", "Bharath", ""], ["Fukumizu", "Kenji", ""], ["Gretton", "Arthur", ""], ["Hyv\u00e4rinen", "Aapo", ""], ["Kumar", "Revant", ""]]}, {"id": "1312.3522", "submitter": "Weizhi  Lu", "authors": "Weizhi Lu and Weiyu Li and Kidiyo Kpalma and Joseph Ronsin", "title": "Sparse Matrix-based Random Projection for Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  As a typical dimensionality reduction technique, random projection can be\nsimply implemented with linear projection, while maintaining the pairwise\ndistances of high-dimensional data with high probability. Considering this\ntechnique is mainly exploited for the task of classification, this paper is\ndeveloped to study the construction of random matrix from the viewpoint of\nfeature selection, rather than of traditional distance preservation. This\nyields a somewhat surprising theoretical result, that is, the sparse random\nmatrix with exactly one nonzero element per column, can present better feature\nselection performance than other more dense matrices, if the projection\ndimension is sufficiently large (namely, not much smaller than the number of\nfeature elements); otherwise, it will perform comparably to others. For random\nprojection, this theoretical result implies considerable improvement on both\ncomplexity and performance, which is widely confirmed with the classification\nexperiments on both synthetic data and real data.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2013 15:26:57 GMT"}, {"version": "v2", "created": "Sun, 30 Mar 2014 13:56:04 GMT"}, {"version": "v3", "created": "Sun, 12 Oct 2014 22:10:13 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Lu", "Weizhi", ""], ["Li", "Weiyu", ""], ["Kpalma", "Kidiyo", ""], ["Ronsin", "Joseph", ""]]}, {"id": "1312.3525", "submitter": "Anders Bredahl Kock", "authors": "Mehmet Caner and Anders Bredahl Kock", "title": "Oracle Inequalities for Convex Loss Functions with Non-Linear Targets", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper consider penalized empirical loss minimization of convex loss\nfunctions with unknown non-linear target functions. Using the elastic net\npenalty we establish a finite sample oracle inequality which bounds the loss of\nour estimator from above with high probability. If the unknown target is linear\nthis inequality also provides an upper bound of the estimation error of the\nestimated parameter vector. These are new results and they generalize the\neconometrics and statistics literature. Next, we use the non-asymptotic results\nto show that the excess loss of our estimator is asymptotically of the same\norder as that of the oracle. If the target is linear we give sufficient\nconditions for consistency of the estimated parameter vector. Next, we briefly\ndiscuss how a thresholded version of our estimator can be used to perform\nconsistent variable selection. We give two examples of loss functions covered\nby our framework and show how penalized nonparametric series estimation is\ncontained as a special case and provide a finite sample upper bound on the mean\nsquare error of the elastic net series estimator.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2013 15:36:08 GMT"}], "update_date": "2013-12-13", "authors_parsed": [["Caner", "Mehmet", ""], ["Kock", "Anders Bredahl", ""]]}, {"id": "1312.3613", "submitter": "Jean-Baptiste  Tristan", "authors": "Jean-Baptiste Tristan, Daniel Huang, Joseph Tassarotti, Adam Pocock,\n  Stephen J. Green, Guy L. Steele Jr", "title": "Augur: a Modeling Language for Data-Parallel Probabilistic Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is time-consuming and error-prone to implement inference procedures for\neach new probabilistic model. Probabilistic programming addresses this problem\nby allowing a user to specify the model and having a compiler automatically\ngenerate an inference procedure for it. For this approach to be practical, it\nis important to generate inference code that has reasonable performance. In\nthis paper, we present a probabilistic programming language and compiler for\nBayesian networks designed to make effective use of data-parallel architectures\nsuch as GPUs. Our language is fully integrated within the Scala programming\nlanguage and benefits from tools such as IDE support, type-checking, and code\ncompletion. We show that the compiler can generate data-parallel inference code\nscalable to thousands of GPU cores by making use of the conditional\nindependence relationships in the Bayesian network.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2013 20:23:20 GMT"}, {"version": "v2", "created": "Tue, 10 Jun 2014 19:53:09 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Tristan", "Jean-Baptiste", ""], ["Huang", "Daniel", ""], ["Tassarotti", "Joseph", ""], ["Pocock", "Adam", ""], ["Green", "Stephen J.", ""], ["Steele", "Guy L.", "Jr"]]}, {"id": "1312.3790", "submitter": "Remi Gribonval", "authors": "R\\'emi Gribonval (INRIA - IRISA), Rodolphe Jenatton (INRIA Paris -\n  Rocquencourt, CMAP), Francis Bach (INRIA Paris - Rocquencourt, LIENS), Martin\n  Kleinsteuber (TUM), Matthias Seibert (TUM)", "title": "Sample Complexity of Dictionary Learning and other Matrix Factorizations", "comments": "to appear", "journal-ref": "IEEE Transactions on Information Theory, Institute of Electrical\n  and Electronics Engineers (IEEE), 2015, pp.18", "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern tools in machine learning and signal processing, such as sparse\ndictionary learning, principal component analysis (PCA), non-negative matrix\nfactorization (NMF), $K$-means clustering, etc., rely on the factorization of a\nmatrix obtained by concatenating high-dimensional vectors from a training\ncollection. While the idealized task would be to optimize the expected quality\nof the factors over the underlying distribution of training vectors, it is\nachieved in practice by minimizing an empirical average over the considered\ncollection. The focus of this paper is to provide sample complexity estimates\nto uniformly control how much the empirical average deviates from the expected\ncost function. Standard arguments imply that the performance of the empirical\npredictor also exhibit such guarantees. The level of genericity of the approach\nencompasses several possible constraints on the factors (tensor product\nstructure, shift-invariance, sparsity \\ldots), thus providing a unified\nperspective on the sample complexity of several widely used matrix\nfactorization schemes. The derived generalization bounds behave proportional to\n$\\sqrt{\\log(n)/n}$ w.r.t.\\ the number of samples $n$ for the considered matrix\nfactorization techniques.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2013 12:32:46 GMT"}, {"version": "v2", "created": "Tue, 2 Dec 2014 20:12:38 GMT"}, {"version": "v3", "created": "Thu, 9 Apr 2015 07:35:59 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Gribonval", "R\u00e9mi", "", "INRIA - IRISA"], ["Jenatton", "Rodolphe", "", "INRIA Paris -\n  Rocquencourt, CMAP"], ["Bach", "Francis", "", "INRIA Paris - Rocquencourt, LIENS"], ["Kleinsteuber", "Martin", "", "TUM"], ["Seibert", "Matthias", "", "TUM"]]}, {"id": "1312.3970", "submitter": "Michael Smith", "authors": "Michael R. Smith and Tony Martinez", "title": "An Extensive Evaluation of Filtering Misclassified Instances in\n  Supervised Classification Tasks", "comments": "29 pages, 3 Figures, 20 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Removing or filtering outliers and mislabeled instances prior to training a\nlearning algorithm has been shown to increase classification accuracy. A\npopular approach for handling outliers and mislabeled instances is to remove\nany instance that is misclassified by a learning algorithm. However, an\nexamination of which learning algorithms to use for filtering as well as their\neffects on multiple learning algorithms over a large set of data sets has not\nbeen done. Previous work has generally been limited due to the large\ncomputational requirements to run such an experiment, and, thus, the\nexamination has generally been limited to learning algorithms that are\ncomputationally inexpensive and using a small number of data sets. In this\npaper, we examine 9 learning algorithms as filtering algorithms as well as\nexamining the effects of filtering in the 9 chosen learning algorithms on a set\nof 54 data sets. In addition to using each learning algorithm individually as a\nfilter, we also use the set of learning algorithms as an ensemble filter and\nuse an adaptive algorithm that selects a subset of the learning algorithms for\nfiltering for a specific task and learning algorithm. We find that for most\ncases, using an ensemble of learning algorithms for filtering produces the\ngreatest increase in classification accuracy. We also compare filtering with a\nmajority voting ensemble. The voting ensemble significantly outperforms\nfiltering unless there are high amounts of noise present in the data set.\nAdditionally, we find that a majority voting ensemble is robust to noise as\nfiltering with a voting ensemble does not increase the classification accuracy\nof the voting ensemble.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2013 21:59:00 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Smith", "Michael R.", ""], ["Martinez", "Tony", ""]]}, {"id": "1312.4353", "submitter": "Tim Genewein", "authors": "Tim Genewein, Daniel A. Braun", "title": "Abstraction in decision-makers with limited information processing\n  capabilities", "comments": "Presented at the NIPS 2013 Workshop on Planning with Information\n  Constraints", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A distinctive property of human and animal intelligence is the ability to\nform abstractions by neglecting irrelevant information which allows to separate\nstructure from noise. From an information theoretic point of view abstractions\nare desirable because they allow for very efficient information processing. In\nartificial systems abstractions are often implemented through computationally\ncostly formations of groups or clusters. In this work we establish the relation\nbetween the free-energy framework for decision making and rate-distortion\ntheory and demonstrate how the application of rate-distortion for\ndecision-making leads to the emergence of abstractions. We argue that\nabstractions are induced due to a limit in information processing capacity.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 13:38:10 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2013 17:30:03 GMT"}], "update_date": "2013-12-20", "authors_parsed": [["Genewein", "Tim", ""], ["Braun", "Daniel A.", ""]]}, {"id": "1312.4382", "submitter": "Hideaki Shimazaki", "authors": "Hideaki Shimazaki", "title": "Single-trial estimation of stimulus and spike-history effects on\n  time-varying ensemble spiking activity of multiple neurons: a simulation\n  study", "comments": "12 pages, 3 figures", "journal-ref": "J. Phys.: Conf. Ser. (2013) 473, 012009", "doi": "10.1088/1742-6596/473/1/012009", "report-no": null, "categories": "q-bio.NC cond-mat.stat-mech stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Neurons in cortical circuits exhibit coordinated spiking activity, and can\nproduce correlated synchronous spikes during behavior and cognition. We\nrecently developed a method for estimating the dynamics of correlated ensemble\nactivity by combining a model of simultaneous neuronal interactions (e.g., a\nspin-glass model) with a state-space method (Shimazaki et al. 2012 PLoS Comput\nBiol 8 e1002385). This method allows us to estimate stimulus-evoked dynamics of\nneuronal interactions which is reproducible in repeated trials under identical\nexperimental conditions. However, the method may not be suitable for detecting\nstimulus responses if the neuronal dynamics exhibits significant variability\nacross trials. In addition, the previous model does not include effects of past\nspiking activity of the neurons on the current state of ensemble activity. In\nthis study, we develop a parametric method for simultaneously estimating the\nstimulus and spike-history effects on the ensemble activity from single-trial\ndata even if the neurons exhibit dynamics that is largely unrelated to these\neffects. For this goal, we model ensemble neuronal activity as a latent process\nand include the stimulus and spike-history effects as exogenous inputs to the\nlatent process. We develop an expectation-maximization algorithm that\nsimultaneously achieves estimation of the latent process, stimulus responses,\nand spike-history effects. The proposed method is useful to analyze an\ninteraction of internal cortical states and sensory evoked activity.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 14:50:26 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Shimazaki", "Hideaki", ""]]}, {"id": "1312.4426", "submitter": "R.J. Vanderbei", "authors": "Robert Vanderbei and Han Liu and Lie Wang and Kevin Lin", "title": "Optimization for Compressed Sensing: the Simplex Method and Kronecker\n  Sparsification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present two new approaches to efficiently solve large-scale\ncompressed sensing problems. These two ideas are independent of each other and\ncan therefore be used either separately or together. We consider all\npossibilities.\n  For the first approach, we note that the zero vector can be taken as the\ninitial basic (infeasible) solution for the linear programming problem and\ntherefore, if the true signal is very sparse, some variants of the simplex\nmethod can be expected to take only a small number of pivots to arrive at a\nsolution. We implemented one such variant and demonstrate a dramatic\nimprovement in computation time on very sparse signals.\n  The second approach requires a redesigned sensing mechanism in which the\nvector signal is stacked into a matrix. This allows us to exploit the Kronecker\ncompressed sensing (KCS) mechanism. We show that the Kronecker sensing requires\nstronger conditions for perfect recovery compared to the original vector\nproblem. However, the Kronecker sensing, modeled correctly, is a much sparser\nlinear optimization problem. Hence, algorithms that benefit from sparse problem\nrepresentation, such as interior-point methods, can solve the Kronecker sensing\nproblems much faster than the corresponding vector problem. In our numerical\nstudies, we demonstrate a ten-fold improvement in the computation time.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 16:51:51 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Vanderbei", "Robert", ""], ["Liu", "Han", ""], ["Wang", "Lie", ""], ["Lin", "Kevin", ""]]}, {"id": "1312.4479", "submitter": "Jean-Baptiste Durand", "authors": "Pierre Fernique (VP, AGAP), Jean-Baptiste Durand (VP, INRIA Grenoble\n  Rh\\^one-Alpes / LJK Laboratoire Jean Kuntzmann), Yann Gu\\'edon (VP, AGAP)", "title": "Parametric Modelling of Multivariate Count Data Using Probabilistic\n  Graphical Models", "comments": null, "journal-ref": "3rd Workshop on Algorithmic issues for Inference in Graphical\n  Models - AIGM13, Paris : France (2013)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate count data are defined as the number of items of different\ncategories issued from sampling within a population, which individuals are\ngrouped into categories. The analysis of multivariate count data is a recurrent\nand crucial issue in numerous modelling problems, particularly in the fields of\nbiology and ecology (where the data can represent, for example, children counts\nassociated with multitype branching processes), sociology and econometrics. We\nfocus on I) Identifying categories that appear simultaneously, or on the\ncontrary that are mutually exclusive. This is achieved by identifying\nconditional independence relationships between the variables; II)Building\nparsimonious parametric models consistent with these relationships; III)\nCharacterising and testing the effects of covariates on the joint distribution\nof the counts. To achieve these goals, we propose an approach based on\ngraphical probabilistic models, and more specifically partially directed\nacyclic graphs.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 19:38:35 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Fernique", "Pierre", "", "VP, AGAP"], ["Durand", "Jean-Baptiste", "", "VP, INRIA Grenoble\n  Rh\u00f4ne-Alpes / LJK Laboratoire Jean Kuntzmann"], ["Gu\u00e9don", "Yann", "", "VP, AGAP"]]}, {"id": "1312.4527", "submitter": "Khoat Than", "authors": "Khoat Than and Tu Bao Ho", "title": "Probable convexity and its application to Correlated Topic Models", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-convex optimization problems often arise from probabilistic modeling,\nsuch as estimation of posterior distributions. Non-convexity makes the problems\nintractable, and poses various obstacles for us to design efficient algorithms.\nIn this work, we attack non-convexity by first introducing the concept of\n\\emph{probable convexity} for analyzing convexity of real functions in\npractice. We then use the new concept to analyze an inference problem in the\n\\emph{Correlated Topic Model} (CTM) and related nonconjugate models. Contrary\nto the existing belief of intractability, we show that this inference problem\nis concave under certain conditions. One consequence of our analyses is a novel\nalgorithm for learning CTM which is significantly more scalable and qualitative\nthan existing methods. Finally, we highlight that stochastic gradient\nalgorithms might be a practical choice to resolve efficiently non-convex\nproblems. This finding might find beneficial in many contexts which are beyond\nprobabilistic modeling.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 09:34:43 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Than", "Khoat", ""], ["Ho", "Tu Bao", ""]]}, {"id": "1312.4551", "submitter": "Aram Galstyan", "authors": "Armen E. Allahverdyan and Aram Galstyan", "title": "Comparative Analysis of Viterbi Training and Maximum Likelihood\n  Estimation for HMMs", "comments": "Appeared in Neural Information Processing Systems (NIPS) 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an asymptotic analysis of Viterbi Training (VT) and contrast it\nwith a more conventional Maximum Likelihood (ML) approach to parameter\nestimation in Hidden Markov Models. While ML estimator works by (locally)\nmaximizing the likelihood of the observed data, VT seeks to maximize the\nprobability of the most likely hidden state sequence. We develop an analytical\nframework based on a generating function formalism and illustrate it on an\nexactly solvable model of HMM with one unambiguous symbol. For this particular\nmodel the ML objective function is continuously degenerate. VT objective, in\ncontrast, is shown to have only finite degeneracy. Furthermore, VT converges\nfaster and results in sparser (simpler) models, thus realizing an automatic\nOccam's razor for HMM learning. For more general scenario VT can be worse\ncompared to ML but still capable of correctly recovering most of the\nparameters.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 21:03:28 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Allahverdyan", "Armen E.", ""], ["Galstyan", "Aram", ""]]}, {"id": "1312.4564", "submitter": "Peilin Zhao", "authors": "Peilin Zhao, Jinwei Yang, Tong Zhang, Ping Li", "title": "Adaptive Stochastic Alternating Direction Method of Multipliers", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Alternating Direction Method of Multipliers (ADMM) has been studied for\nyears. The traditional ADMM algorithm needs to compute, at each iteration, an\n(empirical) expected loss function on all training examples, resulting in a\ncomputational complexity proportional to the number of training examples. To\nreduce the time complexity, stochastic ADMM algorithms were proposed to replace\nthe expected function with a random loss function associated with one uniformly\ndrawn example plus a Bregman divergence. The Bregman divergence, however, is\nderived from a simple second order proximal function, the half squared norm,\nwhich could be a suboptimal choice.\n  In this paper, we present a new family of stochastic ADMM algorithms with\noptimal second order proximal functions, which produce a new family of adaptive\nsubgradient methods. We theoretically prove that their regret bounds are as\ngood as the bounds which could be achieved by the best proximal function that\ncan be chosen in hindsight. Encouraging empirical results on a variety of\nreal-world datasets confirm the effectiveness and efficiency of the proposed\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 21:22:46 GMT"}, {"version": "v2", "created": "Sun, 22 Dec 2013 01:59:05 GMT"}, {"version": "v3", "created": "Thu, 5 Jun 2014 07:03:48 GMT"}, {"version": "v4", "created": "Mon, 9 Jun 2014 09:31:13 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Zhao", "Peilin", ""], ["Yang", "Jinwei", ""], ["Zhang", "Tong", ""], ["Li", "Ping", ""]]}, {"id": "1312.4605", "submitter": "Xiangyu Wang", "authors": "Xiangyu Wang, David B. Dunson", "title": "Parallelizing MCMC via Weierstrass Sampler", "comments": "The original Algorithm 1 removed. Provided some theoretical\n  justification for refinement sampling (Theorem 2). Added a new algorithm in\n  addition to the rejection sampling for handling dimensionality curse. New\n  simulations and graphs (with new colors and designs). A real data analysis is\n  also provided", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapidly growing scales of statistical problems, subset based\ncommunication-free parallel MCMC methods are a promising future for large scale\nBayesian analysis. In this article, we propose a new Weierstrass sampler for\nparallel MCMC based on independent subsets. The new sampler approximates the\nfull data posterior samples via combining the posterior draws from independent\nsubset MCMC chains, and thus enjoys a higher computational efficiency. We show\nthat the approximation error for the Weierstrass sampler is bounded by some\ntuning parameters and provide suggestions for choice of the values. Simulation\nstudy shows the Weierstrass sampler is very competitive compared to other\nmethods for combining MCMC chains generated for subsets, including averaging\nand kernel smoothing.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 01:43:39 GMT"}, {"version": "v2", "created": "Sun, 25 May 2014 18:46:47 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Wang", "Xiangyu", ""], ["Dunson", "David B.", ""]]}, {"id": "1312.4626", "submitter": "Alex Gittens", "authors": "Raffay Hamid and Ying Xiao and Alex Gittens and Dennis DeCoste", "title": "Compact Random Feature Maps", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel approximation using randomized feature maps has recently gained a lot\nof interest. In this work, we identify that previous approaches for polynomial\nkernel approximation create maps that are rank deficient, and therefore do not\nutilize the capacity of the projected feature space effectively. To address\nthis challenge, we propose compact random feature maps (CRAFTMaps) to\napproximate polynomial kernels more concisely and accurately. We prove the\nerror bounds of CRAFTMaps demonstrating their superior kernel reconstruction\nperformance compared to the previous approximation schemes. We show how\nstructured random matrices can be used to efficiently generate CRAFTMaps, and\npresent a single-pass algorithm using CRAFTMaps to learn non-linear multi-class\nclassifiers. We present experiments on multiple standard data-sets with\nperformance competitive with state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 03:33:08 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Hamid", "Raffay", ""], ["Xiao", "Ying", ""], ["Gittens", "Alex", ""], ["DeCoste", "Dennis", ""]]}, {"id": "1312.4664", "submitter": "Motonobu Kanagawa", "authors": "Motonobu Kanagawa, Yu Nishiyama, Arthur Gretton, Kenji Fukumizu", "title": "Filtering with State-Observation Examples via Kernel Monte Carlo Filter", "comments": "56 pages, 25 figures; Final version (accepted to Neural Computation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of filtering with a state-space model.\nStandard approaches for filtering assume that a probabilistic model for\nobservations (i.e. the observation model) is given explicitly or at least\nparametrically. We consider a setting where this assumption is not satisfied;\nwe assume that the knowledge of the observation model is only provided by\nexamples of state-observation pairs. This setting is important and appears when\nstate variables are defined as quantities that are very different from the\nobservations. We propose Kernel Monte Carlo Filter, a novel filtering method\nthat is focused on this setting. Our approach is based on the framework of\nkernel mean embeddings, which enables nonparametric posterior inference using\nthe state-observation examples. The proposed method represents state\ndistributions as weighted samples, propagates these samples by sampling,\nestimates the state posteriors by Kernel Bayes' Rule, and resamples by Kernel\nHerding. In particular, the sampling and resampling procedures are novel in\nbeing expressed using kernel mean embeddings, so we theoretically analyze their\nbehaviors. We reveal the following properties, which are similar to those of\ncorresponding procedures in particle methods: (1) the performance of sampling\ncan degrade if the effective sample size of a weighted sample is small; (2)\nresampling improves the sampling performance by increasing the effective sample\nsize. We first demonstrate these theoretical findings by synthetic experiments.\nThen we show the effectiveness of the proposed filter by artificial and real\ndata experiments, which include vision-based mobile robot localization.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 07:03:41 GMT"}, {"version": "v2", "created": "Tue, 19 May 2015 06:45:26 GMT"}, {"version": "v3", "created": "Wed, 26 Aug 2015 04:58:09 GMT"}, {"version": "v4", "created": "Thu, 22 Oct 2015 05:12:26 GMT"}], "update_date": "2015-10-23", "authors_parsed": [["Kanagawa", "Motonobu", ""], ["Nishiyama", "Yu", ""], ["Gretton", "Arthur", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "1312.4710", "submitter": "Manfred Claassen", "authors": "Eirini Arvaniti and Manfred Claassen", "title": "Markov Network Structure Learning via Ensemble-of-Forests Models", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real world systems typically feature a variety of different dependency types\nand topologies that complicate model selection for probabilistic graphical\nmodels. We introduce the ensemble-of-forests model, a generalization of the\nensemble-of-trees model. Our model enables structure learning of Markov random\nfields (MRF) with multiple connected components and arbitrary potentials. We\npresent two approximate inference techniques for this model and demonstrate\ntheir performance on synthetic data. Our results suggest that the\nensemble-of-forests approach can accurately recover sparse, possibly\ndisconnected MRF topologies, even in presence of non-Gaussian dependencies\nand/or low sample size. We applied the ensemble-of-forests model to learn the\nstructure of perturbed signaling networks of immune cells and found that these\nfrequently exhibit non-Gaussian dependencies with disconnected MRF topologies.\nIn summary, we expect that the ensemble-of-forests model will enable MRF\nstructure learning in other high dimensional real world settings that are\ngoverned by non-trivial dependencies.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 10:26:02 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Arvaniti", "Eirini", ""], ["Claassen", "Manfred", ""]]}, {"id": "1312.4717", "submitter": "Zhihua Zhang", "authors": "Zhihua Zhang", "title": "The Matrix Ridge Approximation: Algorithms and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are concerned with an approximation problem for a symmetric positive\nsemidefinite matrix due to motivation from a class of nonlinear machine\nlearning methods. We discuss an approximation approach that we call {matrix\nridge approximation}. In particular, we define the matrix ridge approximation\nas an incomplete matrix factorization plus a ridge term. Moreover, we present\nprobabilistic interpretations using a normal latent variable model and a\nWishart model for this approximation approach. The idea behind the latent\nvariable model in turn leads us to an efficient EM iterative method for\nhandling the matrix ridge approximation problem. Finally, we illustrate the\napplications of the approximation approach in multivariate data analysis.\nEmpirical studies in spectral clustering and Gaussian process regression show\nthat the matrix ridge approximation with the EM iteration is potentially\nuseful.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 11:01:29 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Zhang", "Zhihua", ""]]}, {"id": "1312.4719", "submitter": "Zhihua Zhang", "authors": "Zhihua Zhang", "title": "The Bernstein Function: A Unifying Framework of Nonconvex Penalization\n  in Sparse Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study nonconvex penalization using Bernstein functions.\nSince the Bernstein function is concave and nonsmooth at the origin, it can\ninduce a class of nonconvex functions for high-dimensional sparse estimation\nproblems. We derive a threshold function based on the Bernstein penalty and\ngive its mathematical properties in sparsity modeling. We show that a\ncoordinate descent algorithm is especially appropriate for penalized regression\nproblems with the Bernstein penalty. Additionally, we prove that the Bernstein\nfunction can be defined as the concave conjugate of a $\\varphi$-divergence and\ndevelop a conjugate maximization algorithm for finding the sparse solution.\nFinally, we particularly exemplify a family of Bernstein nonconvex penalties\nbased on a generalized Gamma measure and conduct empirical analysis for this\nfamily.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 11:10:06 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Zhang", "Zhihua", ""]]}, {"id": "1312.4852", "submitter": "Roger Frigola", "authors": "Roger Frigola, Fredrik Lindsten, Thomas B. Sch\\\"on, Carl E. Rasmussen", "title": "Identification of Gaussian Process State-Space Models with Particle\n  Stochastic Approximation EM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SY", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Gaussian process state-space models (GP-SSMs) are a very flexible family of\nmodels of nonlinear dynamical systems. They comprise a Bayesian nonparametric\nrepresentation of the dynamics of the system and additional (hyper-)parameters\ngoverning the properties of this nonparametric representation. The Bayesian\nformalism enables systematic reasoning about the uncertainty in the system\ndynamics. We present an approach to maximum likelihood identification of the\nparameters in GP-SSMs, while retaining the full nonparametric description of\nthe dynamics. The method is based on a stochastic approximation version of the\nEM algorithm that employs recent developments in particle Markov chain Monte\nCarlo for efficient identification.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 16:32:54 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Frigola", "Roger", ""], ["Lindsten", "Fredrik", ""], ["Sch\u00f6n", "Thomas B.", ""], ["Rasmussen", "Carl E.", ""]]}, {"id": "1312.4895", "submitter": "Nikolaos Freris", "authors": "Nikolaos M. Freris, Orhan \\\"O\\c{c}al and Martin Vetterli", "title": "Recursive Compressed Sensing", "comments": "Submitted to IEEE Transactions on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a recursive algorithm for performing compressed sensing on\nstreaming data. The approach consists of a) recursive encoding, where we sample\nthe input stream via overlapping windowing and make use of the previous\nmeasurement in obtaining the next one, and b) recursive decoding, where the\nsignal estimate from the previous window is utilized in order to achieve faster\nconvergence in an iterative optimization scheme applied to decode the new one.\nTo remove estimation bias, a two-step estimation procedure is proposed\ncomprising support set detection and signal amplitude estimation. Estimation\naccuracy is enhanced by a non-linear voting method and averaging estimates over\nmultiple windows. We analyze the computational complexity and estimation error,\nand show that the normalized error variance asymptotically goes to zero for\nsublinear sparsity. Our simulation results show speed up of an order of\nmagnitude over traditional CS, while obtaining significantly lower\nreconstruction error under mild conditions on the signal magnitudes and the\nnoise level.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 19:06:59 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Freris", "Nikolaos M.", ""], ["\u00d6\u00e7al", "Orhan", ""], ["Vetterli", "Martin", ""]]}, {"id": "1312.5023", "submitter": "Matt Wytock", "authors": "Matt Wytock and J. Zico Kolter", "title": "Contextually Supervised Source Separation with Application to Energy\n  Disaggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for single-channel source separation that lies\nbetween the fully supervised and unsupervised setting. Instead of supervision,\nwe provide input features for each source signal and use convex methods to\nestimate the correlations between these features and the unobserved signal\ndecomposition. We analyze the case of $\\ell_2$ loss theoretically and show that\nrecovery of the signal components depends only on cross-correlation between\nfeatures for different signals, not on correlations between features for the\nsame signal. Contextually supervised source separation is a natural fit for\ndomains with large amounts of data but no explicit supervision; our motivating\napplication is energy disaggregation of hourly smart meter data (the separation\nof whole-home power signals into different energy uses). Here we apply\ncontextual supervision to disaggregate the energy usage of thousands homes over\nfour years, a significantly larger scale than previously published efforts, and\ndemonstrate on synthetic data that our method outperforms the unsupervised\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 02:21:01 GMT"}], "update_date": "2013-12-19", "authors_parsed": [["Wytock", "Matt", ""], ["Kolter", "J. Zico", ""]]}, {"id": "1312.5066", "submitter": "Marine Depecker-Qu\\'echon Dr", "authors": "St\\'ephan Cl\\'emen\\c{c}on and Marine Depecker", "title": "Functional Bipartite Ranking: a Wavelet-Based Filtering Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is the main goal of this article to address the bipartite ranking issue\nfrom the perspective of functional data analysis (FDA). Given a training set of\nindependent realizations of a (possibly sampled) second-order random function\nwith a (locally) smooth autocorrelation structure and to which a binary label\nis randomly assigned, the objective is to learn a scoring function s with\noptimal ROC curve. Based on linear/nonlinear wavelet-based approximations, it\nis shown how to select compact finite dimensional representations of the input\ncurves adaptively, in order to build accurate ranking rules, using recent\nadvances in the ranking problem for multivariate data with binary feedback.\nBeyond theoretical considerations, the performance of the learning methods for\nfunctional bipartite ranking proposed in this paper are illustrated by\nnumerical experiments.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 09:15:59 GMT"}], "update_date": "2013-12-19", "authors_parsed": [["Cl\u00e9men\u00e7on", "St\u00e9phan", ""], ["Depecker", "Marine", ""]]}, {"id": "1312.5124", "submitter": "Paul Fogel", "authors": "Paul Fogel", "title": "Permuted NMF: A Simple Algorithm Intended to Minimize the Volume of the\n  Score Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-Negative Matrix Factorization, NMF, attempts to find a number of\narchetypal response profiles, or parts, such that any sample profile in the\ndataset can be approximated by a close profile among these archetypes or a\nlinear combination of these profiles. The non-negativity constraint is imposed\nwhile estimating archetypal profiles, due to the non-negative nature of the\nobserved signal. Apart from non negativity, a volume constraint can be applied\non the Score matrix W to enhance the ability of learning parts of NMF. In this\nreport, we describe a very simple algorithm, which in effect achieves volume\nminimization, although indirectly.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 13:13:39 GMT"}], "update_date": "2013-12-19", "authors_parsed": [["Fogel", "Paul", ""]]}, {"id": "1312.5179", "submitter": "Leonardo Jost", "authors": "Matthias Hein, Simon Setzer, Leonardo Jost, Syama Sundar Rangapuram", "title": "The Total Variation on Hypergraphs - Learning on Hypergraphs Revisited", "comments": "Long version of paper accepted at NIPS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypergraphs allow one to encode higher-order relationships in data and are\nthus a very flexible modeling tool. Current learning methods are either based\non approximations of the hypergraphs via graphs or on tensor methods which are\nonly applicable under special conditions. In this paper, we present a new\nlearning framework on hypergraphs which fully uses the hypergraph structure.\nThe key element is a family of regularization functionals based on the total\nvariation on hypergraphs.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 15:35:32 GMT"}], "update_date": "2013-12-19", "authors_parsed": [["Hein", "Matthias", ""], ["Setzer", "Simon", ""], ["Jost", "Leonardo", ""], ["Rangapuram", "Syama Sundar", ""]]}, {"id": "1312.5192", "submitter": "Leonardo Jost", "authors": "Leonardo Jost, Simon Setzer, Matthias Hein", "title": "Nonlinear Eigenproblems in Data Analysis - Balanced Graph Cuts and the\n  RatioDCA-Prox", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been recently shown that a large class of balanced graph cuts allows\nfor an exact relaxation into a nonlinear eigenproblem. We review briefly some\nof these results and propose a family of algorithms to compute nonlinear\neigenvectors which encompasses previous work as special cases. We provide a\ndetailed analysis of the properties and the convergence behavior of these\nalgorithms and then discuss their application in the area of balanced graph\ncuts.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 16:01:25 GMT"}, {"version": "v2", "created": "Mon, 24 Mar 2014 10:10:42 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Jost", "Leonardo", ""], ["Setzer", "Simon", ""], ["Hein", "Matthias", ""]]}, {"id": "1312.5198", "submitter": "Ashutosh Modi", "authors": "Ashutosh Modi and Ivan Titov", "title": "Learning Semantic Script Knowledge with Event Embeddings", "comments": "4 Pages, 1 figure, ICLR Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Induction of common sense knowledge about prototypical sequences of events\nhas recently received much attention. Instead of inducing this knowledge in the\nform of graphs, as in much of the previous work, in our method, distributed\nrepresentations of event realizations are computed based on distributed\nrepresentations of predicates and their arguments, and then these\nrepresentations are used to predict prototypical event orderings. The\nparameters of the compositional process for computing the event representations\nand the ranking component of the model are jointly estimated from texts. We\nshow that this approach results in a substantial boost in ordering performance\nwith respect to previous methods.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 16:13:08 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2014 10:42:35 GMT"}, {"version": "v3", "created": "Mon, 17 Feb 2014 17:08:34 GMT"}, {"version": "v4", "created": "Fri, 25 Apr 2014 13:31:53 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Modi", "Ashutosh", ""], ["Titov", "Ivan", ""]]}, {"id": "1312.5258", "submitter": "Vincent Dumoulin", "authors": "Vincent Dumoulin, Ian J. Goodfellow, Aaron Courville, Yoshua Bengio", "title": "On the Challenges of Physical Implementations of RBMs", "comments": null, "journal-ref": "Proc. AAAI 2014, pp. 1199-1205", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzmann machines (RBMs) are powerful machine learning models,\nbut learning and some kinds of inference in the model require sampling-based\napproximations, which, in classical digital computers, are implemented using\nexpensive MCMC. Physical computation offers the opportunity to reduce the cost\nof sampling by building physical systems whose natural dynamics correspond to\ndrawing samples from the desired RBM distribution. Such a system avoids the\nburn-in and mixing cost of a Markov chain. However, hardware implementations of\nthis variety usually entail limitations such as low-precision and limited range\nof the parameters and restrictions on the size and topology of the RBM. We\nconduct software simulations to determine how harmful each of these\nrestrictions is. Our simulations are designed to reproduce aspects of the\nD-Wave quantum computer, but the issues we investigate arise in most forms of\nphysical computation.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 18:30:51 GMT"}, {"version": "v2", "created": "Fri, 24 Oct 2014 19:16:14 GMT"}], "update_date": "2014-10-27", "authors_parsed": [["Dumoulin", "Vincent", ""], ["Goodfellow", "Ian J.", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1312.5271", "submitter": "Michel Fliess", "authors": "Michel Fliess (LIX, AL.I.E.N.), C\\'edric Join (AL.I.E.N., CRAN, INRIA\n  Lille - Nord Europe)", "title": "Systematic and multifactor risk models revisited", "comments": "First Paris Financial Management Conference, Paris : France (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM cs.CE math.LO q-fin.CP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systematic and multifactor risk models are revisited via methods which were\nalready successfully developed in signal processing and in automatic control.\nThe results, which bypass the usual criticisms on those risk modeling, are\nillustrated by several successful computer experiments.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 19:21:24 GMT"}], "update_date": "2013-12-19", "authors_parsed": [["Fliess", "Michel", "", "LIX, AL.I.E.N."], ["Join", "C\u00e9dric", "", "AL.I.E.N., CRAN, INRIA\n  Lille - Nord Europe"]]}, {"id": "1312.5370", "submitter": "Yubin Park", "authors": "Yubin Park and Joydeep Ghosh", "title": "Perturbed Gibbs Samplers for Synthetic Data Release", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a categorical data synthesizer with a quantifiable disclosure\nrisk. Our algorithm, named Perturbed Gibbs Sampler, can handle high-dimensional\ncategorical data that are often intractable to represent as contingency tables.\nThe algorithm extends a multiple imputation strategy for fully synthetic data\nby utilizing feature hashing and non-parametric distribution approximations.\nCalifornia Patient Discharge data are used to demonstrate statistical\nproperties of the proposed synthesizing methodology. Marginal and conditional\ndistributions, as well as the coefficients of regression models built on the\nsynthesized data are compared to those obtained from the original data.\nIntruder scenarios are simulated to evaluate disclosure risks of the\nsynthesized data from multiple angles. Limitations and extensions of the\nproposed algorithm are also discussed.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 23:16:52 GMT"}], "update_date": "2013-12-20", "authors_parsed": [["Park", "Yubin", ""], ["Ghosh", "Joydeep", ""]]}, {"id": "1312.5386", "submitter": "Robert Nishihara", "authors": "Robert Nishihara, Thomas Minka, Daniel Tarlow", "title": "Detecting Parameter Symmetries in Probabilistic Models", "comments": "24 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic models often have parameters that can be translated, scaled,\npermuted, or otherwise transformed without changing the model. These symmetries\ncan lead to strong correlation and multimodality in the posterior distribution\nover the model's parameters, which can pose challenges both for performing\ninference and interpreting the results. In this work, we address the automatic\ndetection of common problematic model symmetries. To do so, we introduce local\nsymmetries, which cover many common cases and are amenable to automatic\ndetection. We show how to derive algorithms to detect several broad classes of\nlocal symmetries. Our algorithms are compatible with probabilistic programming\nconstructs such as arrays, for loops, and if statements, and they scale to\nmodels with many variables.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 01:11:48 GMT"}], "update_date": "2013-12-20", "authors_parsed": [["Nishihara", "Robert", ""], ["Minka", "Thomas", ""], ["Tarlow", "Daniel", ""]]}, {"id": "1312.5394", "submitter": "Michael S. Gashler Ph.D.", "authors": "Michael S. Gashler, Michael R. Smith, Richard Morris, Tony Martinez", "title": "Missing Value Imputation With Unsupervised Backpropagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many data mining and data analysis techniques operate on dense matrices or\ncomplete tables of data. Real-world data sets, however, often contain unknown\nvalues. Even many classification algorithms that are designed to operate with\nmissing values still exhibit deteriorated accuracy. One approach to handling\nmissing values is to fill in (impute) the missing values. In this paper, we\npresent a technique for unsupervised learning called Unsupervised\nBackpropagation (UBP), which trains a multi-layer perceptron to fit to the\nmanifold sampled by a set of observed point-vectors. We evaluate UBP with the\ntask of imputing missing values in datasets, and show that UBP is able to\npredict missing values with significantly lower sum-squared error than other\ncollaborative filtering and imputation techniques. We also demonstrate with 24\ndatasets and 9 supervised learning algorithms that classification accuracy is\nusually higher when randomly-withheld values are imputed using UBP, rather than\nwith other methods.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 02:38:40 GMT"}], "update_date": "2013-12-20", "authors_parsed": [["Gashler", "Michael S.", ""], ["Smith", "Michael R.", ""], ["Morris", "Richard", ""], ["Martinez", "Tony", ""]]}, {"id": "1312.5398", "submitter": "Michael Tetelman", "authors": "Michael Tetelman", "title": "Continuous Learning: Engineering Super Features With Feature Algebras", "comments": "Submitted to ICLR 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a problem of searching a space of predictive models\nfor a given training data set. We propose an iterative procedure for deriving a\nsequence of improving models and a corresponding sequence of sets of non-linear\nfeatures on the original input space. After a finite number of iterations N,\nthe non-linear features become 2^N -degree polynomials on the original space.\nWe show that in a limit of an infinite number of iterations derived non-linear\nfeatures must form an associative algebra: a product of two features is equal\nto a linear combination of features from the same feature space for any given\ninput point. Because each iteration consists of solving a series of convex\nproblems that contain all previous solutions, the likelihood of the models in\nthe sequence is increasing with each iteration while the dimension of the model\nparameter space is set to a limited controlled value.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 03:24:58 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2014 20:32:00 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Tetelman", "Michael", ""]]}, {"id": "1312.5412", "submitter": "Taichi Kiwaki Mr", "authors": "Taichi Kiwaki, Takaki Makino, Kazuyuki Aihara", "title": "Approximated Infomax Early Stopping: Revisiting Gaussian RBMs on Natural\n  Images", "comments": "9 pages with 1 page appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We pursue an early stopping technique that helps Gaussian Restricted\nBoltzmann Machines (GRBMs) to gain good natural image representations in terms\nof overcompleteness and data fitting. GRBMs are widely considered as an\nunsuitable model for natural images because they gain non-overcomplete\nrepresentations which include uniform filters that do not represent useful\nimage features. We have recently found that GRBMs once gain and subsequently\nlose useful filters during their training, contrary to this common perspective.\nWe attribute this phenomenon to a tradeoff between overcompleteness of GRBM\nrepresentations and data fitting. To gain GRBM representations that are\novercomplete and fit data well, we propose a measure for GRBM representation\nquality, approximated mutual information, and an early stopping technique based\non this measure. The proposed method boosts performance of classifiers trained\non GRBM representations.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 05:37:50 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2014 16:45:15 GMT"}, {"version": "v3", "created": "Mon, 6 Jan 2014 20:07:09 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Kiwaki", "Taichi", ""], ["Makino", "Takaki", ""], ["Aihara", "Kazuyuki", ""]]}, {"id": "1312.5465", "submitter": "Shaobo Lin", "authors": "Shaobo Lin, Jinshan Zeng, Jian Fang and Zongben Xu", "title": "Learning rates of $l^q$ coefficient regularization learning with\n  Gaussian kernel", "comments": "26 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization is a well recognized powerful strategy to improve the\nperformance of a learning machine and $l^q$ regularization schemes with\n$0<q<\\infty$ are central in use. It is known that different $q$ leads to\ndifferent properties of the deduced estimators, say, $l^2$ regularization leads\nto smooth estimators while $l^1$ regularization leads to sparse estimators.\nThen, how does the generalization capabilities of $l^q$ regularization learning\nvary with $q$? In this paper, we study this problem in the framework of\nstatistical learning theory and show that implementing $l^q$ coefficient\nregularization schemes in the sample dependent hypothesis space associated with\nGaussian kernel can attain the same almost optimal learning rates for all\n$0<q<\\infty$. That is, the upper and lower bounds of learning rates for $l^q$\nregularization learning are asymptotically identical for all $0<q<\\infty$. Our\nfinding tentatively reveals that, in some modeling contexts, the choice of $q$\nmight not have a strong impact with respect to the generalization capability.\nFrom this perspective, $q$ can be arbitrarily specified, or specified merely by\nother no generalization criteria like smoothness, computational complexity,\nsparsity, etc..\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 10:10:02 GMT"}, {"version": "v2", "created": "Thu, 18 Sep 2014 09:55:10 GMT"}, {"version": "v3", "created": "Thu, 25 Sep 2014 02:31:30 GMT"}], "update_date": "2014-09-26", "authors_parsed": [["Lin", "Shaobo", ""], ["Zeng", "Jinshan", ""], ["Fang", "Jian", ""], ["Xu", "Zongben", ""]]}, {"id": "1312.5578", "submitter": "Li Yao", "authors": "Sherjil Ozair, Li Yao and Yoshua Bengio", "title": "Multimodal Transitions for Generative Stochastic Networks", "comments": "7 figures, 9 pages, submitted to ICLR14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Stochastic Networks (GSNs) have been recently introduced as an\nalternative to traditional probabilistic modeling: instead of parametrizing the\ndata distribution directly, one parametrizes a transition operator for a Markov\nchain whose stationary distribution is an estimator of the data generating\ndistribution. The result of training is therefore a machine that generates\nsamples through this Markov chain. However, the previously introduced GSN\nconsistency theorems suggest that in order to capture a wide class of\ndistributions, the transition operator in general should be multimodal,\nsomething that has not been done before this paper. We introduce for the first\ntime multimodal transition distributions for GSNs, in particular using models\nin the NADE family (Neural Autoregressive Density Estimator) as output\ndistributions of the transition operator. A NADE model is related to an RBM\n(and can thus model multimodal distributions) but its likelihood (and\nlikelihood gradient) can be computed easily. The parameters of the NADE are\nobtained as a learned function of the previous state of the learned Markov\nchain. Experiments clearly illustrate the advantage of such multimodal\ntransition distributions over unimodal GSNs.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 15:08:37 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2013 01:25:05 GMT"}, {"version": "v3", "created": "Mon, 6 Jan 2014 05:39:34 GMT"}, {"version": "v4", "created": "Fri, 24 Jan 2014 22:24:15 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Ozair", "Sherjil", ""], ["Yao", "Li", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1312.5604", "submitter": "Qiang Qiu", "authors": "Qiang Qiu, Guillermo Sapiro", "title": "Learning Transformations for Classification Forests", "comments": "arXiv admin note: text overlap with arXiv:1309.2074", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces a transformation-based learner model for classification\nforests. The weak learner at each split node plays a crucial role in a\nclassification tree. We propose to optimize the splitting objective by learning\na linear transformation on subspaces using nuclear norm as the optimization\ncriteria. The learned linear transformation restores a low-rank structure for\ndata from the same class, and, at the same time, maximizes the separation\nbetween different classes, thereby improving the performance of the split\nfunction. Theoretical and experimental results support the proposed framework.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 16:01:41 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2014 12:24:54 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["Qiu", "Qiang", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1312.5734", "submitter": "Andrew Lan", "authors": "Andrew S. Lan, Christoph Studer and Richard G. Baraniuk", "title": "Time-varying Learning and Content Analytics via Sparse Factor Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose SPARFA-Trace, a new machine learning-based framework for\ntime-varying learning and content analytics for education applications. We\ndevelop a novel message passing-based, blind, approximate Kalman filter for\nsparse factor analysis (SPARFA), that jointly (i) traces learner concept\nknowledge over time, (ii) analyzes learner concept knowledge state transitions\n(induced by interacting with learning resources, such as textbook sections,\nlecture videos, etc, or the forgetting effect), and (iii) estimates the content\norganization and intrinsic difficulty of the assessment questions. These\nquantities are estimated solely from binary-valued (correct/incorrect) graded\nlearner response data and a summary of the specific actions each learner\nperforms (e.g., answering a question or studying a learning resource) at each\ntime instance. Experimental results on two online course datasets demonstrate\nthat SPARFA-Trace is capable of tracing each learner's concept knowledge\nevolution over time, as well as analyzing the quality and content organization\nof learning resources, the question-concept associations, and the question\nintrinsic difficulties. Moreover, we show that SPARFA-Trace achieves comparable\nor better performance in predicting unobserved learner responses than existing\ncollaborative filtering and knowledge tracing approaches for personalized\neducation.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 20:44:44 GMT"}], "update_date": "2013-12-20", "authors_parsed": [["Lan", "Andrew S.", ""], ["Studer", "Christoph", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1312.5753", "submitter": "Matias Carrasco Kind", "authors": "M. Carrasco Kind and R. J. Brunner (Department of Astronomy,\n  University of Illinois at Urbana-Champaign)", "title": "SOMz: photometric redshift PDFs with self organizing maps and random\n  atlas", "comments": "14 pages, 8 figures. Accepted for publication in MNRAS. The code can\n  be found at http://lcdm.astro.illinois.edu/research/SOMZ.html", "journal-ref": null, "doi": "10.1093/mnras/stt2456", "report-no": null, "categories": "astro-ph.IM astro-ph.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore the applicability of the unsupervised machine\nlearning technique of Self Organizing Maps (SOM) to estimate galaxy photometric\nredshift probability density functions (PDFs). This technique takes a\nspectroscopic training set, and maps the photometric attributes, but not the\nredshifts, to a two dimensional surface by using a process of competitive\nlearning where neurons compete to more closely resemble the training data\nmultidimensional space. The key feature of a SOM is that it retains the\ntopology of the input set, revealing correlations between the attributes that\nare not easily identified. We test three different 2D topological mapping:\nrectangular, hexagonal, and spherical, by using data from the DEEP2 survey. We\nalso explore different implementations and boundary conditions on the map and\nalso introduce the idea of a random atlas where a large number of different\nmaps are created and their individual predictions are aggregated to produce a\nmore robust photometric redshift PDF. We also introduced a new metric, the\n$I$-score, which efficiently incorporates different metrics, making it easier\nto compare different results (from different parameters or different\nphotometric redshift codes). We find that by using a spherical topology mapping\nwe obtain a better representation of the underlying multidimensional topology,\nwhich provides more accurate results that are comparable to other,\nstate-of-the-art machine learning algorithms. Our results illustrate that\nunsupervised approaches have great potential for many astronomical problems,\nand in particular for the computation of photometric redshifts.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 20:18:33 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Kind", "M. Carrasco", "", "Department of Astronomy,\n  University of Illinois at Urbana-Champaign"], ["Brunner", "R. J.", "", "Department of Astronomy,\n  University of Illinois at Urbana-Champaign"]]}, {"id": "1312.5766", "submitter": "Seunghak Lee", "authors": "Seunghak Lee, Jin Kyu Kim, Qirong Ho, Garth A. Gibson, Eric P. Xing", "title": "Structure-Aware Dynamic Scheduler for Parallel Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training large machine learning (ML) models with many variables or parameters\ncan take a long time if one employs sequential procedures even with stochastic\nupdates. A natural solution is to turn to distributed computing on a cluster;\nhowever, naive, unstructured parallelization of ML algorithms does not usually\nlead to a proportional speedup and can even result in divergence, because\ndependencies between model elements can attenuate the computational gains from\nparallelization and compromise correctness of inference. Recent efforts toward\nthis issue have benefited from exploiting the static, a priori block structures\nresiding in ML algorithms. In this paper, we take this path further by\nexploring the dynamic block structures and workloads therein present during ML\nprogram execution, which offers new opportunities for improving convergence,\ncorrectness, and load balancing in distributed ML. We propose and showcase a\ngeneral-purpose scheduler, STRADS, for coordinating distributed updates in ML\nalgorithms, which harnesses the aforementioned opportunities in a systematic\nway. We provide theoretical guarantees for our scheduler, and demonstrate its\nefficacy versus static block structures on Lasso and Matrix Factorization.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 22:05:11 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2013 06:20:05 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Lee", "Seunghak", ""], ["Kim", "Jin Kyu", ""], ["Ho", "Qirong", ""], ["Gibson", "Garth A.", ""], ["Xing", "Eric P.", ""]]}, {"id": "1312.5770", "submitter": "Samory Kpotufe", "authors": "Samory Kpotufe, Eleni Sgouritsa, Dominik Janzing, and Bernhard\n  Sch\\\"olkopf", "title": "Consistency of Causal Inference under the Additive Noise Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze a family of methods for statistical causal inference from sample\nunder the so-called Additive Noise Model. While most work on the subject has\nconcentrated on establishing the soundness of the Additive Noise Model, the\nstatistical consistency of the resulting inference methods has received little\nattention. We derive general conditions under which the given family of\ninference methods consistently infers the causal direction in a nonparametric\nsetting.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 22:15:40 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2013 16:59:04 GMT"}, {"version": "v3", "created": "Wed, 5 Feb 2014 03:37:30 GMT"}], "update_date": "2014-02-06", "authors_parsed": [["Kpotufe", "Samory", ""], ["Sgouritsa", "Eleni", ""], ["Janzing", "Dominik", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1312.5799", "submitter": "Peter Richtarik", "authors": "Olivier Fercoq and Peter Richt\\'arik", "title": "Accelerated, Parallel and Proximal Coordinate Descent", "comments": "25 pages, 2 algorithms, 6 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new stochastic coordinate descent method for minimizing the sum\nof convex functions each of which depends on a small number of coordinates\nonly. Our method (APPROX) is simultaneously Accelerated, Parallel and PROXimal;\nthis is the first time such a method is proposed. In the special case when the\nnumber of processors is equal to the number of coordinates, the method\nconverges at the rate $2\\bar{\\omega}\\bar{L} R^2/(k+1)^2 $, where $k$ is the\niteration counter, $\\bar{\\omega}$ is an average degree of separability of the\nloss function, $\\bar{L}$ is the average of Lipschitz constants associated with\nthe coordinates and individual functions in the sum, and $R$ is the distance of\nthe initial point from the minimizer. We show that the method can be\nimplemented without the need to perform full-dimensional vector operations,\nwhich is the major bottleneck of existing accelerated coordinate descent\nmethods. The fact that the method depends on the average degree of\nseparability, and not on the maximum degree of separability, can be attributed\nto the use of new safe large stepsizes, leading to improved expected separable\noverapproximation (ESO). These are of independent interest and can be utilized\nin all existing parallel stochastic coordinate descent algorithms based on the\nconcept of ESO.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 03:00:15 GMT"}, {"version": "v2", "created": "Sat, 1 Mar 2014 17:04:59 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Fercoq", "Olivier", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1312.5845", "submitter": "Takashi Shinozaki", "authors": "Takashi Shinozaki and Yasushi Naruse", "title": "Competitive Learning with Feedforward Supervisory Signal for Pre-trained\n  Multilayered Networks", "comments": "This paper has been withdrawn by the author since the review process\n  for the conference to which it was applied ended", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel learning method for multilayered neural networks which\nuses feedforward supervisory signal and associates classification of a new\ninput with that of pre-trained input. The proposed method effectively uses rich\ninput information in the earlier layer for robust leaning and revising internal\nrepresentation in a multilayer neural network.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 08:24:48 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2013 08:59:56 GMT"}, {"version": "v3", "created": "Mon, 3 Feb 2014 11:09:07 GMT"}, {"version": "v4", "created": "Mon, 17 Feb 2014 19:11:14 GMT"}, {"version": "v5", "created": "Fri, 21 Feb 2014 02:11:46 GMT"}, {"version": "v6", "created": "Fri, 9 May 2014 00:50:33 GMT"}, {"version": "v7", "created": "Mon, 16 Feb 2015 09:37:18 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Shinozaki", "Takashi", ""], ["Naruse", "Yasushi", ""]]}, {"id": "1312.5847", "submitter": "Sergey Plis", "authors": "Sergey M. Plis and Devon R. Hjelm and Ruslan Salakhutdinov and Vince\n  D. Calhoun", "title": "Deep learning for neuroimaging: a validation study", "comments": "ICLR 2014 revisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods have recently made notable advances in the tasks of\nclassification and representation learning. These tasks are important for brain\nimaging and neuroscience discovery, making the methods attractive for porting\nto a neuroimager's toolbox. Success of these methods is, in part, explained by\nthe flexibility of deep learning models. However, this flexibility makes the\nprocess of porting to new areas a difficult parameter optimization problem. In\nthis work we demonstrate our results (and feasible parameter ranges) in\napplication of deep learning methods to structural and functional brain imaging\ndata. We also describe a novel constraint-based approach to visualizing high\ndimensional data. We use it to analyze the effect of parameter choices on data\ntransformations. Our results show that deep learning methods are able to learn\nphysiologically important representations and detect latent relations in\nneuroimaging data.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 08:30:55 GMT"}, {"version": "v2", "created": "Wed, 8 Jan 2014 04:22:55 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2014 16:00:08 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Plis", "Sergey M.", ""], ["Hjelm", "Devon R.", ""], ["Salakhutdinov", "Ruslan", ""], ["Calhoun", "Vince D.", ""]]}, {"id": "1312.5857", "submitter": "Dawen Liang", "authors": "Dawen Liang, Matthew D. Hoffman, Gautham J. Mysore", "title": "A Generative Product-of-Filters Model of Audio", "comments": "ICLR 2014 conference-track submission. Added link to the source code", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the product-of-filters (PoF) model, a generative model that\ndecomposes audio spectra as sparse linear combinations of \"filters\" in the\nlog-spectral domain. PoF makes similar assumptions to those used in the classic\nhomomorphic filtering approach to signal processing, but replaces hand-designed\ndecompositions built of basic signal processing operations with a learned\ndecomposition based on statistical inference. This paper formulates the PoF\nmodel and derives a mean-field method for posterior inference and a variational\nEM algorithm to estimate the model's free parameters. We demonstrate PoF's\npotential for audio processing on a bandwidth expansion task, and show that PoF\ncan serve as an effective unsupervised feature extractor for a speaker\nidentification task.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 08:59:36 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2013 15:22:16 GMT"}, {"version": "v3", "created": "Mon, 17 Feb 2014 06:10:11 GMT"}, {"version": "v4", "created": "Tue, 18 Feb 2014 16:55:01 GMT"}, {"version": "v5", "created": "Tue, 25 Nov 2014 22:26:12 GMT"}], "update_date": "2014-11-27", "authors_parsed": [["Liang", "Dawen", ""], ["Hoffman", "Matthew D.", ""], ["Mysore", "Gautham J.", ""]]}, {"id": "1312.5889", "submitter": "Mikkel Schmidt", "authors": "Mikkel N. Schmidt and Morten M{\\o}rup", "title": "Non-parametric Bayesian modeling of complex networks", "comments": null, "journal-ref": "Signal Processing Magazine, IEEE (Volume:30, Issue:3, Year:2013)", "doi": "10.1109/MSP.2012.2235191", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling structure in complex networks using Bayesian non-parametrics makes\nit possible to specify flexible model structures and infer the adequate model\ncomplexity from the observed data. This paper provides a gentle introduction to\nnon-parametric Bayesian modeling of complex networks: Using an infinite mixture\nmodel as running example we go through the steps of deriving the model as an\ninfinite limit of a finite parametric model, inferring the model parameters by\nMarkov chain Monte Carlo, and checking the model's fit and predictive\nperformance. We explain how advanced non-parametric models for complex networks\ncan be derived and point out relevant literature.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 11:09:33 GMT"}], "update_date": "2013-12-23", "authors_parsed": [["Schmidt", "Mikkel N.", ""], ["M\u00f8rup", "Morten", ""]]}, {"id": "1312.5891", "submitter": "George Karystinos", "authors": "Megasthenis Asteris, Dimitris S. Papailiopoulos, George N. Karystinos", "title": "The Sparse Principal Component of a Constant-rank Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computation of the sparse principal component of a matrix is equivalent\nto the identification of its principal submatrix with the largest maximum\neigenvalue. Finding this optimal submatrix is what renders the problem\n${\\mathcal{NP}}$-hard. In this work, we prove that, if the matrix is positive\nsemidefinite and its rank is constant, then its sparse principal component is\npolynomially computable. Our proof utilizes the auxiliary unit vector technique\nthat has been recently developed to identify problems that are polynomially\nsolvable. Moreover, we use this technique to design an algorithm which, for any\nsparsity value, computes the sparse principal component with complexity\n${\\mathcal O}\\left(N^{D+1}\\right)$, where $N$ and $D$ are the matrix size and\nrank, respectively. Our algorithm is fully parallelizable and memory efficient.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 11:11:03 GMT"}], "update_date": "2013-12-23", "authors_parsed": [["Asteris", "Megasthenis", ""], ["Papailiopoulos", "Dimitris S.", ""], ["Karystinos", "George N.", ""]]}, {"id": "1312.5921", "submitter": "Arto Klami", "authors": "Arto Klami, Guillaume Bouchard and Abhishek Tripathi", "title": "Group-sparse Embeddings in Collective Matrix Factorization", "comments": "9+2 pages, submitted for International Conference on Learning\n  Representations 2014. This version fixes minor typographic mistakes, has one\n  new paragraph on computational efficiency, and describes the algorithm in\n  more detail in the Supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CMF is a technique for simultaneously learning low-rank representations based\non a collection of matrices with shared entities. A typical example is the\njoint modeling of user-item, item-property, and user-feature matrices in a\nrecommender system. The key idea in CMF is that the embeddings are shared\nacross the matrices, which enables transferring information between them. The\nexisting solutions, however, break down when the individual matrices have\nlow-rank structure not shared with others. In this work we present a novel CMF\nsolution that allows each of the matrices to have a separate low-rank structure\nthat is independent of the other matrices, as well as structures that are\nshared only by a subset of them. We compare MAP and variational Bayesian\nsolutions based on alternating optimization algorithms and show that the model\nautomatically infers the nature of each factor using group-wise sparsity. Our\napproach supports in a principled way continuous, binary and count observations\nand is efficient for sparse matrices involving missing data. We illustrate the\nsolution on a number of examples, focusing in particular on an interesting\nuse-case of augmented multi-view learning.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 12:42:15 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2014 09:44:23 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Klami", "Arto", ""], ["Bouchard", "Guillaume", ""], ["Tripathi", "Abhishek", ""]]}, {"id": "1312.6002", "submitter": "Mathias Berglund", "authors": "Mathias Berglund, Tapani Raiko", "title": "Stochastic Gradient Estimate Variance in Contrastive Divergence and\n  Persistent Contrastive Divergence", "comments": "ICLR2014 Workshop Track submission. Rephrased parts of text. Results\n  unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive Divergence (CD) and Persistent Contrastive Divergence (PCD) are\npopular methods for training the weights of Restricted Boltzmann Machines.\nHowever, both methods use an approximate method for sampling from the model\ndistribution. As a side effect, these approximations yield significantly\ndifferent biases and variances for stochastic gradient estimates of individual\ndata points. It is well known that CD yields a biased gradient estimate. In\nthis paper we however show empirically that CD has a lower stochastic gradient\nestimate variance than exact sampling, while the mean of subsequent PCD\nestimates has a higher variance than exact sampling. The results give one\nexplanation to the finding that CD can be used with smaller minibatches or\nhigher learning rates than PCD.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 16:13:54 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2014 11:27:22 GMT"}, {"version": "v3", "created": "Fri, 14 Feb 2014 09:47:11 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Berglund", "Mathias", ""], ["Raiko", "Tapani", ""]]}, {"id": "1312.6026", "submitter": "KyungHyun Cho", "authors": "Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio", "title": "How to Construct Deep Recurrent Neural Networks", "comments": "Accepted at ICLR 2014 (Conference Track). 10-page text + 3-page\n  references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore different ways to extend a recurrent neural network\n(RNN) to a \\textit{deep} RNN. We start by arguing that the concept of depth in\nan RNN is not as clear as it is in feedforward neural networks. By carefully\nanalyzing and understanding the architecture of an RNN, however, we find three\npoints of an RNN which may be made deeper; (1) input-to-hidden function, (2)\nhidden-to-hidden transition and (3) hidden-to-output function. Based on this\nobservation, we propose two novel architectures of a deep RNN which are\northogonal to an earlier attempt of stacking multiple recurrent layers to build\na deep RNN (Schmidhuber, 1992; El Hihi and Bengio, 1996). We provide an\nalternative interpretation of these deep RNNs using a novel framework based on\nneural operators. The proposed deep RNNs are empirically evaluated on the tasks\nof polyphonic music prediction and language modeling. The experimental result\nsupports our claim that the proposed deep RNNs benefit from the depth and\noutperform the conventional, shallow RNNs.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 16:39:39 GMT"}, {"version": "v2", "created": "Sun, 5 Jan 2014 18:39:22 GMT"}, {"version": "v3", "created": "Wed, 29 Jan 2014 22:56:47 GMT"}, {"version": "v4", "created": "Fri, 14 Feb 2014 18:15:46 GMT"}, {"version": "v5", "created": "Thu, 24 Apr 2014 15:17:07 GMT"}], "update_date": "2014-04-25", "authors_parsed": [["Pascanu", "Razvan", ""], ["Gulcehre", "Caglar", ""], ["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1312.6114", "submitter": "Diederik P Kingma M.Sc.", "authors": "Diederik P Kingma, Max Welling", "title": "Auto-Encoding Variational Bayes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we perform efficient inference and learning in directed probabilistic\nmodels, in the presence of continuous latent variables with intractable\nposterior distributions, and large datasets? We introduce a stochastic\nvariational inference and learning algorithm that scales to large datasets and,\nunder some mild differentiability conditions, even works in the intractable\ncase. Our contributions is two-fold. First, we show that a reparameterization\nof the variational lower bound yields a lower bound estimator that can be\nstraightforwardly optimized using standard stochastic gradient methods. Second,\nwe show that for i.i.d. datasets with continuous latent variables per\ndatapoint, posterior inference can be made especially efficient by fitting an\napproximate inference model (also called a recognition model) to the\nintractable posterior using the proposed lower bound estimator. Theoretical\nadvantages are reflected in experimental results.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 20:58:10 GMT"}, {"version": "v10", "created": "Thu, 1 May 2014 15:43:28 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2013 13:19:52 GMT"}, {"version": "v3", "created": "Tue, 24 Dec 2013 16:08:10 GMT"}, {"version": "v4", "created": "Fri, 27 Dec 2013 16:59:25 GMT"}, {"version": "v5", "created": "Thu, 9 Jan 2014 20:28:50 GMT"}, {"version": "v6", "created": "Tue, 21 Jan 2014 19:41:37 GMT"}, {"version": "v7", "created": "Tue, 4 Feb 2014 14:10:27 GMT"}, {"version": "v8", "created": "Mon, 3 Mar 2014 16:41:45 GMT"}, {"version": "v9", "created": "Thu, 10 Apr 2014 16:06:37 GMT"}], "update_date": "2014-05-02", "authors_parsed": [["Kingma", "Diederik P", ""], ["Welling", "Max", ""]]}, {"id": "1312.6115", "submitter": "David Reichert", "authors": "David P. Reichert, Thomas Serre", "title": "Neuronal Synchrony in Complex-Valued Deep Networks", "comments": "ICLR 2014, accepted to conference track. This version: added\n  proceedings note, minor additions", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has recently led to great successes in tasks such as image\nrecognition (e.g Krizhevsky et al., 2012). However, deep networks are still\noutmatched by the power and versatility of the brain, perhaps in part due to\nthe richer neuronal computations available to cortical circuits. The challenge\nis to identify which neuronal mechanisms are relevant, and to find suitable\nabstractions to model them. Here, we show how aspects of spike timing, long\nhypothesized to play a crucial role in cortical information processing, could\nbe incorporated into deep networks to build richer, versatile representations.\n  We introduce a neural network formulation based on complex-valued neuronal\nunits that is not only biologically meaningful but also amenable to a variety\nof deep learning frameworks. Here, units are attributed both a firing rate and\na phase, the latter indicating properties of spike timing. We show how this\nformulation qualitatively captures several aspects thought to be related to\nneuronal synchrony, including gating of information processing and dynamic\nbinding of distributed object representations. Focusing on the latter, we\ndemonstrate the potential of the approach in several simple experiments. Thus,\nneuronal synchrony could be a flexible mechanism that fulfills multiple\nfunctional roles in deep networks.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 20:59:11 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2013 13:34:08 GMT"}, {"version": "v3", "created": "Mon, 30 Dec 2013 18:42:17 GMT"}, {"version": "v4", "created": "Tue, 18 Feb 2014 01:28:47 GMT"}, {"version": "v5", "created": "Sat, 22 Mar 2014 20:25:27 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Reichert", "David P.", ""], ["Serre", "Thomas", ""]]}, {"id": "1312.6116", "submitter": "Jost Tobias Springenberg", "authors": "Jost Tobias Springenberg, Martin Riedmiller", "title": "Improving Deep Neural Networks with Probabilistic Maxout Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a probabilistic variant of the recently introduced maxout unit.\nThe success of deep neural networks utilizing maxout can partly be attributed\nto favorable performance under dropout, when compared to rectified linear\nunits. It however also depends on the fact that each maxout unit performs a\npooling operation over a group of linear transformations and is thus partially\ninvariant to changes in its input. Starting from this observation we ask the\nquestion: Can the desirable properties of maxout units be preserved while\nimproving their invariance properties ? We argue that our probabilistic maxout\n(probout) units successfully achieve this balance. We quantitatively verify\nthis claim and report classification performance matching or exceeding the\ncurrent state of the art on three challenging image classification benchmarks\n(CIFAR-10, CIFAR-100 and SVHN).\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 20:59:15 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2014 11:13:48 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Springenberg", "Jost Tobias", ""], ["Riedmiller", "Martin", ""]]}, {"id": "1312.6120", "submitter": "Andrew Saxe", "authors": "Andrew M. Saxe, James L. McClelland, Surya Ganguli", "title": "Exact solutions to the nonlinear dynamics of learning in deep linear\n  neural networks", "comments": "Submission to ICLR2014. Revised based on reviewer feedback", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cond-mat.dis-nn cs.CV cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the widespread practical success of deep learning methods, our\ntheoretical understanding of the dynamics of learning in deep neural networks\nremains quite sparse. We attempt to bridge the gap between the theory and\npractice of deep learning by systematically analyzing learning dynamics for the\nrestricted case of deep linear neural networks. Despite the linearity of their\ninput-output map, such networks have nonlinear gradient descent dynamics on\nweights that change with the addition of each new hidden layer. We show that\ndeep linear networks exhibit nonlinear learning phenomena similar to those seen\nin simulations of nonlinear networks, including long plateaus followed by rapid\ntransitions to lower error solutions, and faster convergence from greedy\nunsupervised pretraining initial conditions than from random initial\nconditions. We provide an analytical description of these phenomena by finding\nnew exact solutions to the nonlinear dynamics of deep learning. Our theoretical\nanalysis also reveals the surprising finding that as the depth of a network\napproaches infinity, learning speed can nevertheless remain finite: for a\nspecial class of initial conditions on the weights, very deep networks incur\nonly a finite, depth independent, delay in learning speed relative to shallow\nnetworks. We show that, under certain conditions on the training data,\nunsupervised pretraining can find this special class of initial conditions,\nwhile scaled random Gaussian initializations cannot. We further exhibit a new\nclass of random orthogonal initial conditions on weights that, like\nunsupervised pre-training, enjoys depth independent learning times. We further\nshow that these initial conditions also lead to faithful propagation of\ngradients even in deep nonlinear networks, as long as they operate in a special\nregime known as the edge of chaos.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 20:24:00 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2014 20:39:04 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2014 17:26:57 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Saxe", "Andrew M.", ""], ["McClelland", "James L.", ""], ["Ganguli", "Surya", ""]]}, {"id": "1312.6182", "submitter": "Weifeng Liu", "authors": "W. Liu, H. Zhang, D. Tao, Y. Wang, K. Lu", "title": "Large-Scale Paralleled Sparse Principal Component Analysis", "comments": "submitted to Multimedia Tools and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is a statistical technique commonly used\nin multivariate data analysis. However, PCA can be difficult to interpret and\nexplain since the principal components (PCs) are linear combinations of the\noriginal variables. Sparse PCA (SPCA) aims to balance statistical fidelity and\ninterpretability by approximating sparse PCs whose projections capture the\nmaximal variance of original data. In this paper we present an efficient and\nparalleled method of SPCA using graphics processing units (GPUs), which can\nprocess large blocks of data in parallel. Specifically, we construct parallel\nimplementations of the four optimization formulations of the generalized power\nmethod of SPCA (GP-SPCA), one of the most efficient and effective SPCA\napproaches, on a GPU. The parallel GPU implementation of GP-SPCA (using CUBLAS)\nis up to eleven times faster than the corresponding CPU implementation (using\nCBLAS), and up to 107 times faster than a MatLab implementation. Extensive\ncomparative experiments in several real-world datasets confirm that SPCA offers\na practical advantage.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 00:38:02 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Liu", "W.", ""], ["Zhang", "H.", ""], ["Tao", "D.", ""], ["Wang", "Y.", ""], ["Lu", "K.", ""]]}, {"id": "1312.6197", "submitter": "David Warde-Farley", "authors": "David Warde-Farley, Ian J. Goodfellow, Aaron Courville and Yoshua\n  Bengio", "title": "An empirical analysis of dropout in piecewise linear networks", "comments": "Extensive updates; 8 pages plus acknowledgements/references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently introduced dropout training criterion for neural networks has\nbeen the subject of much attention due to its simplicity and remarkable\neffectiveness as a regularizer, as well as its interpretation as a training\nprocedure for an exponentially large ensemble of networks that share\nparameters. In this work we empirically investigate several questions related\nto the efficacy of dropout, specifically as it concerns networks employing the\npopular rectified linear activation function. We investigate the quality of the\ntest time weight-scaling inference procedure by evaluating the geometric\naverage exactly in small models, as well as compare the performance of the\ngeometric mean to the arithmetic mean more commonly employed by ensemble\ntechniques. We explore the effect of tied weights on the ensemble\ninterpretation by training ensembles of masked networks without tied weights.\nFinally, we investigate an alternative criterion based on a biased estimator of\nthe maximum likelihood ensemble gradient.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 03:19:33 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2014 12:26:53 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Warde-Farley", "David", ""], ["Goodfellow", "Ian J.", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1312.6205", "submitter": "Sida Wang", "authors": "Sida I. Wang, Roy Frostig, Percy Liang, Christopher D. Manning", "title": "Relaxations for inference in restricted Boltzmann machines", "comments": "ICLR 2014 workshop track submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a relaxation-based approximate inference algorithm that samples\nnear-MAP configurations of a binary pairwise Markov random field. We experiment\non MAP inference tasks in several restricted Boltzmann machines. We also use\nour underlying sampler to estimate the log-partition function of restricted\nBoltzmann machines and compare against other sampling-based methods.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 04:53:56 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2014 07:50:44 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Wang", "Sida I.", ""], ["Frostig", "Roy", ""], ["Liang", "Percy", ""], ["Manning", "Christopher D.", ""]]}, {"id": "1312.6211", "submitter": "Ian Goodfellow", "authors": "Ian J. Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, Yoshua\n  Bengio", "title": "An Empirical Investigation of Catastrophic Forgetting in Gradient-Based\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Catastrophic forgetting is a problem faced by many machine learning models\nand algorithms. When trained on one task, then trained on a second task, many\nmachine learning models \"forget\" how to perform the first task. This is widely\nbelieved to be a serious problem for neural networks. Here, we investigate the\nextent to which the catastrophic forgetting problem occurs for modern neural\nnetworks, comparing both established and recent gradient-based training\nalgorithms and activation functions. We also examine the effect of the\nrelationship between the first task and the second task on catastrophic\nforgetting. We find that it is always best to train using the dropout\nalgorithm--the dropout algorithm is consistently best at adapting to the new\ntask, remembering the old task, and has the best tradeoff curve between these\ntwo extremes. We find that different tasks and relationships between tasks\nresult in very different rankings of activation function performance. This\nsuggests the choice of activation function should always be cross-validated.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 06:31:41 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2014 21:27:34 GMT"}, {"version": "v3", "created": "Wed, 4 Mar 2015 01:43:31 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Goodfellow", "Ian J.", ""], ["Mirza", "Mehdi", ""], ["Xiao", "Da", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1312.6273", "submitter": "Minyar Sassi", "authors": "Sonia Alouane-Ksouri, Minyar Sassi-Hidri, Kamel Barkaoui", "title": "Parallel architectures for fuzzy triadic similarity learning", "comments": null, "journal-ref": "International Conference on Control, Engineering & Information\n  Technology (CEIT), Proceedings Engineering & Technology, Vol. 1, pp. 121-126,\n  2013", "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a context of document co-clustering, we define a new similarity measure\nwhich iteratively computes similarity while combining fuzzy sets in a\nthree-partite graph. The fuzzy triadic similarity (FT-Sim) model can deal with\nuncertainty offers by the fuzzy sets. Moreover, with the development of the Web\nand the high availability of storage spaces, more and more documents become\naccessible. Documents can be provided from multiple sites and make similarity\ncomputation an expensive processing. This problem motivated us to use parallel\ncomputing. In this paper, we introduce parallel architectures which are able to\ntreat large and multi-source data sets by a sequential, a merging or a\nsplitting-based process. Then, we proceed to a local and a central (or global)\ncomputing using the basic FT-Sim measure. The idea behind these architectures\nis to reduce both time and space complexities thanks to parallel computation.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 16:51:26 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Alouane-Ksouri", "Sonia", ""], ["Sassi-Hidri", "Minyar", ""], ["Barkaoui", "Kamel", ""]]}, {"id": "1312.6317", "submitter": "Aleksandr Aravkin", "authors": "Giulio Bottegal and Aleksandr Y. Aravkin and Hakan Hjalmarsson and\n  Gianluigi Pillonetto", "title": "Outlier robust system identification: a Bayesian kernel-based approach", "comments": "5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an outlier-robust regularized kernel-based method\nfor linear system identification. The unknown impulse response is modeled as a\nzero-mean Gaussian process whose covariance (kernel) is given by the recently\nproposed stable spline kernel, which encodes information on regularity and\nexponential stability. To build robustness to outliers, we model the\nmeasurement noise as realizations of independent Laplacian random variables.\nThe identification problem is cast in a Bayesian framework, and solved by a new\nMarkov Chain Monte Carlo (MCMC) scheme. In particular, exploiting the\nrepresentation of the Laplacian random variables as scale mixtures of\nGaussians, we design a Gibbs sampler which quickly converges to the target\ndistribution. Numerical simulations show a substantial improvement in the\naccuracy of the estimates over state-of-the-art kernel-based methods.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 23:04:51 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2013 20:45:23 GMT"}], "update_date": "2013-12-25", "authors_parsed": [["Bottegal", "Giulio", ""], ["Aravkin", "Aleksandr Y.", ""], ["Hjalmarsson", "Hakan", ""], ["Pillonetto", "Gianluigi", ""]]}, {"id": "1312.6430", "submitter": "Kota Hara", "authors": "Kota Hara and Rama Chellappa", "title": "Growing Regression Forests by Classification: Applications to Object\n  Pose Estimation", "comments": "Paper accepted by ECCV 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel node splitting method for regression trees\nand incorporate it into the regression forest framework. Unlike traditional\nbinary splitting, where the splitting rule is selected from a predefined set of\nbinary splitting rules via trial-and-error, the proposed node splitting method\nfirst finds clusters of the training data which at least locally minimize the\nempirical loss without considering the input space. Then splitting rules which\npreserve the found clusters as much as possible are determined by casting the\nproblem into a classification problem. Consequently, our new node splitting\nmethod enjoys more freedom in choosing the splitting rules, resulting in more\nefficient tree structures. In addition to the Euclidean target space, we\npresent a variant which can naturally deal with a circular target space by the\nproper use of circular statistics. We apply the regression forest employing our\nnode splitting to head pose estimation (Euclidean target space) and car\ndirection estimation (circular target space) and demonstrate that the proposed\nmethod significantly outperforms state-of-the-art methods (38.5% and 22.5%\nerror reduction respectively).\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2013 22:10:42 GMT"}, {"version": "v2", "created": "Tue, 15 Jul 2014 02:51:13 GMT"}], "update_date": "2014-07-16", "authors_parsed": [["Hara", "Kota", ""], ["Chellappa", "Rama", ""]]}, {"id": "1312.6607", "submitter": "Jean-Marc Lasgouttes", "authors": "Victorin Martin, Jean-Marc Lasgouttes, Cyril Furtlehner", "title": "Using Latent Binary Variables for Online Reconstruction of Large Scale\n  Systems", "comments": null, "journal-ref": null, "doi": "10.1007/s10472-015-9470-x", "report-no": null, "categories": "math.PR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a probabilistic graphical model realizing a minimal encoding of\nreal variables dependencies based on possibly incomplete observation and an\nempirical cumulative distribution function per variable. The target application\nis a large scale partially observed system, like e.g. a traffic network, where\na small proportion of real valued variables are observed, and the other\nvariables have to be predicted. Our design objective is therefore to have good\nscalability in a real-time setting. Instead of attempting to encode the\ndependencies of the system directly in the description space, we propose a way\nto encode them in a latent space of binary variables, reflecting a rough\nperception of the observable (congested/non-congested for a traffic road). The\nmethod relies in part on message passing algorithms, i.e. belief propagation,\nbut the core of the work concerns the definition of meaningful latent variables\nassociated to the variables of interest and their pairwise dependencies.\nNumerical experiments demonstrate the applicability of the method in practice.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2013 17:11:59 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Martin", "Victorin", ""], ["Lasgouttes", "Jean-Marc", ""], ["Furtlehner", "Cyril", ""]]}, {"id": "1312.6661", "submitter": "Justin Kinney", "authors": "Justin B. Kinney", "title": "Rapid and deterministic estimation of probability densities using\n  scale-free field theories", "comments": "4 pages, 4 figures. Major revision in v3. The \"Density Estimation\n  using Field Theory\" (DEFT) software package is available at\n  https://github.com/jbkinney/13_deft", "journal-ref": "Phys. Rev. E 90, 011301 (2014)", "doi": "10.1103/PhysRevE.90.011301", "report-no": null, "categories": "physics.data-an cs.LG math.ST q-bio.QM stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question of how best to estimate a continuous probability density from\nfinite data is an intriguing open problem at the interface of statistics and\nphysics. Previous work has argued that this problem can be addressed in a\nnatural way using methods from statistical field theory. Here I describe new\nresults that allow this field-theoretic approach to be rapidly and\ndeterministically computed in low dimensions, making it practical for use in\nday-to-day data analysis. Importantly, this approach does not impose a\nprivileged length scale for smoothness of the inferred probability density, but\nrather learns a natural length scale from the data due to the tradeoff between\ngoodness-of-fit and an Occam factor. Open source software implementing this\nmethod in one and two dimensions is provided.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2013 20:13:35 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2014 20:13:24 GMT"}, {"version": "v3", "created": "Fri, 18 Apr 2014 21:29:41 GMT"}], "update_date": "2014-07-16", "authors_parsed": [["Kinney", "Justin B.", ""]]}, {"id": "1312.6820", "submitter": "Ahmed Farahat", "authors": "Ahmed K. Farahat, Ali Ghodsi, Mohamed S. Kamel", "title": "A Fast Greedy Algorithm for Generalized Column Subset Selection", "comments": "NIPS'13 Workshop on Greedy Algorithms, Frank-Wolfe and Friends", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper defines a generalized column subset selection problem which is\nconcerned with the selection of a few columns from a source matrix A that best\napproximate the span of a target matrix B. The paper then proposes a fast\ngreedy algorithm for solving this problem and draws connections to different\nproblems that can be efficiently solved using the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2013 14:19:43 GMT"}], "update_date": "2013-12-25", "authors_parsed": [["Farahat", "Ahmed K.", ""], ["Ghodsi", "Ali", ""], ["Kamel", "Mohamed S.", ""]]}, {"id": "1312.6956", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Samer Mohammed, Dorra Trabelsi, Latifa Oukhellou,\n  Yacine Amirat", "title": "Joint segmentation of multivariate time series with hidden process\n  regression for human activity recognition", "comments": null, "journal-ref": "Neurocomputing, Volume 120, Pages 633-644, November 2013", "doi": "10.1016/j.neucom.2013.04.003", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of human activity recognition is central for understanding and\npredicting the human behavior, in particular in a prospective of assistive\nservices to humans, such as health monitoring, well being, security, etc. There\nis therefore a growing need to build accurate models which can take into\naccount the variability of the human activities over time (dynamic models)\nrather than static ones which can have some limitations in such a dynamic\ncontext. In this paper, the problem of activity recognition is analyzed through\nthe segmentation of the multidimensional time series of the acceleration data\nmeasured in the 3-d space using body-worn accelerometers. The proposed model\nfor automatic temporal segmentation is a specific statistical latent process\nmodel which assumes that the observed acceleration sequence is governed by\nsequence of hidden (unobserved) activities. More specifically, the proposed\napproach is based on a specific multiple regression model incorporating a\nhidden discrete logistic process which governs the switching from one activity\nto another over time. The model is learned in an unsupervised context by\nmaximizing the observed-data log-likelihood via a dedicated\nexpectation-maximization (EM) algorithm. We applied it on a real-world\nautomatic human activity recognition problem and its performance was assessed\nby performing comparisons with alternative approaches, including well-known\nsupervised static classifiers and the standard hidden Markov model (HMM). The\nobtained results are very encouraging and show that the proposed approach is\nquite competitive even it works in an entirely unsupervised way and does not\nrequires a feature extraction preprocessing step.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 11:08:32 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Mohammed", "Samer", ""], ["Trabelsi", "Dorra", ""], ["Oukhellou", "Latifa", ""], ["Amirat", "Yacine", ""]]}, {"id": "1312.6965", "submitter": "Faicel Chamroukhi", "authors": "Dorra Trabelsi, Samer Mohammed, Faicel Chamroukhi, Latifa Oukhellou,\n  Yacine Amirat", "title": "An Unsupervised Approach for Automatic Activity Recognition based on\n  Hidden Markov Model Regression", "comments": null, "journal-ref": "IEEE Transactions on Automation Science and Engineering, Volume:\n  10, Issue: 3, July 2013, Pages: 829-835", "doi": "10.1109/TASE.2013.2256349", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using supervised machine learning approaches to recognize human activities\nfrom on-body wearable accelerometers generally requires a large amount of\nlabelled data. When ground truth information is not available, too expensive,\ntime consuming or difficult to collect, one has to rely on unsupervised\napproaches. This paper presents a new unsupervised approach for human activity\nrecognition from raw acceleration data measured using inertial wearable\nsensors. The proposed method is based upon joint segmentation of\nmultidimensional time series using a Hidden Markov Model (HMM) in a multiple\nregression context. The model is learned in an unsupervised framework using the\nExpectation-Maximization (EM) algorithm where no activity labels are needed.\nThe proposed method takes into account the sequential appearance of the data.\nIt is therefore adapted for the temporal acceleration data to accurately detect\nthe activities. It allows both segmentation and classification of the human\nactivities. Experimental results are provided to demonstrate the efficiency of\nthe proposed approach with respect to standard supervised and unsupervised\nclassification approaches\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 13:03:12 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Trabelsi", "Dorra", ""], ["Mohammed", "Samer", ""], ["Chamroukhi", "Faicel", ""], ["Oukhellou", "Latifa", ""], ["Amirat", "Yacine", ""]]}, {"id": "1312.6966", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Herv\\'e Glotin, Allou Sam\\'e", "title": "Model-based functional mixture discriminant analysis with hidden process\n  regression for curve classification", "comments": null, "journal-ref": "Neurocomputing, Volume 112, Pages 153-163, July 2013", "doi": "10.1016/j.neucom.2012.10.030", "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the modeling and the classification of functional\ndata presenting regime changes over time. We propose a new model-based\nfunctional mixture discriminant analysis approach based on a specific hidden\nprocess regression model that governs the regime changes over time. Our\napproach is particularly adapted to handle the problem of complex-shaped\nclasses of curves, where each class is potentially composed of several\nsub-classes, and to deal with the regime changes within each homogeneous\nsub-class. The proposed model explicitly integrates the heterogeneity of each\nclass of curves via a mixture model formulation, and the regime changes within\neach sub-class through a hidden logistic process. Each class of complex-shaped\ncurves is modeled by a finite number of homogeneous clusters, each of them\nbeing decomposed into several regimes. The model parameters of each class are\nlearned by maximizing the observed-data log-likelihood by using a dedicated\nexpectation-maximization (EM) algorithm. Comparisons are performed with\nalternative curve classification approaches, including functional linear\ndiscriminant analysis and functional mixture discriminant analysis with\npolynomial regression mixtures and spline regression mixtures. Results obtained\non simulated data and real data show that the proposed approach outperforms the\nalternative approaches in terms of discrimination, and significantly improves\nthe curves approximation.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 13:08:47 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Glotin", "Herv\u00e9", ""], ["Sam\u00e9", "Allou", ""]]}, {"id": "1312.6967", "submitter": "Faicel Chamroukhi", "authors": "Allou Sam\\'e, Faicel Chamroukhi, G\\'erard Govaert, Patrice Aknin", "title": "Model-based clustering and segmentation of time series with changes in\n  regime", "comments": null, "journal-ref": "Advances in Data Analysis and Classification, December 2011,\n  Volume 5, Issue 4, pp 301-321", "doi": "10.1007/s11634-011-0096-5", "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture model-based clustering, usually applied to multidimensional data, has\nbecome a popular approach in many data analysis problems, both for its good\nstatistical properties and for the simplicity of implementation of the\nExpectation-Maximization (EM) algorithm. Within the context of a railway\napplication, this paper introduces a novel mixture model for dealing with time\nseries that are subject to changes in regime. The proposed approach consists in\nmodeling each cluster by a regression model in which the polynomial\ncoefficients vary according to a discrete hidden process. In particular, this\napproach makes use of logistic functions to model the (smooth or abrupt)\ntransitions between regimes. The model parameters are estimated by the maximum\nlikelihood method solved by an Expectation-Maximization algorithm. The proposed\napproach can also be regarded as a clustering approach which operates by\nfinding groups of time series having common changes in regime. In addition to\nproviding a time series partition, it therefore provides a time series\nsegmentation. The problem of selecting the optimal numbers of clusters and\nsegments is solved by means of the Bayesian Information Criterion (BIC). The\nproposed approach is shown to be efficient using a variety of simulated time\nseries and real-world time series of electrical power consumption from rail\nswitching operations.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 13:11:04 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Sam\u00e9", "Allou", ""], ["Chamroukhi", "Faicel", ""], ["Govaert", "G\u00e9rard", ""], ["Aknin", "Patrice", ""]]}, {"id": "1312.6968", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Allou Sam\\'e, G\\'erard Govaert, Patrice Aknin", "title": "A hidden process regression model for functional data description.\n  Application to curve discrimination", "comments": null, "journal-ref": "Neurocomputing, Volume 73, Issues 7-9, March 2010, Pages 1210-1221", "doi": "10.1016/j.neucom.2009.12.023", "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach for functional data description is proposed in this paper. It\nconsists of a regression model with a discrete hidden logistic process which is\nadapted for modeling curves with abrupt or smooth regime changes. The model\nparameters are estimated in a maximum likelihood framework through a dedicated\nExpectation Maximization (EM) algorithm. From the proposed generative model, a\ncurve discrimination rule is derived using the Maximum A Posteriori rule. The\nproposed model is evaluated using simulated curves and real world curves\nacquired during railway switch operations, by performing comparisons with the\npiecewise regression approach in terms of curve modeling and classification.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 13:13:09 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Sam\u00e9", "Allou", ""], ["Govaert", "G\u00e9rard", ""], ["Aknin", "Patrice", ""]]}, {"id": "1312.6969", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Allou Sam\\'e, G\\'erard Govaert, Patrice Aknin", "title": "Time series modeling by a regression approach based on a latent process", "comments": null, "journal-ref": "Neural Networks 22(5-6): 593-602 (2009)", "doi": "10.1016/j.neunet.2009.06.040", "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series are used in many domains including finance, engineering,\neconomics and bioinformatics generally to represent the change of a measurement\nover time. Modeling techniques may then be used to give a synthetic\nrepresentation of such data. A new approach for time series modeling is\nproposed in this paper. It consists of a regression model incorporating a\ndiscrete hidden logistic process allowing for activating smoothly or abruptly\ndifferent polynomial regression models. The model parameters are estimated by\nthe maximum likelihood method performed by a dedicated Expectation Maximization\n(EM) algorithm. The M step of the EM algorithm uses a multi-class Iterative\nReweighted Least-Squares (IRLS) algorithm to estimate the hidden process\nparameters. To evaluate the proposed approach, an experimental study on\nsimulated data and real world data was performed using two alternative\napproaches: a heteroskedastic piecewise regression model using a global\noptimization algorithm based on dynamic programming, and a Hidden Markov\nRegression Model whose parameters are estimated by the Baum-Welch algorithm.\nFinally, in the context of the remote monitoring of components of the French\nrailway infrastructure, and more particularly the switch mechanism, the\nproposed approach has been applied to modeling and classifying time series\nrepresenting the condition measurements acquired during switch operations.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 13:13:55 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Sam\u00e9", "Allou", ""], ["Govaert", "G\u00e9rard", ""], ["Aknin", "Patrice", ""]]}, {"id": "1312.6974", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi", "title": "Piecewise regression mixture for simultaneous functional data clustering\n  and optimal segmentation", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel mixture model-based approach for simultaneous\nclustering and optimal segmentation of functional data which are curves\npresenting regime changes. The proposed model consists in a finite mixture of\npiecewise polynomial regression models. Each piecewise polynomial regression\nmodel is associated with a cluster, and within each cluster, each piecewise\npolynomial component is associated with a regime (i.e., a segment). We derive\ntwo approaches for learning the model parameters. The former is an estimation\napproach and consists in maximizing the observed-data likelihood via a\ndedicated expectation-maximization (EM) algorithm. A fuzzy partition of the\ncurves in K clusters is then obtained at convergence by maximizing the\nposterior cluster probabilities. The latter however is a classification\napproach and optimizes a specific classification likelihood criterion through a\ndedicated classification expectation-maximization (CEM) algorithm. The optimal\ncurve segmentation is performed by using dynamic programming. In the\nclassification approach, both the curve clustering and the optimal segmentation\nare performed simultaneously as the CEM learning proceeds. We show that the\nclassification approach is the probabilistic version that generalizes the\ndeterministic K-means-like algorithm proposed in H\\'ebrail et al. (2010). The\nproposed approach is evaluated using simulated curves and real-world curves.\nComparisons with alternatives including regression mixture models and the\nK-means like algorithm for piecewise regression demonstrate the effectiveness\nof the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 13:54:05 GMT"}, {"version": "v2", "created": "Wed, 30 Apr 2014 23:23:20 GMT"}], "update_date": "2014-05-02", "authors_parsed": [["Chamroukhi", "Faicel", ""]]}, {"id": "1312.6978", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Allou Sam\\'e, G\\'erard Govaert, Patrice Aknin", "title": "Mod\\`ele \\`a processus latent et algorithme EM pour la r\\'egression non\n  lin\\'eaire", "comments": null, "journal-ref": "Revue des Nouvelles Technologies de l'Information (RNTI),\n  Statistique et nouvelles technologies de l'information (2011) 15-32", "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A non linear regression approach which consists of a specific regression\nmodel incorporating a latent process, allowing various polynomial regression\nmodels to be activated preferentially and smoothly, is introduced in this\npaper. The model parameters are estimated by maximum likelihood performed via a\ndedicated expecation-maximization (EM) algorithm. An experimental study using\nsimulated and real data sets reveals good performances of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 14:21:48 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Sam\u00e9", "Allou", ""], ["Govaert", "G\u00e9rard", ""], ["Aknin", "Patrice", ""]]}, {"id": "1312.6994", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Allou Sam\\'e, G\\'erard Govaert, Patrice Aknin", "title": "A regression model with a hidden logistic process for signal\n  parametrization", "comments": "In Proceedings of the XVIIth European Symposium on Artificial Neural\n  Networks, Computational Intelligence and Machine Learning (ESANN), Pages\n  503-508, 2009, Bruges, Belgium", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach for signal parametrization, which consists of a specific\nregression model incorporating a discrete hidden logistic process, is proposed.\nThe model parameters are estimated by the maximum likelihood method performed\nby a dedicated Expectation Maximization (EM) algorithm. The parameters of the\nhidden logistic process, in the inner loop of the EM algorithm, are estimated\nusing a multi-class Iterative Reweighted Least-Squares (IRLS) algorithm. An\nexperimental study using simulated and real data reveals good performances of\nthe proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 18:07:41 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Sam\u00e9", "Allou", ""], ["Govaert", "G\u00e9rard", ""], ["Aknin", "Patrice", ""]]}, {"id": "1312.6995", "submitter": "Sourav Bhattacharya", "authors": "Sourav Bhattacharya and Petteri Nurmi and Nils Hammerla and Thomas\n  Pl\\\"otz", "title": "Towards Using Unlabeled Data in a Sparse-coding Framework for Human\n  Activity Recognition", "comments": "18 pages, 12 figures, Pervasive and Mobile Computing, 2014", "journal-ref": null, "doi": "10.1016/j.pmcj.2014.05.006", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a sparse-coding framework for activity recognition in ubiquitous\nand mobile computing that alleviates two fundamental problems of current\nsupervised learning approaches. (i) It automatically derives a compact, sparse\nand meaningful feature representation of sensor data that does not rely on\nprior expert knowledge and generalizes extremely well across domain boundaries.\n(ii) It exploits unlabeled sample data for bootstrapping effective activity\nrecognizers, i.e., substantially reduces the amount of ground truth annotation\nrequired for model estimation. Such unlabeled data is trivial to obtain, e.g.,\nthrough contemporary smartphones carried by users as they go about their\neveryday activities.\n  Based on the self-taught learning paradigm we automatically derive an\nover-complete set of basis vectors from unlabeled data that captures inherent\npatterns present within activity data. Through projecting raw sensor data onto\nthe feature space defined by such over-complete sets of basis vectors effective\nfeature extraction is pursued. Given these learned feature representations,\nclassification backends are then trained using small amounts of labeled\ntraining data.\n  We study the new approach in detail using two datasets which differ in terms\nof the recognition tasks and sensor modalities. Primarily we focus on\ntransportation mode analysis task, a popular task in mobile-phone based\nsensing. The sparse-coding framework significantly outperforms the\nstate-of-the-art in supervised learning approaches. Furthermore, we demonstrate\nthe great practical potential of the new approach by successfully evaluating\nits generalization capabilities across both domain and sensor modalities by\nconsidering the popular Opportunity dataset. Our feature learning approach\noutperforms state-of-the-art approaches to analyzing activities in daily\nliving.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 18:08:44 GMT"}, {"version": "v2", "created": "Sat, 5 Jul 2014 10:32:32 GMT"}, {"version": "v3", "created": "Wed, 23 Jul 2014 13:39:53 GMT"}], "update_date": "2014-07-24", "authors_parsed": [["Bhattacharya", "Sourav", ""], ["Nurmi", "Petteri", ""], ["Hammerla", "Nils", ""], ["Pl\u00f6tz", "Thomas", ""]]}, {"id": "1312.7001", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Allou Sam\\'e, G\\'erard Govaert and Patrice Aknin", "title": "A regression model with a hidden logistic process for feature extraction\n  from time series", "comments": "In Proceedings of the International Joint Conference on Neural\n  Networks (IJCNN), 2009, Atlanta, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach for feature extraction from time series is proposed in this\npaper. This approach consists of a specific regression model incorporating a\ndiscrete hidden logistic process. The model parameters are estimated by the\nmaximum likelihood method performed by a dedicated Expectation Maximization\n(EM) algorithm. The parameters of the hidden logistic process, in the inner\nloop of the EM algorithm, are estimated using a multi-class Iterative\nReweighted Least-Squares (IRLS) algorithm. A piecewise regression algorithm and\nits iterative variant have also been considered for comparisons. An\nexperimental study using simulated and real data reveals good performances of\nthe proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 18:48:12 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Sam\u00e9", "Allou", ""], ["Govaert", "G\u00e9rard", ""], ["Aknin", "Patrice", ""]]}, {"id": "1312.7003", "submitter": "Faicel Chamroukhi", "authors": "Ra\\\"issa Onanena, Faicel Chamroukhi, Latifa Oukhellou, Denis Candusso,\n  Patrice Aknin, Daniel Hissel", "title": "Supervised learning of a regression model based on latent process.\n  Application to the estimation of fuel cell life time", "comments": "In Proceeding of the 8th IEEE International Conference on Machine\n  Learning and Applications (IEEE ICMLA'09), pages 632-637, 2009, Miami Beach,\n  FL, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a pattern recognition approach aiming to estimate fuel\ncell duration time from electrochemical impedance spectroscopy measurements. It\nconsists in first extracting features from both real and imaginary parts of the\nimpedance spectrum. A parametric model is considered in the case of the real\npart, whereas regression model with latent variables is used in the latter\ncase. Then, a linear regression model using different subsets of extracted\nfeatures is used fo r the estimation of fuel cell time duration. The\nperformances of the proposed approach are evaluated on experimental data set to\nshow its feasibility. This could lead to interesting perspectives for\npredictive maintenance policy of fuel cell.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 18:55:59 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Onanena", "Ra\u00efssa", ""], ["Chamroukhi", "Faicel", ""], ["Oukhellou", "Latifa", ""], ["Candusso", "Denis", ""], ["Aknin", "Patrice", ""], ["Hissel", "Daniel", ""]]}, {"id": "1312.7006", "submitter": "Yudong Chen", "authors": "Yudong Chen, Xinyang Yi, Constantine Caramanis", "title": "A Convex Formulation for Mixed Regression with Two Components: Minimax\n  Optimal Rates", "comments": "Added results on minimax lower bounds, which match our upper bounds\n  on recovery errors up to log factors. Appeared in the Conference on Learning\n  Theory (COLT), 2014. (JMLR W&CP 35 :560-604, 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the mixed regression problem with two components, under\nadversarial and stochastic noise. We give a convex optimization formulation\nthat provably recovers the true solution, and provide upper bounds on the\nrecovery errors for both arbitrary noise and stochastic noise settings. We also\ngive matching minimax lower bounds (up to log factors), showing that under\ncertain assumptions, our algorithm is information-theoretically optimal. Our\nresults represent the first tractable algorithm guaranteeing successful\nrecovery with tight bounds on recovery errors and sample complexity.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 19:23:22 GMT"}, {"version": "v2", "created": "Fri, 13 Feb 2015 10:04:51 GMT"}], "update_date": "2015-02-16", "authors_parsed": [["Chen", "Yudong", ""], ["Yi", "Xinyang", ""], ["Caramanis", "Constantine", ""]]}, {"id": "1312.7007", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Her\\'e Glotin, C\\'eline Rabouy", "title": "Functional Mixture Discriminant Analysis with hidden process regression\n  for curve classification", "comments": "In Proceedings of the XXth European Symposium on Artificial Neural\n  Networks, Computational Intelligence and Machine Learning (ESANN), Pages\n  281-286, 2012, Bruges, Belgium", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new mixture model-based discriminant analysis approach for\nfunctional data using a specific hidden process regression model. The approach\nallows for fitting flexible curve-models to each class of complex-shaped curves\npresenting regime changes. The model parameters are learned by maximizing the\nobserved-data log-likelihood for each class by using a dedicated\nexpectation-maximization (EM) algorithm. Comparisons on simulated data with\nalternative approaches show that the proposed approach provides better results.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 19:23:39 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Glotin", "Her\u00e9", ""], ["Rabouy", "C\u00e9line", ""]]}, {"id": "1312.7011", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Allou Sam\\'e, G\\'erard Govaert, Patrice Aknin", "title": "Classification automatique de donn\\'ees temporelles en classes\n  ordonn\\'ees", "comments": "in French, 44\\`emes Journ\\'ees de Statistique, SFdS", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method of segmenting temporal data into ordered\nclasses. It is based on mixture models and a discrete latent process, which\nenables to successively activates the classes. The classification can be\nperformed by maximizing the likelihood via the EM algorithm or by\nsimultaneously optimizing the model parameters and the partition by the CEM\nalgorithm. These two algorithms can be seen as alternatives to Fisher's\nalgorithm, which improve its computing time.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 19:45:07 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Sam\u00e9", "Allou", ""], ["Govaert", "G\u00e9rard", ""], ["Aknin", "Patrice", ""]]}, {"id": "1312.7018", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Herv\\'e Glotin", "title": "Mixture model-based functional discriminant analysis for curve\n  classification", "comments": "In Proceedings of the 2012 International Joint Conference on Neural\n  Networks (IJCNN), 2012, Pages: 1-8, Brisbane, Australia", "journal-ref": null, "doi": "10.1109/IJCNN.2012.6252818", "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical approaches for Functional Data Analysis concern the paradigm for\nwhich the individuals are functions or curves rather than finite dimensional\nvectors. In this paper, we particularly focus on the modeling and the\nclassification of functional data which are temporal curves presenting regime\nchanges over time. More specifically, we propose a new mixture model-based\ndiscriminant analysis approach for functional data using a specific hidden\nprocess regression model. Our approach is particularly adapted to both handle\nthe problem of complex-shaped classes of curves, where each class is composed\nof several sub-classes, and to deal with the regime changes within each\nhomogeneous sub-class. The model explicitly integrates the heterogeneity of\neach class of curves via a mixture model formulation, and the regime changes\nwithin each sub-class through a hidden logistic process. The approach allows\ntherefore for fitting flexible curve-models to each class of complex-shaped\ncurves presenting regime changes through an unsupervised learning scheme, to\nautomatically summarize it into a finite number of homogeneous clusters, each\nof them is decomposed into several regimes. The model parameters are learned by\nmaximizing the observed-data log-likelihood for each class by using a dedicated\nexpectation-maximization (EM) algorithm. Comparisons on simulated data and real\ndata with alternative approaches, including functional linear discriminant\nanalysis and functional mixture discriminant analysis with polynomial\nregression mixtures and spline regression mixtures, show that the proposed\napproach provides better results regarding the discrimination results and\nsignificantly improves the curves approximation.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 20:35:20 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Glotin", "Herv\u00e9", ""]]}, {"id": "1312.7022", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi", "title": "Robust EM algorithm for model-based curve clustering", "comments": "In Proceedings of the 2013 International Joint Conference on Neural\n  Networks (IJCNN), 2013, Dallas, TX, USA", "journal-ref": "In Proceedings of the 2013 International Joint Conference on\n  Neural Networks (IJCNN), 2013, Dallas, TX, USA", "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based clustering approaches concern the paradigm of exploratory data\nanalysis relying on the finite mixture model to automatically find a latent\nstructure governing observed data. They are one of the most popular and\nsuccessful approaches in cluster analysis. The mixture density estimation is\ngenerally performed by maximizing the observed-data log-likelihood by using the\nexpectation-maximization (EM) algorithm. However, it is well-known that the EM\nalgorithm initialization is crucial. In addition, the standard EM algorithm\nrequires the number of clusters to be known a priori. Some solutions have been\nprovided in [31, 12] for model-based clustering with Gaussian mixture models\nfor multivariate data. In this paper we focus on model-based curve clustering\napproaches, when the data are curves rather than vectorial data, based on\nregression mixtures. We propose a new robust EM algorithm for clustering\ncurves. We extend the model-based clustering approach presented in [31] for\nGaussian mixture models, to the case of curve clustering by regression\nmixtures, including polynomial regression mixtures as well as spline or\nB-spline regressions mixtures. Our approach both handles the problem of\ninitialization and the one of choosing the optimal number of clusters as the EM\nlearning proceeds, rather than in a two-fold scheme. This is achieved by\noptimizing a penalized log-likelihood criterion. A simulation study confirms\nthe potential benefit of the proposed algorithm in terms of robustness\nregarding initialization and funding the actual number of clusters.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 21:04:08 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Chamroukhi", "Faicel", ""]]}, {"id": "1312.7024", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Allou Sam\\'e, Patrice Aknin, G\\'erard Govaert", "title": "Model-based clustering with Hidden Markov Model regression for time\n  series with regime changes", "comments": "In Proceedings of the 2011 International Joint Conference on Neural\n  Networks (IJCNN), 2011, Pages 2814 - 2821, San Jose, California", "journal-ref": null, "doi": "10.1109/IJCNN.2011.6033590", "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel model-based clustering approach for clustering\ntime series which present changes in regime. It consists of a mixture of\npolynomial regressions governed by hidden Markov chains. The underlying hidden\nprocess for each cluster activates successively several polynomial regimes\nduring time. The parameter estimation is performed by the maximum likelihood\nmethod through a dedicated Expectation-Maximization (EM) algorithm. The\nproposed approach is evaluated using simulated time series and real-world time\nseries issued from a railway diagnosis application. Comparisons with existing\napproaches for time series clustering, including the stand EM for Gaussian\nmixtures, $K$-means clustering, the standard mixture of regression models and\nmixture of Hidden Markov Models, demonstrate the effectiveness of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 21:25:41 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Sam\u00e9", "Allou", ""], ["Aknin", "Patrice", ""], ["Govaert", "G\u00e9rard", ""]]}, {"id": "1312.7035", "submitter": "Mohammad Mousavi", "authors": "Mohammad Mousavi, Peter W. Glynn", "title": "Shape-constrained Estimation of Value Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.CE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fully nonparametric method to estimate the value function, via\nsimulation, in the context of expected infinite-horizon discounted rewards for\nMarkov chains. Estimating such value functions plays an important role in\napproximate dynamic programming and applied probability in general. We\nincorporate \"soft information\" into the estimation algorithm, such as knowledge\nof convexity, monotonicity, or Lipchitz constants. In the presence of such\ninformation, a nonparametric estimator for the value function can be computed\nthat is provably consistent as the simulated time horizon tends to infinity. As\nan application, we implement our method on price tolling agreement contracts in\nenergy markets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2013 00:23:05 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Mousavi", "Mohammad", ""], ["Glynn", "Peter W.", ""]]}, {"id": "1312.7077", "submitter": "Ankur Parikh", "authors": "Ankur P. Parikh, Avneesh Saluja, Chris Dyer, Eric P. Xing", "title": "Language Modeling with Power Low Rank Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present power low rank ensembles (PLRE), a flexible framework for n-gram\nlanguage modeling where ensembles of low rank matrices and tensors are used to\nobtain smoothed probability estimates of words in context. Our method can be\nunderstood as a generalization of n-gram modeling to non-integer n, and\nincludes standard techniques such as absolute discounting and Kneser-Ney\nsmoothing as special cases. PLRE training is efficient and our approach\noutperforms state-of-the-art modified Kneser Ney baselines in terms of\nperplexity on large corpora as well as on BLEU score in a downstream machine\ntranslation task.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2013 09:45:02 GMT"}, {"version": "v2", "created": "Fri, 3 Oct 2014 08:28:03 GMT"}], "update_date": "2014-10-06", "authors_parsed": [["Parikh", "Ankur P.", ""], ["Saluja", "Avneesh", ""], ["Dyer", "Chris", ""], ["Xing", "Eric P.", ""]]}, {"id": "1312.7167", "submitter": "Abhishek Kumar", "authors": "Abhishek Kumar, Vikas Sindhwani", "title": "Near-separable Non-negative Matrix Factorization with $\\ell_1$- and\n  Bregman Loss Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a family of tractable NMF algorithms have been proposed under the\nassumption that the data matrix satisfies a separability condition Donoho &\nStodden (2003); Arora et al. (2012). Geometrically, this condition reformulates\nthe NMF problem as that of finding the extreme rays of the conical hull of a\nfinite set of vectors. In this paper, we develop several extensions of the\nconical hull procedures of Kumar et al. (2013) for robust ($\\ell_1$)\napproximations and Bregman divergences. Our methods inherit all the advantages\nof Kumar et al. (2013) including scalability and noise-tolerance. We show that\non foreground-background separation problems in computer vision, robust\nnear-separable NMFs match the performance of Robust PCA, considered state of\nthe art on these problems, with an order of magnitude faster training time. We\nalso demonstrate applications in exemplar selection settings.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2013 01:10:00 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Kumar", "Abhishek", ""], ["Sindhwani", "Vikas", ""]]}, {"id": "1312.7258", "submitter": "Leto Peel", "authors": "Leto Peel", "title": "Active Discovery of Network Roles for Predicting the Classes of Network\n  Nodes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nodes in real world networks often have class labels, or underlying\nattributes, that are related to the way in which they connect to other nodes.\nSometimes this relationship is simple, for instance nodes of the same class are\nmay be more likely to be connected. In other cases, however, this is not true,\nand the way that nodes link in a network exhibits a different, more complex\nrelationship to their attributes. Here, we consider networks in which we know\nhow the nodes are connected, but we do not know the class labels of the nodes\nor how class labels relate to the network links. We wish to identify the best\nsubset of nodes to label in order to learn this relationship between node\nattributes and network links. We can then use this discovered relationship to\naccurately predict the class labels of the rest of the network nodes.\n  We present a model that identifies groups of nodes with similar link\npatterns, which we call network roles, using a generative blockmodel. The model\nthen predicts labels by learning the mapping from network roles to class labels\nusing a maximum margin classifier. We choose a subset of nodes to label\naccording to an iterative margin-based active learning strategy. By integrating\nthe discovery of network roles with the classifier optimisation, the active\nlearning process can adapt the network roles to better represent the network\nfor node classification. We demonstrate the model by exploring a selection of\nreal world networks, including a marine food web and a network of English\nwords. We show that, in contrast to other network classifiers, this model\nachieves good classification accuracy for a range of networks with different\nrelationships between class labels and network links.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2013 13:21:51 GMT"}, {"version": "v2", "created": "Tue, 18 Mar 2014 01:25:28 GMT"}], "update_date": "2014-03-19", "authors_parsed": [["Peel", "Leto", ""]]}, {"id": "1312.7308", "submitter": "Kevin Jamieson", "authors": "Kevin Jamieson, Matthew Malloy, Robert Nowak, S\\'ebastien Bubeck", "title": "lil' UCB : An Optimal Exploration Algorithm for Multi-Armed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes a novel upper confidence bound (UCB) procedure for\nidentifying the arm with the largest mean in a multi-armed bandit game in the\nfixed confidence setting using a small number of total samples. The procedure\ncannot be improved in the sense that the number of samples required to identify\nthe best arm is within a constant factor of a lower bound based on the law of\nthe iterated logarithm (LIL). Inspired by the LIL, we construct our confidence\nbounds to explicitly account for the infinite time horizon of the algorithm. In\naddition, by using a novel stopping time for the algorithm we avoid a union\nbound over the arms that has been observed in other UCB-type algorithms. We\nprove that the algorithm is optimal up to constants and also show through\nsimulations that it provides superior performance with respect to the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2013 18:20:09 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Jamieson", "Kevin", ""], ["Malloy", "Matthew", ""], ["Nowak", "Robert", ""], ["Bubeck", "S\u00e9bastien", ""]]}, {"id": "1312.7335", "submitter": "Balazs Kegl", "authors": "Bal\\'azs K\\'egl", "title": "Correlation-based construction of neighborhood and edge features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by an abstract notion of low-level edge detector filters, we\npropose a simple method of unsupervised feature construction based on pairwise\nstatistics of features. In the first step, we construct neighborhoods of\nfeatures by regrouping features that correlate. Then we use these subsets as\nfilters to produce new neighborhood features. Next, we connect neighborhood\nfeatures that correlate, and construct edge features by subtracting the\ncorrelated neighborhood features of each other. To validate the usefulness of\nthe constructed features, we ran AdaBoost.MH on four multi-class classification\nproblems. Our most significant result is a test error of 0.94% on MNIST with an\nalgorithm which is essentially free of any image-specific priors. On CIFAR-10\nour method is suboptimal compared to today's best deep learning techniques,\nnevertheless, we show that the proposed method outperforms not only boosting on\nthe raw pixels, but also boosting on Haar filters.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 19:36:51 GMT"}, {"version": "v2", "created": "Sun, 16 Feb 2014 23:17:39 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["K\u00e9gl", "Bal\u00e1zs", ""]]}, {"id": "1312.7463", "submitter": "Kartik Audhkhasi", "authors": "Kartik Audhkhasi, Abhinav Sethy, Bhuvana Ramabhadran and Shrikanth S.\n  Narayanan", "title": "Generalized Ambiguity Decomposition for Understanding Ensemble Diversity", "comments": "32 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diversity or complementarity of experts in ensemble pattern recognition and\ninformation processing systems is widely-observed by researchers to be crucial\nfor achieving performance improvement upon fusion. Understanding this link\nbetween ensemble diversity and fusion performance is thus an important research\nquestion. However, prior works have theoretically characterized ensemble\ndiversity and have linked it with ensemble performance in very restricted\nsettings. We present a generalized ambiguity decomposition (GAD) theorem as a\nbroad framework for answering these questions. The GAD theorem applies to a\ngeneric convex ensemble of experts for any arbitrary twice-differentiable loss\nfunction. It shows that the ensemble performance approximately decomposes into\na difference of the average expert performance and the diversity of the\nensemble. It thus provides a theoretical explanation for the\nempirically-observed benefit of fusing outputs from diverse classifiers and\nregressors. It also provides a loss function-dependent, ensemble-dependent, and\ndata-dependent definition of diversity. We present extensions of this\ndecomposition to common regression and classification loss functions, and\nreport a simulation-based analysis of the diversity term and the accuracy of\nthe decomposition. We finally present experiments on standard pattern\nrecognition data sets which indicate the accuracy of the decomposition for\nreal-world classification and regression problems.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2013 19:18:44 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Audhkhasi", "Kartik", ""], ["Sethy", "Abhinav", ""], ["Ramabhadran", "Bhuvana", ""], ["Narayanan", "Shrikanth S.", ""]]}, {"id": "1312.7485", "submitter": "Elias Bareinboim", "authors": "Elias Bareinboim and Judea Pearl", "title": "A General Algorithm for Deciding Transportability of Experimental\n  Results", "comments": null, "journal-ref": "Journal of Causal Inference, 2013; 1(1): 107-134", "doi": "10.1515/jci-2012-0004", "report-no": null, "categories": "cs.AI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalizing empirical findings to new environments, settings, or populations\nis essential in most scientific explorations. This article treats a particular\nproblem of generalizability, called \"transportability\", defined as a license to\ntransfer information learned in experimental studies to a different population,\non which only observational studies can be conducted. Given a set of\nassumptions concerning commonalities and differences between the two\npopulations, Pearl and Bareinboim (2011) derived sufficient conditions that\npermit such transfer to take place. This article summarizes their findings and\nsupplements them with an effective procedure for deciding when and how\ntransportability is feasible. It establishes a necessary and sufficient\ncondition for deciding when causal effects in the target population are\nestimable from both the statistical information available and the causal\ninformation transferred from the experiments. The article further provides a\ncomplete algorithm for computing the transport formula, that is, a way of\ncombining observational and experimental information to synthesize bias-free\nestimate of the desired causal relation. Finally, the article examines the\ndifferences between transportability and other variants of generalizability.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2013 00:54:47 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Bareinboim", "Elias", ""], ["Pearl", "Judea", ""]]}, {"id": "1312.7559", "submitter": "Nam Lee", "authors": "Nam H. Lee and Runze Tang and Carey E. Priebe and Michael Rosen", "title": "A model selection approach for clustering a multinomial sequence with\n  non-negative factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a problem of clustering a sequence of multinomial observations by\nway of a model selection criterion. We propose a form of a penalty term for the\nmodel selection procedure. Our approach subsumes both the conventional AIC and\nBIC criteria but also extends the conventional criteria in a way that it can be\napplicable also to a sequence of sparse multinomial observations, where even\nwithin a same cluster, the number of multinomial trials may be different for\ndifferent observations. In addition, as a preliminary estimation step to\nmaximum likelihood estimation, and more generally, to maximum $L_{q}$\nestimation, we propose to use reduced rank projection in combination with\nnon-negative factorization. We motivate our approach by showing that our model\nselection criterion and preliminary estimation step yield consistent estimates\nunder simplifying assumptions. We also illustrate our approach through\nnumerical experiments using real and simulated data.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2013 17:11:47 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2014 17:08:09 GMT"}, {"version": "v3", "created": "Sat, 1 Feb 2014 15:05:10 GMT"}, {"version": "v4", "created": "Fri, 14 Feb 2014 02:58:46 GMT"}, {"version": "v5", "created": "Tue, 9 Sep 2014 13:53:40 GMT"}, {"version": "v6", "created": "Sat, 10 Jan 2015 21:54:46 GMT"}, {"version": "v7", "created": "Fri, 14 Aug 2015 13:30:34 GMT"}], "update_date": "2015-08-17", "authors_parsed": [["Lee", "Nam H.", ""], ["Tang", "Runze", ""], ["Priebe", "Carey E.", ""], ["Rosen", "Michael", ""]]}, {"id": "1312.7604", "submitter": "Sohan Seth", "authors": "Sohan Seth, Manuel J. A. Eugster", "title": "Probabilistic Archetypal Analysis", "comments": "24 pages; added literature review and visualization", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Archetypal analysis represents a set of observations as convex combinations\nof pure patterns, or archetypes. The original geometric formulation of finding\narchetypes by approximating the convex hull of the observations assumes them to\nbe real valued. This, unfortunately, is not compatible with many practical\nsituations. In this paper we revisit archetypal analysis from the basic\nprinciples, and propose a probabilistic framework that accommodates other\nobservation types such as integers, binary, and probability vectors. We\ncorroborate the proposed methodology with convincing real-world applications on\nfinding archetypal winter tourists based on binary survey data, archetypal\ndisaster-affected countries based on disaster count data, and document\narchetypes based on term-frequency data. We also present an appropriate\nvisualization tool to summarize archetypal analysis solution better.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2013 23:42:57 GMT"}, {"version": "v2", "created": "Mon, 7 Apr 2014 14:40:40 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Seth", "Sohan", ""], ["Eugster", "Manuel J. A.", ""]]}, {"id": "1312.7651", "submitter": "Wei Dai", "authors": "Eric P. Xing, Qirong Ho, Wei Dai, Jin Kyu Kim, Jinliang Wei, Seunghak\n  Lee, Xun Zheng, Pengtao Xie, Abhimanu Kumar, Yaoliang Yu", "title": "Petuum: A New Platform for Distributed Machine Learning on Big Data", "comments": "15 pages, 10 figures, final version in KDD 2015 under the same title", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What is a systematic way to efficiently apply a wide spectrum of advanced ML\nprograms to industrial scale problems, using Big Models (up to 100s of billions\nof parameters) on Big Data (up to terabytes or petabytes)? Modern\nparallelization strategies employ fine-grained operations and scheduling beyond\nthe classic bulk-synchronous processing paradigm popularized by MapReduce, or\neven specialized graph-based execution that relies on graph representations of\nML programs. The variety of approaches tends to pull systems and algorithms\ndesign in different directions, and it remains difficult to find a universal\nplatform applicable to a wide range of ML programs at scale. We propose a\ngeneral-purpose framework that systematically addresses data- and\nmodel-parallel challenges in large-scale ML, by observing that many ML programs\nare fundamentally optimization-centric and admit error-tolerant,\niterative-convergent algorithmic solutions. This presents unique opportunities\nfor an integrative system design, such as bounded-error network synchronization\nand dynamic scheduling based on ML program structure. We demonstrate the\nefficacy of these system designs versus well-known implementations of modern ML\nalgorithms, allowing ML programs to run in much less time and at considerably\nlarger model sizes, even on modestly-sized compute clusters.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2013 08:46:01 GMT"}, {"version": "v2", "created": "Thu, 14 May 2015 21:44:39 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Xing", "Eric P.", ""], ["Ho", "Qirong", ""], ["Dai", "Wei", ""], ["Kim", "Jin Kyu", ""], ["Wei", "Jinliang", ""], ["Lee", "Seunghak", ""], ["Zheng", "Xun", ""], ["Xie", "Pengtao", ""], ["Kumar", "Abhimanu", ""], ["Yu", "Yaoliang", ""]]}, {"id": "1312.7734", "submitter": "Suleiman Khan", "authors": "Suleiman A Khan, Seppo Virtanen, Olli P Kallioniemi, Krister\n  Wennerberg, Antti Poso and Samuel Kaski", "title": "Identification of structural features in chemicals associated with\n  cancer drug response: A systematic data-driven analysis", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Analysis of relationships of drug structure to biological\nresponse is key to understanding off-target and unexpected drug effects, and\nfor developing hypotheses on how to tailor drug thera-pies. New methods are\nrequired for integrated analyses of a large number of chemical features of\ndrugs against the corresponding genome-wide responses of multiple cell models.\nResults: In this paper, we present the first comprehensive multi-set analysis\non how the chemical structure of drugs impacts on ge-nome-wide gene expression\nacross several cancer cell lines (CMap database). The task is formulated as\nsearching for drug response components across multiple cancers to reveal shared\neffects of drugs and the chemical features that may be responsible. The\ncom-ponents can be computed with an extension of a very recent ap-proach called\nGroup Factor Analysis (GFA). We identify 11 compo-nents that link the\nstructural descriptors of drugs with specific gene expression responses\nobserved in the three cell lines, and identify structural groups that may be\nresponsible for the responses. Our method quantitatively outperforms the\nlimited earlier studies on CMap and identifies both the previously reported\nassociations and several interesting novel findings, by taking into account\nmultiple cell lines and advanced 3D structural descriptors. The novel\nobservations include: previously unknown similarities in the effects induced by\n15-delta prostaglandin J2 and HSP90 inhibitors, which are linked to the 3D\ndescriptors of the drugs; and the induction by simvastatin of leukemia-specific\nanti-inflammatory response, resem-bling the effects of corticosteroids.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2013 15:10:52 GMT"}, {"version": "v2", "created": "Tue, 29 Apr 2014 18:47:45 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Khan", "Suleiman A", ""], ["Virtanen", "Seppo", ""], ["Kallioniemi", "Olli P", ""], ["Wennerberg", "Krister", ""], ["Poso", "Antti", ""], ["Kaski", "Samuel", ""]]}, {"id": "1312.7750", "submitter": "Venelin Mitov", "authors": "Venelin Mitov, Manfred Claassen", "title": "A Fused Elastic Net Logistic Regression Model for Multi-Task Binary\n  Classification", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning has shown to significantly enhance the performance of\nmultiple related learning tasks in a variety of situations. We present the\nfused logistic regression, a sparse multi-task learning approach for binary\nclassification. Specifically, we introduce sparsity inducing penalties over\nparameter differences of related logistic regression models to encode\nsimilarity across related tasks. The resulting joint learning task is cast into\na form that lends itself to be efficiently optimized with a recursive variant\nof the alternating direction method of multipliers. We show results on\nsynthetic data and describe the regime of settings where our multi-task\napproach achieves significant improvements over the single task learning\napproach and discuss the implications on applying the fused logistic regression\nin different real world settings.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2013 15:54:44 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Mitov", "Venelin", ""], ["Claassen", "Manfred", ""]]}, {"id": "1312.7853", "submitter": "Ohad Shamir", "authors": "Ohad Shamir, Nathan Srebro, Tong Zhang", "title": "Communication Efficient Distributed Optimization using an Approximate\n  Newton-type Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel Newton-type method for distributed optimization, which is\nparticularly well suited for stochastic optimization and learning problems. For\nquadratic objectives, the method enjoys a linear rate of convergence which\nprovably \\emph{improves} with the data size, requiring an essentially constant\nnumber of iterations under reasonable assumptions. We provide theoretical and\nempirical evidence of the advantages of our method compared to other\napproaches, such as one-shot parameter averaging and ADMM.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2013 20:23:38 GMT"}, {"version": "v2", "created": "Wed, 1 Jan 2014 08:27:56 GMT"}, {"version": "v3", "created": "Sat, 1 Feb 2014 08:10:18 GMT"}, {"version": "v4", "created": "Tue, 13 May 2014 20:24:28 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Shamir", "Ohad", ""], ["Srebro", "Nathan", ""], ["Zhang", "Tong", ""]]}, {"id": "1312.7857", "submitter": "Peter Orbanz", "authors": "Peter Orbanz and Daniel M. Roy", "title": "Bayesian Models of Graphs, Arrays and Other Exchangeable Random\n  Structures", "comments": null, "journal-ref": "IEEE Transactions Pattern Analysis and Machine Intelligence 2015,\n  Vol. 37, No. 2, pp. 437-461", "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The natural habitat of most Bayesian methods is data represented by\nexchangeable sequences of observations, for which de Finetti's theorem provides\nthe theoretical foundation. Dirichlet process clustering, Gaussian process\nregression, and many other parametric and nonparametric Bayesian models fall\nwithin the remit of this framework; many problems arising in modern data\nanalysis do not. This article provides an introduction to Bayesian models of\ngraphs, matrices, and other data that can be modeled by random structures. We\ndescribe results in probability theory that generalize de Finetti's theorem to\nsuch data and discuss their relevance to nonparametric Bayesian modeling. With\nthe basic ideas in place, we survey example models available in the literature;\napplications of such models include collaborative filtering, link prediction,\nand graph and network analysis. We also highlight connections to recent\ndevelopments in graph theory and probability, and sketch the more general\nmathematical foundation of Bayesian methods for other types of data beyond\nsequences and arrays.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2013 20:40:18 GMT"}, {"version": "v2", "created": "Fri, 13 Feb 2015 20:01:00 GMT"}], "update_date": "2015-02-16", "authors_parsed": [["Orbanz", "Peter", ""], ["Roy", "Daniel M.", ""]]}, {"id": "1312.7869", "submitter": "Jinliang Wei", "authors": "Jinliang Wei, Wei Dai, Abhimanu Kumar, Xun Zheng, Qirong Ho and Eric\n  P. Xing", "title": "Consistent Bounded-Asynchronous Parameter Servers for Distributed ML", "comments": "Corrected Title", "journal-ref": null, "doi": null, "report-no": "CMU-ML-13-115", "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed ML applications, shared parameters are usually replicated\namong computing nodes to minimize network overhead. Therefore, proper\nconsistency model must be carefully chosen to ensure algorithm's correctness\nand provide high throughput. Existing consistency models used in\ngeneral-purpose databases and modern distributed ML systems are either too\nloose to guarantee correctness of the ML algorithms or too strict and thus fail\nto fully exploit the computing power of the underlying distributed system.\n  Many ML algorithms fall into the category of \\emph{iterative convergent\nalgorithms} which start from a randomly chosen initial point and converge to\noptima by repeating iteratively a set of procedures. We've found that many such\nalgorithms are to a bounded amount of inconsistency and still converge\ncorrectly. This property allows distributed ML to relax strict consistency\nmodels to improve system performance while theoretically guarantees algorithmic\ncorrectness. In this paper, we present several relaxed consistency models for\nasynchronous parallel computation and theoretically prove their algorithmic\ncorrectness. The proposed consistency models are implemented in a distributed\nparameter server and evaluated in the context of a popular ML application:\ntopic modeling.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2013 20:53:09 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2013 22:07:17 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Wei", "Jinliang", ""], ["Dai", "Wei", ""], ["Kumar", "Abhimanu", ""], ["Zheng", "Xun", ""], ["Ho", "Qirong", ""], ["Xing", "Eric P.", ""]]}]