[{"id": "1304.0401", "submitter": "Karim Pichara Baksai", "authors": "Karim Pichara, Pavlos Protopapas, Dae-Won Kim, Jean-Baptiste Marquette\n  and Patrick Tisserand", "title": "An improved quasar detection method in EROS-2 and MACHO LMC datasets", "comments": null, "journal-ref": "Monthly Notices of the Royal Astronomical Society 427 2012 1284", "doi": "10.1111/j.1365-2966.2012.22061.x", "report-no": null, "categories": "astro-ph.IM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new classification method for quasar identification in the\nEROS-2 and MACHO datasets based on a boosted version of Random Forest\nclassifier. We use a set of variability features including parameters of a\ncontinuous auto regressive model. We prove that continuous auto regressive\nparameters are very important discriminators in the classification process. We\ncreate two training sets (one for EROS-2 and one for MACHO datasets) using\nknown quasars found in the LMC. Our model's accuracy in both EROS-2 and MACHO\ntraining sets is about 90% precision and 86% recall, improving the state of the\nart models accuracy in quasar detection. We apply the model on the complete,\nincluding 28 million objects, EROS-2 and MACHO LMC datasets, finding 1160 and\n2551 candidates respectively. To further validate our list of candidates, we\ncrossmatched our list with a previous 663 known strong candidates, getting 74%\nof matches for MACHO and 40% in EROS-2. The main difference on matching level\nis because EROS-2 is a slightly shallower survey which translates to\nsignificantly lower signal-to-noise ratio lightcurves.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2013 17:50:05 GMT"}], "update_date": "2013-04-02", "authors_parsed": [["Pichara", "Karim", ""], ["Protopapas", "Pavlos", ""], ["Kim", "Dae-Won", ""], ["Marquette", "Jean-Baptiste", ""], ["Tisserand", "Patrick", ""]]}, {"id": "1304.0480", "submitter": "Mihailo Stojnic", "authors": "Mihailo Stojnic", "title": "A problem dependent analysis of SOCP algorithms in noisy compressed\n  sensing", "comments": "arXiv admin note: text overlap with arXiv:1304.0002", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under-determined systems of linear equations with sparse solutions have been\nthe subject of an extensive research in last several years above all due to\nresults of \\cite{CRT,CanRomTao06,DonohoPol}. In this paper we will consider\n\\emph{noisy} under-determined linear systems. In a breakthrough\n\\cite{CanRomTao06} it was established that in \\emph{noisy} systems for any\nlinear level of under-determinedness there is a linear sparsity that can be\n\\emph{approximately} recovered through an SOCP (second order cone programming)\noptimization algorithm so that the approximate solution vector is (in an\n$\\ell_2$-norm sense) guaranteed to be no further from the sparse unknown vector\nthan a constant times the noise. In our recent work \\cite{StojnicGenSocp10} we\nestablished an alternative framework that can be used for statistical\nperformance analysis of the SOCP algorithms. To demonstrate how the framework\nworks we then showed in \\cite{StojnicGenSocp10} how one can use it to precisely\ncharacterize the \\emph{generic} (worst-case) performance of the SOCP. In this\npaper we present a different set of results that can be obtained through the\nframework of \\cite{StojnicGenSocp10}. The results will relate to \\emph{problem\ndependent} performance analysis of SOCP's. We will consider specific types of\nunknown sparse vectors and characterize the SOCP performance when used for\nrecovery of such vectors. We will also show that our theoretical predictions\nare in a solid agreement with the results one can get through numerical\nsimulations.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2013 04:35:58 GMT"}], "update_date": "2013-04-03", "authors_parsed": [["Stojnic", "Mihailo", ""]]}, {"id": "1304.0499", "submitter": "Eric Chi", "authors": "Eric C. Chi, Kenneth Lange", "title": "Splitting Methods for Convex Clustering", "comments": "37 pages, 6 figures", "journal-ref": "Journal of Computational and Graphical Statistics, 24(4):994-1013,\n  2015", "doi": "10.1080/10618600.2014.948181", "report-no": null, "categories": "stat.ML math.NA math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is a fundamental problem in many scientific applications. Standard\nmethods such as $k$-means, Gaussian mixture models, and hierarchical\nclustering, however, are beset by local minima, which are sometimes drastically\nsuboptimal. Recently introduced convex relaxations of $k$-means and\nhierarchical clustering shrink cluster centroids toward one another and ensure\na unique global minimizer. In this work we present two splitting methods for\nsolving the convex clustering problem. The first is an instance of the\nalternating direction method of multipliers (ADMM); the second is an instance\nof the alternating minimization algorithm (AMA). In contrast to previously\nconsidered algorithms, our ADMM and AMA formulations provide simple and unified\nframeworks for solving the convex clustering problem under the previously\nstudied norms and open the door to potentially novel norms. We demonstrate the\nperformance of our algorithm on both simulated and real data examples. While\nthe differences between the two algorithms appear to be minor on the surface,\ncomplexity analysis and numerical experiments show AMA to be significantly more\nefficient.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2013 23:37:20 GMT"}, {"version": "v2", "created": "Tue, 18 Mar 2014 16:23:54 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Chi", "Eric C.", ""], ["Lange", "Kenneth", ""]]}, {"id": "1304.0596", "submitter": "Jairo Fuquene", "authors": "Jairo Fuquene", "title": "A Semiparametric Bayesian Extreme Value Model Using a Dirichlet Process\n  Mixture of Gamma Densities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a model with a Dirichlet process mixture of gamma\ndensities in the bulk part below threshold and a generalized Pareto density in\nthe tail for extreme value estimation. The proposed model is simple and\nflexible allowing us posterior density estimation and posterior inference for\nhigh quantiles. The model works well even for small sample sizes and in the\nabsence of prior information. We evaluate the performance of the proposed model\nthrough a simulation study. Finally, the proposed model is applied to a real\nenvironmental data.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2013 11:49:11 GMT"}], "update_date": "2013-04-03", "authors_parsed": [["Fuquene", "Jairo", ""]]}, {"id": "1304.0682", "submitter": "Cem Aksoylar", "authors": "Cem Aksoylar, George Atia, Venkatesh Saligrama", "title": "Sparse Signal Processing with Linear and Nonlinear Observations: A\n  Unified Shannon-Theoretic Approach", "comments": "Final version submitted to Trans. IT", "journal-ref": null, "doi": "10.1109/TIT.2016.2605122", "report-no": null, "categories": "cs.IT cs.LG math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive fundamental sample complexity bounds for recovering sparse and\nstructured signals for linear and nonlinear observation models including sparse\nregression, group testing, multivariate regression and problems with missing\nfeatures. In general, sparse signal processing problems can be characterized in\nterms of the following Markovian property. We are given a set of $N$ variables\n$X_1,X_2,\\ldots,X_N$, and there is an unknown subset of variables $S \\subset\n\\{1,\\ldots,N\\}$ that are relevant for predicting outcomes $Y$. More\nspecifically, when $Y$ is conditioned on $\\{X_n\\}_{n\\in S}$ it is conditionally\nindependent of the other variables, $\\{X_n\\}_{n \\not \\in S}$. Our goal is to\nidentify the set $S$ from samples of the variables $X$ and the associated\noutcomes $Y$. We characterize this problem as a version of the noisy channel\ncoding problem. Using asymptotic information theoretic analyses, we establish\nmutual information formulas that provide sufficient and necessary conditions on\nthe number of samples required to successfully recover the salient variables.\nThese mutual information expressions unify conditions for both linear and\nnonlinear observations. We then compute sample complexity bounds for the\naforementioned models, based on the mutual information expressions in order to\ndemonstrate the applicability and flexibility of our results in general sparse\nsignal processing models.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2013 16:35:28 GMT"}, {"version": "v2", "created": "Sun, 7 Apr 2013 20:07:20 GMT"}, {"version": "v3", "created": "Fri, 18 Oct 2013 21:57:50 GMT"}, {"version": "v4", "created": "Mon, 11 Nov 2013 21:25:31 GMT"}, {"version": "v5", "created": "Thu, 29 Jan 2015 00:44:11 GMT"}, {"version": "v6", "created": "Sat, 14 Feb 2015 02:03:22 GMT"}, {"version": "v7", "created": "Mon, 18 Jan 2016 21:29:56 GMT"}, {"version": "v8", "created": "Thu, 25 Aug 2016 20:46:55 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Aksoylar", "Cem", ""], ["Atia", "George", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1304.0725", "submitter": "Ashok P", "authors": "P. Ashok, G.M Kadhar Nawaz, E. Elayaraja, V. Vadivel", "title": "Improved Performance of Unsupervised Method by Renovated K-Means", "comments": "7 pages, to strengthen the k means algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is a separation of data into groups of similar objects. Every\ngroup called cluster consists of objects that are similar to one another and\ndissimilar to objects of other groups. In this paper, the K-Means algorithm is\nimplemented by three distance functions and to identify the optimal distance\nfunction for clustering methods. The proposed K-Means algorithm is compared\nwith K-Means, Static Weighted K-Means (SWK-Means) and Dynamic Weighted K-Means\n(DWK-Means) algorithm by using Davis Bouldin index, Execution Time and\nIteration count methods. Experimental results show that the proposed K-Means\nalgorithm performed better on Iris and Wine dataset when compared with other\nthree clustering methods.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2013 05:28:06 GMT"}], "update_date": "2013-04-03", "authors_parsed": [["Ashok", "P.", ""], ["Nawaz", "G. M Kadhar", ""], ["Elayaraja", "E.", ""], ["Vadivel", "V.", ""]]}, {"id": "1304.0828", "submitter": "Philippe Rigollet", "authors": "Quentin Berthet and Philippe Rigollet", "title": "Computational Lower Bounds for Sparse PCA", "comments": "Alternate title: \"Complexity Theoretic Lower Bounds for Sparse\n  Principal Component Detection\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of sparse principal component detection, we bring evidence\ntowards the existence of a statistical price to pay for computational\nefficiency. We measure the performance of a test by the smallest signal\nstrength that it can detect and we propose a computationally efficient method\nbased on semidefinite programming. We also prove that the statistical\nperformance of this test cannot be strictly improved by any computationally\nefficient method. Our results can be viewed as complexity theoretic lower\nbounds conditionally on the assumptions that some instances of the planted\nclique problem cannot be solved in randomized polynomial time.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2013 03:11:07 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2013 16:00:10 GMT"}], "update_date": "2013-04-29", "authors_parsed": [["Berthet", "Quentin", ""], ["Rigollet", "Philippe", ""]]}, {"id": "1304.1014", "submitter": "Emanuele Frandi", "authors": "Hector Allende, Emanuele Frandi, Ricardo Nanculef, Claudio Sartori", "title": "A Novel Frank-Wolfe Algorithm. Analysis and Applications to Large-Scale\n  SVM Training", "comments": "REVISED VERSION (October 2013) -- Title and abstract have been\n  revised. Section 5 was added. Some proofs have been summarized (full-length\n  proofs available in the previous version)", "journal-ref": "Information Sciences 285, 66-99, 2014", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been a renewed interest in the machine learning community\nfor variants of a sparse greedy approximation procedure for concave\noptimization known as {the Frank-Wolfe (FW) method}. In particular, this\nprocedure has been successfully applied to train large-scale instances of\nnon-linear Support Vector Machines (SVMs). Specializing FW to SVM training has\nallowed to obtain efficient algorithms but also important theoretical results,\nincluding convergence analysis of training algorithms and new characterizations\nof model sparsity.\n  In this paper, we present and analyze a novel variant of the FW method based\non a new way to perform away steps, a classic strategy used to accelerate the\nconvergence of the basic FW procedure. Our formulation and analysis is focused\non a general concave maximization problem on the simplex. However, the\nspecialization of our algorithm to quadratic forms is strongly related to some\nclassic methods in computational geometry, namely the Gilbert and MDM\nalgorithms.\n  On the theoretical side, we demonstrate that the method matches the\nguarantees in terms of convergence rate and number of iterations obtained by\nusing classic away steps. In particular, the method enjoys a linear rate of\nconvergence, a result that has been recently proved for MDM on quadratic forms.\n  On the practical side, we provide experiments on several classification\ndatasets, and evaluate the results using statistical tests. Experiments show\nthat our method is faster than the FW method with classic away steps, and works\nwell even in the cases in which classic away steps slow down the algorithm.\nFurthermore, these improvements are obtained without sacrificing the predictive\naccuracy of the obtained SVM model.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2013 17:15:43 GMT"}, {"version": "v2", "created": "Sun, 13 Oct 2013 09:50:26 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Allende", "Hector", ""], ["Frandi", "Emanuele", ""], ["Nanculef", "Ricardo", ""], ["Sartori", "Claudio", ""]]}, {"id": "1304.1209", "submitter": "Ben Fulcher", "authors": "Ben D. Fulcher, Max A. Little, Nick S. Jones", "title": "Highly comparative time-series analysis: The empirical structure of time\n  series and their methods", "comments": null, "journal-ref": "J. R. Soc. Interface vol. 10 no. 83 20130048 (2013)", "doi": "10.1098/rsif.2013.0048", "report-no": null, "categories": "physics.data-an cs.CV physics.bio-ph q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of collecting and organizing sets of observations represents a\ncommon theme throughout the history of science. However, despite the ubiquity\nof scientists measuring, recording, and analyzing the dynamics of different\nprocesses, an extensive organization of scientific time-series data and\nanalysis methods has never been performed. Addressing this, annotated\ncollections of over 35 000 real-world and model-generated time series and over\n9000 time-series analysis algorithms are analyzed in this work. We introduce\nreduced representations of both time series, in terms of their properties\nmeasured by diverse scientific methods, and of time-series analysis methods, in\nterms of their behaviour on empirical time series, and use them to organize\nthese interdisciplinary resources. This new approach to comparing across\ndiverse scientific data and methods allows us to organize time-series datasets\nautomatically according to their properties, retrieve alternatives to\nparticular analysis methods developed in other scientific disciplines, and\nautomate the selection of useful methods for time-series classification and\nregression tasks. The broad scientific utility of these tools is demonstrated\non datasets of electroencephalograms, self-affine time series, heart beat\nintervals, speech signals, and others, in each case contributing novel analysis\ntechniques to the existing literature. Highly comparative techniques that\ncompare across an interdisciplinary literature can thus be used to guide more\nfocused research in time-series analysis for applications across the scientific\ndisciplines.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2013 23:24:02 GMT"}], "update_date": "2013-05-23", "authors_parsed": [["Fulcher", "Ben D.", ""], ["Little", "Max A.", ""], ["Jones", "Nick S.", ""]]}, {"id": "1304.1875", "submitter": "Nicolas Dobigeon", "authors": "Nicolas Dobigeon and Jean-Yves Tourneret and C\\'edric Richard and\n  Jos\\'e C. M. Bermudez and Stephen McLaughlin and Alfred O. Hero", "title": "Nonlinear unmixing of hyperspectral images: models and algorithms", "comments": null, "journal-ref": null, "doi": "10.1109/MSP.2013.2279274", "report-no": null, "categories": "physics.data-an stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When considering the problem of unmixing hyperspectral images, most of the\nliterature in the geoscience and image processing areas relies on the widely\nused linear mixing model (LMM). However, the LMM may be not valid and other\nnonlinear models need to be considered, for instance, when there are\nmulti-scattering effects or intimate interactions. Consequently, over the last\nfew years, several significant contributions have been proposed to overcome the\nlimitations inherent in the LMM. In this paper, we present an overview of\nrecent advances in nonlinear unmixing modeling.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2013 10:21:56 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2013 14:01:41 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Dobigeon", "Nicolas", ""], ["Tourneret", "Jean-Yves", ""], ["Richard", "C\u00e9dric", ""], ["Bermudez", "Jos\u00e9 C. M.", ""], ["McLaughlin", "Stephen", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1304.2024", "submitter": "Trong Nghia Hoang", "authors": "Trong Nghia Hoang and Kian Hsiang Low", "title": "A General Framework for Interacting Bayes-Optimally with Self-Interested\n  Agents using Arbitrary Parametric Model and Model Prior", "comments": "23rd International Joint Conference on Artificial Intelligence (IJCAI\n  2013), Extended version with proofs, 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Bayesian reinforcement learning (BRL) have shown that\nBayes-optimality is theoretically achievable by modeling the environment's\nlatent dynamics using Flat-Dirichlet-Multinomial (FDM) prior. In\nself-interested multi-agent environments, the transition dynamics are mainly\ncontrolled by the other agent's stochastic behavior for which FDM's\nindependence and modeling assumptions do not hold. As a result, FDM does not\nallow the other agent's behavior to be generalized across different states nor\nspecified using prior domain knowledge. To overcome these practical limitations\nof FDM, we propose a generalization of BRL to integrate the general class of\nparametric models and model priors, thus allowing practitioners' domain\nknowledge to be exploited to produce a fine-grained and compact representation\nof the other agent's behavior. Empirical evaluation shows that our approach\noutperforms existing multi-agent reinforcement learning algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2013 17:00:37 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2013 11:34:51 GMT"}, {"version": "v3", "created": "Sun, 16 Mar 2014 15:10:35 GMT"}], "update_date": "2014-03-18", "authors_parsed": [["Hoang", "Trong Nghia", ""], ["Low", "Kian Hsiang", ""]]}, {"id": "1304.2302", "submitter": "Jonathan Malmaud", "authors": "Dan Lovell, Jonathan Malmaud, Ryan P. Adams, Vikash K. Mansinghka", "title": "ClusterCluster: Parallel Markov Chain Monte Carlo for Dirichlet Process\n  Mixtures", "comments": "12 pages, 10 figures. Submitted to ICML 2013 during third submission\n  cycle", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dirichlet process (DP) is a fundamental mathematical tool for Bayesian\nnonparametric modeling, and is widely used in tasks such as density estimation,\nnatural language processing, and time series modeling. Although MCMC inference\nmethods for the DP often provide a gold standard in terms asymptotic accuracy,\nthey can be computationally expensive and are not obviously parallelizable. We\npropose a reparameterization of the Dirichlet process that induces conditional\nindependencies between the atoms that form the random measure. This conditional\nindependence enables many of the Markov chain transition operators for DP\ninference to be simulated in parallel across multiple cores. Applied to mixture\nmodeling, our approach enables the Dirichlet process to simultaneously learn\nclusters that describe the data and superclusters that define the granularity\nof parallelization. Unlike previous approaches, our technique does not require\nalteration of the model and leaves the true posterior distribution invariant.\nIt also naturally lends itself to a distributed software implementation in\nterms of Map-Reduce, which we test in cluster configurations of over 50\nmachines and 100 cores. We present experiments exploring the parallel\nefficiency and convergence properties of our approach on both synthetic and\nreal-world data, including runs on 1MM data vectors in 256 dimensions.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2013 18:34:32 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Lovell", "Dan", ""], ["Malmaud", "Jonathan", ""], ["Adams", "Ryan P.", ""], ["Mansinghka", "Vikash K.", ""]]}, {"id": "1304.2331", "submitter": "Niko Br\\\"ummer", "authors": "Niko Brummer and Johan du Preez", "title": "The PAV algorithm optimizes binary proper scoring rules", "comments": "16 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been much recent interest in application of the\npool-adjacent-violators (PAV) algorithm for the purpose of calibrating the\nprobabilistic outputs of automatic pattern recognition and machine learning\nalgorithms. Special cost functions, known as proper scoring rules form natural\nobjective functions to judge the goodness of such calibration. We show that for\nbinary pattern classifiers, the non-parametric optimization of calibration,\nsubject to a monotonicity constraint, can be solved by PAV and that this\nsolution is optimal for all regular binary proper scoring rules. This extends\nprevious results which were limited to convex binary proper scoring rules. We\nfurther show that this result holds not only for calibration of probabilities,\nbut also for calibration of log-likelihood-ratios, in which case optimality\nholds independently of the prior probabilities of the pattern classes.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2013 19:49:51 GMT"}], "update_date": "2013-04-11", "authors_parsed": [["Brummer", "Niko", ""], ["Preez", "Johan du", ""]]}, {"id": "1304.2363", "submitter": "Suk Wah Kwok", "authors": "Suk Wah Kwok, Chris Carter", "title": "Multiple decision trees", "comments": "Appears in Proceedings of the Fourth Conference on Uncertainty in\n  Artificial Intelligence (UAI1988)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1988-PG-213-220", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes experiments, on two domains, to investigate the effect\nof averaging over predictions of multiple decision trees, instead of using a\nsingle tree. Other authors have pointed out theoretical and commonsense reasons\nfor preferring the multiple tree approach. Ideally, we would like to consider\npredictions from all trees, weighted by their probability. However, there is a\nvast number of different trees, and it is difficult to estimate the probability\nof each tree. We sidestep the estimation problem by using a modified version of\nthe ID3 algorithm to build good trees, and average over only these trees. Our\nresults are encouraging. For each domain, we managed to produce a small number\nof good trees. We find that it is best to average across sets of trees with\ndifferent structure; this usually gives better performance than any of the\nconstituent trees, including the ID3 tree.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2013 19:43:53 GMT"}], "update_date": "2013-04-10", "authors_parsed": [["Kwok", "Suk Wah", ""], ["Carter", "Chris", ""]]}, {"id": "1304.2810", "submitter": "Tianxi Li", "authors": "Jie Cheng, Tianxi Li, Elizaveta Levina, Ji Zhu", "title": "High-dimensional Mixed Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While graphical models for continuous data (Gaussian graphical models) and\ndiscrete data (Ising models) have been extensively studied, there is little\nwork on graphical models linking both continuous and discrete variables (mixed\ndata), which are common in many scientific applications. We propose a novel\ngraphical model for mixed data, which is simple enough to be suitable for\nhigh-dimensional data, yet flexible enough to represent all possible graph\nstructures. We develop a computationally efficient regression-based algorithm\nfor fitting the model by focusing on the conditional log-likelihood of each\nvariable given the rest. The parameters have a natural group structure, and\nsparsity in the fitted graph is attained by incorporating a group lasso\npenalty, approximated by a weighted $\\ell_1$ penalty for computational\nefficiency. We demonstrate the effectiveness of our method through an extensive\nsimulation study and apply it to a music annotation data set (CAL500),\nobtaining a sparse and interpretable graphical model relating the continuous\nfeatures of the audio signal to categorical variables such as genre, emotions,\nand usage associated with particular songs. While we focus on binary discrete\nvariables, we also show that the proposed methodology can be easily extended to\ngeneral discrete variables.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2013 22:39:12 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2016 04:36:19 GMT"}, {"version": "v3", "created": "Fri, 19 Aug 2016 04:06:53 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Cheng", "Jie", ""], ["Li", "Tianxi", ""], ["Levina", "Elizaveta", ""], ["Zhu", "Ji", ""]]}, {"id": "1304.2865", "submitter": "Niko Br\\\"ummer", "authors": "Niko Br\\\"ummer and Edward de Villiers", "title": "The BOSARIS Toolkit: Theory, Algorithms and Code for Surviving the New\n  DCF", "comments": "presented at: The NIST SRE'11 Analysis Workshop, Atlanta, December\n  2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The change of two orders of magnitude in the 'new DCF' of NIST's SRE'10,\nrelative to the 'old DCF' evaluation criterion, posed a difficult challenge for\nparticipants and evaluator alike. Initially, participants were at a loss as to\nhow to calibrate their systems, while the evaluator underestimated the required\nnumber of evaluation trials. After the fact, it is now obvious that both\ncalibration and evaluation require very large sets of trials. This poses the\nchallenges of (i) how to decide what number of trials is enough, and (ii) how\nto process such large data sets with reasonable memory and CPU requirements.\nAfter SRE'10, at the BOSARIS Workshop, we built solutions to these problems\ninto the freely available BOSARIS Toolkit. This paper explains the principles\nand algorithms behind this toolkit. The main contributions of the toolkit are:\n1. The Normalized Bayes Error-Rate Plot, which analyses likelihood- ratio\ncalibration over a wide range of DCF operating points. These plots also help in\njudging the adequacy of the sizes of calibration and evaluation databases. 2.\nEfficient algorithms to compute DCF and minDCF for large score files, over the\nrange of operating points required by these plots. 3. A new score file format,\nwhich facilitates working with very large trial lists. 4. A faster logistic\nregression optimizer for fusion and calibration. 5. A principled way to define\nEER (equal error rate), which is of practical interest when the absolute error\ncount is small.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2013 07:32:31 GMT"}], "update_date": "2013-04-11", "authors_parsed": [["Br\u00fcmmer", "Niko", ""], ["de Villiers", "Edward", ""]]}, {"id": "1304.2986", "submitter": "Ryan J. Tibshirani", "authors": "Ryan J. Tibshirani", "title": "Adaptive piecewise polynomial estimation via trend filtering", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1189 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 1, 285-323", "doi": "10.1214/13-AOS1189", "report-no": "IMS-AOS-AOS1189", "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study trend filtering, a recently proposed tool of Kim et al. [SIAM Rev.\n51 (2009) 339-360] for nonparametric regression. The trend filtering estimate\nis defined as the minimizer of a penalized least squares criterion, in which\nthe penalty term sums the absolute $k$th order discrete derivatives over the\ninput points. Perhaps not surprisingly, trend filtering estimates appear to\nhave the structure of $k$th degree spline functions, with adaptively chosen\nknot points (we say ``appear'' here as trend filtering estimates are not really\nfunctions over continuous domains, and are only defined over the discrete set\nof inputs). This brings to mind comparisons to other nonparametric regression\ntools that also produce adaptive splines; in particular, we compare trend\nfiltering to smoothing splines, which penalize the sum of squared derivatives\nacross input points, and to locally adaptive regression splines [Ann. Statist.\n25 (1997) 387-413], which penalize the total variation of the $k$th derivative.\nEmpirically, we discover that trend filtering estimates adapt to the local\nlevel of smoothness much better than smoothing splines, and further, they\nexhibit a remarkable similarity to locally adaptive regression splines. We also\nprovide theoretical support for these empirical findings; most notably, we\nprove that (with the right choice of tuning parameter) the trend filtering\nestimate converges to the true underlying function at the minimax rate for\nfunctions whose $k$th derivative is of bounded variation. This is done via an\nasymptotic pairing of trend filtering and locally adaptive regression splines,\nwhich have already been shown to converge at the minimax rate [Ann. Statist. 25\n(1997) 387-413]. At the core of this argument is a new result tying together\nthe fitted values of two lasso problems that share the same outcome vector, but\nhave different predictor matrices.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2013 15:02:53 GMT"}, {"version": "v2", "created": "Fri, 21 Mar 2014 13:42:10 GMT"}], "update_date": "2014-03-24", "authors_parsed": [["Tibshirani", "Ryan J.", ""]]}, {"id": "1304.3285", "submitter": "Colorado Reed", "authors": "Colorado Reed and Zoubin Ghahramani", "title": "Scaling the Indian Buffet Process via Submodular Maximization", "comments": "13 pages, 8 figures", "journal-ref": "In ICML 2013: JMLR W&CP 28 (3): 1013-1021, 2013", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference for latent feature models is inherently difficult as the inference\nspace grows exponentially with the size of the input data and number of latent\nfeatures. In this work, we use Kurihara & Welling (2008)'s\nmaximization-expectation framework to perform approximate MAP inference for\nlinear-Gaussian latent feature models with an Indian Buffet Process (IBP)\nprior. This formulation yields a submodular function of the features that\ncorresponds to a lower bound on the model evidence. By adding a constant to\nthis function, we obtain a nonnegative submodular function that can be\nmaximized via a greedy algorithm that obtains at least a one-third\napproximation to the optimal solution. Our inference method scales linearly\nwith the size of the input data, and we show the efficacy of our method on the\nlargest datasets currently analyzed using an IBP model.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2013 13:20:51 GMT"}, {"version": "v2", "created": "Wed, 8 May 2013 20:15:08 GMT"}, {"version": "v3", "created": "Tue, 18 Jun 2013 14:24:58 GMT"}, {"version": "v4", "created": "Wed, 24 Jul 2013 19:20:15 GMT"}], "update_date": "2013-07-25", "authors_parsed": [["Reed", "Colorado", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1304.3568", "submitter": "Pierre Chainais", "authors": "Pierre Chainais and C\\'edric Richard", "title": "Distributed dictionary learning over a sensor network", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of distributed dictionary learning, where a set of\nnodes is required to collectively learn a common dictionary from noisy\nmeasurements. This approach may be useful in several contexts including sensor\nnetworks. Diffusion cooperation schemes have been proposed to solve the\ndistributed linear regression problem. In this work we focus on a\ndiffusion-based adaptive dictionary learning strategy: each node records\nobservations and cooperates with its neighbors by sharing its local dictionary.\nThe resulting algorithm corresponds to a distributed block coordinate descent\n(alternate optimization). Beyond dictionary learning, this strategy could be\nadapted to many matrix factorization problems and generalized to various\nsettings. This article presents our approach and illustrates its efficiency on\nsome numerical examples.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2013 08:47:38 GMT"}], "update_date": "2013-04-15", "authors_parsed": [["Chainais", "Pierre", ""], ["Richard", "C\u00e9dric", ""]]}, {"id": "1304.3577", "submitter": "Richard Savage", "authors": "Richard S. Savage, Zoubin Ghahramani, Jim E. Griffin, Paul Kirk, David\n  L. Wild", "title": "Identifying cancer subtypes in glioblastoma by combining genomic,\n  transcriptomic and epigenomic data", "comments": null, "journal-ref": "International Conference on Machine Learning (ICML) 2012: Workshop\n  on Machine Learning in Genetics and Genomics", "doi": null, "report-no": null, "categories": "q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a nonparametric Bayesian method for disease subtype discovery in\nmulti-dimensional cancer data. Our method can simultaneously analyse a wide\nrange of data types, allowing for both agreement and disagreement between their\nunderlying clustering structure. It includes feature selection and infers the\nmost likely number of disease subtypes, given the data.\n  We apply the method to 277 glioblastoma samples from The Cancer Genome Atlas,\nfor which there are gene expression, copy number variation, methylation and\nmicroRNA data. We identify 8 distinct consensus subtypes and study their\nprognostic value for death, new tumour events, progression and recurrence. The\nconsensus subtypes are prognostic of tumour recurrence (log-rank p-value of\n$3.6 \\times 10^{-4}$ after correction for multiple hypothesis tests). This is\ndriven principally by the methylation data (log-rank p-value of $2.0 \\times\n10^{-3}$) but the effect is strengthened by the other 3 data types,\ndemonstrating the value of integrating multiple data types.\n  Of particular note is a subtype of 47 patients characterised by very low\nlevels of methylation. This subtype has very low rates of tumour recurrence and\nno new events in 10 years of follow up. We also identify a small gene\nexpression subtype of 6 patients that shows particularly poor survival\noutcomes. Additionally, we note a consensus subtype that showly a highly\ndistinctive data signature and suggest that it is therefore a biologically\ndistinct subtype of glioblastoma.\n  The code is available from https://sites.google.com/site/multipledatafusion/\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2013 09:06:45 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2013 09:40:34 GMT"}], "update_date": "2013-04-16", "authors_parsed": [["Savage", "Richard S.", ""], ["Ghahramani", "Zoubin", ""], ["Griffin", "Jim E.", ""], ["Kirk", "Paul", ""], ["Wild", "David L.", ""]]}, {"id": "1304.3708", "submitter": "Yevgeny Seldin", "authors": "Yevgeny Seldin and Peter Bartlett and Koby Crammer", "title": "Advice-Efficient Prediction with Expert Advice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advice-efficient prediction with expert advice (in analogy to label-efficient\nprediction) is a variant of prediction with expert advice game, where on each\nround of the game we are allowed to ask for advice of a limited number $M$ out\nof $N$ experts. This setting is especially interesting when asking for advice\nof every expert on every round is expensive. We present an algorithm for\nadvice-efficient prediction with expert advice that achieves\n$O(\\sqrt{\\frac{N}{M}T\\ln N})$ regret on $T$ rounds of the game.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2013 19:09:56 GMT"}], "update_date": "2013-04-15", "authors_parsed": [["Seldin", "Yevgeny", ""], ["Bartlett", "Peter", ""], ["Crammer", "Koby", ""]]}, {"id": "1304.3745", "submitter": "Khadoudja Ghanem", "authors": "Khadoudja Ghanem", "title": "Towards more accurate clustering method by using dynamic time warping", "comments": "12 pages, 1 figure, 2 tables, journal. arXiv admin note: text overlap\n  with arXiv:1206.3509 by other authors", "journal-ref": "International Journal of Data Mining & Knowledge Management\n  Process (IJDKP) Vol.3, No.2, March 2013", "doi": "10.5121/ijdkp.2013.3207", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An intrinsic problem of classifiers based on machine learning (ML) methods is\nthat their learning time grows as the size and complexity of the training\ndataset increases. For this reason, it is important to have efficient\ncomputational methods and algorithms that can be applied on large datasets,\nsuch that it is still possible to complete the machine learning tasks in\nreasonable time. In this context, we present in this paper a more accurate\nsimple process to speed up ML methods. An unsupervised clustering algorithm is\ncombined with Expectation, Maximization (EM) algorithm to develop an efficient\nHidden Markov Model (HMM) training. The idea of the proposed process consists\nof two steps. In the first step, training instances with similar inputs are\nclustered and a weight factor which represents the frequency of these instances\nis assigned to each representative cluster. Dynamic Time Warping technique is\nused as a dissimilarity function to cluster similar examples. In the second\nstep, all formulas in the classical HMM training algorithm (EM) associated with\nthe number of training instances are modified to include the weight factor in\nappropriate terms. This process significantly accelerates HMM training while\nmaintaining the same initial, transition and emission probabilities matrixes as\nthose obtained with the classical HMM training algorithm. Accordingly, the\nclassification accuracy is preserved. Depending on the size of the training\nset, speedups of up to 2200 times is possible when the size is about 100.000\ninstances. The proposed approach is not limited to training HMMs, but it can be\nemployed for a large variety of MLs methods.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2013 22:23:53 GMT"}], "update_date": "2013-04-16", "authors_parsed": [["Ghanem", "Khadoudja", ""]]}, {"id": "1304.3760", "submitter": "Eric Bair", "authors": "Sheila Gaynor and Eric Bair", "title": "Identification of relevant subtypes via preweighted sparse clustering", "comments": "Version 4: 49 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG q-bio.QM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster analysis methods are used to identify homogeneous subgroups in a data\nset. In biomedical applications, one frequently applies cluster analysis in\norder to identify biologically interesting subgroups. In particular, one may\nwish to identify subgroups that are associated with a particular outcome of\ninterest. Conventional clustering methods generally do not identify such\nsubgroups, particularly when there are a large number of high-variance features\nin the data set. Conventional methods may identify clusters associated with\nthese high-variance features when one wishes to obtain secondary clusters that\nare more interesting biologically or more strongly associated with a particular\noutcome of interest. A modification of sparse clustering can be used to\nidentify such secondary clusters or clusters associated with an outcome of\ninterest. This method correctly identifies such clusters of interest in several\nsimulation scenarios. The method is also applied to a large prospective cohort\nstudy of temporomandibular disorders and a leukemia microarray data set.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2013 02:15:20 GMT"}, {"version": "v2", "created": "Sat, 12 Oct 2013 14:14:13 GMT"}, {"version": "v3", "created": "Wed, 21 Sep 2016 23:59:53 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Gaynor", "Sheila", ""], ["Bair", "Eric", ""]]}, {"id": "1304.4058", "submitter": "Conrad Lee", "authors": "Conrad Lee, Bobo Nick, Ulrik Brandes, P\\'adraig Cunningham", "title": "Link Prediction with Social Vector Clocks", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art link prediction utilizes combinations of complex features\nderived from network panel data. We here show that computationally less\nexpensive features can achieve the same performance in the common scenario in\nwhich the data is available as a sequence of interactions. Our features are\nbased on social vector clocks, an adaptation of the vector-clock concept\nintroduced in distributed computing to social interaction networks. In fact,\nour experiments suggest that by taking into account the order and spacing of\ninteractions, social vector clocks exploit different aspects of link formation\nso that their combination with previous approaches yields the most accurate\npredictor to date.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2013 11:37:56 GMT"}], "update_date": "2013-04-16", "authors_parsed": [["Lee", "Conrad", ""], ["Nick", "Bobo", ""], ["Brandes", "Ulrik", ""], ["Cunningham", "P\u00e1draig", ""]]}, {"id": "1304.4344", "submitter": "Conrad Sanderson", "authors": "Mehrtash T. Harandi, Conrad Sanderson, Richard Hartley, Brian C.\n  Lovell", "title": "Sparse Coding and Dictionary Learning for Symmetric Positive Definite\n  Matrices: A Kernel Approach", "comments": null, "journal-ref": "European Conference on Computer Vision, Lecture Notes in Computer\n  Science (LNCS), Vol. 7573, pp. 216-229, 2012", "doi": "10.1007/978-3-642-33709-3_16", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances suggest that a wide range of computer vision problems can be\naddressed more appropriately by considering non-Euclidean geometry. This paper\ntackles the problem of sparse coding and dictionary learning in the space of\nsymmetric positive definite matrices, which form a Riemannian manifold. With\nthe aid of the recently introduced Stein kernel (related to a symmetric version\nof Bregman matrix divergence), we propose to perform sparse coding by embedding\nRiemannian manifolds into reproducing kernel Hilbert spaces. This leads to a\nconvex and kernel version of the Lasso problem, which can be solved\nefficiently. We furthermore propose an algorithm for learning a Riemannian\ndictionary (used for sparse coding), closely tied to the Stein kernel.\nExperiments on several classification tasks (face recognition, texture\nclassification, person re-identification) show that the proposed sparse coding\napproach achieves notable improvements in discrimination accuracy, in\ncomparison to state-of-the-art methods such as tensor sparse coding, Riemannian\nlocality preserving projection, and symmetry-driven accumulation of local\nfeatures.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2013 06:47:03 GMT"}], "update_date": "2013-04-17", "authors_parsed": [["Harandi", "Mehrtash T.", ""], ["Sanderson", "Conrad", ""], ["Hartley", "Richard", ""], ["Lovell", "Brian C.", ""]]}, {"id": "1304.4549", "submitter": "Arnak Dalalyan", "authors": "Arnak S. Dalalyan (LIGM, CREST), Mohamed Hebiri (LAMA), Katia\n  M\\'eziani (CEREMADE), Joseph Salmon (TSI)", "title": "Learning Heteroscedastic Models by Convex Programming under Group\n  Sparsity", "comments": "Proceedings of the 30 th International Conference on Machine Learning\n  (2013) http://icml.cc/2013/?page_id=43", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popular sparse estimation methods based on $\\ell_1$-relaxation, such as the\nLasso and the Dantzig selector, require the knowledge of the variance of the\nnoise in order to properly tune the regularization parameter. This constitutes\na major obstacle in applying these methods in several frameworks---such as time\nseries, random fields, inverse problems---for which the noise is rarely\nhomoscedastic and its level is hard to know in advance. In this paper, we\npropose a new approach to the joint estimation of the conditional mean and the\nconditional variance in a high-dimensional (auto-) regression setting. An\nattractive feature of the proposed estimator is that it is efficiently\ncomputable even for very large scale problems by solving a second-order cone\nprogram (SOCP). We present theoretical analysis and numerical results assessing\nthe performance of the proposed procedure.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2013 18:54:37 GMT"}], "update_date": "2013-04-17", "authors_parsed": [["Dalalyan", "Arnak S.", "", "LIGM, CREST"], ["Hebiri", "Mohamed", "", "LAMA"], ["M\u00e9ziani", "Katia", "", "CEREMADE"], ["Salmon", "Joseph", "", "TSI"]]}, {"id": "1304.4610", "submitter": "Yuxin Chen", "authors": "Yuxin Chen, Yuejie Chi", "title": "Spectral Compressed Sensing via Structured Matrix Completion", "comments": "accepted to International Conference on Machine Learning (ICML 2013)", "journal-ref": "Journal of Machine Learning Research, W&CP 28 (3) :414-422, 2013", "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper studies the problem of recovering a spectrally sparse object from a\nsmall number of time domain samples. Specifically, the object of interest with\nambient dimension $n$ is assumed to be a mixture of $r$ complex\nmulti-dimensional sinusoids, while the underlying frequencies can assume any\nvalue in the unit disk. Conventional compressed sensing paradigms suffer from\nthe {\\em basis mismatch} issue when imposing a discrete dictionary on the\nFourier representation. To address this problem, we develop a novel\nnonparametric algorithm, called enhanced matrix completion (EMaC), based on\nstructured matrix completion. The algorithm starts by arranging the data into a\nlow-rank enhanced form with multi-fold Hankel structure, then attempts recovery\nvia nuclear norm minimization. Under mild incoherence conditions, EMaC allows\nperfect recovery as soon as the number of samples exceeds the order of\n$\\mathcal{O}(r\\log^{2} n)$. We also show that, in many instances, accurate\ncompletion of a low-rank multi-fold Hankel matrix is possible when the number\nof observed entries is proportional to the information theoretical limits\n(except for a logarithmic gap). The robustness of EMaC against bounded noise\nand its applicability to super resolution are further demonstrated by numerical\nexperiments.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2013 20:26:15 GMT"}, {"version": "v2", "created": "Wed, 1 May 2013 00:29:31 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Chen", "Yuxin", ""], ["Chi", "Yuejie", ""]]}, {"id": "1304.4634", "submitter": "Leonardo Torres", "authors": "Leonardo Torres and Sidnei J. S. Sant'Anna and Corina C. Freitas and\n  Alejandro C. Frery", "title": "Speckle Reduction in Polarimetric SAR Imagery with Stochastic Distances\n  and Nonlocal Means", "comments": "Accepted for publication in Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.GR math.IT stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a technique for reducing speckle in Polarimetric\nSynthetic Aperture Radar (PolSAR) imagery using Nonlocal Means and a\nstatistical test based on stochastic divergences. The main objective is to\nselect homogeneous pixels in the filtering area through statistical tests\nbetween distributions. This proposal uses the complex Wishart model to describe\nPolSAR data, but the technique can be extended to other models. The weights of\nthe location-variant linear filter are function of the p-values of tests which\nverify the hypothesis that two samples come from the same distribution and,\ntherefore, can be used to compute a local mean. The test stems from the family\nof (h-phi) divergences which originated in Information Theory. This novel\ntechnique was compared with the Boxcar, Refined Lee and IDAN filters. Image\nquality assessment methods on simulated and real data are employed to validate\nthe performance of this approach. We show that the proposed filter also\nenhances the polarimetric entropy and preserves the scattering information of\nthe targets.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2013 22:11:53 GMT"}], "update_date": "2013-04-18", "authors_parsed": [["Torres", "Leonardo", ""], ["Sant'Anna", "Sidnei J. S.", ""], ["Freitas", "Corina C.", ""], ["Frery", "Alejandro C.", ""]]}, {"id": "1304.4672", "submitter": "Akshay Krishnamurthy", "authors": "Akshay Krishnamurthy, Aarti Singh", "title": "Low-Rank Matrix and Tensor Completion via Adaptive Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study low rank matrix and tensor completion and propose novel algorithms\nthat employ adaptive sampling schemes to obtain strong performance guarantees.\nOur algorithms exploit adaptivity to identify entries that are highly\ninformative for learning the column space of the matrix (tensor) and\nconsequently, our results hold even when the row space is highly coherent, in\ncontrast with previous analyses. In the absence of noise, we show that one can\nexactly recover a $n \\times n$ matrix of rank $r$ from merely $\\Omega(n\nr^{3/2}\\log(r))$ matrix entries. We also show that one can recover an order $T$\ntensor using $\\Omega(n r^{T-1/2}T^2 \\log(r))$ entries. For noisy recovery, our\nalgorithm consistently estimates a low rank matrix corrupted with noise using\n$\\Omega(n r^{3/2} \\textrm{polylog}(n))$ entries. We complement our study with\nsimulations that verify our theory and demonstrate the scalability of our\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2013 03:05:30 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2013 18:10:19 GMT"}, {"version": "v3", "created": "Sat, 9 Nov 2013 04:41:09 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Krishnamurthy", "Akshay", ""], ["Singh", "Aarti", ""]]}, {"id": "1304.4786", "submitter": "Pedro Galeano", "authors": "Esdras Joseph, Pedro Galeano and Rosa E. Lillo", "title": "The Mahalanobis distance for functional data with applications to\n  classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a general notion of Mahalanobis distance for functional\ndata that extends the classical multivariate concept to situations where the\nobserved data are points belonging to curves generated by a stochastic process.\nMore precisely, a new semi-distance for functional observations that generalize\nthe usual Mahalanobis distance for multivariate datasets is introduced. For\nthat, the development uses a regularized square root inverse operator in\nHilbert spaces. Some of the main characteristics of the functional Mahalanobis\nsemi-distance are shown. Afterwards, new versions of several well known\nfunctional classification procedures are developed using the Mahalanobis\ndistance for functional data as a measure of proximity between functional\nobservations. The performance of several well known functional classification\nprocedures are compared with those methods used in conjunction with the\nMahalanobis distance for functional data, with positive results, through a\nMonte Carlo study and the analysis of two real data examples.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2013 12:13:58 GMT"}], "update_date": "2013-04-18", "authors_parsed": [["Joseph", "Esdras", ""], ["Galeano", "Pedro", ""], ["Lillo", "Rosa E.", ""]]}, {"id": "1304.4806", "submitter": "Daniil Ryabko", "authors": "Daniil Ryabko", "title": "Unsupervised model-free representation learning", "comments": "The update is the journal version appearing in IEEE IT transactions\n  under the title \"Time-series information and unsupervised learning of\n  representations.\" This version includes important corrections and new\n  results. Some of the results (presented in previous versions) were reported\n  at ISIT'13 and ALT'13", "journal-ref": null, "doi": "10.1109/TIT.2019.2961814", "report-no": null, "categories": "cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous control and learning problems face the situation where sequences of\nhigh-dimensional highly dependent data are available but no or little feedback\nis provided to the learner, which makes any inference rather challenging. To\naddress this challenge, we formulate the following problem. Given a series of\nobservations $X_0,\\dots,X_n$ coming from a large (high-dimensional) space\n$\\mathcal X$, find a representation function $f$ mapping $\\mathcal X$ to a\nfinite space $\\mathcal Y$ such that the series $f(X_0),\\dots,f(X_n)$ preserves\nas much information as possible about the original time-series dependence in\n$X_0,\\dots,X_n$. We show that, for stationary time series, the function $f$ can\nbe selected as the one maximizing a certain information criterion that we call\ntime-series information. Some properties of this functions are investigated,\nincluding its uniqueness and consistency of its empirical estimates.\n  Implications for the problem of optimal control are presented.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2013 13:06:59 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2013 14:00:35 GMT"}, {"version": "v3", "created": "Mon, 22 Jul 2013 09:45:16 GMT"}, {"version": "v4", "created": "Wed, 25 Dec 2019 18:08:49 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Ryabko", "Daniil", ""]]}, {"id": "1304.4910", "submitter": "Divyanshu Vats", "authors": "Divyanshu Vats and Robert Nowak", "title": "A Junction Tree Framework for Undirected Graphical Model Selection", "comments": "This paper will appear in the Journal of Machine Learning Research\n  (JMLR). See http://www.ima.umn.edu/~dvats/JunctionTreeUGMS.html for code", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An undirected graphical model is a joint probability distribution defined on\nan undirected graph G*, where the vertices in the graph index a collection of\nrandom variables and the edges encode conditional independence relationships\namong random variables. The undirected graphical model selection (UGMS) problem\nis to estimate the graph G* given observations drawn from the undirected\ngraphical model. This paper proposes a framework for decomposing the UGMS\nproblem into multiple subproblems over clusters and subsets of the separators\nin a junction tree. The junction tree is constructed using a graph that\ncontains a superset of the edges in G*. We highlight three main properties of\nusing junction trees for UGMS. First, different regularization parameters or\ndifferent UGMS algorithms can be used to learn different parts of the graph.\nThis is possible since the subproblems we identify can be solved independently\nof each other. Second, under certain conditions, a junction tree based UGMS\nalgorithm can produce consistent results with fewer observations than the usual\nrequirements of existing algorithms. Third, both our theoretical and\nexperimental results show that the junction tree framework does a significantly\nbetter job at finding the weakest edges in a graph than existing methods. This\nproperty is a consequence of both the first and second properties. Finally, we\nnote that our framework is independent of the choice of the UGMS algorithm and\ncan be used as a wrapper around standard UGMS algorithms for more accurate\ngraph estimation.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2013 18:29:06 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2013 17:30:46 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Vats", "Divyanshu", ""], ["Nowak", "Robert", ""]]}, {"id": "1304.5245", "submitter": "Sayan Dasgupta", "authors": "Sayan Dasgupta, Yair Goldberg and Michael Kosorok", "title": "Feature Elimination in Kernel Machines in moderately high dimensions", "comments": "50 pages, 5 figures, submitted to Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an approach for feature elimination in statistical learning with\nkernel machines, based on recursive elimination of features.We present\ntheoretical properties of this method and show that it is uniformly consistent\nin finding the correct feature space under certain generalized assumptions.We\npresent four case studies to show that the assumptions are met in most\npractical situations and present simulation results to demonstrate performance\nof the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2013 20:25:15 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2015 22:09:57 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Dasgupta", "Sayan", ""], ["Goldberg", "Yair", ""], ["Kosorok", "Michael", ""]]}, {"id": "1304.5299", "submitter": "Anoop Korattikara", "authors": "Anoop Korattikara, Yutian Chen, Max Welling", "title": "Austerity in MCMC Land: Cutting the Metropolis-Hastings Budget", "comments": "v4 - version accepted by ICML2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we make Bayesian posterior MCMC sampling more efficient when faced with\nvery large datasets? We argue that computing the likelihood for N datapoints in\nthe Metropolis-Hastings (MH) test to reach a single binary decision is\ncomputationally inefficient. We introduce an approximate MH rule based on a\nsequential hypothesis test that allows us to accept or reject samples with high\nconfidence using only a fraction of the data required for the exact MH rule.\nWhile this method introduces an asymptotic bias, we show that this bias can be\ncontrolled and is more than offset by a decrease in variance due to our ability\nto draw more samples per unit of time.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2013 02:51:52 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2013 21:13:59 GMT"}, {"version": "v3", "created": "Fri, 19 Jul 2013 18:05:53 GMT"}, {"version": "v4", "created": "Fri, 14 Feb 2014 07:42:15 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Korattikara", "Anoop", ""], ["Chen", "Yutian", ""], ["Welling", "Max", ""]]}, {"id": "1304.5350", "submitter": "Emile Contal", "authors": "Emile Contal and David Buffoni and Alexandre Robicquet and Nicolas\n  Vayatis", "title": "Parallel Gaussian Process Optimization with Upper Confidence Bound and\n  Pure Exploration", "comments": null, "journal-ref": "Proceedings of ECML 2013, pp.225-240", "doi": "10.1007/978-3-642-40988-2_15", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the challenge of maximizing an unknown function f\nfor which evaluations are noisy and are acquired with high cost. An iterative\nprocedure uses the previous measures to actively select the next estimation of\nf which is predicted to be the most useful. We focus on the case where the\nfunction can be evaluated in parallel with batches of fixed size and analyze\nthe benefit compared to the purely sequential procedure in terms of cumulative\nregret. We introduce the Gaussian Process Upper Confidence Bound and Pure\nExploration algorithm (GP-UCB-PE) which combines the UCB strategy and Pure\nExploration in the same batch of evaluations along the parallel iterations. We\nprove theoretical upper bounds on the regret with batches of size K for this\nprocedure which show the improvement of the order of sqrt{K} for fixed\niteration cost over purely sequential versions. Moreover, the multiplicative\nconstants involved have the property of being dimension-free. We also confirm\nempirically the efficiency of GP-UCB-PE on real and synthetic problems compared\nto state-of-the-art competitors.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2013 09:11:34 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2013 14:35:17 GMT"}, {"version": "v3", "created": "Mon, 2 Sep 2013 07:57:23 GMT"}], "update_date": "2013-09-03", "authors_parsed": [["Contal", "Emile", ""], ["Buffoni", "David", ""], ["Robicquet", "Alexandre", ""], ["Vayatis", "Nicolas", ""]]}, {"id": "1304.5417", "submitter": "Alejandro Frery", "authors": "Alejandro C. Frery and Abra\\~ao D. C. Nascimento and Renato J. Cintra", "title": "Analytic Expressions for Stochastic Distances Between Relaxed Complex\n  Wishart Distributions", "comments": "Accepted for publication in the IEEE Transactions on Geoscience and\n  Remote Sensing journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scaled complex Wishart distribution is a widely used model for multilook\nfull polarimetric SAR data whose adequacy has been attested in the literature.\nClassification, segmentation, and image analysis techniques which depend on\nthis model have been devised, and many of them employ some type of\ndissimilarity measure. In this paper we derive analytic expressions for four\nstochastic distances between relaxed scaled complex Wishart distributions in\ntheir most general form and in important particular cases. Using these\ndistances, inequalities are obtained which lead to new ways of deriving the\nBartlett and revised Wishart distances. The expressiveness of the four analytic\ndistances is assessed with respect to the variation of parameters. Such\ndistances are then used for deriving new tests statistics, which are proved to\nhave asymptotic chi-square distribution. Adopting the test size as a comparison\ncriterion, a sensitivity study is performed by means of Monte Carlo experiments\nsuggesting that the Bhattacharyya statistic outperforms all the others. The\npower of the tests is also assessed. Applications to actual data illustrate the\ndiscrimination and homogeneity identification capabilities of these distances.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2013 13:38:59 GMT"}], "update_date": "2013-04-22", "authors_parsed": [["Frery", "Alejandro C.", ""], ["Nascimento", "Abra\u00e3o D. C.", ""], ["Cintra", "Renato J.", ""]]}, {"id": "1304.5504", "submitter": "Tianbao Yang", "authors": "Jianhui Chen, Tianbao Yang, Qihang Lin, Lijun Zhang, Yi Chang", "title": "Optimal Stochastic Strongly Convex Optimization with a Logarithmic\n  Number of Projections", "comments": "Accepted by the 32th Conference on Uncertainty in Artificial\n  Intelligence (UAI 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider stochastic strongly convex optimization with a complex inequality\nconstraint. This complex inequality constraint may lead to computationally\nexpensive projections in algorithmic iterations of the stochastic gradient\ndescent~(SGD) methods. To reduce the computation costs pertaining to the\nprojections, we propose an Epoch-Projection Stochastic Gradient\nDescent~(Epro-SGD) method. The proposed Epro-SGD method consists of a sequence\nof epochs; it applies SGD to an augmented objective function at each iteration\nwithin the epoch, and then performs a projection at the end of each epoch.\nGiven a strongly convex optimization and for a total number of $T$ iterations,\nEpro-SGD requires only $\\log(T)$ projections, and meanwhile attains an optimal\nconvergence rate of $O(1/T)$, both in expectation and with a high probability.\nTo exploit the structure of the optimization problem, we propose a proximal\nvariant of Epro-SGD, namely Epro-ORDA, based on the optimal regularized dual\naveraging method. We apply the proposed methods on real-world applications; the\nempirical results demonstrate the effectiveness of our methods.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2013 18:51:07 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2013 18:36:52 GMT"}, {"version": "v3", "created": "Fri, 3 May 2013 16:24:20 GMT"}, {"version": "v4", "created": "Tue, 7 May 2013 18:26:55 GMT"}, {"version": "v5", "created": "Fri, 10 May 2013 05:19:10 GMT"}, {"version": "v6", "created": "Tue, 24 May 2016 05:07:06 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Chen", "Jianhui", ""], ["Yang", "Tianbao", ""], ["Lin", "Qihang", ""], ["Zhang", "Lijun", ""], ["Chang", "Yi", ""]]}, {"id": "1304.5530", "submitter": "Rachael  Tappenden Dr", "authors": "Rachael Tappenden and Peter Richt\\'arik and Jacek Gondzio", "title": "Inexact Coordinate Descent: Complexity and Preconditioning", "comments": "32 pages, 6 tables, 2 figures, 1 algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of minimizing a convex function using a\nrandomized block coordinate descent method. One of the key steps at each\niteration of the algorithm is determining the update to a block of variables.\nExisting algorithms assume that in order to compute the update, a particular\nsubproblem is solved exactly. In his work we relax this requirement, and allow\nfor the subproblem to be solved inexactly, leading to an inexact block\ncoordinate descent method. Our approach incorporates the best known results for\nexact updates as a special case. Moreover, these theoretical guarantees are\ncomplemented by practical considerations: the use of iterative techniques to\ndetermine the update as well as the use of preconditioning for further\nacceleration.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2013 20:02:04 GMT"}, {"version": "v2", "created": "Wed, 10 Dec 2014 17:10:49 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["Tappenden", "Rachael", ""], ["Richt\u00e1rik", "Peter", ""], ["Gondzio", "Jacek", ""]]}, {"id": "1304.5575", "submitter": "Qichao Que", "authors": "Qichao Que and Mikhail Belkin", "title": "Inverse Density as an Inverse Problem: The Fredholm Equation Approach", "comments": "Fixing a few typos in last version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of estimating the ratio $\\frac{q}{p}$\nwhere $p$ is a density function and $q$ is another density, or, more generally\nan arbitrary function. Knowing or approximating this ratio is needed in various\nproblems of inference and integration, in particular, when one needs to average\na function with respect to one probability distribution, given a sample from\nanother. It is often referred as {\\it importance sampling} in statistical\ninference and is also closely related to the problem of {\\it covariate shift}\nin transfer learning as well as to various MCMC methods. It may also be useful\nfor separating the underlying geometry of a space, say a manifold, from the\ndensity function defined on it.\n  Our approach is based on reformulating the problem of estimating\n$\\frac{q}{p}$ as an inverse problem in terms of an integral operator\ncorresponding to a kernel, and thus reducing it to an integral equation, known\nas the Fredholm problem of the first kind. This formulation, combined with the\ntechniques of regularization and kernel methods, leads to a principled\nkernel-based framework for constructing algorithms and for analyzing them\ntheoretically.\n  The resulting family of algorithms (FIRE, for Fredholm Inverse Regularized\nEstimator) is flexible, simple and easy to implement.\n  We provide detailed theoretical analysis including concentration bounds and\nconvergence rates for the Gaussian kernel in the case of densities defined on\n$\\R^d$, compact domains in $\\R^d$ and smooth $d$-dimensional sub-manifolds of\nthe Euclidean space.\n  We also show experimental results including applications to classification\nand semi-supervised learning within the covariate shift framework and\ndemonstrate some encouraging experimental comparisons. We also show how the\nparameters of our algorithms can be chosen in a completely unsupervised manner.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2013 00:57:35 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2013 11:46:51 GMT"}], "update_date": "2013-04-26", "authors_parsed": [["Que", "Qichao", ""], ["Belkin", "Mikhail", ""]]}, {"id": "1304.5583", "submitter": "Ameet Talwalkar", "authors": "Ameet Talwalkar, Lester Mackey, Yadong Mu, Shih-Fu Chang, Michael I.\n  Jordan", "title": "Distributed Low-rank Subspace Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision problems ranging from image clustering to motion segmentation to\nsemi-supervised learning can naturally be framed as subspace segmentation\nproblems, in which one aims to recover multiple low-dimensional subspaces from\nnoisy and corrupted input data. Low-Rank Representation (LRR), a convex\nformulation of the subspace segmentation problem, is provably and empirically\naccurate on small problems but does not scale to the massive sizes of modern\nvision datasets. Moreover, past work aimed at scaling up low-rank matrix\nfactorization is not applicable to LRR given its non-decomposable constraints.\nIn this work, we propose a novel divide-and-conquer algorithm for large-scale\nsubspace segmentation that can cope with LRR's non-decomposable constraints and\nmaintains LRR's strong recovery guarantees. This has immediate implications for\nthe scalability of subspace segmentation, which we demonstrate on a benchmark\nface recognition dataset and in simulations. We then introduce novel\napplications of LRR-based subspace segmentation to large-scale semi-supervised\nlearning for multimedia event detection, concept detection, and image tagging.\nIn each case, we obtain state-of-the-art results and order-of-magnitude speed\nups.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2013 03:54:48 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2013 02:55:18 GMT"}], "update_date": "2013-10-17", "authors_parsed": [["Talwalkar", "Ameet", ""], ["Mackey", "Lester", ""], ["Mu", "Yadong", ""], ["Chang", "Shih-Fu", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1304.5678", "submitter": "Felix Breuer", "authors": "Carly Stambaugh, Hui Yang, Felix Breuer", "title": "Analytic Feature Selection for Support Vector Machines", "comments": "To be presented at 9th International Conference on Machine Learning\n  and Data Mining MLDM 2013. 15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support vector machines (SVMs) rely on the inherent geometry of a data set to\nclassify training data. Because of this, we believe SVMs are an excellent\ncandidate to guide the development of an analytic feature selection algorithm,\nas opposed to the more commonly used heuristic methods. We propose a\nfilter-based feature selection algorithm based on the inherent geometry of a\nfeature set. Through observation, we identified six geometric properties that\ndiffer between optimal and suboptimal feature sets, and have statistically\nsignificant correlations to classifier performance. Our algorithm is based on\nlogistic and linear regression models using these six geometric properties as\npredictor variables. The proposed algorithm achieves excellent results on high\ndimensional text data sets, with features that can be organized into a handful\nof feature types; for example, unigrams, bigrams or semantic structural\nfeatures. We believe this algorithm is a novel and effective approach to\nsolving the feature selection problem for linear SVMs.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2013 23:38:45 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Stambaugh", "Carly", ""], ["Yang", "Hui", ""], ["Breuer", "Felix", ""]]}, {"id": "1304.5758", "submitter": "Sebastien Bubeck", "authors": "S\\'ebastien Bubeck and Che-Yu Liu", "title": "Prior-free and prior-dependent regret bounds for Thompson Sampling", "comments": "A previous version appeared under the title 'A note on the Bayesian\n  regret of Thompson Sampling with an arbitrary prior'", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the stochastic multi-armed bandit problem with a prior\ndistribution on the reward distributions. We are interested in studying\nprior-free and prior-dependent regret bounds, very much in the same spirit as\nthe usual distribution-free and distribution-dependent bounds for the\nnon-Bayesian stochastic bandit. Building on the techniques of Audibert and\nBubeck [2009] and Russo and Roy [2013] we first show that Thompson Sampling\nattains an optimal prior-free bound in the sense that for any prior\ndistribution its Bayesian regret is bounded from above by $14 \\sqrt{n K}$. This\nresult is unimprovable in the sense that there exists a prior distribution such\nthat any algorithm has a Bayesian regret bounded from below by $\\frac{1}{20}\n\\sqrt{n K}$. We also study the case of priors for the setting of Bubeck et al.\n[2013] (where the optimal mean is known as well as a lower bound on the\nsmallest gap) and we show that in this case the regret of Thompson Sampling is\nin fact uniformly bounded over time, thus showing that Thompson Sampling can\ngreatly take advantage of the nice properties of these priors.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2013 15:58:56 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2013 00:48:53 GMT"}], "update_date": "2013-10-04", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Liu", "Che-Yu", ""]]}, {"id": "1304.5862", "submitter": "Forrest Briggs", "authors": "Forrest Briggs, Xiaoli Z. Fern, Jed Irvine", "title": "Multi-Label Classifier Chains for Bird Sound", "comments": "6 pages, 1 figure, submission to ICML 2013 workshop on bioacoustics.\n  Note: this is a minor revision- the blind submission format has been replaced\n  with one that shows author names, and a few corrections have been made", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bird sound data collected with unattended microphones for automatic surveys,\nor mobile devices for citizen science, typically contain multiple\nsimultaneously vocalizing birds of different species. However, few works have\nconsidered the multi-label structure in birdsong. We propose to use an ensemble\nof classifier chains combined with a histogram-of-segments representation for\nmulti-label classification of birdsong. The proposed method is compared with\nbinary relevance and three multi-instance multi-label learning (MIML)\nalgorithms from prior work (which focus more on structure in the sound, and\nless on structure in the label sets). Experiments are conducted on two\nreal-world birdsong datasets, and show that the proposed method usually\noutperforms binary relevance (using the same features and base-classifier), and\nis better in some cases and worse in others compared to the MIML algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2013 07:44:05 GMT"}, {"version": "v2", "created": "Wed, 29 May 2013 17:36:07 GMT"}], "update_date": "2013-05-30", "authors_parsed": [["Briggs", "Forrest", ""], ["Fern", "Xiaoli Z.", ""], ["Irvine", "Jed", ""]]}, {"id": "1304.6233", "submitter": "Hongyang Zhang", "authors": "Hongyang Zhang, Zhouchen Lin, Chao Zhang", "title": "A Counterexample for the Validity of Using Nuclear Norm as a Convex\n  Surrogate of Rank", "comments": "accepted by ECML PKDD2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Rank minimization has attracted a lot of attention due to its robustness in\ndata recovery. To overcome the computational difficulty, rank is often replaced\nwith nuclear norm. For several rank minimization problems, such a replacement\nhas been theoretically proven to be valid, i.e., the solution to nuclear norm\nminimization problem is also the solution to rank minimization problem.\nAlthough it is easy to believe that such a replacement may not always be valid,\nno concrete example has ever been found. We argue that such a validity checking\ncannot be done by numerical computation and show, by analyzing the noiseless\nlatent low rank representation (LatLRR) model, that even for very simple rank\nminimization problems the validity may still break down. As a by-product, we\nfind that the solution to the nuclear norm minimization formulation of LatLRR\nis non-unique. Hence the results of LatLRR reported in the literature may be\nquestionable.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2013 11:01:53 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2013 07:21:04 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Zhang", "Hongyang", ""], ["Lin", "Zhouchen", ""], ["Zhang", "Chao", ""]]}, {"id": "1304.6478", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Miguel \\'A. Carreira-Perpi\\~n\\'an and Weiran Wang", "title": "The K-modes algorithm for clustering", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many clustering algorithms exist that estimate a cluster centroid, such as\nK-means, K-medoids or mean-shift, but no algorithm seems to exist that clusters\ndata by returning exactly K meaningful modes. We propose a natural definition\nof a K-modes objective function by combining the notions of density and cluster\nassignment. The algorithm becomes K-means and K-medoids in the limit of very\nlarge and very small scales. Computationally, it is slightly slower than\nK-means but much faster than mean-shift or K-medoids. Unlike K-means, it is\nable to find centroids that are valid patterns, truly representative of a\ncluster, even with nonconvex clusters, and appears robust to outliers and\nmisspecification of the scale and number of clusters.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2013 03:59:39 GMT"}], "update_date": "2013-04-25", "authors_parsed": [["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""], ["Wang", "Weiran", ""]]}, {"id": "1304.6480", "submitter": "Liwei Wang", "authors": "Yining Wang, Liwei Wang, Yuanzhi Li, Di He, Tie-Yan Liu, Wei Chen", "title": "A Theoretical Analysis of NDCG Type Ranking Measures", "comments": "COLT 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central problem in ranking is to design a ranking measure for evaluation of\nranking functions. In this paper we study, from a theoretical perspective, the\nwidely used Normalized Discounted Cumulative Gain (NDCG)-type ranking measures.\nAlthough there are extensive empirical studies of NDCG, little is known about\nits theoretical properties. We first show that, whatever the ranking function\nis, the standard NDCG which adopts a logarithmic discount, converges to 1 as\nthe number of items to rank goes to infinity. On the first sight, this result\nis very surprising. It seems to imply that NDCG cannot differentiate good and\nbad ranking functions, contradicting to the empirical success of NDCG in many\napplications. In order to have a deeper understanding of ranking measures in\ngeneral, we propose a notion referred to as consistent distinguishability. This\nnotion captures the intuition that a ranking measure should have such a\nproperty: For every pair of substantially different ranking functions, the\nranking measure can decide which one is better in a consistent manner on almost\nall datasets. We show that NDCG with logarithmic discount has consistent\ndistinguishability although it converges to the same limit for all ranking\nfunctions. We next characterize the set of all feasible discount functions for\nNDCG according to the concept of consistent distinguishability. Specifically we\nshow that whether NDCG has consistent distinguishability depends on how fast\nthe discount decays, and 1/r is a critical point. We then turn to the cut-off\nversion of NDCG, i.e., NDCG@k. We analyze the distinguishability of NDCG@k for\nvarious choices of k and the discount functions. Experimental results on real\nWeb search datasets agree well with the theory.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2013 04:08:23 GMT"}], "update_date": "2013-04-25", "authors_parsed": [["Wang", "Yining", ""], ["Wang", "Liwei", ""], ["Li", "Yuanzhi", ""], ["He", "Di", ""], ["Liu", "Tie-Yan", ""], ["Chen", "Wei", ""]]}, {"id": "1304.6487", "submitter": "Liangli Zhen", "authors": "Liangli Zhen, Zhang Yi, Xi Peng, Dezhong Peng", "title": "Locally linear representation for image clustering", "comments": null, "journal-ref": "Electronics Letters 50 (13), 942-943, 2014", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a key to construct a similarity graph in graph-oriented subspace\nlearning and clustering. In a similarity graph, each vertex denotes a data\npoint and the edge weight represents the similarity between two points. There\nare two popular schemes to construct a similarity graph, i.e., pairwise\ndistance based scheme and linear representation based scheme. Most existing\nworks have only involved one of the above schemes and suffered from some\nlimitations. Specifically, pairwise distance based methods are sensitive to the\nnoises and outliers compared with linear representation based methods. On the\nother hand, there is the possibility that linear representation based\nalgorithms wrongly select inter-subspaces points to represent a point, which\nwill degrade the performance. In this paper, we propose an algorithm, called\nLocally Linear Representation (LLR), which integrates pairwise distance with\nlinear representation together to address the problems. The proposed algorithm\ncan automatically encode each data point over a set of points that not only\ncould denote the objective point with less residual error, but also are close\nto the point in Euclidean space. The experimental results show that our\napproach is promising in subspace learning and subspace clustering.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2013 06:37:07 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 13:28:28 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Zhen", "Liangli", ""], ["Yi", "Zhang", ""], ["Peng", "Xi", ""], ["Peng", "Dezhong", ""]]}, {"id": "1304.6655", "submitter": "Mohammad Javad Abdi", "authors": "Mohammad Javad Abdi", "title": "Comparison of several reweighted l1-algorithms for solving cardinality\n  minimization problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reweighted l1-algorithms have attracted a lot of attention in the field of\napplied mathematics. A unified framework of such algorithms has been recently\nproposed by Zhao and Li. In this paper we construct a few new examples of\nreweighted l1-methods. These functions are certain concave approximations of\nthe l0-norm function. We focus on the numerical comparison between some new and\nexisting reweighted l1-algorithms. We show how the change of parameters in\nreweighted algorithms may affect the performance of the algorithms for finding\nthe solution of the cardinality minimization problem. In our experiments, the\nproblem data were generated according to different statistical distributions,\nand we test the algorithms on different sparsity level of the solution of the\nproblem. Our numerical results demonstrate that the reweighted l1-method is one\nof the efficient methods for locating the solution of the cardinality\nminimization problem.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2013 16:29:14 GMT"}], "update_date": "2013-04-25", "authors_parsed": [["Abdi", "Mohammad Javad", ""]]}, {"id": "1304.6663", "submitter": "Bamdev Mishra", "authors": "B. Mishra, G. Meyer and R. Sepulchre", "title": "Low-rank optimization for distance matrix completion", "comments": "In Proceedings of the 50th IEEE Conference on Decision and Control\n  and European Control Conference, 2011", "journal-ref": null, "doi": "10.1109/CDC.2011.6160810", "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of low-rank distance matrix completion. This\nproblem amounts to recover the missing entries of a distance matrix when the\ndimension of the data embedding space is possibly unknown but small compared to\nthe number of considered data points. The focus is on high-dimensional\nproblems. We recast the considered problem into an optimization problem over\nthe set of low-rank positive semidefinite matrices and propose two efficient\nalgorithms for low-rank distance matrix completion. In addition, we propose a\nstrategy to determine the dimension of the embedding space. The resulting\nalgorithms scale to high-dimensional problems and monotonically converge to a\nglobal solution of the problem. Finally, numerical experiments illustrate the\ngood performance of the proposed algorithms on benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2013 16:52:34 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2013 09:26:19 GMT"}], "update_date": "2013-04-26", "authors_parsed": [["Mishra", "B.", ""], ["Meyer", "G.", ""], ["Sepulchre", "R.", ""]]}, {"id": "1304.6803", "submitter": "Song Liu Mr", "authors": "Song Liu, John A. Quinn, Michael U. Gutmann, Taiji Suzuki, Masashi\n  Sugiyama", "title": "Direct Learning of Sparse Changes in Markov Networks by Density Ratio\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for detecting changes in Markov network structure\nbetween two sets of samples. Instead of naively fitting two Markov network\nmodels separately to the two data sets and figuring out their difference, we\n\\emph{directly} learn the network structure change by estimating the ratio of\nMarkov network models. This density-ratio formulation naturally allows us to\nintroduce sparsity in the network structure change, which highly contributes to\nenhancing interpretability. Furthermore, computation of the normalization term,\nwhich is a critical bottleneck of the naive approach, can be remarkably\nmitigated. We also give the dual formulation of the optimization problem, which\nfurther reduces the computation cost for large-scale Markov networks. Through\nexperiments, we demonstrate the usefulness of our method.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2013 05:37:55 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2013 20:05:08 GMT"}, {"version": "v3", "created": "Thu, 3 Oct 2013 18:10:39 GMT"}, {"version": "v4", "created": "Sun, 20 Oct 2013 13:08:51 GMT"}, {"version": "v5", "created": "Wed, 1 Jan 2014 07:13:09 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Liu", "Song", ""], ["Quinn", "John A.", ""], ["Gutmann", "Michael U.", ""], ["Suzuki", "Taiji", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1304.7045", "submitter": "Ohad Shamir", "authors": "Roi Livni, Shai Shalev-Shwartz, Ohad Shamir", "title": "An Algorithm for Training Polynomial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider deep neural networks, in which the output of each node is a\nquadratic function of its inputs. Similar to other deep architectures, these\nnetworks can compactly represent any function on a finite training set. The\nmain goal of this paper is the derivation of an efficient layer-by-layer\nalgorithm for training such networks, which we denote as the \\emph{Basis\nLearner}. The algorithm is a universal learner in the sense that the training\nerror is guaranteed to decrease at every iteration, and can eventually reach\nzero under mild conditions. We present practical implementations of this\nalgorithm, as well as preliminary experimental results. We also compare our\ndeep architecture to other shallow architectures for learning polynomials, in\nparticular kernel learning.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2013 00:35:37 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2014 13:14:59 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Livni", "Roi", ""], ["Shalev-Shwartz", "Shai", ""], ["Shamir", "Ohad", ""]]}, {"id": "1304.7230", "submitter": "David Kessler", "authors": "David C. Kessler and Jack Taylor and David B. Dunson", "title": "Learning Densities Conditional on Many Interacting Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a distribution conditional on a set of discrete-valued features is a\ncommonly encountered task. This becomes more challenging with a\nhigh-dimensional feature set when there is the possibility of interaction\nbetween the features. In addition, many frequently applied techniques consider\nonly prediction of the mean, but the complete conditional density is needed to\nanswer more complex questions. We demonstrate a novel nonparametric Bayes\nmethod based upon a tensor factorization of feature-dependent weights for\nGaussian kernels. The method makes use of multistage feature selection for\ndimension reduction. The resulting conditional density morphs flexibly with the\nselected features.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2013 16:56:30 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2013 21:16:46 GMT"}], "update_date": "2013-05-01", "authors_parsed": [["Kessler", "David C.", ""], ["Taylor", "Jack", ""], ["Dunson", "David B.", ""]]}, {"id": "1304.7284", "submitter": "Zenglin Xu", "authors": "Shandian Zhe, Zenglin Xu, and Yuan Qi", "title": "Supervised Heterogeneous Multiview Learning for Joint Association Study\n  and Disease Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given genetic variations and various phenotypical traits, such as Magnetic\nResonance Imaging (MRI) features, we consider two important and related tasks\nin biomedical research: i)to select genetic and phenotypical markers for\ndisease diagnosis and ii) to identify associations between genetic and\nphenotypical data. These two tasks are tightly coupled because underlying\nassociations between genetic variations and phenotypical features contain the\nbiological basis for a disease. While a variety of sparse models have been\napplied for disease diagnosis and canonical correlation analysis and its\nextensions have bee widely used in association studies (e.g., eQTL analysis),\nthese two tasks have been treated separately. To unify these two tasks, we\npresent a new sparse Bayesian approach for joint association study and disease\ndiagnosis. In this approach, common latent features are extracted from\ndifferent data sources based on sparse projection matrices and used to predict\nmultiple disease severity levels based on Gaussian process ordinal regression;\nin return, the disease status is used to guide the discovery of relationships\nbetween the data sources. The sparse projection matrices not only reveal\ninteractions between data sources but also select groups of biomarkers related\nto the disease. To learn the model from data, we develop an efficient\nvariational expectation maximization algorithm. Simulation results demonstrate\nthat our approach achieves higher accuracy in both predicting ordinal labels\nand discovering associations between data sources than alternative methods. We\napply our approach to an imaging genetics dataset for the study of Alzheimer's\nDisease (AD). Our method identifies biologically meaningful relationships\nbetween genetic variations, MRI features, and AD status, and achieves\nsignificantly higher accuracy for predicting ordinal AD stages than the\ncompeting methods.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2013 20:47:46 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2013 07:04:04 GMT"}], "update_date": "2013-10-17", "authors_parsed": [["Zhe", "Shandian", ""], ["Xu", "Zenglin", ""], ["Qi", "Yuan", ""]]}, {"id": "1304.7528", "submitter": "Toke Jansen Hansen", "authors": "Toke J. Hansen and Michael W. Mahoney", "title": "Semi-supervised Eigenvectors for Large-scale Locally-biased Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, one has side information, e.g., labels that are\nprovided in a semi-supervised manner, about a specific target region of a large\ndata set, and one wants to perform machine learning and data analysis tasks\n\"nearby\" that prespecified target region. For example, one might be interested\nin the clustering structure of a data graph near a prespecified \"seed set\" of\nnodes, or one might be interested in finding partitions in an image that are\nnear a prespecified \"ground truth\" set of pixels. Locally-biased problems of\nthis sort are particularly challenging for popular eigenvector-based machine\nlearning and data analysis tools. At root, the reason is that eigenvectors are\ninherently global quantities, thus limiting the applicability of\neigenvector-based methods in situations where one is interested in very local\nproperties of the data.\n  In this paper, we address this issue by providing a methodology to construct\nsemi-supervised eigenvectors of a graph Laplacian, and we illustrate how these\nlocally-biased eigenvectors can be used to perform locally-biased machine\nlearning. These semi-supervised eigenvectors capture\nsuccessively-orthogonalized directions of maximum variance, conditioned on\nbeing well-correlated with an input seed set of nodes that is assumed to be\nprovided in a semi-supervised manner. We show that these semi-supervised\neigenvectors can be computed quickly as the solution to a system of linear\nequations; and we also describe several variants of our basic method that have\nimproved scaling properties. We provide several empirical examples\ndemonstrating how these semi-supervised eigenvectors can be used to perform\nlocally-biased learning; and we discuss the relationship between our results\nand recent machine learning algorithms that use global eigenvectors of the\ngraph Laplacian.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2013 21:52:12 GMT"}], "update_date": "2013-04-30", "authors_parsed": [["Hansen", "Toke J.", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1304.7577", "submitter": "Preyas Popat", "authors": "Rina Panigrahy and Preyas Popat", "title": "Optimal amortized regret in every interval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the classical problem of predicting the next bit in a sequence of\nbits. A standard performance measure is {\\em regret} (loss in payoff) with\nrespect to a set of experts. For example if we measure performance with respect\nto two constant experts one that always predicts 0's and another that always\npredicts 1's it is well known that one can get regret $O(\\sqrt T)$ with respect\nto the best expert by using, say, the weighted majority algorithm. But this\nalgorithm does not provide performance guarantee in any interval. There are\nother algorithms that ensure regret $O(\\sqrt {x \\log T})$ in any interval of\nlength $x$. In this paper we show a randomized algorithm that in an amortized\nsense gets a regret of $O(\\sqrt x)$ for any interval when the sequence is\npartitioned into intervals arbitrarily. We empirically estimated the constant\nin the $O()$ for $T$ upto 2000 and found it to be small -- around 2.1. We also\nexperimentally evaluate the efficacy of this algorithm in predicting high\nfrequency stock data.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2013 07:17:31 GMT"}], "update_date": "2013-04-30", "authors_parsed": [["Panigrahy", "Rina", ""], ["Popat", "Preyas", ""]]}, {"id": "1304.7717", "submitter": "David Lopez-Paz", "authors": "David Lopez-Paz, Philipp Hennig, Bernhard Sch\\\"olkopf", "title": "The Randomized Dependence Coefficient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Randomized Dependence Coefficient (RDC), a measure of\nnon-linear dependence between random variables of arbitrary dimension based on\nthe Hirschfeld-Gebelein-R\\'enyi Maximum Correlation Coefficient. RDC is defined\nin terms of correlation of random non-linear copula projections; it is\ninvariant with respect to marginal distribution transformations, has low\ncomputational cost and is easy to implement: just five lines of R code,\nincluded at the end of the paper.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2013 17:27:50 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2013 09:24:00 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Lopez-Paz", "David", ""], ["Hennig", "Philipp", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1304.7981", "submitter": "Cencheng Shen", "authors": "Cencheng Shen, Ming Sun, Minh Tang, Carey E. Priebe", "title": "Generalized Canonical Correlation Analysis for Classification", "comments": "28 pages, 3 figures, 7 tables", "journal-ref": "Journal of Multivariate Analysis 130 (2014) 310-322", "doi": "10.1016/j.jmva.2014.05.011", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For multiple multivariate data sets, we derive conditions under which\nGeneralized Canonical Correlation Analysis (GCCA) improves classification\nperformance of the projected datasets, compared to standard Canonical\nCorrelation Analysis (CCA) using only two data sets. We illustrate our\ntheoretical results with simulations and a real data experiment.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 13:06:35 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2013 11:20:33 GMT"}, {"version": "v3", "created": "Wed, 4 Dec 2013 00:11:22 GMT"}, {"version": "v4", "created": "Sat, 5 Apr 2014 15:13:47 GMT"}, {"version": "v5", "created": "Thu, 26 Jun 2014 21:41:48 GMT"}], "update_date": "2014-06-30", "authors_parsed": [["Shen", "Cencheng", ""], ["Sun", "Ming", ""], ["Tang", "Minh", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1304.8020", "submitter": "Daniele Calandriello", "authors": "Daniele Calandriello, Gang Niu, Masashi Sugiyama", "title": "Semi-Supervised Information-Maximization Clustering", "comments": "Slightly change metadata. arXiv admin note: text overlap with\n  arXiv:1112.0611", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised clustering aims to introduce prior knowledge in the decision\nprocess of a clustering algorithm. In this paper, we propose a novel\nsemi-supervised clustering algorithm based on the information-maximization\nprinciple. The proposed method is an extension of a previous unsupervised\ninformation-maximization clustering algorithm based on squared-loss mutual\ninformation to effectively incorporate must-links and cannot-links. The\nproposed method is computationally efficient because the clustering solution\ncan be obtained analytically via eigendecomposition. Furthermore, the proposed\nmethod allows systematic optimization of tuning parameters such as the kernel\nwidth, given the degree of belief in the must-links and cannot-links. The\nusefulness of the proposed method is demonstrated through experiments.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 14:59:49 GMT"}, {"version": "v2", "created": "Wed, 1 May 2013 11:53:05 GMT"}], "update_date": "2013-05-02", "authors_parsed": [["Calandriello", "Daniele", ""], ["Niu", "Gang", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1304.8126", "submitter": "Yuxin Chen", "authors": "Yuxin Chen, Yuejie Chi", "title": "Robust Spectral Compressed Sensing via Structured Matrix Completion", "comments": "accepted to IEEE Transactions on Information Theory", "journal-ref": "IEEE Transactions on Information Theory, Vol. 60, No. 10, pp. 6576\n  - 6601, October 2014", "doi": "10.1109/TIT.2014.2343623", "report-no": null, "categories": "cs.IT cs.SY math.IT math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper explores the problem of \\emph{spectral compressed sensing}, which\naims to recover a spectrally sparse signal from a small random subset of its\n$n$ time domain samples. The signal of interest is assumed to be a\nsuperposition of $r$ multi-dimensional complex sinusoids, while the underlying\nfrequencies can assume any \\emph{continuous} values in the normalized frequency\ndomain. Conventional compressed sensing paradigms suffer from the basis\nmismatch issue when imposing a discrete dictionary on the Fourier\nrepresentation. To address this issue, we develop a novel algorithm, called\n\\emph{Enhanced Matrix Completion (EMaC)}, based on structured matrix completion\nthat does not require prior knowledge of the model order. The algorithm starts\nby arranging the data into a low-rank enhanced form exhibiting multi-fold\nHankel structure, and then attempts recovery via nuclear norm minimization.\nUnder mild incoherence conditions, EMaC allows perfect recovery as soon as the\nnumber of samples exceeds the order of $r\\log^{4}n$, and is stable against\nbounded noise. Even if a constant portion of samples are corrupted with\narbitrary magnitude, EMaC still allows exact recovery, provided that the sample\ncomplexity exceeds the order of $r^{2}\\log^{3}n$. Along the way, our results\ndemonstrate the power of convex relaxation in completing a low-rank multi-fold\nHankel or Toeplitz matrix from minimal observed entries. The performance of our\nalgorithm and its applicability to super resolution are further validated by\nnumerical experiments.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 19:24:22 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2013 16:05:33 GMT"}, {"version": "v3", "created": "Mon, 17 Mar 2014 04:37:30 GMT"}, {"version": "v4", "created": "Tue, 18 Mar 2014 05:13:23 GMT"}, {"version": "v5", "created": "Sun, 20 Jul 2014 21:02:36 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Chen", "Yuxin", ""], ["Chi", "Yuejie", ""]]}, {"id": "1304.8132", "submitter": "Zeyuan Allen Zhu", "authors": "Zeyuan Allen Zhu and Silvio Lattanzi and Vahab Mirrokni", "title": "Local Graph Clustering Beyond Cheeger's Inequality", "comments": "An extended abstract of this paper has appeared in the proceedings of\n  the 30th International Conference on Machine Learning (ICML 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications of large-scale graph clustering, we study\nrandom-walk-based LOCAL algorithms whose running times depend only on the size\nof the output cluster, rather than the entire graph. All previously known such\nalgorithms guarantee an output conductance of $\\tilde{O}(\\sqrt{\\phi(A)})$ when\nthe target set $A$ has conductance $\\phi(A)\\in[0,1]$. In this paper, we improve\nit to $$\\tilde{O}\\bigg( \\min\\Big\\{\\sqrt{\\phi(A)},\n\\frac{\\phi(A)}{\\sqrt{\\mathsf{Conn}(A)}} \\Big\\} \\bigg)\\enspace, $$ where the\ninternal connectivity parameter $\\mathsf{Conn}(A) \\in [0,1]$ is defined as the\nreciprocal of the mixing time of the random walk over the induced subgraph on\n$A$.\n  For instance, using $\\mathsf{Conn}(A) = \\Omega(\\lambda(A) / \\log n)$ where\n$\\lambda$ is the second eigenvalue of the Laplacian of the induced subgraph on\n$A$, our conductance guarantee can be as good as\n$\\tilde{O}(\\phi(A)/\\sqrt{\\lambda(A)})$. This builds an interesting connection\nto the recent advance of the so-called improved Cheeger's Inequality [KKL+13],\nwhich says that global spectral algorithms can provide a conductance guarantee\nof $O(\\phi_{\\mathsf{opt}}/\\sqrt{\\lambda_3})$ instead of\n$O(\\sqrt{\\phi_{\\mathsf{opt}}})$.\n  In addition, we provide theoretical guarantee on the clustering accuracy (in\nterms of precision and recall) of the output set. We also prove that our\nanalysis is tight, and perform empirical evaluation to support our theory on\nboth synthetic and real data.\n  It is worth noting that, our analysis outperforms prior work when the cluster\nis well-connected. In fact, the better it is well-connected inside, the more\nsignificant improvement (both in terms of conductance and accuracy) we can\nobtain. Our results shed light on why in practice some random-walk-based\nalgorithms perform better than its previous theory, and help guide future\nresearch about local clustering.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 19:57:36 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2013 18:25:15 GMT"}], "update_date": "2013-11-08", "authors_parsed": [["Zhu", "Zeyuan Allen", ""], ["Lattanzi", "Silvio", ""], ["Mirrokni", "Vahab", ""]]}]