[{"id": "1607.00021", "submitter": "Jacob Bien", "authors": "Jacob Bien", "title": "The Simulator: An Engine to Streamline Simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simulator is an R package that streamlines the process of performing\nsimulations by creating a common infrastructure that can be easily used and\nreused across projects. Methodological statisticians routinely write\nsimulations to compare their methods to preexisting ones. While developing\nideas, there is a temptation to write \"quick and dirty\" simulations to try out\nideas. This approach of rapid prototyping is useful but can sometimes backfire\nif bugs are introduced. Using the simulator allows one to remove the \"dirty\"\nwithout sacrificing the \"quick.\" Coding is quick because the statistician\nfocuses exclusively on those aspects of the simulation that are specific to the\nparticular paper being written. Code written with the simulator is succinct,\nhighly readable, and easily shared with others. The modular nature of\nsimulations written with the simulator promotes code reusability, which saves\ntime and facilitates reproducibility. The syntax of the simulator leads to\nsimulation code that is easily human-readable. Other benefits of using the\nsimulator include the ability to \"step in\" to a simulation and change one\naspect without having to rerun the entire simulation from scratch, the\nstraightforward integration of parallel computing into simulations, and the\nability to rapidly generate plots, tables, and reports with minimal effort.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 20:04:45 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Bien", "Jacob", ""]]}, {"id": "1607.00034", "submitter": "Tom Hope", "authors": "Tom Hope and Dafna Shahaf", "title": "Ballpark Learning: Estimating Labels from Rough Group Comparisons", "comments": "To appear in the European Conference on Machine Learning and\n  Principles and Practice of Knowledge Discovery (ECML-PKDD) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in estimating individual labels given only coarse,\naggregated signal over the data points. In our setting, we receive sets\n(\"bags\") of unlabeled instances with constraints on label proportions. We relax\nthe unrealistic assumption of known label proportions, made in previous work;\ninstead, we assume only to have upper and lower bounds, and constraints on bag\ndifferences. We motivate the problem, propose an intuitive formulation and\nalgorithm, and apply our methods to real-world scenarios. Across several\ndomains, we show how using only proportion constraints and no labeled examples,\nwe can achieve surprisingly high accuracy. In particular, we demonstrate how to\npredict income level using rough stereotypes and how to perform sentiment\nanalysis using very little information. We also apply our method to guide\nexploratory analysis, recovering geographical differences in twitter dialect.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 20:40:24 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Hope", "Tom", ""], ["Shahaf", "Dafna", ""]]}, {"id": "1607.00051", "submitter": "Alireza Dirafzoon", "authors": "Alireza Dirafzoon, Alper Bozkurt, and Edgar Lobaton", "title": "Geometric Learning and Topological Inference with Biobotic Networks:\n  Convergence Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO math.AT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we present and analyze a framework for geometric and\ntopological estimation for mapping of unknown environments. We consider agents\nmimicking motion behaviors of cyborg insects, known as biobots, and exploit\ncoordinate-free local interactions among them to infer geometric and\ntopological information about the environment, under minimal sensing and\nlocalization constraints. Local interactions are used to create a graphical\nrepresentation referred to as the encounter graph. A metric is estimated over\nthe encounter graph of the agents in order to construct a geometric point cloud\nusing manifold learning techniques. Topological data analysis (TDA), in\nparticular persistent homology, is used in order to extract topological\nfeatures of the space and a classification method is proposed to infer robust\nfeatures of interest (e.g. existence of obstacles). We examine the asymptotic\nbehavior of the proposed metric in terms of the convergence to the geodesic\ndistances in the underlying manifold of the domain, and provide stability\nanalysis results for the topological persistence. The proposed framework and\nits convergences and stability analysis are demonstrated through numerical\nsimulations and experiments.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 21:23:17 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Dirafzoon", "Alireza", ""], ["Bozkurt", "Alper", ""], ["Lobaton", "Edgar", ""]]}, {"id": "1607.00067", "submitter": "Fariba Yousefi", "authors": "Fariba Yousefi, Zhenwen Dai, Carl Henrik Ek, Neil Lawrence", "title": "Unsupervised Learning with Imbalanced Data via Structure Consolidation\n  Latent Variable Model", "comments": "ICLR 2016 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning on imbalanced data is challenging because, when given\nimbalanced data, current model is often dominated by the major category and\nignores the categories with small amount of data. We develop a latent variable\nmodel that can cope with imbalanced data by dividing the latent space into a\nshared space and a private space. Based on Gaussian Process Latent Variable\nModels, we propose a new kernel formulation that enables the separation of\nlatent space and derives an efficient variational inference method. The\nperformance of our model is demonstrated with an imbalanced medical image\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 22:25:20 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Yousefi", "Fariba", ""], ["Dai", "Zhenwen", ""], ["Ek", "Carl Henrik", ""], ["Lawrence", "Neil", ""]]}, {"id": "1607.00071", "submitter": "Robert Vandermeulen", "authors": "Robert A. Vandermeulen and Clayton D. Scott", "title": "An Operator Theoretic Approach to Nonparametric Mixture Models", "comments": "Contains and greatly extends the results from our previous work,\n  arXiv:1502.06644, and thus contains some overlap with that work. This version\n  contains some small grammatical and technical corrections as well as some\n  changes for improved clarity", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When estimating finite mixture models, it is common to make assumptions on\nthe mixture components, such as parametric assumptions. In this work, we make\nno distributional assumptions on the mixture components and instead assume that\nobservations from the mixture model are grouped, such that observations in the\nsame group are known to be drawn from the same mixture component. We precisely\ncharacterize the number of observations $n$ per group needed for the mixture\nmodel to be identifiable, as a function of the number $m$ of mixture\ncomponents. In addition to our assumption-free analysis, we also study the\nsettings where the mixture components are either linearly independent or\njointly irreducible. Furthermore, our analysis considers two kinds of\nidentifiability -- where the mixture model is the simplest one explaining the\ndata, and where it is the only one. As an application of these results, we\nprecisely characterize identifiability of multinomial mixture models. Our\nanalysis relies on an operator-theoretic framework that associates mixture\nmodels in the grouped-sample setting with certain infinite-dimensional tensors.\nBased on this framework, we introduce general spectral algorithms for\nrecovering the mixture components and illustrate their use on a synthetic data\nset.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 23:01:37 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 02:59:07 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Vandermeulen", "Robert A.", ""], ["Scott", "Clayton D.", ""]]}, {"id": "1607.00076", "submitter": "Daria Reshetova", "authors": "Daria Reshetova", "title": "Multi-class classification: mirror descent approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of multi-class classification and a stochastic opti-\nmization approach to it. We derive risk bounds for stochastic mirror descent\nalgorithm and provide examples of set geometries that make the use of the\nalgorithm efficient in terms of error in k.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 23:12:20 GMT"}, {"version": "v2", "created": "Thu, 8 Dec 2016 08:15:53 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Reshetova", "Daria", ""]]}, {"id": "1607.00084", "submitter": "Xueyu Mao", "authors": "Xueyu Mao, Purnamrita Sarkar, Deepayan Chakrabarti", "title": "On Mixed Memberships and Symmetric Nonnegative Matrix Factorizations", "comments": "In ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of finding overlapping communities in networks has gained much\nattention recently. Optimization-based approaches use non-negative matrix\nfactorization (NMF) or variants, but the global optimum cannot be provably\nattained in general. Model-based approaches, such as the popular\nmixed-membership stochastic blockmodel or MMSB (Airoldi et al., 2008), use\nparameters for each node to specify the overlapping communities, but standard\ninference techniques cannot guarantee consistency. We link the two approaches,\nby (a) establishing sufficient conditions for the symmetric NMF optimization to\nhave a unique solution under MMSB, and (b) proposing a computationally\nefficient algorithm called GeoNMF that is provably optimal and hence consistent\nfor a broad parameter regime. We demonstrate its accuracy on both simulated and\nreal-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 00:17:01 GMT"}, {"version": "v2", "created": "Fri, 23 Jun 2017 00:17:38 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Mao", "Xueyu", ""], ["Sarkar", "Purnamrita", ""], ["Chakrabarti", "Deepayan", ""]]}, {"id": "1607.00101", "submitter": "Zhaosong Lu", "authors": "Zhaosong Lu", "title": "Randomized block proximal damped Newton method for composite\n  self-concordant minimization", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the composite self-concordant (CSC) minimization\nproblem, which minimizes the sum of a self-concordant function $f$ and a\n(possibly nonsmooth) proper closed convex function $g$. The CSC minimization is\nthe cornerstone of the path-following interior point methods for solving a\nbroad class of convex optimization problems. It has also found numerous\napplications in machine learning. The proximal damped Newton (PDN) methods have\nbeen well studied in the literature for solving this problem that enjoy a nice\niteration complexity. Given that at each iteration these methods typically\nrequire evaluating or accessing the Hessian of $f$ and also need to solve a\nproximal Newton subproblem, the cost per iteration can be prohibitively high\nwhen applied to large-scale problems. Inspired by the recent success of block\ncoordinate descent methods, we propose a randomized block proximal damped\nNewton (RBPDN) method for solving the CSC minimization. Compared to the PDN\nmethods, the computational cost per iteration of RBPDN is usually significantly\nlower. The computational experiment on a class of regularized logistic\nregression problems demonstrate that RBPDN is indeed promising in solving\nlarge-scale CSC minimization problems. The convergence of RBPDN is also\nanalyzed in the paper. In particular, we show that RBPDN is globally convergent\nwhen $g$ is Lipschitz continuous. It is also shown that RBPDN enjoys a local\nlinear convergence. Moreover, we show that for a class of $g$ including the\ncase where $g$ is Lipschitz differentiable, RBPDN enjoys a global linear\nconvergence. As a striking consequence, it shows that the classical damped\nNewton methods [22,40] and the PDN [31] for such $g$ are globally linearly\nconvergent, which was previously unknown in the literature. Moreover, this\nresult can be used to sharpen the existing iteration complexity of these\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 03:16:57 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Lu", "Zhaosong", ""]]}, {"id": "1607.00110", "submitter": "Iman Alodah", "authors": "Iman Alodah and Jennifer Neville", "title": "Combining Gradient Boosting Machines with Collective Inference to\n  Predict Continuous Values", "comments": "7 pages, 3 Figures, Sixth International Workshop on Statistical\n  Relational AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient boosting of regression trees is a competitive procedure for learning\npredictive models of continuous data that fits the data with an additive\nnon-parametric model. The classic version of gradient boosting assumes that the\ndata is independent and identically distributed. However, relational data with\ninterdependent, linked instances is now common and the dependencies in such\ndata can be exploited to improve predictive performance. Collective inference\nis one approach to exploit relational correlation patterns and significantly\nreduce classification error. However, much of the work on collective learning\nand inference has focused on discrete prediction tasks rather than continuous.\n%target values has not got that attention in terms of collective inference. In\nthis work, we investigate how to combine these two paradigms together to\nimprove regression in relational domains. Specifically, we propose a boosting\nalgorithm for learning a collective inference model that predicts a continuous\ntarget variable. In the algorithm, we learn a basic relational model,\ncollectively infer the target values, and then iteratively learn relational\nmodels to predict the residuals. We evaluate our proposed algorithm on a real\nnetwork dataset and show that it outperforms alternative boosting methods.\nHowever, our investigation also revealed that the relational features interact\ntogether to produce better predictions.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 05:21:15 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Alodah", "Iman", ""], ["Neville", "Jennifer", ""]]}, {"id": "1607.00133", "submitter": "Ilya Mironov", "authors": "Mart\\'in Abadi and Andy Chu and Ian Goodfellow and H. Brendan McMahan\n  and Ilya Mironov and Kunal Talwar and Li Zhang", "title": "Deep Learning with Differential Privacy", "comments": null, "journal-ref": "Proceedings of the 2016 ACM SIGSAC Conference on Computer and\n  Communications Security (ACM CCS), pp. 308-318, 2016", "doi": "10.1145/2976749.2978318", "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning techniques based on neural networks are achieving remarkable\nresults in a wide variety of domains. Often, the training of models requires\nlarge, representative datasets, which may be crowdsourced and contain sensitive\ninformation. The models should not expose private information in these\ndatasets. Addressing this goal, we develop new algorithmic techniques for\nlearning and a refined analysis of privacy costs within the framework of\ndifferential privacy. Our implementation and experiments demonstrate that we\ncan train deep neural networks with non-convex objectives, under a modest\nprivacy budget, and at a manageable cost in software complexity, training\nefficiency, and model quality.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 07:29:10 GMT"}, {"version": "v2", "created": "Mon, 24 Oct 2016 11:59:40 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Abadi", "Mart\u00edn", ""], ["Chu", "Andy", ""], ["Goodfellow", "Ian", ""], ["McMahan", "H. Brendan", ""], ["Mironov", "Ilya", ""], ["Talwar", "Kunal", ""], ["Zhang", "Li", ""]]}, {"id": "1607.00136", "submitter": "Collins Leke", "authors": "Collins Leke and Tshilidzi Marwala", "title": "Missing Data Estimation in High-Dimensional Datasets: A Swarm\n  Intelligence-Deep Neural Network Approach", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we examine the problem of missing data in high-dimensional\ndatasets by taking into consideration the Missing Completely at Random and\nMissing at Random mechanisms, as well as theArbitrary missing pattern.\nAdditionally, this paper employs a methodology based on Deep Learning and Swarm\nIntelligence algorithms in order to provide reliable estimates for missing\ndata. The deep learning technique is used to extract features from the input\ndata via an unsupervised learning approach by modeling the data distribution\nbased on the input. This deep learning technique is then used as part of the\nobjective function for the swarm intelligence technique in order to estimate\nthe missing data after a supervised fine-tuning phase by minimizing an error\nfunction based on the interrelationship and correlation between features in the\ndataset. The investigated methodology in this paper therefore has longer\nrunning times, however, the promising potential outcomes justify the trade-off.\nAlso, basic knowledge of statistics is presumed.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 07:34:50 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Leke", "Collins", ""], ["Marwala", "Tshilidzi", ""]]}, {"id": "1607.00146", "submitter": "Kush Bhatia", "authors": "Kush Bhatia, Prateek Jain, Parameswaran Kamalaruban, Purushottam Kar", "title": "Efficient and Consistent Robust Time Series Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of robust time series analysis under the standard\nauto-regressive (AR) time series model in the presence of arbitrary outliers.\nWe devise an efficient hard thresholding based algorithm which can obtain a\nconsistent estimate of the optimal AR model despite a large fraction of the\ntime series points being corrupted. Our algorithm alternately estimates the\ncorrupted set of points and the model parameters, and is inspired by recent\nadvances in robust regression and hard-thresholding methods. However, a direct\napplication of existing techniques is hindered by a critical difference in the\ntime-series domain: each point is correlated with all previous points rendering\nexisting tools inapplicable directly. We show how to overcome this hurdle using\nnovel proof techniques. Using our techniques, we are also able to provide the\nfirst efficient and provably consistent estimator for the robust regression\nproblem where a standard linear observation model with white additive noise is\ncorrupted arbitrarily. We illustrate our methods on synthetic datasets and show\nthat our methods indeed are able to consistently recover the optimal parameters\ndespite a large fraction of points being corrupted.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 08:17:27 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Bhatia", "Kush", ""], ["Jain", "Prateek", ""], ["Kamalaruban", "Parameswaran", ""], ["Kar", "Purushottam", ""]]}, {"id": "1607.00148", "submitter": "Pankaj Malhotra Mr.", "authors": "Pankaj Malhotra, Anusha Ramakrishnan, Gaurangi Anand, Lovekesh Vig,\n  Puneet Agarwal, Gautam Shroff", "title": "LSTM-based Encoder-Decoder for Multi-sensor Anomaly Detection", "comments": "Accepted at ICML 2016 Anomaly Detection Workshop, New York, NY, USA,\n  2016. Reference update in this version (v2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mechanical devices such as engines, vehicles, aircrafts, etc., are typically\ninstrumented with numerous sensors to capture the behavior and health of the\nmachine. However, there are often external factors or variables which are not\ncaptured by sensors leading to time-series which are inherently unpredictable.\nFor instance, manual controls and/or unmonitored environmental conditions or\nload may lead to inherently unpredictable time-series. Detecting anomalies in\nsuch scenarios becomes challenging using standard approaches based on\nmathematical models that rely on stationarity, or prediction models that\nutilize prediction errors to detect anomalies. We propose a Long Short Term\nMemory Networks based Encoder-Decoder scheme for Anomaly Detection (EncDec-AD)\nthat learns to reconstruct 'normal' time-series behavior, and thereafter uses\nreconstruction error to detect anomalies. We experiment with three publicly\navailable quasi predictable time-series datasets: power demand, space shuttle,\nand ECG, and two real-world engine datasets with both predictive and\nunpredictable behavior. We show that EncDec-AD is robust and can detect\nanomalies from predictable, unpredictable, periodic, aperiodic, and\nquasi-periodic time-series. Further, we show that EncDec-AD is able to detect\nanomalies from short time-series (length as small as 30) as well as long\ntime-series (length as large as 500).\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 08:25:48 GMT"}, {"version": "v2", "created": "Mon, 11 Jul 2016 09:33:48 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Malhotra", "Pankaj", ""], ["Ramakrishnan", "Anusha", ""], ["Anand", "Gaurangi", ""], ["Vig", "Lovekesh", ""], ["Agarwal", "Puneet", ""], ["Shroff", "Gautam", ""]]}, {"id": "1607.00215", "submitter": "Ian Osband", "authors": "Ian Osband, Benjamin Van Roy", "title": "Why is Posterior Sampling Better than Optimism for Reinforcement\n  Learning?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational results demonstrate that posterior sampling for reinforcement\nlearning (PSRL) dramatically outperforms algorithms driven by optimism, such as\nUCRL2. We provide insight into the extent of this performance boost and the\nphenomenon that drives it. We leverage this insight to establish an\n$\\tilde{O}(H\\sqrt{SAT})$ Bayesian expected regret bound for PSRL in\nfinite-horizon episodic Markov decision processes, where $H$ is the horizon,\n$S$ is the number of states, $A$ is the number of actions and $T$ is the time\nelapsed. This improves upon the best previous bound of $\\tilde{O}(H S\n\\sqrt{AT})$ for any reinforcement learning algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 11:58:28 GMT"}, {"version": "v2", "created": "Fri, 22 Jul 2016 22:43:10 GMT"}, {"version": "v3", "created": "Tue, 13 Jun 2017 15:54:51 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Osband", "Ian", ""], ["Van Roy", "Benjamin", ""]]}, {"id": "1607.00274", "submitter": "Nicolas Garcia Trillos", "authors": "Nicolas Garcia Trillos, Ryan Murray", "title": "A new analytical approach to consistency and overfitting in regularized\n  empirical risk minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers the problem of binary classification: given training data\n$x_1, \\dots, x_n$ from a certain population, together with associated labels\n$y_1,\\dots, y_n \\in \\left\\{0,1 \\right\\}$, determine the best label for an\nelement $x$ not among the training data. More specifically, this work considers\na variant of the regularized empirical risk functional which is defined\nintrinsically to the observed data and does not depend on the underlying\npopulation. Tools from modern analysis are used to obtain a concise proof of\nasymptotic consistency as regularization parameters are taken to zero at rates\nrelated to the size of the sample. These analytical tools give a new framework\nfor understanding overfitting and underfitting, and rigorously connect the\nnotion of overfitting with a loss of compactness.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 15:03:05 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Trillos", "Nicolas Garcia", ""], ["Murray", "Ryan", ""]]}, {"id": "1607.00279", "submitter": "Nick Condry", "authors": "Nick Condry", "title": "Meaningful Models: Utilizing Conceptual Structure to Improve Machine\n  Learning Interpretability", "comments": "5 pages, 3 figures, presented at 2016 ICML Workshop on Human\n  Interpretability in Machine Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last decade has seen huge progress in the development of advanced machine\nlearning models; however, those models are powerless unless human users can\ninterpret them. Here we show how the mind's construction of concepts and\nmeaning can be used to create more interpretable machine learning models. By\nproposing a novel method of classifying concepts, in terms of 'form' and\n'function', we elucidate the nature of meaning and offer proposals to improve\nmodel understandability. As machine learning begins to permeate daily life,\ninterpretable models may serve as a bridge between domain-expert authors and\nnon-expert users.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 15:07:52 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Condry", "Nick", ""]]}, {"id": "1607.00315", "submitter": "Javier Turek Mr.", "authors": "Eran Treister and Javier S. Turek and Irad Yavneh", "title": "A multilevel framework for sparse optimization with application to\n  inverse covariance estimation and logistic regression", "comments": "To appear on SISC journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving l1 regularized optimization problems is common in the fields of\ncomputational biology, signal processing and machine learning. Such l1\nregularization is utilized to find sparse minimizers of convex functions. A\nwell-known example is the LASSO problem, where the l1 norm regularizes a\nquadratic function. A multilevel framework is presented for solving such l1\nregularized sparse optimization problems efficiently. We take advantage of the\nexpected sparseness of the solution, and create a hierarchy of problems of\nsimilar type, which is traversed in order to accelerate the optimization\nprocess. This framework is applied for solving two problems: (1) the sparse\ninverse covariance estimation problem, and (2) l1-regularized logistic\nregression. In the first problem, the inverse of an unknown covariance matrix\nof a multivariate normal distribution is estimated, under the assumption that\nit is sparse. To this end, an l1 regularized log-determinant optimization\nproblem needs to be solved. This task is challenging especially for large-scale\ndatasets, due to time and memory limitations. In the second problem, the\nl1-regularization is added to the logistic regression classification objective\nto reduce overfitting to the data and obtain a sparse model. Numerical\nexperiments demonstrate the efficiency of the multilevel framework in\naccelerating existing iterative solvers for both of these problems.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 16:59:13 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Treister", "Eran", ""], ["Turek", "Javier S.", ""], ["Yavneh", "Irad", ""]]}, {"id": "1607.00345", "submitter": "Simon Lacoste-Julien", "authors": "Simon Lacoste-Julien", "title": "Convergence Rate of Frank-Wolfe for Non-Convex Objectives", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a simple proof that the Frank-Wolfe algorithm obtains a stationary\npoint at a rate of $O(1/\\sqrt{t})$ on non-convex objectives with a Lipschitz\ncontinuous gradient. Our analysis is affine invariant and is the first, to the\nbest of our knowledge, giving a similar rate to what was already proven for\nprojected gradient methods (though on slightly different measures of\nstationarity).\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 18:37:33 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Lacoste-Julien", "Simon", ""]]}, {"id": "1607.00360", "submitter": "Aditya Menon", "authors": "Richard Nock and Aditya Krishna Menon and Cheng Soon Ong", "title": "A scaled Bregman theorem with applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bregman divergences play a central role in the design and analysis of a range\nof machine learning algorithms. This paper explores the use of Bregman\ndivergences to establish reductions between such algorithms and their analyses.\nWe present a new scaled isodistortion theorem involving Bregman divergences\n(scaled Bregman theorem for short) which shows that certain \"Bregman\ndistortions'\" (employing a potentially non-convex generator) may be exactly\nre-written as a scaled Bregman divergence computed over transformed data.\nAdmissible distortions include geodesic distances on curved manifolds and\nprojections or gauge-normalisation, while admissible data include scalars,\nvectors and matrices.\n  Our theorem allows one to leverage to the wealth and convenience of Bregman\ndivergences when analysing algorithms relying on the aforementioned Bregman\ndistortions. We illustrate this with three novel applications of our theorem: a\nreduction from multi-class density ratio to class-probability estimation, a new\nadaptive projection free yet norm-enforcing dual norm mirror descent algorithm,\nand a reduction from clustering on flat manifolds to clustering on curved\nmanifolds. Experiments on each of these domains validate the analyses and\nsuggest that the scaled Bregman theorem might be a worthy addition to the\npopular handful of Bregman divergence properties that have been pervasive in\nmachine learning.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 19:27:28 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Nock", "Richard", ""], ["Menon", "Aditya Krishna", ""], ["Ong", "Cheng Soon", ""]]}, {"id": "1607.00435", "submitter": "Jianwen Xie", "authors": "Jianwen Xie, Pamela K. Douglas, Ying Nian Wu, Arthur L. Brody, Ariana\n  E. Anderson", "title": "Decoding the Encoding of Functional Brain Networks: an fMRI\n  Classification Comparison of Non-negative Matrix Factorization (NMF),\n  Independent Component Analysis (ICA), and Sparse Coding Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain networks in fMRI are typically identified using spatial independent\ncomponent analysis (ICA), yet mathematical constraints such as sparse coding\nand positivity both provide alternate biologically-plausible frameworks for\ngenerating brain networks. Non-negative Matrix Factorization (NMF) would\nsuppress negative BOLD signal by enforcing positivity. Spatial sparse coding\nalgorithms ($L1$ Regularized Learning and K-SVD) would impose local\nspecialization and a discouragement of multitasking, where the total observed\nactivity in a single voxel originates from a restricted number of possible\nbrain networks.\n  The assumptions of independence, positivity, and sparsity to encode\ntask-related brain networks are compared; the resulting brain networks for\ndifferent constraints are used as basis functions to encode the observed\nfunctional activity at a given time point. These encodings are decoded using\nmachine learning to compare both the algorithms and their assumptions, using\nthe time series weights to predict whether a subject is viewing a video,\nlistening to an audio cue, or at rest, in 304 fMRI scans from 51 subjects.\n  For classifying cognitive activity, the sparse coding algorithm of $L1$\nRegularized Learning consistently outperformed 4 variations of ICA across\ndifferent numbers of networks and noise levels (p$<$0.001). The NMF algorithms,\nwhich suppressed negative BOLD signal, had the poorest accuracy. Within each\nalgorithm, encodings using sparser spatial networks (containing more\nzero-valued voxels) had higher classification accuracy (p$<$0.001). The success\nof sparse coding algorithms may suggest that algorithms which enforce sparse\ncoding, discourage multitasking, and promote local specialization may capture\nbetter the underlying source processes than those which allow inexhaustible\nlocal processes such as ICA.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 23:48:35 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Xie", "Jianwen", ""], ["Douglas", "Pamela K.", ""], ["Wu", "Ying Nian", ""], ["Brody", "Arthur L.", ""], ["Anderson", "Ariana E.", ""]]}, {"id": "1607.00446", "submitter": "Adam White", "authors": "Martha White and Adam White", "title": "A Greedy Approach to Adapting the Trace Parameter for Temporal\n  Difference Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main obstacles to broad application of reinforcement learning\nmethods is the parameter sensitivity of our core learning algorithms. In many\nlarge-scale applications, online computation and function approximation\nrepresent key strategies in scaling up reinforcement learning algorithms. In\nthis setting, we have effective and reasonably well understood algorithms for\nadapting the learning-rate parameter, online during learning. Such\nmeta-learning approaches can improve robustness of learning and enable\nspecialization to current task, improving learning speed. For\ntemporal-difference learning algorithms which we study here, there is yet\nanother parameter, $\\lambda$, that similarly impacts learning speed and\nstability in practice. Unfortunately, unlike the learning-rate parameter,\n$\\lambda$ parametrizes the objective function that temporal-difference methods\noptimize. Different choices of $\\lambda$ produce different fixed-point\nsolutions, and thus adapting $\\lambda$ online and characterizing the\noptimization is substantially more complex than adapting the learning-rate\nparameter. There are no meta-learning method for $\\lambda$ that can achieve (1)\nincremental updating, (2) compatibility with function approximation, and (3)\nmaintain stability of learning under both on and off-policy sampling. In this\npaper we contribute a novel objective function for optimizing $\\lambda$ as a\nfunction of state rather than time. We derive a new incremental, linear\ncomplexity $\\lambda$-adaption algorithm that does not require offline batch\nupdating or access to a model of the world, and present a suite of experiments\nillustrating the practicality of our new algorithm in three different settings.\nTaken together, our contributions represent a concrete step towards black-box\napplication of temporal-difference learning methods in real world problems.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2016 01:33:00 GMT"}, {"version": "v2", "created": "Mon, 24 Oct 2016 19:25:51 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["White", "Martha", ""], ["White", "Adam", ""]]}, {"id": "1607.00455", "submitter": "Ehsan Hosseini-Asl", "authors": "Ehsan Hosseini-Asl, Robert Keynto, Ayman El-Baz", "title": "Alzheimer's Disease Diagnostics by Adaptation of 3D Convolutional\n  Network", "comments": "This paper is accepted for publication at IEEE ICIP 2016 conference", "journal-ref": null, "doi": "10.1109/ICIP.2016.7532332", "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early diagnosis, playing an important role in preventing progress and\ntreating the Alzheimer\\{'}s disease (AD), is based on classification of\nfeatures extracted from brain images. The features have to accurately capture\nmain AD-related variations of anatomical brain structures, such as, e.g.,\nventricles size, hippocampus shape, cortical thickness, and brain volume. This\npaper proposed to predict the AD with a deep 3D convolutional neural network\n(3D-CNN), which can learn generic features capturing AD biomarkers and adapt to\ndifferent domain datasets. The 3D-CNN is built upon a 3D convolutional\nautoencoder, which is pre-trained to capture anatomical shape variations in\nstructural brain MRI scans. Fully connected upper layers of the 3D-CNN are then\nfine-tuned for each task-specific AD classification. Experiments on the\nCADDementia MRI dataset with no skull-stripping preprocessing have shown our\n3D-CNN outperforms several conventional classifiers by accuracy. Abilities of\nthe 3D-CNN to generalize the features learnt and adapt to other domains have\nbeen validated on the ADNI dataset.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2016 02:55:16 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Hosseini-Asl", "Ehsan", ""], ["Keynto", "Robert", ""], ["El-Baz", "Ayman", ""]]}, {"id": "1607.00485", "submitter": "Simone Scardapane", "authors": "Simone Scardapane, Danilo Comminiello, Amir Hussain, Aurelio Uncini", "title": "Group Sparse Regularization for Deep Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2017.02.029", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the joint task of simultaneously optimizing (i)\nthe weights of a deep neural network, (ii) the number of neurons for each\nhidden layer, and (iii) the subset of active input features (i.e., feature\nselection). While these problems are generally dealt with separately, we\npresent a simple regularized formulation allowing to solve all three of them in\nparallel, using standard optimization routines. Specifically, we extend the\ngroup Lasso penalty (originated in the linear regression literature) in order\nto impose group-level sparsity on the network's connections, where each group\nis defined as the set of outgoing weights from a unit. Depending on the\nspecific case, the weights can be related to an input variable, to a hidden\nneuron, or to a bias unit, thus performing simultaneously all the\naforementioned tasks in order to obtain a compact network. We perform an\nextensive experimental evaluation, by comparing with classical weight decay and\nLasso penalties. We show that a sparse version of the group Lasso penalty is\nable to achieve competitive performances, while at the same time resulting in\nextremely compact networks with a smaller number of input features. We evaluate\nboth on a toy dataset for handwritten digit recognition, and on multiple\nrealistic large-scale classification problems.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2016 09:55:26 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Scardapane", "Simone", ""], ["Comminiello", "Danilo", ""], ["Hussain", "Amir", ""], ["Uncini", "Aurelio", ""]]}, {"id": "1607.00494", "submitter": "Mehdi Korki", "authors": "Hadi Zayyani, Farzan Haddadi, and Mehdi Korki", "title": "Double-detector for Sparse Signal Detection from One Bit Compressed\n  Sensing Measurements", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": "10.1109/LSP.2016.2613898", "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter presents the sparse vector signal detection from one bit\ncompressed sensing measurements, in contrast to the previous works which deal\nwith scalar signal detection. In this letter, available results are extended to\nthe vector case and the GLRT detector and the optimal quantizer design are\nobtained. Also, a double-detector scheme is introduced in which a sensor level\nthreshold detector is integrated into network level GLRT to improve the\nperformance. The detection criteria of oracle and clairvoyant detectors are\nalso derived. Simulation results show that with careful design of the threshold\ndetector, the overall detection performance of double-detector scheme would be\nbetter than the sign-GLRT proposed in [1] and close to oracle and clairvoyant\ndetectors. Also, the proposed detector is applied to spectrum sensing and the\nresults are near the well known energy detector which uses the real valued data\nwhile the proposed detector only uses the sign of the data.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2016 11:51:25 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Zayyani", "Hadi", ""], ["Haddadi", "Farzan", ""], ["Korki", "Mehdi", ""]]}, {"id": "1607.00514", "submitter": "Nicolo Colombo", "authors": "Nicolo Colombo and Nikos Vlassis", "title": "Approximate Joint Matrix Triangularization", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of approximate joint triangularization of a set of\nnoisy jointly diagonalizable real matrices. Approximate joint triangularizers\nare commonly used in the estimation of the joint eigenstructure of a set of\nmatrices, with applications in signal processing, linear algebra, and tensor\ndecomposition. By assuming the input matrices to be perturbations of\nnoise-free, simultaneously diagonalizable ground-truth matrices, the\napproximate joint triangularizers are expected to be perturbations of the exact\njoint triangularizers of the ground-truth matrices. We provide a priori and a\nposteriori perturbation bounds on the `distance' between an approximate joint\ntriangularizer and its exact counterpart. The a priori bounds are theoretical\ninequalities that involve functions of the ground-truth matrices and noise\nmatrices, whereas the a posteriori bounds are given in terms of observable\nquantities that can be computed from the input matrices. From a practical\nperspective, the problem of finding the best approximate joint triangularizer\nof a set of noisy matrices amounts to solving a nonconvex optimization problem.\nWe show that, under a condition on the noise level of the input matrices, it is\npossible to find a good initial triangularizer such that the solution obtained\nby any local descent-type algorithm has certain global guarantees. Finally, we\ndiscuss the application of approximate joint matrix triangularization to\ncanonical tensor decomposition and we derive novel estimation error bounds.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2016 14:25:58 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Colombo", "Nicolo", ""], ["Vlassis", "Nikos", ""]]}, {"id": "1607.00556", "submitter": "Ehsan Hosseini-Asl", "authors": "Ehsan Hosseini-Asl, Georgy Gimel'farb, Ayman El-Baz", "title": "Alzheimer's Disease Diagnostics by a Deeply Supervised Adaptable 3D\n  Convolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early diagnosis, playing an important role in preventing progress and\ntreating the Alzheimer's disease (AD), is based on classification of features\nextracted from brain images. The features have to accurately capture main\nAD-related variations of anatomical brain structures, such as, e.g., ventricles\nsize, hippocampus shape, cortical thickness, and brain volume. This paper\nproposes to predict the AD with a deep 3D convolutional neural network\n(3D-CNN), which can learn generic features capturing AD biomarkers and adapt to\ndifferent domain datasets. The 3D-CNN is built upon a 3D convolutional\nautoencoder, which is pre-trained to capture anatomical shape variations in\nstructural brain MRI scans. Fully connected upper layers of the 3D-CNN are then\nfine-tuned for each task-specific AD classification. Experiments on the\n\\emph{ADNI} MRI dataset with no skull-stripping preprocessing have shown our\n3D-CNN outperforms several conventional classifiers by accuracy and robustness.\nAbilities of the 3D-CNN to generalize the features learnt and adapt to other\ndomains have been validated on the \\emph{CADDementia} dataset.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2016 19:55:56 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Hosseini-Asl", "Ehsan", ""], ["Gimel'farb", "Georgy", ""], ["El-Baz", "Ayman", ""]]}, {"id": "1607.00559", "submitter": "Peng Xu", "authors": "Peng Xu, Jiyan Yang, Farbod Roosta-Khorasani, Christopher R\\'e,\n  Michael W. Mahoney", "title": "Sub-sampled Newton Methods with Non-uniform Sampling", "comments": "minor fix on v1", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding the minimizer of a convex function $F:\n\\mathbb R^d \\rightarrow \\mathbb R$ of the form $F(w) := \\sum_{i=1}^n f_i(w) +\nR(w)$ where a low-rank factorization of $\\nabla^2 f_i(w)$ is readily available.\nWe consider the regime where $n \\gg d$. As second-order methods prove to be\neffective in finding the minimizer to a high-precision, in this work, we\npropose randomized Newton-type algorithms that exploit \\textit{non-uniform}\nsub-sampling of $\\{\\nabla^2 f_i(w)\\}_{i=1}^{n}$, as well as inexact updates, as\nmeans to reduce the computational complexity. Two non-uniform sampling\ndistributions based on {\\it block norm squares} and {\\it block partial leverage\nscores} are considered in order to capture important terms among $\\{\\nabla^2\nf_i(w)\\}_{i=1}^{n}$. We show that at each iteration non-uniformly sampling at\nmost $\\mathcal O(d \\log d)$ terms from $\\{\\nabla^2 f_i(w)\\}_{i=1}^{n}$ is\nsufficient to achieve a linear-quadratic convergence rate in $w$ when a\nsuitable initial point is provided. In addition, we show that our algorithms\nachieve a lower computational complexity and exhibit more robustness and better\ndependence on problem specific quantities, such as the condition number,\ncompared to similar existing methods, especially the ones based on uniform\nsampling. Finally, we empirically demonstrate that our methods are at least\ntwice as fast as Newton's methods with ridge logistic regression on several\nreal datasets.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2016 20:32:08 GMT"}, {"version": "v2", "created": "Tue, 5 Jul 2016 22:46:55 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Xu", "Peng", ""], ["Yang", "Jiyan", ""], ["Roosta-Khorasani", "Farbod", ""], ["R\u00e9", "Christopher", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1607.00567", "submitter": "Yury Maximov", "authors": "Yury Maximov, Massih-Reza Amini, Zaid Harchaoui", "title": "Rademacher Complexity Bounds for a Penalized Multiclass Semi-Supervised\n  Algorithm", "comments": "26 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Rademacher complexity bounds for multiclass classifiers trained\nwith a two-step semi-supervised model. In the first step, the algorithm\npartitions the partially labeled data and then identifies dense clusters\ncontaining $\\kappa$ predominant classes using the labeled training examples\nsuch that the proportion of their non-predominant classes is below a fixed\nthreshold. In the second step, a classifier is trained by minimizing a margin\nempirical loss over the labeled training set and a penalization term measuring\nthe disability of the learner to predict the $\\kappa$ predominant classes of\nthe identified clusters. The resulting data-dependent generalization error\nbound involves the margin distribution of the classifier, the stability of the\nclustering technique used in the first step and Rademacher complexity terms\ncorresponding to partially labeled training data. Our theoretical result\nexhibit convergence rates extending those proposed in the literature for the\nbinary case, and experimental results on different multiclass classification\nproblems show empirical evidence that supports the theory.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2016 22:20:59 GMT"}, {"version": "v2", "created": "Fri, 29 Jul 2016 10:13:05 GMT"}, {"version": "v3", "created": "Thu, 25 Jan 2018 08:37:30 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Maximov", "Yury", ""], ["Amini", "Massih-Reza", ""], ["Harchaoui", "Zaid", ""]]}, {"id": "1607.00653", "submitter": "Aditya Grover", "authors": "Aditya Grover, Jure Leskovec", "title": "node2vec: Scalable Feature Learning for Networks", "comments": "In Proceedings of the 22nd ACM SIGKDD International Conference on\n  Knowledge Discovery and Data Mining, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction tasks over nodes and edges in networks require careful effort in\nengineering features used by learning algorithms. Recent research in the\nbroader field of representation learning has led to significant progress in\nautomating prediction by learning the features themselves. However, present\nfeature learning approaches are not expressive enough to capture the diversity\nof connectivity patterns observed in networks. Here we propose node2vec, an\nalgorithmic framework for learning continuous feature representations for nodes\nin networks. In node2vec, we learn a mapping of nodes to a low-dimensional\nspace of features that maximizes the likelihood of preserving network\nneighborhoods of nodes. We define a flexible notion of a node's network\nneighborhood and design a biased random walk procedure, which efficiently\nexplores diverse neighborhoods. Our algorithm generalizes prior work which is\nbased on rigid notions of network neighborhoods, and we argue that the added\nflexibility in exploring neighborhoods is the key to learning richer\nrepresentations. We demonstrate the efficacy of node2vec over existing\nstate-of-the-art techniques on multi-label classification and link prediction\nin several real-world networks from diverse domains. Taken together, our work\nrepresents a new way for efficiently learning state-of-the-art task-independent\nrepresentations in complex networks.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2016 16:09:30 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Grover", "Aditya", ""], ["Leskovec", "Jure", ""]]}, {"id": "1607.00662", "submitter": "Danilo Jimenez Rezende", "authors": "Danilo Jimenez Rezende and S. M. Ali Eslami and Shakir Mohamed and\n  Peter Battaglia and Max Jaderberg and Nicolas Heess", "title": "Unsupervised Learning of 3D Structure from Images", "comments": "Appears in Advances in Neural Information Processing Systems 29 (NIPS\n  2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key goal of computer vision is to recover the underlying 3D structure from\n2D observations of the world. In this paper we learn strong deep generative\nmodels of 3D structures, and recover these structures from 3D and 2D images via\nprobabilistic inference. We demonstrate high-quality samples and report\nlog-likelihoods on several datasets, including ShapeNet [2], and establish the\nfirst benchmarks in the literature. We also show how these models and their\ninference networks can be trained end-to-end from 2D images. This demonstrates\nfor the first time the feasibility of learning to infer 3D representations of\nthe world in a purely unsupervised manner.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2016 17:53:11 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 17:26:53 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Rezende", "Danilo Jimenez", ""], ["Eslami", "S. M. Ali", ""], ["Mohamed", "Shakir", ""], ["Battaglia", "Peter", ""], ["Jaderberg", "Max", ""], ["Heess", "Nicolas", ""]]}, {"id": "1607.00669", "submitter": "Charbel Sakr", "authors": "Charbel Sakr, Ameya Patil, Sai Zhang, Yongjune Kim, Naresh Shanbhag", "title": "Understanding the Energy and Precision Requirements for Online Learning", "comments": "14 pages, 5 figures 4 of which have 2 subfigures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that the precision of data, hyperparameters, and internal\nrepresentations employed in learning systems directly impacts its energy,\nthroughput, and latency. The precision requirements for the training algorithm\nare also important for systems that learn on-the-fly. Prior work has shown that\nthe data and hyperparameters can be quantized heavily without incurring much\npenalty in classification accuracy when compared to floating point\nimplementations. These works suffer from two key limitations. First, they\nassume uniform precision for the classifier and for the training algorithm and\nthus miss out on the opportunity to further reduce precision. Second, prior\nworks are empirical studies. In this article, we overcome both these\nlimitations by deriving analytical lower bounds on the precision requirements\nof the commonly employed stochastic gradient descent (SGD) on-line learning\nalgorithm in the specific context of a support vector machine (SVM). Lower\nbounds on the data precision are derived in terms of the the desired\nclassification accuracy and precision of the hyperparameters used in the\nclassifier. Additionally, lower bounds on the hyperparameter precision in the\nSGD training algorithm are obtained. These bounds are validated using both\nsynthetic and the UCI breast cancer dataset. Additionally, the impact of these\nprecisions on the energy consumption of a fixed-point SVM with on-line training\nis studied.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2016 18:54:25 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 23:01:12 GMT"}, {"version": "v3", "created": "Fri, 26 Aug 2016 21:56:03 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Sakr", "Charbel", ""], ["Patil", "Ameya", ""], ["Zhang", "Sai", ""], ["Kim", "Yongjune", ""], ["Shanbhag", "Naresh", ""]]}, {"id": "1607.00696", "submitter": "Nicolas Garcia Trillos", "authors": "Nicolas Garcia Trillos", "title": "Variational limits of k-NN graph based functionals on data clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AP math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the large sample asymptotics of data analysis procedures\nbased on the optimization of functionals defined on $k$-NN graphs on point\nclouds. The paper is framed in the context of minimization of balanced cut\nfunctionals, but our techniques, ideas and results can be adapted to other\nfunctionals of relevance. We rigorously show that provided the number of\nneighbors in the graph $k:=k_n$ scales with the number of points in the cloud\nas $n \\gg k_n \\gg \\log(n)$, then with probability one, the solution to the\ngraph cut optimization problem converges towards the solution of an analogue\nvariational problem at the continuum level.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2016 22:51:48 GMT"}, {"version": "v2", "created": "Mon, 20 Feb 2017 16:18:44 GMT"}, {"version": "v3", "created": "Mon, 21 May 2018 20:45:05 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Trillos", "Nicolas Garcia", ""]]}, {"id": "1607.00706", "submitter": "Papis Wongchaisuwat", "authors": "Papis Wongchaisuwat, Diego Klabjan, Siddhartha R. Jonnalagadda", "title": "A Semi-supervised learning approach to enhance health care\n  Community-based Question Answering: A case study in alcoholism", "comments": "28 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community-based Question Answering (CQA) sites play an important role in\naddressing health information needs. However, a significant number of posted\nquestions remain unanswered. Automatically answering the posted questions can\nprovide a useful source of information for online health communities. In this\nstudy, we developed an algorithm to automatically answer health-related\nquestions based on past questions and answers (QA). We also aimed to understand\ninformation embedded within online health content that are good features in\nidentifying valid answers. Our proposed algorithm uses information retrieval\ntechniques to identify candidate answers from resolved QA. In order to rank\nthese candidates, we implemented a semi-supervised leaning algorithm that\nextracts the best answer to a question. We assessed this approach on a curated\ncorpus from Yahoo! Answers and compared against a rule-based string similarity\nbaseline. On our dataset, the semi-supervised learning algorithm has an\naccuracy of 86.2%. UMLS-based (health-related) features used in the model\nenhance the algorithm's performance by proximately 8 %. A reasonably high rate\nof accuracy is obtained given that the data is considerably noisy. Important\nfeatures distinguishing a valid answer from an invalid answer include text\nlength, number of stop words contained in a test question, a distance between\nthe test question and other questions in the corpus as well as a number of\noverlapping health-related terms between questions. Overall, our automated QA\nsystem based on historical QA pairs is shown to be effective according to the\ndata set in this case study. It is developed for general use in the health care\ndomain which can also be applied to other CQA sites.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 00:17:08 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Wongchaisuwat", "Papis", ""], ["Klabjan", "Diego", ""], ["Jonnalagadda", "Siddhartha R.", ""]]}, {"id": "1607.00710", "submitter": "Anh Tong", "authors": "Anh Tong and Jaesik Choi", "title": "Automatic Generation of Probabilistic Programming from Time Series Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic programming languages represent complex data with intermingled\nmodels in a few lines of code. Efficient inference algorithms in probabilistic\nprogramming languages make possible to build unified frameworks to compute\ninteresting probabilities of various large, real-world problems. When the\nstructure of model is given, constructing a probabilistic program is rather\nstraightforward. Thus, main focus have been to learn the best model parameters\nand compute marginal probabilities. In this paper, we provide a new perspective\nto build expressive probabilistic program from continue time series data when\nthe structure of model is not given. The intuition behind of our method is to\nfind a descriptive covariance structure of time series data in nonparametric\nGaussian process regression. We report that such descriptive covariance\nstructure efficiently derives a probabilistic programming description\naccurately.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 00:50:30 GMT"}, {"version": "v2", "created": "Thu, 14 Jul 2016 01:42:14 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Tong", "Anh", ""], ["Choi", "Jaesik", ""]]}, {"id": "1607.00743", "submitter": "Miles Lopes", "authors": "Miles E. Lopes", "title": "A Residual Bootstrap for High-Dimensional Regression with Near Low-Rank\n  Designs", "comments": "The main text of this paper was published at NIPS 2014. Proofs are\n  included here in the appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the residual bootstrap (RB) method in the context of\nhigh-dimensional linear regression. Specifically, we analyze the distributional\napproximation of linear contrasts $c^{\\top} (\\hat{\\beta}_{\\rho}-\\beta)$, where\n$\\hat{\\beta}_{\\rho}$ is a ridge-regression estimator. When regression\ncoefficients are estimated via least squares, classical results show that RB\nconsistently approximates the laws of contrasts, provided that $p\\ll n$, where\nthe design matrix is of size $n\\times p$. Up to now, relatively little work has\nconsidered how additional structure in the linear model may extend the validity\nof RB to the setting where $p/n\\asymp 1$. In this setting, we propose a version\nof RB that resamples residuals obtained from ridge regression. Our main\nstructural assumption on the design matrix is that it is nearly low rank --- in\nthe sense that its singular values decay according to a power-law profile.\nUnder a few extra technical assumptions, we derive a simple criterion for\nensuring that RB consistently approximates the law of a given contrast. We then\nspecialize this result to study confidence intervals for mean response values\n$X_i^{\\top} \\beta$, where $X_i^{\\top}$ is the $i$th row of the design. More\nprecisely, we show that conditionally on a Gaussian design with near low-rank\nstructure, RB simultaneously approximates all of the laws\n$X_i^{\\top}(\\hat{\\beta}_{\\rho}-\\beta)$, $i=1,\\dots,n$. This result is also\nnotable as it imposes no sparsity assumptions on $\\beta$. Furthermore, since\nour consistency results are formulated in terms of the Mallows (Kantorovich)\nmetric, the existence of a limiting distribution is not required.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 05:50:19 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Lopes", "Miles E.", ""]]}, {"id": "1607.01027", "submitter": "Tianbao Yang", "authors": "Yi Xu, Qihang Lin, Tianbao Yang", "title": "Accelerate Stochastic Subgradient Method by Leveraging Local Growth\n  Condition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new theory is developed for first-order stochastic convex\noptimization, showing that the global convergence rate is sufficiently\nquantified by a local growth rate of the objective function in a neighborhood\nof the optimal solutions. In particular, if the objective function $F(\\mathbf\nw)$ in the $\\epsilon$-sublevel set grows as fast as $\\|\\mathbf w - \\mathbf\nw_*\\|_2^{1/\\theta}$, where $\\mathbf w_*$ represents the closest optimal\nsolution to $\\mathbf w$ and $\\theta\\in(0,1]$ quantifies the local growth rate,\nthe iteration complexity of first-order stochastic optimization for achieving\nan $\\epsilon$-optimal solution can be $\\widetilde O(1/\\epsilon^{2(1-\\theta)})$,\nwhich is optimal at most up to a logarithmic factor. To achieve the faster\nglobal convergence, we develop two different accelerated stochastic subgradient\nmethods by iteratively solving the original problem approximately in a local\nregion around a historical solution with the size of the local region gradually\ndecreasing as the solution approaches the optimal set. Besides the theoretical\nimprovements, this work also includes new contributions towards making the\nproposed algorithms practical: (i) we present practical variants of accelerated\nstochastic subgradient methods that can run without the knowledge of\nmultiplicative growth constant and even the growth rate $\\theta$; (ii) we\nconsider a broad family of problems in machine learning to demonstrate that the\nproposed algorithms enjoy faster convergence than traditional stochastic\nsubgradient method. We also characterize the complexity of the proposed\nalgorithms for ensuring the gradient is small without the smoothness\nassumption.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 20:01:17 GMT"}, {"version": "v2", "created": "Sun, 4 Dec 2016 03:03:55 GMT"}, {"version": "v3", "created": "Tue, 17 Jan 2017 15:24:58 GMT"}, {"version": "v4", "created": "Sun, 21 Jul 2019 20:44:28 GMT"}, {"version": "v5", "created": "Wed, 6 May 2020 03:21:49 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Xu", "Yi", ""], ["Lin", "Qihang", ""], ["Yang", "Tianbao", ""]]}, {"id": "1607.01036", "submitter": "Jun Han Mr", "authors": "Jun Han, Qiang Liu", "title": "Bootstrap Model Aggregation for Distributed Statistical Learning", "comments": "This paper is about variance reduction on Monte Carol estimation of\n  KL divergence, NIPS, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In distributed, or privacy-preserving learning, we are often given a set of\nprobabilistic models estimated from different local repositories, and asked to\ncombine them into a single model that gives efficient statistical estimation. A\nsimple method is to linearly average the parameters of the local models, which,\nhowever, tends to be degenerate or not applicable on non-convex models, or\nmodels with different parameter dimensions. One more practical strategy is to\ngenerate bootstrap samples from the local models, and then learn a joint model\nbased on the combined bootstrap set. Unfortunately, the bootstrap procedure\nintroduces additional noise and can significantly deteriorate the performance.\nIn this work, we propose two variance reduction methods to correct the\nbootstrap noise, including a weighted M-estimator that is both statistically\nefficient and practically powerful. Both theoretical and empirical analysis is\nprovided to demonstrate our methods.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 20:12:41 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 20:06:40 GMT"}, {"version": "v3", "created": "Sun, 5 Feb 2017 03:07:51 GMT"}, {"version": "v4", "created": "Mon, 27 Feb 2017 21:01:20 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Han", "Jun", ""], ["Liu", "Qiang", ""]]}, {"id": "1607.01152", "submitter": "Nicolas Goix", "authors": "Nicolas Goix (LTCI)", "title": "How to Evaluate the Quality of Unsupervised Anomaly Detection\n  Algorithms?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When sufficient labeled data are available, classical criteria based on\nReceiver Operating Characteristic (ROC) or Precision-Recall (PR) curves can be\nused to compare the performance of un-supervised anomaly detection algorithms.\nHowever , in many situations, few or no data are labeled. This calls for\nalternative criteria one can compute on non-labeled data. In this paper, two\ncriteria that do not require labels are empirically shown to discriminate\naccurately (w.r.t. ROC or PR based criteria) between algorithms. These criteria\nare based on existing Excess-Mass (EM) and Mass-Volume (MV) curves, which\ngenerally cannot be well estimated in large dimension. A methodology based on\nfeature sub-sampling and aggregating is also described and tested, extending\nthe use of these criteria to high-dimensional datasets and solving major\ndrawbacks inherent to standard EM and MV curves.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 08:58:44 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Goix", "Nicolas", "", "LTCI"]]}, {"id": "1607.01224", "submitter": "John Santerre", "authors": "John W. Santerre, James J. Davis, Fangfang Xia, Rick Stevens", "title": "Machine Learning for Antimicrobial Resistance", "comments": "presented at 2016 ICML Workshop on #Data4Good: Machine Learning in\n  Social Good Applications, New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological datasets amenable to applied machine learning are more available\ntoday than ever before, yet they lack adequate representation in the\nData-for-Good community. Here we present a work in progress case study\nperforming analysis on antimicrobial resistance (AMR) using standard ensemble\nmachine learning techniques and note the successes and pitfalls such work\nentails. Broadly, applied machine learning (AML) techniques are well suited to\nAMR, with classification accuracies ranging from mid-90% to low- 80% depending\non sample size. Additionally, these techniques prove successful at identifying\ngene regions known to be associated with the AMR phenotype. We believe that the\nextensive amount of biological data available, the plethora of problems\npresented, and the global impact of such work merits the consideration of the\nData- for-Good community.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 12:42:01 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Santerre", "John W.", ""], ["Davis", "James J.", ""], ["Xia", "Fangfang", ""], ["Stevens", "Rick", ""]]}, {"id": "1607.01231", "submitter": "Shiqian Ma", "authors": "Xiao Wang, Shiqian Ma, Donald Goldfarb, Wei Liu", "title": "Stochastic Quasi-Newton Methods for Nonconvex Stochastic Optimization", "comments": "published in SIAM Journal on Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study stochastic quasi-Newton methods for nonconvex\nstochastic optimization, where we assume that noisy information about the\ngradients of the objective function is available via a stochastic first-order\noracle (SFO). We propose a general framework for such methods, for which we\nprove almost sure convergence to stationary points and analyze its worst-case\niteration complexity. When a randomly chosen iterate is returned as the output\nof such an algorithm, we prove that in the worst-case, the SFO-calls complexity\nis $O(\\epsilon^{-2})$ to ensure that the expectation of the squared norm of the\ngradient is smaller than the given accuracy tolerance $\\epsilon$. We also\npropose a specific algorithm, namely a stochastic damped L-BFGS (SdLBFGS)\nmethod, that falls under the proposed framework. {Moreover, we incorporate the\nSVRG variance reduction technique into the proposed SdLBFGS method, and analyze\nits SFO-calls complexity. Numerical results on a nonconvex binary\nclassification problem using SVM, and a multiclass classification problem using\nneural networks are reported.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 12:51:33 GMT"}, {"version": "v2", "created": "Mon, 11 Jul 2016 08:45:20 GMT"}, {"version": "v3", "created": "Wed, 21 Sep 2016 07:18:03 GMT"}, {"version": "v4", "created": "Sun, 21 May 2017 06:23:50 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Wang", "Xiao", ""], ["Ma", "Shiqian", ""], ["Goldfarb", "Donald", ""], ["Liu", "Wei", ""]]}, {"id": "1607.01312", "submitter": "Parthan Kasarapu Dr", "authors": "Parthan Kasarapu", "title": "Mixtures of Bivariate von Mises Distributions with Applications to\n  Modelling of Protein Dihedral Angles", "comments": "arXiv admin note: text overlap with arXiv:1506.08105", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modelling of empirically observed data is commonly done using mixtures of\nprobability distributions. In order to model angular data, directional\nprobability distributions such as the bivariate von Mises (BVM) is typically\nused. The critical task involved in mixture modelling is to determine the\noptimal number of component probability distributions. We employ the Bayesian\ninformation-theoretic principle of minimum message length (MML) to distinguish\nmixture models by balancing the trade-off between the model's complexity and\nits goodness-of-fit to the data. We consider the problem of modelling angular\ndata resulting from the spatial arrangement of protein structures using BVM\ndistributions. The main contributions of the paper include the development of\nthe mixture modelling apparatus along with the MML estimation of the parameters\nof the BVM distribution. We demonstrate that statistical inference using the\nMML framework supersedes the traditional methods and offers a mechanism to\nobjectively determine models that are of practical significance.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 16:09:06 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 00:03:20 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Kasarapu", "Parthan", ""]]}, {"id": "1607.01354", "submitter": "Kumar Eswaran Dr.", "authors": "Vishwajeet Singh, Killamsetti Ravi Kumar and K Eswaran", "title": "Learning Discriminative Features using Encoder-Decoder type Deep Neural\n  Nets", "comments": "12 pages, 8 figures and 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machine learning is applied to an increasing variety of complex problems,\nwhich are defined by high dimensional and complex data sets, the necessity for\ntask oriented feature learning grows in importance. With the advancement of\nDeep Learning algorithms, various successful feature learning techniques have\nevolved. In this paper, we present a novel way of learning discriminative\nfeatures by training Deep Neural Nets which have Encoder or Decoder type\narchitecture similar to an Autoencoder. We demonstrate that our approach can\nlearn discriminative features which can perform better at pattern\nclassification tasks when the number of training samples is relatively small in\nsize.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 18:46:13 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Singh", "Vishwajeet", ""], ["Kumar", "Killamsetti Ravi", ""], ["Eswaran", "K", ""]]}, {"id": "1607.01369", "submitter": "Keith Levin", "authors": "Vince Lyzinski, Keith Levin, Donniell E. Fishkind, Carey E. Priebe", "title": "On the Consistency of the Likelihood Maximization Vertex Nomination\n  Scheme: Bridging the Gap Between Maximum Likelihood Estimation and Graph\n  Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph in which a few vertices are deemed interesting a priori, the\nvertex nomination task is to order the remaining vertices into a nomination\nlist such that there is a concentration of interesting vertices at the top of\nthe list. Previous work has yielded several approaches to this problem, with\ntheoretical results in the setting where the graph is drawn from a stochastic\nblock model (SBM), including a vertex nomination analogue of the Bayes optimal\nclassifier. In this paper, we prove that maximum likelihood (ML)-based vertex\nnomination is consistent, in the sense that the performance of the ML-based\nscheme asymptotically matches that of the Bayes optimal scheme. We prove\ntheorems of this form both when model parameters are known and unknown.\nAdditionally, we introduce and prove consistency of a related, more scalable\nrestricted-focus ML vertex nomination scheme. Finally, we incorporate vertex\nand edge features into ML-based vertex nomination and briefly explore the\nempirical effectiveness of this approach.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 19:08:48 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2016 16:00:32 GMT"}, {"version": "v3", "created": "Sat, 27 Aug 2016 14:58:03 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Lyzinski", "Vince", ""], ["Levin", "Keith", ""], ["Fishkind", "Donniell E.", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1607.01375", "submitter": "Raghu Pasupathy", "authors": "Kalyani Nagaraj, Jie Xu, Raghu Pasupathy, and Soumyadip Ghosh", "title": "Efficient Estimation in the Tails of Gaussian Copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the question of efficient estimation in the tails of Gaussian\ncopulas. Our special focus is estimating expectations over multi-dimensional\nconstrained sets that have a small implied measure under the Gaussian copula.\nWe propose three estimators, all of which rely on a simple idea: identify\ncertain \\emph{dominating} point(s) of the feasible set, and appropriately shift\nand scale an exponential distribution for subsequent use within an importance\nsampling measure. As we show, the efficiency of such estimators depends\ncrucially on the local structure of the feasible set around the dominating\npoints. The first of our proposed estimators $\\estOpt$ is the\n\"full-information\" estimator that actively exploits such local structure to\nachieve bounded relative error in Gaussian settings. The second and third\nestimators $\\estExp$, $\\estLap$ are \"partial-information\" estimators, for use\nwhen complete information about the constraint set is not available, they do\nnot exhibit bounded relative error but are shown to achieve polynomial\nefficiency. We provide sharp asymptotics for all three estimators. For the\nNORTA setting where no ready information about the dominating points or the\nfeasible set structure is assumed, we construct a multinomial mixture of the\npartial-information estimator $\\estLap$ resulting in a fourth estimator\n$\\estNt$ with polynomial efficiency, and implementable through the ecoNORTA\nalgorithm. Numerical results on various example problems are remarkable, and\nconsistent with theory.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 19:26:39 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Nagaraj", "Kalyani", ""], ["Xu", "Jie", ""], ["Pasupathy", "Raghu", ""], ["Ghosh", "Soumyadip", ""]]}, {"id": "1607.01381", "submitter": "Yahel David", "authors": "Yahel David, Dotan Di Castro and Zohar Karnin", "title": "One-Shot Session Recommendation Systems with Combinatorial Items", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, content recommendation systems in large websites (or\n\\emph{content providers}) capture an increased focus. While the type of content\nvaries, e.g.\\ movies, articles, music, advertisements, etc., the high level\nproblem remains the same. Based on knowledge obtained so far on the user,\nrecommend the most desired content. In this paper we present a method to handle\nthe well known user-cold-start problem in recommendation systems. In this\nscenario, a recommendation system encounters a new user and the objective is to\npresent items as relevant as possible with the hope of keeping the user's\nsession as long as possible. We formulate an optimization problem aimed to\nmaximize the length of this initial session, as this is believed to be the key\nto have the user come back and perhaps register to the system. In particular,\nour model captures the fact that a single round with low quality recommendation\nis likely to terminate the session. In such a case, we do not proceed to the\nnext round as the user leaves the system, possibly never to seen again. We\ndenote this phenomenon a \\emph{One-Shot Session}. Our optimization problem is\nformulated as an MDP where the action space is of a combinatorial nature as we\nrecommend in each round, multiple items. This huge action space presents a\ncomputational challenge making the straightforward solution intractable. We\nanalyze the structure of the MDP to prove monotone and submodular like\nproperties that allow a computationally efficient solution via a method denoted\nby \\emph{Greedy Value Iteration} (G-VI).\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 19:40:56 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["David", "Yahel", ""], ["Di Castro", "Dotan", ""], ["Karnin", "Zohar", ""]]}, {"id": "1607.01400", "submitter": "Young Woong Park", "authors": "Young Woong Park and Diego Klabjan", "title": "An Aggregate and Iterative Disaggregate Algorithm with Proven Optimality\n  in Machine Learning", "comments": null, "journal-ref": "Machine Learning 105 (2016) 199 - 232", "doi": "10.1007/s10994-016-5562-z", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a clustering-based iterative algorithm to solve certain\noptimization problems in machine learning, where we start the algorithm by\naggregating the original data, solving the problem on aggregated data, and then\nin subsequent steps gradually disaggregate the aggregated data. We apply the\nalgorithm to common machine learning problems such as the least absolute\ndeviation regression problem, support vector machines, and semi-supervised\nsupport vector machines. We derive model-specific data aggregation and\ndisaggregation procedures. We also show optimality, convergence, and the\noptimality gap of the approximated solution in each iteration. A computational\nstudy is provided.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 20:04:57 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Park", "Young Woong", ""], ["Klabjan", "Diego", ""]]}, {"id": "1607.01417", "submitter": "Young Woong Park", "authors": "Young Woong Park, Yan Jiang, Diego Klabjan, Loren Williams", "title": "Algorithms for Generalized Cluster-wise Linear Regression", "comments": null, "journal-ref": "INFORMS Journal on Computing 29-2(2017): 301 - 317", "doi": "10.1287/ijoc.2016.0729", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster-wise linear regression (CLR), a clustering problem intertwined with\nregression, is to find clusters of entities such that the overall sum of\nsquared errors from regressions performed over these clusters is minimized,\nwhere each cluster may have different variances. We generalize the CLR problem\nby allowing each entity to have more than one observation, and refer to it as\ngeneralized CLR. We propose an exact mathematical programming based approach\nrelying on column generation, a column generation based heuristic algorithm\nthat clusters predefined groups of entities, a metaheuristic genetic algorithm\nwith adapted Lloyd's algorithm for K-means clustering, a two-stage approach,\nand a modified algorithm of Sp{\\\"a}th \\cite{Spath1979} for solving generalized\nCLR. We examine the performance of our algorithms on a stock keeping unit (SKU)\nclustering problem employed in forecasting halo and cannibalization effects in\npromotions using real-world retail data from a large supermarket chain. In the\nSKU clustering problem, the retailer needs to cluster SKUs based on their\nseasonal effects in response to promotions. The seasonal effects are the\nresults of regressions with predictors being promotion mechanisms and seasonal\ndummies performed over clusters generated. We compare the performance of all\nproposed algorithms for the SKU problem with real-world and synthetic data.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 21:04:08 GMT"}, {"version": "v2", "created": "Mon, 11 Jul 2016 21:38:18 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Park", "Young Woong", ""], ["Jiang", "Yan", ""], ["Klabjan", "Diego", ""], ["Williams", "Loren", ""]]}, {"id": "1607.01434", "submitter": "Jason Klusowski M", "authors": "Jason M. Klusowski and Andrew R. Barron", "title": "Risk Bounds for High-dimensional Ridge Function Combinations Including\n  Neural Networks", "comments": "Submitted to Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $ f^{\\star} $ be a function on $ \\mathbb{R}^d $ with an assumption of a\nspectral norm $ v_{f^{\\star}} $. For various noise settings, we show that $\n\\mathbb{E}\\|\\hat{f} - f^{\\star} \\|^2 \\leq \\left(v^4_{f^{\\star}}\\frac{\\log\nd}{n}\\right)^{1/3} $, where $ n $ is the sample size and $ \\hat{f} $ is either\na penalized least squares estimator or a greedily obtained version of such\nusing linear combinations of sinusoidal, sigmoidal, ramp, ramp-squared or other\nsmooth ridge functions. The candidate fits may be chosen from a continuum of\nfunctions, thus avoiding the rigidity of discretizations of the parameter\nspace. On the other hand, if the candidate fits are chosen from a\ndiscretization, we show that $ \\mathbb{E}\\|\\hat{f} - f^{\\star} \\|^2 \\leq\n\\left(v^3_{f^{\\star}}\\frac{\\log d}{n}\\right)^{2/5} $. This work bridges\nnon-linear and non-parametric function estimation and includes single-hidden\nlayer nets. Unlike past theory for such settings, our bound shows that the risk\nis small even when the input dimension $ d $ of an infinite-dimensional\nparameterized dictionary is much larger than the available sample size. When\nthe dimension is larger than the cube root of the sample size, this quantity is\nseen to improve the more familiar risk bound of $\nv_{f^{\\star}}\\left(\\frac{d\\log (n/d)}{n}\\right)^{1/2} $, also investigated\nhere.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 22:41:10 GMT"}, {"version": "v2", "created": "Thu, 7 Jul 2016 03:15:22 GMT"}, {"version": "v3", "created": "Tue, 26 Jul 2016 18:27:58 GMT"}, {"version": "v4", "created": "Mon, 29 Oct 2018 19:53:21 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Klusowski", "Jason M.", ""], ["Barron", "Andrew R.", ""]]}, {"id": "1607.01462", "submitter": "Yingfei Wang", "authors": "Yingfei Wang and Warren Powell", "title": "An optimal learning method for developing personalized treatment regimes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A treatment regime is a function that maps individual patient information to\na recommended treatment, hence explicitly incorporating the heterogeneity in\nneed for treatment across individuals. Patient responses are dichotomous and\ncan be predicted through an unknown relationship that depends on the patient\ninformation and the selected treatment. The goal is to find the treatments that\nlead to the best patient responses on average. Each experiment is expensive,\nforcing us to learn the most from each experiment. We adopt a Bayesian approach\nboth to incorporate possible prior information and to update our treatment\nregime continuously as information accrues, with the potential to allow smaller\nyet more informative trials and for patients to receive better treatment. By\nformulating the problem as contextual bandits, we introduce a knowledge\ngradient policy to guide the treatment assignment by maximizing the expected\nvalue of information, for which an approximation method is used to overcome\ncomputational challenges. We provide a detailed study on how to make sequential\nmedical decisions under uncertainty to reduce health care costs on a real world\nknee replacement dataset. We use clustering and LASSO to deal with the\nintrinsic sparsity in health datasets. We show experimentally that even though\nthe problem is sparse, through careful selection of physicians (versus picking\nthem at random), we can significantly improve the success rates.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 02:34:21 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Wang", "Yingfei", ""], ["Powell", "Warren", ""]]}, {"id": "1607.01624", "submitter": "Konstantina Palla Miss", "authors": "Konstantina Palla, Francois Caron, Yee Whye Teh", "title": "Bayesian nonparametrics for Sparse Dynamic Networks", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian nonparametric prior for time-varying networks. To each\nnode of the network is associated a positive parameter, modeling the\nsociability of that node. Sociabilities are assumed to evolve over time, and\nare modeled via a dynamic point process model. The model is able to (a) capture\nsmooth evolution of the interaction between nodes, allowing edges to\nappear/disappear over time (b) capture long term evolution of the sociabilities\nof the nodes (c) and yield sparse graphs, where the number of edges grows\nsubquadratically with the number of nodes. The evolution of the sociabilities\nis described by a tractable time-varying gamma process. We provide some\ntheoretical insights into the model and apply it to three real world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 14:02:43 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Palla", "Konstantina", ""], ["Caron", "Francois", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1607.01668", "submitter": "Xiao Fu", "authors": "Nicholas D. Sidiropoulos, Lieven De Lathauwer, Xiao Fu, Kejun Huang,\n  Evangelos E. Papalexakis, Christos Faloutsos", "title": "Tensor Decomposition for Signal Processing and Machine Learning", "comments": "revised version, overview article", "journal-ref": null, "doi": "10.1109/TSP.2017.2690524", "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensors or {\\em multi-way arrays} are functions of three or more indices\n$(i,j,k,\\cdots)$ -- similar to matrices (two-way arrays), which are functions\nof two indices $(r,c)$ for (row,column). Tensors have a rich history,\nstretching over almost a century, and touching upon numerous disciplines; but\nthey have only recently become ubiquitous in signal and data analytics at the\nconfluence of signal processing, statistics, data mining and machine learning.\nThis overview article aims to provide a good starting point for researchers and\npractitioners interested in learning about and working with tensors. As such,\nit focuses on fundamentals and motivation (using various application examples),\naiming to strike an appropriate balance of breadth {\\em and depth} that will\nenable someone having taken first graduate courses in matrix algebra and\nprobability to get started doing research and/or developing tensor algorithms\nand software. Some background in applied optimization is useful but not\nstrictly required. The material covered includes tensor rank and rank\ndecomposition; basic tensor factorization models and their relationships and\nproperties (including fairly good coverage of identifiability); broad coverage\nof algorithms ranging from alternating optimization to stochastic gradient;\nstatistical performance analysis; and applications ranging from source\nseparation to collaborative filtering, mixture and topic modeling,\nclassification, and multilinear subspace learning.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 15:22:31 GMT"}, {"version": "v2", "created": "Wed, 14 Dec 2016 15:16:53 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Sidiropoulos", "Nicholas D.", ""], ["De Lathauwer", "Lieven", ""], ["Fu", "Xiao", ""], ["Huang", "Kejun", ""], ["Papalexakis", "Evangelos E.", ""], ["Faloutsos", "Christos", ""]]}, {"id": "1607.01718", "submitter": "Justin Eldridge", "authors": "Justin Eldridge, Mikhail Belkin, Yusu Wang", "title": "Graphons, mergeons, and so on!", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we develop a theory of hierarchical clustering for graphs. Our\nmodeling assumption is that graphs are sampled from a graphon, which is a\npowerful and general model for generating graphs and analyzing large networks.\nGraphons are a far richer class of graph models than stochastic blockmodels,\nthe primary setting for recent progress in the statistical theory of graph\nclustering. We define what it means for an algorithm to produce the \"correct\"\nclustering, give sufficient conditions in which a method is statistically\nconsistent, and provide an explicit algorithm satisfying these properties.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 17:35:55 GMT"}, {"version": "v2", "created": "Sat, 29 Oct 2016 01:05:41 GMT"}, {"version": "v3", "created": "Tue, 8 Nov 2016 01:03:26 GMT"}, {"version": "v4", "created": "Mon, 22 May 2017 20:07:27 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Eldridge", "Justin", ""], ["Belkin", "Mikhail", ""], ["Wang", "Yusu", ""]]}, {"id": "1607.01981", "submitter": "David Barber", "authors": "Aleksandar Botev, Guy Lever, David Barber", "title": "Nesterov's Accelerated Gradient and Momentum as approximations to\n  Regularised Update Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unifying framework for adapting the update direction in\ngradient-based iterative optimization methods. As natural special cases we\nre-derive classical momentum and Nesterov's accelerated gradient method,\nlending a new intuitive interpretation to the latter algorithm. We show that a\nnew algorithm, which we term Regularised Gradient Descent, can converge more\nquickly than either Nesterov's algorithm or the classical momentum algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 12:12:11 GMT"}, {"version": "v2", "created": "Mon, 11 Jul 2016 08:05:18 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Botev", "Aleksandar", ""], ["Lever", "Guy", ""], ["Barber", "David", ""]]}, {"id": "1607.02011", "submitter": "Yang Song", "authors": "Yang Song, Jun Zhu, Yong Ren", "title": "Kernel Bayesian Inference with Posterior Regularization", "comments": "NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a vector-valued regression problem whose solution is equivalent to\nthe reproducing kernel Hilbert space (RKHS) embedding of the Bayesian posterior\ndistribution. This equivalence provides a new understanding of kernel Bayesian\ninference. Moreover, the optimization problem induces a new regularization for\nthe posterior embedding estimator, which is faster and has comparable\nperformance to the squared regularization in kernel Bayes' rule. This\nregularization coincides with a former thresholding approach used in kernel\nPOMDPs whose consistency remains to be established. Our theoretical work solves\nthis open problem and provides consistency analysis in regression settings.\nBased on our optimizational formulation, we propose a flexible Bayesian\nposterior regularization framework which for the first time enables us to put\nregularization at the distribution level. We apply this method to nonparametric\nstate-space filtering tasks with extremely nonlinear dynamics and show\nperformance gains over all other baselines.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 13:44:44 GMT"}, {"version": "v2", "created": "Wed, 26 Oct 2016 16:09:01 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Song", "Yang", ""], ["Zhu", "Jun", ""], ["Ren", "Yong", ""]]}, {"id": "1607.02024", "submitter": "Maurizio Filippone", "authors": "Yufei Han, Maurizio Filippone", "title": "Mini-Batch Spectral Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The cost of computing the spectrum of Laplacian matrices hinders the\napplication of spectral clustering to large data sets. While approximations\nrecover computational tractability, they can potentially affect clustering\nperformance. This paper proposes a practical approach to learn spectral\nclustering based on adaptive stochastic gradient optimization. Crucially, the\nproposed approach recovers the exact spectrum of Laplacian matrices in the\nlimit of the iterations, and the cost of each iteration is linear in the number\nof samples. Extensive experimental validation on data sets with up to half a\nmillion samples demonstrate its scalability and its ability to outperform\nstate-of-the-art approximate methods to learn spectral clustering for a given\ncomputational budget.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 14:06:06 GMT"}, {"version": "v2", "created": "Fri, 12 Aug 2016 12:52:39 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Han", "Yufei", ""], ["Filippone", "Maurizio", ""]]}, {"id": "1607.02066", "submitter": "Stefano Favaro", "authors": "Marco Battiston, Stefano Favaro, Daniel M. Roy, Yee Whye Teh", "title": "A characterization of product-form exchangeable feature probability\n  functions", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterize the class of exchangeable feature allocations assigning\nprobability $V_{n,k}\\prod_{l=1}^{k}W_{m_{l}}U_{n-m_{l}}$ to a feature\nallocation of $n$ individuals, displaying $k$ features with counts\n$(m_{1},\\ldots,m_{k})$ for these features. Each element of this class is\nparametrized by a countable matrix $V$ and two sequences $U$ and $W$ of\nnon-negative weights. Moreover, a consistency condition is imposed to guarantee\nthat the distribution for feature allocations of $n-1$ individuals is recovered\nfrom that of $n$ individuals, when the last individual is integrated out. In\nTheorem 1.1, we prove that the only members of this class satisfying the\nconsistency condition are mixtures of the Indian Buffet Process over its mass\nparameter $\\gamma$ and mixtures of the Beta--Bernoulli model over its\ndimensionality parameter $N$. Hence, we provide a characterization of these two\nmodels as the only, up to randomization of the parameters, consistent\nexchangeable feature allocations having the required product form.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 16:07:09 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Battiston", "Marco", ""], ["Favaro", "Stefano", ""], ["Roy", "Daniel M.", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1607.02085", "submitter": "Yuan Shen", "authors": "Yuan Shen and Peter Tino and Krasimira Tsaneva-Atanasova", "title": "A Classification Framework for Partially Observed Dynamical Systems", "comments": null, "journal-ref": "Phys. Rev. E 95, 043303 (2017)", "doi": "10.1103/PhysRevE.95.043303", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general framework for classifying partially observed dynamical\nsystems based on the idea of learning in the model space. In contrast to the\nexisting approaches using model point estimates to represent individual data\nitems, we employ posterior distributions over models, thus taking into account\nin a principled manner the uncertainty due to both the generative\n(observational and/or dynamic noise) and observation (sampling in time)\nprocesses. We evaluate the framework on two testbeds - a biological pathway\nmodel and a stochastic double-well system. Crucially, we show that the\nclassifier performance is not impaired when the model class used for inferring\nposterior distributions is much more simple than the observation-generating\nmodel class, provided the reduced complexity inferential model class captures\nthe essential characteristics needed for the given classification task.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 17:17:04 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Shen", "Yuan", ""], ["Tino", "Peter", ""], ["Tsaneva-Atanasova", "Krasimira", ""]]}, {"id": "1607.02109", "submitter": "John J Nay", "authors": "John J. Nay", "title": "Predicting and Understanding Law-Making with Word Vectors and an\n  Ensemble Model", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0176999", "report-no": null, "categories": "cs.CL physics.soc-ph stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Out of nearly 70,000 bills introduced in the U.S. Congress from 2001 to 2015,\nonly 2,513 were enacted. We developed a machine learning approach to\nforecasting the probability that any bill will become law. Starting in 2001\nwith the 107th Congress, we trained models on data from previous Congresses,\npredicted all bills in the current Congress, and repeated until the 113th\nCongress served as the test. For prediction we scored each sentence of a bill\nwith a language model that embeds legislative vocabulary into a\nhigh-dimensional, semantic-laden vector space. This language representation\nenables our investigation into which words increase the probability of\nenactment for any topic. To test the relative importance of text and context,\nwe compared the text model to a context-only model that uses variables such as\nwhether the bill's sponsor is in the majority party. To test the effect of\nchanges to bills after their introduction on our ability to predict their final\noutcome, we compared using the bill text and meta-data available at the time of\nintroduction with using the most recent data. At the time of introduction\ncontext-only predictions outperform text-only, and with the newest data\ntext-only outperforms context-only. Combining text and context always performs\nbest. We conducted a global sensitivity analysis on the combined model to\ndetermine important variables predicting enactment.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 18:08:59 GMT"}, {"version": "v2", "created": "Sat, 29 Apr 2017 17:12:33 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Nay", "John J.", ""]]}, {"id": "1607.02173", "submitter": "John Hershey", "authors": "Yusuf Isik, Jonathan Le Roux, Zhuo Chen, Shinji Watanabe, John R.\n  Hershey", "title": "Single-Channel Multi-Speaker Separation using Deep Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep clustering is a recently introduced deep learning architecture that uses\ndiscriminatively trained embeddings as the basis for clustering. It was\nrecently applied to spectrogram segmentation, resulting in impressive results\non speaker-independent multi-speaker separation. In this paper we extend the\nbaseline system with an end-to-end signal approximation objective that greatly\nimproves performance on a challenging speech separation. We first significantly\nimprove upon the baseline system performance by incorporating better\nregularization, larger temporal context, and a deeper architecture, culminating\nin an overall improvement in signal to distortion ratio (SDR) of 10.3 dB\ncompared to the baseline of 6.0 dB for two-speaker separation, as well as a 7.1\ndB SDR improvement for three-speaker separation. We then extend the model to\nincorporate an enhancement layer to refine the signal estimates, and perform\nend-to-end training through both the clustering and enhancement stages to\nmaximize signal fidelity. We evaluate the results using automatic speech\nrecognition. The new signal approximation objective, combined with end-to-end\ntraining, produces unprecedented performance, reducing the word error rate\n(WER) from 89.1% down to 30.8%. This represents a major advancement towards\nsolving the cocktail party problem.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 21:06:48 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Isik", "Yusuf", ""], ["Roux", "Jonathan Le", ""], ["Chen", "Zhuo", ""], ["Watanabe", "Shinji", ""], ["Hershey", "John R.", ""]]}, {"id": "1607.02188", "submitter": "Anders Hildeman", "authors": "Anders Hildeman, David Bolin, Jonas Wallin, Adam Johansson, Tufve\n  Nyholm, Thomas Asklund, Jun Yu", "title": "Whole-brain substitute CT generation using Markov random field mixture\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computed tomography (CT) equivalent information is needed for attenuation\ncorrection in PET imaging and for dose planning in radiotherapy. Prior work has\nshown that Gaussian mixture models can be used to generate a substitute CT\n(s-CT) image from a specific set of MRI modalities. This work introduces a more\nflexible class of mixture models for s-CT generation, that incorporates spatial\ndependency in the data through a Markov random field prior on the latent field\nof class memberships associated with a mixture model. Furthermore, the mixture\ndistributions are extended from Gaussian to normal inverse Gaussian (NIG),\nallowing heavier tails and skewness. The amount of data needed to train a model\nfor s-CT generation is of the order of 100 million voxels. The computational\nefficiency of the parameter estimation and prediction methods are hence\nparamount, especially when spatial dependency is included in the models. A\nstochastic Expectation Maximization (EM) gradient algorithm is proposed in\norder to tackle this challenge. The advantages of the spatial model and NIG\ndistributions are evaluated with a cross-validation study based on data from 14\npatients. The study show that the proposed model enhances the predictive\nquality of the s-CT images by reducing the mean absolute error with 17.9%.\nAlso, the distribution of CT values conditioned on the MR images are better\nexplained by the proposed model as evaluated using continuous ranked\nprobability scores.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 22:52:36 GMT"}, {"version": "v2", "created": "Wed, 28 Sep 2016 15:11:38 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Hildeman", "Anders", ""], ["Bolin", "David", ""], ["Wallin", "Jonas", ""], ["Johansson", "Adam", ""], ["Nyholm", "Tufve", ""], ["Asklund", "Thomas", ""], ["Yu", "Jun", ""]]}, {"id": "1607.02387", "submitter": "Gilles Blanchard", "authors": "Gilles Blanchard, Nicole Kr\\\"amer", "title": "Convergence rates of Kernel Conjugate Gradient for random design\n  regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove statistical rates of convergence for kernel-based least squares\nregression from i.i.d. data using a conjugate gradient algorithm, where\nregularization against overfitting is obtained by early stopping. This method\nis related to Kernel Partial Least Squares, a regression method that combines\nsupervised dimensionality reduction with least squares projection. Following\nthe setting introduced in earlier related literature, we study so-called \"fast\nconvergence rates\" depending on the regularity of the target regression\nfunction (measured by a source condition in terms of the kernel integral\noperator) and on the effective dimensionality of the data mapped into the\nkernel space. We obtain upper bounds, essentially matching known minimax lower\nbounds, for the $\\mathcal{L}^2$ (prediction) norm as well as for the stronger\nHilbert norm, if the true regression function belongs to the reproducing kernel\nHilbert space. If the latter assumption is not fulfilled, we obtain similar\nconvergence rates for appropriate norms, provided additional unlabeled data are\navailable.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 14:41:17 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Blanchard", "Gilles", ""], ["Kr\u00e4mer", "Nicole", ""]]}, {"id": "1607.02413", "submitter": "Jonathan Scarlett", "authors": "Jonathan Scarlett and Volkan Cevher", "title": "Lower Bounds on Active Learning for Graphical Model Selection", "comments": "Accepted to AISTATS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG cs.SI math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the underlying graph associated with a\nMarkov random field, with the added twist that the decoding algorithm can\niteratively choose which subsets of nodes to sample based on the previous\nsamples, resulting in an active learning setting. Considering both Ising and\nGaussian models, we provide algorithm-independent lower bounds for\nhigh-probability recovery within the class of degree-bounded graphs. Our main\nresults are minimax lower bounds for the active setting that match the best\nknown lower bounds for the passive setting, which in turn are known to be tight\nin several cases of interest. Our analysis is based on Fano's inequality, along\nwith novel mutual information bounds for the active learning setting, and the\napplication of restricted graph ensembles. While we consider ensembles that are\nsimilar or identical to those used in the passive setting, we require different\nanalysis techniques, with a key challenge being bounding a mutual information\nquantity associated with observed subsets of nodes, as opposed to full\nobservations.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 15:25:46 GMT"}, {"version": "v2", "created": "Tue, 7 Feb 2017 16:42:59 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Scarlett", "Jonathan", ""], ["Cevher", "Volkan", ""]]}, {"id": "1607.02435", "submitter": "Philippe Rigollet", "authors": "Nicolas Flammarion, Cheng Mao and Philippe Rigollet", "title": "Optimal Rates of Statistical Seriation", "comments": "V2 corrects an error in Lemma A.1, v3 corrects appendix F on unimodal\n  regression where the bounds now hold with polynomial probability rather than\n  exponential", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a matrix the seriation problem consists in permuting its rows in such\nway that all its columns have the same shape, for example, they are monotone\nincreasing. We propose a statistical approach to this problem where the matrix\nof interest is observed with noise and study the corresponding minimax rate of\nestimation of the matrices. Specifically, when the columns are either unimodal\nor monotone, we show that the least squares estimator is optimal up to\nlogarithmic factors and adapts to matrices with a certain natural structure.\nFinally, we propose a computationally efficient estimator in the monotonic case\nand study its performance both theoretically and experimentally. Our work is at\nthe intersection of shape constrained estimation and recent work that involves\npermutation learning, such as graph denoising and ranking.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 16:12:47 GMT"}, {"version": "v2", "created": "Fri, 22 Jul 2016 19:59:56 GMT"}, {"version": "v3", "created": "Mon, 1 Aug 2016 14:24:26 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Flammarion", "Nicolas", ""], ["Mao", "Cheng", ""], ["Rigollet", "Philippe", ""]]}, {"id": "1607.02450", "submitter": "Kush Varshney", "authors": "Kush R. Varshney", "title": "Proceedings of the 2016 ICML Workshop on #Data4Good: Machine Learning in\n  Social Good Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning\nin Social Good Applications, which was held on June 24, 2016 in New York.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 16:55:31 GMT"}, {"version": "v2", "created": "Sun, 28 Aug 2016 15:23:47 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Varshney", "Kush R.", ""]]}, {"id": "1607.02516", "submitter": "Johan Westerborn Alenl\\\"ov", "authors": "Johan Alenl\\\"ov and Arnaud Doucet and Fredrik Lindsten", "title": "Pseudo-Marginal Hamiltonian Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference in the presence of an intractable likelihood function is\ncomputationally challenging. When following a Markov chain Monte Carlo (MCMC)\napproach to approximate the posterior distribution in this context, one\ntypically either uses MCMC schemes which target the joint posterior of the\nparameters and some auxiliary latent variables, or pseudo-marginal\nMetropolis--Hastings (MH) schemes. The latter mimic a MH algorithm targeting\nthe marginal posterior of the parameters by approximating unbiasedly the\nintractable likelihood. However, in scenarios where the parameters and\nauxiliary variables are strongly correlated under the posterior and/or this\nposterior is multimodal, Gibbs sampling or Hamiltonian Monte Carlo (HMC) will\nperform poorly and the pseudo-marginal MH algorithm, as any other MH scheme,\nwill be inefficient for high dimensional parameters. We propose here an\noriginal MCMC algorithm, termed pseudo-marginal HMC, which combines the\nadvantages of both HMC and pseudo-marginal schemes. Specifically, the\npseudo-marginal HMC method is controlled by a precision parameter N,\ncontrolling the approximation of the likelihood and, for any N, it samples the\nmarginal posterior of the parameters. Additionally, as N tends to infinity, its\nsample trajectories and acceptance probability converge to those of an ideal,\nbut intractable, HMC algorithm which would have access to the marginal\nposterior of parameters and its gradient. We demonstrate through experiments\nthat pseudo-marginal HMC can outperform significantly both standard HMC and\npseudo-marginal MH schemes.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 20:06:43 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 08:50:23 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Alenl\u00f6v", "Johan", ""], ["Doucet", "Arnaud", ""], ["Lindsten", "Fredrik", ""]]}, {"id": "1607.02531", "submitter": "Kush Varshney", "authors": "Been Kim, Dmitry M. Malioutov, Kush R. Varshney", "title": "Proceedings of the 2016 ICML Workshop on Human Interpretability in\n  Machine Learning (WHI 2016)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the Proceedings of the 2016 ICML Workshop on Human Interpretability\nin Machine Learning (WHI 2016), which was held in New York, NY, June 23, 2016.\n  Invited speakers were Susan Athey, Rich Caruana, Jacob Feldman, Percy Liang,\nand Hanna Wallach.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 21:07:54 GMT"}, {"version": "v2", "created": "Wed, 27 Jul 2016 19:00:49 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Kim", "Been", ""], ["Malioutov", "Dmitry M.", ""], ["Varshney", "Kush R.", ""]]}, {"id": "1607.02533", "submitter": "Alexey Kurakin", "authors": "Alexey Kurakin, Ian Goodfellow and Samy Bengio", "title": "Adversarial examples in the physical world", "comments": "14 pages, 6 figures. Demo available at https://youtu.be/zQ_uMenoBCk", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing machine learning classifiers are highly vulnerable to\nadversarial examples. An adversarial example is a sample of input data which\nhas been modified very slightly in a way that is intended to cause a machine\nlearning classifier to misclassify it. In many cases, these modifications can\nbe so subtle that a human observer does not even notice the modification at\nall, yet the classifier still makes a mistake. Adversarial examples pose\nsecurity concerns because they could be used to perform an attack on machine\nlearning systems, even if the adversary has no access to the underlying model.\nUp to now, all previous work have assumed a threat model in which the adversary\ncan feed data directly into the machine learning classifier. This is not always\nthe case for systems operating in the physical world, for example those which\nare using signals from cameras and other sensors as an input. This paper shows\nthat even in such physical world scenarios, machine learning systems are\nvulnerable to adversarial examples. We demonstrate this by feeding adversarial\nimages obtained from cell-phone camera to an ImageNet Inception classifier and\nmeasuring the classification accuracy of the system. We find that a large\nfraction of adversarial examples are classified incorrectly even when perceived\nthrough the camera.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 21:12:11 GMT"}, {"version": "v2", "created": "Tue, 23 Aug 2016 22:57:31 GMT"}, {"version": "v3", "created": "Fri, 4 Nov 2016 20:34:37 GMT"}, {"version": "v4", "created": "Sat, 11 Feb 2017 00:39:39 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Kurakin", "Alexey", ""], ["Goodfellow", "Ian", ""], ["Bengio", "Samy", ""]]}, {"id": "1607.02624", "submitter": "Aleksandr Aravkin", "authors": "Rajiv Kumar, Oscar L\\'opez, Damek Davis, Aleksandr Y. Aravkin and\n  Felix J. Herrmann", "title": "Beating level-set methods for 3D seismic data interpolation: a\n  primal-dual alternating approach", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acquisition cost is a crucial bottleneck for seismic workflows, and low-rank\nformulations for data interpolation allow practitioners to `fill in' data\nvolumes from critically subsampled data acquired in the field. Tremendous size\nof seismic data volumes required for seismic processing remains a major\nchallenge for these techniques.\n  We propose a new approach to solve residual constrained formulations for\ninterpolation. We represent the data volume using matrix factors, and build a\nblock-coordinate algorithm with constrained convex subproblems that are solved\nwith a primal-dual splitting scheme. The new approach is competitive with state\nof the art level-set algorithms that interchange the role of objectives with\nconstraints. We use the new algorithm to successfully interpolate a large scale\n5D seismic data volume, generated from the geologically complex synthetic 3D\nCompass velocity model, where 80% of the data has been removed.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 15:38:22 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Kumar", "Rajiv", ""], ["L\u00f3pez", "Oscar", ""], ["Davis", "Damek", ""], ["Aravkin", "Aleksandr Y.", ""], ["Herrmann", "Felix J.", ""]]}, {"id": "1607.02654", "submitter": "Yanwei Cui", "authors": "Yanwei Cui, S\\'ebastien Lefevre, Laetitia Chapel, Anne Puissant", "title": "Combining multiple resolutions into hierarchical representations for\n  kernel-based image classification", "comments": "International Conference on Geographic Object-Based Image Analysis\n  (GEOBIA 2016), University of Twente in Enschede, The Netherlands", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geographic object-based image analysis (GEOBIA) framework has gained\nincreasing interest recently. Following this popular paradigm, we propose a\nnovel multiscale classification approach operating on a hierarchical image\nrepresentation built from two images at different resolutions. They capture the\nsame scene with different sensors and are naturally fused together through the\nhierarchical representation, where coarser levels are built from a Low Spatial\nResolution (LSR) or Medium Spatial Resolution (MSR) image while finer levels\nare generated from a High Spatial Resolution (HSR) or Very High Spatial\nResolution (VHSR) image. Such a representation allows one to benefit from the\ncontext information thanks to the coarser levels, and subregions spatial\narrangement information thanks to the finer levels. Two dedicated structured\nkernels are then used to perform machine learning directly on the constructed\nhierarchical representation. This strategy overcomes the limits of conventional\nGEOBIA classification procedures that can handle only one or very few\npre-selected scales. Experiments run on an urban classification task show that\nthe proposed approach can highly improve the classification accuracy w.r.t.\nconventional approaches working on a single scale.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 20:07:37 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 08:34:49 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Cui", "Yanwei", ""], ["Lefevre", "S\u00e9bastien", ""], ["Chapel", "Laetitia", ""], ["Puissant", "Anne", ""]]}, {"id": "1607.02665", "submitter": "Anurag Kumar", "authors": "Anurag Kumar, Bhiksha Raj", "title": "Classifier Risk Estimation under Limited Labeling Resources", "comments": "PAKDD 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose strategies for estimating performance of a\nclassifier when labels cannot be obtained for the whole test set. The number of\ntest instances which can be labeled is very small compared to the whole test\ndata size. The goal then is to obtain a precise estimate of classifier\nperformance using as little labeling resource as possible. Specifically, we try\nto answer, how to select a subset of the large test set for labeling such that\nthe performance of a classifier estimated on this subset is as close as\npossible to the one on the whole test set. We propose strategies based on\nstratified sampling for selecting this subset. We show that these strategies\ncan reduce the variance in estimation of classifier accuracy by a significant\namount compared to simple random sampling (over 65% in several cases). Hence,\nour proposed methods are much more precise compared to random sampling for\naccuracy estimation under restricted labeling resources. The reduction in\nnumber of samples required (compared to random sampling) to estimate the\nclassifier accuracy with only 1% error is high as 60% in some cases.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 21:18:23 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 20:18:35 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Kumar", "Anurag", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1607.02670", "submitter": "Garret Vo", "authors": "Garret Vo, Debdeep Pati", "title": "Sparse additive Gaussian process with soft interactions", "comments": "Submitted to Technometrics Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive nonparametric regression models provide an attractive tool for\nvariable selection in high dimensions when the relationship between the\nresponse and predictors is complex. They offer greater flexibility compared to\nparametric non-linear regression models and better interpretability and\nscalability than the non-parametric regression models. However, achieving\nsparsity simultaneously in the number of nonparametric components as well as in\nthe variables within each nonparametric component poses a stiff computational\nchallenge. In this article, we develop a novel Bayesian additive regression\nmodel using a combination of hard and soft shrinkages to separately control the\nnumber of additive components and the variables within each component. An\nefficient algorithm is developed to select the importance variables and\nestimate the interaction network. Excellent performance is obtained in\nsimulated and real data examples.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 22:29:32 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Vo", "Garret", ""], ["Pati", "Debdeep", ""]]}, {"id": "1607.02675", "submitter": "Bowei Yan", "authors": "Bowei Yan and Purnamrita Sarkar", "title": "Covariate Regularized Community Detection in Sparse Graphs", "comments": "27 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate community detection in networks in the presence\nof node covariates. In many instances, covariates and networks individually\nonly give a partial view of the cluster structure. One needs to jointly infer\nthe full cluster structure by considering both. In statistics, an emerging body\nof work has been focused on combining information from both the edges in the\nnetwork and the node covariates to infer community memberships. However, so far\nthe theoretical guarantees have been established in the dense regime, where the\nnetwork can lead to perfect clustering under a broad parameter regime, and\nhence the role of covariates is often not clear. In this paper, we examine\nsparse networks in conjunction with finite dimensional sub-gaussian mixtures as\ncovariates under moderate separation conditions. In this setting each\nindividual source can only cluster a non-vanishing fraction of nodes correctly.\nWe propose a simple optimization framework which provably improves clustering\naccuracy when the two sources carry partial information about the cluster\nmemberships, and hence perform poorly on their own. Our optimization problem\ncan be solved using scalable convex optimization algorithms. Using a variety of\nsimulated and real data examples, we show that the proposed method outperforms\nother existing methodology.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 01:07:16 GMT"}, {"version": "v2", "created": "Sat, 22 Oct 2016 16:59:02 GMT"}, {"version": "v3", "created": "Sat, 3 Dec 2016 06:51:51 GMT"}, {"version": "v4", "created": "Wed, 25 Apr 2018 14:14:03 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Yan", "Bowei", ""], ["Sarkar", "Purnamrita", ""]]}, {"id": "1607.02676", "submitter": "Bereket Kindo", "authors": "Bereket P. Kindo, Hao Wang, Timothy Hanson and Edsel A. Pe\\~na", "title": "Bayesian quantile additive regression trees", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble of regression trees have become popular statistical tools for the\nestimation of conditional mean given a set of predictors. However, quantile\nregression trees and their ensembles have not yet garnered much attention\ndespite the increasing popularity of the linear quantile regression model. This\nwork proposes a Bayesian quantile additive regression trees model that shows\nvery good predictive performance illustrated using simulation studies and real\ndata applications. Further extension to tackle binary classification problems\nis also considered.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 01:20:57 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Kindo", "Bereket P.", ""], ["Wang", "Hao", ""], ["Hanson", "Timothy", ""], ["Pe\u00f1a", "Edsel A.", ""]]}, {"id": "1607.02738", "submitter": "Nilesh Tripuraneni", "authors": "Nilesh Tripuraneni, Mark Rowland, Zoubin Ghahramani, Richard Turner", "title": "Magnetic Hamiltonian Monte Carlo", "comments": "34th International Conference on Machine Learning (ICML 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hamiltonian Monte Carlo (HMC) exploits Hamiltonian dynamics to construct\nefficient proposals for Markov chain Monte Carlo (MCMC). In this paper, we\npresent a generalization of HMC which exploits \\textit{non-canonical}\nHamiltonian dynamics. We refer to this algorithm as magnetic HMC, since in 3\ndimensions a subset of the dynamics map onto the mechanics of a charged\nparticle coupled to a magnetic field. We establish a theoretical basis for the\nuse of non-canonical Hamiltonian dynamics in MCMC, and construct a symplectic,\nleapfrog-like integrator allowing for the implementation of magnetic HMC.\nFinally, we exhibit several examples where these non-canonical dynamics can\nlead to improved mixing of magnetic HMC relative to ordinary HMC.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 12:19:29 GMT"}, {"version": "v2", "created": "Sat, 19 Aug 2017 20:18:38 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Tripuraneni", "Nilesh", ""], ["Rowland", "Mark", ""], ["Ghahramani", "Zoubin", ""], ["Turner", "Richard", ""]]}, {"id": "1607.02763", "submitter": "Oran Richman", "authors": "Oran Richman, Shie Mannor", "title": "How to Allocate Resources For Features Acquisition?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study classification problems where features are corrupted by noise and\nwhere the magnitude of the noise in each feature is influenced by the resources\nallocated to its acquisition. This is the case, for example, when multiple\nsensors share a common resource (power, bandwidth, attention, etc.). We develop\na method for computing the optimal resource allocation for a variety of\nscenarios and derive theoretical bounds concerning the benefit that may arise\nby non-uniform allocation. We further demonstrate the effectiveness of the\ndeveloped method in simulations.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 16:19:00 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Richman", "Oran", ""], ["Mannor", "Shie", ""]]}, {"id": "1607.02793", "submitter": "Xingguo Li", "authors": "Xingguo Li, Tuo Zhao, Raman Arora, Han Liu, Mingyi Hong", "title": "On Faster Convergence of Cyclic Block Coordinate Descent-type Methods\n  for Strongly Convex Minimization", "comments": "Accepted by JLMR", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cyclic block coordinate descent-type (CBCD-type) methods, which performs\niterative updates for a few coordinates (a block) simultaneously throughout the\nprocedure, have shown remarkable computational performance for solving strongly\nconvex minimization problems. Typical applications include many popular\nstatistical machine learning methods such as elastic-net regression, ridge\npenalized logistic regression, and sparse additive regression. Existing\noptimization literature has shown that for strongly convex minimization, the\nCBCD-type methods attain iteration complexity of\n$\\mathcal{O}(p\\log(1/\\epsilon))$, where $\\epsilon$ is a pre-specified accuracy\nof the objective value, and $p$ is the number of blocks. However, such\niteration complexity explicitly depends on $p$, and therefore is at least $p$\ntimes worse than the complexity $\\mathcal{O}(\\log(1/\\epsilon))$ of gradient\ndescent (GD) methods. To bridge this theoretical gap, we propose an improved\nconvergence analysis for the CBCD-type methods. In particular, we first show\nthat for a family of quadratic minimization problems, the iteration complexity\n$\\mathcal{O}(\\log^2(p)\\cdot\\log(1/\\epsilon))$ of the CBCD-type methods matches\nthat of the GD methods in term of dependency on $p$, up to a $\\log^2 p$ factor.\nThus our complexity bounds are sharper than the existing bounds by at least a\nfactor of $p/\\log^2(p)$. We also provide a lower bound to confirm that our\nimproved complexity bounds are tight (up to a $\\log^2 (p)$ factor), under the\nassumption that the largest and smallest eigenvalues of the Hessian matrix do\nnot scale with $p$. Finally, we generalize our analysis to other strongly\nconvex minimization problems beyond quadratic ones.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 23:15:18 GMT"}, {"version": "v2", "created": "Tue, 21 Mar 2017 15:04:11 GMT"}, {"version": "v3", "created": "Wed, 22 Nov 2017 15:40:31 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Li", "Xingguo", ""], ["Zhao", "Tuo", ""], ["Arora", "Raman", ""], ["Liu", "Han", ""], ["Hong", "Mingyi", ""]]}, {"id": "1607.02801", "submitter": "Francesco Renna", "authors": "Hugo Reboredo and Francesco Renna and Robert Calderbank and Miguel R.\n  D. Rodrigues", "title": "Bounds on the Number of Measurements for Reliable Compressive\n  Classification", "comments": "16 pages, 5 figures, 4 tables. Submitted to the IEEE Transactions on\n  Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2016.2599496", "report-no": null, "categories": "cs.IT cs.CV math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the classification of high-dimensional Gaussian signals\nfrom low-dimensional noisy, linear measurements. In particular, it provides\nupper bounds (sufficient conditions) on the number of measurements required to\ndrive the probability of misclassification to zero in the low-noise regime,\nboth for random measurements and designed ones. Such bounds reveal two\nimportant operational regimes that are a function of the characteristics of the\nsource: i) when the number of classes is less than or equal to the dimension of\nthe space spanned by signals in each class, reliable classification is possible\nin the low-noise regime by using a one-vs-all measurement design; ii) when the\ndimension of the spaces spanned by signals in each class is lower than the\nnumber of classes, reliable classification is guaranteed in the low-noise\nregime by using a simple random measurement design. Simulation results both\nwith synthetic and real data show that our analysis is sharp, in the sense that\nit is able to gauge the number of measurements required to drive the\nmisclassification probability to zero in the low-noise regime.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 01:08:14 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2016 17:59:32 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Reboredo", "Hugo", ""], ["Renna", "Francesco", ""], ["Calderbank", "Robert", ""], ["Rodrigues", "Miguel R. D.", ""]]}, {"id": "1607.02802", "submitter": "Franck Dernoncourt", "authors": "Franck Dernoncourt", "title": "Mapping distributional to model-theoretic semantic spaces: a baseline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings have been shown to be useful across state-of-the-art systems\nin many natural language processing tasks, ranging from question answering\nsystems to dependency parsing. (Herbelot and Vecchi, 2015) explored word\nembeddings and their utility for modeling language semantics. In particular,\nthey presented an approach to automatically map a standard distributional\nsemantic space onto a set-theoretic model using partial least squares\nregression. We show in this paper that a simple baseline achieves a +51%\nrelative improvement compared to their model on one of the two datasets they\nused, and yields competitive results on the second dataset.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 01:20:57 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Dernoncourt", "Franck", ""]]}, {"id": "1607.02914", "submitter": "Junichi Takeuchi", "authors": "Masanori Kawakita and Jun'ichi Takeuchi", "title": "Minimum Description Length Principle in Supervised Learning with\n  Application to Lasso", "comments": "Sumbitted, IEEE Transactions on Information Theory, on May 16th, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The minimum description length (MDL) principle in supervised learning is\nstudied. One of the most important theories for the MDL principle is Barron and\nCover's theory (BC theory), which gives a mathematical justification of the MDL\nprinciple. The original BC theory, however, can be applied to supervised\nlearning only approximately and limitedly. Though Barron et al. recently\nsucceeded in removing a similar approximation in case of unsupervised learning,\ntheir idea cannot be essentially applied to supervised learning in general. To\novercome this issue, an extension of BC theory to supervised learning is\nproposed. The derived risk bound has several advantages inherited from the\noriginal BC theory. First, the risk bound holds for finite sample size. Second,\nit requires remarkably few assumptions. Third, the risk bound has a form of\nredundancy of the two-stage code for the MDL procedure. Hence, the proposed\nextension gives a mathematical justification of the MDL principle to supervised\nlearning like the original BC theory. As an important example of application,\nnew risk and (probabilistic) regret bounds of lasso with random design are\nderived. The derived risk bound holds for any finite sample size $n$ and\nfeature number $p$ even if $n\\ll p$ without boundedness of features in contrast\nto the past work. Behavior of the regret bound is investigated by numerical\nsimulations. We believe that this is the first extension of BC theory to\ngeneral supervised learning with random design without approximation.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 12:18:20 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Kawakita", "Masanori", ""], ["Takeuchi", "Jun'ichi", ""]]}, {"id": "1607.02959", "submitter": "Asish Ghoshal", "authors": "Asish Ghoshal and Jean Honorio", "title": "From Behavior to Sparse Graphical Games: Efficient Recovery of\n  Equilibria", "comments": "Accepted at 54th Annual Allerton Conference on Communication,\n  Control, and Computing (2016)", "journal-ref": "Allerton Conference on Communication, Control, and Computing\n  (2016)", "doi": null, "report-no": null, "categories": "cs.GT cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of exact recovery of the pure-strategy\nNash equilibria (PSNE) set of a graphical game from noisy observations of joint\nactions of the players alone. We consider sparse linear influence games --- a\nparametric class of graphical games with linear payoffs, and represented by\ndirected graphs of n nodes (players) and in-degree of at most k. We present an\n$\\ell_1$-regularized logistic regression based algorithm for recovering the\nPSNE set exactly, that is both computationally efficient --- i.e. runs in\npolynomial time --- and statistically efficient --- i.e. has logarithmic sample\ncomplexity. Specifically, we show that the sufficient number of samples\nrequired for exact PSNE recovery scales as $\\mathcal{O}(\\mathrm{poly}(k) \\log\nn)$. We also validate our theoretical results using synthetic experiments.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 14:05:16 GMT"}, {"version": "v2", "created": "Wed, 19 Oct 2016 19:56:19 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Ghoshal", "Asish", ""], ["Honorio", "Jean", ""]]}, {"id": "1607.03026", "submitter": "Cyrus Samii", "authors": "Cyrus Samii, Laura Paler and Sarah Zukerman Daly", "title": "Retrospective Causal Inference with Machine Learning Ensembles: An\n  Application to Anti-Recidivism Policies in Colombia", "comments": null, "journal-ref": "Political Analysis (2016)", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new methods to estimate causal effects retrospectively from micro\ndata with the assistance of a machine learning ensemble. This approach\novercomes two important limitations in conventional methods like regression\nmodeling or matching: (i) ambiguity about the pertinent retrospective\ncounterfactuals and (ii) potential misspecification, overfitting, and otherwise\nbias-prone or inefficient use of a large identifying covariate set in the\nestimation of causal effects. Our method targets the analysis toward a well\ndefined ``retrospective intervention effect'' (RIE) based on hypothetical\npopulation interventions and applies a machine learning ensemble that allows\ndata to guide us, in a controlled fashion, on how to use a large identifying\ncovariate set. We illustrate with an analysis of policy options for reducing\nex-combatant recidivism in Colombia.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 16:47:47 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Samii", "Cyrus", ""], ["Paler", "Laura", ""], ["Daly", "Sarah Zukerman", ""]]}, {"id": "1607.03050", "submitter": "Daniel Jiwoong  Im", "authors": "Daniel Jiwoong Im, Graham W. Taylor", "title": "Learning a metric for class-conditional KNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Naive Bayes Nearest Neighbour (NBNN) is a simple and effective framework\nwhich addresses many of the pitfalls of K-Nearest Neighbour (KNN)\nclassification. It has yielded competitive results on several computer vision\nbenchmarks. Its central tenet is that during NN search, a query is not compared\nto every example in a database, ignoring class information. Instead, NN\nsearches are performed within each class, generating a score per class. A key\nproblem with NN techniques, including NBNN, is that they fail when the data\nrepresentation does not capture perceptual (e.g.~class-based) similarity. NBNN\ncircumvents this by using independent engineered descriptors (e.g.~SIFT). To\nextend its applicability outside of image-based domains, we propose to learn a\nmetric which captures perceptual similarity. Similar to how Neighbourhood\nComponents Analysis optimizes a differentiable form of KNN classification, we\npropose \"Class Conditional\" metric learning (CCML), which optimizes a soft form\nof the NBNN selection rule. Typical metric learning algorithms learn either a\nglobal or local metric. However, our proposed method can be adjusted to a\nparticular level of locality by tuning a single parameter. An empirical\nevaluation on classification and retrieval tasks demonstrates that our proposed\nmethod clearly outperforms existing learned distance metrics across a variety\nof image and non-image datasets.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 17:29:19 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Im", "Daniel Jiwoong", ""], ["Taylor", "Graham W.", ""]]}, {"id": "1607.03081", "submitter": "Hiva Ghanbari", "authors": "Hiva Ghanbari, Katya Scheinberg", "title": "Proximal Quasi-Newton Methods for Regularized Convex Optimization with\n  Linear and Accelerated Sublinear Convergence Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In [19], a general, inexact, efficient proximal quasi-Newton algorithm for\ncomposite optimization problems has been proposed and a sublinear global\nconvergence rate has been established. In this paper, we analyze the\nconvergence properties of this method, both in the exact and inexact setting,\nin the case when the objective function is strongly convex. We also investigate\na practical variant of this method by establishing a simple stopping criterion\nfor the subproblem optimization. Furthermore, we consider an accelerated\nvariant, based on FISTA [1], to the proximal quasi-Newton algorithm. A similar\naccelerated method has been considered in [7], where the convergence rate\nanalysis relies on very strong impractical assumptions. We present a modified\nanalysis while relaxing these assumptions and perform a practical comparison of\nthe accelerated proximal quasi- Newton algorithm and the regular one. Our\nanalysis and computational results show that acceleration may not bring any\nbenefit in the quasi-Newton setting.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 19:20:06 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 01:08:29 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Ghanbari", "Hiva", ""], ["Scheinberg", "Katya", ""]]}, {"id": "1607.03084", "submitter": "Sebastien Bubeck", "authors": "S\\'ebastien Bubeck and Ronen Eldan and Yin Tat Lee", "title": "Kernel-based methods for bandit convex optimization", "comments": "45 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the adversarial convex bandit problem and we build the first\n$\\mathrm{poly}(T)$-time algorithm with $\\mathrm{poly}(n) \\sqrt{T}$-regret for\nthis problem. To do so we introduce three new ideas in the derivative-free\noptimization literature: (i) kernel methods, (ii) a generalization of Bernoulli\nconvolutions, and (iii) a new annealing schedule for exponential weights (with\nincreasing learning rate). The basic version of our algorithm achieves\n$\\tilde{O}(n^{9.5} \\sqrt{T})$-regret, and we show that a simple variant of this\nalgorithm can be run in $\\mathrm{poly}(n \\log(T))$-time per step at the cost of\nan additional $\\mathrm{poly}(n) T^{o(1)}$ factor in the regret. These results\nimprove upon the $\\tilde{O}(n^{11} \\sqrt{T})$-regret and\n$\\exp(\\mathrm{poly}(T))$-time result of the first two authors, and the\n$\\log(T)^{\\mathrm{poly}(n)} \\sqrt{T}$-regret and\n$\\log(T)^{\\mathrm{poly}(n)}$-time result of Hazan and Li. Furthermore we\nconjecture that another variant of the algorithm could achieve\n$\\tilde{O}(n^{1.5} \\sqrt{T})$-regret, and moreover that this regret is\nunimprovable (the current best lower bound being $\\Omega(n \\sqrt{T})$ and it is\nachieved with linear functions). For the simpler situation of zeroth order\nstochastic convex optimization this corresponds to the conjecture that the\noptimal query complexity is of order $n^3 / \\epsilon^2$.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 19:25:07 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Eldan", "Ronen", ""], ["Lee", "Yin Tat", ""]]}, {"id": "1607.03183", "submitter": "Andrej Risteski", "authors": "Andrej Risteski", "title": "How to calculate partition functions using convex programming\n  hierarchies: provable bounds for variational methods", "comments": "This paper was accepted for presentation at Conference on Learning\n  Theory (COLT) 2016", "journal-ref": "29th Annual Conference on Learning Theory (pp. 1402-1416), 2016", "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of approximating partition functions for Ising\nmodels. We make use of recent tools in combinatorial optimization: the\nSherali-Adams and Lasserre convex programming hierarchies, in combination with\nvariational methods to get algorithms for calculating partition functions in\nthese families. These techniques give new, non-trivial approximation guarantees\nfor the partition function beyond the regime of correlation decay. They also\ngeneralize some classical results from statistical physics about the\nCurie-Weiss ferromagnetic Ising model, as well as provide a partition function\ncounterpart of classical results about max-cut on dense graphs\n\\cite{arora1995polynomial}. With this, we connect techniques from two\napparently disparate research areas -- optimization and counting/partition\nfunction approximations. (i.e. \\#-P type of problems).\n  Furthermore, we design to the best of our knowledge the first provable,\nconvex variational methods. Though in the literature there are a host of convex\nversions of variational methods \\cite{wainwright2003tree, wainwright2005new,\nheskes2006convexity, meshi2009convexifying}, they come with no guarantees\n(apart from some extremely special cases, like e.g. the graph has a single\ncycle \\cite{weiss2000correctness}). We consider dense and low threshold rank\ngraphs, and interestingly, the reason our approach works on these types of\ngraphs is because local correlations propagate to global correlations --\ncompletely the opposite of algorithms based on correlation decay. In the\nprocess we design novel entropy approximations based on the low-order moments\nof a distribution.\n  Our proof techniques are very simple and generic, and likely to be applicable\nto many other settings other than Ising models.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 22:10:04 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Risteski", "Andrej", ""]]}, {"id": "1607.03191", "submitter": "Vaneet Aggarwal", "authors": "Wenqi Wang and Shuchin Aeron and Vaneet Aggarwal", "title": "On Deterministic Conditions for Subspace Clustering under Missing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present deterministic conditions for success of sparse\nsubspace clustering (SSC) under missing data, when data is assumed to come from\na Union of Subspaces (UoS) model. We consider two algorithms, which are\nvariants of SSC with entry-wise zero-filling that differ in terms of the\noptimization problems used to find affinity matrix for spectral clustering. For\nboth the algorithms, we provide deterministic conditions for any pattern of\nmissing data such that perfect clustering can be achieved. We provide extensive\nsets of simulation results for clustering as well as completion of data at\nmissing entries, under the UoS model. Our experimental results indicate that in\ncontrast to the full data case, accurate clustering does not imply accurate\nsubspace identification and completion, indicating the natural order of\nrelative hardness of these problems.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 22:40:31 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Wang", "Wenqi", ""], ["Aeron", "Shuchin", ""], ["Aggarwal", "Vaneet", ""]]}, {"id": "1607.03200", "submitter": "Fionn Murtagh", "authors": "Fionn Murtagh, Michael Orlov, Boris Mirkin", "title": "Qualitative Judgement of Research Impact: Domain Taxonomy as a\n  Fundamental Framework for Judgement of the Quality of Research", "comments": "22 pages, 7 figures, Journal of Classification, Online First, March\n  25, 2018", "journal-ref": null, "doi": "10.1007/s00357-018-9247-0", "report-no": null, "categories": "cs.DL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The appeal of metric evaluation of research impact has attracted considerable\ninterest in recent times. Although the public at large and administrative\nbodies are much interested in the idea, scientists and other researchers are\nmuch more cautious, insisting that metrics are but an auxiliary instrument to\nthe qualitative peer-based judgement. The goal of this article is to propose\navailing of such a well positioned construct as domain taxonomy as a tool for\ndirectly assessing the scope and quality of research. We first show how\ntaxonomies can be used to analyse the scope and perspectives of a set of\nresearch projects or papers. Then we proceed to define a research team or\nresearcher's rank by those nodes in the hierarchy that have been created or\nsignificantly transformed by the results of the researcher. An experimental\ntest of the approach in the data analysis domain is described. Although the\nconcept of taxonomy seems rather simplistic to describe all the richness of a\nresearch domain, its changes and use can be made transparent and subject to\nopen discussions.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 23:42:36 GMT"}, {"version": "v2", "created": "Sun, 8 Apr 2018 11:49:19 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Murtagh", "Fionn", ""], ["Orlov", "Michael", ""], ["Mirkin", "Boris", ""]]}, {"id": "1607.03202", "submitter": "Eric Lundquist", "authors": "Anders Drachen, Eric Thurston Lundquist, Yungjen Kung, Pranav Simha\n  Rao, Diego Klabjan, Rafet Sifa and Julian Runge", "title": "Rapid Prediction of Player Retention in Free-to-Play Mobile Games", "comments": "Draft Submitted to AIIDE-16. 7 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting and improving player retention is crucial to the success of mobile\nFree-to-Play games. This paper explores the problem of rapid retention\nprediction in this context. Heuristic modeling approaches are introduced as a\nway of building simple rules for predicting short-term retention. Compared to\ncommon classification algorithms, our heuristic-based approach achieves\nreasonable and comparable performance using information from the first session,\nday, and week of player activity.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 00:03:05 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Drachen", "Anders", ""], ["Lundquist", "Eric Thurston", ""], ["Kung", "Yungjen", ""], ["Rao", "Pranav Simha", ""], ["Klabjan", "Diego", ""], ["Sifa", "Rafet", ""], ["Runge", "Julian", ""]]}, {"id": "1607.03204", "submitter": "Rajiv Khanna", "authors": "Rajiv Khanna, Joydeep Ghosh, Russell Poldrack, Oluwasanmi Koyejo", "title": "Information Projection and Approximate Inference for Structured Sparse\n  Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate inference via information projection has been recently introduced\nas a general-purpose approach for efficient probabilistic inference given\nsparse variables. This manuscript goes beyond classical sparsity by proposing\nefficient algorithms for approximate inference via information projection that\nare applicable to any structure on the set of variables that admits enumeration\nusing a \\emph{matroid}. We show that the resulting information projection can\nbe reduced to combinatorial submodular optimization subject to matroid\nconstraints. Further, leveraging recent advances in submodular optimization, we\nprovide an efficient greedy algorithm with strong optimization-theoretic\nguarantees. The class of probabilistic models that can be expressed in this way\nis quite broad and, as we show, includes group sparse regression, group sparse\nprincipal components analysis and sparse canonical correlation analysis, among\nothers. Moreover, empirical results on simulated data and high dimensional\nneuroimaging data highlight the superior performance of the information\nprojection approach as compared to established baselines for a range of\nprobabilistic models.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 00:11:59 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Khanna", "Rajiv", ""], ["Ghosh", "Joydeep", ""], ["Poldrack", "Russell", ""], ["Koyejo", "Oluwasanmi", ""]]}, {"id": "1607.03300", "submitter": "David Lopez-Paz", "authors": "David Lopez-Paz", "title": "From Dependence to Causation", "comments": "PhD thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is the science of discovering statistical dependencies in\ndata, and the use of those dependencies to perform predictions. During the last\ndecade, machine learning has made spectacular progress, surpassing human\nperformance in complex tasks such as object recognition, car driving, and\ncomputer gaming. However, the central role of prediction in machine learning\navoids progress towards general-purpose artificial intelligence. As one way\nforward, we argue that causal inference is a fundamental component of human\nintelligence, yet ignored by learning algorithms.\n  Causal inference is the problem of uncovering the cause-effect relationships\nbetween the variables of a data generating system. Causal structures provide\nunderstanding about how these systems behave under changing, unseen\nenvironments. In turn, knowledge about these causal dynamics allows to answer\n\"what if\" questions, describing the potential responses of the system under\nhypothetical manipulations and interventions. Thus, understanding cause and\neffect is one step from machine learning towards machine reasoning and machine\nintelligence. But, currently available causal inference algorithms operate in\nspecific regimes, and rely on assumptions that are difficult to verify in\npractice.\n  This thesis advances the art of causal inference in three different ways.\nFirst, we develop a framework for the study of statistical dependence based on\ncopulas and random features. Second, we build on this framework to interpret\nthe problem of causal inference as the task of distribution classification,\nyielding a family of novel causal inference algorithms. Third, we discover\ncausal structures in convolutional neural network features using our\nalgorithms. The algorithms presented in this thesis are scalable, exhibit\nstrong theoretical guarantees, and achieve state-of-the-art performance in a\nvariety of real-world benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 10:32:02 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Lopez-Paz", "David", ""]]}, {"id": "1607.03313", "submitter": "Andreas Loukas", "authors": "Andreas Loukas and Nathanael Perraudin", "title": "Predicting the evolution of stationary graph signals", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An emerging way of tackling the dimensionality issues arising in the modeling\nof a multivariate process is to assume that the inherent data structure can be\ncaptured by a graph. Nevertheless, though state-of-the-art graph-based methods\nhave been successful for many learning tasks, they do not consider\ntime-evolving signals and thus are not suitable for prediction. Based on the\nrecently introduced joint stationarity framework for time-vertex processes,\nthis letter considers multivariate models that exploit the graph topology so as\nto facilitate the prediction. The resulting method yields similar accuracy to\nthe joint (time-graph) mean-squared error estimator but at lower complexity,\nand outperforms purely time-based methods.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 11:30:30 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Loukas", "Andreas", ""], ["Perraudin", "Nathanael", ""]]}, {"id": "1607.03360", "submitter": "Andrej Risteski", "authors": "Yuanzhi Li, Andrej Risteski", "title": "Approximate maximum entropy principles via Goemans-Williamson with\n  applications to provable variational methods", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The well known maximum-entropy principle due to Jaynes, which states that\ngiven mean parameters, the maximum entropy distribution matching them is in an\nexponential family, has been very popular in machine learning due to its\n\"Occam's razor\" interpretation. Unfortunately, calculating the potentials in\nthe maximum-entropy distribution is intractable \\cite{bresler2014hardness}. We\nprovide computationally efficient versions of this principle when the mean\nparameters are pairwise moments: we design distributions that approximately\nmatch given pairwise moments, while having entropy which is comparable to the\nmaximum entropy distribution matching those moments.\n  We additionally provide surprising applications of the approximate maximum\nentropy principle to designing provable variational methods for partition\nfunction calculations for Ising models without any assumptions on the\npotentials of the model. More precisely, we show that in every temperature, we\ncan get approximation guarantees for the log-partition function comparable to\nthose in the low-temperature limit, which is the setting of optimization of\nquadratic forms over the hypercube. \\cite{alon2006approximating}\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 14:09:03 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Li", "Yuanzhi", ""], ["Risteski", "Andrej", ""]]}, {"id": "1607.03392", "submitter": "Christian Dansereau", "authors": "Christian Dansereau, Yassine Benhajali, Celine Risterucci, Emilio\n  Merlo Pich, Pierre Orban, Douglas Arnold, Pierre Bellec", "title": "Statistical power and prediction accuracy in multisite resting-state\n  fMRI connectivity", "comments": null, "journal-ref": "NeuroImage.Vol 149, p. 220-232 (2017)", "doi": "10.1016/j.neuroimage.2017.01.072", "report-no": null, "categories": "q-bio.QM cs.CE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Connectivity studies using resting-state functional magnetic resonance\nimaging are increasingly pooling data acquired at multiple sites. While this\nmay allow investigators to speed up recruitment or increase sample size,\nmultisite studies also potentially introduce systematic biases in connectivity\nmeasures across sites. In this work, we measure the inter-site effect in\nconnectivity and its impact on our ability to detect individual and group\ndifferences. Our study was based on real, as opposed to simulated, multisite\nfMRI datasets collected in N=345 young, healthy subjects across 8 scanning\nsites with 3T scanners and heterogeneous scanning protocols, drawn from the\n1000 functional connectome project. We first empirically show that typical\nfunctional networks were reliably found at the group level in all sites, and\nthat the amplitude of the inter-site effects was small to moderate, with a\nCohen's effect size below 0.5 on average across brain connections. We then\nimplemented a series of Monte-Carlo simulations, based on real data, to\nevaluate the impact of the multisite effects on detection power in statistical\ntests comparing two groups (with and without the effect) using a general linear\nmodel, as well as on the prediction of group labels with a support-vector\nmachine. As a reference, we also implemented the same simulations with fMRI\ndata collected at a single site using an identical sample size. Simulations\nrevealed that using data from heterogeneous sites only slightly decreased our\nability to detect changes compared to a monosite study with the GLM, and had a\ngreater impact on prediction accuracy. Taken together, our results support the\nfeasibility of multisite studies in rs-fMRI provided the sample size is large\nenough.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 15:22:52 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 19:47:13 GMT"}, {"version": "v3", "created": "Fri, 27 Jan 2017 17:57:06 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Dansereau", "Christian", ""], ["Benhajali", "Yassine", ""], ["Risterucci", "Celine", ""], ["Pich", "Emilio Merlo", ""], ["Orban", "Pierre", ""], ["Arnold", "Douglas", ""], ["Bellec", "Pierre", ""]]}, {"id": "1607.03428", "submitter": "Pantita Palittapongarnpim", "authors": "Pantita Palittapongarnpim, Peter Wittek, Ehsan Zahedinejad, Shakib\n  Vedaie, Barry C. Sanders", "title": "Learning in Quantum Control: High-Dimensional Global Optimization for\n  Noisy Quantum Dynamics", "comments": "32 pages, 4 figures, extension of proceedings in ESANN 2016\n  conference submitted to Neurocomputing", "journal-ref": "Neurocomputing 268 (2017) 116-126", "doi": "10.1016/j.neucom.2016.12.087", "report-no": null, "categories": "cs.LG cs.SY quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum control is valuable for various quantum technologies such as\nhigh-fidelity gates for universal quantum computing, adaptive quantum-enhanced\nmetrology, and ultra-cold atom manipulation. Although supervised machine\nlearning and reinforcement learning are widely used for optimizing control\nparameters in classical systems, quantum control for parameter optimization is\nmainly pursued via gradient-based greedy algorithms. Although the quantum\nfitness landscape is often compatible with greedy algorithms, sometimes greedy\nalgorithms yield poor results, especially for large-dimensional quantum\nsystems. We employ differential evolution algorithms to circumvent the\nstagnation problem of non-convex optimization. We improve quantum control\nfidelity for noisy system by averaging over the objective function. To reduce\ncomputational cost, we introduce heuristics for early termination of runs and\nfor adaptive selection of search subspaces. Our implementation is massively\nparallel and vectorized to reduce run time even further. We demonstrate our\nmethods with two examples, namely quantum phase estimation and quantum gate\ndesign, for which we achieve superior fidelity and scalability than obtained\nusing greedy algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 16:17:38 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 21:47:53 GMT"}, {"version": "v3", "created": "Fri, 25 Nov 2016 23:24:10 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Palittapongarnpim", "Pantita", ""], ["Wittek", "Peter", ""], ["Zahedinejad", "Ehsan", ""], ["Vedaie", "Shakib", ""], ["Sanders", "Barry C.", ""]]}, {"id": "1607.03456", "submitter": "Moshe Salhov", "authors": "Amit Bermanis, Aviv Rotbart, Moshe Salhov and Amir Averbuch", "title": "Incomplete Pivoted QR-based Dimensionality Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional big data appears in many research fields such as image\nrecognition, biology and collaborative filtering. Often, the exploration of\nsuch data by classic algorithms is encountered with difficulties due to `curse\nof dimensionality' phenomenon. Therefore, dimensionality reduction methods are\napplied to the data prior to its analysis. Many of these methods are based on\nprincipal components analysis, which is statistically driven, namely they map\nthe data into a low-dimension subspace that preserves significant statistical\nproperties of the high-dimensional data. As a consequence, such methods do not\ndirectly address the geometry of the data, reflected by the mutual distances\nbetween multidimensional data point. Thus, operations such as classification,\nanomaly detection or other machine learning tasks may be affected.\n  This work provides a dictionary-based framework for geometrically driven data\nanalysis that includes dimensionality reduction, out-of-sample extension and\nanomaly detection. It embeds high-dimensional data in a low-dimensional\nsubspace. This embedding preserves the original high-dimensional geometry of\nthe data up to a user-defined distortion rate. In addition, it identifies a\nsubset of landmark data points that constitute a dictionary for the analyzed\ndataset. The dictionary enables to have a natural extension of the\nlow-dimensional embedding to out-of-sample data points, which gives rise to a\ndistortion-based criterion for anomaly detection. The suggested method is\ndemonstrated on synthetic and real-world datasets and achieves good results for\nclassification, anomaly detection and out-of-sample tasks.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 18:20:23 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Bermanis", "Amit", ""], ["Rotbart", "Aviv", ""], ["Salhov", "Moshe", ""], ["Averbuch", "Amir", ""]]}, {"id": "1607.03463", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Yuanzhi Li", "title": "LazySVD: Even Faster SVD Decomposition Yet Without Agonizing Pain", "comments": "first circulated on May 20, 2016; this newer version improves writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DS cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study $k$-SVD that is to obtain the first $k$ singular vectors of a matrix\n$A$. Recently, a few breakthroughs have been discovered on $k$-SVD: Musco and\nMusco [1] proved the first gap-free convergence result using the block Krylov\nmethod, Shamir [2] discovered the first variance-reduction stochastic method,\nand Bhojanapalli et al. [3] provided the fastest $O(\\mathsf{nnz}(A) +\n\\mathsf{poly}(1/\\varepsilon))$-time algorithm using alternating minimization.\n  In this paper, we put forward a new and simple LazySVD framework to improve\nthe above breakthroughs. This framework leads to a faster gap-free method\noutperforming [1], and the first accelerated and stochastic method\noutperforming [2]. In the $O(\\mathsf{nnz}(A) + \\mathsf{poly}(1/\\varepsilon))$\nrunning-time regime, LazySVD outperforms [3] in certain parameter regimes\nwithout even using alternating minimization.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 18:41:52 GMT"}, {"version": "v2", "created": "Mon, 23 Jan 2017 18:55:31 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Li", "Yuanzhi", ""]]}, {"id": "1607.03475", "submitter": "Ping Li", "authors": "Ping Li", "title": "Nystrom Method for Approximating the GMM Kernel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The GMM (generalized min-max) kernel was recently proposed (Li, 2016) as a\nmeasure of data similarity and was demonstrated effective in machine learning\ntasks. In order to use the GMM kernel for large-scale datasets, the prior work\nresorted to the (generalized) consistent weighted sampling (GCWS) to convert\nthe GMM kernel to linear kernel. We call this approach as ``GMM-GCWS''.\n  In the machine learning literature, there is a popular algorithm which we\ncall ``RBF-RFF''. That is, one can use the ``random Fourier features'' (RFF) to\nconvert the ``radial basis function'' (RBF) kernel to linear kernel. It was\nempirically shown in (Li, 2016) that RBF-RFF typically requires substantially\nmore samples than GMM-GCWS in order to achieve comparable accuracies.\n  The Nystrom method is a general tool for computing nonlinear kernels, which\nagain converts nonlinear kernels into linear kernels. We apply the Nystrom\nmethod for approximating the GMM kernel, a strategy which we name as\n``GMM-NYS''. In this study, our extensive experiments on a set of fairly large\ndatasets confirm that GMM-NYS is also a strong competitor of RBF-RFF.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 19:42:40 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "1607.03502", "submitter": "Manuel J. A. Eugster", "authors": "Manuel J. A. Eugster, Tuukka Ruotsalo, Michiel M. Spap\\'e, Oswald\n  Barral, Niklas Ravaja, Giulio Jacucci, Samuel Kaski", "title": "Natural brain-information interfaces: Recommending information by\n  relevance inferred from human brain signals", "comments": null, "journal-ref": "Scientific Reports 6, Article number: 38580 (2016)", "doi": "10.1038/srep38580", "report-no": null, "categories": "cs.IR cs.HC q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding relevant information from large document collections such as the\nWorld Wide Web is a common task in our daily lives. Estimation of a user's\ninterest or search intention is necessary to recommend and retrieve relevant\ninformation from these collections. We introduce a brain-information interface\nused for recommending information by relevance inferred directly from brain\nsignals. In experiments, participants were asked to read Wikipedia documents\nabout a selection of topics while their EEG was recorded. Based on the\nprediction of word relevance, the individual's search intent was modeled and\nsuccessfully used for retrieving new, relevant documents from the whole English\nWikipedia corpus. The results show that the users' interests towards digital\ncontent can be modeled from the brain signals evoked by reading. The introduced\nbrain-relevance paradigm enables the recommendation of information without any\nexplicit user interaction, and may be applied across diverse\ninformation-intensive applications.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 20:17:00 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Eugster", "Manuel J. A.", ""], ["Ruotsalo", "Tuukka", ""], ["Spap\u00e9", "Michiel M.", ""], ["Barral", "Oswald", ""], ["Ravaja", "Niklas", ""], ["Jacucci", "Giulio", ""], ["Kaski", "Samuel", ""]]}, {"id": "1607.03516", "submitter": "Muhammad Ghifary", "authors": "Muhammad Ghifary and W. Bastiaan Kleijn and Mengjie Zhang and David\n  Balduzzi and Wen Li", "title": "Deep Reconstruction-Classification Networks for Unsupervised Domain\n  Adaptation", "comments": "to appear in European Conference on Computer Vision (ECCV) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel unsupervised domain adaptation algorithm\nbased on deep learning for visual object recognition. Specifically, we design a\nnew model called Deep Reconstruction-Classification Network (DRCN), which\njointly learns a shared encoding representation for two tasks: i) supervised\nclassification of labeled source data, and ii) unsupervised reconstruction of\nunlabeled target data.In this way, the learnt representation not only preserves\ndiscriminability, but also encodes useful information from the target domain.\nOur new DRCN model can be optimized by using backpropagation similarly as the\nstandard neural networks.\n  We evaluate the performance of DRCN on a series of cross-domain object\nrecognition tasks, where DRCN provides a considerable improvement (up to ~8% in\naccuracy) over the prior state-of-the-art algorithms. Interestingly, we also\nobserve that the reconstruction pipeline of DRCN transforms images from the\nsource domain into images whose appearance resembles the target dataset. This\nsuggests that DRCN's performance is due to constructing a single composite\nrepresentation that encodes information about both the structure of target\nimages and the classification of source images. Finally, we provide a formal\nanalysis to justify the algorithm's objective in domain adaptation context.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 20:48:58 GMT"}, {"version": "v2", "created": "Mon, 1 Aug 2016 09:58:13 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Ghifary", "Muhammad", ""], ["Kleijn", "W. Bastiaan", ""], ["Zhang", "Mengjie", ""], ["Balduzzi", "David", ""], ["Li", "Wen", ""]]}, {"id": "1607.03559", "submitter": "Stefanie Jegelka", "authors": "Chengtao Li, Stefanie Jegelka, Suvrit Sra", "title": "Fast Sampling for Strongly Rayleigh Measures with Application to\n  Determinantal Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we consider sampling from (non-homogeneous) strongly Rayleigh\nprobability measures. As an important corollary, we obtain a fast mixing Markov\nChain sampler for Determinantal Point Processes.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 01:22:04 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Li", "Chengtao", ""], ["Jegelka", "Stefanie", ""], ["Sra", "Suvrit", ""]]}, {"id": "1607.03574", "submitter": "Keisuke Yamazaki", "authors": "Keisuke Yamazaki", "title": "Effects of Additional Data on Bayesian Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical probabilistic models, such as mixture models, are used for\ncluster analysis. These models have two types of variables: observable and\nlatent. In cluster analysis, the latent variable is estimated, and it is\nexpected that additional information will improve the accuracy of the\nestimation of the latent variable. Many proposed learning methods are able to\nuse additional data; these include semi-supervised learning and transfer\nlearning. However, from a statistical point of view, a complex probabilistic\nmodel that encompasses both the initial and additional data might be less\naccurate due to having a higher-dimensional parameter. The present paper\npresents a theoretical analysis of the accuracy of such a model and clarifies\nwhich factor has the greatest effect on its accuracy, the advantages of\nobtaining additional data, and the disadvantages of increasing the complexity.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 02:41:24 GMT"}, {"version": "v2", "created": "Fri, 25 Nov 2016 08:03:20 GMT"}, {"version": "v3", "created": "Tue, 28 Feb 2017 02:12:09 GMT"}, {"version": "v4", "created": "Fri, 23 Jun 2017 04:55:08 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Yamazaki", "Keisuke", ""]]}, {"id": "1607.03615", "submitter": "Sheng-Mao Chang", "authors": "Ray-Bing Chen, Kuang-Hung Cheng, Sheng-Mao Chang, Shuen-Lin Jeng,\n  Ping-Yang Chen, Chun-Hao Yang, and Chi-Chun Hsia", "title": "Multiple-Instance Logistic Regression with LASSO Penalty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider a manufactory process which can be described by a\nmultiple-instance logistic regression model. In order to compute the maximum\nlikelihood estimation of the unknown coefficient, an expectation-maximization\nalgorithm is proposed, and the proposed modeling approach can be extended to\nidentify the important covariates by adding the coefficient penalty term into\nthe likelihood function. In addition to essential technical details, we\ndemonstrate the usefulness of the proposed method by simulations and real\nexamples.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 07:30:57 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Chen", "Ray-Bing", ""], ["Cheng", "Kuang-Hung", ""], ["Chang", "Sheng-Mao", ""], ["Jeng", "Shuen-Lin", ""], ["Chen", "Ping-Yang", ""], ["Yang", "Chun-Hao", ""], ["Hsia", "Chi-Chun", ""]]}, {"id": "1607.03730", "submitter": "Hamid Dadkhahi", "authors": "Hamid Dadkhahi, Nazir Saleheen, Santosh Kumar, Benjamin Marlin", "title": "Learning Shallow Detection Cascades for Wearable Sensor-Based Mobile\n  Health Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of mobile health aims to leverage recent advances in wearable\non-body sensing technology and smart phone computing capabilities to develop\nsystems that can monitor health states and deliver just-in-time adaptive\ninterventions. However, existing work has largely focused on analyzing\ncollected data in the off-line setting. In this paper, we propose a novel\napproach to learning shallow detection cascades developed explicitly for use in\na real-time wearable-phone or wearable-phone-cloud systems. We apply our\napproach to the problem of cigarette smoking detection from a combination of\nwrist-worn actigraphy data and respiration chest band data using two and three\nstage cascades.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 13:47:49 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Dadkhahi", "Hamid", ""], ["Saleheen", "Nazir", ""], ["Kumar", "Santosh", ""], ["Marlin", "Benjamin", ""]]}, {"id": "1607.03792", "submitter": "Hanyuan Hang", "authors": "Hanyuan Hang, Ingo Steinwart, Yunlong Feng, Johan A.K. Suykens", "title": "Kernel Density Estimation for Dynamical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the density estimation problem with observations generated by\ncertain dynamical systems that admit a unique underlying invariant Lebesgue\ndensity. Observations drawn from dynamical systems are not independent and\nmoreover, usual mixing concepts may not be appropriate for measuring the\ndependence among these observations. By employing the $\\mathcal{C}$-mixing\nconcept to measure the dependence, we conduct statistical analysis on the\nconsistency and convergence of the kernel density estimator. Our main results\nare as follows: First, we show that with properly chosen bandwidth, the kernel\ndensity estimator is universally consistent under $L_1$-norm; Second, we\nestablish convergence rates for the estimator with respect to several classes\nof dynamical systems under $L_1$-norm. In the analysis, the density function\n$f$ is only assumed to be H\\\"{o}lder continuous which is a weak assumption in\nthe literature of nonparametric density estimation and also more realistic in\nthe dynamical system context. Last but not least, we prove that the same\nconvergence rates of the estimator under $L_\\infty$-norm and $L_1$-norm can be\nachieved when the density function is H\\\"{o}lder continuous, compactly\nsupported and bounded. The bandwidth selection problem of the kernel density\nestimator for dynamical system is also discussed in our study via numerical\nsimulations.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 15:32:35 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Hang", "Hanyuan", ""], ["Steinwart", "Ingo", ""], ["Feng", "Yunlong", ""], ["Suykens", "Johan A. K.", ""]]}, {"id": "1607.03815", "submitter": "Tianbao Yang", "authors": "Yi Xu, Yan Yan, Qihang Lin, Tianbao Yang", "title": "Homotopy Smoothing for Non-Smooth Problems with Lower Complexity than\n  $O(1/\\epsilon)$", "comments": "This is a long version of the paper accepted by NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a novel {\\bf ho}moto{\\bf p}y {\\bf s}moothing (HOPS)\nalgorithm for solving a family of non-smooth problems that is composed of a\nnon-smooth term with an explicit max-structure and a smooth term or a simple\nnon-smooth term whose proximal mapping is easy to compute. The best known\niteration complexity for solving such non-smooth optimization problems is\n$O(1/\\epsilon)$ without any assumption on the strong convexity. In this work,\nwe will show that the proposed HOPS achieved a lower iteration complexity of\n$\\widetilde O(1/\\epsilon^{1-\\theta})$\\footnote{$\\widetilde O()$ suppresses a\nlogarithmic factor.} with $\\theta\\in(0,1]$ capturing the local sharpness of the\nobjective function around the optimal solutions. To the best of our knowledge,\nthis is the lowest iteration complexity achieved so far for the considered\nnon-smooth optimization problems without strong convexity assumption. The HOPS\nalgorithm employs Nesterov's smoothing technique and Nesterov's accelerated\ngradient method and runs in stages, which gradually decreases the smoothing\nparameter in a stage-wise manner until it yields a sufficiently good\napproximation of the original function. We show that HOPS enjoys a linear\nconvergence for many well-known non-smooth problems (e.g., empirical risk\nminimization with a piece-wise linear loss function and $\\ell_1$ norm\nregularizer, finding a point in a polyhedron, cone programming, etc).\nExperimental results verify the effectiveness of HOPS in comparison with\nNesterov's smoothing algorithm and the primal-dual style of first-order\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 16:20:52 GMT"}, {"version": "v2", "created": "Thu, 3 Nov 2016 16:06:16 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Xu", "Yi", ""], ["Yan", "Yan", ""], ["Lin", "Qihang", ""], ["Yang", "Tianbao", ""]]}, {"id": "1607.03822", "submitter": "Tony Basil", "authors": "Choudur Lakshminarayan and Tony Basil", "title": "Feature Extraction and Automated Classification of Heartbeats by Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present algorithms for the detection of a class of heart arrhythmias with\nthe goal of eventual adoption by practicing cardiologists. In clinical\npractice, detection is based on a small number of meaningful features extracted\nfrom the heartbeat cycle. However, techniques proposed in the literature use\nhigh dimensional vectors consisting of morphological, and time based features\nfor detection. Using electrocardiogram (ECG) signals, we found smaller subsets\nof features sufficient to detect arrhythmias with high accuracy. The features\nwere found by an iterative step-wise feature selection method. We depart from\ncommon literature in the following aspects: 1. As opposed to a high dimensional\nfeature vectors, we use a small set of features with meaningful clinical\ninterpretation, 2. we eliminate the necessity of short-duration\npatient-specific ECG data to append to the global training data for\nclassification 3. We apply semi-parametric classification procedures (in an\nensemble framework) for arrhythmia detection, and 4. our approach is based on a\nreduced sampling rate of ~ 115 Hz as opposed to 360 Hz in standard literature.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 16:46:55 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Lakshminarayan", "Choudur", ""], ["Basil", "Tony", ""]]}, {"id": "1607.03842", "submitter": "Marek Petrik", "authors": "Marek Petrik, Yinlam Chow, Mohammad Ghavamzadeh", "title": "Safe Policy Improvement by Minimizing Robust Baseline Regret", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important problem in sequential decision-making under uncertainty is to\nuse limited data to compute a safe policy, i.e., a policy that is guaranteed to\nperform at least as well as a given baseline strategy. In this paper, we\ndevelop and analyze a new model-based approach to compute a safe policy when we\nhave access to an inaccurate dynamics model of the system with known accuracy\nguarantees. Our proposed robust method uses this (inaccurate) model to directly\nminimize the (negative) regret w.r.t. the baseline policy. Contrary to the\nexisting approaches, minimizing the regret allows one to improve the baseline\npolicy in states with accurate dynamics and seamlessly fall back to the\nbaseline policy, otherwise. We show that our formulation is NP-hard and propose\nan approximate algorithm. Our empirical results on several domains show that\neven this relatively simple approximate algorithm can significantly outperform\nstandard approaches.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 17:55:45 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Petrik", "Marek", ""], ["Chow", "Yinlam", ""], ["Ghavamzadeh", "Mohammad", ""]]}, {"id": "1607.03849", "submitter": "Piotr Beben", "authors": "Piotr Beben", "title": "Fitting a Simplicial Complex using a Variation of k-means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a simple and effective two stage algorithm for approximating a point\ncloud $\\mathcal{S}\\subset\\mathbb{R}^m$ by a simplicial complex $K$. The first\nstage is an iterative fitting procedure that generalizes k-means clustering,\nwhile the second stage involves deleting redundant simplices. A form of\ndimension reduction of $\\mathcal{S}$ is obtained as a consequence.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 18:15:52 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2016 15:34:40 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Beben", "Piotr", ""]]}, {"id": "1607.03854", "submitter": "John V Monaco", "authors": "John V. Monaco and Charles C. Tappert", "title": "The Partially Observable Hidden Markov Model and its Application to\n  Keystroke Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR math.IT stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The partially observable hidden Markov model is an extension of the hidden\nMarkov Model in which the hidden state is conditioned on an independent Markov\nchain. This structure is motivated by the presence of discrete metadata, such\nas an event type, that may partially reveal the hidden state but itself\nemanates from a separate process. Such a scenario is encountered in keystroke\ndynamics whereby a user's typing behavior is dependent on the text that is\ntyped. Under the assumption that the user can be in either an active or passive\nstate of typing, the keyboard key names are event types that partially reveal\nthe hidden state due to the presence of relatively longer time intervals\nbetween words and sentences than between letters of a word. Using five public\ndatasets, the proposed model is shown to consistently outperform other anomaly\ndetectors, including the standard HMM, in biometric identification and\nverification tasks and is generally preferred over the HMM in a Monte Carlo\ngoodness of fit test.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 18:30:33 GMT"}, {"version": "v2", "created": "Tue, 23 Aug 2016 14:03:42 GMT"}, {"version": "v3", "created": "Mon, 29 Aug 2016 15:22:22 GMT"}, {"version": "v4", "created": "Wed, 21 Dec 2016 20:54:21 GMT"}, {"version": "v5", "created": "Tue, 11 Apr 2017 17:22:17 GMT"}, {"version": "v6", "created": "Fri, 17 Nov 2017 14:48:33 GMT"}, {"version": "v7", "created": "Mon, 20 Nov 2017 16:02:39 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Monaco", "John V.", ""], ["Tappert", "Charles C.", ""]]}, {"id": "1607.03954", "submitter": "Charles Matthews", "authors": "Charles Matthews and Jonathan Weare and Benedict Leimkuhler", "title": "Ensemble preconditioning for Markov chain Monte Carlo simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe parallel Markov chain Monte Carlo methods that propagate a\ncollective ensemble of paths, with local covariance information calculated from\nneighboring replicas. The use of collective dynamics eliminates multiplicative\nnoise and stabilizes the dynamics thus providing a practical approach to\ndifficult anisotropic sampling problems in high dimensions. Numerical\nexperiments with model problems demonstrate that dramatic potential speedups,\ncompared to various alternative schemes, are attainable.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 23:12:05 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Matthews", "Charles", ""], ["Weare", "Jonathan", ""], ["Leimkuhler", "Benedict", ""]]}, {"id": "1607.03975", "submitter": "Eric Strobl", "authors": "Eric V. Strobl, Peter L. Spirtes, Shyam Visweswaran", "title": "Estimating and Controlling the False Discovery Rate for the PC Algorithm\n  Using Edge-Specific P-Values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The PC algorithm allows investigators to estimate a complete partially\ndirected acyclic graph (CPDAG) from a finite dataset, but few groups have\ninvestigated strategies for estimating and controlling the false discovery rate\n(FDR) of the edges in the CPDAG. In this paper, we introduce PC with p-values\n(PC-p), a fast algorithm which robustly computes edge-specific p-values and\nthen estimates and controls the FDR across the edges. PC-p specifically uses\nthe p-values returned by many conditional independence tests to upper bound the\np-values of more complex edge-specific hypothesis tests. The algorithm then\nestimates and controls the FDR using the bounded p-values and the\nBenjamini-Yekutieli FDR procedure. Modifications to the original PC algorithm\nalso help PC-p accurately compute the upper bounds despite non-zero Type II\nerror rates. Experiments show that PC-p yields more accurate FDR estimation and\ncontrol across the edges in a variety of CPDAGs compared to alternative\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 01:34:57 GMT"}, {"version": "v2", "created": "Wed, 10 May 2017 03:43:48 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Strobl", "Eric V.", ""], ["Spirtes", "Peter L.", ""], ["Visweswaran", "Shyam", ""]]}, {"id": "1607.04209", "submitter": "Kirstin Early Kirstin Early", "authors": "Kirstin Early, Jennifer Mankoff, Stephen E. Fienberg", "title": "Dynamic Question Ordering in Online Surveys", "comments": "In submission to the Journal of Official Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online surveys have the potential to support adaptive questions, where later\nquestions depend on earlier responses. Past work has taken a rule-based\napproach, uniformly across all respondents. We envision a richer interpretation\nof adaptive questions, which we call dynamic question ordering (DQO), where\nquestion order is personalized. Such an approach could increase engagement, and\ntherefore response rate, as well as imputation quality. We present a DQO\nframework to improve survey completion and imputation. In the general\nsurvey-taking setting, we want to maximize survey completion, and so we focus\non ordering questions to engage the respondent and collect hopefully all\ninformation, or at least the information that most characterizes the\nrespondent, for accurate imputations. In another scenario, our goal is to\nprovide a personalized prediction. Since it is possible to give reasonable\npredictions with only a subset of questions, we are not concerned with\nmotivating users to answer all questions. Instead, we want to order questions\nto get information that reduces prediction uncertainty, while not being too\nburdensome. We illustrate this framework with an example of providing energy\nestimates to prospective tenants. We also discuss DQO for national surveys and\nconsider connections between our statistics-based question-ordering approach\nand cognitive survey methodology.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 17:03:24 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Early", "Kirstin", ""], ["Mankoff", "Jennifer", ""], ["Fienberg", "Stephen E.", ""]]}, {"id": "1607.04228", "submitter": "Evgeny Frolov", "authors": "Evgeny Frolov, Ivan Oseledets", "title": "Fifty Shades of Ratings: How to Benefit from a Negative Feedback in\n  Top-N Recommendations Tasks", "comments": "Accepted as a long paper at ACM RecSys 2016 conference, 8 pages, 6\n  figures, 2 tables", "journal-ref": null, "doi": "10.1145/2959100.2959170", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional collaborative filtering techniques treat a top-n recommendations\nproblem as a task of generating a list of the most relevant items. This\nformulation, however, disregards an opposite - avoiding recommendations with\ncompletely irrelevant items. Due to that bias, standard algorithms, as well as\ncommonly used evaluation metrics, become insensitive to negative feedback. In\norder to resolve this problem we propose to treat user feedback as a\ncategorical variable and model it with users and items in a ternary way. We\nemploy a third-order tensor factorization technique and implement a higher\norder folding-in method to support online recommendations. The method is\nequally sensitive to entire spectrum of user ratings and is able to accurately\npredict relevant items even from a negative only feedback. Our method may\npartially eliminate the need for complicated rating elicitation process as it\nprovides means for personalized recommendations from the very beginning of an\ninteraction with a recommender system. We also propose a modification of\nstandard metrics which helps to reveal unwanted biases and account for\nsensitivity to a negative feedback. Our model achieves state-of-the-art quality\nin standard recommendation tasks while significantly outperforming other\nmethods in the cold-start \"no-positive-feedback\" scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 17:55:33 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Frolov", "Evgeny", ""], ["Oseledets", "Ivan", ""]]}, {"id": "1607.04315", "submitter": "Tsendsuren Munkhdalai", "authors": "Tsendsuren Munkhdalai and Hong Yu", "title": "Neural Semantic Encoders", "comments": "Accepted in EACL 2017, added: comparison with NTM, qualitative\n  analysis and memory visualization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a memory augmented neural network for natural language\nunderstanding: Neural Semantic Encoders. NSE is equipped with a novel memory\nupdate rule and has a variable sized encoding memory that evolves over time and\nmaintains the understanding of input sequences through read}, compose and write\noperations. NSE can also access multiple and shared memories. In this paper, we\ndemonstrated the effectiveness and the flexibility of NSE on five different\nnatural language tasks: natural language inference, question answering,\nsentence classification, document sentiment analysis and machine translation\nwhere NSE achieved state-of-the-art performance when evaluated on publically\navailable benchmarks. For example, our shared-memory model showed an\nencouraging result on neural machine translation, improving an attention-based\nbaseline by approximately 1.0 BLEU.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 20:58:26 GMT"}, {"version": "v2", "created": "Wed, 20 Jul 2016 14:01:11 GMT"}, {"version": "v3", "created": "Thu, 5 Jan 2017 15:41:13 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Munkhdalai", "Tsendsuren", ""], ["Yu", "Hong", ""]]}, {"id": "1607.04331", "submitter": "Subhaneil Lahiri", "authors": "Subhaneil Lahiri, Peiran Gao, Surya Ganguli", "title": "Random projections of random manifolds", "comments": "45 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interesting data often concentrate on low dimensional smooth manifolds inside\na high dimensional ambient space. Random projections are a simple, powerful\ntool for dimensionality reduction of such data. Previous works have studied\nbounds on how many projections are needed to accurately preserve the geometry\nof these manifolds, given their intrinsic dimensionality, volume and curvature.\nHowever, such works employ definitions of volume and curvature that are\ninherently difficult to compute. Therefore such theory cannot be easily tested\nagainst numerical simulations to understand the tightness of the proven bounds.\nWe instead study typical distortions arising in random projections of an\nensemble of smooth Gaussian random manifolds. We find explicitly computable,\napproximate theoretical bounds on the number of projections required to\naccurately preserve the geometry of these manifolds. Our bounds, while\napproximate, can only be violated with a probability that is exponentially\nsmall in the ambient dimension, and therefore they hold with high probability\nin cases of practical interest. Moreover, unlike previous work, we test our\ntheoretical bounds against numerical experiments on the actual geometric\ndistortions that typically occur for random projections of random smooth\nmanifolds. We find our bounds are tighter than previous results by several\norders of magnitude.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 21:43:39 GMT"}, {"version": "v2", "created": "Sat, 10 Sep 2016 02:37:47 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Lahiri", "Subhaneil", ""], ["Gao", "Peiran", ""], ["Ganguli", "Surya", ""]]}, {"id": "1607.04492", "submitter": "Tsendsuren Munkhdalai", "authors": "Tsendsuren Munkhdalai and Hong Yu", "title": "Neural Tree Indexers for Text Understanding", "comments": "Accepted at EACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) process input text sequentially and model\nthe conditional transition between word tokens. In contrast, the advantages of\nrecursive networks include that they explicitly model the compositionality and\nthe recursive structure of natural language. However, the current recursive\narchitecture is limited by its dependence on syntactic tree. In this paper, we\nintroduce a robust syntactic parsing-independent tree structured model, Neural\nTree Indexers (NTI) that provides a middle ground between the sequential RNNs\nand the syntactic treebased recursive models. NTI constructs a full n-ary tree\nby processing the input text with its node function in a bottom-up fashion.\nAttention mechanism can then be applied to both structure and node function. We\nimplemented and evaluated a binarytree model of NTI, showing the model achieved\nthe state-of-the-art performance on three different NLP tasks: natural language\ninference, answer sentence selection, and sentence classification,\noutperforming state-of-the-art recurrent and recursive neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 12:59:01 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 17:10:33 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Munkhdalai", "Tsendsuren", ""], ["Yu", "Hong", ""]]}, {"id": "1607.04566", "submitter": "Stefan Steinerberger", "authors": "Alexander Cloninger, Stefan Steinerberger", "title": "Spectral Echolocation via the Wave Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral embedding uses eigenfunctions of the discrete Laplacian on a\nweighted graph to obtain coordinates for an embedding of an abstract data set\ninto Euclidean space. We propose a new pre-processing step of first using the\neigenfunctions to simulate a low-frequency wave moving over the data and using\nboth position as well as change in time of the wave to obtain a refined metric\nto which classical methods of dimensionality reduction can then applied. This\nis motivated by the behavior of waves, symmetries of the wave equation and the\nhunting technique of bats. It is shown to be effective in practice and also\nworks for other partial differential equations -- the method yields improved\nresults even for the classical heat equation.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 16:08:57 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Cloninger", "Alexander", ""], ["Steinerberger", "Stefan", ""]]}, {"id": "1607.04573", "submitter": "Luiz Gustavo Hafemann", "authors": "Luiz G. Hafemann, Robert Sabourin, Luiz S. Oliveira", "title": "Analyzing features learned for Offline Signature Verification using Deep\n  CNNs", "comments": "Accepted as a conference paper to ICPR 2016", "journal-ref": null, "doi": "10.1109/ICPR.2016.7900092", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on Offline Handwritten Signature Verification explored a large\nvariety of handcrafted feature extractors, ranging from graphology, texture\ndescriptors to interest points. In spite of advancements in the last decades,\nperformance of such systems is still far from optimal when we test the systems\nagainst skilled forgeries - signature forgeries that target a particular\nindividual. In previous research, we proposed a formulation of the problem to\nlearn features from data (signature images) in a Writer-Independent format,\nusing Deep Convolutional Neural Networks (CNNs), seeking to improve performance\non the task. In this research, we push further the performance of such method,\nexploring a range of architectures, and obtaining a large improvement in\nstate-of-the-art performance on the GPDS dataset, the largest publicly\navailable dataset on the task. In the GPDS-160 dataset, we obtained an Equal\nError Rate of 2.74%, compared to 6.97% in the best result published in\nliterature (that used a combination of multiple classifiers). We also present a\nvisual analysis of the feature space learned by the model, and an analysis of\nthe errors made by the classifier. Our analysis shows that the model is very\neffective in separating signatures that have a different global appearance,\nwhile being particularly vulnerable to forgeries that very closely resemble\ngenuine signatures, even if their line quality is bad, which is the case of\nslowly-traced forgeries.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 16:35:20 GMT"}, {"version": "v2", "created": "Fri, 26 Aug 2016 14:52:55 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Hafemann", "Luiz G.", ""], ["Sabourin", "Robert", ""], ["Oliveira", "Luiz S.", ""]]}, {"id": "1607.04579", "submitter": "Bo Dai", "authors": "Bo Dai, Niao He, Yunpeng Pan, Byron Boots, Le Song", "title": "Learning from Conditional Distributions via Dual Embeddings", "comments": "24 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning tasks, such as learning with invariance and policy\nevaluation in reinforcement learning, can be characterized as problems of\nlearning from conditional distributions. In such problems, each sample $x$\nitself is associated with a conditional distribution $p(z|x)$ represented by\nsamples $\\{z_i\\}_{i=1}^M$, and the goal is to learn a function $f$ that links\nthese conditional distributions to target values $y$. These learning problems\nbecome very challenging when we only have limited samples or in the extreme\ncase only one sample from each conditional distribution. Commonly used\napproaches either assume that $z$ is independent of $x$, or require an\noverwhelmingly large samples from each conditional distribution.\n  To address these challenges, we propose a novel approach which employs a new\nmin-max reformulation of the learning from conditional distribution problem.\nWith such new reformulation, we only need to deal with the joint distribution\n$p(z,x)$. We also design an efficient learning algorithm, Embedding-SGD, and\nestablish theoretical sample complexity for such problems. Finally, our\nnumerical experiments on both synthetic and real-world datasets show that the\nproposed approach can significantly improve over the existing algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 16:56:22 GMT"}, {"version": "v2", "created": "Sat, 31 Dec 2016 06:54:37 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Dai", "Bo", ""], ["He", "Niao", ""], ["Pan", "Yunpeng", ""], ["Boots", "Byron", ""], ["Song", "Le", ""]]}, {"id": "1607.04903", "submitter": "Stephanie L. Hyland", "authors": "Stephanie L. Hyland, Gunnar R\\\"atsch", "title": "Learning Unitary Operators with Help From u(n)", "comments": "9 pages, 3 figures, 5 figures inc. subfigures, to appear at AAAI-17", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in the training of recurrent neural networks is the\nso-called vanishing or exploding gradient problem. The use of a norm-preserving\ntransition operator can address this issue, but parametrization is challenging.\nIn this work we focus on unitary operators and describe a parametrization using\nthe Lie algebra $\\mathfrak{u}(n)$ associated with the Lie group $U(n)$ of $n\n\\times n$ unitary matrices. The exponential map provides a correspondence\nbetween these spaces, and allows us to define a unitary matrix using $n^2$ real\ncoefficients relative to a basis of the Lie algebra. The parametrization is\nclosed under additive updates of these coefficients, and thus provides a simple\nspace in which to do gradient descent. We demonstrate the effectiveness of this\nparametrization on the problem of learning arbitrary unitary operators,\ncomparing to several baselines and outperforming a recently-proposed\nlower-dimensional parametrization. We additionally use our parametrization to\ngeneralize a recently-proposed unitary recurrent neural network to arbitrary\nunitary matrices, using it to solve standard long-memory tasks.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jul 2016 18:58:12 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2016 16:02:25 GMT"}, {"version": "v3", "created": "Tue, 10 Jan 2017 11:13:35 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Hyland", "Stephanie L.", ""], ["R\u00e4tsch", "Gunnar", ""]]}, {"id": "1607.05002", "submitter": "Pourya Habib Zadeh", "authors": "Pourya Habib Zadeh, Reshad Hosseini and Suvrit Sra", "title": "Geometric Mean Metric Learning", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the task of learning a Euclidean metric from data. We approach\nthis problem from first principles and formulate it as a surprisingly simple\noptimization problem. Indeed, our formulation even admits a closed form\nsolution. This solution possesses several very attractive properties: (i) an\ninnate geometric appeal through the Riemannian geometry of positive definite\nmatrices; (ii) ease of interpretability; and (iii) computational speed several\norders of magnitude faster than the widely used LMNN and ITML methods.\nFurthermore, on standard benchmark datasets, our closed-form solution\nconsistently attains higher classification accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 10:14:46 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Zadeh", "Pourya Habib", ""], ["Hosseini", "Reshad", ""], ["Sra", "Suvrit", ""]]}, {"id": "1607.05047", "submitter": "Susan Murphy A", "authors": "S.A. Murphy, Y. Deng, E.B. Laber, H.R. Maei, R.S. Sutton, K.\n  Witkiewitz", "title": "A Batch, Off-Policy, Actor-Critic Algorithm for Optimizing the Average\n  Reward", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an off-policy actor-critic algorithm for learning an optimal\npolicy from a training set composed of data from multiple individuals. This\nalgorithm is developed with a view towards its use in mobile health.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 12:43:40 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Murphy", "S. A.", ""], ["Deng", "Y.", ""], ["Laber", "E. B.", ""], ["Maei", "H. R.", ""], ["Sutton", "R. S.", ""], ["Witkiewitz", "K.", ""]]}, {"id": "1607.05073", "submitter": "Christos Chatzichristos", "authors": "Christos Chatzichristos, Eleftherios Kofidis, Giannis Kopsinis,\n  Sergios Theodoridis", "title": "Higher-Order Block Term Decomposition for Spatially Folded fMRI Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing use of neuroimaging technologies generates a massive amount of\nbiomedical data that exhibit high dimensionality. Tensor-based analysis of\nbrain imaging data has been proved quite effective in exploiting their multiway\nnature. The advantages of tensorial methods over matrix-based approaches have\nalso been demonstrated in the characterization of functional magnetic resonance\nimaging (fMRI) data, where the spatial (voxel) dimensions are commonly grouped\n(unfolded) as a single way/mode of the 3-rd order array, the other two ways\ncorresponding to time and subjects. However, such methods are known to be\nineffective in more demanding scenarios, such as the ones with strong noise\nand/or significant overlapping of activated regions. This paper aims at\ninvestigating the possible gains from a better exploitation of the spatial\ndimension, through a higher- (4 or 5) order tensor modeling of the fMRI signal.\nIn this context, and in order to increase the degrees of freedom of the\nmodeling process, a higher-order Block Term Decomposition (BTD) is applied, for\nthe first time in fMRI analysis. Its effectiveness is demonstrated via\nextensive simulation results.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 09:28:48 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Chatzichristos", "Christos", ""], ["Kofidis", "Eleftherios", ""], ["Kopsinis", "Giannis", ""], ["Theodoridis", "Sergios", ""]]}, {"id": "1607.05154", "submitter": "Giorgio Matteo Vitetta Prof.", "authors": "Martino Uccellari, Francesca Facchini, Matteo Sola, Emilio Sirignano,\n  Giorgio M. Vitetta, Andrea Barbieri and Stefano Tondelli", "title": "On the Application of Support Vector Machines to the Prediction of\n  Propagation Losses at 169 MHz for Smart Metering Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the need of deploying new wireless networks for smart gas metering\nhas raised the problem of radio planning in the169 MHz band. Unluckily,\nsoftware tools commonly adopted for radio planning in cellular communication\nsystems cannot be employed to solve this problem because of the substantially\nlower transmission frequencies characterizing this application. In this\nmanuscript a novel data-centric solution, based on the use of support vector\nmachine techniques for classification and regression, is proposed. Our method\nrequires the availability of a limited set of received signal strength\nmeasurements and the knowledge of a three-dimensional map of the propagation\nenvironment of interest, and generates both an estimate of the coverage area\nand a prediction of the field strength within it. Numerical results referring\nto different Italian villages and cities evidence that our method is able to\nachieve good accuracy at the price of an acceptable computational cost and of a\nlimited effort for the acquisition of measurements in the considered\nenvironments.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 16:08:36 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Uccellari", "Martino", ""], ["Facchini", "Francesca", ""], ["Sola", "Matteo", ""], ["Sirignano", "Emilio", ""], ["Vitetta", "Giorgio M.", ""], ["Barbieri", "Andrea", ""], ["Tondelli", "Stefano", ""]]}, {"id": "1607.05241", "submitter": "Khanh Nguyen", "authors": "Khanh Nguyen", "title": "Imitation Learning with Recurrent Neural Networks", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel view that unifies two frameworks that aim to solve\nsequential prediction problems: learning to search (L2S) and recurrent neural\nnetworks (RNN). We point out equivalences between elements of the two\nframeworks. By complementing what is missing from one framework comparing to\nthe other, we introduce a more advanced imitation learning framework that, on\none hand, augments L2S s notion of search space and, on the other hand,\nenhances RNNs training procedure to be more robust to compounding errors\narising from training on highly correlated examples.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 19:01:00 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Nguyen", "Khanh", ""]]}, {"id": "1607.05432", "submitter": "Nicolas Durrande", "authors": "Didier Rulli\\`ere, Nicolas Durrande, Fran\\c{c}ois Bachoc, Cl\\'ement\n  Chevalier", "title": "Nested Kriging predictions for datasets with large number of\n  observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work falls within the context of predicting the value of a real function\nat some input locations given a limited number of observations of this\nfunction. The Kriging interpolation technique (or Gaussian process regression)\nis often considered to tackle such a problem but the method suffers from its\ncomputational burden when the number of observation points is large. We\nintroduce in this article nested Kriging predictors which are constructed by\naggregating sub-models based on subsets of observation points. This approach is\nproven to have better theoretical properties than other aggregation methods\nthat can be found in the literature. Contrarily to some other methods it can be\nshown that the proposed aggregation method is consistent. Finally, the\npractical interest of the proposed method is illustrated on simulated datasets\nand on an industrial test case with $10^4$ observations in a 6-dimensional\nspace.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 07:27:02 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2016 12:06:20 GMT"}, {"version": "v3", "created": "Tue, 25 Jul 2017 12:53:28 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Rulli\u00e8re", "Didier", ""], ["Durrande", "Nicolas", ""], ["Bachoc", "Fran\u00e7ois", ""], ["Chevalier", "Cl\u00e9ment", ""]]}, {"id": "1607.05506", "submitter": "Junping Zhang", "authors": "Xinxing Wu, Junping Zhang", "title": "Distribution-dependent concentration inequalities for tighter\n  generalization bounds", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concentration inequalities are indispensable tools for studying the\ngeneralization capacity of learning models. Hoeffding's and McDiarmid's\ninequalities are commonly used, giving bounds independent of the data\ndistribution. Although this makes them widely applicable, a drawback is that\nthe bounds can be too loose in some specific cases. Although efforts have been\ndevoted to improving the bounds, we find that the bounds can be further\ntightened in some distribution-dependent scenarios and conditions for the\ninequalities can be relaxed. In particular, we propose four types of conditions\nfor probabilistic boundedness and bounded differences, and derive several\ndistribution-dependent extensions of Hoeffding's and McDiarmid's inequalities.\nThese extensions provide bounds for functions not satisfying the conditions of\nthe existing inequalities, and in some special cases, tighter bounds.\nFurthermore, we obtain generalization bounds for unbounded and\nhierarchy-bounded loss functions. Finally we discuss the potential applications\nof our extensions to learning theory.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 10:23:57 GMT"}, {"version": "v2", "created": "Mon, 20 Feb 2017 13:16:37 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Wu", "Xinxing", ""], ["Zhang", "Junping", ""]]}, {"id": "1607.05573", "submitter": "Ruimin Zhu", "authors": "Ruimin Zhu, Wenxin Jiang", "title": "Combining Random Walks and Nonparametric Bayesian Topic Model for\n  Community Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection has been an active research area for decades. Among all\nprobabilistic models, Stochastic Block Model has been the most popular one.\nThis paper introduces a novel probabilistic model: RW-HDP, based on random\nwalks and Hierarchical Dirichlet Process, for community extraction. In RW-HDP,\nrandom walks conducted in a social network are treated as documents; nodes are\ntreated as words. By using Hierarchical Dirichlet Process, a nonparametric\nBayesian model, we are not only able to cluster nodes into different\ncommunities, but also determine the number of communities automatically. We use\nStochastic Variational Inference for our model inference, which makes our\nmethod time efficient and can be easily extended to an online learning\nalgorithm.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 13:46:15 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 01:39:38 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Zhu", "Ruimin", ""], ["Jiang", "Wenxin", ""]]}, {"id": "1607.05691", "submitter": "Francois Chollet", "authors": "Fran\\c{c}ois Chollet", "title": "Information-theoretical label embeddings for large-scale image\n  classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for training multi-label, massively multi-class image\nclassification models, that is faster and more accurate than supervision via a\nsigmoid cross-entropy loss (logistic regression). Our method consists in\nembedding high-dimensional sparse labels onto a lower-dimensional dense sphere\nof unit-normed vectors, and treating the classification problem as a cosine\nproximity regression problem on this sphere. We test our method on a dataset of\n300 million high-resolution images with 17,000 labels, where it yields\nconsiderably faster convergence, as well as a 7% higher mean average precision\ncompared to logistic regression.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 18:40:01 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Chollet", "Fran\u00e7ois", ""]]}, {"id": "1607.05709", "submitter": "Guo Xian Yau", "authors": "Guo Xian Yau, Chong Zhang", "title": "Multi-category Angle-based Classifier Refit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification is an important statistical learning tool. In real\napplication, besides high prediction accuracy, it is often desirable to\nestimate class conditional probabilities for new observations. For traditional\nproblems where the number of observations is large, there exist many well\ndeveloped approaches. Recently, high dimensional low sample size problems are\nbecoming increasingly popular. Margin-based classifiers, such as logistic\nregression, are well established methods in the literature. On the other hand,\nin terms of probability estimation, it is known that for binary classifiers,\nthe commonly used methods tend to under-estimate the norm of the classification\nfunction. This can lead to biased probability estimation. Remedy approaches\nhave been proposed in the literature. However, for the simultaneous\nmulticategory classification framework, much less work has been done. We fill\nthe gap in this paper. In particular, we give theoretical insights on why heavy\nregularization terms are often needed in high dimensional applications, and how\nthis can lead to bias in probability estimation. To overcome this difficulty,\nwe propose a new refit strategy for multicategory angle-based classifiers. Our\nnew method only adds a small computation cost to the problem, and is able to\nattain prediction accuracy that is as good as the regular margin-based\nclassifiers. On the other hand, the improvement of probability estimation can\nbe very significant. Numerical results suggest that the new refit approach is\nhighly competitive.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 19:49:55 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Yau", "Guo Xian", ""], ["Zhang", "Chong", ""]]}, {"id": "1607.05832", "submitter": "Varvara Kollia", "authors": "Varvara Kollia", "title": "Personalization Effect on Emotion Recognition from Physiological Data:\n  An Investigation of Performance on Different Setups and Classifiers", "comments": "8 pages, 7 png figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of emotion recognition from physiological\nsignals. Features are extracted and ranked based on their effect on\nclassification accuracy. Different classifiers are compared. The inter-subject\nvariability and the personalization effect are thoroughly investigated, through\ntrial-based and subject-based cross-validation. Finally, a personalized model\nis introduced, that would allow for enhanced emotional state prediction, based\non the physiological data of subjects that exhibit a certain degree of\nsimilarity, without the requirement of further feedback.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 06:32:16 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Kollia", "Varvara", ""]]}, {"id": "1607.05966", "submitter": "Philip Schniter", "authors": "Mark Borgerding and Philip Schniter", "title": "Onsager-corrected deep learning for sparse linear inverse problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has gained great popularity due to its widespread success on\nmany inference problems. We consider the application of deep learning to the\nsparse linear inverse problem encountered in compressive sensing, where one\nseeks to recover a sparse signal from a small number of noisy linear\nmeasurements. In this paper, we propose a novel neural-network architecture\nthat decouples prediction errors across layers in the same way that the\napproximate message passing (AMP) algorithm decouples them across iterations:\nthrough Onsager correction. Numerical experiments suggest that our \"learned\nAMP\" network significantly improves upon Gregor and LeCun's \"learned ISTA\"\nnetwork in both accuracy and complexity.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 14:14:49 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Borgerding", "Mark", ""], ["Schniter", "Philip", ""]]}, {"id": "1607.05970", "submitter": "James Edwards", "authors": "James Edwards, Paul Fearnhead, Kevin Glazebrook", "title": "On the Identification and Mitigation of Weaknesses in the Knowledge\n  Gradient Policy for Multi-Armed Bandits", "comments": "Minor typos corrected", "journal-ref": "Probability in the Engineering and Informational Sciences, 31(2)\n  239-263 (2017)", "doi": "10.1017/S0269964816000279", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Knowledge Gradient (KG) policy was originally proposed for online ranking\nand selection problems but has recently been adapted for use in online decision\nmaking in general and multi-armed bandit problems (MABs) in particular. We\nstudy its use in a class of exponential family MABs and identify weaknesses,\nincluding a propensity to take actions which are dominated with respect to both\nexploitation and exploration. We propose variants of KG which avoid such\nerrors. These new policies include an index heuristic which deploys a KG\napproach to develop an approximation to the Gittins index. A numerical study\nshows this policy to perform well over a range of MABs including those for\nwhich index policies are not optimal. While KG does not make dominated actions\nwhen bandits are Gaussian, it fails to be index consistent and appears not to\nenjoy a performance advantage over competitor policies when arms are correlated\nto compensate for its greater computational demands.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 14:21:42 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 15:36:30 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Edwards", "James", ""], ["Fearnhead", "Paul", ""], ["Glazebrook", "Kevin", ""]]}, {"id": "1607.05974", "submitter": "Romain Laby", "authors": "Romain Laby (LTCI), Fran\\c{c}ois Roueff (LTCI), Alexandre Gramfort\n  (LTCI)", "title": "Anomaly Detection and Localisation using Mixed Graphical Models", "comments": "in ICML 2016 Anomaly Detection Workshop, Jun 2016, New York, United\n  States", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method that performs anomaly detection and localisation within\nheterogeneous data using a pairwise undirected mixed graphical model. The data\nare a mixture of categorical and quantitative variables, and the model is\nlearned over a dataset that is supposed not to contain any anomaly. We then use\nthe model over temporal data, potentially a data stream, using a version of the\ntwo-sided CUSUM algorithm. The proposed decision statistic is based on a\nconditional likelihood ratio computed for each variable given the others. Our\nresults show that this function allows to detect anomalies variable by\nvariable, and thus to localise the variables involved in the anomalies more\nprecisely than univariate methods based on simple marginals.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 14:31:30 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Laby", "Romain", "", "LTCI"], ["Roueff", "Fran\u00e7ois", "", "LTCI"], ["Gramfort", "Alexandre", "", "LTCI"]]}, {"id": "1607.06011", "submitter": "Julius Julius", "authors": "Julius, Gopinath Mahale, Sumana T., C. S. Adityakrishna", "title": "On the Modeling of Error Functions as High Dimensional Landscapes for\n  Weight Initialization in Learning Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next generation deep neural networks for classification hosted on embedded\nplatforms will rely on fast, efficient, and accurate learning algorithms.\nInitialization of weights in learning networks has a great impact on the\nclassification accuracy. In this paper we focus on deriving good initial\nweights by modeling the error function of a deep neural network as a\nhigh-dimensional landscape. We observe that due to the inherent complexity in\nits algebraic structure, such an error function may conform to general results\nof the statistics of large systems. To this end we apply some results from\nRandom Matrix Theory to analyse these functions. We model the error function in\nterms of a Hamiltonian in N-dimensions and derive some theoretical results\nabout its general behavior. These results are further used to make better\ninitial guesses of weights for the learning algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 16:25:27 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Julius", "", ""], ["Mahale", "Gopinath", ""], ["T.", "Sumana", ""], ["Adityakrishna", "C. S.", ""]]}, {"id": "1607.06017", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Yuanzhi Li", "title": "Doubly Accelerated Methods for Faster CCA and Generalized\n  Eigendecomposition", "comments": "We have now stated more clearly why this paper has outperformed\n  relevant previous results, and included discussions for doubly-stochastic\n  methods. arXiv admin note: text overlap with arXiv:1607.03463", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study $k$-GenEV, the problem of finding the top $k$ generalized\neigenvectors, and $k$-CCA, the problem of finding the top $k$ vectors in\ncanonical-correlation analysis. We propose algorithms $\\mathtt{LazyEV}$ and\n$\\mathtt{LazyCCA}$ to solve the two problems with running times linearly\ndependent on the input size and on $k$.\n  Furthermore, our algorithms are DOUBLY-ACCELERATED: our running times depend\nonly on the square root of the matrix condition number, and on the square root\nof the eigengap. This is the first such result for both $k$-GenEV or $k$-CCA.\nWe also provide the first gap-free results, which provide running times that\ndepend on $1/\\sqrt{\\varepsilon}$ rather than the eigengap.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 16:43:18 GMT"}, {"version": "v2", "created": "Sat, 26 Nov 2016 03:18:24 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Li", "Yuanzhi", ""]]}, {"id": "1607.06146", "submitter": "Leonardo Banchi", "authors": "Leonardo Banchi, Nicola Pancotti, Sougato Bose", "title": "Supervised quantum gate \"teaching\" for quantum hardware design", "comments": "6 pages, 1 figure, based on arXiv:1509.04298", "journal-ref": "Proceedings of the European Symposium on Artificial Neural\n  Networks 2016", "doi": null, "report-no": null, "categories": "cs.LG quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to train a quantum network of pairwise interacting qubits such\nthat its evolution implements a target quantum algorithm into a given network\nsubset. Our strategy is inspired by supervised learning and is designed to help\nthe physical construction of a quantum computer which operates with minimal\nexternal classical control.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 22:46:32 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Banchi", "Leonardo", ""], ["Pancotti", "Nicola", ""], ["Bose", "Sougato", ""]]}, {"id": "1607.06280", "submitter": "Julie Moeyersoms", "authors": "Julie Moeyersoms, Brian d'Alessandro, Foster Provost, David Martens", "title": "Explaining Classification Models Built on High-Dimensional Sparse Data", "comments": "5 pages, 1 figure, 2 Tables; ICML conference, Workshop on Human\n  Interpretability In Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive modeling applications increasingly use data representing people's\nbehavior, opinions, and interactions. Fine-grained behavior data often has\ndifferent structure from traditional data, being very high-dimensional and\nsparse. Models built from these data are quite difficult to interpret, since\nthey contain many thousands or even many millions of features. Listing features\nwith large model coefficients is not sufficient, because the model coefficients\ndo not incorporate information on feature presence, which is key when analysing\nsparse data. In this paper we introduce two alternatives for explaining\npredictive models by listing important features. We evaluate these alternatives\nin terms of explanation \"bang for the buck,\", i.e., how many examples'\ninferences are explained for a given number of features listed. The bottom\nline: (i) The proposed alternatives have double the bang-for-the-buck as\ncompared to just listing the high-coefficient features, and (ii) interestingly,\nalthough they come from different sources and motivations, the two new\nalternatives provide strikingly similar rankings of important features.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 11:50:41 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 23:01:11 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Moeyersoms", "Julie", ""], ["d'Alessandro", "Brian", ""], ["Provost", "Foster", ""], ["Martens", "David", ""]]}, {"id": "1607.06294", "submitter": "Santiago Segarra", "authors": "Gunnar Carlsson, Facundo M\\'emoli, Alejandro Ribeiro, Santiago Segarra", "title": "Hierarchical Clustering of Asymmetric Networks", "comments": "arXiv admin note: substantial text overlap with arXiv:1301.7724", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers networks where relationships between nodes are\nrepresented by directed dissimilarities. The goal is to study methods that,\nbased on the dissimilarity structure, output hierarchical clusters, i.e., a\nfamily of nested partitions indexed by a connectivity parameter. Our\nconstruction of hierarchical clustering methods is built around the concept of\nadmissible methods, which are those that abide by the axioms of value - nodes\nin a network with two nodes are clustered together at the maximum of the two\ndissimilarities between them - and transformation - when dissimilarities are\nreduced, the network may become more clustered but not less. Two particular\nmethods, termed reciprocal and nonreciprocal clustering, are shown to provide\nupper and lower bounds in the space of admissible methods. Furthermore,\nalternative clustering methodologies and axioms are considered. In particular,\nmodifying the axiom of value such that clustering in two-node networks occurs\nat the minimum of the two dissimilarities entails the existence of a unique\nadmissible clustering method.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 12:32:47 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Carlsson", "Gunnar", ""], ["M\u00e9moli", "Facundo", ""], ["Ribeiro", "Alejandro", ""], ["Segarra", "Santiago", ""]]}, {"id": "1607.06333", "submitter": "Massil Achab", "authors": "Massil Achab, Emmanuel Bacry, St\\'ephane Ga\\\"iffas, Iacopo\n  Mastromatteo, Jean-Francois Muzy", "title": "Uncovering Causality from Multivariate Hawkes Integrated Cumulants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a new nonparametric method that allows one to estimate the matrix\nof integrated kernels of a multivariate Hawkes process. This matrix not only\nencodes the mutual influences of each nodes of the process, but also\ndisentangles the causality relationships between them. Our approach is the\nfirst that leads to an estimation of this matrix without any parametric\nmodeling and estimation of the kernels themselves. A consequence is that it can\ngive an estimation of causality relationships between nodes (or users), based\non their activity timestamps (on a social network for instance), without\nknowing or estimating the shape of the activities lifetime. For that purpose,\nwe introduce a moment matching method that fits the third-order integrated\ncumulants of the process. We show on numerical experiments that our approach is\nindeed very robust to the shape of the kernels, and gives appealing results on\nthe MemeTracker database.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 14:19:23 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 22:46:27 GMT"}, {"version": "v3", "created": "Tue, 30 May 2017 00:06:14 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Achab", "Massil", ""], ["Bacry", "Emmanuel", ""], ["Ga\u00efffas", "St\u00e9phane", ""], ["Mastromatteo", "Iacopo", ""], ["Muzy", "Jean-Francois", ""]]}, {"id": "1607.06335", "submitter": "Santiago Segarra", "authors": "Gunnar Carlsson, Facundo M\\'emoli, Alejandro Ribeiro, Santiago Segarra", "title": "Admissible Hierarchical Clustering Methods and Algorithms for Asymmetric\n  Networks", "comments": "arXiv admin note: substantial text overlap with arXiv:1301.7724", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper characterizes hierarchical clustering methods that abide by two\npreviously introduced axioms -- thus, denominated admissible methods -- and\nproposes tractable algorithms for their implementation. We leverage the fact\nthat, for asymmetric networks, every admissible method must be contained\nbetween reciprocal and nonreciprocal clustering, and describe three families of\nintermediate methods. Grafting methods exchange branches between dendrograms\ngenerated by different admissible methods. The convex combination family\ncombines admissible methods through a convex operation in the space of\ndendrograms, and thirdly, the semi-reciprocal family clusters nodes that are\nrelated by strong cyclic influences in the network. Algorithms for the\ncomputation of hierarchical clusters generated by reciprocal and nonreciprocal\nclustering as well as the grafting, convex combination, and semi-reciprocal\nfamilies are derived using matrix operations in a dioid algebra. Finally, the\nintroduced clustering methods and algorithms are exemplified through their\napplication to a network describing the interrelation between sectors of the\nUnited States (U.S.) economy.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 14:22:12 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Carlsson", "Gunnar", ""], ["M\u00e9moli", "Facundo", ""], ["Ribeiro", "Alejandro", ""], ["Segarra", "Santiago", ""]]}, {"id": "1607.06364", "submitter": "Simone Scardapane", "authors": "Simone Scardapane", "title": "Distributed Supervised Learning using Neural Networks", "comments": "Author's Ph.D. thesis (DIET Dept., Sapienza University of Rome, May\n  2016). Supervisor: Prof. Aurelio Uncini", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed learning is the problem of inferring a function in the case where\ntraining data is distributed among multiple geographically separated sources.\nParticularly, the focus is on designing learning strategies with low\ncomputational requirements, in which communication is restricted only to\nneighboring agents, with no reliance on a centralized authority. In this\nthesis, we analyze multiple distributed protocols for a large number of neural\nnetwork architectures. The first part of the thesis is devoted to a definition\nof the problem, followed by an extensive overview of the state-of-the-art.\nNext, we introduce different strategies for a relatively simple class of single\nlayer neural networks, where a linear output layer is preceded by a nonlinear\nlayer, whose weights are stochastically assigned in the beginning of the\nlearning process. We consider both batch and sequential learning, with\nhorizontally and vertically partitioned data. In the third part, we consider\ninstead the more complex problem of semi-supervised distributed learning, where\neach agent is provided with an additional set of unlabeled training samples. We\npropose two different algorithms based on diffusion processes for linear\nsupport vector machines and kernel ridge regression. Subsequently, the fourth\npart extends the discussion to learning with time-varying data (e.g.\ntime-series) using recurrent neural networks. We consider two different\nfamilies of networks, namely echo state networks (extending the algorithms\nintroduced in the second part), and spline adaptive filters. Overall, the\nalgorithms presented throughout the thesis cover a wide range of possible\npractical applications, and lead the way to numerous future extensions, which\nare briefly summarized in the conclusive chapter.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 15:32:47 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Scardapane", "Simone", ""]]}, {"id": "1607.06450", "submitter": "Jimmy Ba", "authors": "Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton", "title": "Layer Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training state-of-the-art, deep neural networks is computationally expensive.\nOne way to reduce the training time is to normalize the activities of the\nneurons. A recently introduced technique called batch normalization uses the\ndistribution of the summed input to a neuron over a mini-batch of training\ncases to compute a mean and variance which are then used to normalize the\nsummed input to that neuron on each training case. This significantly reduces\nthe training time in feed-forward neural networks. However, the effect of batch\nnormalization is dependent on the mini-batch size and it is not obvious how to\napply it to recurrent neural networks. In this paper, we transpose batch\nnormalization into layer normalization by computing the mean and variance used\nfor normalization from all of the summed inputs to the neurons in a layer on a\nsingle training case. Like batch normalization, we also give each neuron its\nown adaptive bias and gain which are applied after the normalization but before\nthe non-linearity. Unlike batch normalization, layer normalization performs\nexactly the same computation at training and test times. It is also\nstraightforward to apply to recurrent neural networks by computing the\nnormalization statistics separately at each time step. Layer normalization is\nvery effective at stabilizing the hidden state dynamics in recurrent networks.\nEmpirically, we show that layer normalization can substantially reduce the\ntraining time compared with previously published techniques.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 19:57:52 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Ba", "Jimmy Lei", ""], ["Kiros", "Jamie Ryan", ""], ["Hinton", "Geoffrey E.", ""]]}, {"id": "1607.06520", "submitter": "Tolga Bolukbasi", "authors": "Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, Adam\n  Kalai", "title": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The blind application of machine learning runs the risk of amplifying biases\npresent in data. Such a danger is facing us with word embedding, a popular\nframework to represent text data as vectors which has been used in many machine\nlearning and natural language processing tasks. We show that even word\nembeddings trained on Google News articles exhibit female/male gender\nstereotypes to a disturbing extent. This raises concerns because their\nwidespread use, as we describe, often tends to amplify these biases.\nGeometrically, gender bias is first shown to be captured by a direction in the\nword embedding. Second, gender neutral words are shown to be linearly separable\nfrom gender definition words in the word embedding. Using these properties, we\nprovide a methodology for modifying an embedding to remove gender stereotypes,\nsuch as the association between between the words receptionist and female,\nwhile maintaining desired associations such as between the words queen and\nfemale. We define metrics to quantify both direct and indirect gender biases in\nembeddings, and develop algorithms to \"debias\" the embedding. Using\ncrowd-worker evaluation as well as standard benchmarks, we empirically\ndemonstrate that our algorithms significantly reduce gender bias in embeddings\nwhile preserving the its useful properties such as the ability to cluster\nrelated concepts and to solve analogy tasks. The resulting embeddings can be\nused in applications without amplifying gender bias.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 22:26:20 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Bolukbasi", "Tolga", ""], ["Chang", "Kai-Wei", ""], ["Zou", "James", ""], ["Saligrama", "Venkatesh", ""], ["Kalai", "Adam", ""]]}, {"id": "1607.06534", "submitter": "Song Mei", "authors": "Song Mei, Yu Bai, Andrea Montanari", "title": "The Landscape of Empirical Risk for Non-convex Losses", "comments": "This version presents a general framework, and applies it to several\n  statistical learning problems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most high-dimensional estimation and prediction methods propose to minimize a\ncost function (empirical risk) that is written as a sum of losses associated to\neach data point. In this paper we focus on the case of non-convex losses, which\nis practically important but still poorly understood. Classical empirical\nprocess theory implies uniform convergence of the empirical risk to the\npopulation risk. While uniform convergence implies consistency of the resulting\nM-estimator, it does not ensure that the latter can be computed efficiently.\n  In order to capture the complexity of computing M-estimators, we propose to\nstudy the landscape of the empirical risk, namely its stationary points and\ntheir properties. We establish uniform convergence of the gradient and Hessian\nof the empirical risk to their population counterparts, as soon as the number\nof samples becomes larger than the number of unknown parameters (modulo\nlogarithmic factors). Consequently, good properties of the population risk can\nbe carried to the empirical risk, and we can establish one-to-one\ncorrespondence of their stationary points. We demonstrate that in several\nproblems such as non-convex binary classification, robust regression, and\nGaussian mixture model, this result implies a complete characterization of the\nlandscape of the empirical risk, and of the convergence properties of descent\nalgorithms.\n  We extend our analysis to the very high-dimensional setting in which the\nnumber of parameters exceeds the number of samples, and provide a\ncharacterization of the empirical risk landscape under a nearly\ninformation-theoretically minimal condition. Namely, if the number of samples\nexceeds the sparsity of the unknown parameters vector (modulo logarithmic\nfactors), then a suitable uniform convergence result takes place. We apply this\nresult to non-convex binary classification and robust regression in very\nhigh-dimension.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 01:37:30 GMT"}, {"version": "v2", "created": "Mon, 25 Jul 2016 19:01:20 GMT"}, {"version": "v3", "created": "Sat, 14 Jan 2017 10:31:03 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Mei", "Song", ""], ["Bai", "Yu", ""], ["Montanari", "Andrea", ""]]}, {"id": "1607.06617", "submitter": "Xuhui Zhang Mr", "authors": "Xuhui Zhang, Kevin B. Korb, Ann E. Nicholson, Steven Mascaro", "title": "Latent Variable Discovery Using Dependency Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The causal discovery of Bayesian networks is an active and important research\narea, and it is based upon searching the space of causal models for those which\ncan best explain a pattern of probabilistic dependencies shown in the data.\nHowever, some of those dependencies are generated by causal structures\ninvolving variables which have not been measured, i.e., latent variables. Some\nsuch patterns of dependency \"reveal\" themselves, in that no model based solely\nupon the observed variables can explain them as well as a model using a latent\nvariable. That is what latent variable discovery is based upon. Here we did a\nsearch for finding them systematically, so that they may be applied in latent\nvariable discovery in a more rigorous fashion.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 09:48:25 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Zhang", "Xuhui", ""], ["Korb", "Kevin B.", ""], ["Nicholson", "Ann E.", ""], ["Mascaro", "Steven", ""]]}, {"id": "1607.06781", "submitter": "Fabio Massimo Zennaro", "authors": "Fabio Massimo Zennaro, Ke Chen", "title": "On the Use of Sparse Filtering for Covariate Shift Adaptation", "comments": "40 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we formally analyse the use of sparse filtering algorithms to\nperform covariate shift adaptation. We provide a theoretical analysis of sparse\nfiltering by evaluating the conditions required to perform covariate shift\nadaptation. We prove that sparse filtering can perform adaptation only if the\nconditional distribution of the labels has a structure explained by a cosine\nmetric. To overcome this limitation, we propose a new algorithm, named periodic\nsparse filtering, and carry out the same theoretical analysis regarding\ncovariate shift adaptation. We show that periodic sparse filtering can perform\nadaptation under the looser and more realistic requirement that the conditional\ndistribution of the labels has a periodic structure, which may be satisfied,\nfor instance, by user-dependent data sets. We experimentally validate our\ntheoretical results on synthetic data. Moreover, we apply periodic sparse\nfiltering to real-world data sets to demonstrate that this simple and\ncomputationally efficient algorithm is able to achieve competitive\nperformances.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 18:17:10 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 14:45:16 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Zennaro", "Fabio Massimo", ""], ["Chen", "Ke", ""]]}, {"id": "1607.06801", "submitter": "Stefan Wager", "authors": "Stefan Wager, Wenfei Du, Jonathan Taylor and Robert Tibshirani", "title": "High-dimensional regression adjustments in randomized experiments", "comments": "To appear in the Proceedings of the National Academy of Sciences. The\n  present draft does not reflect final copyediting by the PNAS staff", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of treatment effect estimation in randomized experiments\nwith high-dimensional covariate information, and show that essentially any\nrisk-consistent regression adjustment can be used to obtain efficient estimates\nof the average treatment effect. Our results considerably extend the range of\nsettings where high-dimensional regression adjustments are guaranteed to\nprovide valid inference about the population average treatment effect. We then\npropose cross-estimation, a simple method for obtaining finite-sample-unbiased\ntreatment effect estimates that leverages high-dimensional regression\nadjustments. Our method can be used when the regression model is estimated\nusing the lasso, the elastic net, subset selection, etc. Finally, we extend our\nanalysis to allow for adaptive specification search via cross-validation, and\nflexible non-parametric regression adjustments with machine learning methods\nsuch as random forests or neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 19:38:46 GMT"}, {"version": "v2", "created": "Sat, 30 Jul 2016 23:35:46 GMT"}, {"version": "v3", "created": "Thu, 27 Oct 2016 21:36:45 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Wager", "Stefan", ""], ["Du", "Wenfei", ""], ["Taylor", "Jonathan", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1607.06988", "submitter": "Shankar Vembu", "authors": "Shankar Vembu, Sandra Zilles", "title": "Interactive Learning from Multiple Noisy Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive learning is a process in which a machine learning algorithm is\nprovided with meaningful, well-chosen examples as opposed to randomly chosen\nexamples typical in standard supervised learning. In this paper, we propose a\nnew method for interactive learning from multiple noisy labels where we exploit\nthe disagreement among annotators to quantify the easiness (or meaningfulness)\nof an example. We demonstrate the usefulness of this method in estimating the\nparameters of a latent variable classification model, and conduct experimental\nanalyses on a range of synthetic and benchmark datasets. Furthermore, we\ntheoretically analyze the performance of perceptron in this interactive\nlearning framework.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jul 2016 01:14:19 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Vembu", "Shankar", ""], ["Zilles", "Sandra", ""]]}, {"id": "1607.06993", "submitter": "Zongming Ma", "authors": "Chao Gao, Zongming Ma, Anderson Y. Zhang, Harrison H. Zhou", "title": "Community Detection in Degree-Corrected Block Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SI stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection is a central problem of network data analysis. Given a\nnetwork, the goal of community detection is to partition the network nodes into\na small number of clusters, which could often help reveal interesting\nstructures. The present paper studies community detection in Degree-Corrected\nBlock Models (DCBMs). We first derive asymptotic minimax risks of the problem\nfor a misclassification proportion loss under appropriate conditions. The\nminimax risks are shown to depend on degree-correction parameters, community\nsizes, and average within and between community connectivities in an intuitive\nand interpretable way. In addition, we propose a polynomial time algorithm to\nadaptively perform consistent and even asymptotically optimal community\ndetection in DCBMs.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jul 2016 02:53:38 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Gao", "Chao", ""], ["Ma", "Zongming", ""], ["Zhang", "Anderson Y.", ""], ["Zhou", "Harrison H.", ""]]}, {"id": "1607.06996", "submitter": "Weizhong Zhang", "authors": "Weizhong Zhang and Bin Hong and Wei Liu and Jieping Ye and Deng Cai\n  and Xiaofei He and Jie Wang", "title": "Scaling Up Sparse Support Vector Machines by Simultaneous Feature and\n  Sample Reduction", "comments": "accepted by JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse support vector machine (SVM) is a popular classification technique\nthat can simultaneously learn a small set of the most interpretable features\nand identify the support vectors. It has achieved great successes in many\nreal-world applications. However, for large-scale problems involving a huge\nnumber of samples and ultra-high dimensional features, solving sparse SVMs\nremains challenging. By noting that sparse SVMs induce sparsities in both\nfeature and sample spaces, we propose a novel approach, which is based on\naccurate estimations of the primal and dual optima of sparse SVMs, to\nsimultaneously identify the inactive features and samples that are guaranteed\nto be irrelevant to the outputs. Thus, we can remove the identified inactive\nsamples and features from the training phase, leading to substantial savings in\nthe computational cost without sacrificing the accuracy. Moreover, we show that\nour method can be extended to multi-class sparse support vector machines. To\nthe best of our knowledge, the proposed method is the \\emph{first}\n\\emph{static} feature and sample reduction method for sparse SVMs and\nmulti-class sparse SVMs. Experiments on both synthetic and real data sets\ndemonstrate that our approach significantly outperforms state-of-the-art\nmethods and the speedup gained by our approach can be orders of magnitude.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jul 2016 04:00:30 GMT"}, {"version": "v2", "created": "Sun, 14 Aug 2016 18:45:52 GMT"}, {"version": "v3", "created": "Sat, 25 Feb 2017 14:25:20 GMT"}, {"version": "v4", "created": "Wed, 8 Mar 2017 02:07:36 GMT"}, {"version": "v5", "created": "Fri, 23 Jun 2017 08:33:14 GMT"}, {"version": "v6", "created": "Thu, 18 Jul 2019 10:21:41 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Zhang", "Weizhong", ""], ["Hong", "Bin", ""], ["Liu", "Wei", ""], ["Ye", "Jieping", ""], ["Cai", "Deng", ""], ["He", "Xiaofei", ""], ["Wang", "Jie", ""]]}, {"id": "1607.07195", "submitter": "Mathieu Blondel", "authors": "Mathieu Blondel, Akinori Fujino, Naonori Ueda and Masakazu Ishihata", "title": "Higher-Order Factorization Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factorization machines (FMs) are a supervised learning approach that can use\nsecond-order feature combinations even when the data is very high-dimensional.\nUnfortunately, despite increasing interest in FMs, there exists to date no\nefficient training algorithm for higher-order FMs (HOFMs). In this paper, we\npresent the first generic yet efficient algorithms for training arbitrary-order\nHOFMs. We also present new variants of HOFMs with shared parameters, which\ngreatly reduce model size and prediction times while maintaining similar\naccuracy. We demonstrate the proposed approaches on four different link\nprediction tasks.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 10:19:27 GMT"}, {"version": "v2", "created": "Fri, 14 Oct 2016 06:32:13 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Blondel", "Mathieu", ""], ["Fujino", "Akinori", ""], ["Ueda", "Naonori", ""], ["Ishihata", "Masakazu", ""]]}, {"id": "1607.07249", "submitter": "Joern Hees", "authors": "J\\\"orn Hees, Rouven Bauer, Joachim Folz, Damian Borth and Andreas\n  Dengel", "title": "An Evolutionary Algorithm to Learn SPARQL Queries for\n  Source-Target-Pairs: Finding Patterns for Human Associations in DBpedia", "comments": "15 pages, 2 figures, as of 2016-09-13\n  6a19d5d7020770dc0711081ce2c1e52f71bf4b86", "journal-ref": null, "doi": "10.1007/978-3-319-49004-5_22", "report-no": null, "categories": "cs.AI cs.DB cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient usage of the knowledge provided by the Linked Data community is\noften hindered by the need for domain experts to formulate the right SPARQL\nqueries to answer questions. For new questions they have to decide which\ndatasets are suitable and in which terminology and modelling style to phrase\nthe SPARQL query.\n  In this work we present an evolutionary algorithm to help with this\nchallenging task. Given a training list of source-target node-pair examples our\nalgorithm can learn patterns (SPARQL queries) from a SPARQL endpoint. The\nlearned patterns can be visualised to form the basis for further investigation,\nor they can be used to predict target nodes for new source nodes.\n  Amongst others, we apply our algorithm to a dataset of several hundred human\nassociations (such as \"circle - square\") to find patterns for them in DBpedia.\nWe show the scalability of the algorithm by running it against a SPARQL\nendpoint loaded with > 7.9 billion triples. Further, we use the resulting\nSPARQL queries to mimic human associations with a Mean Average Precision (MAP)\nof 39.9 % and a Recall@10 of 63.9 %.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 12:47:38 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 12:13:14 GMT"}, {"version": "v3", "created": "Tue, 13 Sep 2016 10:27:06 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Hees", "J\u00f6rn", ""], ["Bauer", "Rouven", ""], ["Folz", "Joachim", ""], ["Borth", "Damian", ""], ["Dengel", "Andreas", ""]]}, {"id": "1607.07270", "submitter": "Francesco Solera", "authors": "Francesco Solera and Andrea Palazzi", "title": "A Statistical Test for Joint Distributions Equivalence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a distribution-free test that can be used to determine whether any\ntwo joint distributions $p$ and $q$ are statistically different by inspection\nof a large enough set of samples. Following recent efforts from Long et al.\n[1], we rely on joint kernel distribution embedding to extend the kernel\ntwo-sample test of Gretton et al. [2] to the case of joint probability\ndistributions. Our main result can be directly applied to verify if a\ndataset-shift has occurred between training and test distributions in a\nlearning framework, without further assuming the shift has occurred only in the\ninput, in the target or in the conditional distribution.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 13:48:20 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Solera", "Francesco", ""], ["Palazzi", "Andrea", ""]]}, {"id": "1607.07329", "submitter": "Ji Liu", "authors": "Mengdi Wang and Ji Liu and Ethan X. Fang", "title": "Accelerating Stochastic Composition Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the stochastic composition optimization problem where the objective\nis a composition of two expected-value functions. We propose a new stochastic\nfirst-order method, namely the accelerated stochastic compositional proximal\ngradient (ASC-PG) method, which updates based on queries to the sampling oracle\nusing two different timescales. The ASC-PG is the first proximal gradient\nmethod for the stochastic composition problem that can deal with nonsmooth\nregularization penalty. We show that the ASC-PG exhibits faster convergence\nthan the best known algorithms, and that it achieves the optimal sample-error\ncomplexity in several important special cases. We further demonstrate the\napplication of ASC-PG to reinforcement learning and conduct numerical\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 15:56:49 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Wang", "Mengdi", ""], ["Liu", "Ji", ""], ["Fang", "Ethan X.", ""]]}, {"id": "1607.07384", "submitter": "Moin Nadeem", "authors": "Moin Nadeem", "title": "Identifying Depression on Twitter", "comments": "9 pages, 7 figures, 1 table. Written as a part of my AP Research\n  course. Comments and criticisms are welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media has recently emerged as a premier method to disseminate\ninformation online. Through these online networks, tens of millions of\nindividuals communicate their thoughts, personal experiences, and social\nideals. We therefore explore the potential of social media to predict, even\nprior to onset, Major Depressive Disorder (MDD) in online personas. We employ a\ncrowdsourced method to compile a list of Twitter users who profess to being\ndiagnosed with depression. Using up to a year of prior social media postings,\nwe utilize a Bag of Words approach to quantify each tweet. Lastly, we leverage\nseveral statistical classifiers to provide estimates to the risk of depression.\nOur work posits a new methodology for constructing our classifier by treating\nsocial as a text-classification problem, rather than a behavioral one on social\nmedia platforms. By using a corpus of 2.5M tweets, we achieved an 81% accuracy\nrate in classification, with a precision score of .86. We believe that this\nmethod may be helpful in developing tools that estimate the risk of an\nindividual being depressed, can be employed by physicians, concerned\nindividuals, and healthcare agencies to aid in diagnosis, even possibly\nenabling those suffering from depression to be more proactive about recovering\nfrom their mental health.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 18:00:04 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Nadeem", "Moin", ""]]}, {"id": "1607.07387", "submitter": "Francesco Silvestri", "authors": "Francesco Silvestri and Gerhard Reinelt and Christoph Schn\\\"orr", "title": "Symmetry-free SDP Relaxations for Affine Subspace Clustering", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.DM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider clustering problems where the goal is to determine an optimal\npartition of a given point set in Euclidean space in terms of a collection of\naffine subspaces. While there is vast literature on heuristics for this kind of\nproblem, such approaches are known to be susceptible to poor initializations\nand getting trapped in bad local optima. We alleviate these issues by\nintroducing a semidefinite relaxation based on Lasserre's method of moments.\nWhile a similiar approach is known for classical Euclidean clustering problems,\na generalization to our more general subspace scenario is not straightforward,\ndue to the high symmetry of the objective function that weakens any convex\nrelaxation. We therefore introduce a new mechanism for symmetry breaking based\non covering the feasible region with polytopes. Additionally, we introduce and\nanalyze a deterministic rounding heuristic.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 18:01:17 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Silvestri", "Francesco", ""], ["Reinelt", "Gerhard", ""], ["Schn\u00f6rr", "Christoph", ""]]}, {"id": "1607.07423", "submitter": "Arin Chaudhuri", "authors": "Deovrat Kakde, Sergriy Peredriy, Arin Chaudhuri, Anya Mcguirk", "title": "A Non-Parametric Control Chart For High Frequency Multivariate Data", "comments": null, "journal-ref": null, "doi": "10.1109/RAM.2017.7889786", "report-no": null, "categories": "cs.LG stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support Vector Data Description (SVDD) is a machine learning technique used\nfor single class classification and outlier detection. SVDD based K-chart was\nfirst introduced by Sun and Tsung for monitoring multivariate processes when\nunderlying distribution of process parameters or quality characteristics depart\nfrom Normality. The method first trains a SVDD model on data obtained from\nstable or in-control operations of the process to obtain a threshold $R^2$ and\nkernel center a. For each new observation, its Kernel distance from the Kernel\ncenter a is calculated. The kernel distance is compared against the threshold\n$R^2$ to determine if the observation is within the control limits. The\nnon-parametric K-chart provides an attractive alternative to the traditional\ncontrol charts such as the Hotelling's $T^2$ charts when distribution of the\nunderlying multivariate data is either non-normal or is unknown. But there are\nchallenges when K-chart is deployed in practice. The K-chart requires\ncalculating kernel distance of each new observation but there are no guidelines\non how to interpret the kernel distance plot and infer about shifts in process\nmean or changes in process variation. This limits the application of K-charts\nin big-data applications such as equipment health monitoring, where\nobservations are generated at a very high frequency. In this scenario, the\nanalyst using the K-chart is inundated with kernel distance results at a very\nhigh frequency, generally without any recourse for detecting presence of any\nassignable causes of variation. We propose a new SVDD based control chart,\ncalled as $K_T$ chart, which addresses challenges encountered when using\nK-chart for big-data applications. The $K_T$ charts can be used to\nsimultaneously track process variation and central tendency. We illustrate the\nsuccessful use of $K_T$ chart using the Tennessee Eastman process data.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 19:40:55 GMT"}, {"version": "v2", "created": "Wed, 27 Jul 2016 22:23:47 GMT"}, {"version": "v3", "created": "Fri, 29 Jul 2016 20:31:54 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Kakde", "Deovrat", ""], ["Peredriy", "Sergriy", ""], ["Chaudhuri", "Arin", ""], ["Mcguirk", "Anya", ""]]}, {"id": "1607.07519", "submitter": "Phuoc Nguyen", "authors": "Phuoc Nguyen, Truyen Tran, Nilmini Wickramasinghe, Svetha Venkatesh", "title": "Deepr: A Convolutional Net for Medical Records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature engineering remains a major bottleneck when creating predictive\nsystems from electronic medical records. At present, an important missing\nelement is detecting predictive regular clinical motifs from irregular episodic\nrecords. We present Deepr (short for Deep record), a new end-to-end deep\nlearning system that learns to extract features from medical records and\npredicts future risk automatically. Deepr transforms a record into a sequence\nof discrete elements separated by coded time gaps and hospital transfers. On\ntop of the sequence is a convolutional neural net that detects and combines\npredictive local clinical motifs to stratify the risk. Deepr permits\ntransparent inspection and visualization of its inner working. We validate\nDeepr on hospital data to predict unplanned readmission after discharge. Deepr\nachieves superior accuracy compared to traditional techniques, detects\nmeaningful clinical motifs, and uncovers the underlying structure of the\ndisease and intervention space.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 02:06:33 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Nguyen", "Phuoc", ""], ["Tran", "Truyen", ""], ["Wickramasinghe", "Nilmini", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1607.07573", "submitter": "Alberto Llera", "authors": "A. Llera, D. Vidaurre, R.H.R. Pruim, C. F. Beckmann", "title": "Variational Mixture Models with Gamma or inverse-Gamma components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture models with Gamma and or inverse-Gamma distributed mixture components\nare useful for medical image tissue segmentation or as post-hoc models for\nregression coefficients obtained from linear regression within a Generalised\nLinear Modeling framework (GLM), used in this case to separate stochastic\n(Gaussian) noise from some kind of positive or negative \"activation\" (modeled\nas Gamma or inverse-Gamma distributed). To date, the most common choice in this\ncontext it is Gaussian/Gamma mixture models learned through a maximum\nlikelihood (ML) approach; we recently extended such algorithm for mixture\nmodels with inverse-Gamma components. Here, we introduce a fully analytical\nVariational Bayes (VB) learning framework for both Gamma and/or inverse-Gamma\ncomponents. We use synthetic and resting state fMRI data to compare the\nperformance of the ML and VB algorithms in terms of area under the curve and\ncomputational cost. We observed that the ML Gaussian/Gamma model is very\nexpensive specially when considering high resolution images; furthermore, these\nsolutions are highly variable and they occasionally can overestimate the\nactivations severely. The Bayesian Gauss-Gamma is in general the fastest\nalgorithm but provides too dense solutions. The maximum likelihood\nGaussian/inverse-Gamma is also very fast but provides in general very sparse\nsolutions. The variational Gaussian/inverse-Gamma mixture model is the most\nrobust and its cost is acceptable even for high resolution images. Further, the\npresented methodology represents an essential building block that can be\ndirectly used in more complex inference tasks, specially designed to analyse\nMRI-fMRI data; such models include for example analytical variational mixture\nmodels with adaptive spatial regularization or better source models for new\nspatial blind source separation approaches.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 07:47:02 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Llera", "A.", ""], ["Vidaurre", "D.", ""], ["Pruim", "R. H. R.", ""], ["Beckmann", "C. F.", ""]]}, {"id": "1607.07590", "submitter": "Satoru Tokuda", "authors": "Satoru Tokuda, Kenji Nagata, and Masato Okada", "title": "Simultaneous Estimation of Noise Variance and Number of Peaks in\n  Bayesian Spectral Deconvolution", "comments": null, "journal-ref": null, "doi": "10.7566/JPSJ.86.024001", "report-no": null, "categories": "physics.data-an cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The heuristic identification of peaks from noisy complex spectra often leads\nto misunderstanding of the physical and chemical properties of matter. In this\npaper, we propose a framework based on Bayesian inference, which enables us to\nseparate multipeak spectra into single peaks statistically and consists of two\nsteps. The first step is estimating both the noise variance and the number of\npeaks as hyperparameters based on Bayes free energy, which generally is not\nanalytically tractable. The second step is fitting the parameters of each peak\nfunction to the given spectrum by calculating the posterior density, which has\na problem of local minima and saddles since multipeak models are nonlinear and\nhierarchical. Our framework enables the escape from local minima or saddles by\nusing the exchange Monte Carlo method and calculates Bayes free energy via the\nmultiple histogram method. We discuss a simulation demonstrating how efficient\nour framework is and show that estimating both the noise variance and the\nnumber of peaks prevents overfitting, overpenalizing, and misunderstanding the\nprecision of parameter estimation.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 08:36:41 GMT"}, {"version": "v2", "created": "Thu, 15 Dec 2016 11:43:21 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Tokuda", "Satoru", ""], ["Nagata", "Kenji", ""], ["Okada", "Masato", ""]]}, {"id": "1607.07607", "submitter": "Gianna Maria Del Corso", "authors": "Gianna M. Del Corso and Francesco Romani", "title": "Adaptive Nonnegative Matrix Factorization and Measure Comparisons for\n  Recommender Systems", "comments": null, "journal-ref": "Applied Mathematics and Computation 354, pp. 164-179, 2019", "doi": "10.1016/j.amc.2019.01.047", "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Nonnegative Matrix Factorization (NMF) of the rating matrix has shown to\nbe an effective method to tackle the recommendation problem. In this paper we\npropose new methods based on the NMF of the rating matrix and we compare them\nwith some classical algorithms such as the SVD and the regularized and\nunregularized non-negative matrix factorization approach. In particular a new\nalgorithm is obtained changing adaptively the function to be minimized at each\nstep, realizing a sort of dynamic prior strategy. Another algorithm is obtained\nmodifying the function to be minimized in the NMF formulation by enforcing the\nreconstruction of the unknown ratings toward a prior term. We then combine\ndifferent methods obtaining two mixed strategies which turn out to be very\neffective in the reconstruction of missing observations. We perform a\nthoughtful comparison of different methods on the basis of several evaluation\nmeasures. We consider in particular rating, classification and ranking measures\nshowing that the algorithm obtaining the best score for a given measure is in\ngeneral the best also when different measures are considered, lowering the\ninterest in designing specific evaluation measures. The algorithms have been\ntested on different datasets, in particular the 1M, and 10M MovieLens datasets\ncontaining ratings on movies, the Jester dataset with ranting on jokes and\nAmazon Fine Foods dataset with ratings on foods. The comparison of the\ndifferent algorithms, shows the good performance of methods employing both an\nexplicit and an implicit regularization scheme. Moreover we can get a boost by\nmixed strategies combining a fast method with a more accurate one.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 09:26:20 GMT"}, {"version": "v2", "created": "Mon, 27 Aug 2018 10:06:23 GMT"}, {"version": "v3", "created": "Thu, 29 Aug 2019 09:11:05 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Del Corso", "Gianna M.", ""], ["Romani", "Francesco", ""]]}, {"id": "1607.07745", "submitter": "Arin Chaudhuri", "authors": "Deovrat Kakde, Arin Chaudhuri", "title": "Leveraging Unstructured Data to Detect Emerging Reliability Issues", "comments": null, "journal-ref": null, "doi": "10.1109/RAMS.2015.7105093", "report-no": null, "categories": "cs.AI stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unstructured data refers to information that does not have a predefined data\nmodel or is not organized in a pre-defined manner. Loosely speaking,\nunstructured data refers to text data that is generated by humans. In\nafter-sales service businesses, there are two main sources of unstructured\ndata: customer complaints, which generally describe symptoms, and technician\ncomments, which outline diagnostics and treatment information. A legitimate\ncustomer complaint can eventually be tracked to a failure or a claim. However,\nthere is a delay between the time of a customer complaint and the time of a\nfailure or a claim. A proactive strategy aimed at analyzing customer complaints\nfor symptoms can help service providers detect reliability problems in advance\nand initiate corrective actions such as recalls. This paper introduces\nessential text mining concepts in the context of reliability analysis and a\nmethod to detect emerging reliability issues. The application of the method is\nillustrated using a case study.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 15:19:12 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Kakde", "Deovrat", ""], ["Chaudhuri", "Arin", ""]]}, {"id": "1607.07762", "submitter": "Zi Wang", "authors": "Zi Wang, Stefanie Jegelka, Leslie Pack Kaelbling, Tom\\'as\n  Lozano-P\\'erez", "title": "Focused Model-Learning and Planning for Non-Gaussian Continuous\n  State-Action Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework for model learning and planning in stochastic\ndomains with continuous state and action spaces and non-Gaussian transition\nmodels. It is efficient because (1) local models are estimated only when the\nplanner requires them; (2) the planner focuses on the most relevant states to\nthe current planning problem; and (3) the planner focuses on the most\ninformative and/or high-value actions. Our theoretical analysis shows the\nvalidity and asymptotic optimality of the proposed approach. Empirically, we\ndemonstrate the effectiveness of our algorithm on a simulated multi-modal\npushing problem.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 15:48:03 GMT"}, {"version": "v2", "created": "Thu, 22 Sep 2016 18:08:50 GMT"}, {"version": "v3", "created": "Sun, 2 Oct 2016 05:21:17 GMT"}, {"version": "v4", "created": "Sun, 23 Oct 2016 04:05:34 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Wang", "Zi", ""], ["Jegelka", "Stefanie", ""], ["Kaelbling", "Leslie Pack", ""], ["Lozano-P\u00e9rez", "Tom\u00e1s", ""]]}, {"id": "1607.07819", "submitter": "Jason Klusowski M", "authors": "Jason M. Klusowski and Andrew R. Barron", "title": "Approximation by Combinations of ReLU and Squared ReLU Ridge Functions\n  with $ \\ell^1 $ and $ \\ell^0 $ Controls", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish $ L^{\\infty} $ and $ L^2 $ error bounds for functions of many\nvariables that are approximated by linear combinations of ReLU (rectified\nlinear unit) and squared ReLU ridge functions with $ \\ell^1 $ and $ \\ell^0 $\ncontrols on their inner and outer parameters. With the squared ReLU ridge\nfunction, we show that the $ L^2 $ approximation error is inversely\nproportional to the inner layer $ \\ell^0 $ sparsity and it need only be\nsublinear in the outer layer $ \\ell^0 $ sparsity. Our constructions are\nobtained using a variant of the Jones-Barron probabilistic method, which can be\ninterpreted as either stratified sampling with proportionate allocation or\ntwo-stage cluster sampling. We also provide companion error lower bounds that\nreveal near optimality of our constructions. Despite the sparsity assumptions,\nwe showcase the richness and flexibility of these ridge combinations by\ndefining a large family of functions, in terms of certain spectral conditions,\nthat are particularly well approximated by them.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 17:52:00 GMT"}, {"version": "v2", "created": "Mon, 8 Aug 2016 14:41:26 GMT"}, {"version": "v3", "created": "Wed, 23 May 2018 22:02:46 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Klusowski", "Jason M.", ""], ["Barron", "Andrew R.", ""]]}, {"id": "1607.07837", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Yuanzhi Li", "title": "First Efficient Convergence for Streaming k-PCA: a Global, Gap-Free, and\n  Near-Optimal Rate", "comments": "REMARK: v4 adds discussions and polishes writing; v3 contains a\n  stronger Theorem 2, a new lower bound Theorem 6, as well as new Oja++ results\n  Theorem 4 and Theorem 5", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study streaming principal component analysis (PCA), that is to find, in\n$O(dk)$ space, the top $k$ eigenvectors of a $d\\times d$ hidden matrix $\\bf\n\\Sigma$ with online vectors drawn from covariance matrix $\\bf \\Sigma$.\n  We provide $\\textit{global}$ convergence for Oja's algorithm which is\npopularly used in practice but lacks theoretical understanding for $k>1$. We\nalso provide a modified variant $\\mathsf{Oja}^{++}$ that runs $\\textit{even\nfaster}$ than Oja's. Our results match the information theoretic lower bound in\nterms of dependency on error, on eigengap, on rank $k$, and on dimension $d$,\nup to poly-log factors. In addition, our convergence rate can be made gap-free,\nthat is proportional to the approximation error and independent of the\neigengap.\n  In contrast, for general rank $k$, before our work (1) it was open to design\nany algorithm with efficient global convergence rate; and (2) it was open to\ndesign any algorithm with (even local) gap-free convergence rate in $O(dk)$\nspace.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 18:46:21 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 02:00:20 GMT"}, {"version": "v3", "created": "Fri, 4 Nov 2016 17:09:52 GMT"}, {"version": "v4", "created": "Mon, 17 Apr 2017 02:40:11 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Li", "Yuanzhi", ""]]}, {"id": "1607.07959", "submitter": "Ansaf Salleb-Aouissi", "authors": "Ilia Vovsha, Ansaf Salleb-Aouissi, Anita Raja, Thomas Koch, Alex\n  Rybchuk, Axinia Radeva, Ashwath Rajan, Yiwen Huang, Hatim Diab, Ashish Tomar,\n  and Ronald Wapner", "title": "Using Kernel Methods and Model Selection for Prediction of Preterm Birth", "comments": "Presented at 2016 Machine Learning and Healthcare Conference (MLHC\n  2016), Los Angeles, CA. In this revision, we updated page 4 by adding the\n  reference Vovsha et al. (2013) (incorrectly referenced as XXX in the previous\n  version due to double blind reviewing). The bibtex entry is now added to the\n  references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an application of machine learning to the problem of predicting\npreterm birth. We conduct a secondary analysis on a clinical trial dataset\ncollected by the National In- stitute of Child Health and Human Development\n(NICHD) while focusing our attention on predicting different classes of preterm\nbirth. We compare three approaches for deriving predictive models: a support\nvector machine (SVM) approach with linear and non-linear kernels, logistic\nregression with different model selection along with a model based on decision\nrules prescribed by physician experts for prediction of preterm birth. Our\napproach highlights the pre-processing methods applied to handle the inherent\ndynamics, noise and gaps in the data and describe techniques used to handle\nskewed class distributions. Empirical experiments demonstrate significant\nimprovement in predicting preterm birth compared to past work.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 04:56:57 GMT"}, {"version": "v2", "created": "Mon, 5 Sep 2016 12:25:00 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Vovsha", "Ilia", ""], ["Salleb-Aouissi", "Ansaf", ""], ["Raja", "Anita", ""], ["Koch", "Thomas", ""], ["Rybchuk", "Alex", ""], ["Radeva", "Axinia", ""], ["Rajan", "Ashwath", ""], ["Huang", "Yiwen", ""], ["Diab", "Hatim", ""], ["Tomar", "Ashish", ""], ["Wapner", "Ronald", ""]]}, {"id": "1607.08110", "submitter": "Daniel Malinsky", "authors": "Joseph D. Ramsey, Daniel Malinsky, Kevin V. Bui", "title": "algcomparison: Comparing the Performance of Graphical Structure Learning\n  Algorithms with TETRAD", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report we describe a tool for comparing the performance of graphical\ncausal structure learning algorithms implemented in the TETRAD freeware suite\nof causal analysis methods. Currently the tool is available as package in the\nTETRAD source code (written in Java). Simulations can be done varying the\nnumber of runs, sample sizes, and data modalities. Performance on this\nsimulated data can then be compared for a number of algorithms, with parameters\nvaried and with performance statistics as selected, producing a publishable\nreport. The package presented here may also be used to compare structure\nlearning methods across platforms and programming languages, i.e., to compare\nalgorithms implemented in TETRAD with those implemented in MATLAB, Python, or\nR.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 14:22:30 GMT"}, {"version": "v2", "created": "Sat, 1 Oct 2016 21:18:48 GMT"}, {"version": "v3", "created": "Wed, 26 Oct 2016 16:38:09 GMT"}, {"version": "v4", "created": "Mon, 23 Oct 2017 14:53:00 GMT"}, {"version": "v5", "created": "Tue, 24 Oct 2017 07:45:42 GMT"}, {"version": "v6", "created": "Wed, 18 Sep 2019 02:28:03 GMT"}, {"version": "v7", "created": "Mon, 13 Jul 2020 15:40:15 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Ramsey", "Joseph D.", ""], ["Malinsky", "Daniel", ""], ["Bui", "Kevin V.", ""]]}, {"id": "1607.08161", "submitter": "Chlo\\'e-Agathe Azencott", "authors": "Chlo\\'e-Agathe Azencott", "title": "Network-Guided Biomarker Discovery", "comments": "18 pages, 1 figure, published in LNCS 9605 (Machine Learning for\n  Health Informatics)", "journal-ref": null, "doi": "10.1007/978-3-319-50478-0_16", "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying measurable genetic indicators (or biomarkers) of a specific\ncondition of a biological system is a key element of precision medicine. Indeed\nit allows to tailor diagnostic, prognostic and treatment choice to individual\ncharacteristics of a patient. In machine learning terms, biomarker discovery\ncan be framed as a feature selection problem on whole-genome data sets.\nHowever, classical feature selection methods are usually underpowered to\nprocess these data sets, which contain orders of magnitude more features than\nsamples. This can be addressed by making the assumption that genetic features\nthat are linked on a biological network are more likely to work jointly towards\nexplaining the phenotype of interest. We review here three families of methods\nfor feature selection that integrate prior knowledge in the form of networks.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 15:53:02 GMT"}, {"version": "v2", "created": "Thu, 15 Dec 2016 13:09:49 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Azencott", "Chlo\u00e9-Agathe", ""]]}, {"id": "1607.08188", "submitter": "Yehezkel Resheff", "authors": "Yehezkel S. Resheff", "title": "Online Trajectory Segmentation and Summary With Applications to\n  Visualization and Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trajectory segmentation is the process of subdividing a trajectory into parts\neither by grouping points similar with respect to some measure of interest, or\nby minimizing a global objective function. Here we present a novel online\nalgorithm for segmentation and summary, based on point density along the\ntrajectory, and based on the nature of the naturally occurring structure of\nintermittent bouts of locomotive and local activity. We show an application to\nvisualization of trajectory datasets, and discuss the use of the summary as an\nindex allowing efficient queries which are otherwise impossible or\ncomputationally expensive, over very large datasets.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jul 2016 14:50:45 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Resheff", "Yehezkel S.", ""]]}, {"id": "1607.08194", "submitter": "Vardan Papyan", "authors": "Vardan Papyan, Yaniv Romano and Michael Elad", "title": "Convolutional Neural Networks Analyzed via Convolutional Sparse Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) have led to many state-of-the-art results\nspanning through various fields. However, a clear and profound theoretical\nunderstanding of the forward pass, the core algorithm of CNN, is still lacking.\nIn parallel, within the wide field of sparse approximation, Convolutional\nSparse Coding (CSC) has gained increasing attention in recent years. A\ntheoretical study of this model was recently conducted, establishing it as a\nreliable and stable alternative to the commonly practiced patch-based\nprocessing. Herein, we propose a novel multi-layer model, ML-CSC, in which\nsignals are assumed to emerge from a cascade of CSC layers. This is shown to be\ntightly connected to CNN, so much so that the forward pass of the CNN is in\nfact the thresholding pursuit serving the ML-CSC model. This connection brings\na fresh view to CNN, as we are able to attribute to this architecture\ntheoretical claims such as uniqueness of the representations throughout the\nnetwork, and their stable estimation, all guaranteed under simple local\nsparsity conditions. Lastly, identifying the weaknesses in the above pursuit\nscheme, we propose an alternative to the forward pass, which is connected to\ndeconvolutional, recurrent and residual networks, and has better theoretical\nguarantees.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 17:44:05 GMT"}, {"version": "v2", "created": "Sat, 30 Jul 2016 18:58:02 GMT"}, {"version": "v3", "created": "Thu, 6 Oct 2016 21:14:01 GMT"}, {"version": "v4", "created": "Mon, 10 Oct 2016 22:37:55 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Papyan", "Vardan", ""], ["Romano", "Yaniv", ""], ["Elad", "Michael", ""]]}, {"id": "1607.08254", "submitter": "Sashank J. Reddi", "authors": "Sashank J. Reddi, Suvrit Sra, Barnabas Poczos, Alex Smola", "title": "Stochastic Frank-Wolfe Methods for Nonconvex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Frank-Wolfe methods for nonconvex stochastic and finite-sum\noptimization problems. Frank-Wolfe methods (in the convex case) have gained\ntremendous recent interest in machine learning and optimization communities due\nto their projection-free property and their ability to exploit structured\nconstraints. However, our understanding of these algorithms in the nonconvex\nsetting is fairly limited. In this paper, we propose nonconvex stochastic\nFrank-Wolfe methods and analyze their convergence properties. For objective\nfunctions that decompose into a finite-sum, we leverage ideas from variance\nreduction techniques for convex optimization to obtain new variance reduced\nnonconvex Frank-Wolfe methods that have provably faster convergence than the\nclassical Frank-Wolfe method. Finally, we show that the faster convergence\nrates of our variance reduced methods also translate into improved convergence\nrates for the stochastic setting.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 20:03:47 GMT"}, {"version": "v2", "created": "Fri, 29 Jul 2016 05:01:34 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Reddi", "Sashank J.", ""], ["Sra", "Suvrit", ""], ["Poczos", "Barnabas", ""], ["Smola", "Alex", ""]]}, {"id": "1607.08310", "submitter": "Truyen Tran", "authors": "Truyen Tran, Wei Luo, Dinh Phung, Jonathan Morris, Kristen Rickard,\n  Svetha Venkatesh", "title": "Preterm Birth Prediction: Deriving Stable and Interpretable Rules from\n  High Dimensional Data", "comments": "Presented at 2016 Machine Learning and Healthcare Conference (MLHC\n  2016), Los Angeles, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preterm births occur at an alarming rate of 10-15%. Preemies have a higher\nrisk of infant mortality, developmental retardation and long-term disabilities.\nPredicting preterm birth is difficult, even for the most experienced\nclinicians. The most well-designed clinical study thus far reaches a modest\nsensitivity of 18.2-24.2% at specificity of 28.6-33.3%. We take a different\napproach by exploiting databases of normal hospital operations. We aims are\ntwofold: (i) to derive an easy-to-use, interpretable prediction rule with\nquantified uncertainties, and (ii) to construct accurate classifiers for\npreterm birth prediction. Our approach is to automatically generate and select\nfrom hundreds (if not thousands) of possible predictors using stability-aware\ntechniques. Derived from a large database of 15,814 women, our simplified\nprediction rule with only 10 items has sensitivity of 62.3% at specificity of\n81.5%.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 04:25:30 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Tran", "Truyen", ""], ["Luo", "Wei", ""], ["Phung", "Dinh", ""], ["Morris", "Jonathan", ""], ["Rickard", "Kristen", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1607.08316", "submitter": "Ilija Ilievski", "authors": "Ilija Ilievski and Taimoor Akhtar and Jiashi Feng and Christine\n  Annette Shoemaker", "title": "Efficient Hyperparameter Optimization of Deep Learning Algorithms Using\n  Deterministic RBF Surrogates", "comments": "AAAI-17 Camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically searching for optimal hyperparameter configurations is of\ncrucial importance for applying deep learning algorithms in practice. Recently,\nBayesian optimization has been proposed for optimizing hyperparameters of\nvarious machine learning algorithms. Those methods adopt probabilistic\nsurrogate models like Gaussian processes to approximate and minimize the\nvalidation error function of hyperparameter values. However, probabilistic\nsurrogates require accurate estimates of sufficient statistics (e.g.,\ncovariance) of the error distribution and thus need many function evaluations\nwith a sizeable number of hyperparameters. This makes them inefficient for\noptimizing hyperparameters of deep learning algorithms, which are highly\nexpensive to evaluate. In this work, we propose a new deterministic and\nefficient hyperparameter optimization method that employs radial basis\nfunctions as error surrogates. The proposed mixed integer algorithm, called\nHORD, searches the surrogate for the most promising hyperparameter values\nthrough dynamic coordinate search and requires many fewer function evaluations.\nHORD does well in low dimensions but it is exceptionally better in higher\ndimensions. Extensive evaluations on MNIST and CIFAR-10 for four deep neural\nnetworks demonstrate HORD significantly outperforms the well-established\nBayesian optimization methods such as GP, SMAC, and TPE. For instance, on\naverage, HORD is more than 6 times faster than GP-EI in obtaining the best\nconfiguration of 19 hyperparameters.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 05:03:32 GMT"}, {"version": "v2", "created": "Sat, 21 Jan 2017 03:26:06 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Ilievski", "Ilija", ""], ["Akhtar", "Taimoor", ""], ["Feng", "Jiashi", ""], ["Shoemaker", "Christine Annette", ""]]}, {"id": "1607.08379", "submitter": "Ludovica Bachschmid-Romano", "authors": "Ludovica Bachschmid-Romano, Claudia Battistin, Manfred Opper, Yasser\n  Roudi", "title": "Variational perturbation and extended Plefka approaches to dynamics on\n  random networks: the case of the kinetic Ising model", "comments": null, "journal-ref": null, "doi": "10.1088/1751-8113/49/43/434003", "report-no": null, "categories": "cond-mat.dis-nn physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe and analyze some novel approaches for studying the dynamics of\nIsing spin glass models. We first briefly consider the variational approach\nbased on minimizing the Kullback-Leibler divergence between independent\ntrajectories and the real ones and note that this approach only coincides with\nthe mean field equations from the saddle point approximation to the generating\nfunctional when the dynamics is defined through a logistic link function, which\nis the case for the kinetic Ising model with parallel update. We then spend the\nrest of the paper developing two ways of going beyond the saddle point\napproximation to the generating functional. In the first one, we develop a\nvariational perturbative approximation to the generating functional by\nexpanding the action around a quadratic function of the local fields and\nconjugate local fields whose parameters are optimized. We derive analytical\nexpressions for the optimal parameters and show that when the optimization is\nsuitably restricted, we recover the mean field equations that are exact for the\nfully asymmetric random couplings (M\\'ezard and Sakellariou, 2011). However,\nwithout this restriction the results are different. We also describe an\nextended Plefka expansion in which in addition to the magnetization, we also\nfix the correlation and response functions. Finally, we numerically study the\nperformance of these approximations for Sherrington-Kirkpatrick type couplings\nfor various coupling strengths, degrees of coupling symmetry and external\nfields. We show that the dynamical equations derived from the extended Plefka\nexpansion outperform the others in all regimes, although it is computationally\nmore demanding. The unconstrained variational approach does not perform well in\nthe small coupling regime, while it approaches dynamical TAP equations of\n(Roudi and Hertz, 2011) for strong couplings.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 09:40:44 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Bachschmid-Romano", "Ludovica", ""], ["Battistin", "Claudia", ""], ["Opper", "Manfred", ""], ["Roudi", "Yasser", ""]]}, {"id": "1607.08456", "submitter": "Matth\\\"aus Kleindessner", "authors": "Matth\\\"aus Kleindessner and Ulrike von Luxburg", "title": "Kernel functions based on triplet comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given only information in the form of similarity triplets \"Object A is more\nsimilar to object B than to object C\" about a data set, we propose two ways of\ndefining a kernel function on the data set. While previous approaches construct\na low-dimensional Euclidean embedding of the data set that reflects the given\nsimilarity triplets, we aim at defining kernel functions that correspond to\nhigh-dimensional embeddings. These kernel functions can subsequently be used to\napply any kernel method to the data set.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 13:46:06 GMT"}, {"version": "v2", "created": "Sun, 29 Oct 2017 21:33:41 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Kleindessner", "Matth\u00e4us", ""], ["von Luxburg", "Ulrike", ""]]}, {"id": "1607.08458", "submitter": "Alexandre Gramfort", "authors": "Daniel Strohmeier, Yousra Bekhti, Jens Haueisen, Alexandre Gramfort", "title": "The iterative reweighted Mixed-Norm Estimate for spatio-temporal MEG/EEG\n  source reconstruction", "comments": null, "journal-ref": null, "doi": "10.1109/TMI.2016.2553445", "report-no": null, "categories": "stat.AP q-bio.NC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Source imaging based on magnetoencephalography (MEG) and\nelectroencephalography (EEG) allows for the non-invasive analysis of brain\nactivity with high temporal and good spatial resolution. As the\nbioelectromagnetic inverse problem is ill-posed, constraints are required. For\nthe analysis of evoked brain activity, spatial sparsity of the neuronal\nactivation is a common assumption. It is often taken into account using convex\nconstraints based on the l1-norm. The resulting source estimates are however\nbiased in amplitude and often suboptimal in terms of source selection due to\nhigh correlations in the forward model. In this work, we demonstrate that an\ninverse solver based on a block-separable penalty with a Frobenius norm per\nblock and a l0.5-quasinorm over blocks addresses both of these issues. For\nsolving the resulting non-convex optimization problem, we propose the iterative\nreweighted Mixed Norm Estimate (irMxNE), an optimization scheme based on\niterative reweighted convex surrogate optimization problems, which are solved\nefficiently using a block coordinate descent scheme and an active set strategy.\nWe compare the proposed sparse imaging method to the dSPM and the RAP-MUSIC\napproach based on two MEG data sets. We provide empirical evidence based on\nsimulations and analysis of MEG data that the proposed method improves on the\nstandard Mixed Norm Estimate (MxNE) in terms of amplitude bias, support\nrecovery, and stability.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 13:56:04 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Strohmeier", "Daniel", ""], ["Bekhti", "Yousra", ""], ["Haueisen", "Jens", ""], ["Gramfort", "Alexandre", ""]]}, {"id": "1607.08601", "submitter": "Minh Tang", "authors": "Minh Tang, Carey E. Priebe", "title": "Limit theorems for eigenvectors of the normalized Laplacian for random\n  graphs", "comments": "52 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a central limit theorem for the components of the eigenvectors\ncorresponding to the $d$ largest eigenvalues of the normalized Laplacian matrix\nof a finite dimensional random dot product graph. As a corollary, we show that\nfor stochastic blockmodel graphs, the rows of the spectral embedding of the\nnormalized Laplacian converge to multivariate normals and furthermore the mean\nand the covariance matrix of each row are functions of the associated vertex's\nblock membership. Together with prior results for the eigenvectors of the\nadjacency matrix, we then compare, via the Chernoff information between\nmultivariate normal distributions, how the choice of embedding method impacts\nsubsequent inference. We demonstrate that neither embedding method dominates\nwith respect to the inference task of recovering the latent block assignments.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 19:56:43 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Tang", "Minh", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1607.08647", "submitter": "Rounak Dey", "authors": "Rounak Dey and Seunggeun Lee", "title": "Asymptotic properties of Principal Component Analysis and shrinkage-bias\n  adjustment under the Generalized Spiked Population model", "comments": null, "journal-ref": "J. Multivariate Anal., 173 (2019), pp. 145-164", "doi": "10.1016/j.jmva.2019.02.007", "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of high-throughput technologies, principal component\nanalysis (PCA) in the high-dimensional regime is of great interest. Most of the\nexisting theoretical and methodological results for high-dimensional PCA are\nbased on the spiked population model in which all the population eigenvalues\nare equal except for a few large ones. Due to the presence of local correlation\namong features, however, this assumption may not be satisfied in many\nreal-world datasets. To address this issue, we investigated the asymptotic\nbehaviors of PCA under the generalized spiked population model. Based on the\ntheoretical results, we proposed a series of methods for the consistent\nestimation of population eigenvalues, angles between the sample and population\neigenvectors, correlation coefficients between the sample and population\nprincipal component (PC) scores, and the shrinkage bias adjustment for the\npredicted PC scores. Using numerical experiments and real data examples from\nthe genetics literature, we showed that our methods can greatly reduce bias and\nimprove prediction accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 21:45:58 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Dey", "Rounak", ""], ["Lee", "Seunggeun", ""]]}, {"id": "1607.08691", "submitter": "Hamidreza Alvari", "authors": "Hamidreza Alvari, Paulo Shakarian, J.E. Kelly Snyder", "title": "A Non-Parametric Learning Approach to Identify Online Human Trafficking", "comments": "Accepted in IEEE Intelligence and Security Informatics 2016\n  Conference (ISI 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human trafficking is among the most challenging law enforcement problems\nwhich demands persistent fight against from all over the globe. In this study,\nwe leverage readily available data from the website \"Backpage\"-- used for\nclassified advertisement-- to discern potential patterns of human trafficking\nactivities which manifest online and identify most likely trafficking related\nadvertisements. Due to the lack of ground truth, we rely on two human analysts\n--one human trafficking victim survivor and one from law enforcement, for\nhand-labeling the small portion of the crawled data. We then present a\nsemi-supervised learning approach that is trained on the available labeled and\nunlabeled data and evaluated on unseen data with further verification of\nexperts.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 06:05:08 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2016 00:48:29 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Alvari", "Hamidreza", ""], ["Shakarian", "Paulo", ""], ["Snyder", "J. E. Kelly", ""]]}, {"id": "1607.08720", "submitter": "Benjamin Rubinstein", "authors": "Jiazhen He, Benjamin I. P. Rubinstein, James Bailey, Rui Zhang, Sandra\n  Milligan", "title": "TopicResponse: A Marriage of Topic Modelling and Rasch Modelling for\n  Automatic Measurement in MOOCs", "comments": "In preparation for journal submission; Revisions to improve clarity\n  with additional examples", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the suitability of using automatically discovered topics\nfrom MOOC discussion forums for modelling students' academic abilities. The\nRasch model from psychometrics is a popular generative probabilistic model that\nrelates latent student skill, latent item difficulty, and observed student-item\nresponses within a principled, unified framework. According to scholarly\neducational theory, discovered topics can be regarded as appropriate\nmeasurement items if (1) students' participation across the discovered topics\nis well fit by the Rasch model, and if (2) the topics are interpretable to\nsubject-matter experts as being educationally meaningful. Such Rasch-scaled\ntopics, with associated difficulty levels, could be of potential benefit to\ncurriculum refinement, student assessment and personalised feedback. The\ntechnical challenge that remains, is to discover meaningful topics that\nsimultaneously achieve good statistical fit with the Rasch model. To address\nthis challenge, we combine the Rasch model with non-negative matrix\nfactorisation based topic modelling, jointly fitting both models. We\ndemonstrate the suitability of our approach with quantitative experiments on\ndata from three Coursera MOOCs, and with qualitative survey results on topic\ninterpretability on a Discrete Optimisation MOOC.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 08:17:45 GMT"}, {"version": "v2", "created": "Mon, 20 Mar 2017 04:30:38 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["He", "Jiazhen", ""], ["Rubinstein", "Benjamin I. P.", ""], ["Bailey", "James", ""], ["Zhang", "Rui", ""], ["Milligan", "Sandra", ""]]}, {"id": "1607.08756", "submitter": "Andrea Cristofari", "authors": "Andrea Cristofari", "title": "Data Filtering for Cluster Analysis by $\\ell_0$-Norm Regularization", "comments": "Optimization Letters (2017)", "journal-ref": null, "doi": "10.1007/s11590-017-1152-7", "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A data filtering method for cluster analysis is proposed, based on minimizing\na least squares function with a weighted $\\ell_0$-norm penalty. To overcome the\ndiscontinuity of the objective function, smooth non-convex functions are\nemployed to approximate the $\\ell_0$-norm. The convergence of the global\nminimum points of the approximating problems towards global minimum points of\nthe original problem is stated. The proposed method also exploits a suitable\ntechnique to choose the penalty parameter. Numerical results on synthetic and\nreal data sets are finally provided, showing how some existing clustering\nmethods can take advantages from the proposed filtering strategy.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 10:21:24 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 14:23:09 GMT"}, {"version": "v3", "created": "Mon, 22 May 2017 12:54:58 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Cristofari", "Andrea", ""]]}, {"id": "1607.08810", "submitter": "Mathieu Blondel", "authors": "Mathieu Blondel, Masakazu Ishihata, Akinori Fujino, Naonori Ueda", "title": "Polynomial Networks and Factorization Machines: New Insights and\n  Efficient Training Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polynomial networks and factorization machines are two recently-proposed\nmodels that can efficiently use feature interactions in classification and\nregression tasks. In this paper, we revisit both models from a unified\nperspective. Based on this new view, we study the properties of both models and\npropose new efficient training algorithms. Key to our approach is to cast\nparameter learning as a low-rank symmetric tensor estimation problem, which we\nsolve by multi-convex optimization. We demonstrate our approach on regression\nand recommender system tasks.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 13:54:51 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Blondel", "Mathieu", ""], ["Ishihata", "Masakazu", ""], ["Fujino", "Akinori", ""], ["Ueda", "Naonori", ""]]}, {"id": "1607.08877", "submitter": "Stephen Rush", "authors": "Stephen T Rush and Christine H Lee and Washington Mio and Peter T Kim", "title": "The Phylogenetic LASSO and the Microbiome", "comments": "31 pages, 6 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific investigations that incorporate next generation sequencing involve\nanalyses of high-dimensional data where the need to organize, collate and\ninterpret the outcomes are pressingly important. Currently, data can be\ncollected at the microbiome level leading to the possibility of personalized\nmedicine whereby treatments can be tailored at this scale. In this paper, we\nlay down a statistical framework for this type of analysis with a view toward\nsynthesis of products tailored to individual patients. Although the paper\napplies the technique to data for a particular infectious disease, the\nmethodology is sufficiently rich to be expanded to other problems in medicine,\nespecially those in which coincident `-omics' covariates and clinical responses\nare simultaneously captured.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 17:56:44 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Rush", "Stephen T", ""], ["Lee", "Christine H", ""], ["Mio", "Washington", ""], ["Kim", "Peter T", ""]]}, {"id": "1607.08891", "submitter": "Brian Helfer", "authors": "Brian S. Helfer, James R. Williamson, Benjamin A. Miller, Joseph\n  Perricone, Thomas F. Quatieri", "title": "Assessing Functional Neural Connectivity as an Indicator of Cognitive\n  Performance", "comments": "Oral presentation at MLINI 2015 - 5th NIPS Workshop on Machine\n  Learning and Interpretation in Neuroimaging (arXiv:1605.04435)", "journal-ref": null, "doi": null, "report-no": "MLINI/2015/17", "categories": "stat.ML q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies in recent years have demonstrated that neural organization and\nstructure impact an individual's ability to perform a given task. Specifically,\nindividuals with greater neural efficiency have been shown to outperform those\nwith less organized functional structure. In this work, we compare the\npredictive ability of properties of neural connectivity on a working memory\ntask. We provide two novel approaches for characterizing functional network\nconnectivity from electroencephalography (EEG), and compare these features to\nthe average power across frequency bands in EEG channels. Our first novel\napproach represents functional connectivity structure through the distribution\nof eigenvalues making up channel coherence matrices in multiple frequency\nbands. Our second approach creates a connectivity network at each frequency\nband, and assesses variability in average path lengths of connected components\nand degree across the network. Failures in digit and sentence recall on single\ntrials are detected using a Gaussian classifier for each feature set, at each\nfrequency band. The classifier results are then fused across frequency bands,\nwith the resulting detection performance summarized using the area under the\nreceiver operating characteristic curve (AUC) statistic. Fused AUC results of\n0.63/0.58/0.61 for digit recall failure and 0.58/0.59/0.54 for sentence recall\nfailure are obtained from the connectivity structure, graph variability, and\nchannel power features respectively.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 18:42:39 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Helfer", "Brian S.", ""], ["Williamson", "James R.", ""], ["Miller", "Benjamin A.", ""], ["Perricone", "Joseph", ""], ["Quatieri", "Thomas F.", ""]]}]