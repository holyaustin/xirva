[{"id": "1904.00014", "submitter": "Daniel Muthukrishna", "authors": "Daniel Muthukrishna, Gautham Narayan, Kaisey S. Mandel, Rahul Biswas,\n  Ren\\'ee Hlo\\v{z}ek", "title": "RAPID: Early Classification of Explosive Transients using Deep Learning", "comments": "Accepted version. 28 pages, 16 figures, 2 tables, PASP Special Issue\n  on Methods for Time-Domain Astrophysics. Submitted: 13 December 2018,\n  Accepted: 26 March 2019", "journal-ref": "PASP 131, 118002 (2019)", "doi": "10.1088/1538-3873/ab1609", "report-no": null, "categories": "astro-ph.IM astro-ph.HE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present RAPID (Real-time Automated Photometric IDentification), a novel\ntime-series classification tool capable of automatically identifying transients\nfrom within a day of the initial alert, to the full lifetime of a light curve.\nUsing a deep recurrent neural network with Gated Recurrent Units (GRUs), we\npresent the first method specifically designed to provide early classifications\nof astronomical time-series data, typing 12 different transient classes. Our\nclassifier can process light curves with any phase coverage, and it does not\nrely on deriving computationally expensive features from the data, making RAPID\nwell-suited for processing the millions of alerts that ongoing and upcoming\nwide-field surveys such as the Zwicky Transient Facility (ZTF), and the Large\nSynoptic Survey Telescope (LSST) will produce. The classification accuracy\nimproves over the lifetime of the transient as more photometric data becomes\navailable, and across the 12 transient classes, we obtain an average area under\nthe receiver operating characteristic curve of 0.95 and 0.98 at early and late\nepochs, respectively. We demonstrate RAPID's ability to effectively provide\nearly classifications of observed transients from the ZTF data stream. We have\nmade RAPID available as an open-source software package\n(https://astrorapid.readthedocs.io) for machine learning-based alert-brokers to\nuse for the autonomous and quick classification of several thousand light\ncurves within a few seconds.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 18:00:00 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 16:14:46 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Muthukrishna", "Daniel", ""], ["Narayan", "Gautham", ""], ["Mandel", "Kaisey S.", ""], ["Biswas", "Rahul", ""], ["Hlo\u017eek", "Ren\u00e9e", ""]]}, {"id": "1904.00035", "submitter": "Subramanya Nageshrao", "authors": "Subramanya Nageshrao, Eric Tseng and Dimitar Filev", "title": "Autonomous Highway Driving using Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The operational space of an autonomous vehicle (AV) can be diverse and vary\nsignificantly. This may lead to a scenario that was not postulated in the\ndesign phase. Due to this, formulating a rule based decision maker for\nselecting maneuvers may not be ideal. Similarly, it may not be effective to\ndesign an a-priori cost function and then solve the optimal control problem in\nreal-time. In order to address these issues and to avoid peculiar behaviors\nwhen encountering unforeseen scenario, we propose a reinforcement learning (RL)\nbased method, where the ego car, i.e., an autonomous vehicle, learns to make\ndecisions by directly interacting with simulated traffic. The decision maker\nfor AV is implemented as a deep neural network providing an action choice for a\ngiven system state. In a critical application such as driving, an RL agent\nwithout explicit notion of safety may not converge or it may need extremely\nlarge number of samples before finding a reliable policy. To best address the\nissue, this paper incorporates reinforcement learning with an additional short\nhorizon safety check (SC). In a critical scenario, the safety check will also\nprovide an alternate safe action to the agent provided if it exists. This leads\nto two novel contributions. First, it generalizes the states that could lead to\nundesirable \"near-misses\" or \"collisions \". Second, inclusion of safety check\ncan provide a safe and stable training environment. This significantly enhances\nlearning efficiency without inhibiting meaningful exploration to ensure safe\nand optimal learned behavior. We demonstrate the performance of the developed\nalgorithm in highway driving scenario where the trained AV encounters varying\ntraffic density in a highway setting.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 18:15:24 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Nageshrao", "Subramanya", ""], ["Tseng", "Eric", ""], ["Filev", "Dimitar", ""]]}, {"id": "1904.00045", "submitter": "Collin Burns", "authors": "Collin Burns, Jesse Thomason, and Wesley Tansey", "title": "Interpreting Black Box Models via Hypothesis Testing", "comments": "FODS 2020", "journal-ref": null, "doi": "10.1145/3412815.3416889", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In science and medicine, model interpretations may be reported as discoveries\nof natural phenomena or used to guide patient treatments. In such high-stakes\ntasks, false discoveries may lead investigators astray. These applications\nwould therefore benefit from control over the finite-sample error rate of\ninterpretations. We reframe black box model interpretability as a multiple\nhypothesis testing problem. The task is to discover \"important\" features by\ntesting whether the model prediction is significantly different from what would\nbe expected if the features were replaced with uninformative counterfactuals.\nWe propose two testing methods: one that provably controls the false discovery\nrate but which is not yet feasible for large-scale applications, and an\napproximate testing method which can be applied to real-world data sets. In\nsimulation, both tests have high power relative to existing interpretability\nmethods. When applied to state-of-the-art vision and language models, the\nframework selects features that intuitively explain model predictions. The\nresulting explanations have the additional advantage that they are themselves\neasy to interpret.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 18:47:58 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 03:18:23 GMT"}, {"version": "v3", "created": "Mon, 17 Aug 2020 17:28:57 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Burns", "Collin", ""], ["Thomason", "Jesse", ""], ["Tansey", "Wesley", ""]]}, {"id": "1904.00070", "submitter": "Yi Hao", "authors": "Yi Hao, Alon Orlitsky, Ananda T. Suresh, Yihong Wu", "title": "Data Amplification: A Unified and Competitive Approach to Property\n  Estimation", "comments": "In NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating properties of discrete distributions is a fundamental problem in\nstatistical learning. We design the first unified, linear-time, competitive,\nproperty estimator that for a wide class of properties and for all underlying\ndistributions uses just $2n$ samples to achieve the performance attained by the\nempirical estimator with $n\\sqrt{\\log n}$ samples. This provides off-the-shelf,\ndistribution-independent, \"amplification\" of the amount of data available\nrelative to common-practice estimators.\n  We illustrate the estimator's practical advantages by comparing it to\nexisting estimators for a wide variety of properties and distributions. In most\ncases, its performance with $n$ samples is even as good as that of the\nempirical estimator with $n\\log n$ samples, and for essentially all properties,\nits performance is comparable to that of the best existing estimator designed\nspecifically for that property.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 19:49:01 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Hao", "Yi", ""], ["Orlitsky", "Alon", ""], ["Suresh", "Ananda T.", ""], ["Wu", "Yihong", ""]]}, {"id": "1904.00093", "submitter": "Souvik Chakraborty", "authors": "Rajdip Nayek and Souvik Chakraborty and Sriram Narasimhan", "title": "A Gaussian process latent force model for joint input-state estimation\n  in linear structural systems", "comments": "Submitted to Mechanical Systems and Signal Processing", "journal-ref": null, "doi": "10.1016/j.ymssp.2019.03.048", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of combined state and input estimation of linear structural\nsystems based on measured responses and a priori knowledge of structural model\nis considered. A novel methodology using Gaussian process latent force models\nis proposed to tackle the problem in a stochastic setting. Gaussian process\nlatent force models (GPLFMs) are hybrid models that combine differential\nequations representing a physical system with data-driven non-parametric\nGaussian process models. In this work, the unknown input forces acting on a\nstructure are modelled as Gaussian processes with some chosen covariance\nfunctions which are combined with the mechanistic differential equation\nrepresenting the structure to construct a GPLFM. The GPLFM is then conveniently\nformulated as an augmented stochastic state-space model with additional states\nrepresenting the latent force components, and the joint input and state\ninference of the resulting model is implemented using Kalman filter. The\naugmented state-space model of GPLFM is shown as a generalization of the class\nof input-augmented state-space models, is proven observable, and is robust\ncompared to conventional augmented formulations in terms of numerical\nstability. The hyperparameters governing the covariance functions are estimated\nusing maximum likelihood optimization based on the observed data, thus\novercoming the need for manual tuning of the hyperparameters by\ntrial-and-error. To assess the performance of the proposed GPLFM method,\nseveral cases of state and input estimation are demonstrated using numerical\nsimulations on a 10-dof shear building and a 76-storey ASCE benchmark office\ntower. Results obtained indicate the superior performance of the proposed\napproach over conventional Kalman filter based approaches.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 21:29:54 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 01:21:50 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Nayek", "Rajdip", ""], ["Chakraborty", "Souvik", ""], ["Narasimhan", "Sriram", ""]]}, {"id": "1904.00138", "submitter": "Chen Hao", "authors": "K.S. Rajput, S. Wibowo, C. Hao, M. Majmudar", "title": "On Arrhythmia Detection by Deep Learning and Multidimensional\n  Representation", "comments": "draft paper; prepared for journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An electrocardiogram (ECG) is a time-series signal that is represented by\none-dimensional (1-D) data. Higher dimensional representation contains more\ninformation that is accessible for feature extraction. Hidden variables such as\nfrequency relation and morphology of segment is not directly accessible in the\ntime domain. In this paper, 1-D time series data is converted into\nmulti-dimensional representation in the form of multichannel 2-D images.\nFollowing that, deep learning was used to train a deep neural network based\nclassifier to detect arrhythmias. The results of simulation on testing database\ndemonstrate the effectiveness of the proposed methodology by showing an\noutstanding classification performance compared to other existing methods and\nhand-crafted annotations made by certified cardiologists.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 02:50:23 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 04:52:45 GMT"}, {"version": "v3", "created": "Wed, 3 Apr 2019 10:51:52 GMT"}, {"version": "v4", "created": "Thu, 11 Apr 2019 08:27:51 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Rajput", "K. S.", ""], ["Wibowo", "S.", ""], ["Hao", "C.", ""], ["Majmudar", "M.", ""]]}, {"id": "1904.00152", "submitter": "Chieh-Hsin Lai", "authors": "Chieh-Hsin Lai, Dongmian Zou, and Gilad Lerman", "title": "Robust Subspace Recovery Layer for Unsupervised Anomaly Detection", "comments": "This work is on the ICLR 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a neural network for unsupervised anomaly detection with a novel\nrobust subspace recovery layer (RSR layer). This layer seeks to extract the\nunderlying subspace from a latent representation of the given data and removes\noutliers that lie away from this subspace. It is used within an autoencoder.\nThe encoder maps the data into a latent space, from which the RSR layer\nextracts the subspace. The decoder then smoothly maps back the underlying\nsubspace to a \"manifold\" close to the original inliers. Inliers and outliers\nare distinguished according to the distances between the original and mapped\npositions (small for inliers and large for outliers). Extensive numerical\nexperiments with both image and document datasets demonstrate state-of-the-art\nprecision and recall.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 05:30:54 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2019 22:44:25 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Lai", "Chieh-Hsin", ""], ["Zou", "Dongmian", ""], ["Lerman", "Gilad", ""]]}, {"id": "1904.00170", "submitter": "Jingcai Guo", "authors": "Jingcai Guo, Song Guo", "title": "Adaptive Adjustment with Semantic Feature Space for Zero-Shot\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most recent years, zero-shot recognition (ZSR) has gained increasing\nattention in machine learning and image processing fields. It aims at\nrecognizing unseen class instances with knowledge transferred from seen\nclasses. This is typically achieved by exploiting a pre-defined semantic\nfeature space (FS), i.e., semantic attributes or word vectors, as a bridge to\ntransfer knowledge between seen and unseen classes. However, due to the absence\nof unseen classes during training, the conventional ZSR easily suffers from\ndomain shift and hubness problems. In this paper, we propose a novel ZSR\nlearning framework that can handle these two issues well by adaptively\nadjusting semantic FS. To the best of our knowledge, our work is the first to\nconsider the adaptive adjustment of semantic FS in ZSR. Moreover, our solution\ncan be formulated to a more efficient framework that significantly boosts the\ntraining. Extensive experiments show the remarkable performance improvement of\nour model compared with other existing methods.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 08:39:03 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Guo", "Jingcai", ""], ["Guo", "Song", ""]]}, {"id": "1904.00172", "submitter": "Jingcai Guo", "authors": "Jingcai Guo, Song Guo", "title": "EE-AE: An Exclusivity Enhanced Unsupervised Feature Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning is becoming more and more important recently. As one of\nits key components, the autoencoder (AE) aims to learn a latent feature\nrepresentation of data which is more robust and discriminative. However, most\nAE based methods only focus on the reconstruction within the encoder-decoder\nphase, which ignores the inherent relation of data, i.e., statistical and\ngeometrical dependence, and easily causes overfitting. In order to deal with\nthis issue, we propose an Exclusivity Enhanced (EE) unsupervised feature\nlearning approach to improve the conventional AE. To the best of our knowledge,\nour research is the first to utilize such exclusivity concept to cooperate with\nfeature extraction within AE. Moreover, in this paper we also make some\nimprovements to the stacked AE structure especially for the connection of\ndifferent layers from decoders, this could be regarded as a weight\ninitialization trial. The experimental results show that our proposed approach\ncan achieve remarkable performance compared with other related methods.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 08:46:23 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Guo", "Jingcai", ""], ["Guo", "Song", ""]]}, {"id": "1904.00173", "submitter": "Daniil Ryabko", "authors": "Daniil Ryabko", "title": "Asymptotic nonparametric statistical analysis of stationary time series", "comments": "This is the author's version of the homonymous volume published by\n  Springer. The final authenticated version is available online at:\n  https://doi.org/10.1007/978-3-030-12564-6 Further updates and corrections may\n  be made here", "journal-ref": null, "doi": "10.1007/978-3-030-12564-6", "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stationarity is a very general, qualitative assumption, that can be assessed\non the basis of application specifics. It is thus a rather attractive\nassumption to base statistical analysis on, especially for problems for which\nless general qualitative assumptions, such as independence or finite memory,\nclearly fail. However, it has long been considered too general to allow for\nstatistical inference to be made. One of the reasons for this is that rates of\nconvergence, even of frequencies to the mean, are not available under this\nassumption alone. Recently, it has been shown that, while some natural and\nsimple problems such as homogeneity, are indeed provably impossible to solve if\none only assumes that the data is stationary (or stationary ergodic), many\nothers can be solved using rather simple and intuitive algorithms. The latter\nproblems include clustering and change point estimation. In this volume I\nsummarize these results. The emphasis is on asymptotic consistency, since this\nthe strongest property one can obtain assuming stationarity alone. While for\nmost of the problems for which a solution is found this solution is\nalgorithmically realizable, the main objective in this area of research, the\nobjective which is only partially attained, is to understand what is possible\nand what is not possible to do for stationary time series. The considered\nproblems include homogeneity testing, clustering with respect to distribution,\nclustering with respect to independence, change-point estimation, identity\ntesting, and the general question of composite hypotheses testing. For the\nlatter problem, a topological criterion for the existence of a consistent test\nis presented. In addition, several open questions are discussed.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 08:47:46 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Ryabko", "Daniil", ""]]}, {"id": "1904.00176", "submitter": "Zhipeng Wang", "authors": "Zhipeng Wang and David W. Scott", "title": "Nonparametric Density Estimation for High-Dimensional Data - Algorithms\n  and Applications", "comments": null, "journal-ref": "Wiley Interdisciplinary Reviews: Computational Statistics, 2019", "doi": "10.1002/wics.1461", "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Density Estimation is one of the central areas of statistics whose purpose is\nto estimate the probability density function underlying the observed data. It\nserves as a building block for many tasks in statistical inference,\nvisualization, and machine learning. Density Estimation is widely adopted in\nthe domain of unsupervised learning especially for the application of\nclustering. As big data become pervasive in almost every area of data sciences,\nanalyzing high-dimensional data that have many features and variables appears\nto be a major focus in both academia and industry. High-dimensional data pose\nchallenges not only from the theoretical aspects of statistical inference, but\nalso from the algorithmic/computational considerations of machine learning and\ndata analytics. This paper reviews a collection of selected nonparametric\ndensity estimation algorithms for high-dimensional data, some of them are\nrecently published and provide interesting mathematical insights. The important\napplication domain of nonparametric density estimation, such as { modal\nclustering}, are also included in this paper. Several research directions\nrelated to density estimation and high-dimensional data analysis are suggested\nby the authors.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 09:08:45 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Wang", "Zhipeng", ""], ["Scott", "David W.", ""]]}, {"id": "1904.00197", "submitter": "Suraj Tripathi", "authors": "Abhay Kumar, Nishant Jain, Chirag Singh, Suraj Tripathi", "title": "Exploiting SIFT Descriptor for Rotation Invariant Convolutional Neural\n  Network", "comments": "Accepted in IEEE INDICON 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach to exploit the distinctive invariant\nfeatures in convolutional neural network. The proposed CNN model uses Scale\nInvariant Feature Transform (SIFT) descriptor instead of the max-pooling layer.\nMax-pooling layer discards the pose, i.e., translational and rotational\nrelationship between the low-level features, and hence unable to capture the\nspatial hierarchies between low and high level features. The SIFT descriptor\nlayer captures the orientation and the spatial relationship of the features\nextracted by convolutional layer. The proposed SIFT Descriptor CNN therefore\ncombines the feature extraction capabilities of CNN model and rotation\ninvariance of SIFT descriptor. Experimental results on the MNIST and\nfashionMNIST datasets indicates reasonable improvements over conventional\nmethods available in literature.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 11:00:21 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Kumar", "Abhay", ""], ["Jain", "Nishant", ""], ["Singh", "Chirag", ""], ["Tripathi", "Suraj", ""]]}, {"id": "1904.00231", "submitter": "Junjie Wang", "authors": "Junjie Wang, Qichao Zhang, Dongbin Zhao, Yaran Chen", "title": "Lane Change Decision-making through Deep Reinforcement Learning with\n  Rule-based Constraints", "comments": "6 pages, 5 figures, accepted at 2019 International Joint Conference\n  on Neural Networks(IJCNN)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving decision-making is a great challenge due to the complexity\nand uncertainty of the traffic environment. Combined with the rule-based\nconstraints, a Deep Q-Network (DQN) based method is applied for autonomous\ndriving lane change decision-making task in this study. Through the combination\nof high-level lateral decision-making and low-level rule-based trajectory\nmodification, a safe and efficient lane change behavior can be achieved. With\nthe setting of our state representation and reward function, the trained agent\nis able to take appropriate actions in a real-world-like simulator. The\ngenerated policy is evaluated on the simulator for 10 times, and the results\ndemonstrate that the proposed rule-based DQN method outperforms the rule-based\napproach and the DQN method.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 15:16:39 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 01:26:22 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Wang", "Junjie", ""], ["Zhang", "Qichao", ""], ["Zhao", "Dongbin", ""], ["Chen", "Yaran", ""]]}, {"id": "1904.00242", "submitter": "Yuan Zhou", "authors": "Yingkai Li and Yining Wang and Yuan Zhou", "title": "Nearly Minimax-Optimal Regret for Linearly Parameterized Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the linear contextual bandit problem with finite action sets. When\nthe problem dimension is $d$, the time horizon is $T$, and there are $n \\leq\n2^{d/2}$ candidate actions per time period, we (1) show that the minimax\nexpected regret is $\\Omega(\\sqrt{dT (\\log T) (\\log n)})$ for every algorithm,\nand (2) introduce a Variable-Confidence-Level (VCL) SupLinUCB algorithm whose\nregret matches the lower bound up to iterated logarithmic factors. Our\nalgorithmic result saves two $\\sqrt{\\log T}$ factors from previous analysis,\nand our information-theoretical lower bound also improves previous results by\none $\\sqrt{\\log T}$ factor, revealing a regret scaling quite different from\nclassical multi-armed bandits in which no logarithmic $T$ term is present in\nminimax regret. Our proof techniques include variable confidence levels and a\ncareful analysis of layer sizes of SupLinUCB on the upper bound side, and\ndelicately constructed adversarial sequences showing the tightness of\nelliptical potential lemmas on the lower bound side.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 16:16:23 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 22:51:34 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Li", "Yingkai", ""], ["Wang", "Yining", ""], ["Zhou", "Yuan", ""]]}, {"id": "1904.00243", "submitter": "Hugo Caselles-Dupr\\'e", "authors": "Hugo Caselles-Dupr\\'e, Michael Garcia-Ortiz, David Filliat", "title": "Symmetry-Based Disentangled Representation Learning requires Interaction\n  with Environments", "comments": "33rd Conference on Neural Information Processing Systems (NeurIPS\n  2019), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding a generally accepted formal definition of a disentangled\nrepresentation in the context of an agent behaving in an environment is an\nimportant challenge towards the construction of data-efficient autonomous\nagents. Higgins et al. recently proposed Symmetry-Based Disentangled\nRepresentation Learning, a definition based on a characterization of symmetries\nin the environment using group theory. We build on their work and make\nobservations, theoretical and empirical, that lead us to argue that\nSymmetry-Based Disentangled Representation Learning cannot only be based on\nstatic observations: agents should interact with the environment to discover\nits symmetries. Our experiments can be reproduced in Colab and the code is\navailable on GitHub.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 16:21:20 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 09:33:39 GMT"}, {"version": "v3", "created": "Thu, 19 Sep 2019 12:05:02 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Caselles-Dupr\u00e9", "Hugo", ""], ["Garcia-Ortiz", "Michael", ""], ["Filliat", "David", ""]]}, {"id": "1904.00275", "submitter": "Mei-Yun Chen", "authors": "Mei-Yun Chen, Ya-Bo Huang, Sheng-Ping Chang and Ming Ouhyoung", "title": "Prediction Model for Semitransparent Watercolor Pigment Mixtures Using\n  Deep Learning with a Dataset of Transmittance and Reflectance", "comments": "26 pages and 25 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Learning color mixing is difficult for novice painters. In order to support\nnovice painters in learning color mixing, we propose a prediction model for\nsemitransparent pigment mixtures and use its prediction results to create a\nSmart Palette system. Such a system is constructed by first building a\nwatercolor dataset with two types of color mixing data, indicated by\ntransmittance and reflectance: incrementation of the same primary pigment and a\nmixture of two different pigments. Next, we apply the collected data to a deep\nneural network to train a model for predicting the results of semitransparent\npigment mixtures. Finally, we constructed a Smart Palette that provides\neasily-followable instructions on mixing a target color with two primary\npigments in real life: when users pick a pixel, an RGB color, from an image,\nthe system returns its mixing recipe which indicates the two primary pigments\nbeing used and their quantities.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 19:27:33 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Chen", "Mei-Yun", ""], ["Huang", "Ya-Bo", ""], ["Chang", "Sheng-Ping", ""], ["Ouhyoung", "Ming", ""]]}, {"id": "1904.00284", "submitter": "Chieh Hubert Lin", "authors": "Chieh Hubert Lin, Chia-Che Chang, Yu-Sheng Chen, Da-Cheng Juan, Wei\n  Wei, Hwann-Tzong Chen", "title": "COCO-GAN: Generation by Parts via Conditional Coordinating", "comments": "Accepted to ICCV'19 (oral). All images are compressed due to size\n  limit, please access the full-resolution version via Google Drive:\n  http://bit.ly/COCO-GAN-full", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can only interact with part of the surrounding environment due to\nbiological restrictions. Therefore, we learn to reason the spatial\nrelationships across a series of observations to piece together the surrounding\nenvironment. Inspired by such behavior and the fact that machines also have\ncomputational constraints, we propose \\underline{CO}nditional\n\\underline{CO}ordinate GAN (COCO-GAN) of which the generator generates images\nby parts based on their spatial coordinates as the condition. On the other\nhand, the discriminator learns to justify realism across multiple assembled\npatches by global coherence, local appearance, and edge-crossing continuity.\nDespite the full images are never generated during training, we show that\nCOCO-GAN can produce \\textbf{state-of-the-art-quality} full images during\ninference. We further demonstrate a variety of novel applications enabled by\nteaching the network to be aware of coordinates. First, we perform\nextrapolation to the learned coordinate manifold and generate off-the-boundary\npatches. Combining with the originally generated full image, COCO-GAN can\nproduce images that are larger than training samples, which we called\n\"beyond-boundary generation\". We then showcase panorama generation within a\ncylindrical coordinate system that inherently preserves horizontally cyclic\ntopology. On the computation side, COCO-GAN has a built-in divide-and-conquer\nparadigm that reduces memory requisition during training and inference,\nprovides high-parallelism, and can generate parts of images on-demand.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 20:37:24 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 15:55:44 GMT"}, {"version": "v3", "created": "Mon, 5 Aug 2019 05:58:45 GMT"}, {"version": "v4", "created": "Sun, 5 Jan 2020 06:28:59 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Lin", "Chieh Hubert", ""], ["Chang", "Chia-Che", ""], ["Chen", "Yu-Sheng", ""], ["Juan", "Da-Cheng", ""], ["Wei", "Wei", ""], ["Chen", "Hwann-Tzong", ""]]}, {"id": "1904.00314", "submitter": "Seokho Kang", "authors": "Elman Mansimov, Omar Mahmood, Seokho Kang, Kyunghyun Cho", "title": "Molecular geometry prediction using a deep generative graph neural\n  network", "comments": "15 pages, 6 figures", "journal-ref": "Scientific Reports 9: 20381, 2019", "doi": "10.1038/s41598-019-56773-5", "report-no": null, "categories": "cs.LG physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A molecule's geometry, also known as conformation, is one of a molecule's\nmost important properties, determining the reactions it participates in, the\nbonds it forms, and the interactions it has with other molecules. Conventional\nconformation generation methods minimize hand-designed molecular force field\nenergy functions that are often not well correlated with the true energy\nfunction of a molecule observed in nature. They generate geometrically diverse\nsets of conformations, some of which are very similar to the lowest-energy\nconformations and others of which are very different. In this paper, we propose\na conditional deep generative graph neural network that learns an energy\nfunction by directly learning to generate molecular conformations that are\nenergetically favorable and more likely to be observed experimentally in\ndata-driven manner. On three large-scale datasets containing small molecules,\nwe show that our method generates a set of conformations that on average is far\nmore likely to be close to the corresponding reference conformations than are\nthose obtained from conventional force field methods. Our method maintains\ngeometrical diversity by generating conformations that are not too similar to\neach other, and is also computationally faster. We also show that our method\ncan be used to provide initial coordinates for conventional force field\nmethods. On one of the evaluated datasets we show that this combination allows\nus to combine the best of both methods, yielding generated conformations that\nare on average close to reference conformations with some very similar to\nreference conformations.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 01:06:22 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 00:53:28 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Mansimov", "Elman", ""], ["Mahmood", "Omar", ""], ["Kang", "Seokho", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1904.00326", "submitter": "Chengsheng Mao", "authors": "Chengsheng Mao, Liang Yao, Yuan Luo", "title": "MedGCN: Graph Convolutional Networks for Multiple Medical Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Laboratory testing and medication prescription are two of the most important\nroutines in daily clinical practice. Developing an artificial intelligence\nsystem that can automatically make lab test imputations and medication\nrecommendations can save cost on potentially redundant lab tests and inform\nphysicians in more effective prescription. We present an intelligent model that\ncan automatically recommend the patients' medications based on their incomplete\nlab tests, and can even accurately estimate the lab values that have not been\ntaken. We model the complex relations between multiple types of medical\nentities with their inherent features in a heterogeneous graph. Then we learn a\ndistributed representation for each entity in the graph based on graph\nconvolutional networks to make the representations integrate information from\nmultiple types of entities. Since the entity representations incorporate\nmultiple types of medical information, they can be used for multiple medical\ntasks. In our experiments, we construct a graph to associate patients,\nencounters, lab tests and medications, and conduct the two tasks: medication\nrecommendation and lab test imputation. The experimental results demonstrate\nthat our model can outperform the state-of-the-art models in both tasks.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 02:48:50 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Mao", "Chengsheng", ""], ["Yao", "Liang", ""], ["Luo", "Yuan", ""]]}, {"id": "1904.00350", "submitter": "Sungjoon Park", "authors": "Sungjoon Park, Donghyun Kim, Alice Oh", "title": "Conversation Model Fine-Tuning for Classifying Client Utterances in\n  Counseling Dialogues", "comments": "9 pages, 2 figures, NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent surge of text-based online counseling applications enables us to\ncollect and analyze interactions between counselors and clients. A dataset of\nthose interactions can be used to learn to automatically classify the client\nutterances into categories that help counselors in diagnosing client status and\npredicting counseling outcome. With proper anonymization, we collect\ncounselor-client dialogues, define meaningful categories of client utterances\nwith professional counselors, and develop a novel neural network model for\nclassifying the client utterances. The central idea of our model, ConvMFiT, is\na pre-trained conversation model which consists of a general language model\nbuilt from an out-of-domain corpus and two role-specific language models built\nfrom unlabeled in-domain dialogues. The classification result shows that\nConvMFiT outperforms state-of-the-art comparison models. Further, the attention\nweights in the learned model confirm that the model finds expected linguistic\npatterns for each category.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 07:30:47 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Park", "Sungjoon", ""], ["Kim", "Donghyun", ""], ["Oh", "Alice", ""]]}, {"id": "1904.00370", "submitter": "Sayna Ebrahimi", "authors": "Samarth Sinha, Sayna Ebrahimi, Trevor Darrell", "title": "Variational Adversarial Active Learning", "comments": "First two authors contributed equally, listed alphabetically.\n  Accepted as Oral at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning aims to develop label-efficient algorithms by sampling the\nmost representative queries to be labeled by an oracle. We describe a\npool-based semi-supervised active learning algorithm that implicitly learns\nthis sampling mechanism in an adversarial manner. Unlike conventional active\nlearning algorithms, our approach is task agnostic, i.e., it does not depend on\nthe performance of the task for which we are trying to acquire labeled data.\nOur method learns a latent space using a variational autoencoder (VAE) and an\nadversarial network trained to discriminate between unlabeled and labeled data.\nThe mini-max game between the VAE and the adversarial network is played such\nthat while the VAE tries to trick the adversarial network into predicting that\nall data points are from the labeled pool, the adversarial network learns how\nto discriminate between dissimilarities in the latent space. We extensively\nevaluate our method on various image classification and semantic segmentation\nbenchmark datasets and establish a new state of the art on\n$\\text{CIFAR10/100}$, $\\text{Caltech-256}$, $\\text{ImageNet}$,\n$\\text{Cityscapes}$, and $\\text{BDD100K}$. Our results demonstrate that our\nadversarial approach learns an effective low dimensional latent space in\nlarge-scale settings and provides for a computationally efficient sampling\nmethod. Our code is available at https://github.com/sinhasam/vaal.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 09:54:17 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 18:48:22 GMT"}, {"version": "v3", "created": "Mon, 28 Oct 2019 18:03:08 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Sinha", "Samarth", ""], ["Ebrahimi", "Sayna", ""], ["Darrell", "Trevor", ""]]}, {"id": "1904.00374", "submitter": "Ben Day", "authors": "Enxhell Luzhnica, Ben Day and Pietro Lio'", "title": "Clique pooling for graph classification", "comments": "Under review as a workshop paper at RLGM 2019 @ ICML", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel graph pooling operation using cliques as the unit pool. As\nthis approach is purely topological, rather than featural, it is more readily\ninterpretable, a better analogue to image coarsening than filtering or pruning\ntechniques, and entirely nonparametric. The operation is implemented within\ngraph convolution network (GCN) and GraphSAGE architectures and tested against\nstandard graph classification benchmarks. In addition, we explore the backwards\ncompatibility of the pooling to regular graphs, demonstrating competitive\nperformance when replacing two-by-two pooling in standard convolutional neural\nnetworks (CNNs) with our mechanism.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 10:17:50 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 13:19:11 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Luzhnica", "Enxhell", ""], ["Day", "Ben", ""], ["Lio'", "Pietro", ""]]}, {"id": "1904.00377", "submitter": "Mones Raslan", "authors": "Gitta Kutyniok, Philipp Petersen, Mones Raslan, Reinhold Schneider", "title": "A Theoretical Analysis of Deep Neural Networks and Parametric PDEs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA math.FA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive upper bounds on the complexity of ReLU neural networks\napproximating the solution maps of parametric partial differential equations.\nIn particular, without any knowledge of its concrete shape, we use the inherent\nlow-dimensionality of the solution manifold to obtain approximation rates which\nare significantly superior to those provided by classical neural network\napproximation results. Concretely, we use the existence of a small reduced\nbasis to construct, for a large variety of parametric partial differential\nequations, neural networks that yield approximations of the parametric solution\nmaps in such a way that the sizes of these networks essentially only depend on\nthe size of the reduced basis.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 10:51:16 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 12:14:13 GMT"}, {"version": "v3", "created": "Thu, 14 May 2020 12:34:55 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Kutyniok", "Gitta", ""], ["Petersen", "Philipp", ""], ["Raslan", "Mones", ""], ["Schneider", "Reinhold", ""]]}, {"id": "1904.00435", "submitter": "Huyan Huang", "authors": "Huyan Huang, Yipeng Liu, Ce Zhu", "title": "Robust Low-Rank Tensor Ring Completion", "comments": null, "journal-ref": null, "doi": "10.1109/TCI.2020.3006718", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank tensor completion recovers missing entries based on different tensor\ndecompositions. Due to its outstanding performance in exploiting some\nhigher-order data structure, low rank tensor ring has been applied in tensor\ncompletion. To further deal with its sensitivity to sparse component as it does\nin tensor principle component analysis, we propose robust tensor ring\ncompletion (RTRC), which separates latent low-rank tensor component from sparse\ncomponent with limited number of measurements. The low rank tensor component is\nconstrained by the weighted sum of nuclear norms of its balanced unfoldings,\nwhile the sparse component is regularized by its l1 norm. We analyze the RTRC\nmodel and gives the exact recovery guarantee. The alternating direction method\nof multipliers is used to divide the problem into several sub-problems with\nfast solutions. In numerical experiments, we verify the recovery condition of\nthe proposed method on synthetic data, and show the proposed method outperforms\nthe state-of-the-art ones in terms of both accuracy and computational\ncomplexity in a number of real-world data based tasks, i.e., light-field image\nrecovery, shadow removal in face images, and background extraction in color\nvideo.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 15:33:13 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 01:49:21 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2020 08:25:00 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Huang", "Huyan", ""], ["Liu", "Yipeng", ""], ["Zhu", "Ce", ""]]}, {"id": "1904.00438", "submitter": "George Adam", "authors": "George Adam, Jonathan Lorraine", "title": "Understanding Neural Architecture Search Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic methods for generating state-of-the-art neural network\narchitectures without human experts have generated significant attention\nrecently. This is because of the potential to remove human experts from the\ndesign loop which can reduce costs and decrease time to model deployment.\nNeural architecture search (NAS) techniques have improved significantly in\ntheir computational efficiency since the original NAS was proposed. This\nreduction in computation is enabled via weight sharing such as in Efficient\nNeural Architecture Search (ENAS). However, recently a body of work confirms\nour discovery that ENAS does not do significantly better than random search\nwith weight sharing, contradicting the initial claims of the authors. We\nprovide an explanation for this phenomenon by investigating the\ninterpretability of the ENAS controller's hidden state. We find models sampled\nfrom identical controller hidden states have no correlation with various graph\nsimilarity metrics, so no notion of structural similarity is learned. This\nfailure mode implies the RNN controller does not condition on past architecture\nchoices. Lastly, we propose a solution to this failure mode by forcing the\ncontroller's hidden state to encode pasts decisions by training it with a\nmemory buffer of previously sampled architectures. Doing this improves hidden\nstate interpretability by increasing the correlation between controller hidden\nstates and graph similarity metrics.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 15:48:49 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 17:49:27 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Adam", "George", ""], ["Lorraine", "Jonathan", ""]]}, {"id": "1904.00442", "submitter": "Diogo Pernes", "authors": "Diogo Pernes and Jaime S. Cardoso", "title": "SpaMHMM: Sparse Mixture of Hidden Markov Models for Graph Connected\n  Entities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework to model the distribution of sequential data coming\nfrom a set of entities connected in a graph with a known topology. The method\nis based on a mixture of shared hidden Markov models (HMMs), which are jointly\ntrained in order to exploit the knowledge of the graph structure and in such a\nway that the obtained mixtures tend to be sparse. Experiments in different\napplication domains demonstrate the effectiveness and versatility of the\nmethod.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 16:18:56 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Pernes", "Diogo", ""], ["Cardoso", "Jaime S.", ""]]}, {"id": "1904.00445", "submitter": "Nicholas Heller", "authors": "Nicholas Heller, Niranjan Sathianathen, Arveen Kalapara, Edward\n  Walczak, Keenan Moore, Heather Kaluzniak, Joel Rosenberg, Paul Blake, Zachary\n  Rengel, Makinna Oestreich, Joshua Dean, Michael Tradewell, Aneri Shah, Resha\n  Tejpaul, Zachary Edgerton, Matthew Peterson, Shaneabbas Raza, Subodh Regmi,\n  Nikolaos Papanikolopoulos, and Christopher Weight", "title": "The KiTS19 Challenge Data: 300 Kidney Tumor Cases with Clinical Context,\n  CT Semantic Segmentations, and Surgical Outcomes", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The morphometry of a kidney tumor revealed by contrast-enhanced Computed\nTomography (CT) imaging is an important factor in clinical decision making\nsurrounding the lesion's diagnosis and treatment. Quantitative study of the\nrelationship between kidney tumor morphology and clinical outcomes is difficult\ndue to data scarcity and the laborious nature of manually quantifying imaging\npredictors. Automatic semantic segmentation of kidneys and kidney tumors is a\npromising tool towards automatically quantifying a wide array of morphometric\nfeatures, but no sizeable annotated dataset is currently available to train\nmodels for this task. We present the KiTS19 challenge dataset: A collection of\nmulti-phase CT imaging, segmentation masks, and comprehensive clinical outcomes\nfor 300 patients who underwent nephrectomy for kidney tumors at our center\nbetween 2010 and 2018. 210 (70%) of these patients were selected at random as\nthe training set for the 2019 MICCAI KiTS Kidney Tumor Segmentation Challenge\nand have been released publicly. With the presence of clinical context and\nsurgical outcomes, this data can serve not only for benchmarking semantic\nsegmentation models, but also for developing and studying biomarkers which make\nuse of the imaging and semantic segmentation masks.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 16:56:10 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2020 14:06:45 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Heller", "Nicholas", ""], ["Sathianathen", "Niranjan", ""], ["Kalapara", "Arveen", ""], ["Walczak", "Edward", ""], ["Moore", "Keenan", ""], ["Kaluzniak", "Heather", ""], ["Rosenberg", "Joel", ""], ["Blake", "Paul", ""], ["Rengel", "Zachary", ""], ["Oestreich", "Makinna", ""], ["Dean", "Joshua", ""], ["Tradewell", "Michael", ""], ["Shah", "Aneri", ""], ["Tejpaul", "Resha", ""], ["Edgerton", "Zachary", ""], ["Peterson", "Matthew", ""], ["Raza", "Shaneabbas", ""], ["Regmi", "Subodh", ""], ["Papanikolopoulos", "Nikolaos", ""], ["Weight", "Christopher", ""]]}, {"id": "1904.00469", "submitter": "Luca Ambrogioni", "authors": "Luca Ambrogioni and Marcel A. J. van Gerven", "title": "Perturbative estimation of stochastic gradients", "comments": "Needs improvements, the experiments are too limited", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a family of stochastic gradient estimation\ntechniques based of the perturbative expansion around the mean of the sampling\ndistribution. We characterize the bias and variance of the resulting\nTaylor-corrected estimators using the Lagrange error formula. Furthermore, we\nintroduce a family of variance reduction techniques that can be applied to\nother gradient estimators. Finally, we show that these new perturbative methods\ncan be extended to discrete functions using analytic continuation. Using this\ntechnique, we derive a new gradient descent method for training stochastic\nnetworks with binary weights. In our experiments, we show that the perturbative\ncorrection improves the convergence of stochastic variational inference both in\nthe continuous and in the discrete case.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 20:00:50 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 09:09:48 GMT"}, {"version": "v3", "created": "Mon, 8 Apr 2019 11:30:54 GMT"}, {"version": "v4", "created": "Fri, 15 Nov 2019 08:35:16 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Ambrogioni", "Luca", ""], ["van Gerven", "Marcel A. J.", ""]]}, {"id": "1904.00479", "submitter": "Will Wei Sun", "authors": "Botao Hao, Boxiang Wang, Pengyuan Wang, Jingfei Zhang, Jian Yang, Will\n  Wei Sun", "title": "Sparse Tensor Additive Regression", "comments": "Accepted by Journal of Machine Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensors are becoming prevalent in modern applications such as medical imaging\nand digital marketing. In this paper, we propose a sparse tensor additive\nregression (STAR) that models a scalar response as a flexible nonparametric\nfunction of tensor covariates. The proposed model effectively exploits the\nsparse and low-rank structures in the tensor additive regression. We formulate\nthe parameter estimation as a non-convex optimization problem, and propose an\nefficient penalized alternating minimization algorithm. We establish a\nnon-asymptotic error bound for the estimator obtained from each iteration of\nthe proposed algorithm, which reveals an interplay between the optimization\nerror and the statistical rate of convergence. We demonstrate the efficacy of\nSTAR through extensive comparative simulation studies, and an application to\nthe click-through-rate prediction in online advertising.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 20:45:50 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 13:58:39 GMT"}, {"version": "v3", "created": "Fri, 5 Mar 2021 16:31:58 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Hao", "Botao", ""], ["Wang", "Boxiang", ""], ["Wang", "Pengyuan", ""], ["Zhang", "Jingfei", ""], ["Yang", "Jian", ""], ["Sun", "Will Wei", ""]]}, {"id": "1904.00507", "submitter": "Soumyabrata Pal", "authors": "Arya Mazumdar, Soumyabrata Pal", "title": "Semisupervised Clustering by Queries and Locally Encodable Source Coding", "comments": "16 pages, 11 figures. Some of the results of this paper have appeared\n  in the proceedings of the 2017 Conference on Neural Information Processing\n  Systems (NeurIPS 2017)", "journal-ref": "IEEE Transactions on Information Theory, 2020", "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Source coding is the canonical problem of data compression in information\ntheory. In a locally encodable source coding, each compressed bit depends on\nonly few bits of the input. In this paper, we show that a recently popular\nmodel of semi-supervised clustering is equivalent to locally encodable source\ncoding. In this model, the task is to perform multiclass labeling of unlabeled\nelements. At the beginning, we can ask in parallel a set of simple queries to\nan oracle who provides (possibly erroneous) binary answers to the queries. The\nqueries cannot involve more than two (or a fixed constant number of) elements.\nNow the labeling of all the elements (or clustering) must be performed based on\nthe noisy query answers. The goal is to recover all the correct labelings while\nminimizing the number of such queries. The equivalence to locally encodable\nsource codes leads us to find lower bounds on the number of queries required in\na variety of scenarios. We provide querying schemes based on pairwise `same\ncluster' queries - and pairwise AND queries and show provable performance\nguarantees for each of the schemes.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 23:16:45 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 08:40:23 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Mazumdar", "Arya", ""], ["Pal", "Soumyabrata", ""]]}, {"id": "1904.00516", "submitter": "Subbayya Sastry Pidaparthy", "authors": "Soumyajit Mitra and P S Sastry", "title": "Summarizing Event Sequences with Serial Episodes: A Statistical Model\n  and an Application", "comments": "12 pages. Under review for IEEE TKDE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of discovering a small set of frequent\nserial episodes from sequential data so as to adequately characterize or\nsummarize the data. We discuss an algorithm based on the Minimum Description\nLength (MDL) principle and the algorithm is a slight modification of an earlier\nmethod, called CSC-2. We present a novel generative model for sequence data\ncontaining prominent pairs of serial episodes and, using this, provide some\nstatistical justification for the algorithm. We believe this is the first\ninstance of such a statistical justification for an MDL based algorithm for\nsummarizing event sequence data. We then present a novel application of this\ndata mining algorithm in text classification. By considering text documents as\ntemporal sequences of words, the data mining algorithm can find a set of\ncharacteristic episodes for all the training data as a whole. The words that\nare part of these characteristic episodes could then be considered the only\nrelevant words for the dictionary thus resulting in a considerably reduced\nfeature vector dimension. We show, through simulation experiments using\nbenchmark data sets, that the discovered frequent episodes can be used to\nachieve more than four-fold reduction in dictionary size without losing any\nclassification accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 00:29:15 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Mitra", "Soumyajit", ""], ["Sastry", "P S", ""]]}, {"id": "1904.00542", "submitter": "Ramy Baly", "authors": "Ramy Baly (MIT Computer Science and Artificial Intelligence\n  Laboratory, MA, USA) and Georgi Karadzhov (SiteGround Hosting EOOD, Bulgaria)\n  and Abdelrhman Saleh (Harvard University, MA, USA) and James Glass (MIT\n  Computer Science and Artificial Intelligence Laboratory, MA, USA) and Preslav\n  Nakov (Qatar Computing Research Institute, HBKU, Qatar)", "title": "Multi-Task Ordinal Regression for Jointly Predicting the Trustworthiness\n  and the Leading Political Ideology of News Media", "comments": "Fact-checking, political ideology, news media, NAACL-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of fake news, bias, and propaganda, we study two important but\nrelatively under-explored problems: (i) trustworthiness estimation (on a\n3-point scale) and (ii) political ideology detection (left/right bias on a\n7-point scale) of entire news outlets, as opposed to evaluating individual\narticles. In particular, we propose a multi-task ordinal regression framework\nthat models the two problems jointly. This is motivated by the observation that\nhyper-partisanship is often linked to low trustworthiness, e.g., appealing to\nemotions rather than sticking to the facts, while center media tend to be\ngenerally more impartial and trustworthy. We further use several auxiliary\ntasks, modeling centrality, hyperpartisanship, as well as left-vs.-right bias\non a coarse-grained scale. The evaluation results show sizable performance\ngains by the joint models over models that target the problems in isolation.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 02:54:54 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Baly", "Ramy", "", "MIT Computer Science and Artificial Intelligence\n  Laboratory, MA, USA"], ["Karadzhov", "Georgi", "", "SiteGround Hosting EOOD, Bulgaria"], ["Saleh", "Abdelrhman", "", "Harvard University, MA, USA"], ["Glass", "James", "", "MIT\n  Computer Science and Artificial Intelligence Laboratory, MA, USA"], ["Nakov", "Preslav", "", "Qatar Computing Research Institute, HBKU, Qatar"]]}, {"id": "1904.00548", "submitter": "Yaniv Shulman", "authors": "Yaniv Shulman", "title": "Unsupervised Contextual Anomaly Detection using Joint Deep Variational\n  Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for unsupervised contextual anomaly detection is proposed using a\ncross-linked pair of Variational Auto-Encoders for assigning a normality score\nto an observation. The method enables a distinct separation of contextual from\nbehavioral attributes and is robust to the presence of anomalous or novel\ncontextual attributes. The method can be trained with data sets that contain\nanomalies without any special pre-processing.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 03:39:01 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Shulman", "Yaniv", ""]]}, {"id": "1904.00561", "submitter": "Matthew Britton", "authors": "Matthew Britton", "title": "VINE: Visualizing Statistical Interactions in Black Box Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  As machine learning becomes more pervasive, there is an urgent need for\ninterpretable explanations of predictive models. Prior work has developed\neffective methods for visualizing global model behavior, as well as generating\nlocal (instance-specific) explanations. However, relatively little work has\naddressed regional explanations - how groups of similar instances behave in a\ncomplex model, and the related issue of visualizing statistical feature\ninteractions. The lack of utilities available for these analytical needs\nhinders the development of models that are mission-critical, transparent, and\nalign with social goals. We present VINE (Visual INteraction Effects), a novel\nalgorithm to extract and visualize statistical interaction effects in black box\nmodels. We also present a novel evaluation metric for visualizations in the\ninterpretable ML space.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 04:42:07 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Britton", "Matthew", ""]]}, {"id": "1904.00562", "submitter": "Wanli Wang", "authors": "Jinguang Sun, Wanli Wang, Xian Wei, Li Fang, Xiaoliang Tang, Yusheng\n  Xu, Hui Yu and Wei Yao", "title": "Deep Clustering With Intra-class Distance Constraint for Hyperspectral\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high dimensionality of hyperspectral images often results in the\ndegradation of clustering performance. Due to the powerful ability of deep\nfeature extraction and non-linear feature representation, the clustering\nalgorithm based on deep learning has become a hot research topic in the field\nof hyperspectral remote sensing. However, most deep clustering algorithms for\nhyperspectral images utilize deep neural networks as feature extractor without\nconsidering prior knowledge constraints that are suitable for clustering. To\nsolve this problem, we propose an intra-class distance constrained deep\nclustering algorithm for high-dimensional hyperspectral images. The proposed\nalgorithm constrains the feature mapping procedure of the auto-encoder network\nby intra-class distance so that raw images are transformed from the original\nhigh-dimensional space to the low-dimensional feature space that is more\nconducive to clustering. Furthermore, the related learning process is treated\nas a joint optimization problem of deep feature extraction and clustering.\nExperimental results demonstrate the intense competitiveness of the proposed\nalgorithm in comparison with state-of-the-art clustering methods of\nhyperspectral images.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 04:42:18 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Sun", "Jinguang", ""], ["Wang", "Wanli", ""], ["Wei", "Xian", ""], ["Fang", "Li", ""], ["Tang", "Xiaoliang", ""], ["Xu", "Yusheng", ""], ["Yu", "Hui", ""], ["Yao", "Wei", ""]]}, {"id": "1904.00575", "submitter": "Cheng Cheng", "authors": "Wenqian Jiang, Cheng Cheng, Beitong Zhou, Guijun Ma and Ye Yuan", "title": "A Novel GAN-based Fault Diagnosis Approach for Imbalanced Industrial\n  Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel fault diagnosis approach based on generative\nadversarial networks (GAN) for imbalanced industrial time series where normal\nsamples are much larger than failure cases. We combine a well-designed feature\nextractor with GAN to help train the whole network. Aimed at obtaining data\ndistribution and hidden pattern in both original distinguishing features and\nlatent space, the encoder-decoder-encoder three-sub-network is employed in GAN,\nbased on Deep Convolution Generative Adversarial Networks (DCGAN) but without\nTanh activation layer and only trained on normal samples. In order to verify\nthe validity and feasibility of our approach, we test it on rolling bearing\ndata from Case Western Reserve University and further verify it on data\ncollected from our laboratory. The results show that our proposed approach can\nachieve excellent performance in detecting faulty by outputting much larger\nevaluation scores.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 06:11:44 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Jiang", "Wenqian", ""], ["Cheng", "Cheng", ""], ["Zhou", "Beitong", ""], ["Ma", "Guijun", ""], ["Yuan", "Ye", ""]]}, {"id": "1904.00577", "submitter": "Weilin Zhou", "authors": "Weilin Zhou, Frederic Precioso", "title": "Adaptive Bayesian Linear Regression for Automated Machine Learning", "comments": "Added references;Corrected typos.Revised argument,results unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To solve a machine learning problem, one typically needs to perform data\npreprocessing, modeling, and hyperparameter tuning, which is known as model\nselection and hyperparameter optimization.The goal of automated machine\nlearning (AutoML) is to design methods that can automatically perform model\nselection and hyperparameter optimization without human interventions for a\ngiven dataset. In this paper, we propose a meta-learning method that can search\nfor a high-performance machine learning pipeline from the predefined set of\ncandidate pipelines for supervised classification datasets in an efficient way\nby leveraging meta-data collected from previous experiments. More specifically,\nour method combines an adaptive Bayesian regression model with a neural network\nbasis function and the acquisition function from Bayesian optimization. The\nadaptive Bayesian regression model is able to capture knowledge from previous\nmeta-data and thus make predictions of the performances of machine learning\npipelines on a new dataset. The acquisition function is then used to guide the\nsearch of possible pipelines based on the predictions.The experiments\ndemonstrate that our approach can quickly identify high-performance pipelines\nfor a range of test datasets and outperforms the baseline methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 06:21:31 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 03:47:00 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Zhou", "Weilin", ""], ["Precioso", "Frederic", ""]]}, {"id": "1904.00583", "submitter": "Xiaolie Li", "authors": "Jing Chi, Xiaolei Li, Haozhong Wang, Dazhi Gao, Peter Gerstoft", "title": "Sound source ranging using a feed-forward neural network with\n  fitting-based early stopping", "comments": null, "journal-ref": null, "doi": "10.1121/1.5126115", "report-no": null, "categories": "cs.LG cs.SD eess.SP physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a feed-forward neural network (FNN) is trained for source ranging in an\nocean waveguide, it is difficult evaluating the range accuracy of the FNN on\nunlabeled test data. A fitting-based early stopping (FEAST) method is\nintroduced to evaluate the range error of the FNN on test data where the\ndistance of source is unknown. Based on FEAST, when the evaluated range error\nof the FNN reaches the minimum on test data, stopping training, which will help\nto improve the ranging accuracy of the FNN on the test data. The FEAST is\ndemonstrated on simulated and experimental data.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 06:36:33 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Chi", "Jing", ""], ["Li", "Xiaolei", ""], ["Wang", "Haozhong", ""], ["Gao", "Dazhi", ""], ["Gerstoft", "Peter", ""]]}, {"id": "1904.00601", "submitter": "Mohit Sharma", "authors": "Mohit K.Sharma, Alessio Zappone, Mohamad Assaad, Merouane Debbah,\n  Spyridon Vassilaras", "title": "Distributed Power Control for Large Energy Harvesting Networks: A\n  Multi-Agent Deep Reinforcement Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a multi-agent reinforcement learning (MARL)\nframework to obtain online power control policies for a large energy harvesting\n(EH) multiple access channel, when only causal information about the EH process\nand wireless channel is available. In the proposed framework, we model the\nonline power control problem as a discrete-time mean-field game (MFG), and\nanalytically show that the MFG has a unique stationary solution. Next, we\nleverage the fictitious play property of the mean-field games, and the deep\nreinforcement learning technique to learn the stationary solution of the game,\nin a completely distributed fashion. We analytically show that the proposed\nprocedure converges to the unique stationary solution of the MFG. This, in\nturn, ensures that the optimal policies can be learned in a completely\ndistributed fashion. In order to benchmark the performance of the distributed\npolicies, we also develop a deep neural network (DNN) based centralized as well\nas distributed online power control schemes. Our simulation results show the\nefficacy of the proposed power control policies. In particular, the DNN based\ncentralized power control policies provide a very good performance for large EH\nnetworks for which the design of optimal policies is intractable using the\nconventional methods such as Markov decision processes. Further, performance of\nboth the distributed policies is close to the throughput achieved by the\ncentralized policies.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 07:16:51 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 08:00:54 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Sharma", "Mohit K.", ""], ["Zappone", "Alessio", ""], ["Assaad", "Mohamad", ""], ["Debbah", "Merouane", ""], ["Vassilaras", "Spyridon", ""]]}, {"id": "1904.00655", "submitter": "Pankaj Malhotra", "authors": "Priyanka Gupta, Pankaj Malhotra, Jyoti Narwariya, Lovekesh Vig, Gautam\n  Shroff", "title": "Transfer Learning for Clinical Time Series Analysis using Deep Neural\n  Networks", "comments": "Updated version of this work appeared in Journal of Healthcare\n  Informatics Research, Vol. 4, 2020. arXiv admin note: text overlap with\n  arXiv:1807.01705", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have shown promising results for various clinical\nprediction tasks. However, training deep networks such as those based on\nRecurrent Neural Networks (RNNs) requires large labeled data, significant\nhyper-parameter tuning effort and expertise, and high computational resources.\nIn this work, we investigate as to what extent can transfer learning address\nthese issues when using deep RNNs to model multivariate clinical time series.\nWe consider two scenarios for transfer learning using RNNs: i)\ndomain-adaptation, i.e., leveraging a deep RNN - namely, TimeNet - pre-trained\nfor feature extraction on time series from diverse domains, and adapting it for\nfeature extraction and subsequent target tasks in healthcare domain, ii)\ntask-adaptation, i.e., pre-training a deep RNN - namely, HealthNet - on diverse\ntasks in healthcare domain, and adapting it to new target tasks in the same\ndomain. We evaluate the above approaches on publicly available MIMIC-III\nbenchmark dataset, and demonstrate that (a) computationally-efficient linear\nmodels trained using features extracted via pre-trained RNNs outperform or, in\nthe worst case, perform as well as deep RNNs and statistical hand-crafted\nfeatures based models trained specifically for target task; (b) models obtained\nby adapting pre-trained models for target tasks are significantly more robust\nto the size of labeled data compared to task-specific RNNs, while also being\ncomputationally efficient. We, therefore, conclude that pre-trained deep models\nlike TimeNet and HealthNet allow leveraging the advantages of deep learning for\nclinical time series analysis tasks, while also minimize dependence on\nhand-crafted features, deal robustly with scarce labeled training data\nscenarios without overfitting, as well as reduce dependence on expertise and\nresources required to train deep networks from scratch.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 09:31:34 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 13:11:04 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Gupta", "Priyanka", ""], ["Malhotra", "Pankaj", ""], ["Narwariya", "Jyoti", ""], ["Vig", "Lovekesh", ""], ["Shroff", "Gautam", ""]]}, {"id": "1904.00670", "submitter": "Borislav Ikonomov", "authors": "Borislav Ikonomov, Michael U. Gutmann", "title": "Robust Optimisation Monte Carlo", "comments": "8 pages + 6 page appendix; v2: made clarifications, added a second\n  possible algorithm implementation and its results; v3: small clarifications,\n  to be published in AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is on Bayesian inference for parametric statistical models that\nare defined by a stochastic simulator which specifies how data is generated.\nExact sampling is then possible but evaluating the likelihood function is\ntypically prohibitively expensive. Approximate Bayesian Computation (ABC) is a\nframework to perform approximate inference in such situations. While basic ABC\nalgorithms are widely applicable, they are notoriously slow and much research\nhas focused on increasing their efficiency. Optimisation Monte Carlo (OMC) has\nrecently been proposed as an efficient and embarrassingly parallel method that\nleverages optimisation to accelerate the inference. In this paper, we\ndemonstrate an important previously unrecognised failure mode of OMC: It\ngenerates strongly overconfident approximations by collapsing regions of\nsimilar or near-constant likelihood into a single point. We propose an\nefficient, robust generalisation of OMC that corrects this. It makes fewer\nassumptions, retains the main benefits of OMC, and can be performed either as\npost-processing to OMC or as a stand-alone computation. We demonstrate the\neffectiveness of the proposed Robust OMC on toy examples and tasks in\ninverse-graphics where we perform Bayesian inference with a complex image\nrenderer.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 09:50:41 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 15:54:56 GMT"}, {"version": "v3", "created": "Fri, 28 Feb 2020 13:45:56 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Ikonomov", "Borislav", ""], ["Gutmann", "Michael U.", ""]]}, {"id": "1904.00687", "submitter": "Gilad Yehudai", "authors": "Gilad Yehudai and Ohad Shamir", "title": "On the Power and Limitations of Random Features for Understanding Neural\n  Networks", "comments": "Comparison to previous version: Fixed a bug in the proof of Theorem\n  4.1. Changed the polynomial dependency of ||w^*|| in Theorem 4.1 from d^2 to\n  d^3 and of |b^*| from O(d^3) to O(d^4)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a spate of papers have provided positive theoretical results for\ntraining over-parameterized neural networks (where the network size is larger\nthan what is needed to achieve low error). The key insight is that with\nsufficient over-parameterization, gradient-based methods will implicitly leave\nsome components of the network relatively unchanged, so the optimization\ndynamics will behave as if those components are essentially fixed at their\ninitial random values. In fact, fixing these explicitly leads to the well-known\napproach of learning with random features. In other words, these techniques\nimply that we can successfully learn with neural networks, whenever we can\nsuccessfully learn with random features. In this paper, we first review these\ntechniques, providing a simple and self-contained analysis for one-hidden-layer\nnetworks. We then argue that despite the impressive positive results, random\nfeature approaches are also inherently limited in what they can explain. In\nparticular, we rigorously show that random features cannot be used to learn\neven a single ReLU neuron with standard Gaussian inputs, unless the network\nsize (or magnitude of the weights) is exponentially large. Since a single\nneuron is learnable with gradient-based methods, we conclude that we are still\nfar from a satisfying general explanation for the empirical success of neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 10:21:24 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 16:40:26 GMT"}, {"version": "v3", "created": "Wed, 4 Mar 2020 08:13:41 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Yehudai", "Gilad", ""], ["Shamir", "Ohad", ""]]}, {"id": "1904.00689", "submitter": "Olga Taran", "authors": "Olga Taran, Shideh Rezaeifar, Taras Holotyak, Slava Voloshynovskiy", "title": "Defending against adversarial attacks by randomized diversification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vulnerability of machine learning systems to adversarial attacks\nquestions their usage in many applications. In this paper, we propose a\nrandomized diversification as a defense strategy. We introduce a multi-channel\narchitecture in a gray-box scenario, which assumes that the architecture of the\nclassifier and the training data set are known to the attacker. The attacker\ndoes not only have access to a secret key and to the internal states of the\nsystem at the test time. The defender processes an input in multiple channels.\nEach channel introduces its own randomization in a special transform domain\nbased on a secret key shared between the training and testing stages. Such a\ntransform based randomization with a shared key preserves the gradients in\nkey-defined sub-spaces for the defender but it prevents gradient back\npropagation and the creation of various bypass systems for the attacker. An\nadditional benefit of multi-channel randomization is the aggregation that fuses\nsoft-outputs from all channels, thus increasing the reliability of the final\nscore. The sharing of a secret key creates an information advantage to the\ndefender. Experimental evaluation demonstrates an increased robustness of the\nproposed method to a number of known state-of-the-art attacks.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 10:27:33 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Taran", "Olga", ""], ["Rezaeifar", "Shideh", ""], ["Holotyak", "Taras", ""], ["Voloshynovskiy", "Slava", ""]]}, {"id": "1904.00690", "submitter": "Abdelrahim Ahmad", "authors": "Abdelrahim Kasem Ahmad, Assef Jafar and Kadan Aljoumaa", "title": "Customer churn prediction in telecom using machine learning and social\n  network analysis in big data platform", "comments": "24 pages, 14 figures. PDF https://rdcu.be/budKg", "journal-ref": "Journal of Big Data 2019 6:28", "doi": "10.1186/s40537-019-0191-6", "report-no": null, "categories": "cs.CY cs.DC cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Customer churn is a major problem and one of the most important concerns for\nlarge companies. Due to the direct effect on the revenues of the companies,\nespecially in the telecom field, companies are seeking to develop means to\npredict potential customer to churn. Therefore, finding factors that increase\ncustomer churn is important to take necessary actions to reduce this churn. The\nmain contribution of our work is to develop a churn prediction model which\nassists telecom operators to predict customers who are most likely subject to\nchurn. The model developed in this work uses machine learning techniques on big\ndata platform and builds a new way of features' engineering and selection. In\norder to measure the performance of the model, the Area Under Curve (AUC)\nstandard measure is adopted, and the AUC value obtained is 93.3%. Another main\ncontribution is to use customer social network in the prediction model by\nextracting Social Network Analysis (SNA) features. The use of SNA enhanced the\nperformance of the model from 84 to 93.3% against AUC standard. The model was\nprepared and tested through Spark environment by working on a large dataset\ncreated by transforming big raw data provided by SyriaTel telecom company. The\ndataset contained all customers' information over 9 months, and was used to\ntrain, test, and evaluate the system at SyriaTel. The model experimented four\nalgorithms: Decision Tree, Random Forest, Gradient Boosted Machine Tree \"GBM\"\nand Extreme Gradient Boosting \"XGBOOST\". However, the best results were\nobtained by applying XGBOOST algorithm. This algorithm was used for\nclassification in this churn predictive model.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 10:30:04 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Ahmad", "Abdelrahim Kasem", ""], ["Jafar", "Assef", ""], ["Aljoumaa", "Kadan", ""]]}, {"id": "1904.00735", "submitter": "Mark Stamp", "authors": "Neeraj Chavan, Fabio Di Troia, Mark Stamp", "title": "A Comparative Analysis of Android Malware", "comments": "3rd International Workshop on Formal Methods for Security Engineering\n  (ForSE 2019), in conjunction with the 5th International Conference on\n  Information Systems Security and Privacy (ICISSP 2019), Prague, Czech\n  Republic, February 23-25, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a comparative analysis of benign and malicious\nAndroid applications, based on static features. In particular, we focus our\nattention on the permissions requested by an application. We consider both\nbinary classification of malware versus benign, as well as the multiclass\nproblem, where we classify malware samples into their respective families. Our\nexperiments are based on substantial malware datasets and we employ a wide\nvariety of machine learning techniques, including decision trees and random\nforests, support vector machines, logistic model trees, AdaBoost, and\nartificial neural networks. We find that permissions are a strong feature and\nthat by careful feature engineering, we can significantly reduce the number of\nfeatures needed for highly accurate detection and classification.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 02:05:55 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Chavan", "Neeraj", ""], ["Di Troia", "Fabio", ""], ["Stamp", "Mark", ""]]}, {"id": "1904.00737", "submitter": "Onn Shehory", "authors": "Eitan Farchi, Onn Shehory, Guy Barash", "title": "Defending via strategic ML selection", "comments": "EDSMLS 2019 @ AAAI workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The results of a learning process depend on the input data. There are cases\nin which an adversary can strategically tamper with the input data to affect\nthe outcome of the learning process. While some datasets are difficult to\nattack, many others are susceptible to manipulation. A resourceful attacker can\ntamper with large portions of the dataset and affect them. An attacker can\nadditionally strategically focus on a preferred subset of the attributes in the\ndataset to maximize the effectiveness of the attack and minimize the resources\nallocated to data manipulation. In light of this vulnerability, we introduce a\nsolution according to which the defender implements an array of learners, and\ntheir activation is performed strategically. The defender computes the (game\ntheoretic) strategy space and accordingly applies a dominant strategy where\npossible, and a Nash-stable strategy otherwise. In this paper we provide the\ndetails of this approach. We analyze Nash equilibrium in such a strategic\nlearning environment, and demonstrate our solution by specific examples.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 13:19:11 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Farchi", "Eitan", ""], ["Shehory", "Onn", ""], ["Barash", "Guy", ""]]}, {"id": "1904.00739", "submitter": "Kevin Meng", "authors": "Kevin Meng, Yu Meng", "title": "Through-Wall Pose Imaging in Real-Time with a Many-to-Many\n  Encoder/Decoder Paradigm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overcoming the visual barrier and developing \"see-through vision\" has been\none of mankind's long-standing dreams. Unlike visible light, Radio Frequency\n(RF) signals penetrate opaque obstructions and reflect highly off humans. This\npaper establishes a deep-learning model that can be trained to reconstruct\ncontinuous video of a 15-point human skeleton even through visual occlusion.\nThe training process adopts a student/teacher learning procedure inspired by\nthe Feynman learning technique, in which video frames and RF data are first\ncollected simultaneously using a co-located setup containing an optical camera\nand an RF antenna array transceiver. Next, the video frames are processed with\na computer-vision-based gait analysis \"teacher\" module to generate ground-truth\nhuman skeletons for each frame. Then, the same type of skeleton is predicted\nfrom corresponding RF data using a \"student\" deep-learning model consisting of\na Residual Convolutional Neural Network (CNN), Region Proposal Network (RPN),\nand Recurrent Neural Network with Long-Short Term Memory (LSTM) that 1)\nextracts spatial features from RF images, 2) detects all people present in a\nscene, and 3) aggregates information over many time-steps, respectively. The\nmodel is shown to both accurately and completely predict the pose of humans\nbehind visual obstruction solely using RF signals. Primary academic\ncontributions include the novel many-to-many imaging methodology, unique\nintegration of RPN and LSTM networks, and original training pipeline.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 19:05:05 GMT"}, {"version": "v2", "created": "Sun, 20 Oct 2019 05:52:38 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Meng", "Kevin", ""], ["Meng", "Yu", ""]]}, {"id": "1904.00741", "submitter": "Benjamin Chamberlain", "authors": "Elaine M. Bettaney, Stephen R. Hardwick, Odysseas Zisimopoulos,\n  Benjamin Paul Chamberlain", "title": "Fashion Outfit Generation for E-commerce", "comments": "9 pages, 9 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining items of clothing into an outfit is a major task in fashion retail.\nRecommending sets of items that are compatible with a particular seed item is\nuseful for providing users with guidance and inspiration, but is currently a\nmanual process that requires expert stylists and is therefore not scalable or\neasy to personalise. We use a multilayer neural network fed by visual and\ntextual features to learn embeddings of items in a latent style space such that\ncompatible items of different types are embedded close to one another. We train\nour model using the ASOS outfits dataset, which consists of a large number of\noutfits created by professional stylists and which we release to the research\ncommunity. Our model shows strong performance in an offline outfit\ncompatibility prediction task. We use our model to generate outfits and for the\nfirst time in this field perform an AB test, comparing our generated outfits to\nthose produced by a baseline model which matches appropriate product types but\nuses no information on style. Users approved of outfits generated by our model\n21% and 34% more frequently than those generated by the baseline model for\nwomenswear and menswear respectively.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 11:19:34 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Bettaney", "Elaine M.", ""], ["Hardwick", "Stephen R.", ""], ["Zisimopoulos", "Odysseas", ""], ["Chamberlain", "Benjamin Paul", ""]]}, {"id": "1904.00759", "submitter": "Juncheng Li", "authors": "Juncheng Li, Frank R. Schmidt, J. Zico Kolter", "title": "Adversarial camera stickers: A physical camera-based attack on deep\n  learning systems", "comments": null, "journal-ref": "Proceedings of the 36th International Conference on Machine\n  Learning, PMLR 97:3896-3904, 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has documented the susceptibility of deep learning systems to\nadversarial examples, but most such attacks directly manipulate the digital\ninput to a classifier. Although a smaller line of work considers physical\nadversarial attacks, in all cases these involve manipulating the object of\ninterest, e.g., putting a physical sticker on an object to misclassify it, or\nmanufacturing an object specifically intended to be misclassified. In this\nwork, we consider an alternative question: is it possible to fool deep\nclassifiers, over all perceived objects of a certain type, by physically\nmanipulating the camera itself? We show that by placing a carefully crafted and\nmainly-translucent sticker over the lens of a camera, one can create universal\nperturbations of the observed images that are inconspicuous, yet misclassify\ntarget objects as a different (targeted) class. To accomplish this, we propose\nan iterative procedure for both updating the attack perturbation (to make it\nadversarial for a given classifier), and the threat model itself (to ensure it\nis physically realizable). For example, we show that we can achieve\nphysically-realizable attacks that fool ImageNet classifiers in a targeted\nfashion 49.6% of the time. This presents a new class of physically-realizable\nthreat models to consider in the context of adversarially robust machine\nlearning. Our demo video can be viewed at: https://youtu.be/wUVmL33Fx54\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 23:33:12 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 01:46:23 GMT"}, {"version": "v3", "created": "Wed, 3 Apr 2019 17:31:40 GMT"}, {"version": "v4", "created": "Sat, 8 Jun 2019 19:23:56 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Li", "Juncheng", ""], ["Schmidt", "Frank R.", ""], ["Kolter", "J. Zico", ""]]}, {"id": "1904.00760", "submitter": "Wieland Brendel", "authors": "Wieland Brendel and Matthias Bethge", "title": "Approximating CNNs with Bag-of-local-Features models works surprisingly\n  well on ImageNet", "comments": "Published as a conference paper at the Seventh International\n  Conference on Learning Representations (ICLR 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) excel on many complex perceptual tasks but it has\nproven notoriously difficult to understand how they reach their decisions. We\nhere introduce a high-performance DNN architecture on ImageNet whose decisions\nare considerably easier to explain. Our model, a simple variant of the\nResNet-50 architecture called BagNet, classifies an image based on the\noccurrences of small local image features without taking into account their\nspatial ordering. This strategy is closely related to the bag-of-feature (BoF)\nmodels popular before the onset of deep learning and reaches a surprisingly\nhigh accuracy on ImageNet (87.6% top-5 for 33 x 33 px features and Alexnet\nperformance for 17 x 17 px features). The constraint on local features makes it\nstraight-forward to analyse how exactly each part of the image influences the\nclassification. Furthermore, the BagNets behave similar to state-of-the art\ndeep neural networks such as VGG-16, ResNet-152 or DenseNet-169 in terms of\nfeature sensitivity, error distribution and interactions between image parts.\nThis suggests that the improvements of DNNs over previous bag-of-feature\nclassifiers in the last few years is mostly achieved by better fine-tuning\nrather than by qualitatively different decision strategies.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 16:37:17 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Brendel", "Wieland", ""], ["Bethge", "Matthias", ""]]}, {"id": "1904.00761", "submitter": "Casper Hansen", "authors": "Christian Hansen, Casper Hansen, Stephen Alstrup, Jakob Grue Simonsen,\n  Christina Lioma", "title": "Neural Speed Reading with Structural-Jump-LSTM", "comments": "10 pages", "journal-ref": "7th International Conference on Learning Representations (ICLR)\n  2019", "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) can model natural language by sequentially\n'reading' input tokens and outputting a distributed representation of each\ntoken. Due to the sequential nature of RNNs, inference time is linearly\ndependent on the input length, and all inputs are read regardless of their\nimportance. Efforts to speed up this inference, known as 'neural speed\nreading', either ignore or skim over part of the input. We present\nStructural-Jump-LSTM: the first neural speed reading model to both skip and\njump text during inference. The model consists of a standard LSTM and two\nagents: one capable of skipping single words when reading, and one capable of\nexploiting punctuation structure (sub-sentence separators (,:), sentence end\nsymbols (.!?), or end of text markers) to jump ahead after reading a word. A\ncomprehensive experimental evaluation of our model against all five\nstate-of-the-art neural reading models shows that Structural-Jump-LSTM achieves\nthe best overall floating point operations (FLOP) reduction (hence is faster),\nwhile keeping the same accuracy or even improving it compared to a vanilla LSTM\nthat reads the whole text.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 12:01:46 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 08:59:34 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Hansen", "Christian", ""], ["Hansen", "Casper", ""], ["Alstrup", "Stephen", ""], ["Simonsen", "Jakob Grue", ""], ["Lioma", "Christina", ""]]}, {"id": "1904.00762", "submitter": "Subba Reddy Oota", "authors": "Subba Reddy Oota, Adithya Avvaru, Mounika Marreddy, Radhika Mamidi", "title": "Affect in Tweets Using Experts Model", "comments": "10 pages, 6 figures, The 32nd Pacific Asia Conference on Language,\n  Information and Computation (PACLIC 32)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimating the intensity of emotion has gained significance as modern textual\ninputs in potential applications like social media, e-retail markets,\npsychology, advertisements etc., carry a lot of emotions, feelings, expressions\nalong with its meaning. However, the approaches of traditional sentiment\nanalysis primarily focuses on classifying the sentiment in general (positive or\nnegative) or at an aspect level(very positive, low negative, etc.) and cannot\nexploit the intensity information. Moreover, automatically identifying emotions\nlike anger, fear, joy, sadness, disgust etc., from text introduces challenging\nscenarios where single tweet may contain multiple emotions with different\nintensities and some emotions may even co-occur in some of the tweets. In this\npaper, we propose an architecture, Experts Model, inspired from the standard\nMixture of Experts (MoE) model. The key idea here is each expert learns\ndifferent sets of features from the feature vector which helps in better\nemotion detection from the tweet. We compared the results of our Experts Model\nwith both baseline results and top five performers of SemEval-2018 Task-1,\nAffect in Tweets (AIT). The experimental results show that our proposed\napproach deals with the emotion detection problem and stands at top-5 results.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 11:10:29 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Oota", "Subba Reddy", ""], ["Avvaru", "Adithya", ""], ["Marreddy", "Mounika", ""], ["Mamidi", "Radhika", ""]]}, {"id": "1904.00763", "submitter": "Samy Blusseau", "authors": "Bastien Ponchon (CMM, LTCI), Santiago Velasco-Forero (CMM), Samy\n  Blusseau (CMM), Jesus Angulo (CMM), Isabelle Bloch (LTCI)", "title": "Part-based approximations for morphological operators using asymmetric\n  auto-encoders", "comments": null, "journal-ref": "International Symposium on Mathematical Morphology, Jul 2019,\n  Saarbr{\\\"u}cken, Germany", "doi": null, "report-no": null, "categories": "cs.CV cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the issue of building a part-based representation of a\ndataset of images. More precisely, we look for a non-negative, sparse\ndecomposition of the images on a reduced set of atoms, in order to unveil a\nmorphological and interpretable structure of the data. Additionally, we want\nthis decomposition to be computed online for any new sample that is not part of\nthe initial dataset. Therefore, our solution relies on a sparse, non-negative\nauto-encoder where the encoder is deep (for accuracy) and the decoder shallow\n(for interpretability). This method compares favorably to the state-of-the-art\nonline methods on two datasets (MNIST and Fashion MNIST), according to\nclassical metrics and to a new one we introduce, based on the invariance of the\nrepresentation to morphological dilation.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 08:16:48 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 12:03:34 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Ponchon", "Bastien", "", "CMM, LTCI"], ["Velasco-Forero", "Santiago", "", "CMM"], ["Blusseau", "Samy", "", "CMM"], ["Angulo", "Jesus", "", "CMM"], ["Bloch", "Isabelle", "", "LTCI"]]}, {"id": "1904.00764", "submitter": "Hazrat Ali", "authors": "Mohammad Farhad Bulbul, Saiful Islam, Hazrat Ali", "title": "3D human action analysis and recognition through GLAC descriptor on 2D\n  motion and static posture images", "comments": "Multimed Tools Appl (2019)", "journal-ref": null, "doi": "10.1007/s11042-019-7365-2", "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present an approach for identification of actions within\ndepth action videos. First, we process the video to get motion history images\n(MHIs) and static history images (SHIs) corresponding to an action video based\non the use of 3D Motion Trail Model (3DMTM). We then characterize the action\nvideo by extracting the Gradient Local Auto-Correlations (GLAC) features from\nthe SHIs and the MHIs. The two sets of features i.e., GLAC features from MHIs\nand GLAC features from SHIs are concatenated to obtain a representation vector\nfor action. Finally, we perform the classification on all the action samples by\nusing the l2-regularized Collaborative Representation Classifier (l2-CRC) to\nrecognize different human actions in an effective way. We perform evaluation of\nthe proposed method on three action datasets, MSR-Action3D, DHA and UTD-MHAD.\nThrough experimental results, we observe that the proposed method performs\nsuperior to other approaches.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 17:52:16 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Bulbul", "Mohammad Farhad", ""], ["Islam", "Saiful", ""], ["Ali", "Hazrat", ""]]}, {"id": "1904.00768", "submitter": "Pranav Shenoy K P", "authors": "Yongqing Sun, Pranav Shenoy K P, Jun Shimamura, Atsushi Sagata", "title": "Concatenated Feature Pyramid Network for Instance Segmentation", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Low level features like edges and textures play an important role in\naccurately localizing instances in neural networks. In this paper, we propose\nan architecture which improves feature pyramid networks commonly used instance\nsegmentation networks by incorporating low level features in all layers of the\npyramid in an optimal and efficient way. Specifically, we introduce a new layer\nwhich learns new correlations from feature maps of multiple feature pyramid\nlevels holistically and enhances the semantic information of the feature\npyramid to improve accuracy. Our architecture is simple to implement in\ninstance segmentation or object detection frameworks to boost accuracy. Using\nthis method in Mask RCNN, our model achieves consistent improvement in\nprecision on COCO Dataset with the computational overhead compared to the\noriginal feature pyramid network.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 07:44:10 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Sun", "Yongqing", ""], ["P", "Pranav Shenoy K", ""], ["Shimamura", "Jun", ""], ["Sagata", "Atsushi", ""]]}, {"id": "1904.00770", "submitter": "Daniel Salles Civitarese", "authors": "Reinaldo Mozart Silva, Lais Baroni, Rodrigo S. Ferreira, Daniel\n  Civitarese, Daniela Szwarcman, Emilio Vital Brazil", "title": "Netherlands Dataset: A New Public Dataset for Machine Learning in\n  Seismic Interpretation", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV physics.geo-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning and, more specifically, deep learning algorithms have seen\nremarkable growth in their popularity and usefulness in the last years. This is\narguably due to three main factors: powerful computers, new techniques to train\ndeeper networks and larger datasets. Although the first two are readily\navailable in modern computers and ML libraries, the last one remains a\nchallenge for many domains. It is a fact that big data is a reality in almost\nall fields nowadays, and geosciences are not an exception. However, to achieve\nthe success of general-purpose applications such as ImageNet - for which there\nare +14 million labeled images for 1000 target classes - we not only need more\ndata, we need more high-quality labeled data. When it comes to the Oil&Gas\nindustry, confidentiality issues hamper even more the sharing of datasets. In\nthis work, we present the Netherlands interpretation dataset, a contribution to\nthe development of machine learning in seismic interpretation. The Netherlands\nF3 dataset acquisition was carried out in the North Sea, Netherlands offshore.\nThe data is publicly available and contains pos-stack data, 8 horizons and well\nlogs of 4 wells. For the purposes of our machine learning tasks, the original\ndataset was reinterpreted, generating 9 horizons separating different seismic\nfacies intervals. The interpreted horizons were used to generate approximatelly\n190,000 labeled images for inlines and crosslines. Finally, we present two deep\nlearning applications in which the proposed dataset was employed and produced\ncompelling results.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 13:12:14 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Silva", "Reinaldo Mozart", ""], ["Baroni", "Lais", ""], ["Ferreira", "Rodrigo S.", ""], ["Civitarese", "Daniel", ""], ["Szwarcman", "Daniela", ""], ["Brazil", "Emilio Vital", ""]]}, {"id": "1904.00771", "submitter": "Junichi Yamagishi", "authors": "Hieu-Thi Luong, Xin Wang, Junichi Yamagishi, Nobuyuki Nishizawa", "title": "Training Multi-Speaker Neural Text-to-Speech Systems using\n  Speaker-Imbalanced Speech Corpora", "comments": "Submitted to Interspeech 2019, Graz, Austria", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the available data of a target speaker is insufficient to train a high\nquality speaker-dependent neural text-to-speech (TTS) system, we can combine\ndata from multiple speakers and train a multi-speaker TTS model instead. Many\nstudies have shown that neural multi-speaker TTS model trained with a small\namount data from multiple speakers combined can generate synthetic speech with\nbetter quality and stability than a speaker-dependent one. However when the\namount of data from each speaker is highly unbalanced, the best approach to\nmake use of the excessive data remains unknown. Our experiments showed that\nsimply combining all available data from every speaker to train a multi-speaker\nmodel produces better than or at least similar performance to its\nspeaker-dependent counterpart. Moreover by using an ensemble multi-speaker\nmodel, in which each subsystem is trained on a subset of available data, we can\nfurther improve the quality of the synthetic speech especially for\nunderrepresented speakers whose training data is limited.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 12:39:05 GMT"}, {"version": "v2", "created": "Sun, 7 Apr 2019 23:35:06 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Luong", "Hieu-Thi", ""], ["Wang", "Xin", ""], ["Yamagishi", "Junichi", ""], ["Nishizawa", "Nobuyuki", ""]]}, {"id": "1904.00775", "submitter": "Ramchalam Kinattinkara Ramakrishnan Mr", "authors": "Ramchalam Kinattinkara Ramakrishnan, Shangling Jui and Vahid Patrovi\n  Nia", "title": "Deep Demosaicing for Edge Implementation", "comments": "Accepted in the 16th International Conference of Image Analysis and\n  Recognition (ICIAR 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most digital cameras use sensors coated with a Color Filter Array (CFA) to\ncapture channel components at every pixel location, resulting in a mosaic image\nthat does not contain pixel values in all channels. Current research on\nreconstructing these missing channels, also known as demosaicing, introduces\nmany artifacts, such as zipper effect and false color. Many deep learning\ndemosaicing techniques outperform other classical techniques in reducing the\nimpact of artifacts. However, most of these models tend to be\nover-parametrized. Consequently, edge implementation of the state-of-the-art\ndeep learning-based demosaicing algorithms on low-end edge devices is a major\nchallenge. We provide an exhaustive search of deep neural network architectures\nand obtain a pareto front of Color Peak Signal to Noise Ratio (CPSNR) as the\nperformance criterion versus the number of parameters as the model complexity\nthat beats the state-of-the-art. Architectures on the pareto front can then be\nused to choose the best architecture for a variety of resource constraints.\nSimple architecture search methods such as exhaustive search and grid search\nrequire some conditions of the loss function to converge to the optimum. We\nclarify these conditions in a brief theoretical study.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 15:04:17 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 19:30:39 GMT"}, {"version": "v3", "created": "Thu, 23 May 2019 15:20:54 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Ramakrishnan", "Ramchalam Kinattinkara", ""], ["Jui", "Shangling", ""], ["Nia", "Vahid Patrovi", ""]]}, {"id": "1904.00781", "submitter": "Dawei Li", "authors": "Dawei Li, Serafettin Tasci, Shalini Ghosh, Jingwen Zhu, Junting Zhang,\n  Larry Heck", "title": "RILOD: Near Real-Time Incremental Learning for Object Detection at the\n  Edge", "comments": "Camera-ready for ACM/IEEE SEC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection models shipped with camera-equipped edge devices cannot\ncover the objects of interest for every user. Therefore, the incremental\nlearning capability is a critical feature for a robust and personalized object\ndetection system that many applications would rely on. In this paper, we\npresent an efficient yet practical system, RILOD, to incrementally train an\nexisting object detection model such that it can detect new object classes\nwithout losing its capability to detect old classes. The key component of RILOD\nis a novel incremental learning algorithm that trains end-to-end for one-stage\ndeep object detection models only using training data of new object classes.\nSpecifically to avoid catastrophic forgetting, the algorithm distills three\ntypes of knowledge from the old model to mimic the old model's behavior on\nobject classification, bounding box regression and feature extraction. In\naddition, since the training data for the new classes may not be available, a\nreal-time dataset construction pipeline is designed to collect training images\non-the-fly and automatically label the images with both category and bounding\nbox annotations. We have implemented RILOD under both edge-cloud and edge-only\nsetups. Experiment results show that the proposed system can learn to detect a\nnew object class in just a few minutes, including both dataset construction and\nmodel training. In comparison, traditional fine-tuning based method may take a\nfew hours for training, and in most cases would also need a tedious and costly\nmanual dataset labeling step.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 17:22:01 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 17:37:55 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Li", "Dawei", ""], ["Tasci", "Serafettin", ""], ["Ghosh", "Shalini", ""], ["Zhu", "Jingwen", ""], ["Zhang", "Junting", ""], ["Heck", "Larry", ""]]}, {"id": "1904.00784", "submitter": "Sai Krishna Rallabandi", "authors": "Sunayana Sitaram, Khyathi Raghavi Chandu, Sai Krishna Rallabandi and\n  Alan W Black", "title": "A Survey of Code-switched Speech and Language Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Code-switching, the alternation of languages within a conversation or\nutterance, is a common communicative phenomenon that occurs in multilingual\ncommunities across the world. This survey reviews computational approaches for\ncode-switched Speech and Natural Language Processing. We motivate why\nprocessing code-switched text and speech is essential for building intelligent\nagents and systems that interact with users in multilingual communities. As\ncode-switching data and resources are scarce, we list what is available in\nvarious code-switched language pairs with the language processing tasks they\ncan be used for. We review code-switching research in various Speech and NLP\napplications, including language processing tools and end-to-end systems. We\nconclude with future directions and open problems in the field.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 14:36:50 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 14:18:31 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 23:55:01 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Sitaram", "Sunayana", ""], ["Chandu", "Khyathi Raghavi", ""], ["Rallabandi", "Sai Krishna", ""], ["Black", "Alan W", ""]]}, {"id": "1904.00785", "submitter": "Aleksandr Perevalov", "authors": "Aleksandr Perevalov, Daniil Kurushin, Rustam Faizrakhmanov and Farida\n  Khabibrakhmanova", "title": "Question Embeddings Based on Shannon Entropy: Solving intent\n  classification task in goal-oriented dialogue system", "comments": "Proceedings of International Conference on Applied Innovation in IT", "journal-ref": "2019/03/06, Volume 7, Issue 1, Koethen Germany, ISBN:\n  978-3-96057-086-8 (Online)", "doi": "10.25673/13485", "report-no": null, "categories": "cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Question-answering systems and voice assistants are becoming major part of\nclient service departments of many organizations, helping them to reduce the\nlabor costs of staff. In many such systems, there is always natural language\nunderstanding module that solves intent classification task. This task is\ncomplicated because of its case-dependency - every subject area has its own\nsemantic kernel. The state of art approaches for intent classification are\ndifferent machine learning and deep learning methods that use text vector\nrepresentations as input. The basic vector representation models such as Bag of\nwords and TF-IDF generate sparse matrixes, which are becoming very big as the\namount of input data grows. Modern methods such as word2vec and FastText use\nneural networks to evaluate word embeddings with fixed dimension size. As we\nare developing a question-answering system for students and enrollees of the\nPerm National Research Polytechnic University, we have faced the problem of\nuser's intent detection. The subject area of our system is very specific, that\nis why there is a lack of training data. This aspect makes intent\nclassification task more challenging for using state of the art deep learning\nmethods. In this paper, we propose an approach of the questions embeddings\nrepresentation based on calculation of Shannon entropy.The goal of the approach\nis to produce low dimensional question vectors as neural approaches do and to\noutperform related methods, described above in condition of small dataset. We\nevaluate and compare our model with existing ones using logistic regression and\ndataset that contains questions asked by students and enrollees. The data is\nlabeled into six classes. Experimental comparison of proposed approach and\nother models revealed that proposed model performed better in the given task.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 18:59:32 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Perevalov", "Aleksandr", ""], ["Kurushin", "Daniil", ""], ["Faizrakhmanov", "Rustam", ""], ["Khabibrakhmanova", "Farida", ""]]}, {"id": "1904.00788", "submitter": "Soheil Esmaeilzadeh", "authors": "Soheil Esmaeilzadeh, Gao Xian Peh, Angela Xu", "title": "Neural Abstractive Text Summarization and Fake News Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study abstractive text summarization by exploring different\nmodels such as LSTM-encoder-decoder with attention, pointer-generator networks,\ncoverage mechanisms, and transformers. Upon extensive and careful\nhyperparameter tuning we compare the proposed architectures against each other\nfor the abstractive text summarization task. Finally, as an extension of our\nwork, we apply our text summarization model as a feature extractor for a fake\nnews detection task where the news articles prior to classification will be\nsummarized and the results are compared against the classification using only\nthe original news text.\n  keywords: LSTM, encoder-deconder, abstractive text summarization,\npointer-generator, coverage mechanism, transformers, fake news detection\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 07:27:51 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 07:46:43 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Esmaeilzadeh", "Soheil", ""], ["Peh", "Gao Xian", ""], ["Xu", "Angela", ""]]}, {"id": "1904.00791", "submitter": "Lin Zhang", "authors": "Lin Zhang, Petko Bogdanov", "title": "DSL: Discriminative Subgraph Learning via Sparse Self-Representation", "comments": "9 pages", "journal-ref": "SIAM International Conference on Data Mining(SDM) 2019", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal in network state prediction (NSP) is to classify the global state\n(label) associated with features embedded in a graph. This graph structure\nencoding feature relationships is the key distinctive aspect of NSP compared to\nclassical supervised learning. NSP arises in various applications: gene\nexpression samples embedded in a protein-protein interaction (PPI) network,\ntemporal snapshots of infrastructure or sensor networks, and fMRI coherence\nnetwork samples from multiple subjects to name a few. Instances from these\ndomains are typically ``wide'' (more features than samples), and thus, feature\nsub-selection is required for robust and generalizable prediction. How to best\nemploy the network structure in order to learn succinct connected subgraphs\nencompassing the most discriminative features becomes a central challenge in\nNSP. Prior work employs connected subgraph sampling or graph smoothing within\noptimization frameworks, resulting in either large variance of quality or weak\ncontrol over the connectivity of selected subgraphs.\n  In this work we propose an optimization framework for discriminative subgraph\nlearning (DSL) which simultaneously enforces (i) sparsity, (ii) connectivity\nand (iii) high discriminative power of the resulting subgraphs of features. Our\noptimization algorithm is a single-step solution for the NSP and the associated\nfeature selection problem. It is rooted in the rich literature on\nmaximal-margin optimization, spectral graph methods and sparse subspace\nself-representation. DSL simultaneously ensures solution interpretability and\nsuperior predictive power (up to 16% improvement in challenging instances\ncompared to baselines), with execution times up to an hour for large instances.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 16:52:54 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Zhang", "Lin", ""], ["Bogdanov", "Petko", ""]]}, {"id": "1904.00805", "submitter": "Ben Gelman", "authors": "Jessica Moore, Ben Gelman, David Slater", "title": "A Convolutional Neural Network for Language-Agnostic Source Code\n  Summarization", "comments": "ENASE 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Descriptive comments play a crucial role in the software engineering process.\nThey decrease development time, enable better bug detection, and facilitate the\nreuse of previously written code. However, comments are commonly the last of a\nsoftware developer's priorities and are thus either insufficient or missing\nentirely. Automatic source code summarization may therefore have the ability to\nsignificantly improve the software development process. We introduce a novel\nencoder-decoder model that summarizes source code, effectively writing a\ncomment to describe the code's functionality. We make two primary innovations\nbeyond current source code summarization models. First, our encoder is fully\nlanguage-agnostic and requires no complex input preprocessing. Second, our\ndecoder has an open vocabulary, enabling it to predict any word, even ones not\nseen in training. We demonstrate results comparable to state-of-the-art methods\non a single-language data set and provide the first results on a data set\nconsisting of multiple programming languages.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 15:53:28 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Moore", "Jessica", ""], ["Gelman", "Ben", ""], ["Slater", "David", ""]]}, {"id": "1904.00815", "submitter": "Chollette Olisah Dr", "authors": "Chollette C. Olisah, Lyndon Smith", "title": "Understanding Unconventional Preprocessors in Deep Convolutional Neural\n  Networks for Face Identification", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks have achieved huge successes in application domains like object\nand face recognition. The performance gain is attributed to different facets of\nthe network architecture such as: depth of the convolutional layers, activation\nfunction, pooling, batch normalization, forward and back propagation and many\nmore. However, very little emphasis is made on the preprocessors. Therefore, in\nthis paper, the network's preprocessing module is varied across different\npreprocessing approaches while keeping constant other facets of the network\narchitecture, to investigate the contribution preprocessing makes to the\nnetwork. Commonly used preprocessors are the data augmentation and\nnormalization and are termed conventional preprocessors. Others are termed the\nunconventional preprocessors, they are: color space converters; HSV, CIE L*a*b*\nand YCBCR, grey-level resolution preprocessors; full-based and plane-based\nimage quantization, illumination normalization and insensitive feature\npreprocessing using: histogram equalization (HE), local contrast normalization\n(LN) and complete face structural pattern (CFSP). To achieve fixed network\nparameters, CNNs with transfer learning is employed. Knowledge from the\nhigh-level feature vectors of the Inception-V3 network is transferred to\noffline preprocessed LFW target data; and features trained using the SoftMax\nclassifier for face identification. The experiments show that the\ndiscriminative capability of the deep networks can be improved by preprocessing\nRGB data with HE, full-based and plane-based quantization, rgbGELog, and YCBCR,\npreprocessors before feeding it to CNNs. However, for best performance, the\nright setup of preprocessed data with augmentation and/or normalization is\nrequired. The plane-based image quantization is found to increase the\nhomogeneity of neighborhood pixels and utilizes reduced bit depth for better\nstorage efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 13:05:55 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 10:54:14 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Olisah", "Chollette C.", ""], ["Smith", "Lyndon", ""]]}, {"id": "1904.00816", "submitter": "Kuo Teng Ding", "authors": "Yi-Lun Pan, Min-Jhih Huang, Kuo-Teng Ding, Ja-Ling Wu, Jyh-Shing Jang", "title": "k-Same-Siamese-GAN: k-Same Algorithm with Generative Adversarial Network\n  for Facial Image De-identification with Hyperparameter Tuning and Mixed\n  Precision Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a data holder, such as a hospital or a government entity, who has a\nprivately held collection of personal data, in which the revealing and/or\nprocessing of the personal identifiable data is restricted and prohibited by\nlaw. Then, \"how can we ensure the data holder does conceal the identity of each\nindividual in the imagery of personal data while still preserving certain\nuseful aspects of the data after de-identification?\" becomes a challenge issue.\nIn this work, we propose an approach towards high-resolution facial image\nde-identification, called k-Same-Siamese-GAN, which leverages the\nk-Same-Anonymity mechanism, the Generative Adversarial Network, and the\nhyperparameter tuning methods. Moreover, to speed up model training and reduce\nmemory consumption, the mixed precision training technique is also applied to\nmake kSS-GAN provide guarantees regarding privacy protection on close-form\nidentities and be trained much more efficiently as well. Finally, to validate\nits applicability, the proposed work has been applied to actual datasets - RafD\nand CelebA for performance testing. Besides protecting privacy of\nhigh-resolution facial images, the proposed system is also justified for its\nability in automating parameter tuning and breaking through the limitation of\nthe number of adjustable parameters.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 14:27:07 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 05:24:19 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Pan", "Yi-Lun", ""], ["Huang", "Min-Jhih", ""], ["Ding", "Kuo-Teng", ""], ["Wu", "Ja-Ling", ""], ["Jang", "Jyh-Shing", ""]]}, {"id": "1904.00824", "submitter": "Sebastian Hartwig", "authors": "Sebastian Hartwig, Timo Ropinski", "title": "Training Object Detectors on Synthetic Images Containing Reflecting\n  Materials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the grand challenges of deep learning is the requirement to obtain\nlarge labeled training data sets. While synthesized data sets can be used to\novercome this challenge, it is important that these data sets close the reality\ngap, i.e., a model trained on synthetic image data is able to generalize to\nreal images. Whereas, the reality gap can be considered bridged in several\napplication scenarios, training on synthesized images containing reflecting\nmaterials requires further research. Since the appearance of objects with\nreflecting materials is dominated by the surrounding environment, this\ninteraction needs to be considered during training data generation. Therefore,\nwithin this paper we examine the effect of reflecting materials in the context\nof synthetic image generation for training object detectors. We investigate the\ninfluence of rendering approach used for image synthesis, the effect of domain\nrandomization, as well as the amount of used training data. To be able to\ncompare our results to the state-of-the-art, we focus on indoor scenes as they\nhave been investigated extensively. Within this scenario, bathroom furniture is\na natural choice for objects with reflecting materials, for which we report our\nfindings on real and synthetic testing data.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 13:27:34 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Hartwig", "Sebastian", ""], ["Ropinski", "Timo", ""]]}, {"id": "1904.00864", "submitter": "Kyung-Su Kim", "authors": "Kyung-Su Kim, Sae-Young Chung", "title": "Tree Search Network for Sparse Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classical sparse regression problem of recovering a sparse\nsignal $x_0$ given a measurement vector $y = \\Phi x_0+w$. We propose a tree\nsearch algorithm driven by the deep neural network for sparse regression (TSN).\nTSN improves the signal reconstruction performance of the deep neural network\ndesigned for sparse regression by performing a tree search with pruning. It is\nobserved in both noiseless and noisy cases, TSN recovers synthetic and real\nsignals with lower complexity than a conventional tree search and is superior\nto existing algorithms by a large margin for various types of the sensing\nmatrix $\\Phi$, widely used in sparse regression.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 14:05:41 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Kim", "Kyung-Su", ""], ["Chung", "Sae-Young", ""]]}, {"id": "1904.00865", "submitter": "Benjamin Guedj", "authors": "Benjamin Guedj and Juliette Rengot", "title": "Non-linear aggregation of filters to improve image denoising", "comments": "To appear at Computing Conference 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce a novel aggregation method to efficiently perform image\ndenoising. Preliminary filters are aggregated in a non-linear fashion, using a\nnew metric of pixel proximity based on how the pool of filters reaches a\nconsensus. We provide a theoretical bound to support our aggregation scheme,\nits numerical performance is illustrated and we show that the aggregate\nsignificantly outperforms each of the preliminary filters.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 14:10:21 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 18:54:55 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2020 15:43:09 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Guedj", "Benjamin", ""], ["Rengot", "Juliette", ""]]}, {"id": "1904.00935", "submitter": "Vadim Markovtsev", "authors": "Vadim Markovtsev, Waren Long, Hugo Mougard, Konstantin Slavnov, Egor\n  Bulychev", "title": "STYLE-ANALYZER: fixing code style inconsistencies with interpretable\n  unsupervised algorithms", "comments": "10 pages; Mining Software Repositories 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Source code reviews are manual, time-consuming, and expensive. Human\ninvolvement should be focused on analyzing the most relevant aspects of the\nprogram, such as logic and maintainability, rather than amending style, syntax,\nor formatting defects. Some tools with linting capabilities can format code\nautomatically and report various stylistic violations for supported programming\nlanguages. They are based on rules written by domain experts, hence, their\nconfiguration is often tedious, and it is impractical for the given set of\nrules to cover all possible corner cases. Some machine learning-based solutions\nexist, but they remain uninterpretable black boxes. This paper introduces\nSTYLE-ANALYZER, a new open source tool to automatically fix code formatting\nviolations using the decision tree forest model which adapts to each codebase\nand is fully unsupervised. STYLE-ANALYZER is built on top of our novel assisted\ncode review framework, Lookout. It accurately mines the formatting style of\neach analyzed Git repository and expresses the found format patterns with\ncompact human-readable rules. STYLE-ANALYZER can then suggest style\ninconsistency fixes in the form of code review comments. We evaluate the output\nquality and practical relevance of STYLE-ANALYZER by demonstrating that it can\nreproduce the original style with high precision, measured on 19 popular\nJavaScript projects, and by showing that it yields promising results in fixing\nreal style mistakes. STYLE-ANALYZER includes a web application to visualize how\nthe rules are triggered. We release STYLE-ANALYZER as a reusable and extendable\nopen source software package on GitHub for the benefit of the community.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 16:15:38 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Markovtsev", "Vadim", ""], ["Long", "Waren", ""], ["Mougard", "Hugo", ""], ["Slavnov", "Konstantin", ""], ["Bulychev", "Egor", ""]]}, {"id": "1904.00938", "submitter": "Erwei Wang", "authors": "Erwei Wang, James J. Davis, Peter Y. K. Cheung, George A.\n  Constantinides", "title": "LUTNet: Rethinking Inference in FPGA Soft Logic", "comments": "Accepted manuscript uploaded 01/04/19. DOA 03/03/19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research has shown that deep neural networks contain significant redundancy,\nand that high classification accuracies can be achieved even when weights and\nactivations are quantised down to binary values. Network binarisation on FPGAs\ngreatly increases area efficiency by replacing resource-hungry multipliers with\nlightweight XNOR gates. However, an FPGA's fundamental building block, the\nK-LUT, is capable of implementing far more than an XNOR: it can perform any\nK-input Boolean operation. Inspired by this observation, we propose LUTNet, an\nend-to-end hardware-software framework for the construction of area-efficient\nFPGA-based neural network accelerators using the native LUTs as inference\noperators. We demonstrate that the exploitation of LUT flexibility allows for\nfar heavier pruning than possible in prior works, resulting in significant area\nsavings while achieving comparable accuracy. Against the state-of-the-art\nbinarised neural network implementation, we achieve twice the area efficiency\nfor several standard network models when inferencing popular datasets. We also\ndemonstrate that even greater energy efficiency improvements are obtainable.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 16:20:05 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Wang", "Erwei", ""], ["Davis", "James J.", ""], ["Cheung", "Peter Y. K.", ""], ["Constantinides", "George A.", ""]]}, {"id": "1904.00942", "submitter": "Wouter Van Amsterdam", "authors": "Wouter A.C. van Amsterdam, Marinus J.C. Eijkemans", "title": "Controlling for Biasing Signals in Images for Prognostic Models:\n  Survival Predictions for Lung Cancer with Deep Learning", "comments": "Initial version presented at AAAI-WHY spring symposium, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has shown remarkable results for image analysis and is expected\nto aid individual treatment decisions in health care. To achieve this, deep\nlearning methods need to be promoted from the level of mere associations to\nbeing able to answer causal questions. We present a scenario with real-world\nmedical images (CT-scans of lung cancers) and simulated outcome data. Through\nthe sampling scheme, the images contain two distinct factors of variation that\nrepresent a collider and a prognostic factor. We show that when this collider\ncan be quantified, unbiased individual prognosis predictions are attainable\nwith deep learning. This is achieved by (1) setting a dual task for the network\nto predict both the outcome and the collider and (2) enforcing independence of\nthe activation distributions of the last layer with ordinary least squares. Our\nmethod provides an example of combining deep learning and structural causal\nmodels for unbiased individual prognosis predictions.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 16:24:14 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["van Amsterdam", "Wouter A. C.", ""], ["Eijkemans", "Marinus J. C.", ""]]}, {"id": "1904.00956", "submitter": "Russell Mendonca", "authors": "Russell Mendonca, Abhishek Gupta, Rosen Kralev, Pieter Abbeel, Sergey\n  Levine, Chelsea Finn", "title": "Guided Meta-Policy Search", "comments": "Published at Neurips 2019. Website :\n  https://sites.google.com/berkeley.edu/guided-metapolicy-search", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) algorithms have demonstrated promising results on\ncomplex tasks, yet often require impractical numbers of samples since they\nlearn from scratch. Meta-RL aims to address this challenge by leveraging\nexperience from previous tasks so as to more quickly solve new tasks. However,\nin practice, these algorithms generally also require large amounts of on-policy\nexperience during the meta-training process, making them impractical for use in\nmany problems. To this end, we propose to learn a reinforcement learning\nprocedure in a federated way, where individual off-policy learners can solve\nthe individual meta-training tasks, and then consolidate these solutions into a\nsingle meta-learner. Since the central meta-learner learns by imitating the\nsolutions to the individual tasks, it can accommodate either the standard\nmeta-RL problem setting or a hybrid setting where some or all tasks are\nprovided with example demonstrations. The former results in an approach that\ncan leverage policies learned for previous tasks without significant amounts of\non-policy data during meta-training, whereas the latter is particularly useful\nin cases where demonstrations are easy for a person to provide. Across a number\nof continuous control meta-RL problems, we demonstrate significant improvements\nin meta-RL sample efficiency in comparison to prior work as well as the ability\nto scale to domains with visual observations.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 16:47:28 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 09:27:41 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Mendonca", "Russell", ""], ["Gupta", "Abhishek", ""], ["Kralev", "Rosen", ""], ["Abbeel", "Pieter", ""], ["Levine", "Sergey", ""], ["Finn", "Chelsea", ""]]}, {"id": "1904.00962", "submitter": "Yang You", "authors": "Yang You and Jing Li and Sashank Reddi and Jonathan Hseu and Sanjiv\n  Kumar and Srinadh Bhojanapalli and Xiaodan Song and James Demmel and Kurt\n  Keutzer and Cho-Jui Hsieh", "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes", "comments": "Published as a conference paper at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training large deep neural networks on massive datasets is computationally\nvery challenging. There has been recent surge in interest in using large batch\nstochastic optimization methods to tackle this issue. The most prominent\nalgorithm in this line of research is LARS, which by employing layerwise\nadaptive learning rates trains ResNet on ImageNet in a few minutes. However,\nLARS performs poorly for attention models like BERT, indicating that its\nperformance gains are not consistent across tasks. In this paper, we first\nstudy a principled layerwise adaptation strategy to accelerate training of deep\nneural networks using large mini-batches. Using this strategy, we develop a new\nlayerwise adaptive large batch optimization technique called LAMB; we then\nprovide convergence analysis of LAMB as well as LARS, showing convergence to a\nstationary point in general nonconvex settings. Our empirical results\ndemonstrate the superior performance of LAMB across various tasks such as BERT\nand ResNet-50 training with very little hyperparameter tuning. In particular,\nfor BERT training, our optimizer enables use of very large batch sizes of 32868\nwithout any degradation of performance. By increasing the batch size to the\nmemory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to\njust 76 minutes (Table 1). The LAMB implementation is available at\nhttps://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 16:53:35 GMT"}, {"version": "v2", "created": "Thu, 23 May 2019 06:20:00 GMT"}, {"version": "v3", "created": "Fri, 24 May 2019 17:09:47 GMT"}, {"version": "v4", "created": "Wed, 25 Sep 2019 16:07:11 GMT"}, {"version": "v5", "created": "Fri, 3 Jan 2020 06:53:00 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["You", "Yang", ""], ["Li", "Jing", ""], ["Reddi", "Sashank", ""], ["Hseu", "Jonathan", ""], ["Kumar", "Sanjiv", ""], ["Bhojanapalli", "Srinadh", ""], ["Song", "Xiaodan", ""], ["Demmel", "James", ""], ["Keutzer", "Kurt", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "1904.00977", "submitter": "Ibai Roman", "authors": "Ibai Roman, Alexander Mendiburu, Roberto Santana and Jose A. Lozano", "title": "Sentiment analysis with genetically evolved Gaussian kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Sentiment analysis consists of evaluating opinions or statements from the\nanalysis of text. Among the methods used to estimate the degree in which a text\nexpresses a given sentiment, are those based on Gaussian Processes. However,\ntraditional Gaussian Processes methods use a predefined kernel with\nhyperparameters that can be tuned but whose structure can not be adapted. In\nthis paper, we propose the application of Genetic Programming for evolving\nGaussian Process kernels that are more precise for sentiment analysis. We use\nuse a very flexible representation of kernels combined with a multi-objective\napproach that simultaneously considers two quality metrics and the\ncomputational time spent by the kernels. Our results show that the algorithm\ncan outperform Gaussian Processes with traditional kernels for some of the\nsentiment analysis tasks considered.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 17:28:35 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 08:48:49 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Roman", "Ibai", ""], ["Mendiburu", "Alexander", ""], ["Santana", "Roberto", ""], ["Lozano", "Jose A.", ""]]}, {"id": "1904.01002", "submitter": "Dongrui Wu", "authors": "Xiao Zhang and Dongrui Wu", "title": "On the Vulnerability of CNN Classifiers in EEG-Based BCIs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been successfully used in numerous applications because of\nits outstanding performance and the ability to avoid manual feature\nengineering. One such application is electroencephalogram (EEG) based\nbrain-computer interface (BCI), where multiple convolutional neural network\n(CNN) models have been proposed for EEG classification. However, it has been\nfound that deep learning models can be easily fooled with adversarial examples,\nwhich are normal examples with small deliberate perturbations. This paper\nproposes an unsupervised fast gradient sign method (UFGSM) to attack three\npopular CNN classifiers in BCIs, and demonstrates its effectiveness. We also\nverify the transferability of adversarial examples in BCIs, which means we can\nperform attacks even without knowing the architecture and parameters of the\ntarget models, or the datasets they were trained on. To our knowledge, this is\nthe first study on the vulnerability of CNN classifiers in EEG-based BCIs, and\nhopefully will trigger more attention on the security of BCI systems.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 06:27:08 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Zhang", "Xiao", ""], ["Wu", "Dongrui", ""]]}, {"id": "1904.01014", "submitter": "Joshua Peeples", "authors": "Joshua Peeples, Matthew Cook, Daniel Suen, Alina Zare, and James\n  Keller", "title": "Comparison of Possibilistic Fuzzy Local Information C-Means and\n  Possibilistic K-Nearest Neighbors for Synthetic Aperture Sonar Image\n  Segmentation", "comments": null, "journal-ref": "Proc. SPIE 110120, Detection and Sensing of Mines, Explosive\n  Objects, and Obscured Targets XXIV (10 May 2019)", "doi": null, "report-no": null, "categories": "eess.IV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic aperture sonar (SAS) imagery can generate high resolution images of\nthe seafloor. Thus, segmentation algorithms can be used to partition the images\ninto different seafloor environments. In this paper, we compare two\npossibilistic segmentation approaches. Possibilistic approaches allow for the\nability to detect novel or outlier environments as well as well known classes.\nThe Possibilistic Fuzzy Local Information C-Means (PFLICM) algorithm has been\npreviously applied to segment SAS imagery. Additionally, the Possibilistic\nK-Nearest Neighbors (PKNN) algorithm has been used in other domains such as\nlandmine detection and hyperspectral imagery. In this paper, we compare the\nsegmentation performance of a semi-supervised approach using PFLICM and a\nsupervised method using Possibilistic K-NN. We include final segmentation\nresults on multiple SAS images and a quantitative assessment of each algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 16:18:28 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Peeples", "Joshua", ""], ["Cook", "Matthew", ""], ["Suen", "Daniel", ""], ["Zare", "Alina", ""], ["Keller", "James", ""]]}, {"id": "1904.01033", "submitter": "Maximilian Igl", "authors": "Maximilian Igl, Andrew Gambardella, Jinke He, Nantas Nardelli, N.\n  Siddharth, Wendelin B\\\"ohmer, Shimon Whiteson", "title": "Multitask Soft Option Learning", "comments": "Published at UAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Multitask Soft Option Learning(MSOL), a hierarchical multitask\nframework based on Planning as Inference. MSOL extends the concept of options,\nusing separate variational posteriors for each task, regularized by a shared\nprior. This ''soft'' version of options avoids several instabilities during\ntraining in a multitask setting, and provides a natural way to learn both\nintra-option policies and their terminations. Furthermore, it allows\nfine-tuning of options for new tasks without forgetting their learned policies,\nleading to faster training without reducing the expressiveness of the\nhierarchical policy. We demonstrate empirically that MSOL significantly\noutperforms both hierarchical and flat transfer-learning baselines.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 18:01:34 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 13:53:11 GMT"}, {"version": "v3", "created": "Sun, 21 Jun 2020 10:36:45 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Igl", "Maximilian", ""], ["Gambardella", "Andrew", ""], ["He", "Jinke", ""], ["Nardelli", "Nantas", ""], ["Siddharth", "N.", ""], ["B\u00f6hmer", "Wendelin", ""], ["Whiteson", "Shimon", ""]]}, {"id": "1904.01049", "submitter": "Ben Letham", "authors": "Benjamin Letham, Eytan Bakshy", "title": "Bayesian Optimization for Policy Search via Online-Offline\n  Experimentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online field experiments are the gold-standard way of evaluating changes to\nreal-world interactive machine learning systems. Yet our ability to explore\ncomplex, multi-dimensional policy spaces - such as those found in\nrecommendation and ranking problems - is often constrained by the limited\nnumber of experiments that can be run simultaneously. To alleviate these\nconstraints, we augment online experiments with an offline simulator and apply\nmulti-task Bayesian optimization to tune live machine learning systems. We\ndescribe practical issues that arise in these types of applications, including\nbiases that arise from using a simulator and assumptions for the multi-task\nkernel. We measure empirical learning curves which show substantial gains from\nincluding data from biased offline experiments, and show how these learning\ncurves are consistent with theoretical results for multi-task Gaussian process\ngeneralization. We find that improved kernel inference is a significant driver\nof multi-task generalization. Finally, we show several examples of Bayesian\noptimization efficiently tuning a live machine learning system by combining\noffline and online experiments.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 18:19:11 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 16:38:06 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Letham", "Benjamin", ""], ["Bakshy", "Eytan", ""]]}, {"id": "1904.01059", "submitter": "Marco Romanelli", "authors": "Marco Romanelli and Konstantinos Chatzikokolakis and Catuscia\n  Palamidessi", "title": "Optimal Obfuscation Mechanisms via Machine Learning", "comments": "Preprint version of a paper that will appear on the Proceedings of\n  the IEEE 33rd Computer Security Foundations Symposium, CSF 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of obfuscating sensitive information while preserving\nutility, and we propose a machine learning approach inspired by the generative\nadversarial networks paradigm. The idea is to set up two nets: the generator,\nthat tries to produce an optimal obfuscation mechanism to protect the data, and\nthe classifier, that tries to de-obfuscate the data. By letting the two nets\ncompete against each other, the mechanism improves its degree of protection,\nuntil an equilibrium is reached. We apply our method to the case of location\nprivacy, and we perform experiments on synthetic data and on real data from the\nGowalla dataset. We evaluate the privacy of the mechanism not only by its\ncapacity to defeat the classifier, but also in terms of the Bayes error, which\nrepresents the strongest possible adversary. We compare the privacy-utility\ntradeoff of our method to that of the planar Laplace mechanism used in\ngeo-indistinguishability, showing favorable results. Like the Laplace\nmechanism, our system can be deployed at the user end for protecting his\nlocation.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 18:33:54 GMT"}, {"version": "v2", "created": "Sat, 9 May 2020 19:16:51 GMT"}, {"version": "v3", "created": "Tue, 12 May 2020 10:36:22 GMT"}, {"version": "v4", "created": "Sat, 10 Oct 2020 16:55:29 GMT"}, {"version": "v5", "created": "Sun, 25 Oct 2020 17:18:12 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Romanelli", "Marco", ""], ["Chatzikokolakis", "Konstantinos", ""], ["Palamidessi", "Catuscia", ""]]}, {"id": "1904.01067", "submitter": "Yang Zhang", "authors": "Ahmed Salem and Apratim Bhattacharya and Michael Backes and Mario\n  Fritz and Yang Zhang", "title": "Updates-Leak: Data Set Inference and Reconstruction Attacks in Online\n  Learning", "comments": "USENIX Security 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) has progressed rapidly during the past decade and the\nmajor factor that drives such development is the unprecedented large-scale\ndata. As data generation is a continuous process, this leads to ML model owners\nupdating their models frequently with newly-collected data in an online\nlearning scenario. In consequence, if an ML model is queried with the same set\nof data samples at two different points in time, it will provide different\nresults.\n  In this paper, we investigate whether the change in the output of a black-box\nML model before and after being updated can leak information of the dataset\nused to perform the update, namely the updating set. This constitutes a new\nattack surface against black-box ML models and such information leakage may\ncompromise the intellectual property and data privacy of the ML model owner. We\npropose four attacks following an encoder-decoder formulation, which allows\ninferring diverse information of the updating set. Our new attacks are\nfacilitated by state-of-the-art deep learning techniques. In particular, we\npropose a hybrid generative model (CBM-GAN) that is based on generative\nadversarial networks (GANs) but includes a reconstructive loss that allows\nreconstructing accurate samples. Our experiments show that the proposed attacks\nachieve strong performance.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 19:08:49 GMT"}, {"version": "v2", "created": "Sat, 30 Nov 2019 14:53:28 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Salem", "Ahmed", ""], ["Bhattacharya", "Apratim", ""], ["Backes", "Michael", ""], ["Fritz", "Mario", ""], ["Zhang", "Yang", ""]]}, {"id": "1904.01070", "submitter": "Peyman Hosseinzadeh Kassani", "authors": "Peyman Hosseinzadeh Kassani, Alexej Gossmann, and Yu-Ping Wang", "title": "Multimodal Sparse Classifier for Adolescent Brain Age Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of healthy brain development helps to better understand the brain\ntransformation and brain connectivity patterns which happen during childhood to\nadulthood. This study presents a sparse machine learning solution across\nwhole-brain functional connectivity (FC) measures of three sets of data,\nderived from resting state functional magnetic resonance imaging (rs-fMRI) and\ntask fMRI data, including a working memory n-back task (nb-fMRI) and an emotion\nidentification task (em-fMRI). These multi-modal image data are collected on a\nsample of adolescents from the Philadelphia Neurodevelopmental Cohort (PNC) for\nthe prediction of brain ages. Due to extremely large variable-to-instance ratio\nof PNC data, a high dimensional matrix with several irrelevant and highly\ncorrelated features is generated and hence a pattern learning approach is\nnecessary to extract significant features. We propose a sparse learner based on\nthe residual errors along the estimation of an inverse problem for the extreme\nlearning machine (ELM) neural network. The purpose of the approach is to\novercome the overlearning problem through pruning of several redundant features\nand their corresponding output weights. The proposed multimodal sparse ELM\nclassifier based on residual errors (RES-ELM) is highly competitive in terms of\nthe classification accuracy compared to its counterparts such as conventional\nELM, and sparse Bayesian learning ELM.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 19:13:07 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Kassani", "Peyman Hosseinzadeh", ""], ["Gossmann", "Alexej", ""], ["Wang", "Yu-Ping", ""]]}, {"id": "1904.01083", "submitter": "Ardavan Bidgoli", "authors": "Ardavan Bidgoli, Pedro Veloso", "title": "DeepCloud. The Application of a Data-driven, Generative Model in Design", "comments": null, "journal-ref": "ACADIA 2018: Recalibration. On imprecision and infidelity.\n  Proceedings of the 38th Annual Conference of the Association for Computer\n  Aided Design in Architecture (ACADIA) ISBN 978-0-692-17729-7, Mexico City,\n  2018, pp. 176-185", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative systems have a significant potential to synthesize innovative\ndesign alternatives. Still, most of the common systems that have been adopted\nin design require the designer to explicitly define the specifications of the\nprocedures and in some cases the design space. In contrast, a generative system\ncould potentially learn both aspects through processing a database of existing\nsolutions without the supervision of the designer. To explore this possibility,\nwe review recent advancements of generative models in machine learning and\ncurrent applications of learning techniques in design. Then, we describe the\ndevelopment of a data-driven generative system titled DeepCloud. It combines an\nautoencoder architecture for point clouds with a web-based interface and analog\ninput devices to provide an intuitive experience for data-driven generation of\ndesign alternatives. We delineate the implementation of two prototypes of\nDeepCloud, their contributions, and potentials for generative design.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 19:45:45 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Bidgoli", "Ardavan", ""], ["Veloso", "Pedro", ""]]}, {"id": "1904.01098", "submitter": "Yunsheng Bai", "authors": "Yunsheng Bai, Hao Ding, Yang Qiao, Agustin Marinovic, Ken Gu, Ting\n  Chen, Yizhou Sun, Wei Wang", "title": "Unsupervised Inductive Graph-Level Representation Learning via\n  Graph-Graph Proximity", "comments": "IJCAI 2019 camera ready version with supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel approach to graph-level representation learning, which\nis to embed an entire graph into a vector space where the embeddings of two\ngraphs preserve their graph-graph proximity. Our approach, UGRAPHEMB, is a\ngeneral framework that provides a novel means to performing graph-level\nembedding in a completely unsupervised and inductive manner. The learned neural\nnetwork can be considered as a function that receives any graph as input,\neither seen or unseen in the training set, and transforms it into an embedding.\nA novel graph-level embedding generation mechanism called Multi-Scale Node\nAttention (MSNA), is proposed. Experiments on five real graph datasets show\nthat UGRAPHEMB achieves competitive accuracy in the tasks of graph\nclassification, similarity ranking, and graph visualization.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 20:33:27 GMT"}, {"version": "v2", "created": "Sun, 2 Jun 2019 22:41:29 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Bai", "Yunsheng", ""], ["Ding", "Hao", ""], ["Qiao", "Yang", ""], ["Marinovic", "Agustin", ""], ["Gu", "Ken", ""], ["Chen", "Ting", ""], ["Sun", "Yizhou", ""], ["Wang", "Wei", ""]]}, {"id": "1904.01125", "submitter": "Melissa Aczon", "authors": "Eugene Laksana, Melissa Aczon, Long Ho, Cameron Carlin, David\n  Ledbetter, Randall Wetzel", "title": "The Impact of Extraneous Variables on the Performance of Recurrent\n  Neural Network Models in Clinical Tasks", "comments": "9 pages (2 in appendix), 2 figures, 5 tables (2 in appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic Medical Records (EMR) are a rich source of patient information,\nincluding measurements reflecting physiologic signs and administered therapies.\nIdentifying which variables are useful in predicting clinical outcomes can be\nchallenging. Advanced algorithms such as deep neural networks were designed to\nprocess high-dimensional inputs containing variables in their measured form,\nthus bypass separate feature selection or engineering steps. We investigated\nthe effect of extraneous input variables on the predictive performance of\nRecurrent Neural Networks (RNN) by including in the input vector extraneous\nvariables randomly drawn from theoretical and empirical distributions. RNN\nmodels using different input vectors (EMR variables; EMR and extraneous\nvariables; extraneous variables only) were trained to predict three clinical\noutcomes: in-ICU mortality, 72-hour ICU re-admission, and 30-day ICU-free days.\nThe measured degradations of the RNN's predictive performance with the addition\nof extraneous variables to EMR variables were negligible.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 21:58:20 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Laksana", "Eugene", ""], ["Aczon", "Melissa", ""], ["Ho", "Long", ""], ["Carlin", "Cameron", ""], ["Ledbetter", "David", ""], ["Wetzel", "Randall", ""]]}, {"id": "1904.01127", "submitter": "Pedro M. Ferreira", "authors": "Nuno Dion\\'isio, Fernando Alves, Pedro M. Ferreira, Alysson Bessani", "title": "Cyberthreat Detection from Twitter using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To be prepared against cyberattacks, most organizations resort to security\ninformation and event management systems to monitor their infrastructures.\nThese systems depend on the timeliness and relevance of the latest updates,\npatches and threats provided by cyberthreat intelligence feeds. Open source\nintelligence platforms, namely social media networks such as Twitter, are\ncapable of aggregating a vast amount of cybersecurity-related sources. To\nprocess such information streams, we require scalable and efficient tools\ncapable of identifying and summarizing relevant information for specified\nassets. This paper presents the processing pipeline of a novel tool that uses\ndeep neural networks to process cybersecurity information received from\nTwitter. A convolutional neural network identifies tweets containing\nsecurity-related information relevant to assets in an IT infrastructure. Then,\na bidirectional long short-term memory network extracts named entities from\nthese tweets to form a security alert or to fill an indicator of compromise.\nThe proposed pipeline achieves an average 94% true positive rate and 91% true\nnegative rate for the classification task and an average F1-score of 92% for\nthe named entity recognition task, across three case study infrastructures.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 22:04:29 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Dion\u00edsio", "Nuno", ""], ["Alves", "Fernando", ""], ["Ferreira", "Pedro M.", ""], ["Bessani", "Alysson", ""]]}, {"id": "1904.01139", "submitter": "Yannick Schroecker", "authors": "Yannick Schroecker, Mel Vecerik, Jonathan Scholz", "title": "Generative predecessor models for sample-efficient imitation learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Generative Predecessor Models for Imitation Learning (GPRIL), a\nnovel imitation learning algorithm that matches the state-action distribution\nto the distribution observed in expert demonstrations, using generative models\nto reason probabilistically about alternative histories of demonstrated states.\nWe show that this approach allows an agent to learn robust policies using only\na small number of expert demonstrations and self-supervised interactions with\nthe environment. We derive this approach from first principles and compare it\nempirically to a state-of-the-art imitation learning method, showing that it\noutperforms or matches its performance on two simulated robot manipulation\ntasks and demonstrate significantly higher sample efficiency by applying the\nalgorithm on a real robot.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 23:13:24 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Schroecker", "Yannick", ""], ["Vecerik", "Mel", ""], ["Scholz", "Jonathan", ""]]}, {"id": "1904.01156", "submitter": "Nikos Kargas", "authors": "Nikos Kargas, Nicholas D. Sidiropoulos", "title": "Learning Mixtures of Smooth Product Distributions: Identifiability and\n  Algorithm", "comments": "accepted to appear in AISTATS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning a mixture model of non-parametric product\ndistributions. The problem of learning a mixture model is that of finding the\ncomponent distributions along with the mixing weights using observed samples\ngenerated from the mixture. The problem is well-studied in the parametric\nsetting, i.e., when the component distributions are members of a parametric\nfamily -- such as Gaussian distributions. In this work, we focus on\nmultivariate mixtures of non-parametric product distributions and propose a\ntwo-stage approach which recovers the component distributions of the mixture\nunder a smoothness condition. Our approach builds upon the identifiability\nproperties of the canonical polyadic (low-rank) decomposition of tensors, in\ntandem with Fourier and Shannon-Nyquist sampling staples from signal\nprocessing. We demonstrate the effectiveness of the approach on synthetic and\nreal datasets.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 00:34:26 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Kargas", "Nikos", ""], ["Sidiropoulos", "Nicholas D.", ""]]}, {"id": "1904.01178", "submitter": "Shahinur Alam", "authors": "Shahinur Alam, Mohammed Yeasin", "title": "Person Identification with Visual Summary for a Safe Access to a Smart\n  Home", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SafeAccess is an integrated system designed to provide easier and safer\naccess to a smart home for people with or without disabilities. The system is\ndesigned to enhance safety and promote the independence of people with\ndisability (i.e., visually impaired). The key functionality of the system\nincludes the detection and identification of human and generating contextual\nvisual summary from the real-time video streams obtained from the cameras\nplaced in strategic locations around the house. In addition, the system\nclassifies human into groups (i.e. friends/families/caregiver versus\nintruders/burglars/unknown). These features allow the user to grant/deny remote\naccess to the premises or ability to call emergency services. In this paper, we\nfocus on designing a prototype system for the smart home and building a robust\nrecognition engine that meets the system criteria and addresses speed,\naccuracy, deployment and environmental challenges under a wide variety of\npractical and real-life situations. To interact with the system, we implemented\na dialog enabled interface to create a personalized profile using face images\nor video of friend/families/caregiver. To improve computational efficiency, we\napply change detection to filter out frames and use Faster-RCNN to detect the\nhuman presence and extract faces using Multitask Cascaded Convolutional\nNetworks (MTCNN). Subsequently, we apply LBP/FaceNet to identify a person and\ngroups by matching extracted faces with the profile. SafeAccess sends a visual\nsummary to the users with an MMS containing a person's name if any match found\nor as \"Unknown\", scene image, facial description, and contextual information.\nSafeAccess identifies friends/families/caregiver versus intruders/unknown with\nan average F-score 0.97 and generates a visual summary from 10 classes with an\naverage accuracy of 98.01%.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 02:25:31 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 23:24:00 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Alam", "Shahinur", ""], ["Yeasin", "Mohammed", ""]]}, {"id": "1904.01184", "submitter": "Zhiming Zhou", "authors": "Zhiming Zhou, Jian Shen, Yuxuan Song, Weinan Zhang, Yong Yu", "title": "Towards Efficient and Unbiased Implementation of Lipschitz Continuity in\n  GANs", "comments": "Submitted to IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lipschitz continuity recently becomes popular in generative adversarial\nnetworks (GANs). It was observed that the Lipschitz regularized discriminator\nleads to improved training stability and sample quality. The mainstream\nimplementations of Lipschitz continuity include gradient penalty and spectral\nnormalization. In this paper, we demonstrate that gradient penalty introduces\nundesired bias, while spectral normalization might be over restrictive. We\naccordingly propose a new method which is efficient and unbiased. Our\nexperiments verify our analysis and show that the proposed method is able to\nachieve successful training in various situations where gradient penalty and\nspectral normalization fail.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 02:57:19 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Zhou", "Zhiming", ""], ["Shen", "Jian", ""], ["Song", "Yuxuan", ""], ["Zhang", "Weinan", ""], ["Yu", "Yong", ""]]}, {"id": "1904.01186", "submitter": "Hanting Chen", "authors": "Hanting Chen, Yunhe Wang, Chang Xu, Zhaohui Yang, Chuanjian Liu, Boxin\n  Shi, Chunjing Xu, Chao Xu, Qi Tian", "title": "Data-Free Learning of Student Networks", "comments": null, "journal-ref": "ICCV 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning portable neural networks is very essential for computer vision for\nthe purpose that pre-trained heavy deep models can be well applied on edge\ndevices such as mobile phones and micro sensors. Most existing deep neural\nnetwork compression and speed-up methods are very effective for training\ncompact deep models, when we can directly access the training dataset. However,\ntraining data for the given deep network are often unavailable due to some\npractice problems (e.g. privacy, legal issue, and transmission), and the\narchitecture of the given network are also unknown except some interfaces. To\nthis end, we propose a novel framework for training efficient deep neural\nnetworks by exploiting generative adversarial networks (GANs). To be specific,\nthe pre-trained teacher networks are regarded as a fixed discriminator and the\ngenerator is utilized for derivating training samples which can obtain the\nmaximum response on the discriminator. Then, an efficient network with smaller\nmodel size and computational complexity is trained using the generated data and\nthe teacher network, simultaneously. Efficient student networks learned using\nthe proposed Data-Free Learning (DAFL) method achieve 92.22% and 74.47%\naccuracies using ResNet-18 without any training data on the CIFAR-10 and\nCIFAR-100 datasets, respectively. Meanwhile, our student network obtains an\n80.56% accuracy on the CelebA benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 03:00:06 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 06:50:22 GMT"}, {"version": "v3", "created": "Thu, 5 Sep 2019 06:54:30 GMT"}, {"version": "v4", "created": "Tue, 31 Dec 2019 06:58:35 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Chen", "Hanting", ""], ["Wang", "Yunhe", ""], ["Xu", "Chang", ""], ["Yang", "Zhaohui", ""], ["Liu", "Chuanjian", ""], ["Shi", "Boxin", ""], ["Xu", "Chunjing", ""], ["Xu", "Chao", ""], ["Tian", "Qi", ""]]}, {"id": "1904.01191", "submitter": "Zaheer Abbas", "authors": "Yi Wan, Zaheer Abbas, Adam White, Martha White and Richard S. Sutton", "title": "Planning with Expectation Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distribution and sample models are two popular model choices in model-based\nreinforcement learning (MBRL). However, learning these models can be\nintractable, particularly when the state and action spaces are large.\nExpectation models, on the other hand, are relatively easier to learn due to\ntheir compactness and have also been widely used for deterministic\nenvironments. For stochastic environments, it is not obvious how expectation\nmodels can be used for planning as they only partially characterize a\ndistribution. In this paper, we propose a sound way of using approximate\nexpectation models for MBRL. In particular, we 1) show that planning with an\nexpectation model is equivalent to planning with a distribution model if the\nstate value function is linear in state features, 2) analyze two common\nparametrization choices for approximating the expectation: linear and\nnon-linear expectation models, 3) propose a sound model-based policy evaluation\nalgorithm and present its convergence results, and 4) empirically demonstrate\nthe effectiveness of the proposed planning algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 03:25:25 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 04:48:48 GMT"}, {"version": "v3", "created": "Fri, 6 Dec 2019 04:33:31 GMT"}, {"version": "v4", "created": "Wed, 29 Jul 2020 22:40:04 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Wan", "Yi", ""], ["Abbas", "Zaheer", ""], ["White", "Adam", ""], ["White", "Martha", ""], ["Sutton", "Richard S.", ""]]}, {"id": "1904.01200", "submitter": "Jesus Tordesillas Torres", "authors": "Jesus Tordesillas, Juncal Arbelaiz", "title": "Personalized Cancer Chemotherapy Schedule: a numerical comparison of\n  performance and robustness in model-based and model-free scheduling\n  methodologies", "comments": "Minor changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning algorithms are gaining popularity in fields in which\noptimal scheduling is important, and oncology is not an exception. The complex\nand uncertain dynamics of cancer limit the performance of traditional\nmodel-based scheduling strategies like Optimal Control. Motivated by the recent\nsuccess of model-free Deep Reinforcement Learning (DRL) in challenging control\ntasks and in the design of medical treatments, we use Deep Q-Network (DQN) and\nDeep Deterministic Policy Gradient (DDPG) to design a personalized cancer\nchemotherapy schedule. We show that both of them succeed in the task and\noutperform the Optimal Control solution in the presence of uncertainty.\nFurthermore, we show that DDPG can exterminate cancer more efficiently than DQN\npresumably due to its continuous action space. Finally, we provide some insight\nregarding the amount of samples required for the training.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 03:52:08 GMT"}, {"version": "v2", "created": "Sat, 6 Apr 2019 15:56:55 GMT"}, {"version": "v3", "created": "Mon, 2 Sep 2019 21:53:06 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Tordesillas", "Jesus", ""], ["Arbelaiz", "Juncal", ""]]}, {"id": "1904.01205", "submitter": "Rosalind Wang", "authors": "Mike Li, X. Rosalind Wang", "title": "Peak Alignment of Gas Chromatography-Mass Spectrometry Data with Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ChromAlignNet, a deep learning model for alignment of peaks in Gas\nChromatography-Mass Spectrometry (GC-MS) data. In GC-MS data, a compound's\nretention time (RT) may not stay fixed across multiple chromatograms. To use\nGC-MS data for biomarker discovery requires alignment of identical analyte's RT\nfrom different samples. Current methods of alignment are all based on a set of\nformal, mathematical rules. We present a solution to GC-MS alignment using deep\nlearning neural networks, which are more adept at complex, fuzzy data sets. We\ntested our model on several GC-MS data sets of various complexities and\nanalysed the alignment results quantitatively. We show the model has very good\nperformance (AUC $\\sim 1$ for simple data sets and AUC $\\sim 0.85$ for very\ncomplex data sets). Further, our model easily outperforms existing algorithms\non complex data sets. Compared with existing methods, ChromAlignNet is very\neasy to use as it requires no user input of reference chromatograms and\nparameters. This method can easily be adapted to other similar data such as\nthose from liquid chromatography. The source code is written in Python and\navailable online.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 04:16:45 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 06:31:27 GMT"}, {"version": "v3", "created": "Tue, 20 Aug 2019 01:21:59 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Li", "Mike", ""], ["Wang", "X. Rosalind", ""]]}, {"id": "1904.01209", "submitter": "Connie Kou", "authors": "Cuong Phuc Ngo, Amadeus Aristo Winarto, Connie Kou Khor Li, Sojeong\n  Park, Farhan Akram, Hwee Kuan Lee", "title": "Fence GAN: Towards Better Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection is a classical problem where the aim is to detect anomalous\ndata that do not belong to the normal data distribution. Current\nstate-of-the-art methods for anomaly detection on complex high-dimensional data\nare based on the generative adversarial network (GAN). However, the traditional\nGAN loss is not directly aligned with the anomaly detection objective: it\nencourages the distribution of the generated samples to overlap with the real\ndata and so the resulting discriminator has been found to be ineffective as an\nanomaly detector. In this paper, we propose simple modifications to the GAN\nloss such that the generated samples lie at the boundary of the real data\ndistribution. With our modified GAN loss, our anomaly detection method, called\nFence GAN (FGAN), directly uses the discriminator score as an anomaly\nthreshold. Our experimental results using the MNIST, CIFAR10 and KDD99 datasets\nshow that Fence GAN yields the best anomaly classification accuracy compared to\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 04:36:15 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Ngo", "Cuong Phuc", ""], ["Winarto", "Amadeus Aristo", ""], ["Li", "Connie Kou Khor", ""], ["Park", "Sojeong", ""], ["Akram", "Farhan", ""], ["Lee", "Hwee Kuan", ""]]}, {"id": "1904.01214", "submitter": "Dong Eui Chang", "authors": "Chang Sik Lee, Dong Eui Chang", "title": "Enhancement of Energy-Based Swing-Up Controller via Entropy Search", "comments": "6 pages, 2019 Asian Control Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An energy based approach for stabilizing a mechanical system has offered a\nsimple yet powerful control scheme. However, since it does not impose such\nstrong constraints on parameter space of the controller, finding appropriate\nparameter values for an optimal controller is known to be hard. This paper\nintends to generate an optimal energy-based controller for swinging up a rotary\ninverted pendulum, also known as the Furuta pendulum, by applying the Bayesian\noptimization called Entropy Search. Simulations and experiments show that the\noptimal controller has an improved performance compared to a nominal controller\nfor various initial conditions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 04:56:09 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 16:31:45 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Lee", "Chang Sik", ""], ["Chang", "Dong Eui", ""]]}, {"id": "1904.01269", "submitter": "Volodya Grancharov", "authors": "Stefano Imoscopi, Volodya Grancharov, Sigurdur Sverrisson, Erlendur\n  Karlsson, Harald Pobloth", "title": "Experiments on Open-Set Speaker Identification with Discriminatively\n  Trained Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a study on discriminative artificial neural network\nclassifiers in the context of open-set speaker identification. Both 2-class and\nmulti-class architectures are tested against the conventional Gaussian mixture\nmodel based classifier on enrolled speaker sets of different sizes. The\nperformance evaluation shows that the multi-class neural network system has\nsuperior performance for large population sizes.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 07:59:49 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Imoscopi", "Stefano", ""], ["Grancharov", "Volodya", ""], ["Sverrisson", "Sigurdur", ""], ["Karlsson", "Erlendur", ""], ["Pobloth", "Harald", ""]]}, {"id": "1904.01334", "submitter": "Konstantin Posch", "authors": "Konstantin Posch, J\\\"urgen Pilz", "title": "Correlated Parameters to Accurately Measure Uncertainty in Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article a novel approach for training deep neural networks using\nBayesian techniques is presented. The Bayesian methodology allows for an easy\nevaluation of model uncertainty and additionally is robust to overfitting.\nThese are commonly the two main problems classical, i.e. non-Bayesian,\narchitectures have to struggle with. The proposed approach applies variational\ninference in order to approximate the intractable posterior distribution. In\nparticular, the variational distribution is defined as product of multiple\nmultivariate normal distributions with tridiagonal covariance matrices. Each\nsingle normal distribution belongs either to the weights, or to the biases\ncorresponding to one network layer. The layer-wise a posteriori variances are\ndefined based on the corresponding expectation values and further the\ncorrelations are assumed to be identical. Therefore, only a few additional\nparameters need to be optimized compared to non-Bayesian settings. The novel\napproach is successfully evaluated on basis of the popular benchmark datasets\nMNIST and CIFAR-10.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 11:06:50 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Posch", "Konstantin", ""], ["Pilz", "J\u00fcrgen", ""]]}, {"id": "1904.01340", "submitter": "Lukas Drude", "authors": "Lukas Drude, Daniel Hasenklever, Reinhold Haeb-Umbach", "title": "Unsupervised training of a deep clustering model for multichannel blind\n  source separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a training scheme to train neural network-based source separation\nalgorithms from scratch when parallel clean data is unavailable. In particular,\nwe demonstrate that an unsupervised spatial clustering algorithm is sufficient\nto guide the training of a deep clustering system. We argue that previous work\non deep clustering requires strong supervision and elaborate on why this is a\nlimitation. We demonstrate that (a) the single-channel deep clustering system\ntrained according to the proposed scheme alone is able to achieve a similar\nperformance as the multi-channel teacher in terms of word error rates and (b)\ninitializing the spatial clustering approach with the deep clustering result\nyields a relative word error rate reduction of 26 % over the unsupervised\nteacher.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 11:25:27 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Drude", "Lukas", ""], ["Hasenklever", "Daniel", ""], ["Haeb-Umbach", "Reinhold", ""]]}, {"id": "1904.01341", "submitter": "Vinod Kumar Kurmi", "authors": "Vinod Kumar Kurmi and Vinay P. Namboodiri", "title": "Looking back at Labels: A Class based Domain Adaptation Technique", "comments": "IJCNN 2019 Accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we solve the problem of adapting classifiers across domains.\nWe consider the problem of domain adaptation for multi-class classification\nwhere we are provided a labeled set of examples in a source dataset and we are\nprovided a target dataset with no supervision. In this setting, we propose an\nadversarial discriminator based approach. While the approach based on\nadversarial discriminator has been previously proposed; in this paper, we\npresent an informed adversarial discriminator. Our observation relies on the\nanalysis that shows that if the discriminator has access to all the information\navailable including the class structure present in the source dataset, then it\ncan guide the transformation of features of the target set of classes to a more\nstructure adapted space. Using this formulation, we obtain state-of-the-art\nresults for the standard evaluation on benchmark datasets. We further provide\ndetailed analysis which shows that using all the labeled information results in\nan improved domain adaptation.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 11:28:19 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Kurmi", "Vinod Kumar", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "1904.01367", "submitter": "Fengxiang He", "authors": "Fengxiang He, Tongliang Liu, and Dacheng Tao", "title": "Why ResNet Works? Residuals Generalize", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residual connections significantly boost the performance of deep neural\nnetworks. However, there are few theoretical results that address the influence\nof residuals on the hypothesis complexity and the generalization ability of\ndeep neural networks. This paper studies the influence of residual connections\non the hypothesis complexity of the neural network in terms of the covering\nnumber of its hypothesis space. We prove that the upper bound of the covering\nnumber is the same as chain-like neural networks, if the total numbers of the\nweight matrices and nonlinearities are fixed, no matter whether they are in the\nresiduals or not. This result demonstrates that residual connections may not\nincrease the hypothesis complexity of the neural network compared with the\nchain-like counterpart. Based on the upper bound of the covering number, we\nthen obtain an $\\mathcal O(1 / \\sqrt{N})$ margin-based multi-class\ngeneralization bound for ResNet, as an exemplary case of any deep neural\nnetwork with residual connections. Generalization guarantees for similar\nstate-of-the-art neural network architectures, such as DenseNet and ResNeXt,\nare straight-forward. From our generalization bound, a practical implementation\nis summarized: to approach a good generalization ability, we need to use\nregularization terms to control the magnitude of the norms of weight matrices\nnot to increase too much, which justifies the standard technique of weight\ndecay.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 12:20:33 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["He", "Fengxiang", ""], ["Liu", "Tongliang", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.01376", "submitter": "Jindong Wang", "authors": "Jindong Wang, Yiqiang Chen, Han Yu, Meiyu Huang, Qiang Yang", "title": "Easy Transfer Learning By Exploiting Intra-domain Structures", "comments": "Camera-ready version of IEEE International Conference on Multimedia\n  and Expo (ICME) 2019; code available at\n  http://transferlearning.xyz/code/traditional/EasyTL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning aims at transferring knowledge from a well-labeled domain\nto a similar but different domain with limited or no labels. Unfortunately,\nexisting learning-based methods often involve intensive model selection and\nhyperparameter tuning to obtain good results. Moreover, cross-validation is not\npossible for tuning hyperparameters since there are often no labels in the\ntarget domain. This would restrict wide applicability of transfer learning\nespecially in computationally-constraint devices such as wearables. In this\npaper, we propose a practically Easy Transfer Learning (EasyTL) approach which\nrequires no model selection and hyperparameter tuning, while achieving\ncompetitive performance. By exploiting intra-domain structures, EasyTL is able\nto learn both non-parametric transfer features and classifiers. Extensive\nexperiments demonstrate that, compared to state-of-the-art traditional and deep\nmethods, EasyTL satisfies the Occam's Razor principle: it is extremely easy to\nimplement and use while achieving comparable or better performance in\nclassification accuracy and much better computational efficiency. Additionally,\nit is shown that EasyTL can increase the performance of existing transfer\nfeature learning methods.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 12:43:53 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 02:44:33 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Wang", "Jindong", ""], ["Chen", "Yiqiang", ""], ["Yu", "Han", ""], ["Huang", "Meiyu", ""], ["Yang", "Qiang", ""]]}, {"id": "1904.01385", "submitter": "James Bagrow", "authors": "Andrew J. Becker and James P. Bagrow", "title": "UAFS: Uncertainty-Aware Feature Selection for Problems with Missing Data", "comments": "Withdrawn due to errors in theoretical derivations", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing data are a concern in many real world data sets and imputation\nmethods are often needed to estimate the values of missing data, but data sets\nwith excessive missingness and high dimensionality challenge most approaches to\nimputation. Here we show that appropriate feature selection can be an effective\npreprocessing step for imputation, allowing for more accurate imputation and\nsubsequent model predictions. The key feature of this preprocessing is that it\nincorporates uncertainty: by accounting for uncertainty due to missingness when\nselecting features we can reduce the degree of missingness while also limiting\nthe number of uninformative features being used to make predictive models. We\nintroduce a method to perform uncertainty-aware feature selection (UAFS),\nprovide a theoretical motivation, and test UAFS on both real and synthetic\nproblems, demonstrating that across a variety of data sets and levels of\nmissingness we can improve the accuracy of imputations. Improved imputation due\nto UAFS also results in improved prediction accuracy when performing supervised\nlearning using these imputed data sets. Our UAFS method is general and can be\nfruitfully coupled with a variety of imputation methods.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 13:02:18 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 21:22:07 GMT"}, {"version": "v3", "created": "Tue, 20 Apr 2021 18:33:16 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Becker", "Andrew J.", ""], ["Bagrow", "James P.", ""]]}, {"id": "1904.01399", "submitter": "Huan Long", "authors": "Yuting Jia, Haiwen Wang, Shuo Shao, Huan Long, Yunsong Zhou, Xinbing\n  Wang", "title": "On Geometric Structure of Activation Spaces in Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the geometric structure of activation spaces of\nfully connected layers in neural networks and then show applications of this\nstudy. We propose an efficient approximation algorithm to characterize the\nconvex hull of massive points in high dimensional space. Based on this new\nalgorithm, four common geometric properties shared by the activation spaces are\nconcluded, which gives a rather clear description of the activation spaces. We\nthen propose an alternative classification method grounding on the geometric\nstructure description, which works better than neural networks alone.\nSurprisingly, this data classification method can be an indicator of\noverfitting in neural networks. We believe our work reveals several critical\nintrinsic properties of modern neural networks and further gives a new metric\nfor evaluating them.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 13:19:53 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Jia", "Yuting", ""], ["Wang", "Haiwen", ""], ["Shao", "Shuo", ""], ["Long", "Huan", ""], ["Zhou", "Yunsong", ""], ["Wang", "Xinbing", ""]]}, {"id": "1904.01401", "submitter": "David Saltiel", "authors": "Eric Benhamou, David Saltiel, Sebastien Verel, Fabien Teytaud", "title": "BCMA-ES: A Bayesian approach to CMA-ES", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper introduces a novel theoretically sound approach for the celebrated\nCMA-ES algorithm. Assuming the parameters of the multi variate normal\ndistribution for the minimum follow a conjugate prior distribution, we derive\ntheir optimal update at each iteration step. Not only provides this Bayesian\nframework a justification for the update of the CMA-ES algorithm but it also\ngives two new versions of CMA-ES either assuming normal-Wishart or\nnormal-Inverse Wishart priors, depending whether we parametrize the likelihood\nby its covariance or precision matrix. We support our theoretical findings by\nnumerical experiments that show fast convergence of these modified versions of\nCMA-ES.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 13:28:49 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Benhamou", "Eric", ""], ["Saltiel", "David", ""], ["Verel", "Sebastien", ""], ["Teytaud", "Fabien", ""]]}, {"id": "1904.01460", "submitter": "Basheer Qolomany", "authors": "Basheer Qolomany, Ala Al-Fuqaha, Ajay Gupta, Driss Benhaddou, Safaa\n  Alwajidi, Junaid Qadir, Alvis C. Fong", "title": "Leveraging Machine Learning and Big Data for Smart Buildings: A\n  Comprehensive Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future buildings will offer new convenience, comfort, and efficiency\npossibilities to their residents. Changes will occur to the way people live as\ntechnology involves into people's lives and information processing is fully\nintegrated into their daily living activities and objects. The future\nexpectation of smart buildings includes making the residents' experience as\neasy and comfortable as possible. The massive streaming data generated and\ncaptured by smart building appliances and devices contains valuable information\nthat needs to be mined to facilitate timely actions and better decision making.\nMachine learning and big data analytics will undoubtedly play a critical role\nto enable the delivery of such smart services. In this paper, we survey the\narea of smart building with a special focus on the role of techniques from\nmachine learning and big data analytics. This survey also reviews the current\ntrends and challenges faced in the development of smart building services.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 16:29:51 GMT"}, {"version": "v2", "created": "Sun, 19 May 2019 15:39:31 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Qolomany", "Basheer", ""], ["Al-Fuqaha", "Ala", ""], ["Gupta", "Ajay", ""], ["Benhaddou", "Driss", ""], ["Alwajidi", "Safaa", ""], ["Qadir", "Junaid", ""], ["Fong", "Alvis C.", ""]]}, {"id": "1904.01466", "submitter": "David Saltiel", "authors": "Eric Benhamou, David Saltiel, Beatrice Guez, Nicolas Paris", "title": "BCMA-ES II: revisiting Bayesian CMA-ES", "comments": "10 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper revisits the Bayesian CMA-ES and provides updates for normal\nWishart. It emphasizes the difference between a normal and normal inverse\nWishart prior. After some computation, we prove that the only difference relies\nsurprisingly in the expected covariance. We prove that the expected covariance\nshould be lower in the normal Wishart prior model because of the convexity of\nthe inverse. We present a mixture model that generalizes both normal Wishart\nand normal inverse Wishart model. We finally present various numerical\nexperiments to compare both methods as well as the generalized method.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 14:39:17 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 08:03:02 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Benhamou", "Eric", ""], ["Saltiel", "David", ""], ["Guez", "Beatrice", ""], ["Paris", "Nicolas", ""]]}, {"id": "1904.01486", "submitter": "Rafael Chaves", "authors": "Askery Canabarro, Felipe Fernandes Fanchini, Andr\\'e Luiz Malvezzi,\n  Rodrigo Pereira, Rafael Chaves", "title": "Unveiling phase transitions with machine learning", "comments": "15 pages, 8 figures", "journal-ref": "Phys. Rev. B 100, 045129 (2019)", "doi": "10.1103/PhysRevB.100.045129", "report-no": null, "categories": "cond-mat.str-el cond-mat.stat-mech stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classification of phase transitions is a central and challenging task in\ncondensed matter physics. Typically, it relies on the identification of order\nparameters and the analysis of singularities in the free energy and its\nderivatives. Here, we propose an alternative framework to identify quantum\nphase transitions, employing both unsupervised and supervised machine learning\ntechniques. Using the axial next-nearest neighbor Ising (ANNNI) model as a\nbenchmark, we show how unsupervised learning can detect three phases\n(ferromagnetic, paramagnetic, and a cluster of the antiphase with the floating\nphase) as well as two distinct regions within the paramagnetic phase. Employing\nsupervised learning we show that transfer learning becomes possible: a machine\ntrained only with nearest-neighbour interactions can learn to identify a new\ntype of phase occurring when next-nearest-neighbour interactions are\nintroduced. All our results rely on few and low dimensional input data (up to\ntwelve lattice sites), thus providing a computational friendly and general\nframework for the study of phase transitions in many-body systems.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 15:20:43 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Canabarro", "Askery", ""], ["Fanchini", "Felipe Fernandes", ""], ["Malvezzi", "Andr\u00e9 Luiz", ""], ["Pereira", "Rodrigo", ""], ["Chaves", "Rafael", ""]]}, {"id": "1904.01490", "submitter": "Jelena Bradic", "authors": "Davide Viviano and Jelena Bradic", "title": "Synthetic learner: model-free inference on treatments over time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding of the effect of a particular treatment or a policy pertains to\nmany areas of interest -- ranging from political economics, marketing to\nhealth-care and personalized treatment studies. In this paper, we develop a\nnon-parametric, model-free test for detecting the effects of treatment over\ntime that extends widely used Synthetic Control tests. The test is built on\ncounterfactual predictions arising from many learning algorithms. In the\nNeyman-Rubin potential outcome framework with possible carry-over effects, we\nshow that the proposed test is asymptotically consistent for stationary, beta\nmixing processes. We do not assume that class of learners captures the correct\nmodel necessarily. We also discuss estimates of the average treatment effect,\nand we provide regret bounds on the predictive performance. To the best of our\nknowledge, this is the first set of results that allow for example any Random\nForest to be useful for provably valid statistical inference in the Synthetic\nControl setting. In experiments, we show that our Synthetic Learner is\nsubstantially more powerful than classical methods based on Synthetic Control\nor Difference-in-Differences, especially in the presence of non-linear outcome\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 15:28:21 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Viviano", "Davide", ""], ["Bradic", "Jelena", ""]]}, {"id": "1904.01508", "submitter": "Stavros Shiaeles Dr", "authors": "Michael Siracusano, Stavros Shiaeles, Bogdan Ghita", "title": "Detection of LDDoS Attacks Based on TCP Connection Parameters", "comments": null, "journal-ref": null, "doi": "10.1109/GIIS.2018.8635701", "report-no": null, "categories": "cs.NI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rate application layer distributed denial of service (LDDoS) attacks are\nboth powerful and stealthy. They force vulnerable webservers to open all\navailable connections to the adversary, denying resources to real users.\nMitigation advice focuses on solutions that potentially degrade quality of\nservice for legitimate connections. Furthermore, without accurate detection\nmechanisms, distributed attacks can bypass these defences. A methodology for\ndetection of LDDoS attacks, based on characteristics of malicious TCP flows, is\nproposed within this paper. Research will be conducted using combinations of\ntwo datasets: one generated from a simulated network, the other from the\npublically available CIC DoS dataset. Both contain the attacks slowread,\nslowheaders and slowbody, alongside legitimate web browsing. TCP flow features\nare extracted from all connections. Experimentation was carried out using six\nsupervised AI algorithms to categorise attack from legitimate flows. Decision\ntrees and k-NN accurately classified up to 99.99% of flows, with exceptionally\nlow false positive and false negative rates, demonstrating the potential of AI\nin LDDoS detection.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 16:56:17 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Siracusano", "Michael", ""], ["Shiaeles", "Stavros", ""], ["Ghita", "Bogdan", ""]]}, {"id": "1904.01509", "submitter": "Jian Xue", "authors": "Yanfu Yan, Ke Lu, Jian Xue, Pengcheng Gao, Jiayi Lyu", "title": "FEAFA: A Well-Annotated Dataset for Facial Expression Analysis and 3D\n  Facial Animation", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expression analysis based on machine learning requires large number of\nwell-annotated data to reflect different changes in facial motion. Publicly\navailable datasets truly help to accelerate research in this area by providing\na benchmark resource, but all of these datasets, to the best of our knowledge,\nare limited to rough annotations for action units, including only their\nabsence, presence, or a five-level intensity according to the Facial Action\nCoding System. To meet the need for videos labeled in great detail, we present\na well-annotated dataset named FEAFA for Facial Expression Analysis and 3D\nFacial Animation. One hundred and twenty-two participants, including children,\nyoung adults and elderly people, were recorded in real-world conditions. In\naddition, 99,356 frames were manually labeled using Expression Quantitative\nTool developed by us to quantify 9 symmetrical FACS action units, 10\nasymmetrical (unilateral) FACS action units, 2 symmetrical FACS action\ndescriptors and 2 asymmetrical FACS action descriptors, and each action unit or\naction descriptor is well-annotated with a floating point number between 0 and\n1. To provide a baseline for use in future research, a benchmark for the\nregression of action unit values based on Convolutional Neural Networks are\npresented. We also demonstrate the potential of our FEAFA dataset for 3D facial\nanimation. Almost all state-of-the-art algorithms for facial animation are\nachieved based on 3D face reconstruction. We hence propose a novel method that\ndrives virtual characters only based on action unit value regression of the 2D\nvideo frames of source actors.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 15:50:11 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Yan", "Yanfu", ""], ["Lu", "Ke", ""], ["Xue", "Jian", ""], ["Gao", "Pengcheng", ""], ["Lyu", "Jiayi", ""]]}, {"id": "1904.01517", "submitter": "Benjamin Fehrman", "authors": "Benjamin Fehrman, Benjamin Gess, Arnulf Jentzen", "title": "Convergence rates for the stochastic gradient descent method for\n  non-convex objective functions", "comments": "59 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove the local convergence to minima and estimates on the rate of\nconvergence for the stochastic gradient descent method in the case of not\nnecessarily globally convex nor contracting objective functions. In particular,\nthe results are applicable to simple objective functions arising in machine\nlearning.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 15:55:32 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 17:34:04 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Fehrman", "Benjamin", ""], ["Gess", "Benjamin", ""], ["Jentzen", "Arnulf", ""]]}, {"id": "1904.01543", "submitter": "Christopher Morris", "authors": "Christopher Morris, Gaurav Rattan, Petra Mutzel", "title": "Weisfeiler and Leman go sparse: Towards scalable higher-order graph\n  embeddings", "comments": "Accepted at NeurIPS 2020, extented version with proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph kernels based on the $1$-dimensional Weisfeiler-Leman algorithm and\ncorresponding neural architectures recently emerged as powerful tools for\n(supervised) learning with graphs. However, due to the purely local nature of\nthe algorithms, they might miss essential patterns in the given data and can\nonly handle binary relations. The $k$-dimensional Weisfeiler-Leman algorithm\naddresses this by considering $k$-tuples, defined over the set of vertices, and\ndefines a suitable notion of adjacency between these vertex tuples. Hence, it\naccounts for the higher-order interactions between vertices. However, it does\nnot scale and may suffer from overfitting when used in a machine learning\nsetting. Hence, it remains an important open problem to design WL-based graph\nlearning methods that are simultaneously expressive, scalable, and\nnon-overfitting. Here, we propose local variants and corresponding neural\narchitectures, which consider a subset of the original neighborhood, making\nthem more scalable, and less prone to overfitting. The expressive power of (one\nof) our algorithms is strictly higher than the original algorithm, in terms of\nability to distinguish non-isomorphic graphs. Our experimental study confirms\nthat the local algorithms, both kernel and neural architectures, lead to vastly\nreduced computation times, and prevent overfitting. The kernel version\nestablishes a new state-of-the-art for graph classification on a wide range of\nbenchmark datasets, while the neural version shows promising performance on\nlarge-scale molecular regression tasks.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 16:59:19 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 17:23:50 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2020 15:54:46 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Morris", "Christopher", ""], ["Rattan", "Gaurav", ""], ["Mutzel", "Petra", ""]]}, {"id": "1904.01555", "submitter": "Amir Ziai", "authors": "Amir Ziai", "title": "Active Learning for Network Intrusion Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network operators are generally aware of common attack vectors that they\ndefend against. For most networks the vast majority of traffic is legitimate.\nHowever new attack vectors are continually designed and attempted by bad actors\nwhich bypass detection and go unnoticed due to low volume. One strategy for\nfinding such activity is to look for anomalous behavior. Investigating\nanomalous behavior requires significant time and resources. Collecting a large\nnumber of labeled examples for training supervised models is both prohibitively\nexpensive and subject to obsoletion as new attacks surface. A purely\nunsupervised methodology is ideal; however, research has shown that even a very\nsmall number of labeled examples can significantly improve the quality of\nanomaly detection. A methodology that minimizes the number of required labels\nwhile maximizing the quality of detection is desirable. False positives in this\ncontext result in wasted effort or blockage of legitimate traffic and false\nnegatives translate to undetected attacks. We propose a general active learning\nframework and experiment with different choices of learners and sampling\nstrategies.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 17:22:39 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Ziai", "Amir", ""]]}, {"id": "1904.01557", "submitter": "David Saxton", "authors": "David Saxton, Edward Grefenstette, Felix Hill, Pushmeet Kohli", "title": "Analysing Mathematical Reasoning Abilities of Neural Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical reasoning---a core ability within human intelligence---presents\nsome unique challenges as a domain: we do not come to understand and solve\nmathematical problems primarily on the back of experience and evidence, but on\nthe basis of inferring, learning, and exploiting laws, axioms, and symbol\nmanipulation rules. In this paper, we present a new challenge for the\nevaluation (and eventually the design) of neural architectures and similar\nsystem, developing a task suite of mathematics problems involving sequential\nquestions and answers in a free-form textual input/output format. The\nstructured nature of the mathematics domain, covering arithmetic, algebra,\nprobability and calculus, enables the construction of training and test splits\ndesigned to clearly illuminate the capabilities and failure-modes of different\narchitectures, as well as evaluate their ability to compose and relate\nknowledge and learned processes. Having described the data generation process\nand its potential future expansions, we conduct a comprehensive analysis of\nmodels from two broad classes of the most powerful sequence-to-sequence\narchitectures and find notable differences in their ability to resolve\nmathematical problems and generalize their knowledge.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 17:26:41 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Saxton", "David", ""], ["Grefenstette", "Edward", ""], ["Hill", "Felix", ""], ["Kohli", "Pushmeet", ""]]}, {"id": "1904.01561", "submitter": "Kevin Yang", "authors": "Kevin Yang, Kyle Swanson, Wengong Jin, Connor Coley, Philipp Eiden,\n  Hua Gao, Angel Guzman-Perez, Timothy Hopper, Brian Kelley, Miriam Mathea,\n  Andrew Palmer, Volker Settels, Tommi Jaakkola, Klavs Jensen, Regina Barzilay", "title": "Analyzing Learned Molecular Representations for Property Prediction", "comments": null, "journal-ref": "Journal of chemical information and modeling 59.8 (2019):\n  3370-3388", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advancements in neural machinery have led to a wide range of algorithmic\nsolutions for molecular property prediction. Two classes of models in\nparticular have yielded promising results: neural networks applied to computed\nmolecular fingerprints or expert-crafted descriptors, and graph convolutional\nneural networks that construct a learned molecular representation by operating\non the graph structure of the molecule. However, recent literature has yet to\nclearly determine which of these two methods is superior when generalizing to\nnew chemical space. Furthermore, prior research has rarely examined these new\nmodels in industry research settings in comparison to existing employed models.\nIn this paper, we benchmark models extensively on 19 public and 16 proprietary\nindustrial datasets spanning a wide variety of chemical endpoints. In addition,\nwe introduce a graph convolutional model that consistently matches or\noutperforms models using fixed molecular descriptors as well as previous graph\nneural architectures on both public and proprietary datasets. Our empirical\nfindings indicate that while approaches based on these representations have yet\nto reach the level of experimental reproducibility, our proposed model\nnevertheless offers significant improvements over models currently used in\nindustrial workflows.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 17:35:27 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 18:13:11 GMT"}, {"version": "v3", "created": "Fri, 12 Jul 2019 23:11:43 GMT"}, {"version": "v4", "created": "Tue, 30 Jul 2019 17:36:39 GMT"}, {"version": "v5", "created": "Wed, 20 Nov 2019 19:51:07 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Yang", "Kevin", ""], ["Swanson", "Kyle", ""], ["Jin", "Wengong", ""], ["Coley", "Connor", ""], ["Eiden", "Philipp", ""], ["Gao", "Hua", ""], ["Guzman-Perez", "Angel", ""], ["Hopper", "Timothy", ""], ["Kelley", "Brian", ""], ["Mathea", "Miriam", ""], ["Palmer", "Andrew", ""], ["Settels", "Volker", ""], ["Jaakkola", "Tommi", ""], ["Jensen", "Klavs", ""], ["Barzilay", "Regina", ""]]}, {"id": "1904.01574", "submitter": "Andreas Kofler", "authors": "Andreas Kofler, Marc Dewey, Tobias Schaeffter, Christian Wald, and\n  Christoph Kolbitsch", "title": "Spatio-Temporal Deep Learning-Based Undersampling Artefact Reduction for\n  2D Radial Cine MRI with Limited Data", "comments": "To be published in IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": "10.1109/TMI.2019.2930318", "report-no": null, "categories": "eess.IV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we reduce undersampling artefacts in two-dimensional ($2D$)\ngolden-angle radial cine cardiac MRI by applying a modified version of the\nU-net. We train the network on $2D$ spatio-temporal slices which are previously\nextracted from the image sequences. We compare our approach to two $2D$ and a\n$3D$ Deep Learning-based post processing methods and to three iterative\nreconstruction methods for dynamic cardiac MRI. Our method outperforms the $2D$\nspatially trained U-net and the $2D$ spatio-temporal U-net. Compared to the\n$3D$ spatio-temporal U-net, our method delivers comparable results, but with\nshorter training times and less training data. Compared to the Compressed\nSensing-based methods $kt$-FOCUSS and a total variation regularised\nreconstruction approach, our method improves image quality with respect to all\nreported metrics. Further, it achieves competitive results when compared to an\niterative reconstruction method based on adaptive regularization with\nDictionary Learning and total variation, while only requiring a small fraction\nof the computational time. A persistent homology analysis demonstrates that the\ndata manifold of the spatio-temporal domain has a lower complexity than the\nspatial domain and therefore, the learning of a projection-like mapping is\nfacilitated. Even when trained on only one single subject without\ndata-augmentation, our approach yields results which are similar to the ones\nobtained on a large training dataset. This makes the method particularly\nsuitable for training a network on limited training data. Finally, in contrast\nto the spatial $2D$ U-net, our proposed method is shown to be naturally robust\nwith respect to image rotation in image space and almost achieves\nrotation-equivariance where neither data-augmentation nor a particular network\ndesign are required.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 20:16:42 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 13:18:45 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Kofler", "Andreas", ""], ["Dewey", "Marc", ""], ["Schaeffter", "Tobias", ""], ["Wald", "Christian", ""], ["Kolbitsch", "Christoph", ""]]}, {"id": "1904.01578", "submitter": "Lukas Drude", "authors": "Lukas Drude, Jahn Heymann, Reinhold Haeb-Umbach", "title": "Unsupervised training of neural mask-based beamforming", "comments": "Correction to Eq. 11: Hermite symbol was on the wrong variable.\n  Replaces y with the normalized version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an unsupervised training approach for a neural network-based mask\nestimator in an acoustic beamforming application. The network is trained to\nmaximize a likelihood criterion derived from a spatial mixture model of the\nobservations. It is trained from scratch without requiring any parallel data\nconsisting of degraded input and clean training targets. Thus, training can be\ncarried out on real recordings of noisy speech rather than simulated ones. In\ncontrast to previous work on unsupervised training of neural mask estimators,\nour approach avoids the need for a possibly pre-trained teacher model entirely.\nWe demonstrate the effectiveness of our approach by speech recognition\nexperiments on two different datasets: one mainly deteriorated by noise (CHiME\n4) and one by reverberation (REVERB). The results show that the performance of\nthe proposed system is on par with a supervised system using oracle target\nmasks for training and with a system trained using a model-based teacher.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 12:10:23 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 12:00:46 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Drude", "Lukas", ""], ["Heymann", "Jahn", ""], ["Haeb-Umbach", "Reinhold", ""]]}, {"id": "1904.01612", "submitter": "Yanwu Xu", "authors": "Yanwu Xu, Mingming Gong, Junxiang Chen, Tongliang Liu, Kun Zhang, and\n  Kayhan Batmanghelich", "title": "Generative-Discriminative Complementary Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Majority of state-of-the-art deep learning methods are discriminative\napproaches, which model the conditional distribution of labels given inputs\nfeatures. The success of such approaches heavily depends on high-quality\nlabeled instances, which are not easy to obtain, especially as the number of\ncandidate classes increases. In this paper, we study the complementary learning\nproblem. Unlike ordinary labels, complementary labels are easy to obtain\nbecause an annotator only needs to provide a yes/no answer to a randomly chosen\ncandidate class for each instance. We propose a generative-discriminative\ncomplementary learning method that estimates the ordinary labels by modeling\nboth the conditional (discriminative) and instance (generative) distributions.\nOur method, we call Complementary Conditional GAN (CCGAN), improves the\naccuracy of predicting ordinary labels and can generate high-quality instances\nin spite of weak supervision. In addition to the extensive empirical studies,\nwe also theoretically show that our model can retrieve the true conditional\ndistribution from the complementarily-labeled data.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 18:37:13 GMT"}, {"version": "v2", "created": "Sun, 7 Apr 2019 14:57:36 GMT"}, {"version": "v3", "created": "Tue, 16 Apr 2019 16:58:20 GMT"}, {"version": "v4", "created": "Wed, 11 Sep 2019 20:23:21 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Xu", "Yanwu", ""], ["Gong", "Mingming", ""], ["Chen", "Junxiang", ""], ["Liu", "Tongliang", ""], ["Zhang", "Kun", ""], ["Batmanghelich", "Kayhan", ""]]}, {"id": "1904.01624", "submitter": "Sree Hari Krishnan Parthasarathi", "authors": "Sree Hari Krishnan Parthasarathi and Nikko Strom", "title": "Lessons from Building Acoustic Models with a Million Hours of Speech", "comments": "\"Copyright 2019 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a report of our lessons learned building acoustic models from 1\nMillion hours of unlabeled speech, while labeled speech is restricted to 7,000\nhours. We employ student/teacher training on unlabeled data, helping scale out\ntarget generation in comparison to confidence model based methods, which\nrequire a decoder and a confidence model. To optimize storage and to\nparallelize target generation, we store high valued logits from the teacher\nmodel. Introducing the notion of scheduled learning, we interleave learning on\nunlabeled and labeled data. To scale distributed training across a large number\nof GPUs, we use BMUF with 64 GPUs, while performing sequence training only on\nlabeled data with gradient threshold compression SGD using 16 GPUs. Our\nexperiments show that extremely large amounts of data are indeed useful; with\nlittle hyper-parameter tuning, we obtain relative WER improvements in the 10 to\n20% range, with higher gains in noisier conditions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 18:58:41 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Parthasarathi", "Sree Hari Krishnan", ""], ["Strom", "Nikko", ""]]}, {"id": "1904.01631", "submitter": "Anthony Hsu", "authors": "Anthony Hsu, Keqiu Hu, Jonathan Hung, Arun Suresh, Zhe Zhang", "title": "TonY: An Orchestrator for Distributed Machine Learning Jobs", "comments": "2 pages, to be published in OpML '19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training machine learning (ML) models on large datasets requires considerable\ncomputing power. To speed up training, it is typical to distribute training\nacross several machines, often with specialized hardware like GPUs or TPUs.\nManaging a distributed training job is complex and requires dealing with\nresource contention, distributed configurations, monitoring, and fault\ntolerance. In this paper, we describe TonY, an open-source orchestrator for\ndistributed ML jobs built at LinkedIn to address these challenges.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 00:18:21 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Hsu", "Anthony", ""], ["Hu", "Keqiu", ""], ["Hung", "Jonathan", ""], ["Suresh", "Arun", ""], ["Zhang", "Zhe", ""]]}, {"id": "1904.01638", "submitter": "Li Yao", "authors": "Li Yao, Jordan Prosky, Ben Covington, Kevin Lyman", "title": "A Strong Baseline for Domain Adaptation and Generalization in Medical\n  Imaging", "comments": "Extended abstract of a journal submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work provides a strong baseline for the problem of multi-source\nmulti-target domain adaptation and generalization in medical imaging. Using a\ndiverse collection of ten chest X-ray datasets, we empirically demonstrate the\nbenefits of training medical imaging deep learning models on varied patient\npopulations for generalization to out-of-sample domains.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 19:38:34 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Yao", "Li", ""], ["Prosky", "Jordan", ""], ["Covington", "Ben", ""], ["Lyman", "Kevin", ""]]}, {"id": "1904.01643", "submitter": "Karel Mundnich", "authors": "Karel Mundnich and Brandon M. Booth and Benjamin Girault and Shrikanth\n  Narayanan", "title": "Generating Labels for Regression of Subjective Constructs using Triplet\n  Embeddings", "comments": "9 pages, 5 figures, accepted journal paper", "journal-ref": "Pattern Recognition Letters Volume 128, 2019, Pages 385-392", "doi": "10.1016/j.patrec.2019.10.003", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human annotations serve an important role in computational models where the\ntarget constructs under study are hidden, such as dimensions of affect. This is\nespecially relevant in machine learning, where subjective labels derived from\nrelated observable signals (e.g., audio, video, text) are needed to support\nmodel training and testing. Current research trends focus on correcting\nartifacts and biases introduced by annotators during the annotation process\nwhile fusing them into a single annotation. In this work, we propose a novel\nannotation approach using triplet embeddings. By lifting the absolute\nannotation process to relative annotations where the annotator compares\nindividual target constructs in triplets, we leverage the accuracy of\ncomparisons over absolute ratings by human annotators. We then build a\n1-dimensional embedding in Euclidean space that is indexed in time and serves\nas a label for regression. In this setting, the annotation fusion occurs\nnaturally as a union of sets of sampled triplet comparisons among different\nannotators. We show that by using our proposed sampling method to find an\nembedding, we are able to accurately represent synthetic hidden constructs in\ntime under noisy sampling conditions. We further validate this approach using\nhuman annotations collected from Mechanical Turk and show that we can recover\nthe underlying structure of the hidden construct up to bias and scaling\nfactors.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 19:53:37 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 17:09:06 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Mundnich", "Karel", ""], ["Booth", "Brandon M.", ""], ["Girault", "Benjamin", ""], ["Narayanan", "Shrikanth", ""]]}, {"id": "1904.01648", "submitter": "Chiwoo Park", "authors": "Chiwoo Park, Peihua Qiu, Jennifer Carpena-N\\'u\\~nez, Rahul Rao,\n  Michael Susner and Benji Maruyama", "title": "Sequential Adaptive Design for Jump Regression Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting input variables or design points for statistical models has been of\ngreat interest in adaptive design and active learning. Motivated by two\nscientific examples, this paper presents a strategy of selecting the design\npoints for a regression model when the underlying regression function is\ndiscontinuous. The first example we undertook was for the purpose of\naccelerating imaging speed in a high resolution material imaging; the second\nwas use of sequential design for the purpose of mapping a chemical phase\ndiagram. In both examples, the underlying regression functions have\ndiscontinuities, so many of the existing design optimization approaches cannot\nbe applied because they mostly assume a continuous regression function.\nAlthough some existing adaptive design strategies developed from treed\nregression models can handle the discontinuities, the Bayesian approaches come\nwith computationally expensive Markov Chain Monte Carlo techniques for\nposterior inferences and subsequent design point selections, which is not\nappropriate for the first motivating example that requires computation at least\nfaster than the original imaging speed. In addition, the treed models are based\non the domain partitioning that are inefficient when the discontinuities occurs\nover complex sub-domain boundaries. We propose a simple and effective adaptive\ndesign strategy for a regression analysis with discontinuities: some\nstatistical properties with a fixed design will be presented first, and then\nthese properties will be used to propose a new criterion of selecting the\ndesign points for the regression analysis. Sequential design with the new\ncriterion will be presented with comprehensive simulated examples, and its\napplication to the two motivating examples will be presented.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 20:14:47 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 13:01:09 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 19:00:31 GMT"}, {"version": "v4", "created": "Wed, 10 Feb 2021 20:22:36 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Park", "Chiwoo", ""], ["Qiu", "Peihua", ""], ["Carpena-N\u00fa\u00f1ez", "Jennifer", ""], ["Rao", "Rahul", ""], ["Susner", "Michael", ""], ["Maruyama", "Benji", ""]]}, {"id": "1904.01670", "submitter": "Daniel Jakubovitz", "authors": "Daniel Jakubovitz, Miguel R. D. Rodrigues, Raja Giryes", "title": "Lautum Regularization for Semi-supervised Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning is a very important tool in deep learning as it allows\npropagating information from one \"source dataset\" to another \"target dataset\",\nespecially in the case of a small number of training examples in the latter.\nYet, discrepancies between the underlying distributions of the source and\ntarget data are commonplace and are known to have a substantial impact on\nalgorithm performance. In this work we suggest a novel information theoretic\napproach for the analysis of the performance of deep neural networks in the\ncontext of transfer learning. We focus on the task of semi-supervised transfer\nlearning, in which unlabeled samples from the target dataset are available\nduring the network training on the source dataset. Our theory suggests that one\nmay improve the transferability of a deep neural network by imposing a Lautum\ninformation based regularization that relates the network weights to the target\ndata. We demonstrate the effectiveness of the proposed approach in various\ntransfer learning experiments.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 21:18:45 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 21:16:06 GMT"}, {"version": "v3", "created": "Thu, 23 Jan 2020 12:05:11 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Jakubovitz", "Daniel", ""], ["Rodrigues", "Miguel R. D.", ""], ["Giryes", "Raja", ""]]}, {"id": "1904.01681", "submitter": "Emilien Dupont", "authors": "Emilien Dupont, Arnaud Doucet, Yee Whye Teh", "title": "Augmented Neural ODEs", "comments": "NeurIPS camera ready, additional experiments, additional datasets,\n  discussion on relation to other models", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that Neural Ordinary Differential Equations (ODEs) learn\nrepresentations that preserve the topology of the input space and prove that\nthis implies the existence of functions Neural ODEs cannot represent. To\naddress these limitations, we introduce Augmented Neural ODEs which, in\naddition to being more expressive models, are empirically more stable,\ngeneralize better and have a lower computational cost than Neural ODEs.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 21:50:34 GMT"}, {"version": "v2", "created": "Wed, 22 May 2019 10:32:32 GMT"}, {"version": "v3", "created": "Sat, 26 Oct 2019 15:37:14 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Dupont", "Emilien", ""], ["Doucet", "Arnaud", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1904.01685", "submitter": "Jeremy Nixon", "authors": "Jeremy Nixon, Mike Dusenberry, Ghassen Jerfel, Timothy Nguyen,\n  Jeremiah Liu, Linchuan Zhang, Dustin Tran", "title": "Measuring Calibration in Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overconfidence and underconfidence in machine learning classifiers is\nmeasured by calibration: the degree to which the probabilities predicted for\neach class match the accuracy of the classifier on that prediction.\n  How one measures calibration remains a challenge: expected calibration error,\nthe most popular metric, has numerous flaws which we outline, and there is no\nclear empirical understanding of how its choices affect conclusions in\npractice, and what recommendations there are to counteract its flaws.\n  In this paper, we perform a comprehensive empirical study of choices in\ncalibration measures including measuring all probabilities rather than just the\nmaximum prediction, thresholding probability values, class conditionality,\nnumber of bins, bins that are adaptive to the datapoint density, and the norm\nused to compare accuracies to confidences. To analyze the sensitivity of\ncalibration measures, we study the impact of optimizing directly for each\nvariant with recalibration techniques. Across MNIST, Fashion MNIST,\nCIFAR-10/100, and ImageNet, we find that conclusions on the rank ordering of\nrecalibration methods is drastically impacted by the choice of calibration\nmeasure. We find that conditioning on the class leads to more effective\ncalibration evaluations, and that using the L2 norm rather than the L1 norm\nimproves both optimization for calibration metrics and the rank correlation\nmeasuring metric consistency. Adaptive binning schemes lead to more stablity of\nmetric rank ordering when the number of bins vary, and is also recommended. We\nopen source a library for the use of our calibration measures.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 22:10:44 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 18:34:21 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Nixon", "Jeremy", ""], ["Dusenberry", "Mike", ""], ["Jerfel", "Ghassen", ""], ["Nguyen", "Timothy", ""], ["Liu", "Jeremiah", ""], ["Zhang", "Linchuan", ""], ["Tran", "Dustin", ""]]}, {"id": "1904.01720", "submitter": "Marko Vasic", "authors": "Marko Vasic, Aditya Kanade, Petros Maniatis, David Bieber, Rishabh\n  Singh", "title": "Neural Program Repair by Jointly Learning to Localize and Repair", "comments": "ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its potential to improve programmer productivity and software quality,\nautomated program repair has been an active topic of research. Newer techniques\nharness neural networks to learn directly from examples of buggy programs and\ntheir fixes. In this work, we consider a recently identified class of bugs\ncalled variable-misuse bugs. The state-of-the-art solution for variable misuse\nenumerates potential fixes for all possible bug locations in a program, before\nselecting the best prediction. We show that it is beneficial to train a model\nthat jointly and directly localizes and repairs variable-misuse bugs. We\npresent multi-headed pointer networks for this purpose, with one head each for\nlocalization and repair. The experimental results show that the joint model\nsignificantly outperforms an enumerative solution that uses a pointer based\nmodel for repair alone.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 00:57:25 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Vasic", "Marko", ""], ["Kanade", "Aditya", ""], ["Maniatis", "Petros", ""], ["Bieber", "David", ""], ["Singh", "Rishabh", ""]]}, {"id": "1904.01747", "submitter": "Dacheng Tao", "authors": "Ya Li, Xinmei Tian, Tongliang Liu, Dacheng Tao", "title": "On Better Exploring and Exploiting Task Relationships in Multi-Task\n  Learning: Joint Model and Feature Learning", "comments": null, "journal-ref": null, "doi": "10.1109/TNNLS.2017.2690683", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multitask learning (MTL) aims to learn multiple tasks simultaneously through\nthe interdependence between different tasks. The way to measure the relatedness\nbetween tasks is always a popular issue. There are mainly two ways to measure\nrelatedness between tasks: common parameters sharing and common features\nsharing across different tasks. However, these two types of relatedness are\nmainly learned independently, leading to a loss of information. In this paper,\nwe propose a new strategy to measure the relatedness that jointly learns shared\nparameters and shared feature representations. The objective of our proposed\nmethod is to transform the features from different tasks into a common feature\nspace in which the tasks are closely related and the shared parameters can be\nbetter optimized. We give a detailed introduction to our proposed multitask\nlearning method. Additionally, an alternating algorithm is introduced to\noptimize the nonconvex objection. A theoretical bound is given to demonstrate\nthat the relatedness between tasks can be better measured by our proposed\nmultitask learning algorithm. We conduct various experiments to verify the\nsuperiority of the proposed joint model and feature a multitask learning\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 03:14:20 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Li", "Ya", ""], ["Tian", "Xinmei", ""], ["Liu", "Tongliang", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.01750", "submitter": "Cheng Tang", "authors": "Cheng Tang", "title": "Exponentially convergent stochastic k-PCA without variance reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Matrix Krasulina, an algorithm for online k-PCA, by generalizing\nthe classic Krasulina's method (Krasulina, 1969) from vector to matrix case. We\nshow, both theoretically and empirically, that the algorithm naturally adapts\nto data low-rankness and converges exponentially fast to the ground-truth\nprincipal subspace. Notably, our result suggests that despite various recent\nefforts to accelerate the convergence of stochastic-gradient based methods by\nadding a O(n)-time variance reduction step, for the k-PCA problem, a truly\nonline SGD variant suffices to achieve exponential convergence on intrinsically\nlow-rank data.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 03:31:50 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Tang", "Cheng", ""]]}, {"id": "1904.01763", "submitter": "Yanjun Han", "authors": "Zijun Gao, Yanjun Han, Zhimei Ren, Zhengqing Zhou", "title": "Batched Multi-armed Bandits Problem", "comments": "To appear in NeurIPS 2019 as an oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the multi-armed bandit problem in the batched setting\nwhere the employed policy must split data into a small number of batches. While\nthe minimax regret for the two-armed stochastic bandits has been completely\ncharacterized in \\cite{perchet2016batched}, the effect of the number of arms on\nthe regret for the multi-armed case is still open. Moreover, the question\nwhether adaptively chosen batch sizes will help to reduce the regret also\nremains underexplored. In this paper, we propose the BaSE (batched successive\nelimination) policy to achieve the rate-optimal regrets (within logarithmic\nfactors) for batched multi-armed bandits, with matching lower bounds even if\nthe batch sizes are determined in an adaptive manner.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 04:31:43 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 03:08:14 GMT"}, {"version": "v3", "created": "Sat, 26 Oct 2019 21:28:48 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Gao", "Zijun", ""], ["Han", "Yanjun", ""], ["Ren", "Zhimei", ""], ["Zhou", "Zhengqing", ""]]}, {"id": "1904.01790", "submitter": "Daichi Nishio", "authors": "Daichi Nishio and Satoshi Yamane", "title": "Random Projection in Neural Episodic Control", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end deep reinforcement learning has enabled agents to learn with\nlittle preprocessing by humans. However, it is still difficult to learn stably\nand efficiently because the learning method usually uses a nonlinear function\napproximation. Neural Episodic Control (NEC), which has been proposed in order\nto improve sample efficiency, is able to learn stably by estimating action\nvalues using a non-parametric method. In this paper, we propose an architecture\nthat incorporates random projection into NEC to train with more stability. In\naddition, we verify the effectiveness of our architecture by Atari's five\ngames. The main idea is to reduce the number of parameters that have to learn\nby replacing neural networks with random projection in order to reduce\ndimensions while keeping the learning end-to-end.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 06:17:33 GMT"}, {"version": "v2", "created": "Sun, 14 Apr 2019 10:05:35 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Nishio", "Daichi", ""], ["Yamane", "Satoshi", ""]]}, {"id": "1904.01793", "submitter": "Michael P. Kim", "authors": "Michael P. Kim and Aleksandra Korolova and Guy N. Rothblum and Gal\n  Yona", "title": "Preference-Informed Fairness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study notions of fairness in decision-making systems when individuals have\ndiverse preferences over the possible outcomes of the decisions. Our starting\npoint is the seminal work of Dwork et al. which introduced a notion of\nindividual fairness (IF): given a task-specific similarity metric, every pair\nof individuals who are similarly qualified according to the metric should\nreceive similar outcomes. We show that when individuals have diverse\npreferences over outcomes, requiring IF may unintentionally lead to\nless-preferred outcomes for the very individuals that IF aims to protect. A\nnatural alternative to IF is the classic notion of fair division, envy-freeness\n(EF): no individual should prefer another individual's outcome over their own.\nAlthough EF allows for solutions where all individuals receive a\nhighly-preferred outcome, EF may also be overly-restrictive. For instance, if\nmany individuals agree on the best outcome, then if any individual receives\nthis outcome, they all must receive it, regardless of each individual's\nunderlying qualifications for the outcome.\n  We introduce and study a new notion of preference-informed individual\nfairness (PIIF) that is a relaxation of both individual fairness and\nenvy-freeness. At a high-level, PIIF requires that outcomes satisfy IF-style\nconstraints, but allows for deviations provided they are in line with\nindividuals' preferences. We show that PIIF can permit outcomes that are more\nfavorable to individuals than any IF solution, while providing considerably\nmore flexibility to the decision-maker than EF. In addition, we show how to\nefficiently optimize any convex objective over the outcomes subject to PIIF for\na rich class of individual preferences. Finally, we demonstrate the broad\napplicability of the PIIF framework by extending our definitions and algorithms\nto the multiple-task targeted advertising setting introduced by Dwork and\nIlvento.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 06:29:30 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 18:07:36 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Kim", "Michael P.", ""], ["Korolova", "Aleksandra", ""], ["Rothblum", "Guy N.", ""], ["Yona", "Gal", ""]]}, {"id": "1904.01806", "submitter": "Edward Beeching", "authors": "Edward Beeching and Christian Wolf and Jilles Dibangoye and Olivier\n  Simonin", "title": "Deep Reinforcement Learning on a Budget: 3D Control and Reasoning\n  Without a Supercomputer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important goal of research in Deep Reinforcement Learning in mobile\nrobotics is to train agents capable of solving complex tasks, which require a\nhigh level of scene understanding and reasoning from an egocentric perspective.\nWhen trained from simulations, optimal environments should satisfy a currently\nunobtainable combination of high-fidelity photographic observations, massive\namounts of different environment configurations and fast simulation speeds. In\nthis paper we argue that research on training agents capable of complex\nreasoning can be simplified by decoupling from the requirement of high fidelity\nphotographic observations. We present a suite of tasks requiring complex\nreasoning and exploration in continuous, partially observable 3D environments.\nThe objective is to provide challenging scenarios and a robust baseline agent\narchitecture that can be trained on mid-range consumer hardware in under 24h.\nOur scenarios combine two key advantages: (i) they are based on a simple but\nhighly efficient 3D environment (ViZDoom) which allows high speed simulation\n(12000fps); (ii) the scenarios provide the user with a range of difficulty\nsettings, in order to identify the limitations of current state of the art\nalgorithms and network architectures. We aim to increase accessibility to the\nfield of Deep-RL by providing baselines for challenging scenarios where new\nideas can be iterated on quickly. We argue that the community should be able to\naddress challenging problems in reasoning of mobile agents without the need for\na large compute infrastructure.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 07:15:46 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Beeching", "Edward", ""], ["Wolf", "Christian", ""], ["Dibangoye", "Jilles", ""], ["Simonin", "Olivier", ""]]}, {"id": "1904.01814", "submitter": "Shao-Bo Lin", "authors": "Charles K. Chui, Shao-Bo Lin, Ding-Xuan Zhou", "title": "Deep Neural Networks for Rotation-Invariance Approximation and Learning", "comments": "34 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the tree architecture, the objective of this paper is to design deep\nneural networks with two or more hidden layers (called deep nets) for\nrealization of radial functions so as to enable rotational invariance for\nnear-optimal function approximation in an arbitrarily high dimensional\nEuclidian space. It is shown that deep nets have much better performance than\nshallow nets (with only one hidden layer) in terms of approximation accuracy\nand learning capabilities. In particular, for learning radial functions, it is\nshown that near-optimal rate can be achieved by deep nets but not by shallow\nnets. Our results illustrate the necessity of depth in neural network design\nfor realization of rotation-invariance target functions.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 07:40:40 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Chui", "Charles K.", ""], ["Lin", "Shao-Bo", ""], ["Zhou", "Ding-Xuan", ""]]}, {"id": "1904.01821", "submitter": "Kyung-Su Kim", "authors": "Kyung-Su Kim, Sae-Young Chung", "title": "Fourier Phase Retrieval with Extended Support Estimation via Deep Neural\n  Network", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2019.2935814", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sparse phase retrieval from Fourier transform\nmagnitudes to recover the $k$-sparse signal vector and its support\n$\\mathcal{T}$. We exploit extended support estimate $\\mathcal{E}$ with size\nlarger than $k$ satisfying $\\mathcal{E} \\supseteq \\mathcal{T}$ and obtained by\na trained deep neural network (DNN). To make the DNN learnable, it provides\n$\\mathcal{E}$ as the union of equivalent solutions of $\\mathcal{T}$ by\nutilizing modulo Fourier invariances. Set $\\mathcal{E}$ can be estimated with\nshort running time via the DNN, and support $\\mathcal{T}$ can be determined\nfrom the DNN output rather than from the full index set by applying hard\nthresholding to $\\mathcal{E}$. Thus, the DNN-based extended support estimation\nimproves the reconstruction performance of the signal with a low complexity\nburden dependent on $k$. Numerical results verify that the proposed scheme has\na superior performance with lower complexity compared to local search-based\ngreedy sparse phase retrieval and a state-of-the-art variant of the Fienup\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 07:55:22 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 08:27:44 GMT"}, {"version": "v3", "created": "Tue, 13 Aug 2019 04:52:40 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Kim", "Kyung-Su", ""], ["Chung", "Sae-Young", ""]]}, {"id": "1904.01855", "submitter": "Navid Azizan Ruhi", "authors": "Navid Azizan and Babak Hassibi", "title": "A Stochastic Interpretation of Stochastic Mirror Descent: Risk-Sensitive\n  Optimality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic mirror descent (SMD) is a fairly new family of algorithms that has\nrecently found a wide range of applications in optimization, machine learning,\nand control. It can be considered a generalization of the classical stochastic\ngradient algorithm (SGD), where instead of updating the weight vector along the\nnegative direction of the stochastic gradient, the update is performed in a\n\"mirror domain\" defined by the gradient of a (strictly convex) potential\nfunction. This potential function, and the mirror domain it yields, provides\nconsiderable flexibility in the algorithm compared to SGD. While many\nproperties of SMD have already been obtained in the literature, in this paper\nwe exhibit a new interpretation of SMD, namely that it is a risk-sensitive\noptimal estimator when the unknown weight vector and additive noise are\nnon-Gaussian and belong to the exponential family of distributions. The\nanalysis also suggests a modified version of SMD, which we refer to as\nsymmetric SMD (SSMD). The proofs rely on some simple properties of Bregman\ndivergence, which allow us to extend results from quadratics and Gaussians to\ncertain convex functions and exponential families in a rather seamless way.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 08:57:18 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Azizan", "Navid", ""], ["Hassibi", "Babak", ""]]}, {"id": "1904.01864", "submitter": "Bakht Zaman", "authors": "Bakht Zaman, Luis Miguel Lopez Ramos, Daniel Romero, Baltasar\n  Beferull-Lozano", "title": "Online Topology Identification from Vector Autoregressive Time Series", "comments": "23 pages including supplementary material, submitted to IEEE\n  Transactions on Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causality graphs are routinely estimated in social sciences, natural\nsciences, and engineering due to their capacity to efficiently represent the\nspatiotemporal structure of multivariate data sets in a format amenable for\nhuman interpretation, forecasting, and anomaly detection. A popular approach to\nmathematically formalize causality is based on vector autoregressive (VAR)\nmodels and constitutes an alternative to the well-known, yet usually\nintractable, Granger causality. Relying on such a VAR causality notion, this\npaper develops two algorithms with complementary benefits to track time-varying\ncausality graphs in an online fashion. Their constant complexity per update\nalso renders these algorithms appealing for big-data scenarios. Despite using\ndata sequentially, both algorithms are shown to asymptotically attain the same\naverage performance as a batch estimator which uses the entire data set at\nonce. To this end, sublinear (static) regret bounds are established.\nPerformance is also characterized in time-varying setups by means of dynamic\nregret analysis. Numerical results with real and synthetic data further support\nthe merits of the proposed algorithms in static and dynamic scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 09:10:44 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 15:42:09 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Zaman", "Bakht", ""], ["Ramos", "Luis Miguel Lopez", ""], ["Romero", "Daniel", ""], ["Beferull-Lozano", "Baltasar", ""]]}, {"id": "1904.01934", "submitter": "Fengqi You", "authors": "Chao Ning, Fengqi You", "title": "Optimization under Uncertainty in the Era of Big Data and Deep Learning:\n  When Machine Learning Meets Mathematical Programming", "comments": null, "journal-ref": "Comput. Chem. Eng., Volume 125, 9 June 2019, Pages 434-448", "doi": "10.1016/j.compchemeng.2019.03.034", "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews recent advances in the field of optimization under\nuncertainty via a modern data lens, highlights key research challenges and\npromise of data-driven optimization that organically integrates machine\nlearning and mathematical programming for decision-making under uncertainty,\nand identifies potential research opportunities. A brief review of classical\nmathematical programming techniques for hedging against uncertainty is first\npresented, along with their wide spectrum of applications in Process Systems\nEngineering. A comprehensive review and classification of the relevant\npublications on data-driven distributionally robust optimization, data-driven\nchance constrained program, data-driven robust optimization, and data-driven\nscenario-based optimization is then presented. This paper also identifies\nfertile avenues for future research that focuses on a closed-loop data-driven\noptimization framework, which allows the feedback from mathematical programming\nto machine learning, as well as scenario-based optimization leveraging the\npower of deep learning techniques. Perspectives on online learning-based\ndata-driven multistage optimization with a learning-while-optimizing scheme is\npresented.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 11:54:22 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Ning", "Chao", ""], ["You", "Fengqi", ""]]}, {"id": "1904.01949", "submitter": "Antonio H. Ribeiro", "authors": "Ant\\^onio H. Ribeiro, Manoel Horta Ribeiro, Gabriela M.M. Paix\\~ao,\n  Derick M. Oliveira, Paulo R. Gomes, J\\'essica A. Canazart, Milton P. S.\n  Ferreira, Carl R. Andersson, Peter W. Macfarlane, Wagner Meira Jr., Thomas B.\n  Sch\\\"on, Antonio Luiz P. Ribeiro", "title": "Automatic diagnosis of the 12-lead ECG using a deep neural network", "comments": "A preliminary version of this work titled: \"Automatic Diagnosis of\n  Short-Duration 12-Lead ECG using a Deep Convolutional Network \" was presented\n  in the Machine Learning for Health Workshop at NeurIPS 2018 and was made\n  available under a different identifier: arXiv:1811.12194. The current version\n  subsumes all previous versions", "journal-ref": "Nature Communications 11, article number: 1760 (2020)", "doi": "10.1038/s41467-020-15432-4", "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The role of automatic electrocardiogram (ECG) analysis in clinical practice\nis limited by the accuracy of existing models. Deep Neural Networks (DNNs) are\nmodels composed of stacked transformations that learn tasks by examples. This\ntechnology has recently achieved striking success in a variety of task and\nthere are great expectations on how it might improve clinical practice. Here we\npresent a DNN model trained in a dataset with more than 2 million labeled exams\nanalyzed by the Telehealth Network of Minas Gerais and collected under the\nscope of the CODE (Clinical Outcomes in Digital Electrocardiology) study. The\nDNN outperform cardiology resident medical doctors in recognizing 6 types of\nabnormalities in 12-lead ECG recordings, with F1 scores above 80% and\nspecificity over 99%. These results indicate ECG analysis based on DNNs,\npreviously studied in a single-lead setup, generalizes well to 12-lead exams,\ntaking the technology closer to the standard clinical practice.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 03:20:08 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 16:07:33 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Ribeiro", "Ant\u00f4nio H.", ""], ["Ribeiro", "Manoel Horta", ""], ["Paix\u00e3o", "Gabriela M. M.", ""], ["Oliveira", "Derick M.", ""], ["Gomes", "Paulo R.", ""], ["Canazart", "J\u00e9ssica A.", ""], ["Ferreira", "Milton P. S.", ""], ["Andersson", "Carl R.", ""], ["Macfarlane", "Peter W.", ""], ["Meira", "Wagner", "Jr."], ["Sch\u00f6n", "Thomas B.", ""], ["Ribeiro", "Antonio Luiz P.", ""]]}, {"id": "1904.01962", "submitter": "Konstantinos Skianis", "authors": "Konstantinos Skianis, Giannis Nikolentzos, Stratis Limnios, Michalis\n  Vazirgiannis", "title": "Rep the Set: Neural Networks for Learning Set Representations", "comments": "AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several domains, data objects can be decomposed into sets of simpler\nobjects. It is then natural to represent each object as the set of its\ncomponents or parts. Many conventional machine learning algorithms are unable\nto process this kind of representations, since sets may vary in cardinality and\nelements lack a meaningful ordering. In this paper, we present a new neural\nnetwork architecture, called RepSet, that can handle examples that are\nrepresented as sets of vectors. The proposed model computes the correspondences\nbetween an input set and some hidden sets by solving a series of network flow\nproblems. This representation is then fed to a standard neural network\narchitecture to produce the output. The architecture allows end-to-end\ngradient-based learning. We demonstrate RepSet on classification tasks,\nincluding text categorization, and graph classification, and we show that the\nproposed neural network achieves performance better or comparable to\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 12:25:54 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 19:19:12 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Skianis", "Konstantinos", ""], ["Nikolentzos", "Giannis", ""], ["Limnios", "Stratis", ""], ["Vazirgiannis", "Michalis", ""]]}, {"id": "1904.01975", "submitter": "Zhengping Che", "authors": "Zhengping Che, Guangyu Li, Tracy Li, Bo Jiang, Xuefeng Shi, Xinsheng\n  Zhang, Ying Lu, Guobin Wu, Yan Liu, Jieping Ye", "title": "D$^2$-City: A Large-Scale Dashcam Video Dataset of Diverse Traffic\n  Scenarios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driving datasets accelerate the development of intelligent driving and\nrelated computer vision technologies, while substantial and detailed\nannotations serve as fuels and powers to boost the efficacy of such datasets to\nimprove learning-based models. We propose D$^2$-City, a large-scale\ncomprehensive collection of dashcam videos collected by vehicles on DiDi's\nplatform. D$^2$-City contains more than 10000 video clips which deeply reflect\nthe diversity and complexity of real-world traffic scenarios in China. We also\nprovide bounding boxes and tracking annotations of 12 classes of objects in all\nframes of 1000 videos and detection annotations on keyframes for the remainder\nof the videos. Compared with existing datasets, D$^2$-City features data in\nvarying weather, road, and traffic conditions and a huge amount of elaborate\ndetection and tracking annotations. By bringing a diverse set of challenging\ncases to the community, we expect the D$^2$-City dataset will advance the\nperception and related areas of intelligent driving.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 12:40:08 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 06:42:25 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Che", "Zhengping", ""], ["Li", "Guangyu", ""], ["Li", "Tracy", ""], ["Jiang", "Bo", ""], ["Shi", "Xuefeng", ""], ["Zhang", "Xinsheng", ""], ["Lu", "Ying", ""], ["Wu", "Guobin", ""], ["Liu", "Yan", ""], ["Ye", "Jieping", ""]]}, {"id": "1904.02016", "submitter": "Guy W Cole", "authors": "Guy W. Cole and Sinead A. Williamson", "title": "Stochastic Blockmodels with Edge Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic blockmodels allow us to represent networks in terms of a latent\ncommunity structure, often yielding intuitions about the underlying social\nstructure. Typically, this structure is inferred based only on a binary network\nrepresenting the presence or absence of interactions between nodes, which\nlimits the amount of information that can be extracted from the data. In\npractice, many interaction networks contain much more information about the\nrelationship between two nodes. For example, in an email network, the volume of\ncommunication between two users and the content of that communication can give\nus information about both the strength and the nature of their relationship.\n  In this paper, we propose the Topic Blockmodel, a stochastic blockmodel that\nuses a count-based topic model to capture the interaction modalities within and\nbetween latent communities. By explicitly incorporating information sent\nbetween nodes in our network representation, we are able to address questions\nof interest in real-world situations, such as predicting recipients for an\nemail message or inferring the content of an unopened email. Further, by\nconsidering topics associated with a pair of communities, we are better able to\ninterpret the nature of each community and the manner in which it interacts\nwith other communities.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 14:12:40 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Cole", "Guy W.", ""], ["Williamson", "Sinead A.", ""]]}, {"id": "1904.02021", "submitter": "James Smith", "authors": "James Smith, Cameron Taylor, Seth Baer, and Constantine Dovrolis", "title": "Unsupervised Progressive Learning and the STAM Architecture", "comments": "Accepted by the 2021 International Joint Conference on Artificial\n  Intelligence (IJCAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We first pose the Unsupervised Progressive Learning (UPL) problem: an online\nrepresentation learning problem in which the learner observes a non-stationary\nand unlabeled data stream, learning a growing number of features that persist\nover time even though the data is not stored or replayed. To solve the UPL\nproblem we propose the Self-Taught Associative Memory (STAM) architecture.\nLayered hierarchies of STAM modules learn based on a combination of online\nclustering, novelty detection, forgetting outliers, and storing only\nprototypical features rather than specific examples. We evaluate STAM\nrepresentations using clustering and classification tasks. While there are no\nexisting learning scenarios that are directly comparable to UPL, we compare the\nSTAM architecture with two recent continual learning models, Memory Aware\nSynapses (MAS) and Gradient Episodic Memories (GEM), after adapting them in the\nUPL setting.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 14:25:08 GMT"}, {"version": "v2", "created": "Sun, 29 Sep 2019 18:02:19 GMT"}, {"version": "v3", "created": "Thu, 3 Oct 2019 13:43:48 GMT"}, {"version": "v4", "created": "Tue, 18 Feb 2020 05:16:25 GMT"}, {"version": "v5", "created": "Wed, 10 Jun 2020 22:00:27 GMT"}, {"version": "v6", "created": "Thu, 13 May 2021 17:55:25 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Smith", "James", ""], ["Taylor", "Cameron", ""], ["Baer", "Seth", ""], ["Dovrolis", "Constantine", ""]]}, {"id": "1904.02033", "submitter": "Ilya Razenshteyn", "authors": "Hao Chen and Ilaria Chillotti and Yihe Dong and Oxana Poburinnaya and\n  Ilya Razenshteyn and M. Sadegh Riazi", "title": "SANNS: Scaling Up Secure Approximate k-Nearest Neighbors Search", "comments": "18 pages, to appear at USENIX Security Symposium 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-Nearest Neighbor Search ($k$-NNS) is the backbone of several\ncloud-based services such as recommender systems, face recognition, and\ndatabase search on text and images. In these services, the client sends the\nquery to the cloud server and receives the response in which case the query and\nresponse are revealed to the service provider. Such data disclosures are\nunacceptable in several scenarios due to the sensitivity of data and/or privacy\nlaws.\n  In this paper, we introduce SANNS, a system for secure $k$-NNS that keeps\nclient's query and the search result confidential. SANNS comprises two\nprotocols: an optimized linear scan and a protocol based on a novel sublinear\ntime clustering-based algorithm. We prove the security of both protocols in the\nstandard semi-honest model. The protocols are built upon several\nstate-of-the-art cryptographic primitives such as lattice-based additively\nhomomorphic encryption, distributed oblivious RAM, and garbled circuits. We\nprovide several contributions to each of these primitives which are applicable\nto other secure computation tasks. Both of our protocols rely on a new circuit\nfor the approximate top-$k$ selection from $n$ numbers that is built from $O(n\n+ k^2)$ comparators.\n  We have implemented our proposed system and performed extensive experimental\nresults on four datasets in two different computation environments,\ndemonstrating more than $18-31\\times$ faster response time compared to\noptimally implemented protocols from the prior work. Moreover, SANNS is the\nfirst work that scales to the database of 10 million entries, pushing the limit\nby more than two orders of magnitude.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 14:38:11 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 08:53:48 GMT"}, {"version": "v3", "created": "Mon, 8 Jul 2019 15:41:33 GMT"}, {"version": "v4", "created": "Wed, 20 Nov 2019 08:15:50 GMT"}, {"version": "v5", "created": "Sun, 8 Mar 2020 23:59:18 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Chen", "Hao", ""], ["Chillotti", "Ilaria", ""], ["Dong", "Yihe", ""], ["Poburinnaya", "Oxana", ""], ["Razenshteyn", "Ilya", ""], ["Riazi", "M. Sadegh", ""]]}, {"id": "1904.02063", "submitter": "Jeremias Knoblauch", "authors": "Jeremias Knoblauch, Jack Jewson, Theodoros Damoulas", "title": "Generalized Variational Inference: Three arguments for deriving new\n  Posteriors", "comments": "103 pages, 23 figures (comprehensive revision of previous version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We advocate an optimization-centric view on and introduce a novel\ngeneralization of Bayesian inference. Our inspiration is the representation of\nBayes' rule as infinite-dimensional optimization problem (Csiszar, 1975;\nDonsker and Varadhan; 1975, Zellner; 1988). First, we use it to prove an\noptimality result of standard Variational Inference (VI): Under the proposed\nview, the standard Evidence Lower Bound (ELBO) maximizing VI posterior is\npreferable to alternative approximations of the Bayesian posterior. Next, we\nargue for generalizing standard Bayesian inference. The need for this arises in\nsituations of severe misalignment between reality and three assumptions\nunderlying standard Bayesian inference: (1) Well-specified priors, (2)\nwell-specified likelihoods, (3) the availability of infinite computing power.\nOur generalization addresses these shortcomings with three arguments and is\ncalled the Rule of Three (RoT). We derive it axiomatically and recover existing\nposteriors as special cases, including the Bayesian posterior and its\napproximation by standard VI. In contrast, approximations based on alternative\nELBO-like objectives violate the axioms. Finally, we study a special case of\nthe RoT that we call Generalized Variational Inference (GVI). GVI posteriors\nare a large and tractable family of belief distributions specified by three\narguments: A loss, a divergence and a variational family. GVI posteriors have\nappealing properties, including consistency and an interpretation as\napproximate ELBO. The last part of the paper explores some attractive\napplications of GVI in popular machine learning models, including robustness\nand more appropriate marginals. After deriving black box inference schemes for\nGVI posteriors, their predictive performance is investigated on Bayesian Neural\nNetworks and Deep Gaussian Processes, where GVI can comprehensively improve\nupon existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 15:31:46 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 10:48:21 GMT"}, {"version": "v3", "created": "Tue, 21 May 2019 01:53:32 GMT"}, {"version": "v4", "created": "Thu, 12 Dec 2019 15:02:50 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Knoblauch", "Jeremias", ""], ["Jewson", "Jack", ""], ["Damoulas", "Theodoros", ""]]}, {"id": "1904.02064", "submitter": "Byoungwook Jang", "authors": "Byoungwook Jang, Alfred Hero", "title": "Minimum Volume Topic Modeling", "comments": "Accepted in AISTATS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new topic modeling procedure that takes advantage of the fact\nthat the Latent Dirichlet Allocation (LDA) log likelihood function is\nasymptotically equivalent to the logarithm of the volume of the topic simplex.\nThis allows topic modeling to be reformulated as finding the probability\nsimplex that minimizes its volume and encloses the documents that are\nrepresented as distributions over words. A convex relaxation of the minimum\nvolume topic model optimization is proposed, and it is shown that the relaxed\nproblem has the same global minimum as the original problem under the\nseparability assumption and the sufficiently scattered assumption introduced by\nArora et al. (2013) and Huang et al. (2016). A locally convergent alternating\ndirection method of multipliers (ADMM) approach is introduced for solving the\nrelaxed minimum volume problem. Numerical experiments illustrate the benefits\nof our approach in terms of computation time and topic recovery performance.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 15:34:20 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Jang", "Byoungwook", ""], ["Hero", "Alfred", ""]]}, {"id": "1904.02092", "submitter": "Sung Hak Lim", "authors": "Amit Chakraborty, Sung Hak Lim, Mihoko M. Nojiri", "title": "Interpretable Deep Learning for Two-Prong Jet Classification with Jet\n  Spectra", "comments": "32 pages, 21 figures, published in JHEP", "journal-ref": "J. High Energ. Phys. 2019, 135 (2019)", "doi": "10.1007/JHEP07(2019)135", "report-no": "KEK-TH-2117", "categories": "hep-ph hep-ex stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classification of jets with deep learning has gained significant attention in\nrecent times. However, the performance of deep neural networks is often\nachieved at the cost of interpretability. Here we propose an interpretable\nnetwork trained on the jet spectrum $S_{2}(R)$ which is a two-point correlation\nfunction of the jet constituents. The spectrum can be derived from a functional\nTaylor series of an arbitrary jet classifier function of energy flows. An\ninterpretable network can be obtained by truncating the series. The\nintermediate feature of the network is an infrared and collinear safe\nC-correlator which allows us to estimate the importance of a $S_{2}(R)$ deposit\nat an angular scale R in the classification. The performance of the\narchitecture is comparable to that of a convolutional neural network (CNN)\ntrained on jet images, although the number of inputs and complexity of\narchitecture is significantly simpler than the CNN classifier. We consider two\nexamples: one is the classification of two-prong jets which differ in color\ncharge of the mother particle, and the other is a comparison between Pythia 8\nand Herwig 7 generated jets.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 16:37:52 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 08:59:33 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Chakraborty", "Amit", ""], ["Lim", "Sung Hak", ""], ["Nojiri", "Mihoko M.", ""]]}, {"id": "1904.02098", "submitter": "Linying Zhang", "authors": "Linying Zhang, Yixin Wang, Anna Ostropolets, Jami J. Mulgrave, David\n  M. Blei, George Hripcsak", "title": "The Medical Deconfounder: Assessing Treatment Effects with Electronic\n  Health Records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The treatment effects of medications play a key role in guiding medical\nprescriptions. They are usually assessed with randomized controlled trials\n(RCTs), which are expensive. Recently, large-scale electronic health records\n(EHRs) have become available, opening up new opportunities for more\ncost-effective assessments. However, assessing a treatment effect from EHRs is\nchallenging: it is biased by unobserved confounders, unmeasured variables that\naffect both patients' medical prescription and their outcome, e.g. the\npatients' social economic status. To adjust for unobserved confounders, we\ndevelop the medical deconfounder, a machine learning algorithm that unbiasedly\nestimates treatment effects from EHRs. The medical deconfounder first\nconstructs a substitute confounder by modeling which medications were\nprescribed to each patient; this substitute confounder is guaranteed to capture\nall multi-medication confounders, observed or unobserved (arXiv:1805.06826). It\nthen uses this substitute confounder to adjust for the confounding bias in the\nanalysis. We validate the medical deconfounder on two simulated and two real\nmedical data sets. Compared to classical approaches, the medical deconfounder\nproduces closer-to-truth treatment effect estimates; it also identifies\neffective medications that are more consistent with the findings in the medical\nliterature.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 16:49:32 GMT"}, {"version": "v2", "created": "Sat, 17 Aug 2019 15:21:08 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Zhang", "Linying", ""], ["Wang", "Yixin", ""], ["Ostropolets", "Anna", ""], ["Mulgrave", "Jami J.", ""], ["Blei", "David M.", ""], ["Hripcsak", "George", ""]]}, {"id": "1904.02101", "submitter": "Mateusz Staniak", "authors": "Mateusz Staniak and Przemyslaw Biecek", "title": "The Landscape of R Packages for Automated Exploratory Data Analysis", "comments": null, "journal-ref": null, "doi": "10.32614/RJ-2019-033", "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The increasing availability of large but noisy data sets with a large number\nof heterogeneous variables leads to the increasing interest in the automation\nof common tasks for data analysis. The most time-consuming part of this process\nis the Exploratory Data Analysis, crucial for better domain understanding, data\ncleaning, data validation, and feature engineering.\n  There is a growing number of libraries that attempt to automate some of the\ntypical Exploratory Data Analysis tasks to make the search for new insights\neasier and faster. In this paper, we present a systematic review of existing\ntools for Automated Exploratory Data Analysis (autoEDA). We explore the\nfeatures of twelve popular R packages to identify the parts of analysis that\ncan be effectively automated with the current tools and to point out new\ndirections for further autoEDA development.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 09:49:33 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 19:38:14 GMT"}, {"version": "v3", "created": "Wed, 18 Sep 2019 08:27:29 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Staniak", "Mateusz", ""], ["Biecek", "Przemyslaw", ""]]}, {"id": "1904.02130", "submitter": "Krishnakumar Balasubramanian", "authors": "Andreas Anastasiou, Krishnakumar Balasubramanian, Murat A. Erdogdu", "title": "Normal Approximation for Stochastic Gradient Descent via Non-Asymptotic\n  Rates of Martingale CLT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide non-asymptotic convergence rates of the Polyak-Ruppert averaged\nstochastic gradient descent (SGD) to a normal random vector for a class of\ntwice-differentiable test functions. A crucial intermediate step is proving a\nnon-asymptotic martingale central limit theorem (CLT), i.e., establishing the\nrates of convergence of a multivariate martingale difference sequence to a\nnormal random vector, which might be of independent interest. We obtain the\nexplicit rates for the multivariate martingale CLT using a combination of\nStein's method and Lindeberg's argument, which is then used in conjunction with\na non-asymptotic analysis of averaged SGD proposed in [PJ92]. Our results have\npotentially interesting consequences for computing confidence intervals for\nparameter estimation with SGD and constructing hypothesis tests with SGD that\nare valid in a non-asymptotic sense.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 17:47:46 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Anastasiou", "Andreas", ""], ["Balasubramanian", "Krishnakumar", ""], ["Erdogdu", "Murat A.", ""]]}, {"id": "1904.02144", "submitter": "Jianbo Chen", "authors": "Jianbo Chen, Michael I. Jordan, Martin J. Wainwright", "title": "HopSkipJumpAttack: A Query-Efficient Decision-Based Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of a decision-based adversarial attack on a trained model is to\ngenerate adversarial examples based solely on observing output labels returned\nby the targeted model. We develop HopSkipJumpAttack, a family of algorithms\nbased on a novel estimate of the gradient direction using binary information at\nthe decision boundary. The proposed family includes both untargeted and\ntargeted attacks optimized for $\\ell_2$ and $\\ell_\\infty$ similarity metrics\nrespectively. Theoretical analysis is provided for the proposed algorithms and\nthe gradient direction estimate. Experiments show HopSkipJumpAttack requires\nsignificantly fewer model queries than Boundary Attack. It also achieves\ncompetitive performance in attacking several widely-used defense mechanisms.\n(HopSkipJumpAttack was named Boundary Attack++ in a previous version of the\npreprint.)\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 17:59:33 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 10:24:22 GMT"}, {"version": "v3", "created": "Mon, 10 Jun 2019 06:36:02 GMT"}, {"version": "v4", "created": "Tue, 17 Sep 2019 04:39:24 GMT"}, {"version": "v5", "created": "Tue, 28 Apr 2020 01:20:45 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Chen", "Jianbo", ""], ["Jordan", "Michael I.", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1904.02205", "submitter": "David Hartmann", "authors": "David Hartmann, Michael Wand", "title": "Progressive Stochastic Binarization of Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A plethora of recent research has focused on improving the memory footprint\nand inference speed of deep networks by reducing the complexity of (i)\nnumerical representations (for example, by deterministic or stochastic\nquantization) and (ii) arithmetic operations (for example, by binarization of\nweights).\n  We propose a stochastic binarization scheme for deep networks that allows for\nefficient inference on hardware by restricting itself to additions of small\nintegers and fixed shifts. Unlike previous approaches, the underlying\nrandomized approximation is progressive, thus permitting an adaptive control of\nthe accuracy of each operation at run-time. In a low-precision setting, we\nmatch the accuracy of previous binarized approaches. Our representation is\nunbiased - it approaches continuous computation with increasing sample size. In\na high-precision regime, the computational costs are competitive with previous\nquantization schemes. Progressive stochastic binarization also permits\nlocalized, dynamic accuracy control within a single network, thereby providing\na new tool for adaptively focusing computational attention.\n  We evaluate our method on networks of various architectures, already\npretrained on ImageNet. With representational costs comparable to previous\nschemes, we obtain accuracies close to the original floating point\nimplementation. This includes pruned networks, except the known special case of\ncertain types of separated convolutions. By focusing computational attention\nusing progressive sampling, we reduce inference costs on ImageNet further by a\nfactor of up to 33% (before network pruning).\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 19:09:35 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Hartmann", "David", ""], ["Wand", "Michael", ""]]}, {"id": "1904.02206", "submitter": "Gabriel de la Cruz Jr", "authors": "Gabriel V. de la Cruz Jr. and Yunshu Du and Matthew E. Taylor", "title": "Jointly Pre-training with Supervised, Autoencoder, and Value Losses for\n  Deep Reinforcement Learning", "comments": "Accepted in Adaptive and Learning Agents (ALA) Workshop at AAMAS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Reinforcement Learning (DRL) algorithms are known to be data\ninefficient. One reason is that a DRL agent learns both the feature and the\npolicy tabula rasa. Integrating prior knowledge into DRL algorithms is one way\nto improve learning efficiency since it helps to build helpful representations.\nIn this work, we consider incorporating human knowledge to accelerate the\nasynchronous advantage actor-critic (A3C) algorithm by pre-training a small\namount of non-expert human demonstrations. We leverage the supervised\nautoencoder framework and propose a novel pre-training strategy that jointly\ntrains a weighted supervised classification loss, an unsupervised\nreconstruction loss, and an expected return loss. The resulting pre-trained\nmodel learns more useful features compared to independently training in\nsupervised or unsupervised fashion. Our pre-training method drastically\nimproved the learning performance of the A3C agent in Atari games of Pong and\nMsPacman, exceeding the performance of the state-of-the-art algorithms at a\nmuch smaller number of game interactions. Our method is light-weight and easy\nto implement in a single machine. For reproducibility, our code is available at\ngithub.com/gabrieledcjr/DeepRL/tree/A3C-ALA2019\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 19:14:15 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Cruz", "Gabriel V. de la", "Jr."], ["Du", "Yunshu", ""], ["Taylor", "Matthew E.", ""]]}, {"id": "1904.02217", "submitter": "Peter Weiderer", "authors": "Peter Weiderer and Ana Maria Tom\\'e and Elmar Wolfgang Lang", "title": "Decomposing Temperature Time Series with Non-Negative Matrix\n  Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV physics.app-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the fabrication of casting parts sensor data is typically\nautomatically recorded and accumulated for process monitoring and defect\ndiagnosis. As casting is a thermal process with many interacting process\nparameters, root cause analysis tends to be tedious and ineffective. We show\nhow a decomposition based on non-negative matrix factorization (NMF), which is\nguided by a knowledge-based initialization strategy, is able to extract\nphysical meaningful sources from temperature time series collected during a\nthermal manufacturing process. The approach assumes the time series to be\ngenerated by a superposition of several simultaneously acting component\nprocesses. NMF is able to reverse the superposition and to identify the hidden\ncomponent processes. The latter can be linked to ongoing physical phenomena and\nprocess variables, which cannot be monitored directly. Our approach provides\nnew insights into the underlying physics and offers a tool, which can assist in\ndiagnosing defect causes. We demonstrate our method by applying it to real\nworld data, collected in a foundry during the series production of casting\nparts for the automobile industry.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 19:46:56 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Weiderer", "Peter", ""], ["Tom\u00e9", "Ana Maria", ""], ["Lang", "Elmar Wolfgang", ""]]}, {"id": "1904.02243", "submitter": "Emily Storey", "authors": "Emily E Storey, Amr S. Helmy", "title": "Optimized Preprocessing and Machine Learning for Quantitative Raman\n  Spectroscopy in Biology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Raman spectroscopy's capability to provide meaningful composition predictions\nis heavily reliant on a pre-processing step to remove insignificant spectral\nvariation. This is crucial in biofluid analysis. Widespread adoption of\ndiagnostics using Raman requires a robust model which can withstand routine\nspectra discrepancies due to unavoidable variations such as age, diet, and\nmedical background. A wealth of pre-processing methods are available, and it is\noften up to trial-and-error or user experience to select the method which gives\nthe best results. This process can be incredibly time consuming and\ninconsistent for multiple operators.\n  In this study we detail a method to analyze the statistical variability\nwithin a set of training spectra and determine suitability to form a robust\nmodel. This allows us to selectively qualify or exclude a pre-processing\nmethod, predetermine robustness, and simultaneously identify the number of\ncomponents which will form the best predictive model. We demonstrate the\nability of this technique to improve predictive models of two artificial\nbiological fluids.\n  Raman spectroscopy is ideal for noninvasive, nondestructive analysis. Routine\nhealth monitoring which maximizes comfort is increasingly crucial, particularly\nin epidemic-level diabetes diagnoses. High variability in spectra of biological\nsamples can hinder Raman's adoption for these methods. Our technique allows the\ndecision of optimal pre-treatment method to be determined for the operator;\nmodel performance is no longer a function of user experience. We foresee this\nstatistical technique being an instrumental element to widening the adoption of\nRaman as a monitoring tool in a field of biofluid analysis.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 21:24:38 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Storey", "Emily E", ""], ["Helmy", "Amr S.", ""]]}, {"id": "1904.02278", "submitter": "Fengwen Chen", "authors": "Fengwen Chen, Shirui Pan, Jing Jiang, Huan Huo, Guodong Long", "title": "DAGCN: Dual Attention Graph Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph convolutional networks (GCNs) have recently become one of the most\npowerful tools for graph analytics tasks in numerous applications, ranging from\nsocial networks and natural language processing to bioinformatics and\nchemoinformatics, thanks to their ability to capture the complex relationships\nbetween concepts. At present, the vast majority of GCNs use a neighborhood\naggregation framework to learn a continuous and compact vector, then performing\na pooling operation to generalize graph embedding for the classification task.\nThese approaches have two disadvantages in the graph classification task:\n(1)when only the largest sub-graph structure ($k$-hop neighbor) is used for\nneighborhood aggregation, a large amount of early-stage information is lost\nduring the graph convolution step; (2) simple average/sum pooling or max\npooling utilized, which loses the characteristics of each node and the topology\nbetween nodes. In this paper, we propose a novel framework called, dual\nattention graph convolutional networks (DAGCN) to address these problems. DAGCN\nautomatically learns the importance of neighbors at different hops using a\nnovel attention graph convolution layer, and then employs a second attention\ncomponent, a self-attention pooling layer, to generalize the graph\nrepresentation from the various aspects of a matrix graph embedding. The dual\nattention network is trained in an end-to-end manner for the graph\nclassification task. We compare our model with state-of-the-art graph kernels\nand other deep learning methods. The experimental results show that our\nframework not only outperforms other baselines but also achieves a better rate\nof convergence.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 00:04:57 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Chen", "Fengwen", ""], ["Pan", "Shirui", ""], ["Jiang", "Jing", ""], ["Huo", "Huan", ""], ["Long", "Guodong", ""]]}, {"id": "1904.02303", "submitter": "Jeremias Knoblauch", "authors": "Jeremias Knoblauch", "title": "Robust Deep Gaussian Processes", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report provides an in-depth overview over the implications and novelty\nGeneralized Variational Inference (GVI) (Knoblauch et al., 2019) brings to Deep\nGaussian Processes (DGPs) (Damianou & Lawrence, 2013). Specifically, robustness\nto model misspecification as well as principled alternatives for uncertainty\nquantification are motivated with an information-geometric view. These\nmodifications have clear interpretations and can be implemented in less than\n100 lines of Python code. Most importantly, the corresponding empirical results\nshow that DGPs can greatly benefit from the presented enhancements.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 01:37:54 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 02:05:44 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Knoblauch", "Jeremias", ""]]}, {"id": "1904.02309", "submitter": "Roozbeh Farhoodi", "authors": "Roozbeh Farhoodi, Khashayar Filom, Ilenna Simone Jones, Konrad Paul\n  Kording", "title": "On functions computed on trees", "comments": "52 pages, 10 figures. The final version. To appear in Neural\n  Computation. May vary slightly from published version", "journal-ref": "Neural Computation 31 (2019), no. 11, 2075--2137", "doi": "10.1162/neco_a_01231", "report-no": null, "categories": "cs.LG math.CO q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Any function can be constructed using a hierarchy of simpler functions\nthrough compositions. Such a hierarchy can be characterized by a binary rooted\ntree. Each node of this tree is associated with a function which takes as\ninputs two numbers from its children and produces one output. Since thinking\nabout functions in terms of computation graphs is getting popular we may want\nto know which functions can be implemented on a given tree. Here, we describe a\nset of necessary constraints in the form of a system of non-linear partial\ndifferential equations that must be satisfied. Moreover, we prove that these\nconditions are sufficient in both contexts of analytic and bit-valued\nfunctions. In the latter case, we explicitly enumerate discrete functions and\nobserve that there are relatively few. Our point of view allows us to compare\ndifferent neural network architectures in regard to their function spaces. Our\nwork connects the structure of computation graphs with the functions they can\nimplement and has potential applications to neuroscience and computer science.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 02:15:35 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 00:59:32 GMT"}, {"version": "v3", "created": "Tue, 30 Jul 2019 17:57:50 GMT"}, {"version": "v4", "created": "Tue, 22 Oct 2019 17:47:17 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Farhoodi", "Roozbeh", ""], ["Filom", "Khashayar", ""], ["Jones", "Ilenna Simone", ""], ["Kording", "Konrad Paul", ""]]}, {"id": "1904.02338", "submitter": "Maruan Al-Shedivat", "authors": "Maruan Al-Shedivat and Ankur P. Parikh", "title": "Consistency by Agreement in Zero-shot Neural Machine Translation", "comments": "NAACL 2019 (14 pages, 5 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalization and reliability of multilingual translation often highly\ndepend on the amount of available parallel data for each language pair of\ninterest. In this paper, we focus on zero-shot generalization---a challenging\nsetup that tests models on translation directions they have not been optimized\nfor at training time. To solve the problem, we (i) reformulate multilingual\ntranslation as probabilistic inference, (ii) define the notion of zero-shot\nconsistency and show why standard training often results in models unsuitable\nfor zero-shot tasks, and (iii) introduce a consistent agreement-based training\nmethod that encourages the model to produce equivalent translations of parallel\nsentences in auxiliary languages. We test our multilingual NMT models on\nmultiple public zero-shot translation benchmarks (IWSLT17, UN corpus, Europarl)\nand show that agreement-based learning often results in 2-3 BLEU zero-shot\nimprovement over strong baselines without any loss in performance on supervised\ntranslation directions.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 03:49:05 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 04:00:03 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Al-Shedivat", "Maruan", ""], ["Parikh", "Ankur P.", ""]]}, {"id": "1904.02361", "submitter": "Mehran Khodabandeh", "authors": "Mehran Khodabandeh, Arash Vahdat, Mani Ranjbar, William G. Macready", "title": "A Robust Learning Approach to Domain Adaptive Object Detection", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain shift is unavoidable in real-world applications of object detection.\nFor example, in self-driving cars, the target domain consists of unconstrained\nroad environments which cannot all possibly be observed in training data.\nSimilarly, in surveillance applications sufficiently representative training\ndata may be lacking due to privacy regulations. In this paper, we address the\ndomain adaptation problem from the perspective of robust learning and show that\nthe problem may be formulated as training with noisy labels. We propose a\nrobust object detection framework that is resilient to noise in bounding box\nclass labels, locations and size annotations. To adapt to the domain shift, the\nmodel is trained on the target domain using a set of noisy object bounding\nboxes that are obtained by a detection model trained only in the source domain.\nWe evaluate the accuracy of our approach in various source/target domain pairs\nand demonstrate that the model significantly improves the state-of-the-art on\nmultiple domain adaptation scenarios on the SIM10K, Cityscapes and KITTI\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 05:50:10 GMT"}, {"version": "v2", "created": "Sun, 11 Aug 2019 05:24:59 GMT"}, {"version": "v3", "created": "Mon, 18 Nov 2019 05:43:00 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Khodabandeh", "Mehran", ""], ["Vahdat", "Arash", ""], ["Ranjbar", "Mani", ""], ["Macready", "William G.", ""]]}, {"id": "1904.02383", "submitter": "Chanshin Park", "authors": "Chanshin Park, Daniel K. Tettey, and Han-Shin Jo", "title": "Artificial Neural Network Modeling for Path Loss Prediction in Urban\n  Environments", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although various linear log-distance path loss models have been developed,\nadvanced models are requiring to more accurately and flexibly represent the\npath loss for complex environments such as the urban area. This letter proposes\nan artificial neural network (ANN) based multi-dimensional regression framework\nfor path loss modeling in urban environments at 3 to 6 GHz frequency band. ANN\nis used to learn the path loss structure from the measured path loss data which\nis a function of distance and frequency. The effect of the network architecture\nparameter (activation function, the number of hidden layers and nodes) on the\nprediction accuracy are analyzed. We observe that the proposed model is more\naccurate and flexible compared to the conventional linear model.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 07:16:28 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Park", "Chanshin", ""], ["Tettey", "Daniel K.", ""], ["Jo", "Han-Shin", ""]]}, {"id": "1904.02405", "submitter": "Yoav Chai", "authors": "Yotam Gil, Yoav Chai, Or Gorodissky and Jonathan Berant", "title": "White-to-Black: Efficient Distillation of Black-Box Adversarial Attacks", "comments": "Accepted to NAACL-HLT 2019 as conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples are important for understanding the behavior of neural\nmodels, and can improve their robustness through adversarial training. Recent\nwork in natural language processing generated adversarial examples by assuming\nwhite-box access to the attacked model, and optimizing the input directly\nagainst it (Ebrahimi et al., 2018). In this work, we show that the knowledge\nimplicit in the optimization procedure can be distilled into another more\nefficient neural network. We train a model to emulate the behavior of a\nwhite-box attack and show that it generalizes well across examples. Moreover,\nit reduces adversarial example generation time by 19x-39x. We also show that\nour approach transfers to a black-box setting, by attacking The Google\nPerspective API and exposing its vulnerability. Our attack flips the\nAPI-predicted label in 42\\% of the generated examples, while humans maintain\nhigh-accuracy in predicting the gold label.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 08:31:15 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Gil", "Yotam", ""], ["Chai", "Yoav", ""], ["Gorodissky", "Or", ""], ["Berant", "Jonathan", ""]]}, {"id": "1904.02420", "submitter": "Quentin Jodelet", "authors": "Quentin Jodelet, Vincent Gripon and Masafumi Hagiwara", "title": "Transfer Learning with Sparse Associative Memories", "comments": "Presented at the 28th International Conference on Artificial Neural\n  Networks (ICANN 2019)", "journal-ref": "Artificial Neural Networks and Machine Learning - ICANN 2019:\n  Theoretical Neural Computation. ICANN 2019. Lecture Notes in Computer\n  Science, vol 11727. Springer, Cham", "doi": "10.1007/978-3-030-30487-4_39", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel layer designed to be used as the output\nof pre-trained neural networks in the context of classification. Based on\nAssociative Memories, this layer can help design Deep Neural Networks which\nsupport incremental learning and that can be (partially) trained in real time\non embedded devices. Experiments on the ImageNet dataset and other different\ndomain specific datasets show that it is possible to design more flexible and\nfaster-to-train Neural Networks at the cost of a slight decrease in accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 09:16:30 GMT"}, {"version": "v2", "created": "Sun, 30 Jun 2019 14:20:08 GMT"}, {"version": "v3", "created": "Thu, 19 Sep 2019 12:30:09 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Jodelet", "Quentin", ""], ["Gripon", "Vincent", ""], ["Hagiwara", "Masafumi", ""]]}, {"id": "1904.02426", "submitter": "Hongyu Chen", "authors": "Hongyu Chen, Li Jiang", "title": "Efficient GAN-based method for cyber-intrusion detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ubiquitous anomalies endanger the security of our system constantly. They may\nbring irreversible damages to the system and cause leakage of privacy. Thus, it\nis of vital importance to promptly detect these anomalies. Traditional\nsupervised methods such as Decision Trees and Support Vector Machine (SVM) are\nused to classify normality and abnormality. However, in some case the abnormal\nstatus are largely rarer than normal status, which leads to decision bias of\nthese methods. Generative adversarial network (GAN) has been proposed to handle\nthe case. With its strong generative ability, it only needs to learn the\ndistribution of normal status, and identify the abnormal status through the gap\nbetween it and the learned distribution. Nevertheless, existing GAN-based\nmodels are not suitable to process data with discrete values, leading to\nimmense degradation of detection performance. To cope with the discrete\nfeatures, in this paper, we propose an efficient GAN-based model with\nspecifically-designed loss function. Experiment results show that our model\noutperforms state-of-the-art models on discrete dataset and remarkably reduce\nthe overhead.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 09:30:40 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 11:10:18 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Chen", "Hongyu", ""], ["Jiang", "Li", ""]]}, {"id": "1904.02436", "submitter": "Richard McKinley", "authors": "Richard McKinley, Michael Rebsamen, Raphael Meier, Mauricio Reyes,\n  Christian Rummel, Roland Wiest", "title": "Few-shot brain segmentation from weakly labeled data with deep\n  heteroscedastic multi-task networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In applications of supervised learning applied to medical image segmentation,\nthe need for large amounts of labeled data typically goes unquestioned. In\nparticular, in the case of brain anatomy segmentation, hundreds or thousands of\nweakly-labeled volumes are often used as training data. In this paper, we first\nobserve that for many brain structures, a small number of training examples,\n(n=9), weakly labeled using Freesurfer 6.0, plus simple data augmentation,\nsuffice as training data to achieve high performance, achieving an overall mean\nDice coefficient of $0.84 \\pm 0.12$ compared to Freesurfer over 28 brain\nstructures in T1-weighted images of $\\approx 4000$ 9-10 year-olds from the\nAdolescent Brain Cognitive Development study. We then examine two varieties of\nheteroscedastic network as a method for improving classification results. An\nexisting proposal by Kendall and Gal, which uses Monte-Carlo inference to learn\nto predict the variance of each prediction, yields an overall mean Dice of\n$0.85 \\pm 0.14$ and showed statistically significant improvements over 25 brain\nstructures. Meanwhile a novel heteroscedastic network which directly learns the\nprobability that an example has been mislabeled yielded an overall mean Dice of\n$0.87 \\pm 0.11$ and showed statistically significant improvements over all but\none of the brain structures considered. The loss function associated to this\nnetwork can be interpreted as performing a form of learned label smoothing,\nwhere labels are only smoothed where they are judged to be uncertain.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 09:56:02 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["McKinley", "Richard", ""], ["Rebsamen", "Michael", ""], ["Meier", "Raphael", ""], ["Reyes", "Mauricio", ""], ["Rummel", "Christian", ""], ["Wiest", "Roland", ""]]}, {"id": "1904.02514", "submitter": "Tom Vander Aa", "authors": "Tom Vander Aa, Imen Chakroun, Thomas J. Ashby, Jaak Simm, Adam Arany,\n  Yves Moreau, Thanh Le Van, Jos\\'e Felipe Golib Dzib, J\\\"org Wegner, Vladimir\n  Chupakhin, Hugo Ceulemans, Roel Wuyts and Wilfried Verachtert", "title": "SMURFF: a High-Performance Framework for Matrix Factorization", "comments": "European Commission Project: EPEEC - European joint Effort toward a\n  Highly Productive Programming Environment for Heterogeneous Exascale\n  Computing (EC-H2020-80151)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian Matrix Factorization (BMF) is a powerful technique for recommender\nsystems because it produces good results and is relatively robust against\noverfitting. Yet BMF is more computationally intensive and thus more\nchallenging to implement for large datasets. In this work we present SMURFF a\nhigh-performance feature-rich framework to compose and construct different\nBayesian matrix-factorization methods. The framework has been successfully used\nin to do large scale runs of compound-activity prediction. SMURFF is available\nas open-source and can be used both on a supercomputer and on a desktop or\nlaptop machine. Documentation and several examples are provided as Jupyter\nnotebooks using SMURFF's high-level Python API.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 12:36:36 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 11:40:15 GMT"}, {"version": "v3", "created": "Mon, 29 Jul 2019 14:02:48 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Aa", "Tom Vander", ""], ["Chakroun", "Imen", ""], ["Ashby", "Thomas J.", ""], ["Simm", "Jaak", ""], ["Arany", "Adam", ""], ["Moreau", "Yves", ""], ["Van", "Thanh Le", ""], ["Dzib", "Jos\u00e9 Felipe Golib", ""], ["Wegner", "J\u00f6rg", ""], ["Chupakhin", "Vladimir", ""], ["Ceulemans", "Hugo", ""], ["Wuyts", "Roel", ""], ["Verachtert", "Wilfried", ""]]}, {"id": "1904.02526", "submitter": "Eric Heim", "authors": "Eric Heim", "title": "Constrained Generative Adversarial Networks for Interactive Image\n  Generation", "comments": "To Appear in the Proceedings of the 2019 Conference on Computer\n  Vision and Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have received a great deal of\nattention due in part to recent success in generating original, high-quality\nsamples from visual domains. However, most current methods only allow for users\nto guide this image generation process through limited interactions. In this\nwork we develop a novel GAN framework that allows humans to be \"in-the-loop\" of\nthe image generation process. Our technique iteratively accepts relative\nconstraints of the form \"Generate an image more like image A than image B\".\nAfter each constraint is given, the user is presented with new outputs from the\nGAN, informing the next round of feedback. This feedback is used to constrain\nthe output of the GAN with respect to an underlying semantic space that can be\ndesigned to model a variety of different notions of similarity (e.g. classes,\nattributes, object relationships, color, etc.). In our experiments, we show\nthat our GAN framework is able to generate images that are of comparable\nquality to equivalent unsupervised GANs while satisfying a large number of the\nconstraints provided by users, effectively changing a GAN into one that allows\nusers interactive control over image generation without sacrificing image\nquality.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 17:59:41 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Heim", "Eric", ""]]}, {"id": "1904.02580", "submitter": "Jianhao Peng", "authors": "Abhishek Agarwal, Jianhao Peng and Olgica Milenkovic", "title": "Online Convex Matrix Factorization with Representative Regions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix factorization (MF) is a versatile learning method that has found wide\napplications in various data-driven disciplines. Still, many MF algorithms do\nnot adequately scale with the size of available datasets and/or lack\ninterpretability. To improve the computational efficiency of the method, an\nonline (streaming) MF algorithm was proposed in Mairal et al. [2010]. To enable\ndata interpretability, a constrained version of MF, termed convex MF, was\nintroduced in Ding et al. [2010]. In the latter work, the basis vectors are\nrequired to lie in the convex hull of the data samples, thereby ensuring that\nevery basis can be interpreted as a weighted combination of data samples. No\ncurrent algorithmic solutions for online convex MF are known as it is\nchallenging to find adequate convex bases without having access to the complete\ndataset. We address both problems by proposing the first online convex MF\nalgorithm that maintains a collection of constant-size sets of representative\ndata samples needed for interpreting each of the basis (Ding et al. [2010]) and\nhas the same almost sure convergence guarantees as the online learning\nalgorithm of Mairal et al. [2010]. Our proof techniques combine random\ncoordinate descent algorithms with specialized quasi-martingale convergence\nanalysis. Experiments on synthetic and real world datasets show significant\ncomputational savings of the proposed online convex MF method compared to\nclassical convex MF. Since the proposed method maintains small representative\nsets of data samples needed for convex interpretations, it is related to a body\nof work in theoretical computer science, pertaining to generating point sets\n(Blum et al. [2016]), and in computer vision, pertaining to archetypal analysis\n(Mei et al. [2018]). Nevertheless, it differs from these lines of work both in\nterms of the objective and algorithmic implementations.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 14:32:19 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 06:48:21 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Agarwal", "Abhishek", ""], ["Peng", "Jianhao", ""], ["Milenkovic", "Olgica", ""]]}, {"id": "1904.02632", "submitter": "Raphael Gontijo Lopes", "authors": "Raphael Gontijo Lopes, David Ha, Douglas Eck, Jonathon Shlens", "title": "A Learned Representation for Scalable Vector Graphics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dramatic advances in generative models have resulted in near photographic\nquality for artificially rendered faces, animals and other objects in the\nnatural world. In spite of such advances, a higher level understanding of\nvision and imagery does not arise from exhaustively modeling an object, but\ninstead identifying higher-level attributes that best summarize the aspects of\nan object. In this work we attempt to model the drawing process of fonts by\nbuilding sequential generative models of vector graphics. This model has the\nbenefit of providing a scale-invariant representation for imagery whose latent\nrepresentation may be systematically manipulated and exploited to perform style\npropagation. We demonstrate these results on a large dataset of fonts and\nhighlight how such a model captures the statistical dependencies and richness\nof this dataset. We envision that our model can find use as a tool for graphic\ndesigners to facilitate font design.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 16:04:03 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Lopes", "Raphael Gontijo", ""], ["Ha", "David", ""], ["Eck", "Douglas", ""], ["Shlens", "Jonathon", ""]]}, {"id": "1904.02642", "submitter": "Michael Volpp", "authors": "Michael Volpp, Lukas P. Fr\\\"ohlich, Kirsten Fischer, Andreas Doerr,\n  Stefan Falkner, Frank Hutter, Christian Daniel", "title": "Meta-Learning Acquisition Functions for Transfer Learning in Bayesian\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transferring knowledge across tasks to improve data-efficiency is one of the\nopen key challenges in the field of global black-box optimization. Readily\navailable algorithms are typically designed to be universal optimizers and,\ntherefore, often suboptimal for specific tasks. We propose a novel transfer\nlearning method to obtain customized optimizers within the well-established\nframework of Bayesian optimization, allowing our algorithm to utilize the\nproven generalization capabilities of Gaussian processes. Using reinforcement\nlearning to meta-train an acquisition function (AF) on a set of related tasks,\nthe proposed method learns to extract implicit structural information and to\nexploit it for improved data-efficiency. We present experiments on a\nsimulation-to-real transfer task as well as on several synthetic functions and\non two hyperparameter search problems. The results show that our algorithm (1)\nautomatically identifies structural properties of objective functions from\navailable source tasks or simulations, (2) performs favourably in settings with\nboth scarse and abundant source data, and (3) falls back to the performance\nlevel of general AFs if no particular structure is present.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 16:27:06 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 15:29:46 GMT"}, {"version": "v3", "created": "Tue, 28 May 2019 13:05:14 GMT"}, {"version": "v4", "created": "Fri, 27 Sep 2019 09:58:29 GMT"}, {"version": "v5", "created": "Fri, 14 Feb 2020 13:24:57 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Volpp", "Michael", ""], ["Fr\u00f6hlich", "Lukas P.", ""], ["Fischer", "Kirsten", ""], ["Doerr", "Andreas", ""], ["Falkner", "Stefan", ""], ["Hutter", "Frank", ""], ["Daniel", "Christian", ""]]}, {"id": "1904.02655", "submitter": "Noelia Oses Fern\\'andez", "authors": "Noelia Oses, Aritz Legarretaetxebarria, Marco Quartulli, Igor\n  Garc\\'ia, Mikel Serrano", "title": "Determining input variable ranges in Industry 4.0: A heuristic for\n  estimating the domain of a real-valued function or trained regression model\n  given an output range", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industrial process control systems try to keep an output variable within a\ngiven tolerance around a target value. PID control systems have been widely\nused in industry to control input variables in order to reach this goal.\nHowever, this kind of Transfer Function based approach cannot be extended to\ncomplex processes where input data might be non-numeric, high dimensional,\nsparse, etc. In such cases, there is still a need for determining the subspace\nof input data that produces an output within a given range. This paper presents\na non-stochastic heuristic to determine input values for a mathematical\nfunction or trained regression model given an output range. The proposed method\ncreates a synthetic training data set of input combinations with a class label\nthat indicates whether the output is within the given target range or not.\nThen, a decision tree classifier is used to determine the subspace of input\ndata of interest. This method is more general than a traditional controller as\nthe target range for the output does not have to be centered around a reference\nvalue and it can be applied given a regression model of the output variable,\nwhich may have categorical variables as inputs and may be high dimensional,\nsparse... The proposed heuristic is validated with a proof of concept on a real\nuse case where the quality of a lamination factory is established to identify\nthe suitable subspace of production variable values.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 11:23:56 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Oses", "Noelia", ""], ["Legarretaetxebarria", "Aritz", ""], ["Quartulli", "Marco", ""], ["Garc\u00eda", "Igor", ""], ["Serrano", "Mikel", ""]]}, {"id": "1904.02664", "submitter": "Branislav Kveton", "authors": "Chih-Wei Hsu, Branislav Kveton, Ofer Meshi, Martin Mladenov, and Csaba\n  Szepesvari", "title": "Empirical Bayes Regret Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most bandit algorithm designs are purely theoretical. Therefore, they have\nstrong regret guarantees, but also are often too conservative in practice. In\nthis work, we pioneer the idea of algorithm design by minimizing the empirical\nBayes regret, the average regret over problem instances sampled from a known\ndistribution. We focus on a tractable instance of this problem, the confidence\ninterval and posterior width tuning, and propose an efficient algorithm for\nsolving it. The tuning algorithm is analyzed and evaluated in multi-armed,\nlinear, and generalized linear bandits. We report several-fold reductions in\nBayes regret for state-of-the-art bandit algorithms, simply by optimizing over\na small sample from a distribution.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 17:00:02 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 05:14:06 GMT"}, {"version": "v3", "created": "Thu, 10 Oct 2019 06:18:52 GMT"}, {"version": "v4", "created": "Wed, 10 Jun 2020 18:47:04 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Hsu", "Chih-Wei", ""], ["Kveton", "Branislav", ""], ["Meshi", "Ofer", ""], ["Mladenov", "Martin", ""], ["Szepesvari", "Csaba", ""]]}, {"id": "1904.02666", "submitter": "Akbar Dehghani", "authors": "Akbar Dehghani, Tristan Glatard, Emad Shihab", "title": "Subject Cross Validation in Human Activity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  K-fold Cross Validation is commonly used to evaluate classifiers and tune\ntheir hyperparameters. However, it assumes that data points are Independent and\nIdentically Distributed (i.i.d.) so that samples used in the training and test\nsets can be selected randomly and uniformly. In Human Activity Recognition\ndatasets, we note that the samples produced by the same subjects are likely to\nbe correlated due to diverse factors. Hence, k-fold cross validation may\noverestimate the performance of activity recognizers, in particular when\noverlapping sliding windows are used. In this paper, we investigate the effect\nof Subject Cross Validation on the performance of Human Activity Recognition,\nboth with non-overlapping and with overlapping sliding windows. Results show\nthat k-fold cross validation artificially increases the performance of\nrecognizers by about 10%, and even by 16% when overlapping windows are used. In\naddition, we do not observe any performance gain from the use of overlapping\nwindows. We conclude that Human Activity Recognition systems should be\nevaluated by Subject Cross Validation, and that overlapping windows are not\nworth their extra computational cost.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 17:02:12 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 17:55:48 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Dehghani", "Akbar", ""], ["Glatard", "Tristan", ""], ["Shihab", "Emad", ""]]}, {"id": "1904.02679", "submitter": "Jesse Vig", "authors": "Jesse Vig", "title": "Visualizing Attention in Transformer-Based Language Representation\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an open-source tool for visualizing multi-head self-attention in\nTransformer-based language representation models. The tool extends earlier work\nby visualizing attention at three levels of granularity: the attention-head\nlevel, the model level, and the neuron level. We describe how each of these\nviews can help to interpret the model, and we demonstrate the tool on the BERT\nmodel and the OpenAI GPT-2 model. We also present three use cases for analyzing\nGPT-2: detecting model bias, identifying recurring patterns, and linking\nneurons to model behavior.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 17:32:49 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 16:00:53 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Vig", "Jesse", ""]]}, {"id": "1904.02773", "submitter": "Yuheng Bu", "authors": "Craig Wilson, Yuheng Bu and Venugopal Veeravalli", "title": "Adaptive Sequential Machine Learning", "comments": "arXiv admin note: text overlap with arXiv:1509.07422", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A framework previously introduced in [3] for solving a sequence of stochastic\noptimization problems with bounded changes in the minimizers is extended and\napplied to machine learning problems such as regression and classification. The\nstochastic optimization problems arising in these machine learning problems is\nsolved using algorithms such as stochastic gradient descent (SGD). A method\nbased on estimates of the change in the minimizers and properties of the\noptimization algorithm is introduced for adaptively selecting the number of\nsamples at each time step to ensure that the excess risk, i.e., the expected\ngap between the loss achieved by the approximate minimizer produced by the\noptimization algorithm and the exact minimizer, does not exceed a target level.\nA bound is developed to show that the estimate of the change in the minimizers\nis non-trivial provided that the excess risk is small enough. Extensions\nrelevant to the machine learning setting are considered, including a cost-based\napproach to select the number of samples with a cost budget over a fixed\nhorizon, and an approach to applying cross-validation for model selection.\nFinally, experiments with synthetic and real data are used to validate the\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 20:03:46 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Wilson", "Craig", ""], ["Bu", "Yuheng", ""], ["Veeravalli", "Venugopal", ""]]}, {"id": "1904.02792", "submitter": "Hugh Zhang", "authors": "Tatsunori B. Hashimoto, Hugh Zhang, Percy Liang", "title": "Unifying Human and Statistical Evaluation for Natural Language\n  Generation", "comments": "NAACL Camera Ready Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we measure whether a natural language generation system produces both\nhigh quality and diverse outputs? Human evaluation captures quality but not\ndiversity, as it does not catch models that simply plagiarize from the training\nset. On the other hand, statistical evaluation (i.e., perplexity) captures\ndiversity but not quality, as models that occasionally emit low quality samples\nwould be insufficiently penalized. In this paper, we propose a unified\nframework which evaluates both diversity and quality, based on the optimal\nerror rate of predicting whether a sentence is human- or machine-generated. We\ndemonstrate that this error rate can be efficiently estimated by combining\nhuman and statistical evaluation, using an evaluation metric which we call\nHUSE. On summarization and chit-chat dialogue, we show that (i) HUSE detects\ndiversity defects which fool pure human evaluation and that (ii) techniques\nsuch as annealing for improving quality actually decrease HUSE due to decreased\ndiversity.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 21:03:34 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Hashimoto", "Tatsunori B.", ""], ["Zhang", "Hugh", ""], ["Liang", "Percy", ""]]}, {"id": "1904.02816", "submitter": "Saiprasad Ravishankar", "authors": "Saiprasad Ravishankar, Jong Chul Ye, and Jeffrey A. Fessler", "title": "Image Reconstruction: From Sparsity to Data-adaptive Methods and Machine\n  Learning", "comments": "To appear in the Proceedings of the IEEE, Special Issue on Biomedical\n  Imaging and Analysis in the Age of Sparsity, Big Data, and Deep Learning", "journal-ref": null, "doi": "10.1109/JPROC.2019.2936204", "report-no": null, "categories": "eess.IV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of medical image reconstruction has seen roughly four types of\nmethods. The first type tended to be analytical methods, such as filtered\nback-projection (FBP) for X-ray computed tomography (CT) and the inverse\nFourier transform for magnetic resonance imaging (MRI), based on simple\nmathematical models for the imaging systems. These methods are typically fast,\nbut have suboptimal properties such as poor resolution-noise trade-off for CT.\nA second type is iterative reconstruction methods based on more complete models\nfor the imaging system physics and, where appropriate, models for the sensor\nstatistics. These iterative methods improved image quality by reducing noise\nand artifacts. The FDA-approved methods among these have been based on\nrelatively simple regularization models. A third type of methods has been\ndesigned to accommodate modified data acquisition methods, such as reduced\nsampling in MRI and CT to reduce scan time or radiation dose. These methods\ntypically involve mathematical image models involving assumptions such as\nsparsity or low-rank. A fourth type of methods replaces mathematically designed\nmodels of signals and systems with data-driven or adaptive models inspired by\nthe field of machine learning. This paper focuses on the two most recent trends\nin medical image reconstruction: methods based on sparsity or low-rank models,\nand data-driven methods based on machine learning techniques.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 23:04:10 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 03:14:57 GMT"}, {"version": "v3", "created": "Fri, 16 Aug 2019 03:07:55 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Ravishankar", "Saiprasad", ""], ["Ye", "Jong Chul", ""], ["Fessler", "Jeffrey A.", ""]]}, {"id": "1904.02818", "submitter": "David Bieber", "authors": "Rui Zhao, David Bieber, Kevin Swersky, Daniel Tarlow", "title": "Neural Networks for Modeling Source Code Edits", "comments": "Deanonymized version of ICLR 2019 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Programming languages are emerging as a challenging and interesting domain\nfor machine learning. A core task, which has received significant attention in\nrecent years, is building generative models of source code. However, to our\nknowledge, previous generative models have always been framed in terms of\ngenerating static snapshots of code. In this work, we instead treat source code\nas a dynamic object and tackle the problem of modeling the edits that software\ndevelopers make to source code files. This requires extracting intent from\nprevious edits and leveraging it to generate subsequent edits. We develop\nseveral neural networks and use synthetic data to test their ability to learn\nchallenging edit patterns that require strong generalization. We then collect\nand train our models on a large-scale dataset of Google source code, consisting\nof millions of fine-grained edits from thousands of Python developers. From the\nmodeling perspective, our main conclusion is that a new composition of\nattentional and pointer network components provides the best overall\nperformance and scalability. From the application perspective, our results\nprovide preliminary evidence of the feasibility of developing tools that learn\nto predict future edits.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 23:06:09 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Zhao", "Rui", ""], ["Bieber", "David", ""], ["Swersky", "Kevin", ""], ["Tarlow", "Daniel", ""]]}, {"id": "1904.02826", "submitter": "Oliver Maclaren", "authors": "Oliver J. Maclaren and Ruanui Nicholson", "title": "What can be estimated? Identifiability, estimability, causal inference\n  and ill-posed inverse problems", "comments": "41 pages, 5 figures. Fixed typos, added references, added examples.\n  New examples (updated again) introduce explicit 'view' and 'undo' operations\n  to complement 'do' operation as part of the translation between structural\n  causal models and our abstract statistical formalism", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider basic conceptual questions concerning the relationship between\nstatistical estimation and causal inference. Firstly, we show how to translate\ncausal inference problems into an abstract statistical formalism without\nrequiring any structure beyond an arbitrarily-indexed family of probability\nmodels. The formalism is simple but can incorporate a variety of causal\nmodelling frameworks, including 'structural causal models', but also models\nexpressed in terms of, e.g., differential equations. We focus primarily on the\nstructural/graphical causal modelling literature, however. Secondly, we\nconsider the extent to which causal and statistical concerns can be cleanly\nseparated, examining the fundamental question: 'What can be estimated from\ndata?'. We call this the problem of estimability. We approach this by analysing\na standard formal definition of 'can be estimated' commonly adopted in the\ncausal inference literature -- identifiability -- in our abstract statistical\nformalism. We use elementary category theory to show that identifiability\nimplies the existence of a Fisher-consistent estimator, but also show that this\nestimator may be discontinuous, and thus unstable, in general. This difficulty\narises because the causal inference problem is, in general, an ill-posed\ninverse problem. Inverse problems have three conditions which must be satisfied\nto be considered well-posed: existence, uniqueness, and stability of solutions.\nHere identifiability corresponds to the question of uniqueness; in contrast, we\ntake estimability to mean satisfaction of all three conditions, i.e.\nwell-posedness. Lack of stability implies that naive translation of a causally\nidentifiable quantity into an achievable statistical estimation target may\nprove impossible. Our article is primarily expository and aimed at unifying\nideas from multiple fields, though we provide new constructions and proofs.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 23:46:44 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 06:36:23 GMT"}, {"version": "v3", "created": "Mon, 20 Jul 2020 04:54:06 GMT"}, {"version": "v4", "created": "Tue, 21 Jul 2020 01:48:09 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Maclaren", "Oliver J.", ""], ["Nicholson", "Ruanui", ""]]}, {"id": "1904.02841", "submitter": "Fatemeh Sheikholeslami", "authors": "Fatemeh Sheikholeslami, Swayambhoo Jain, and Georgios B. Giannakis", "title": "Minimum Uncertainty Based Detection of Adversaries in Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their unprecedented performance in various domains, utilization of\nDeep Neural Networks (DNNs) in safety-critical environments is severely limited\nin the presence of even small adversarial perturbations. The present work\ndevelops a randomized approach to detecting such perturbations based on minimum\nuncertainty metrics that rely on sampling at the hidden layers during the DNN\ninference stage. Inspired by Bayesian approaches to uncertainty estimation, the\nsampling probabilities are designed for effective detection of the\nadversarially corrupted inputs. Being modular, the novel detector of\nadversaries can be conveniently employed by any pre-trained DNN at no extra\ntraining overhead. Selecting which units to sample per hidden layer entails\nquantifying the amount of DNN output uncertainty, where the overall uncertainty\nis expressed in terms of its layer-wise components - what also promotes\nscalability. Sampling probabilities are then sought by minimizing uncertainty\nmeasures layer-by-layer, leading to a novel convex optimization problem that\nadmits an exact solver with superlinear convergence rate. By simplifying the\nobjective function, low-complexity approximate solvers are also developed. In\naddition to valuable insights, these approximations link the novel approach\nwith state-of-the-art randomized adversarial detectors. The effectiveness of\nthe novel detectors in the context of competing alternatives is highlighted\nthrough extensive tests for various types of adversarial attacks with variable\nlevels of strength.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 01:23:10 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 23:18:33 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Sheikholeslami", "Fatemeh", ""], ["Jain", "Swayambhoo", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1904.02843", "submitter": "Jong Chul Ye", "authors": "Shujaat Khan, Jaeyoung Huh, Jong Chul Ye", "title": "Deep Learning-based Universal Beamformer for Ultrasound Imaging", "comments": "Accepted for MICCAI 2019. arXiv admin note: substantial text overlap\n  with arXiv:1901.01706", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In ultrasound (US) imaging, individual channel RF measurements are\nback-propagated and accumulated to form an image after applying specific\ndelays. While this time reversal is usually implemented using a hardware- or\nsoftware-based delay-and-sum (DAS) beamformer, the performance of DAS decreases\nrapidly in situations where data acquisition is not ideal. Herein, for the\nfirst time, we demonstrate that a single data-driven adaptive beamformer\ndesigned as a deep neural network can generate high quality images robustly for\nvarious detector channel configurations and subsampling rates. The proposed\ndeep beamformer is evaluated for two distinct acquisition schemes: focused\nultrasound imaging and planewave imaging. Experimental results showed that the\nproposed deep beamformer exhibit significant performance gain for both focused\nand planar imaging schemes, in terms of contrast-to-noise ratio and structural\nsimilarity.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 01:40:52 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 14:07:27 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Khan", "Shujaat", ""], ["Huh", "Jaeyoung", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1904.02868", "submitter": "Amirata Ghorbani", "authors": "Amirata Ghorbani and James Zou", "title": "Data Shapley: Equitable Valuation of Data for Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As data becomes the fuel driving technological and economic growth, a\nfundamental challenge is how to quantify the value of data in algorithmic\npredictions and decisions. For example, in healthcare and consumer markets, it\nhas been suggested that individuals should be compensated for the data that\nthey generate, but it is not clear what is an equitable valuation for\nindividual data. In this work, we develop a principled framework to address\ndata valuation in the context of supervised machine learning. Given a learning\nalgorithm trained on $n$ data points to produce a predictor, we propose data\nShapley as a metric to quantify the value of each training datum to the\npredictor performance. Data Shapley value uniquely satisfies several natural\nproperties of equitable data valuation. We develop Monte Carlo and\ngradient-based methods to efficiently estimate data Shapley values in practical\nsettings where complex learning algorithms, including neural networks, are\ntrained on large datasets. In addition to being equitable, extensive\nexperiments across biomedical, image and synthetic data demonstrate that data\nShapley has several other benefits: 1) it is more powerful than the popular\nleave-one-out or leverage score in providing insight on what data is more\nvaluable for a given learning task; 2) low Shapley value data effectively\ncapture outliers and corruptions; 3) high Shapley value data inform what type\nof new data to acquire to improve the predictor.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 04:54:10 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 08:10:40 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Ghorbani", "Amirata", ""], ["Zou", "James", ""]]}, {"id": "1904.02872", "submitter": "Jong Chul Ye", "authors": "Boah Kim and Jong Chul Ye", "title": "Mumford-Shah Loss Functional for Image Segmentation with Deep Learning", "comments": "Accepted for IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2019.2941265", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent state-of-the-art image segmentation algorithms are mostly based on\ndeep neural networks, thanks to their high performance and fast computation\ntime. However, these methods are usually trained in a supervised manner, which\nrequires large number of high quality ground-truth segmentation masks. On the\nother hand, classical image segmentation approaches such as level-set methods\nare formulated in a self-supervised manner by minimizing energy functions such\nas Mumford-Shah functional, so they are still useful to help generation of\nsegmentation masks without labels. Unfortunately, these algorithms are usually\ncomputationally expensive and often have limitation in semantic segmentation.\nIn this paper, we propose a novel loss function based on Mumford-Shah\nfunctional that can be used in deep-learning based image segmentation without\nor with small labeled data. This loss function is based on the observation that\nthe softmax layer of deep neural networks has striking similarity to the\ncharacteristic function in the Mumford-Shah functional. We show that the new\nloss function enables semi-supervised and unsupervised segmentation. In\naddition, our loss function can be also used as a regularized function to\nenhance supervised semantic segmentation algorithms. Experimental results on\nmultiple datasets demonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 05:17:18 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 09:14:39 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Kim", "Boah", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1904.02874", "submitter": "Sneha Chaudhari", "authors": "Sneha Chaudhari, Varun Mithal, Gungor Polatkan, Rohan Ramanath", "title": "An Attentive Survey of Attention Models", "comments": "accepted to Transactions on Intelligent Systems and Technology(TIST);\n  33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Attention Model has now become an important concept in neural networks that\nhas been researched within diverse application domains. This survey provides a\nstructured and comprehensive overview of the developments in modeling\nattention. In particular, we propose a taxonomy which groups existing\ntechniques into coherent categories. We review salient neural architectures in\nwhich attention has been incorporated, and discuss applications in which\nmodeling attention has shown a significant impact. We also describe how\nattention has been used to improve the interpretability of neural networks.\nFinally, we discuss some future research directions in attention. We hope this\nsurvey will provide a succinct introduction to attention models and guide\npractitioners while developing approaches for their applications.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 05:26:59 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 23:58:46 GMT"}, {"version": "v3", "created": "Mon, 12 Jul 2021 12:03:52 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Chaudhari", "Sneha", ""], ["Mithal", "Varun", ""], ["Polatkan", "Gungor", ""], ["Ramanath", "Rohan", ""]]}, {"id": "1904.02877", "submitter": "Dimitrios Stamoulis", "authors": "Dimitrios Stamoulis, Ruizhou Ding, Di Wang, Dimitrios Lymberopoulos,\n  Bodhi Priyantha, Jie Liu, Diana Marculescu", "title": "Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4\n  Hours", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we automatically design a Convolutional Network (ConvNet) with the\nhighest image classification accuracy under the runtime constraint of a mobile\ndevice? Neural architecture search (NAS) has revolutionized the design of\nhardware-efficient ConvNets by automating this process. However, the NAS\nproblem remains challenging due to the combinatorially large design space,\ncausing a significant searching time (at least 200 GPU-hours). To alleviate\nthis complexity, we propose Single-Path NAS, a novel differentiable NAS method\nfor designing hardware-efficient ConvNets in less than 4 hours. Our\ncontributions are as follows: 1. Single-path search space: Compared to previous\ndifferentiable NAS methods, Single-Path NAS uses one single-path\nover-parameterized ConvNet to encode all architectural decisions with shared\nconvolutional kernel parameters, hence drastically decreasing the number of\ntrainable parameters and the search cost down to few epochs. 2.\nHardware-efficient ImageNet classification: Single-Path NAS achieves 74.96%\ntop-1 accuracy on ImageNet with 79ms latency on a Pixel 1 phone, which is\nstate-of-the-art accuracy compared to NAS methods with similar constraints\n(<80ms). 3. NAS efficiency: Single-Path NAS search cost is only 8 epochs (30\nTPU-hours), which is up to 5,000x faster compared to prior work. 4.\nReproducibility: Unlike all recent mobile-efficient NAS methods which only\nrelease pretrained models, we open-source our entire codebase at:\nhttps://github.com/dstamoulis/single-path-nas.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 05:49:41 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Stamoulis", "Dimitrios", ""], ["Ding", "Ruizhou", ""], ["Wang", "Di", ""], ["Lymberopoulos", "Dimitrios", ""], ["Priyantha", "Bodhi", ""], ["Liu", "Jie", ""], ["Marculescu", "Diana", ""]]}, {"id": "1904.02892", "submitter": "Kou Tanaka", "authors": "Kou Tanaka, Hirokazu Kameoka, Takuhiro Kaneko, Nobukatsu Hojo", "title": "WaveCycleGAN2: Time-domain Neural Post-filter for Speech Waveform\n  Generation", "comments": "Submitted to INTERSPEECH2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  WaveCycleGAN has recently been proposed to bridge the gap between natural and\nsynthesized speech waveforms in statistical parametric speech synthesis and\nprovides fast inference with a moving average model rather than an\nautoregressive model and high-quality speech synthesis with the adversarial\ntraining. However, the human ear can still distinguish the processed speech\nwaveforms from natural ones. One possible cause of this distinguishability is\nthe aliasing observed in the processed speech waveform via down/up-sampling\nmodules. To solve the aliasing and provide higher quality speech synthesis, we\npropose WaveCycleGAN2, which 1) uses generators without down/up-sampling\nmodules and 2) combines discriminators of the waveform domain and acoustic\nparameter domain. The results show that the proposed method 1) alleviates the\naliasing well, 2) is useful for both speech waveforms generated by\nanalysis-and-synthesis and statistical parametric speech synthesis, and 3)\nachieves a mean opinion score comparable to those of natural speech and speech\nsynthesized by WaveNet (open WaveNet) and WaveGlow while processing speech\nsamples at a rate of more than 150 kHz on an NVIDIA Tesla P100.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 06:53:37 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 01:15:27 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Tanaka", "Kou", ""], ["Kameoka", "Hirokazu", ""], ["Kaneko", "Takuhiro", ""], ["Hojo", "Nobukatsu", ""]]}, {"id": "1904.02910", "submitter": "Jong Chul Ye", "authors": "Sungjun Lim, Sang-Eun Lee, Sunghoe Chang, Jong Chul Ye", "title": "Blind Deconvolution Microscopy Using Cycle Consistent CNN with Explicit\n  PSF Layer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deconvolution microscopy has been extensively used to improve the resolution\nof the widefield fluorescent microscopy. Conventional approaches, which usually\nrequire the point spread function (PSF) measurement or blind estimation, are\nhowever computationally expensive. Recently, CNN based approaches have been\nexplored as a fast and high performance alternative. In this paper, we present\na novel unsupervised deep neural network for blind deconvolution based on cycle\nconsistency and PSF modeling layers. In contrast to the recent CNN approaches\nfor similar problem, the explicit PSF modeling layers improve the robustness of\nthe algorithm. Experimental results confirm the efficacy of the algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 07:43:34 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Lim", "Sungjun", ""], ["Lee", "Sang-Eun", ""], ["Chang", "Sunghoe", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1904.02926", "submitter": "Youngser Park", "authors": "Congyuan Yang, Carey E. Priebe, Youngser Park, David J. Marchette", "title": "Simultaneous Dimensionality and Complexity Model Selection for Spectral\n  Graph Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our problem of interest is to cluster vertices of a graph by identifying\nunderlying community structure. Among various vertex clustering approaches,\nspectral clustering is one of the most popular methods because it is easy to\nimplement while often outperforming more traditional clustering algorithms.\nHowever, there are two inherent model selection problems in spectral\nclustering, namely estimating both the embedding dimension and number of\nclusters. This paper attempts to address the issue by establishing a novel\nmodel selection framework specifically for vertex clustering on graphs under a\nstochastic block model. The first contribution is a probabilistic model which\napproximates the distribution of the extended spectral embedding of a graph.\nThe model is constructed based on a theoretical result of asymptotic normality\nof the informative part of the embedding, and on a simulation result providing\na conjecture for the limiting behavior of the redundant part of the embedding.\nThe second contribution is a simultaneous model selection framework. In\ncontrast with the traditional approaches, our model selection procedure\nestimates embedding dimension and number of clusters simultaneously. Based on\nour conjectured distributional model, a theorem on the consistency of the\nestimates of model parameters is presented, providing support for the validity\nof our method. Algorithms for our simultaneous model selection for vertex\nclustering are proposed, demonstrating superior performance in simulation\nexperiments. We illustrate our method via application to a collection of brain\ngraphs.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 08:12:17 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 01:23:28 GMT"}, {"version": "v3", "created": "Fri, 15 May 2020 17:22:04 GMT"}, {"version": "v4", "created": "Tue, 22 Sep 2020 13:47:17 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Yang", "Congyuan", ""], ["Priebe", "Carey E.", ""], ["Park", "Youngser", ""], ["Marchette", "David J.", ""]]}, {"id": "1904.02931", "submitter": "Takamasa Okudono", "authors": "Takamasa Okudono, Masaki Waga, Taro Sekiyama, Ichiro Hasuo", "title": "Weighted Automata Extraction from Recurrent Neural Networks via\n  Regression on State Spaces", "comments": "AAAI 2020. We are preparing to distribute the implementation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to extract a weighted finite automaton (WFA) from a\nrecurrent neural network (RNN). Our algorithm is based on the WFA learning\nalgorithm by Balle and Mohri, which is in turn an extension of Angluin's\nclassic \\lstar algorithm. Our technical novelty is in the use of\n\\emph{regression} methods for the so-called equivalence queries, thus\nexploiting the internal state space of an RNN to prioritize counterexample\ncandidates. This way we achieve a quantitative/weighted extension of the recent\nwork by Weiss, Goldberg and Yahav that extracts DFAs. We experimentally\nevaluate the accuracy, expressivity and efficiency of the extracted WFAs.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 08:21:45 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 05:33:58 GMT"}, {"version": "v3", "created": "Wed, 20 Nov 2019 07:23:23 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Okudono", "Takamasa", ""], ["Waga", "Masaki", ""], ["Sekiyama", "Taro", ""], ["Hasuo", "Ichiro", ""]]}, {"id": "1904.02958", "submitter": "Hyenkyun Woo", "authors": "Hyenkyun Woo", "title": "Logitron: Perceptron-augmented classification model based on an extended\n  logistic loss function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification is the most important process in data analysis. However, due\nto the inherent non-convex and non-smooth structure of the zero-one loss\nfunction of the classification model, various convex surrogate loss functions\nsuch as hinge loss, squared hinge loss, logistic loss, and exponential loss are\nintroduced. These loss functions have been used for decades in diverse\nclassification models, such as SVM (support vector machine) with hinge loss,\nlogistic regression with logistic loss, and Adaboost with exponential loss and\nso on. In this work, we present a Perceptron-augmented convex classification\nframework, {\\it Logitron}. The loss function of it is a smoothly stitched\nfunction of the extended logistic loss with the famous Perceptron loss\nfunction. The extended logistic loss function is a parameterized function\nestablished based on the extended logarithmic function and the extended\nexponential function. The main advantage of the proposed Logitron\nclassification model is that it shows the connection between SVM and logistic\nregression via polynomial parameterization of the loss function. In more\ndetails, depending on the choice of parameters, we have the Hinge-Logitron\nwhich has the generalized $k$-th order hinge-loss with an additional $k$-th\nroot stabilization function and the Logistic-Logitron which has a logistic-like\nloss function with relatively large $|k|$. Interestingly, even $k=-1$,\nHinge-Logitron satisfies the classification-calibration condition and shows\nreasonable classification performance with low computational cost. The\nnumerical experiment in the linear classifier framework demonstrates that\nHinge-Logitron with $k=4$ (the fourth-order SVM with the fourth root\nstabilization function) outperforms logistic regression, SVM, and other\nLogitron models in terms of classification accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 09:39:57 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Woo", "Hyenkyun", ""]]}, {"id": "1904.02963", "submitter": "Vincenzo Matta", "authors": "Vincenzo Matta, Augusto Santos, Ali H. Sayed", "title": "Graph Learning over Partially Observed Diffusion Networks: Role of\n  Degree Concentration", "comments": "Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.MA stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work examines the problem of graph learning over a diffusion network\nwhen data can be collected from a limited portion of the network (partial\nobservability). The main question is to establish technical guarantees of\nconsistent recovery of the subgraph of probed network nodes, i) despite the\npresence of unobserved nodes; and ii) under different connectivity regimes,\nincluding the dense regime where the probed nodes are influenced by many\nconnections coming from the unobserved ones. We ascertain that suitable\nestimators of the combination matrix (i.e., the matrix that quantifies the\npairwise interaction between nodes) possess an identifiability gap that enables\nthe discrimination between connected and disconnected nodes. Fundamental\nconditions are established under which the subgraph of monitored nodes can be\nrecovered, with high probability as the network size increases, through\nuniversal clustering algorithms. This claim is proved for three matrix\nestimators: i) the Granger estimator that adapts to the partial observability\nsetting the solution that is exact under full observability ; ii) the one-lag\ncorrelation matrix; and iii) the residual estimator based on the difference\nbetween two consecutive time samples. A detailed characterization of the\nasymptotic behavior of these estimators is established in terms of an error\nbias and of the identifiability gap, and a sample complexity analysis is\nperformed to establish how the number of samples scales with the network size\nto achieve consistent learning. Comparison among the estimators is performed\nthrough illustrative examples that show how estimators that are not optimal in\nthe full observability regime can outperform the Granger estimator in the\npartial observability regime. The analysis reveals that the fundamental\nproperty enabling consistent graph learning is the statistical concentration of\nnode degrees.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 09:48:21 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 15:44:19 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Matta", "Vincenzo", ""], ["Santos", "Augusto", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1904.02971", "submitter": "Henri Riihim\\\"aki", "authors": "Henri Riihim\\\"aki, Wojciech Chach\\'olski, Jakob Theorell, Jan Hillert,\n  Ryan Ramanujam", "title": "A topological data analysis based classification method for multiple\n  measurements", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.AT stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning models for repeated measurements are limited. Using\ntopological data analysis (TDA), we present a classifier for repeated\nmeasurements which samples from the data space and builds a network graph based\non the data topology. When applying this to two case studies, accuracy exceeds\nalternative models with additional benefits such as reporting data subsets with\nhigh purity along with feature values. For 300 examples of 3 tree species, the\naccuracy reached 80% after 30 datapoints, which was improved to 90% after\nincreased sampling to 400 datapoints. Using data from 100 examples of each of 6\npoint processes, the classifier achieved 96.8% accuracy. In both datasets, the\nTDA classifier outperformed an alternative model. This algorithm and software\ncan be beneficial for repeated measurement data common in biological sciences,\nas both an accurate classifier and a feature selection tool.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 10:06:55 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Riihim\u00e4ki", "Henri", ""], ["Chach\u00f3lski", "Wojciech", ""], ["Theorell", "Jakob", ""], ["Hillert", "Jan", ""], ["Ramanujam", "Ryan", ""]]}, {"id": "1904.03014", "submitter": "Yu Cheng", "authors": "Duo Wang, Yu Cheng, Mo Yu, Xiaoxiao Guo, Tao Zhang", "title": "A Hybrid Approach with Optimization and Metric-based Meta-Learner for\n  Few-Shot Learning", "comments": "Accepted to Neurocomputing journal, code will be released soon. arXiv\n  admin note: text overlap with arXiv:1901.09890", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning aims to learn classifiers for new classes with only a few\ntraining examples per class. Most existing few-shot learning approaches belong\nto either metric-based meta-learning or optimization-based meta-learning\ncategory, both of which have achieved successes in the simplified \"$k$-shot\n$N$-way\" image classification settings. Specifically, the optimization-based\napproaches train a meta-learner to predict the parameters of the task-specific\nclassifiers. The task-specific classifiers are required to be\nhomogeneous-structured to ease the parameter prediction, so the meta-learning\napproaches could only handle few-shot learning problems where the tasks share a\nuniform number of classes. The metric-based approaches learn one task-invariant\nmetric for all the tasks. Even though the metric-learning approaches allow\ndifferent numbers of classes, they require the tasks all coming from a similar\ndomain such that there exists a uniform metric that could work across tasks. In\nthis work, we propose a hybrid meta-learning model called Meta-Metric-Learner\nwhich combines the merits of both optimization- and metric-based approaches.\nOur meta-metric-learning approach consists of two components, a task-specific\nmetric-based learner as a base model, and a meta-learner that learns and\nspecifies the base model. Thus our model is able to handle flexible numbers of\nclasses as well as generate more generalized metrics for classification across\ntasks. We test our approach in the standard \"$k$-shot $N$-way\" few-shot\nlearning setting following previous works and a new realistic few-shot setting\nwith flexible class numbers in both single-source form and multi-source forms.\nExperiments show that our approach can obtain superior performance in all\nsettings.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 07:31:34 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 01:35:22 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Wang", "Duo", ""], ["Cheng", "Yu", ""], ["Yu", "Mo", ""], ["Guo", "Xiaoxiao", ""], ["Zhang", "Tao", ""]]}, {"id": "1904.03061", "submitter": "Zimin Chen", "authors": "Zimin Chen and Martin Monperrus", "title": "A Literature Study of Embeddings on Source Code", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language processing has improved tremendously after the success of\nword embedding techniques such as word2vec. Recently, the same idea has been\napplied on source code with encouraging results. In this survey, we aim to\ncollect and discuss the usage of word embedding techniques on programs and\nsource code. The articles in this survey have been collected by asking authors\nof related work and with an extensive search on Google Scholar. Each article is\ncategorized into five categories: 1. embedding of tokens 2. embedding of\nfunctions or methods 3. embedding of sequences or sets of method calls 4.\nembedding of binary code 5. other embeddings. We also provide links to\nexperimental data and show some remarkable visualization of code embeddings. In\nsummary, word embedding has been successfully applied on different\ngranularities of source code. With access to countless open-source\nrepositories, we see a great potential of applying other data-driven natural\nlanguage processing techniques on source code in the future.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 13:37:42 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Chen", "Zimin", ""], ["Monperrus", "Martin", ""]]}, {"id": "1904.03063", "submitter": "Edwin D. Simpson", "authors": "Edwin Simpson, Steven Reece, Stephen J. Roberts", "title": "Bayesian Heatmaps: Probabilistic Classification with Multiple Unreliable\n  Information Sources", "comments": null, "journal-ref": "Joint European Conference on Machine Learning and Knowledge\n  Discovery in Databases (2017), pp. 109-125, Springer, Cham", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unstructured data from diverse sources, such as social media and aerial\nimagery, can provide valuable up-to-date information for intelligent situation\nassessment. Mining these different information sources could bring major\nbenefits to applications such as situation awareness in disaster zones and\nmapping the spread of diseases. Such applications depend on classifying the\nsituation across a region of interest, which can be depicted as a spatial\n\"heatmap\". Annotating unstructured data using crowdsourcing or automated\nclassifiers produces individual classifications at sparse locations that\ntypically contain many errors. We propose a novel Bayesian approach that models\nthe relevance, error rates and bias of each information source, enabling us to\nlearn a spatial Gaussian Process classifier by aggregating data from multiple\nsources with varying reliability and relevance. Our method does not require\ngold-labelled data and can make predictions at any location in an area of\ninterest given only sparse observations. We show empirically that our approach\ncan handle noisy and biased data sources, and that simultaneously inferring\nreliability and transferring information between neighbouring reports leads to\nmore accurate predictions. We demonstrate our method on two real-world problems\nfrom disaster response, showing how our approach reduces the amount of\ncrowdsourced data required and can be used to generate valuable heatmap\nvisualisations from SMS messages and satellite images.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 13:39:42 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Simpson", "Edwin", ""], ["Reece", "Steven", ""], ["Roberts", "Stephen J.", ""]]}, {"id": "1904.03081", "submitter": "Thomas M\\\"ollenhoff", "authors": "Michael Moeller, Thomas M\\\"ollenhoff, Daniel Cremers", "title": "Controlling Neural Networks via Energy Dissipation", "comments": "Published as a conference paper at ICCV 2019, Seoul", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last decade has shown a tremendous success in solving various computer\nvision problems with the help of deep learning techniques. Lately, many works\nhave demonstrated that learning-based approaches with suitable network\narchitectures even exhibit superior performance for the solution of (ill-posed)\nimage reconstruction problems such as deblurring, super-resolution, or medical\nimage reconstruction. The drawback of purely learning-based methods, however,\nis that they cannot provide provable guarantees for the trained network to\nfollow a given data formation process during inference. In this work we propose\nenergy dissipating networks that iteratively compute a descent direction with\nrespect to a given cost function or energy at the currently estimated\nreconstruction. Therefore, an adaptive step size rule such as a line-search,\nalong with a suitable number of iterations can guarantee the reconstruction to\nfollow a given data formation model encoded in the energy to arbitrary\nprecision, and hence control the model's behavior even during test time. We\nprove that under standard assumptions, descent using the direction predicted by\nthe network converges (linearly) to the global minimum of the energy. We\nillustrate the effectiveness of the proposed approach in experiments on single\nimage super resolution and computed tomography (CT) reconstruction, and further\nillustrate extensions to convex feasibility problems.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 14:13:55 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 11:54:46 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Moeller", "Michael", ""], ["M\u00f6llenhoff", "Thomas", ""], ["Cremers", "Daniel", ""]]}, {"id": "1904.03136", "submitter": "Cheng Mao", "authors": "Jan-Christian H\\\"utter, Cheng Mao, Philippe Rigollet and Elina Robeva", "title": "Estimation of Monge Matrices", "comments": "42 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monge matrices and their permuted versions known as pre-Monge matrices\nnaturally appear in many domains across science and engineering. While the rich\nstructural properties of such matrices have long been leveraged for algorithmic\npurposes, little is known about their impact on statistical estimation. In this\nwork, we propose to view this structure as a shape constraint and study the\nproblem of estimating a Monge matrix subject to additive random noise. More\nspecifically, we establish the minimax rates of estimation of Monge and\npre-Monge matrices. In the case of pre-Monge matrices, the minimax-optimal\nleast-squares estimator is not efficiently computable, and we propose two\nefficient estimators and establish their rates of convergence. Our theoretical\nfindings are supported by numerical experiments.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 16:01:52 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["H\u00fctter", "Jan-Christian", ""], ["Mao", "Cheng", ""], ["Rigollet", "Philippe", ""], ["Robeva", "Elina", ""]]}, {"id": "1904.03170", "submitter": "Maoying Qiao", "authors": "Maoying Qiao, Wei Bian, Richard Yida Xu, Dacheng Tao", "title": "Diversified Hidden Markov Models for Sequential Labeling", "comments": "14 pages, 12 figures", "journal-ref": "IEEE Transactions on Knowledge and Data Engineering, 27 (2015)\n  2947 - 2960", "doi": "10.1109/TKDE.2015.2433262", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Labeling of sequential data is a prevalent meta-problem for a wide range of\nreal world applications. While the first-order Hidden Markov Models (HMM)\nprovides a fundamental approach for unsupervised sequential labeling, the basic\nmodel does not show satisfying performance when it is directly applied to real\nworld problems, such as part-of-speech tagging (PoS tagging) and optical\ncharacter recognition (OCR). Aiming at improving performance, important\nextensions of HMM have been proposed in the literatures. One of the common key\nfeatures in these extensions is the incorporation of proper prior information.\nIn this paper, we propose a new extension of HMM, termed diversified Hidden\nMarkov Models (dHMM), which utilizes a diversity-encouraging prior over the\nstate-transition probabilities and thus facilitates more dynamic sequential\nlabellings. Specifically, the diversity is modeled by a continuous\ndeterminantal point process prior, which we apply to both unsupervised and\nsupervised scenarios. Learning and inference algorithms for dHMM are derived.\nEmpirical evaluations on benchmark datasets for unsupervised PoS tagging and\nsupervised OCR confirmed the effectiveness of dHMM, with competitive\nperformance to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 17:37:40 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Qiao", "Maoying", ""], ["Bian", "Wei", ""], ["Xu", "Richard Yida", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.03246", "submitter": "Xin Zhang", "authors": "Xin Zhang and Zhengyuan Zhu", "title": "Spatial CUSUM for Signal Region Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting weak clustered signal in spatial data is important but challenging\nin applications such as medical image and epidemiology. A more efficient\ndetection algorithm can provide more precise early warning, and effectively\nreduce the decision risk and cost. To date, many methods have been developed to\ndetect signals with spatial structures. However, most of the existing methods\nare either too conservative for weak signals or computationally too intensive.\nIn this paper, we consider a novel method named Spatial CUSUM (SCUSUM), which\nemploys the idea of the CUSUM procedure and false discovery rate controlling.\nWe develop theoretical properties of the method which indicates that\nasymptotically SCUSUM can reach high classification accuracy. In the simulation\nstudy, we demonstrate that SCUSUM is sensitive to weak spatial signals. This\nnew method is applied to a real fMRI dataset as illustration, and more\nirregular weak spatial signals are detected in the images compared to some\nexisting methods, including the conventional FDR, FDR$_L$ and scan statistics.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 19:25:11 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Zhang", "Xin", ""], ["Zhu", "Zhengyuan", ""]]}, {"id": "1904.03257", "submitter": "Alexander Ratner", "authors": "Alexander Ratner, Dan Alistarh, Gustavo Alonso, David G. Andersen,\n  Peter Bailis, Sarah Bird, Nicholas Carlini, Bryan Catanzaro, Jennifer Chayes,\n  Eric Chung, Bill Dally, Jeff Dean, Inderjit S. Dhillon, Alexandros Dimakis,\n  Pradeep Dubey, Charles Elkan, Grigori Fursin, Gregory R. Ganger, Lise Getoor,\n  Phillip B. Gibbons, Garth A. Gibson, Joseph E. Gonzalez, Justin Gottschlich,\n  Song Han, Kim Hazelwood, Furong Huang, Martin Jaggi, Kevin Jamieson, Michael\n  I. Jordan, Gauri Joshi, Rania Khalaf, Jason Knight, Jakub Kone\\v{c}n\\'y, Tim\n  Kraska, Arun Kumar, Anastasios Kyrillidis, Aparna Lakshmiratan, Jing Li,\n  Samuel Madden, H. Brendan McMahan, Erik Meijer, Ioannis Mitliagkas, Rajat\n  Monga, Derek Murray, Kunle Olukotun, Dimitris Papailiopoulos, Gennady\n  Pekhimenko, Theodoros Rekatsinas, Afshin Rostamizadeh, Christopher R\\'e,\n  Christopher De Sa, Hanie Sedghi, Siddhartha Sen, Virginia Smith, Alex Smola,\n  Dawn Song, Evan Sparks, Ion Stoica, Vivienne Sze, Madeleine Udell, Joaquin\n  Vanschoren, Shivaram Venkataraman, Rashmi Vinayak, Markus Weimer, Andrew\n  Gordon Wilson, Eric Xing, Matei Zaharia, Ce Zhang, Ameet Talwalkar", "title": "MLSys: The New Frontier of Machine Learning Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.DC cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) techniques are enjoying rapidly increasing adoption.\nHowever, designing and implementing the systems that support ML models in\nreal-world deployments remains a significant obstacle, in large part due to the\nradically different development and deployment profile of modern ML methods,\nand the range of practical concerns that come with broader adoption. We propose\nto foster a new systems machine learning research community at the intersection\nof the traditional systems and ML communities, focused on topics such as\nhardware systems for ML, software systems for ML, and ML optimized for metrics\nbeyond predictive accuracy. To do this, we describe a new conference, MLSys,\nthat explicitly targets research at the intersection of systems and machine\nlearning with a program committee split evenly between experts in systems and\nML, and an explicit focus on topics at the intersection of the two.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 12:43:36 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 04:55:56 GMT"}, {"version": "v3", "created": "Sun, 1 Dec 2019 20:27:06 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Ratner", "Alexander", ""], ["Alistarh", "Dan", ""], ["Alonso", "Gustavo", ""], ["Andersen", "David G.", ""], ["Bailis", "Peter", ""], ["Bird", "Sarah", ""], ["Carlini", "Nicholas", ""], ["Catanzaro", "Bryan", ""], ["Chayes", "Jennifer", ""], ["Chung", "Eric", ""], ["Dally", "Bill", ""], ["Dean", "Jeff", ""], ["Dhillon", "Inderjit S.", ""], ["Dimakis", "Alexandros", ""], ["Dubey", "Pradeep", ""], ["Elkan", "Charles", ""], ["Fursin", "Grigori", ""], ["Ganger", "Gregory R.", ""], ["Getoor", "Lise", ""], ["Gibbons", "Phillip B.", ""], ["Gibson", "Garth A.", ""], ["Gonzalez", "Joseph E.", ""], ["Gottschlich", "Justin", ""], ["Han", "Song", ""], ["Hazelwood", "Kim", ""], ["Huang", "Furong", ""], ["Jaggi", "Martin", ""], ["Jamieson", "Kevin", ""], ["Jordan", "Michael I.", ""], ["Joshi", "Gauri", ""], ["Khalaf", "Rania", ""], ["Knight", "Jason", ""], ["Kone\u010dn\u00fd", "Jakub", ""], ["Kraska", "Tim", ""], ["Kumar", "Arun", ""], ["Kyrillidis", "Anastasios", ""], ["Lakshmiratan", "Aparna", ""], ["Li", "Jing", ""], ["Madden", "Samuel", ""], ["McMahan", "H. Brendan", ""], ["Meijer", "Erik", ""], ["Mitliagkas", "Ioannis", ""], ["Monga", "Rajat", ""], ["Murray", "Derek", ""], ["Olukotun", "Kunle", ""], ["Papailiopoulos", "Dimitris", ""], ["Pekhimenko", "Gennady", ""], ["Rekatsinas", "Theodoros", ""], ["Rostamizadeh", "Afshin", ""], ["R\u00e9", "Christopher", ""], ["De Sa", "Christopher", ""], ["Sedghi", "Hanie", ""], ["Sen", "Siddhartha", ""], ["Smith", "Virginia", ""], ["Smola", "Alex", ""], ["Song", "Dawn", ""], ["Sparks", "Evan", ""], ["Stoica", "Ion", ""], ["Sze", "Vivienne", ""], ["Udell", "Madeleine", ""], ["Vanschoren", "Joaquin", ""], ["Venkataraman", "Shivaram", ""], ["Vinayak", "Rashmi", ""], ["Weimer", "Markus", ""], ["Wilson", "Andrew Gordon", ""], ["Xing", "Eric", ""], ["Zaharia", "Matei", ""], ["Zhang", "Ce", ""], ["Talwalkar", "Ameet", ""]]}, {"id": "1904.03259", "submitter": "Stephen Odaibo", "authors": "Stephen G. Odaibo", "title": "Is 'Unsupervised Learning' a Misconceived Term?", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is all of machine learning supervised to some degree? The field of machine\nlearning has traditionally been categorized pedagogically into\n$supervised~vs~unsupervised~learning$; where supervised learning has typically\nreferred to learning from labeled data, while unsupervised learning has\ntypically referred to learning from unlabeled data. In this paper, we assert\nthat all machine learning is in fact supervised to some degree, and that the\nscope of supervision is necessarily commensurate to the scope of learning\npotential. In particular, we argue that clustering algorithms such as k-means,\nand dimensionality reduction algorithms such as principal component analysis,\nvariational autoencoders, and deep belief networks are each internally\nsupervised by the data itself to learn their respective representations of its\nfeatures. Furthermore, these algorithms are not capable of external inference\nuntil their respective outputs (clusters, principal components, or\nrepresentation codes) have been identified and externally labeled in effect. As\nsuch, they do not suffice as examples of unsupervised learning. We propose that\nthe categorization `supervised vs unsupervised learning' be dispensed with, and\ninstead, learning algorithms be categorized as either\n$internally~or~externally~supervised$ (or both). We believe this change in\nperspective will yield new fundamental insights into the structure and\ncharacter of data and of learning algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 20:05:46 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Odaibo", "Stephen G.", ""]]}, {"id": "1904.03272", "submitter": "Brendan Ames", "authors": "Polina Bombina, Brendan Ames", "title": "Convex optimization for the densest subgraph and densest submatrix\n  problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the densest $k$-subgraph problem, which seeks to identify the\n$k$-node subgraph of a given input graph with maximum number of edges. This\nproblem is well-known to be NP-hard, by reduction to the maximum clique\nproblem. We propose a new convex relaxation for the densest $k$-subgraph\nproblem, based on a nuclear norm relaxation of a low-rank plus sparse\ndecomposition of the adjacency matrices of $k$-node subgraphs to partially\naddress this intractability. We establish that the densest $k$-subgraph can be\nrecovered with high probability from the optimal solution of this convex\nrelaxation if the input graph is randomly sampled from a distribution of random\ngraphs constructed to contain an especially dense $k$-node subgraph with high\nprobability. Specifically, the relaxation is exact when the edges of the input\ngraph are added independently at random, with edges within a particular\n$k$-node subgraph added with higher probability than other edges in the graph.\nWe provide a sufficient condition on the size of this subgraph $k$ and the\nexpected density under which the optimal solution of the proposed relaxation\nrecovers this $k$-node subgraph with high probability. Further, we propose a\nfirst-order method for solving this relaxation based on the alternating\ndirection method of multipliers, and empirically confirm our predicted recovery\nthresholds using simulations involving randomly generated graphs, as well as\ngraphs drawn from social and collaborative networks.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 20:47:07 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Bombina", "Polina", ""], ["Ames", "Brendan", ""]]}, {"id": "1904.03275", "submitter": "Tyler Maunu", "authors": "Tyler Maunu, Gilad Lerman", "title": "Robust Subspace Recovery with Adversarial Outliers", "comments": "21 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of robust subspace recovery (RSR) in the presence of\nadversarial outliers. That is, we seek a subspace that contains a large portion\nof a dataset when some fraction of the data points are arbitrarily corrupted.\nWe first examine a theoretical estimator that is intractable to calculate and\nuse it to derive information-theoretic bounds of exact recovery. We then\npropose two tractable estimators: a variant of RANSAC and a simple relaxation\nof the theoretical estimator. The two estimators are fast to compute and\nachieve state-of-the-art theoretical performance in a noiseless RSR setting\nwith adversarial outliers. The former estimator achieves better theoretical\nguarantees in the noiseless case, while the latter estimator is robust to small\nnoise, and its guarantees significantly improve with non-adversarial models of\noutliers. We give a complete comparison of guarantees for the adversarial RSR\nproblem, as well as a short discussion on the estimation of affine subspaces.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 21:00:04 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Maunu", "Tyler", ""], ["Lerman", "Gilad", ""]]}, {"id": "1904.03276", "submitter": "Hexiang Hu", "authors": "Hexiang Hu, Liyu Chen, Boqing Gong, Fei Sha", "title": "Synthesized Policies for Transfer and Adaptation across Tasks and\n  Environments", "comments": "presented at NeurIPS 2018 as a Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The ability to transfer in reinforcement learning is key towards building an\nagent of general artificial intelligence. In this paper, we consider the\nproblem of learning to simultaneously transfer across both environments (ENV)\nand tasks (TASK), probably more importantly, by learning from only sparse (ENV,\nTASK) pairs out of all the possible combinations. We propose a novel\ncompositional neural network architecture which depicts a meta rule for\ncomposing policies from the environment and task embeddings. Notably, one of\nthe main challenges is to learn the embeddings jointly with the meta rule. We\nfurther propose new training methods to disentangle the embeddings, making them\nboth distinctive signatures of the environments and tasks and effective\nbuilding blocks for composing the policies. Experiments on GridWorld and Thor,\nof which the agent takes as input an egocentric view, show that our approach\ngives rise to high success rates on all the (ENV, TASK) pairs after learning\nfrom only 40% of them.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 21:00:07 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 22:23:51 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Hu", "Hexiang", ""], ["Chen", "Liyu", ""], ["Gong", "Boqing", ""], ["Sha", "Fei", ""]]}, {"id": "1904.03292", "submitter": "Alessandro Achille", "authors": "Alessandro Achille, Giovanni Paolini, Glen Mbeng, Stefano Soatto", "title": "The Information Complexity of Learning Tasks, their Structure and their\n  Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": "UCLA CSD180003", "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an asymmetric distance in the space of learning tasks, and a\nframework to compute their complexity. These concepts are foundational for the\npractice of transfer learning, whereby a parametric model is pre-trained for a\ntask, and then fine-tuned for another. The framework we develop is\nnon-asymptotic, captures the finite nature of the training dataset, and allows\ndistinguishing learning from memorization. It encompasses, as special cases,\nclassical notions from Kolmogorov complexity, Shannon, and Fisher Information.\nHowever, unlike some of those frameworks, it can be applied to large-scale\nmodels and real-world datasets. Our framework is the first to measure\ncomplexity in a way that accounts for the effect of the optimization scheme,\nwhich is critical in Deep Learning.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 21:46:27 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 15:50:02 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Achille", "Alessandro", ""], ["Paolini", "Giovanni", ""], ["Mbeng", "Glen", ""], ["Soatto", "Stefano", ""]]}, {"id": "1904.03293", "submitter": "Chao Tao", "authors": "Chao Tao, Qin Zhang, Yuan Zhou", "title": "Collaborative Learning with Limited Interaction: Tight Bounds for\n  Distributed Exploration in Multi-Armed Bandits", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Best arm identification (or, pure exploration) in multi-armed bandits is a\nfundamental problem in machine learning. In this paper we study the distributed\nversion of this problem where we have multiple agents, and they want to learn\nthe best arm collaboratively. We want to quantify the power of collaboration\nunder limited interaction (or, communication steps), as interaction is\nexpensive in many settings. We measure the running time of a distributed\nalgorithm as the speedup over the best centralized algorithm where there is\nonly one agent. We give almost tight round-speedup tradeoffs for this problem,\nalong which we develop several new techniques for proving lower bounds on the\nnumber of communication steps under time or confidence constraints.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 21:48:19 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 01:16:26 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Tao", "Chao", ""], ["Zhang", "Qin", ""], ["Zhou", "Yuan", ""]]}, {"id": "1904.03295", "submitter": "Ishan Durugkar", "authors": "Ishan Durugkar, Matthew Hausknecht, Adith Swaminathan, Patrick\n  MacAlpine", "title": "Multi-Preference Actor Critic", "comments": "NeurIPS Workshop on Deep RL, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy gradient algorithms typically combine discounted future rewards with\nan estimated value function, to compute the direction and magnitude of\nparameter updates. However, for most Reinforcement Learning tasks, humans can\nprovide additional insight to constrain the policy learning. We introduce a\ngeneral method to incorporate multiple different feedback channels into a\nsingle policy gradient loss. In our formulation, the Multi-Preference Actor\nCritic (M-PAC), these different types of feedback are implemented as\nconstraints on the policy. We use a Lagrangian relaxation to satisfy these\nconstraints using gradient descent while learning a policy that maximizes\nrewards. Experiments in Atari and Pendulum verify that constraints are being\nrespected and can accelerate the learning process.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 21:50:50 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Durugkar", "Ishan", ""], ["Hausknecht", "Matthew", ""], ["Swaminathan", "Adith", ""], ["MacAlpine", "Patrick", ""]]}, {"id": "1904.03335", "submitter": "Nicolas Garcia Trillos", "authors": "Nicolas Garcia Trillos, Daniel Sanz-Alonso, Ruiyi Yang", "title": "Local Regularization of Noisy Point Clouds: Improved Global Geometric\n  Estimates and Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several data analysis techniques employ similarity relationships between data\npoints to uncover the intrinsic dimension and geometric structure of the\nunderlying data-generating mechanism. In this paper we work under the model\nassumption that the data is made of random perturbations of feature vectors\nlying on a low-dimensional manifold. We study two questions: how to define the\nsimilarity relationship over noisy data points, and what is the resulting\nimpact of the choice of similarity in the extraction of global geometric\ninformation from the underlying manifold. We provide concrete mathematical\nevidence that using a local regularization of the noisy data to define the\nsimilarity improves the approximation of the hidden Euclidean distance between\nunperturbed points. Furthermore, graph-based objects constructed with the\nlocally regularized similarity function satisfy better error bounds in their\nrecovery of global geometric ones. Our theory is supported by numerical\nexperiments that demonstrate that the gain in geometric understanding\nfacilitated by local regularization translates into a gain in classification\naccuracy in simulated and real data.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 01:52:05 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Trillos", "Nicolas Garcia", ""], ["Sanz-Alonso", "Daniel", ""], ["Yang", "Ruiyi", ""]]}, {"id": "1904.03348", "submitter": "Abram Magner", "authors": "Abram Magner and Wojciech Szpankowski", "title": "Toward Universal Testing of Dynamic Network Models", "comments": "Accepted to the 31st International Conference on Algorithmic Learning\n  Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous networks in the real world change over time, in the sense that nodes\nand edges enter and leave the networks. Various dynamic random graph models\nhave been proposed to explain the macroscopic properties of these systems and\nto provide a foundation for statistical inferences and predictions. It is of\ninterest to have a rigorous way to determine how well these models match\nobserved networks. We thus ask the following goodness of fit question: given a\nsequence of observations/snapshots of a growing random graph, along with a\ncandidate model M, can we determine whether the snapshots came from M or from\nsome arbitrary alternative model that is well-separated from M in some natural\nmetric? We formulate this problem precisely and boil it down to goodness of fit\ntesting for graph-valued, infinite-state Markov processes and exhibit and\nanalyze a universal test based on non-stationary sampling for a natural class\nof models.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 03:08:53 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 18:12:10 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Magner", "Abram", ""], ["Szpankowski", "Wojciech", ""]]}, {"id": "1904.03367", "submitter": "Anthony Manchin Mr.", "authors": "Anthony Manchin, Ehsan Abbasnejad, Anton van den Hengel", "title": "Reinforcement Learning with Attention that Works: A Self-Supervised\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention models have had a significant positive impact on deep learning\nacross a range of tasks. However previous attempts at integrating attention\nwith reinforcement learning have failed to produce significant improvements. We\npropose the first combination of self attention and reinforcement learning that\nis capable of producing significant improvements, including new state of the\nart results in the Arcade Learning Environment. Unlike the selective attention\nmodels used in previous attempts, which constrain the attention via\npreconceived notions of importance, our implementation utilises the Markovian\nproperties inherent in the state input. Our method produces a faithful\nvisualisation of the policy, focusing on the behaviour of the agent. Our\nexperiments demonstrate that the trained policies use multiple simultaneous\nfoci of attention, and are able to modulate attention over time to deal with\nsituations of partial observability.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 05:42:43 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Manchin", "Anthony", ""], ["Abbasnejad", "Ehsan", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1904.03391", "submitter": "Hazrat Ali", "authors": "Sulaiman Khan, Hazrat Ali, Zahid Ullah, Nasru Minallah, Shahid\n  Maqsood, Abdul Hafeez", "title": "KNN and ANN-based Recognition of Handwritten Pashto Letters using Zoning\n  Features", "comments": "(IJACSA) International Journal of Advanced Computer Science and\n  Applications,", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications (IJACSA), 9(10), June 2018", "doi": "10.14569/IJACSA.2018.091070", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a recognition system for handwritten Pashto letters.\nHowever, handwritten character recognition is a challenging task. These letters\nnot only differ in shape and style but also vary among individuals. The\nrecognition becomes further daunting due to the lack of standard datasets for\ninscribed Pashto letters. In this work, we have designed a database of moderate\nsize, which encompasses a total of 4488 images, stemming from 102\ndistinguishing samples for each of the 44 letters in Pashto. The recognition\nframework uses zoning feature extractor followed by K-Nearest Neighbour (KNN)\nand Neural Network (NN) classifiers for classifying individual letter. Based on\nthe evaluation of the proposed system, an overall classification accuracy of\napproximately 70.05% is achieved by using KNN while 72% is achieved by using\nNN.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 09:10:55 GMT"}, {"version": "v2", "created": "Sat, 8 Jun 2019 15:49:26 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Khan", "Sulaiman", ""], ["Ali", "Hazrat", ""], ["Ullah", "Zahid", ""], ["Minallah", "Nasru", ""], ["Maqsood", "Shahid", ""], ["Hafeez", "Abdul", ""]]}, {"id": "1904.03416", "submitter": "Santiago Pascual de la Puente", "authors": "Santiago Pascual, Mirco Ravanelli, Joan Serr\\`a, Antonio Bonafonte,\n  Yoshua Bengio", "title": "Learning Problem-agnostic Speech Representations from Multiple\n  Self-supervised Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning good representations without supervision is still an open issue in\nmachine learning, and is particularly challenging for speech signals, which are\noften characterized by long sequences with a complex hierarchical structure.\nSome recent works, however, have shown that it is possible to derive useful\nspeech representations by employing a self-supervised encoder-discriminator\napproach. This paper proposes an improved self-supervised method, where a\nsingle neural encoder is followed by multiple workers that jointly solve\ndifferent self-supervised tasks. The needed consensus across different tasks\nnaturally imposes meaningful constraints to the encoder, contributing to\ndiscover general representations and to minimize the risk of learning\nsuperficial ones. Experiments show that the proposed approach can learn\ntransferable, robust, and problem-agnostic features that carry on relevant\ninformation from the speech signal, such as speaker identity, phonemes, and\neven higher-level features such as emotional cues. In addition, a number of\ndesign choices make the encoder easily exportable, facilitating its direct\nusage or adaptation to different problems.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 10:51:25 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Pascual", "Santiago", ""], ["Ravanelli", "Mirco", ""], ["Serr\u00e0", "Joan", ""], ["Bonafonte", "Antonio", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1904.03423", "submitter": "Tomasz Kajdanowicz", "authors": "Piotr Bielak, Kamil Tagowski, Maciej Falkiewicz, Tomasz Kajdanowicz,\n  Nitesh V. Chawla", "title": "FILDNE: A Framework for Incremental Learning of Dynamic Networks\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning on graphs has emerged as a powerful mechanism to\nautomate feature vector generation for downstream machine learning tasks. The\nadvances in representation on graphs have centered on both homogeneous and\nheterogeneous graphs, where the latter presenting the challenges associated\nwith multi-typed nodes and/or edges. In this paper, we consider the additional\nchallenge of evolving graphs. We ask the question of whether the advances in\nrepresentation learning for static graphs can be leveraged for dynamic graphs\nand how? It is important to be able to incorporate those advances to maximize\nthe utility and generalization of methods. To that end, we propose the\nFramework for Incremental Learning of Dynamic Networks Embedding (FILDNE),\nwhich can utilize any existing static representation learning method for\nlearning node embeddings, while keeping the computational costs low. FILDNE\nintegrates the feature vectors computed using the standard methods over\ndifferent timesteps into a single representation by developing a convex\ncombination function and alignment mechanism. Experimental results on several\ndownstream tasks, over seven real-world data sets, show that FILDNE is able to\nreduce memory and computational time costs while providing competitive quality\nmeasure gains with respect to the contemporary methods for representation\nlearning on dynamic graphs.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 11:46:54 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 22:21:52 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Bielak", "Piotr", ""], ["Tagowski", "Kamil", ""], ["Falkiewicz", "Maciej", ""], ["Kajdanowicz", "Tomasz", ""], ["Chawla", "Nitesh V.", ""]]}, {"id": "1904.03438", "submitter": "Konrad Zolna", "authors": "Konrad Zolna, Negar Rostamzadeh, Yoshua Bengio, Sungjin Ahn, Pedro O.\n  Pinheiro", "title": "Reinforced Imitation in Heterogeneous Action Space", "comments": "The extended version of the work \"Reinforced Imitation Learning from\n  Observations\" presented on the NeurIPS workshop \"Imitation Learning and its\n  Challenges in Robotics\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imitation learning is an effective alternative approach to learn a policy\nwhen the reward function is sparse. In this paper, we consider a challenging\nsetting where an agent and an expert use different actions from each other. We\nassume that the agent has access to a sparse reward function and state-only\nexpert observations. We propose a method which gradually balances between the\nimitation learning cost and the reinforcement learning objective. In addition,\nthis method adapts the agent's policy based on either mimicking expert behavior\nor maximizing sparse reward. We show, through navigation scenarios, that (i) an\nagent is able to efficiently leverage sparse rewards to outperform standard\nstate-only imitation learning, (ii) it can learn a policy even when its actions\nare different from the expert, and (iii) the performance of the agent is not\nbounded by that of the expert, due to the optimized usage of sparse rewards.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 13:07:12 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2019 15:26:06 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Zolna", "Konrad", ""], ["Rostamzadeh", "Negar", ""], ["Bengio", "Yoshua", ""], ["Ahn", "Sungjin", ""], ["Pinheiro", "Pedro O.", ""]]}, {"id": "1904.03445", "submitter": "{\\L}ukasz Struski", "authors": "{\\L}ukasz Struski, Jacek Tabor, Igor Podolak, Aleksandra Nowak,\n  Krzysztof Maziarz", "title": "Realism Index: Interpolation in Generative Models With Arbitrary Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to perform plausible interpolations in the latent space of a\ngenerative model, we need a measure that credibly reflects if a point in an\ninterpolation is close to the data manifold being modelled, i.e. if it is\nconvincing. In this paper, we introduce a realism index of a point, which can\nbe constructed from an arbitrary prior density, or based on FID score approach\nin case a prior is not available. We propose a numerically efficient algorithm\nthat directly maximises the realism index of an interpolation which, as we\ntheoretically prove, leads to a search of a geodesic with respect to the\ncorresponding Riemann structure. We show that we obtain better interpolations\nthen the classical linear ones, in particular when either the prior density is\nnot convex shaped, or when the soap bubble effect appears.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 13:47:48 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 19:27:18 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Struski", "\u0141ukasz", ""], ["Tabor", "Jacek", ""], ["Podolak", "Igor", ""], ["Nowak", "Aleksandra", ""], ["Maziarz", "Krzysztof", ""]]}, {"id": "1904.03491", "submitter": "Rahul-Vigneswaran K", "authors": "Rahul-Vigneswaran K, Prabaharan Poornachandran and Soman KP", "title": "A Compendium on Network and Host based Intrusion Detection Systems", "comments": "8 pages, Accepted for ICDSMLA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.NE cs.NI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The techniques of deep learning have become the state of the art methodology\nfor executing complicated tasks from various domains of computer vision,\nnatural language processing, and several other areas. Due to its rapid\ndevelopment and promising benchmarks in those fields, researchers started\nexperimenting with this technique to perform in the area of, especially in\nintrusion detection related tasks. Deep learning is a subset and a natural\nextension of classical Machine learning and an evolved model of neural\nnetworks. This paper contemplates and discusses all the methodologies related\nto the leading edge Deep learning and Neural network models purposing to the\narena of Intrusion Detection Systems.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 16:45:42 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["K", "Rahul-Vigneswaran", ""], ["Poornachandran", "Prabaharan", ""], ["KP", "Soman", ""]]}, {"id": "1904.03513", "submitter": "Ramy Baly", "authors": "Abdelrhman Saleh (1), Ramy Baly (2), Alberto Barr\\'on-Cede\\~no (3),\n  Giovanni Da San Martino (3), Mitra Mohtarami (2), Preslav Nakov (3) and James\n  Glass (2) ((1) Harvard University, MA, USA, (2) MIT Computer Science and\n  Artificial Intelligence Laboratory, MA, USA, (3) Qatar Computing Research\n  Institute, HBKU, Qatar)", "title": "Team QCRI-MIT at SemEval-2019 Task 4: Propaganda Analysis Meets\n  Hyperpartisan News Detection", "comments": "Hyperpartisanship, propaganda, news media, fake news, SemEval-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe our submission to SemEval-2019 Task 4 on\nHyperpartisan News Detection. Our system relies on a variety of engineered\nfeatures originally used to detect propaganda. This is based on the assumption\nthat biased messages are propagandistic in the sense that they promote a\nparticular political cause or viewpoint. We trained a logistic regression model\nwith features ranging from simple bag-of-words to vocabulary richness and text\nreadability features. Our system achieved 72.9% accuracy on the test data that\nis annotated manually and 60.8% on the test data that is annotated with distant\nsupervision. Additional experiments showed that significant performance\nimprovements can be achieved with better feature pre-processing.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 19:04:29 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Saleh", "Abdelrhman", ""], ["Baly", "Ramy", ""], ["Barr\u00f3n-Cede\u00f1o", "Alberto", ""], ["Martino", "Giovanni Da San", ""], ["Mohtarami", "Mitra", ""], ["Nakov", "Preslav", ""], ["Glass", "James", ""]]}, {"id": "1904.03515", "submitter": "Micha{\\l} Zaj\\k{a}c", "authors": "Micha{\\l} Zaj\\k{a}c, Konrad Zolna, Stanis{\\l}aw Jastrz\\k{e}bski", "title": "Split Batch Normalization: Improving Semi-Supervised Learning under\n  Domain Shift", "comments": "Under review for ECML PKDD 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that using unlabeled data in semi-supervised learning\nis not always beneficial and can even hurt generalization, especially when\nthere is a class mismatch between the unlabeled and labeled examples. We\ninvestigate this phenomenon for image classification on the CIFAR-10 and the\nImageNet datasets, and with many other forms of domain shifts applied (e.g.\nsalt-and-pepper noise). Our main contribution is Split Batch Normalization\n(Split-BN), a technique to improve SSL when the additional unlabeled data comes\nfrom a shifted distribution. We achieve it by using separate batch\nnormalization statistics for unlabeled examples. Due to its simplicity, we\nrecommend it as a standard practice. Finally, we analyse how domain shift\naffects the SSL training process. In particular, we find that during training\nthe statistics of hidden activations in late layers become markedly different\nbetween the unlabeled and the labeled examples.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 19:10:05 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Zaj\u0105c", "Micha\u0142", ""], ["Zolna", "Konrad", ""], ["Jastrz\u0119bski", "Stanis\u0142aw", ""]]}, {"id": "1904.03516", "submitter": "Hwann-Tzong Chen", "authors": "Songhao Jia, Ding-Jie Chen, Hwann-Tzong Chen", "title": "Instance-Level Meta Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a normalization mechanism called Instance-Level Meta\nNormalization (ILM~Norm) to address a learning-to-normalize problem. ILM~Norm\nlearns to predict the normalization parameters via both the feature\nfeed-forward and the gradient back-propagation paths. ILM~Norm provides a meta\nnormalization mechanism and has several good properties. It can be easily\nplugged into existing instance-level normalization schemes such as Instance\nNormalization, Layer Normalization, or Group Normalization. ILM~Norm normalizes\neach instance individually and therefore maintains high performance even when\nsmall mini-batch is used. The experimental results show that ILM~Norm well\nadapts to different network architectures and tasks, and it consistently\nimproves the performance of the original models. The code is available at\nurl{https://github.com/Gasoonjia/ILM-Norm.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 19:37:18 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Jia", "Songhao", ""], ["Chen", "Ding-Jie", ""], ["Chen", "Hwann-Tzong", ""]]}, {"id": "1904.03535", "submitter": "Nikolaos Tziortziotis", "authors": "Nikolaos Tziortziotis, Christos Dimitrakakis, Michalis Vazirgiannis", "title": "Randomised Bayesian Least-Squares Policy Iteration", "comments": "European Workshop on Reinforcement Learning 14, October 2018, Lille,\n  France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Bayesian least-squares policy iteration (BLSPI), an off-policy,\nmodel-free, policy iteration algorithm that uses the Bayesian least-squares\ntemporal-difference (BLSTD) learning algorithm to evaluate policies. An online\nvariant of BLSPI has been also proposed, called randomised BLSPI (RBLSPI), that\nimproves its policy based on an incomplete policy evaluation step. In online\nsetting, the exploration-exploitation dilemma should be addressed as we try to\ndiscover the optimal policy by using samples collected by ourselves. RBLSPI\nexploits the advantage of BLSTD to quantify our uncertainty about the value\nfunction. Inspired by Thompson sampling, RBLSPI first samples a value function\nfrom a posterior distribution over value functions, and then selects actions\nbased on the sampled value function. The effectiveness and the exploration\nabilities of RBLSPI are demonstrated experimentally in several environments.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 21:50:24 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Tziortziotis", "Nikolaos", ""], ["Dimitrakakis", "Christos", ""], ["Vazirgiannis", "Michalis", ""]]}, {"id": "1904.03543", "submitter": "Huy Phan", "authors": "Huy Phan and Oliver Y. Ch\\'en and Lam Pham and Philipp Koch and\n  Maarten De Vos and Ian McLoughlin and Alfred Mertins", "title": "Spatio-Temporal Attention Pooling for Audio Scene Classification", "comments": "To appear at the 20th Annual Conference of the International Speech\n  Communication Association (INTERSPEECH 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acoustic scenes are rich and redundant in their content. In this work, we\npresent a spatio-temporal attention pooling layer coupled with a convolutional\nrecurrent neural network to learn from patterns that are discriminative while\nsuppressing those that are irrelevant for acoustic scene classification. The\nconvolutional layers in this network learn invariant features from\ntime-frequency input. The bidirectional recurrent layers are then able to\nencode the temporal dynamics of the resulting convolutional features.\nAfterwards, a two-dimensional attention mask is formed via the outer product of\nthe spatial and temporal attention vectors learned from two designated\nattention layers to weigh and pool the recurrent output into a final feature\nvector for classification. The network is trained with between-class examples\ngenerated from between-class data augmentation. Experiments demonstrate that\nthe proposed method not only outperforms a strong convolutional neural network\nbaseline but also sets new state-of-the-art performance on the LITIS Rouen\ndataset.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 22:49:20 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2019 12:32:52 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Phan", "Huy", ""], ["Ch\u00e9n", "Oliver Y.", ""], ["Pham", "Lam", ""], ["Koch", "Philipp", ""], ["De Vos", "Maarten", ""], ["McLoughlin", "Ian", ""], ["Mertins", "Alfred", ""]]}, {"id": "1904.03548", "submitter": "Roger  Fan", "authors": "Roger Fan, Byoungwook Jang, Yuekai Sun, Shuheng Zhou", "title": "Precision Matrix Estimation with Noisy and Missing Data", "comments": "27 pages, 14 figures, to appear in The 22nd International Conference\n  on Artificial Intelligence and Statistics (AISTATS 2019)", "journal-ref": null, "doi": null, "report-no": "Technical Report 545, Department of Statistics, University of\n  Michigan", "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating conditional dependence graphs and precision matrices are some of\nthe most common problems in modern statistics and machine learning. When data\nare fully observed, penalized maximum likelihood-type estimators have become\nstandard tools for estimating graphical models under sparsity conditions.\nExtensions of these methods to more complex settings where data are\ncontaminated with additive or multiplicative noise have been developed in\nrecent years. In these settings, however, the relative performance of different\nmethods is not well understood and algorithmic gaps still exist. In particular,\nin high-dimensional settings these methods require using non-positive\nsemidefinite matrices as inputs, presenting novel optimization challenges. We\ndevelop an alternating direction method of multipliers (ADMM) algorithm for\nthese problems, providing a feasible algorithm to estimate precision matrices\nwith indefinite input and potentially nonconvex penalties. We compare this\nmethod with existing alternative solutions and empirically characterize the\ntradeoffs between them. Finally, we use this method to explore the networks\namong US senators estimated from voting records data.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 00:05:57 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Fan", "Roger", ""], ["Jang", "Byoungwook", ""], ["Sun", "Yuekai", ""], ["Zhou", "Shuheng", ""]]}, {"id": "1904.03549", "submitter": "Jie Gui", "authors": "Jie Gui, Tongliang Liu, Zhenan Sun, Dacheng Tao, and Tieniu Tan", "title": "Supervised Discrete Hashing with Relaxation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-dependent hashing has recently attracted attention due to being able to\nsupport efficient retrieval and storage of high-dimensional data such as\ndocuments, images, and videos. In this paper, we propose a novel learning-based\nhashing method called \"Supervised Discrete Hashing with Relaxation\" (SDHR)\nbased on \"Supervised Discrete Hashing\" (SDH). SDH uses ordinary least squares\nregression and traditional zero-one matrix encoding of class label information\nas the regression target (code words), thus fixing the regression target. In\nSDHR, the regression target is instead optimized. The optimized regression\ntarget matrix satisfies a large margin constraint for correct classification of\neach example. Compared with SDH, which uses the traditional zero-one matrix,\nSDHR utilizes the learned regression target matrix and, therefore, more\naccurately measures the classification error of the regression model and is\nmore flexible. As expected, SDHR generally outperforms SDH. Experimental\nresults on two large-scale image datasets (CIFAR-10 and MNIST) and a\nlarge-scale and challenging face dataset (FRGC) demonstrate the effectiveness\nand efficiency of SDHR.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 00:09:19 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Gui", "Jie", ""], ["Liu", "Tongliang", ""], ["Sun", "Zhenan", ""], ["Tao", "Dacheng", ""], ["Tan", "Tieniu", ""]]}, {"id": "1904.03556", "submitter": "Jie Gui", "authors": "Jie Gui, Tongliang Liu, Zhenan Sun, Dacheng Tao, and Tieniu Tan", "title": "Fast Supervised Discrete Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-based hashing algorithms are ``hot topics\" because they can greatly\nincrease the scale at which existing methods operate. In this paper, we propose\na new learning-based hashing method called ``fast supervised discrete hashing\"\n(FSDH) based on ``supervised discrete hashing\" (SDH). Regressing the training\nexamples (or hash code) to the corresponding class labels is widely used in\nordinary least squares regression. Rather than adopting this method, FSDH uses\na very simple yet effective regression of the class labels of training examples\nto the corresponding hash code to accelerate the algorithm. To the best of our\nknowledge, this strategy has not previously been used for hashing. Traditional\nSDH decomposes the optimization into three sub-problems, with the most critical\nsub-problem - discrete optimization for binary hash codes - solved using\niterative discrete cyclic coordinate descent (DCC), which is time-consuming.\nHowever, FSDH has a closed-form solution and only requires a single rather than\niterative hash code-solving step, which is highly efficient. Furthermore, FSDH\nis usually faster than SDH for solving the projection matrix for least squares\nregression, making FSDH generally faster than SDH. For example, our results\nshow that FSDH is about 12-times faster than SDH when the number of hashing\nbits is 128 on the CIFAR-10 data base, and FSDH is about 151-times faster than\nFastHash when the number of hashing bits is 64 on the MNIST data-base. Our\nexperimental results show that FSDH is not only fast, but also outperforms\nother comparative methods.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 00:35:54 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Gui", "Jie", ""], ["Liu", "Tongliang", ""], ["Sun", "Zhenan", ""], ["Tao", "Dacheng", ""], ["Tan", "Tieniu", ""]]}, {"id": "1904.03564", "submitter": "Matthew Joseph", "authors": "Matthew Joseph, Jieming Mao, Seth Neel, Aaron Roth", "title": "The Role of Interactivity in Local Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the power of interactivity in local differential privacy. First, we\nfocus on the difference between fully interactive and sequentially interactive\nprotocols. Sequentially interactive protocols may query users adaptively in\nsequence, but they cannot return to previously queried users. The vast majority\nof existing lower bounds for local differential privacy apply only to\nsequentially interactive protocols, and before this paper it was not known\nwhether fully interactive protocols were more powerful. We resolve this\nquestion. First, we classify locally private protocols by their\ncompositionality, the multiplicative factor $k \\geq 1$ by which the sum of a\nprotocol's single-round privacy parameters exceeds its overall privacy\nguarantee. We then show how to efficiently transform any fully interactive\n$k$-compositional protocol into an equivalent sequentially interactive protocol\nwith an $O(k)$ blowup in sample complexity. Next, we show that our reduction is\ntight by exhibiting a family of problems such that for any $k$, there is a\nfully interactive $k$-compositional protocol which solves the problem, while no\nsequentially interactive protocol can solve the problem without at least an\n$\\tilde \\Omega(k)$ factor more examples. We then turn our attention to\nhypothesis testing problems. We show that for a large class of compound\nhypothesis testing problems --- which include all simple hypothesis testing\nproblems as a special case --- a simple noninteractive test is optimal among\nthe class of all (possibly fully interactive) tests.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 01:47:04 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 14:29:49 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Joseph", "Matthew", ""], ["Mao", "Jieming", ""], ["Neel", "Seth", ""], ["Roth", "Aaron", ""]]}, {"id": "1904.03579", "submitter": "Guangrun Wang", "authors": "Guangrun Wang, Keze Wang, Liang Lin", "title": "Adaptively Connected Neural Networks", "comments": "Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel adaptively connected neural network (ACNet) to\nimprove the traditional convolutional neural networks (CNNs) {in} two aspects.\nFirst, ACNet employs a flexible way to switch global and local inference in\nprocessing the internal feature representations by adaptively determining the\nconnection status among the feature nodes (e.g., pixels of the feature maps)\n\\footnote{In a computer vision domain, a node refers to a pixel of a feature\nmap{, while} in {the} graph domain, a node denotes a graph node.}. We can show\nthat existing CNNs, the classical multilayer perceptron (MLP), and the recently\nproposed non-local network (NLN) \\cite{nonlocalnn17} are all special cases of\nACNet. Second, ACNet is also capable of handling non-Euclidean data. Extensive\nexperimental analyses on {a variety of benchmarks (i.e.,} ImageNet-1k\nclassification, COCO 2017 detection and segmentation, CUHK03 person\nre-identification, CIFAR analysis, and Cora document categorization)\ndemonstrate that {ACNet} cannot only achieve state-of-the-art performance but\nalso overcome the limitation of the conventional MLP and CNN\n\\footnote{Corresponding author: Liang Lin (linliang@ieee.org)}. The code is\navailable at\n\\url{https://github.com/wanggrun/Adaptively-Connected-Neural-Networks}.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 04:01:27 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Wang", "Guangrun", ""], ["Wang", "Keze", ""], ["Lin", "Liang", ""]]}, {"id": "1904.03590", "submitter": "Phuong Tran", "authors": "Tran Thi Phuong and Le Trieu Phong", "title": "On the Convergence Proof of AMSGrad and a New Version", "comments": "Update publication information", "journal-ref": "IEEE Access, Volume 7, Issue 1, Pages 61706-61716, 2019", "doi": "10.1109/ACCESS.2019.2916341", "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adaptive moment estimation algorithm Adam (Kingma and Ba) is a popular\noptimizer in the training of deep neural networks. However, Reddi et al. have\nrecently shown that the convergence proof of Adam is problematic and proposed a\nvariant of Adam called AMSGrad as a fix. In this paper, we show that the\nconvergence proof of AMSGrad is also problematic. Concretely, the problem in\nthe convergence proof of AMSGrad is in handling the hyper-parameters, treating\nthem as equal while they are not. This is also the neglected issue in the\nconvergence proof of Adam. We provide an explicit counter-example of a simple\nconvex optimization setting to show this neglected issue. Depending on\nmanipulating the hyper-parameters, we present various fixes for this issue. We\nprovide a new convergence proof for AMSGrad as the first fix. We also propose a\nnew version of AMSGrad called AdamX as another fix. Our experiments on the\nbenchmark dataset also support our theoretical results.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 06:10:04 GMT"}, {"version": "v2", "created": "Sun, 21 Apr 2019 02:33:09 GMT"}, {"version": "v3", "created": "Fri, 24 May 2019 02:08:06 GMT"}, {"version": "v4", "created": "Thu, 31 Oct 2019 00:06:04 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Phuong", "Tran Thi", ""], ["Phong", "Le Trieu", ""]]}, {"id": "1904.03595", "submitter": "Youssef Tamaazousti", "authors": "Sara Meftah, Youssef Tamaazousti, Nasredine Semmar, Hassane Essafi,\n  Fatiha Sadat", "title": "Joint Learning of Pre-Trained and Random Units for Domain Adaptation in\n  Part-of-Speech Tagging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-tuning neural networks is widely used to transfer valuable knowledge\nfrom high-resource to low-resource domains. In a standard fine-tuning scheme,\nsource and target problems are trained using the same architecture. Although\ncapable of adapting to new domains, pre-trained units struggle with learning\nuncommon target-specific patterns. In this paper, we propose to augment the\ntarget-network with normalised, weighted and randomly initialised units that\nbeget a better adaptation while maintaining the valuable source knowledge. Our\nexperiments on POS tagging of social media texts (Tweets domain) demonstrate\nthat our method achieves state-of-the-art performances on 3 commonly used\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 06:46:36 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Meftah", "Sara", ""], ["Tamaazousti", "Youssef", ""], ["Semmar", "Nasredine", ""], ["Essafi", "Hassane", ""], ["Sadat", "Fatiha", ""]]}, {"id": "1904.03602", "submitter": "Amit Daniely", "authors": "Amit Daniely and Yishay Mansour", "title": "Competitive ratio versus regret minimization: achieving the best of both\n  worlds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider online algorithms under both the competitive ratio criteria and\nthe regret minimization one. Our main goal is to build a unified methodology\nthat would be able to guarantee both criteria simultaneously.\n  For a general class of online algorithms, namely any Metrical Task System\n(MTS), we show that one can simultaneously guarantee the best known competitive\nratio and a natural regret bound. For the paging problem we further show an\nefficient online algorithm (polynomial in the number of pages) with this\nguarantee.\n  To this end, we extend an existing regret minimization algorithm\n(specifically, Kapralov and Panigrahy) to handle movement cost (the cost of\nswitching between states of the online system). We then show how to use the\nextended regret minimization algorithm to combine multiple online algorithms.\nOur end result is an online algorithm that can combine a \"base\" online\nalgorithm, having a guaranteed competitive ratio, with a range of online\nalgorithms that guarantee a small regret over any interval of time. The\ncombined algorithm guarantees both that the competitive ratio matches that of\nthe base algorithm and a low regret over any time interval.\n  As a by product, we obtain an expert algorithm with close to optimal regret\nbound on every time interval, even in the presence of switching costs. This\nresult is of independent interest.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 08:09:42 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Daniely", "Amit", ""], ["Mansour", "Yishay", ""]]}, {"id": "1904.03620", "submitter": "Varshaneya V", "authors": "Varshaneya V, S Balasubramanian and Vineeth N Balasubramanian", "title": "Teaching GANs to Sketch in Vector Format", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sketching is more fundamental to human cognition than speech. Deep Neural\nNetworks (DNNs) have achieved the state-of-the-art in speech-related tasks but\nhave not made significant development in generating stroke-based sketches a.k.a\nsketches in vector format. Though there are Variational Auto Encoders (VAEs)\nfor generating sketches in vector format, there is no Generative Adversarial\nNetwork (GAN) architecture for the same. In this paper, we propose a standalone\nGAN architecture SkeGAN and a VAE-GAN architecture VASkeGAN, for sketch\ngeneration in vector format. SkeGAN is a stochastic policy in Reinforcement\nLearning (RL), capable of generating both multidimensional continuous and\ndiscrete outputs. VASkeGAN hybridizes a VAE and a GAN, in order to couple the\nefficient representation of data by VAE with the powerful generating\ncapabilities of a GAN, to produce visually appealing sketches. We also propose\na new metric called the Ske-score which quantifies the quality of vector\nsketches. We have validated that SkeGAN and VASkeGAN generate visually\nappealing sketches by using Human Turing Test and Ske-score.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 10:23:47 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["V", "Varshaneya", ""], ["Balasubramanian", "S", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "1904.03626", "submitter": "Guy Hacohen", "authors": "Guy Hacohen and Daphna Weinshall", "title": "On The Power of Curriculum Learning in Training Deep Networks", "comments": "In proceedings, ICML 2019", "journal-ref": "Proc. ICML, 2019", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training neural networks is traditionally done by providing a sequence of\nrandom mini-batches sampled uniformly from the entire training data. In this\nwork, we analyze the effect of curriculum learning, which involves the\nnon-uniform sampling of mini-batches, on the training of deep networks, and\nspecifically CNNs trained for image recognition. To employ curriculum learning,\nthe training algorithm must resolve 2 problems: (i) sort the training examples\nby difficulty; (ii) compute a series of mini-batches that exhibit an increasing\nlevel of difficulty. We address challenge (i) using two methods: transfer\nlearning from some competitive ``teacher\" network, and bootstrapping. In our\nempirical evaluation, both methods show similar benefits in terms of increased\nlearning speed and improved final performance on test data. We address\nchallenge (ii) by investigating different pacing functions to guide the\nsampling. The empirical investigation includes a variety of network\narchitectures, using images from CIFAR-10, CIFAR-100 and subsets of ImageNet.\nWe conclude with a novel theoretical analysis of curriculum learning, where we\nshow how it effectively modifies the optimization landscape. We then define the\nconcept of an ideal curriculum, and show that under mild conditions it does not\nchange the corresponding global minimum of the optimization function.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 11:36:35 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 15:06:37 GMT"}, {"version": "v3", "created": "Wed, 29 May 2019 16:26:35 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Hacohen", "Guy", ""], ["Weinshall", "Daphna", ""]]}, {"id": "1904.03646", "submitter": "Thomas Anthony", "authors": "Thomas Anthony and Robert Nishihara and Philipp Moritz and Tim\n  Salimans and John Schulman", "title": "Policy Gradient Search: Online Planning and Expert Iteration without\n  Search Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo Tree Search (MCTS) algorithms perform simulation-based search to\nimprove policies online. During search, the simulation policy is adapted to\nexplore the most promising lines of play. MCTS has been used by\nstate-of-the-art programs for many problems, however a disadvantage to MCTS is\nthat it estimates the values of states with Monte Carlo averages, stored in a\nsearch tree; this does not scale to games with very high branching factors. We\npropose an alternative simulation-based search method, Policy Gradient Search\n(PGS), which adapts a neural network simulation policy online via policy\ngradient updates, avoiding the need for a search tree. In Hex, PGS achieves\ncomparable performance to MCTS, and an agent trained using Expert Iteration\nwith PGS was able defeat MoHex 2.0, the strongest open-source Hex agent, in 9x9\nHex.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 13:00:21 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Anthony", "Thomas", ""], ["Nishihara", "Robert", ""], ["Moritz", "Philipp", ""], ["Salimans", "Tim", ""], ["Schulman", "John", ""]]}, {"id": "1904.03647", "submitter": "Rico Krueger", "authors": "Prateek Bansal, Rico Krueger, Michel Bierlaire, Ricardo A. Daziano,\n  Taha H. Rashidi", "title": "Bayesian Estimation of Mixed Multinomial Logit Models: Advances and\n  Simulation-Based Evaluations", "comments": null, "journal-ref": "Transportation Research Part B: Methodological, Volume 131,\n  January 2020, Pages 124-142", "doi": "10.1016/j.trb.2019.12.001", "report-no": null, "categories": "stat.ML cs.LG econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Bayes (VB) methods have emerged as a fast and\ncomputationally-efficient alternative to Markov chain Monte Carlo (MCMC)\nmethods for scalable Bayesian estimation of mixed multinomial logit (MMNL)\nmodels. It has been established that VB is substantially faster than MCMC at\npractically no compromises in predictive accuracy. In this paper, we address\ntwo critical gaps concerning the usage and understanding of VB for MMNL. First,\nextant VB methods are limited to utility specifications involving only\nindividual-specific taste parameters. Second, the finite-sample properties of\nVB estimators and the relative performance of VB, MCMC and maximum simulated\nlikelihood estimation (MSLE) are not known. To address the former, this study\nextends several VB methods for MMNL to admit utility specifications including\nboth fixed and random utility parameters. To address the latter, we conduct an\nextensive simulation-based evaluation to benchmark the extended VB methods\nagainst MCMC and MSLE in terms of estimation times, parameter recovery and\npredictive accuracy. The results suggest that all VB variants with the\nexception of the ones relying on an alternative variational lower bound\nconstructed with the help of the modified Jensen's inequality perform as well\nas MCMC and MSLE at prediction and parameter recovery. In particular, VB with\nnonconjugate variational message passing and the delta-method (VB-NCVMP-Delta)\nis up to 16 times faster than MCMC and MSLE. Thus, VB-NCVMP-Delta can be an\nattractive alternative to MCMC and MSLE for fast, scalable and accurate\nestimation of MMNL models.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 13:03:56 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 15:48:39 GMT"}, {"version": "v3", "created": "Thu, 15 Aug 2019 07:12:09 GMT"}, {"version": "v4", "created": "Thu, 12 Dec 2019 14:38:22 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Bansal", "Prateek", ""], ["Krueger", "Rico", ""], ["Bierlaire", "Michel", ""], ["Daziano", "Ricardo A.", ""], ["Rashidi", "Taha H.", ""]]}, {"id": "1904.03673", "submitter": "Kenji Kawaguchi", "authors": "Kenji Kawaguchi, Jiaoyang Huang, Leslie Pack Kaelbling", "title": "Every Local Minimum Value is the Global Minimum Value of Induced Model\n  in Non-convex Machine Learning", "comments": "Neural computation, MIT press", "journal-ref": "Neural computation, volume 31, pages 2293-2323, MIT press, 2019", "doi": "10.1162/neco_a_01234", "report-no": null, "categories": "stat.ML cs.LG cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For nonconvex optimization in machine learning, this article proves that\nevery local minimum achieves the globally optimal value of the perturbable\ngradient basis model at any differentiable point. As a result, nonconvex\nmachine learning is theoretically as supported as convex machine learning with\na handcrafted basis in terms of the loss at differentiable local minima, except\nin the case when a preference is given to the handcrafted basis over the\nperturbable gradient basis. The proofs of these results are derived under mild\nassumptions. Accordingly, the proven results are directly applicable to many\nmachine learning models, including practical deep neural networks, without any\nmodification of practical methods. Furthermore, as special cases of our general\nresults, this article improves or complements several state-of-the-art\ntheoretical results on deep neural networks, deep residual networks, and\noverparameterized deep neural networks with a unified proof technique and novel\ngeometric insights. A special case of our results also contributes to the\ntheoretical foundation of representation learning.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 15:43:59 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 06:00:53 GMT"}, {"version": "v3", "created": "Fri, 15 Nov 2019 23:04:15 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Kawaguchi", "Kenji", ""], ["Huang", "Jiaoyang", ""], ["Kaelbling", "Leslie Pack", ""]]}, {"id": "1904.03677", "submitter": "Shing Chan", "authors": "Shing Chan and Ahmed H. Elsheikh", "title": "Parametrization of stochastic inputs using generative adversarial\n  networks with application in geology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate artificial neural networks as a parametrization tool for\nstochastic inputs in numerical simulations. We address parametrization from the\npoint of view of emulating the data generating process, instead of explicitly\nconstructing a parametric form to preserve predefined statistics of the data.\nThis is done by training a neural network to generate samples from the data\ndistribution using a recent deep learning technique called generative\nadversarial networks. By emulating the data generating process, the relevant\nstatistics of the data are replicated. The method is assessed in subsurface\nflow problems, where effective parametrization of underground properties such\nas permeability is important due to the high dimensionality and presence of\nhigh spatial correlations. We experiment with realizations of binary\nchannelized subsurface permeability and perform uncertainty quantification and\nparameter estimation. Results show that the parametrization using generative\nadversarial networks is very effective in preserving visual realism as well as\nhigh order statistics of the flow responses, while achieving a dimensionality\nreduction of two orders of magnitude.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 16:04:56 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 08:41:26 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Chan", "Shing", ""], ["Elsheikh", "Ahmed H.", ""]]}, {"id": "1904.03688", "submitter": "Farhood Rismanchian", "authors": "Farhood Rismanchian and Karim Rahimian", "title": "Proposing a Localized Relevance Vector Machine for Pattern\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relevance vector machine (RVM) can be seen as a probabilistic version of\nsupport vector machines which is able to produce sparse solutions by linearly\nweighting a small number of basis functions instead using all of them.\nRegardless of a few merits of RVM such as giving probabilistic predictions and\nrelax of parameter tuning, it has poor prediction for test instances that are\nfar away from the relevance vectors. As a solution, we propose a new\ncombination of RVM and k-nearest neighbor (k-NN) rule which resolves this issue\nwith regionally dealing with every test instance. In our settings, we obtain\nthe relevance vectors for each test instance in the local area given by k-NN\nrule. In this way, relevance vectors are closer and more relevant to the test\ninstance which results in a more accurate model. This can be seen as a\npiece-wise learner which locally classifies test instances. The model is hence\ncalled localized relevance vector machine (LRVM). The LRVM is examined on\nseveral datasets of the University of California, Irvine (UCI) repository.\nResults supported by statistical tests indicate that the performance of LRVM is\ncompetitive as compared with a few state-of-the-art classifiers.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 17:00:42 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Rismanchian", "Farhood", ""], ["Rahimian", "Karim", ""]]}, {"id": "1904.03737", "submitter": "Ezequiel Smucler", "authors": "Ezequiel Smucler, Andrea Rotnitzky, James M. Robins", "title": "A unifying approach for doubly-robust $\\ell_1$ regularized estimation of\n  causal contrasts", "comments": "fixed example 11, added example 12", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider inference about a scalar parameter under a non-parametric model\nbased on a one-step estimator computed as a plug in estimator plus the\nempirical mean of an estimator of the parameter's influence function. We focus\non a class of parameters that have influence function which depends on two\ninfinite dimensional nuisance functions and such that the bias of the one-step\nestimator of the parameter of interest is the expectation of the product of the\nestimation errors of the two nuisance functions. Our class includes many\nimportant treatment effect contrasts of interest in causal inference and\neconometrics, such as ATE, ATT, an integrated causal contrast with a continuous\ntreatment, and the mean of an outcome missing not at random. We propose\nestimators of the target parameter that entertain approximately sparse\nregression models for the nuisance functions allowing for the number of\npotential confounders to be even larger than the sample size. By employing\nsample splitting, cross-fitting and $\\ell_1$-regularized regression estimators\nof the nuisance functions based on objective functions whose directional\nderivatives agree with those of the parameter's influence function, we obtain\nestimators of the target parameter with two desirable robustness properties:\n(1) they are rate doubly-robust in that they are root-n consistent and\nasymptotically normal when both nuisance functions follow approximately sparse\nmodels, even if one function has a very non-sparse regression coefficient, so\nlong as the other has a sufficiently sparse regression coefficient, and (2)\nthey are model doubly-robust in that they are root-n consistent and\nasymptotically normal even if one of the nuisance functions does not follow an\napproximately sparse model so long as the other nuisance function follows an\napproximately sparse model with a sufficiently sparse regression coefficient.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 20:34:26 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 19:07:12 GMT"}, {"version": "v3", "created": "Wed, 5 Jun 2019 21:59:14 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Smucler", "Ezequiel", ""], ["Rotnitzky", "Andrea", ""], ["Robins", "James M.", ""]]}, {"id": "1904.03743", "submitter": "Hassan Hafez Kolahi", "authors": "Hassan Hafez-Kolahi, Shohreh Kasaei", "title": "Information Bottleneck and its Applications in Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information Theory (IT) has been used in Machine Learning (ML) from early\ndays of this field. In the last decade, advances in Deep Neural Networks (DNNs)\nhave led to surprising improvements in many applications of ML. The result has\nbeen a paradigm shift in the community toward revisiting previous ideas and\napplications in this new framework. Ideas from IT are no exception. One of the\nideas which is being revisited by many researchers in this new era, is\nInformation Bottleneck (IB); a formulation of information extraction based on\nIT. The IB is promising in both analyzing and improving DNNs. The goal of this\nsurvey is to review the IB concept and demonstrate its applications in deep\nlearning. The information theoretic nature of IB, makes it also a good\ncandidate in showing the more general concept of how IT can be used in ML. Two\nimportant concepts are highlighted in this narrative on the subject, i) the\nconcise and universal view that IT provides on seemingly unrelated methods of\nML, demonstrated by explaining how IB relates to minimal sufficient statistics,\nstochastic gradient descent, and variational auto-encoders, and ii) the common\ntechnical mistakes and problems caused by applying ideas from IT, which is\ndiscussed by a careful study of some recent methods suffering from them.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 21:09:30 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Hafez-Kolahi", "Hassan", ""], ["Kasaei", "Shohreh", ""]]}, {"id": "1904.03746", "submitter": "Yoon Kim", "authors": "Yoon Kim, Alexander M. Rush, Lei Yu, Adhiguna Kuncoro, Chris Dyer,\n  G\\'abor Melis", "title": "Unsupervised Recurrent Neural Network Grammars", "comments": "NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural network grammars (RNNG) are generative models of language\nwhich jointly model syntax and surface structure by incrementally generating a\nsyntax tree and sentence in a top-down, left-to-right order. Supervised RNNGs\nachieve strong language modeling and parsing performance, but require an\nannotated corpus of parse trees. In this work, we experiment with unsupervised\nlearning of RNNGs. Since directly marginalizing over the space of latent trees\nis intractable, we instead apply amortized variational inference. To maximize\nthe evidence lower bound, we develop an inference network parameterized as a\nneural CRF constituency parser. On language modeling, unsupervised RNNGs\nperform as well their supervised counterparts on benchmarks in English and\nChinese. On constituency grammar induction, they are competitive with recent\nneural language models that induce tree structures from words through attention\nmechanisms.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 21:14:43 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 17:00:47 GMT"}, {"version": "v3", "created": "Mon, 15 Apr 2019 17:56:54 GMT"}, {"version": "v4", "created": "Wed, 12 Jun 2019 04:48:38 GMT"}, {"version": "v5", "created": "Fri, 14 Jun 2019 02:58:11 GMT"}, {"version": "v6", "created": "Mon, 5 Aug 2019 01:21:15 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Kim", "Yoon", ""], ["Rush", "Alexander M.", ""], ["Yu", "Lei", ""], ["Kuncoro", "Adhiguna", ""], ["Dyer", "Chris", ""], ["Melis", "G\u00e1bor", ""]]}, {"id": "1904.03779", "submitter": "Chengkun Zhang", "authors": "Chengkun Zhang. Junbin Gao, Stephen Lu", "title": "Cluster Developing 1-Bit Matrix Completion", "comments": "16 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion has a long-time history of usage as the core technique of\nrecommender systems. In particular, 1-bit matrix completion, which considers\nthe prediction as a ``Recommended'' or ``Not Recommended'' question, has proved\nits significance and validity in the field. However, while customers and\nproducts aggregate into interacted clusters, state-of-the-art model-based 1-bit\nrecommender systems do not take the consideration of grouping bias. To tackle\nthe gap, this paper introduced Group-Specific 1-bit Matrix Completion (GS1MC)\nby first-time consolidating group-specific effects into 1-bit recommender\nsystems under the low-rank latent variable framework. Additionally, to empower\nGS1MC even when grouping information is unobtainable, Cluster Developing Matrix\nCompletion (CDMC) was proposed by integrating the sparse subspace clustering\ntechnique into GS1MC. Namely, CDMC allows clustering users/items and to\nleverage their group effects into matrix completion at the same time.\nExperiments on synthetic and real-world data show that GS1MC outperforms the\ncurrent 1-bit matrix completion methods. Meanwhile, it is compelling that CDMC\ncan successfully capture items' genre features only based on sparse binary\nuser-item interactive data. Notably, GS1MC provides a new insight to\nincorporate and evaluate the efficacy of clustering methods while CDMC can be\nserved as a new tool to explore unrevealed social behavior or market\nphenomenon.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 23:50:34 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Gao", "Chengkun Zhang. Junbin", ""], ["Lu", "Stephen", ""]]}, {"id": "1904.03807", "submitter": "Chunsheng Liu", "authors": "Chunsheng Liu", "title": "Binary matrix completion with nonconvex regularizers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many practical problems involve the recovery of a binary matrix from partial\ninformation, which makes the binary matrix completion (BMC) technique received\nincreasing attention in machine learning. In particular, we consider a special\ncase of BMC problem, in which only a subset of positive elements can be\nobserved. In recent years, convex regularization based methods are the\nmainstream approaches for this task. However, the applications of nonconvex\nsurrogates in standard matrix completion have demonstrated better empirical\nperformance. Accordingly, we propose a novel BMC model with nonconvex\nregularizers and provide the recovery guarantee for the model. Furthermore, for\nsolving the resultant nonconvex optimization problem, we improve the popular\nproximal algorithm with acceleration strategies. It can be guaranteed that the\nconvergence rate of the algorithm is in the order of ${1/T}$, where $T$ is the\nnumber of iterations. Extensive experiments conducted on both synthetic and\nreal-world data sets demonstrate the superiority of the proposed approach over\nother competing methods.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 02:52:57 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Liu", "Chunsheng", ""]]}, {"id": "1904.03834", "submitter": "Alexander Greaves-Tunnell", "authors": "Alexander Greaves-Tunnell and Zaid Harchaoui", "title": "A Statistical Investigation of Long Memory in Language and Music", "comments": "29 pages; expanded supplement, added details in background and\n  methods per reviewer feedback, included additional references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation and learning of long-range dependencies is a central challenge\nconfronted in modern applications of machine learning to sequence data. Yet\ndespite the prominence of this issue, the basic problem of measuring long-range\ndependence, either in a given data source or as represented in a trained deep\nmodel, remains largely limited to heuristic tools. We contribute a statistical\nframework for investigating long-range dependence in current applications of\ndeep sequence modeling, drawing on the well-developed theory of long memory\nstochastic processes. This framework yields testable implications concerning\nthe relationship between long memory in real-world data and its learned\nrepresentation in a deep learning architecture, which are explored through a\nsemiparametric framework adapted to the high-dimensional setting.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 04:36:14 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2019 01:15:36 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Greaves-Tunnell", "Alexander", ""], ["Harchaoui", "Zaid", ""]]}, {"id": "1904.03837", "submitter": "Xiaohan Ding", "authors": "Xiaohan Ding, Guiguang Ding, Yuchen Guo, Jungong Han", "title": "Centripetal SGD for Pruning Very Deep Convolutional Networks with\n  Complicated Structure", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The redundancy is widely recognized in Convolutional Neural Networks (CNNs),\nwhich enables to remove unimportant filters from convolutional layers so as to\nslim the network with acceptable performance drop. Inspired by the linear and\ncombinational properties of convolution, we seek to make some filters\nincreasingly close and eventually identical for network slimming. To this end,\nwe propose Centripetal SGD (C-SGD), a novel optimization method, which can\ntrain several filters to collapse into a single point in the parameter\nhyperspace. When the training is completed, the removal of the identical\nfilters can trim the network with NO performance loss, thus no finetuning is\nneeded. By doing so, we have partly solved an open problem of constrained\nfilter pruning on CNNs with complicated structure, where some layers must be\npruned following others. Our experimental results on CIFAR-10 and ImageNet have\njustified the effectiveness of C-SGD-based filter pruning. Moreover, we have\nprovided empirical evidences for the assumption that the redundancy in deep\nneural networks helps the convergence of training by showing that a redundant\nCNN trained using C-SGD outperforms a normally trained counterpart with the\nequivalent width.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 04:48:02 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Ding", "Xiaohan", ""], ["Ding", "Guiguang", ""], ["Guo", "Yuchen", ""], ["Han", "Jungong", ""]]}, {"id": "1904.03858", "submitter": "Alexander Wein", "authors": "Alexander S. Wein, Ahmed El Alaoui, Cristopher Moore", "title": "The Kikuchi Hierarchy and Tensor PCA", "comments": "42 pages. This version adds results on odd-order tensor PCA and\n  even-arity XOR refutation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cond-mat.stat-mech cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the tensor PCA (principal component analysis) problem, we propose a new\nhierarchy of increasingly powerful algorithms with increasing runtime. Our\nhierarchy is analogous to the sum-of-squares (SOS) hierarchy but is instead\ninspired by statistical physics and related algorithms such as belief\npropagation and AMP (approximate message passing). Our level-$\\ell$ algorithm\ncan be thought of as a linearized message-passing algorithm that keeps track of\n$\\ell$-wise dependencies among the hidden variables. Specifically, our\nalgorithms are spectral methods based on the Kikuchi Hessian, which generalizes\nthe well-studied Bethe Hessian to the higher-order Kikuchi free energies.\n  It is known that AMP, the flagship algorithm of statistical physics, has\nsubstantially worse performance than SOS for tensor PCA. In this work we\n'redeem' the statistical physics approach by showing that our hierarchy gives a\npolynomial-time algorithm matching the performance of SOS. Our hierarchy also\nyields a continuum of subexponential-time algorithms, and we prove that these\nachieve the same (conjecturally optimal) tradeoff between runtime and\nstatistical power as SOS. Our proofs are much simpler than prior work, and also\napply to the related problem of refuting random $k$-XOR formulas. The results\nwe present here apply to tensor PCA for tensors of all orders, and to $k$-XOR\nwhen $k$ is even.\n  Our methods suggest a new avenue for systematically obtaining optimal\nalgorithms for Bayesian inference problems, and our results constitute a step\ntoward unifying the statistical physics and sum-of-squares approaches to\nalgorithm design.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 06:26:35 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 14:35:50 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Wein", "Alexander S.", ""], ["Alaoui", "Ahmed El", ""], ["Moore", "Cristopher", ""]]}, {"id": "1904.03866", "submitter": "Abhimanyu Das", "authors": "Abhimanyu Das, Sreenivas Gollapudi, Ravi Kumar, Rina Panigrahy", "title": "On the Learnability of Deep Random Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the learnability of deep random networks from both\ntheoretical and practical points of view. On the theoretical front, we show\nthat the learnability of random deep networks with sign activation drops\nexponentially with its depth. On the practical front, we find that the\nlearnability drops sharply with depth even with the state-of-the-art training\nmethods, suggesting that our stylized theoretical results are closer to\nreality.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 07:02:06 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Das", "Abhimanyu", ""], ["Gollapudi", "Sreenivas", ""], ["Kumar", "Ravi", ""], ["Panigrahy", "Rina", ""]]}, {"id": "1904.03867", "submitter": "Christoph Molnar", "authors": "Christoph Molnar, Giuseppe Casalicchio, Bernd Bischl", "title": "Quantifying Model Complexity via Functional Decomposition for Better\n  Post-Hoc Interpretability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Post-hoc model-agnostic interpretation methods such as partial dependence\nplots can be employed to interpret complex machine learning models. While these\ninterpretation methods can be applied regardless of model complexity, they can\nproduce misleading and verbose results if the model is too complex, especially\nw.r.t. feature interactions. To quantify the complexity of arbitrary machine\nlearning models, we propose model-agnostic complexity measures based on\nfunctional decomposition: number of features used, interaction strength and\nmain effect complexity. We show that post-hoc interpretation of models that\nminimize the three measures is more reliable and compact. Furthermore, we\ndemonstrate the application of these measures in a multi-objective optimization\napproach which simultaneously minimizes loss and complexity.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 07:02:14 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 12:04:16 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Molnar", "Christoph", ""], ["Casalicchio", "Giuseppe", ""], ["Bischl", "Bernd", ""]]}, {"id": "1904.03876", "submitter": "Lucas Ondel", "authors": "Lucas Ondel, Hari Krishna Vydana, Luk\\'a\\v{s} Burget, Jan\n  \\v{C}ernock\\'y", "title": "Bayesian Subspace Hidden Markov Model for Acoustic Unit Discovery", "comments": "Accepted to Interspeech 2019 * corrected typos * Recalculated the\n  segmentation using +-2 frames tolerance to comply with other publications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work tackles the problem of learning a set of language specific acoustic\nunits from unlabeled speech recordings given a set of labeled recordings from\nother languages. Our approach may be described by the following two steps\nprocedure: first the model learns the notion of acoustic units from the\nlabelled data and then the model uses its knowledge to find new acoustic units\non the target language. We implement this process with the Bayesian Subspace\nHidden Markov Model (SHMM), a model akin to the Subspace Gaussian Mixture Model\n(SGMM) where each low dimensional embedding represents an acoustic unit rather\nthan just a HMM's state. The subspace is trained on 3 languages from the\nGlobalPhone corpus (German, Polish and Spanish) and the AUs are discovered on\nthe TIMIT corpus. Results, measured in equivalent Phone Error Rate, show that\nthis approach significantly outperforms previous HMM based acoustic units\ndiscovery systems and compares favorably with the Variational Auto Encoder-HMM.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 07:48:36 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2019 14:35:26 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Ondel", "Lucas", ""], ["Vydana", "Hari Krishna", ""], ["Burget", "Luk\u00e1\u0161", ""], ["\u010cernock\u00fd", "Jan", ""]]}, {"id": "1904.03901", "submitter": "Yong Luo", "authors": "Yong Luo, Tongliang Liu, Dacheng Tao, Chao Xu", "title": "Multi-View Matrix Completion for Multi-Label Image Classification", "comments": null, "journal-ref": "IEEE Transactions on Image Processing (Volume: 24, Issue: 8, Aug.\n  2015)", "doi": "10.1109/TIP.2015.2421309", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is growing interest in multi-label image classification due to its\ncritical role in web-based image analytics-based applications, such as\nlarge-scale image retrieval and browsing. Matrix completion has recently been\nintroduced as a method for transductive (semi-supervised) multi-label\nclassification, and has several distinct advantages, including robustness to\nmissing data and background noise in both feature and label space. However, it\nis limited by only considering data represented by a single-view feature, which\ncannot precisely characterize images containing several semantic concepts. To\nutilize multiple features taken from different views, we have to concatenate\nthe different features as a long vector. But this concatenation is prone to\nover-fitting and often leads to very high time complexity in MC based image\nclassification. Therefore, we propose to weightedly combine the MC outputs of\ndifferent views, and present the multi-view matrix completion (MVMC) framework\nfor transductive multi-label image classification. To learn the view\ncombination weights effectively, we apply a cross validation strategy on the\nlabeled set. In the learning process, we adopt the average precision (AP) loss,\nwhich is particular suitable for multi-label image classification. A least\nsquares loss formulation is also presented for the sake of efficiency, and the\nrobustness of the algorithm based on the AP loss compared with the other losses\nis investigated. Experimental evaluation on two real world datasets (PASCAL\nVOC' 07 and MIR Flickr) demonstrate the effectiveness of MVMC for transductive\n(semi-supervised) multi-label image classification, and show that MVMC can\nexploit complementary properties of different features and output-consistent\nlabels for improved multi-label image classification.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 09:17:56 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Luo", "Yong", ""], ["Liu", "Tongliang", ""], ["Tao", "Dacheng", ""], ["Xu", "Chao", ""]]}, {"id": "1904.03909", "submitter": "Mikhail Langovoy", "authors": "Mikhail A. Langovoy", "title": "Generalized active learning and design of statistical experiments for\n  manifold-valued data", "comments": "To appear in the Proceedings of the 62-nd World Statistics Congress\n  (2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterizing the appearance of real-world surfaces is a fundamental problem\nin multidimensional reflectometry, computer vision and computer graphics. For\nmany applications, appearance is sufficiently well characterized by the\nbidirectional reflectance distribution function (BRDF). We treat BRDF\nmeasurements as samples of points from high-dimensional non-linear non-convex\nmanifolds. BRDF manifolds form an infinite-dimensional space, but typically the\navailable measurements are very scarce for complicated problems such as BRDF\nestimation. Therefore, an efficient learning strategy is crucial when\nperforming the measurements.\n  In this paper, we build the foundation of a mathematical framework that\nallows to develop and apply new techniques within statistical design of\nexperiments and generalized proactive learning, in order to establish more\nefficient sampling and measurement strategies for BRDF data manifolds.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 09:35:13 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Langovoy", "Mikhail A.", ""]]}, {"id": "1904.03911", "submitter": "Soumyadeep Ghosh", "authors": "Soumyadeep Ghosh, Richa Singh, Mayank Vatsa", "title": "On Learning Density Aware Embeddings", "comments": "Accepted in IEEE CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep metric learning algorithms have been utilized to learn discriminative\nand generalizable models which are effective for classifying unseen classes. In\nthis paper, a novel noise tolerant deep metric learning algorithm is proposed.\nThe proposed method, termed as Density Aware Metric Learning, enforces the\nmodel to learn embeddings that are pulled towards the most dense region of the\nclusters for each class. It is achieved by iteratively shifting the estimate of\nthe center towards the dense region of the cluster thereby leading to faster\nconvergence and higher generalizability. In addition to this, the approach is\nrobust to noisy samples in the training data, often present as outliers.\nDetailed experiments and analysis on two challenging cross-modal face\nrecognition databases and two popular object recognition databases exhibit the\nefficacy of the proposed approach. It has superior convergence, requires lesser\ntraining time, and yields better accuracies than several popular deep metric\nlearning methods.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 09:35:23 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Ghosh", "Soumyadeep", ""], ["Singh", "Richa", ""], ["Vatsa", "Mayank", ""]]}, {"id": "1904.03920", "submitter": "Pierre Alquier", "authors": "Badr-Eddine Ch\\'erief-Abdellatif, Pierre Alquier, Mohammad Emtiyaz\n  Khan", "title": "A Generalization Bound for Online Variational Inference", "comments": "Published in the proceedings of ACML 2019", "journal-ref": "Proceedings in Machine Learning Research, 2019, vol. 101, pp.\n  662-677", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference provides an attractive online-learning framework to\nanalyze sequential data, and offers generalization guarantees which hold even\nwith model mismatch and adversaries. Unfortunately, exact Bayesian inference is\nrarely feasible in practice and approximation methods are usually employed, but\ndo such methods preserve the generalization properties of Bayesian inference ?\nIn this paper, we show that this is indeed the case for some variational\ninference (VI) algorithms. We consider a few existing online, tempered VI\nalgorithms, as well as a new algorithm, and derive their generalization bounds.\nOur theoretical result relies on the convexity of the variational objective,\nbut we argue that the result should hold more generally and present empirical\nevidence in support of this. Our work in this paper presents theoretical\njustifications in favor of online algorithms relying on approximate Bayesian\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 09:53:25 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 07:32:41 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Ch\u00e9rief-Abdellatif", "Badr-Eddine", ""], ["Alquier", "Pierre", ""], ["Khan", "Mohammad Emtiyaz", ""]]}, {"id": "1904.03921", "submitter": "Yong Luo", "authors": "Yong Luo, Dacheng Tao, Chang Xu, Chao Xu, Hong Liu, Yonggang Wen", "title": "Multi-view Vector-valued Manifold Regularization for Multi-label Image\n  Classification", "comments": null, "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems (Volume:\n  24, Issue: 5, May 2013)", "doi": "10.1109/TNNLS.2013.2238682", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computer vision, image datasets used for classification are naturally\nassociated with multiple labels and comprised of multiple views, because each\nimage may contain several objects (e.g. pedestrian, bicycle and tree) and is\nproperly characterized by multiple visual features (e.g. color, texture and\nshape). Currently available tools ignore either the label relationship or the\nview complementary. Motivated by the success of the vector-valued function that\nconstructs matrix-valued kernels to explore the multi-label structure in the\noutput space, we introduce multi-view vector-valued manifold regularization\n(MV$\\mathbf{^3}$MR) to integrate multiple features. MV$\\mathbf{^3}$MR exploits\nthe complementary property of different features and discovers the intrinsic\nlocal geometry of the compact support shared by different features under the\ntheme of manifold regularization. We conducted extensive experiments on two\nchallenging, but popular datasets, PASCAL VOC' 07 (VOC) and MIR Flickr (MIR),\nand validated the effectiveness of the proposed MV$\\mathbf{^3}$MR for image\nclassification.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 09:57:18 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Luo", "Yong", ""], ["Tao", "Dacheng", ""], ["Xu", "Chang", ""], ["Xu", "Chao", ""], ["Liu", "Hong", ""], ["Wen", "Yonggang", ""]]}, {"id": "1904.03936", "submitter": "Bharath Bhushan Damodaran", "authors": "Kilian Fatras, Bharath Bhushan Damodaran, Sylvain Lobry, R\\'emi\n  Flamary, Devis Tuia, Nicolas Courty", "title": "Wasserstein Adversarial Regularization (WAR) on label noise", "comments": "In Press, IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (PAMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Noisy labels often occur in vision datasets, especially when they are\nobtained from crowdsourcing or Web scraping. We propose a new regularization\nmethod, which enables learning robust classifiers in presence of noisy data. To\nachieve this goal, we propose a new adversarial regularization scheme based on\nthe Wasserstein distance. Using this distance allows taking into account\nspecific relations between classes by leveraging the geometric properties of\nthe labels space. Our Wasserstein Adversarial Regularization (WAR) encodes a\nselective regularization, which promotes smoothness of the classifier between\nsome classes, while preserving sufficient complexity of the decision boundary\nbetween others. We first discuss how and why adversarial regularization can be\nused in the context of label noise and then show the effectiveness of our\nmethod on five datasets corrupted with noisy labels: in both benchmarks and\nreal datasets, WAR outperforms the state-of-the-art competitors.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 10:28:12 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 16:21:53 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2021 07:45:32 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Fatras", "Kilian", ""], ["Damodaran", "Bharath Bhushan", ""], ["Lobry", "Sylvain", ""], ["Flamary", "R\u00e9mi", ""], ["Tuia", "Devis", ""], ["Courty", "Nicolas", ""]]}, {"id": "1904.03943", "submitter": "Quay Au", "authors": "Quay Au, Daniel Schalk, Giuseppe Casalicchio, Ramona Schoedel, Clemens\n  Stachl, Bernd Bischl", "title": "Component-Wise Boosting of Targets for Multi-Output Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-output prediction deals with the prediction of several targets of\npossibly diverse types. One way to address this problem is the so called\nproblem transformation method. This method is often used in multi-label\nlearning, but can also be used for multi-output prediction due to its\ngenerality and simplicity. In this paper, we introduce an algorithm that uses\nthe problem transformation method for multi-output prediction, while\nsimultaneously learning the dependencies between target variables in a sparse\nand interpretable manner. In a first step, predictions are obtained for each\ntarget individually. Target dependencies are then learned via a component-wise\nboosting approach. We compare our new method with similar approaches in a\nbenchmark using multi-label, multivariate regression and mixed-type datasets.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 10:48:56 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Au", "Quay", ""], ["Schalk", "Daniel", ""], ["Casalicchio", "Giuseppe", ""], ["Schoedel", "Ramona", ""], ["Stachl", "Clemens", ""], ["Bischl", "Bernd", ""]]}, {"id": "1904.03953", "submitter": "Zhongheng Li", "authors": "Fei Wang, Zhongheng Li, Fang He, Rong Wang, Weizhong Yu, Feiping Nie", "title": "Feature Learning Viewpoint of AdaBoost and a New Algorithm", "comments": null, "journal-ref": "IEEE Access, vol. 7, pp. 149890-149899, 2019", "doi": "10.1109/ACCESS.2019.2947359", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The AdaBoost algorithm has the superiority of resisting overfitting.\nUnderstanding the mysteries of this phenomena is a very fascinating fundamental\ntheoretical problem. Many studies are devoted to explaining it from statistical\nview and margin theory. In this paper, we illustrate it from feature learning\nviewpoint, and propose the AdaBoost+SVM algorithm, which can explain the\nresistant to overfitting of AdaBoost directly and easily to understand.\nFirstly, we adopt the AdaBoost algorithm to learn the base classifiers. Then,\ninstead of directly weighted combination the base classifiers, we regard them\nas features and input them to SVM classifier. With this, the new coefficient\nand bias can be obtained, which can be used to construct the final classifier.\nWe explain the rationality of this and illustrate the theorem that when the\ndimension of these features increases, the performance of SVM would not be\nworse, which can explain the resistant to overfitting of AdaBoost.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 11:07:50 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Wang", "Fei", ""], ["Li", "Zhongheng", ""], ["He", "Fang", ""], ["Wang", "Rong", ""], ["Yu", "Weizhong", ""], ["Nie", "Feiping", ""]]}, {"id": "1904.03959", "submitter": "Christian Alexander Scholbeck", "authors": "Christian A. Scholbeck, Christoph Molnar, Christian Heumann, Bernd\n  Bischl, Giuseppe Casalicchio", "title": "Sampling, Intervention, Prediction, Aggregation: A Generalized Framework\n  for Model-Agnostic Interpretations", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-43823-4_18", "report-no": "Cellier P., Driessens K. (eds) Machine Learning and Knowledge\n  Discovery in Databases. ECML PKDD 2019. Communications in Computer and\n  Information Science, vol 1167. Springer, Cham", "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Model-agnostic interpretation techniques allow us to explain the behavior of\nany predictive model. Due to different notations and terminology, it is\ndifficult to see how they are related. A unified view on these methods has been\nmissing. We present the generalized SIPA (sampling, intervention, prediction,\naggregation) framework of work stages for model-agnostic interpretations and\ndemonstrate how several prominent methods for feature effects can be embedded\ninto the proposed framework. Furthermore, we extend the framework to feature\nimportance computations by pointing out how variance-based and\nperformance-based importance measures are based on the same work stages. The\nSIPA framework reduces the diverse set of model-agnostic techniques to a single\nmethodology and establishes a common terminology to discuss them in future\nwork.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 11:20:04 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 06:53:18 GMT"}, {"version": "v3", "created": "Fri, 9 Aug 2019 14:06:54 GMT"}, {"version": "v4", "created": "Thu, 13 Feb 2020 11:08:54 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Scholbeck", "Christian A.", ""], ["Molnar", "Christoph", ""], ["Heumann", "Christian", ""], ["Bischl", "Bernd", ""], ["Casalicchio", "Giuseppe", ""]]}, {"id": "1904.03971", "submitter": "Ehsan Montahaei", "authors": "Ehsan Montahaei, Danial Alihosseini and Mahdieh Soleymani Baghshah", "title": "Jointly Measuring Diversity and Quality in Text Generation Models", "comments": "Accepted by NAACL 2019 workshop (NeuralGen 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text generation is an important Natural Language Processing task with various\napplications. Although several metrics have already been introduced to evaluate\nthe text generation methods, each of them has its own shortcomings. The most\nwidely used metrics such as BLEU only consider the quality of generated\nsentences and neglect their diversity. For example, repeatedly generation of\nonly one high quality sentence would result in a high BLEU score. On the other\nhand, the more recent metric introduced to evaluate the diversity of generated\ntexts known as Self-BLEU ignores the quality of generated texts. In this paper,\nwe propose metrics to evaluate both the quality and diversity simultaneously by\napproximating the distance of the learned generative model and the real data\ndistribution. For this purpose, we first introduce a metric that approximates\nthis distance using n-gram based measures. Then, a feature-based measure which\nis based on a recent highly deep model trained on a large text corpus called\nBERT is introduced. Finally, for oracle training mode in which the generator's\ndensity can also be calculated, we propose to use the distance measures between\nthe corresponding explicit distributions. Eventually, the most popular and\nrecent text generation models are evaluated using both the existing and the\nproposed metrics and the preferences of the proposed metrics are determined.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 11:44:41 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 21:14:54 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Montahaei", "Ehsan", ""], ["Alihosseini", "Danial", ""], ["Baghshah", "Mahdieh Soleymani", ""]]}, {"id": "1904.03990", "submitter": "Bart Theeten", "authors": "Bart Theeten, Frederik Vandeputte, Tom Van Cutsem", "title": "Import2vec - Learning Embeddings for Software Libraries", "comments": "MSR19 Conference 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of developing suitable learning representations\n(embeddings) for library packages that capture semantic similarity among\nlibraries. Such representations are known to improve the performance of\ndownstream learning tasks (e.g. classification) or applications such as\ncontextual search and analogical reasoning.\n  We apply word embedding techniques from natural language processing (NLP) to\ntrain embeddings for library packages (\"library vectors\"). Library vectors\nrepresent libraries by similar context of use as determined by import\nstatements present in source code. Experimental results obtained from training\nsuch embeddings on three large open source software corpora reveals that\nlibrary vectors capture semantically meaningful relationships among software\nlibraries, such as the relationship between frameworks and their plug-ins and\nlibraries commonly used together within ecosystems such as big data\ninfrastructure projects (in Java), front-end and back-end web development\nframeworks (in JavaScript) and data science toolkits (in Python).\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 14:36:19 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Theeten", "Bart", ""], ["Vandeputte", "Frederik", ""], ["Van Cutsem", "Tom", ""]]}, {"id": "1904.04019", "submitter": "Giosu\\'e Lo Bosco", "authors": "Mattia Antonino Di Gangi, Giosu\\'e Lo Bosco, Giovanni Pilato", "title": "Effectiveness of Data-Driven Induction of Semantic Spaces and\n  Traditional Classifiers for Sarcasm Detection", "comments": "37 pages, 7 figures, version 4", "journal-ref": "Natural Language Engineering, 25(2), 257-285 (2019)", "doi": "10.1017/S1351324919000019", "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Irony and sarcasm are two complex linguistic phenomena that are widely used\nin everyday language and especially over the social media, but they represent\ntwo serious issues for automated text understanding. Many labeled corpora have\nbeen extracted from several sources to accomplish this task, and it seems that\nsarcasm is conveyed in different ways for different domains. Nonetheless, very\nlittle work has been done for comparing different methods among the available\ncorpora. Furthermore, usually, each author collects and uses their own datasets\nto evaluate his own method. In this paper, we show that sarcasm detection can\nbe tackled by applying classical machine learning algorithms to input texts\nsub-symbolically represented in a Latent Semantic space. The main consequence\nis that our studies establish both reference datasets and baselines for the\nsarcasm detection problem that could serve the scientific community to test\nnewly proposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 15:49:02 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 10:20:39 GMT"}, {"version": "v3", "created": "Mon, 15 Apr 2019 09:17:12 GMT"}, {"version": "v4", "created": "Fri, 6 Dec 2019 16:45:25 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Di Gangi", "Mattia Antonino", ""], ["Bosco", "Giosu\u00e9 Lo", ""], ["Pilato", "Giovanni", ""]]}, {"id": "1904.04020", "submitter": "Xin Huang", "authors": "Xin Huang, Yulia R. Gel", "title": "CRAD: Clustering with Robust Autocuts and Depth", "comments": "9 pages, 6 figures", "journal-ref": "2017 IEEE International Conference on Data Mining (ICDM),\n  925--930} (2017)", "doi": "10.1109/ICDM.2017.116", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new density-based clustering algorithm named CRAD which is based\non a new neighbor searching function with a robust data depth as the\ndissimilarity measure. Our experiments prove that the new CRAD is highly\ncompetitive at detecting clusters with varying densities, compared with the\nexisting algorithms such as DBSCAN, OPTICS and DBCA. Furthermore, a new\neffective parameter selection procedure is developed to select the optimal\nunderlying parameter in the real-world clustering, when the ground truth is\nunknown. Lastly, we suggest a new clustering framework that extends CRAD from\nspatial data clustering to time series clustering without a-priori knowledge of\nthe true number of clusters. The performance of CRAD is evaluated through\nextensive experimental studies.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 12:49:45 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Huang", "Xin", ""], ["Gel", "Yulia R.", ""]]}, {"id": "1904.04021", "submitter": "Tung Nguyen Thanh", "authors": "Tasnim Mohiuddin, Thanh-Tung Nguyen and Shafiq Joty", "title": "Adaptation of Hierarchical Structured Models for Speech Act Recognition\n  in Asynchronous Conversation", "comments": "To appear in NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of speech act recognition (SAR) in asynchronous\nconversations (forums, emails). Unlike synchronous conversations (e.g.,\nmeetings, phone), asynchronous domains lack large labeled datasets to train an\neffective SAR model. In this paper, we propose methods to effectively leverage\nabundant unlabeled conversational data and the available labeled data from\nsynchronous domains. We carry out our research in three main steps. First, we\nintroduce a neural architecture based on hierarchical LSTMs and conditional\nrandom fields (CRF) for SAR, and show that our method outperforms existing\nmethods when trained on in-domain data only. Second, we improve our initial SAR\nmodels by semi-supervised learning in the form of pretrained word embeddings\nlearned from a large unlabeled conversational corpus. Finally, we employ\nadversarial training to improve the results further by leveraging the labeled\ndata from synchronous domains and by explicitly modeling the distributional\nshift in two domains.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 04:57:25 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Mohiuddin", "Tasnim", ""], ["Nguyen", "Thanh-Tung", ""], ["Joty", "Shafiq", ""]]}, {"id": "1904.04025", "submitter": "Yannis Flet-Berliac", "authors": "Yannis Flet-Berliac, Philippe Preux", "title": "Only Relevant Information Matters: Filtering Out Noisy Samples to Boost\n  RL", "comments": "Accepted at IJCAI 2020", "journal-ref": null, "doi": "10.24963/ijcai.2020/376", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In reinforcement learning, policy gradient algorithms optimize the policy\ndirectly and rely on sampling efficiently an environment. Nevertheless, while\nmost sampling procedures are based on direct policy sampling, self-performance\nmeasures could be used to improve such sampling prior to each policy update.\nFollowing this line of thought, we introduce SAUNA, a method where\nnon-informative transitions are rejected from the gradient update. The level of\ninformation is estimated according to the fraction of variance explained by the\nvalue function: a measure of the discrepancy between V and the empirical\nreturns. In this work, we use this metric to select samples that are useful to\nlearn from, and we demonstrate that this selection can significantly improve\nthe performance of policy gradient methods. In this paper: (a) We define\nSAUNA's metric and introduce its method to filter transitions. (b) We conduct\nexperiments on a set of benchmark continuous control problems. SAUNA\nsignificantly improves performance. (c) We investigate how SAUNA reliably\nselects samples with the most positive impact on learning and study its\nimprovement on both performance and sample efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 12:53:12 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 10:57:34 GMT"}, {"version": "v3", "created": "Wed, 25 Sep 2019 14:16:56 GMT"}, {"version": "v4", "created": "Wed, 13 May 2020 09:45:42 GMT"}, {"version": "v5", "created": "Fri, 20 Nov 2020 16:04:51 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Flet-Berliac", "Yannis", ""], ["Preux", "Philippe", ""]]}, {"id": "1904.04047", "submitter": "Thomas Manzini", "authors": "Thomas Manzini, Yao Chong Lim, Yulia Tsvetkov, Alan W Black", "title": "Black is to Criminal as Caucasian is to Police: Detecting and Removing\n  Multiclass Bias in Word Embeddings", "comments": "Accepted as a conference paper at NAACL. 5 Pages excluding\n  references, additional page for appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online texts -- across genres, registers, domains, and styles -- are riddled\nwith human stereotypes, expressed in overt or subtle ways. Word embeddings,\ntrained on these texts, perpetuate and amplify these stereotypes, and propagate\nbiases to machine learning models that use word embeddings as features. In this\nwork, we propose a method to debias word embeddings in multiclass settings such\nas race and religion, extending the work of (Bolukbasi et al., 2016) from the\nbinary setting, such as binary gender. Next, we propose a novel methodology for\nthe evaluation of multiclass debiasing. We demonstrate that our multiclass\ndebiasing is robust and maintains the efficacy in standard NLP tasks.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 22:17:27 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 16:59:50 GMT"}, {"version": "v3", "created": "Tue, 2 Jul 2019 01:16:15 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Manzini", "Thomas", ""], ["Lim", "Yao Chong", ""], ["Tsvetkov", "Yulia", ""], ["Black", "Alan W", ""]]}, {"id": "1904.04049", "submitter": "Wenbo Zhao", "authors": "Wenbo Zhao, Tagyoung Chung, Anuj Goyal, Angeliki Metallinou", "title": "Simple Question Answering with Subgraph Ranking and Joint-Scoring", "comments": "Accepted by The 2019 Annual Conference of the North American Chapter\n  of the Association for Computational Linguistics (NAACL-HLT 2019). 11 pages,\n  1 figure", "journal-ref": null, "doi": "10.18653/v1/N19-1029", "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph based simple question answering (KBSQA) is a major area of\nresearch within question answering. Although only dealing with simple\nquestions, i.e., questions that can be answered through a single knowledge base\n(KB) fact, this task is neither simple nor close to being solved. Targeting on\nthe two main steps, subgraph selection and fact selection, the research\ncommunity has developed sophisticated approaches. However, the importance of\nsubgraph ranking and leveraging the subject--relation dependency of a KB fact\nhave not been sufficiently explored. Motivated by this, we present a unified\nframework to describe and analyze existing approaches. Using this framework as\na starting point, we focus on two aspects: improving subgraph selection through\na novel ranking method and leveraging the subject--relation dependency by\nproposing a joint scoring CNN model with a novel loss function that enforces\nthe well-order of scores. Our methods achieve a new state of the art (85.44% in\naccuracy) on the SimpleQuestions dataset.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 02:20:50 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Zhao", "Wenbo", ""], ["Chung", "Tagyoung", ""], ["Goyal", "Anuj", ""], ["Metallinou", "Angeliki", ""]]}, {"id": "1904.04055", "submitter": "Jan Koco\\'n", "authors": "Jan Koco\\'n, Micha{\\l} Gawor", "title": "Evaluating KGR10 Polish word embeddings in the recognition of temporal\n  expressions using BiLSTM-CRF", "comments": "Presented at TFML 2019 (Theoretical Foundations of Machine Learning)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article introduces a new set of Polish word embeddings, built using KGR10\ncorpus, which contains more than 4 billion words. These embeddings are\nevaluated in the problem of recognition of temporal expressions (timexes) for\nthe Polish language. We described the process of KGR10 corpus creation and a\nnew approach to the recognition problem using Bidirectional Long-Short Term\nMemory (BiLSTM) network with additional CRF layer, where specific embeddings\nare essential. We presented experiments and conclusions drawn from them.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 14:47:30 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Koco\u0144", "Jan", ""], ["Gawor", "Micha\u0142", ""]]}, {"id": "1904.04057", "submitter": "Hang Zou", "authors": "Hang Zou, Chao Zhang and Samson Lasaulce", "title": "Task Oriented Channel State Information Quantization", "comments": "2 pages, 2 figures", "journal-ref": "IEEE 2018 International Symposium on Personal, Indoor and Mobile\n  Radio Communications (PIMRC'18)", "doi": "10.1109/PIMRC.2018.8580826", "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a new perspective for quantizing a signal and more\nspecifically the channel state information (CSI). The proposed point of view is\nfully relevant for a receiver which has to send a quantized version of the\nchannel state to the transmitter. Roughly, the key idea is that the receiver\nsends the right amount of information to the transmitter so that the latter be\nable to take its (resource allocation) decision. More formally, the decision\ntask of the transmitter is to maximize an utility function u(x;g) with respect\nto x (e.g., a power allocation vector) given the knowledge of a quantized\nversion of the function parameters g. We exhibit a special case of an\nenergy-efficient power control (PC) problem for which the optimal task oriented\nCSI quantizer (TOCQ) can be found analytically. For more general utility\nfunctions, we propose to use neural networks (NN) based learning. Simulations\nshow that the compression rate obtained by adapting the feedback information\nrate to the function to be optimized may be significantly increased.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 14:52:51 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Zou", "Hang", ""], ["Zhang", "Chao", ""], ["Lasaulce", "Samson", ""]]}, {"id": "1904.04061", "submitter": "Yong Luo", "authors": "Yong Luo, Yonggang Wen, Tongliang Liu, Dacheng Tao", "title": "Transferring Knowledge Fragments for Learning Distance Metric from A\n  Heterogeneous Domain", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (Volume: 41, Issue: 4, April 1 2019)", "doi": "10.1109/TPAMI.2018.2824309", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of transfer learning is to improve the performance of target\nlearning task by leveraging information (or transferring knowledge) from other\nrelated tasks. In this paper, we examine the problem of transfer distance\nmetric learning (DML), which usually aims to mitigate the label information\ndeficiency issue in the target DML. Most of the current Transfer DML (TDML)\nmethods are not applicable to the scenario where data are drawn from\nheterogeneous domains. Some existing heterogeneous transfer learning (HTL)\napproaches can learn target distance metric by usually transforming the samples\nof source and target domain into a common subspace. However, these approaches\nlack flexibility in real-world applications, and the learned transformations\nare often restricted to be linear. This motivates us to develop a general\nflexible heterogeneous TDML (HTDML) framework. In particular, any\n(linear/nonlinear) DML algorithms can be employed to learn the source metric\nbeforehand. Then the pre-learned source metric is represented as a set of\nknowledge fragments to help target metric learning. We show how generalization\nerror in the target domain could be reduced using the proposed transfer\nstrategy, and develop novel algorithm to learn either linear or nonlinear\ntarget metric. Extensive experiments on various applications demonstrate the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 13:44:22 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Luo", "Yong", ""], ["Wen", "Yonggang", ""], ["Liu", "Tongliang", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.04063", "submitter": "Afra Alishahi", "authors": "Afra Alishahi and Grzegorz Chrupa{\\l}a and Tal Linzen", "title": "Analyzing and Interpreting Neural Networks for NLP: A Report on the\n  First BlackboxNLP Workshop", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The EMNLP 2018 workshop BlackboxNLP was dedicated to resources and techniques\nspecifically developed for analyzing and understanding the inner-workings and\nrepresentations acquired by neural models of language. Approaches included:\nsystematic manipulation of input to neural networks and investigating the\nimpact on their performance, testing whether interpretable knowledge can be\ndecoded from intermediate representations acquired by neural networks,\nproposing modifications to neural network architectures to make their knowledge\nstate or generated output more explainable, and examining the performance of\nnetworks on simplified or formal languages. Here we review a number of\nrepresentative studies in each category.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 15:15:45 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Alishahi", "Afra", ""], ["Chrupa\u0142a", "Grzegorz", ""], ["Linzen", "Tal", ""]]}, {"id": "1904.04079", "submitter": "Weijia Xu", "authors": "Weijia Xu, Xing Niu, Marine Carpuat", "title": "Differentiable Sampling with Flexible Reference Word Order for Neural\n  Machine Translation", "comments": "Accepted at NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite some empirical success at correcting exposure bias in machine\ntranslation, scheduled sampling algorithms suffer from a major drawback: they\nincorrectly assume that words in the reference translations and in sampled\nsequences are aligned at each time step. Our new differentiable sampling\nalgorithm addresses this issue by optimizing the probability that the reference\ncan be aligned with the sampled output, based on a soft alignment predicted by\nthe model itself. As a result, the output distribution at each time step is\nevaluated with respect to the whole predicted sequence. Experiments on IWSLT\ntranslation tasks show that our approach improves BLEU compared to maximum\nlikelihood and scheduled sampling baselines. In addition, our approach is\nsimpler to train with no need for sampling schedule and yields models that\nachieve larger improvements with smaller beam sizes.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 04:48:07 GMT"}, {"version": "v2", "created": "Sat, 4 May 2019 21:46:15 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Xu", "Weijia", ""], ["Niu", "Xing", ""], ["Carpuat", "Marine", ""]]}, {"id": "1904.04081", "submitter": "Yong Luo", "authors": "Yong Luo, Yonggang Wen, Dacheng Tao", "title": "Heterogeneous Multi-task Metric Learning across Multiple Domains", "comments": null, "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems (Volume:\n  29, Issue: 9, Sept. 2018)", "doi": "10.1109/TNNLS.2017.2750321", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance metric learning (DML) plays a crucial role in diverse machine\nlearning algorithms and applications. When the labeled information in target\ndomain is limited, transfer metric learning (TML) helps to learn the metric by\nleveraging the sufficient information from other related domains. Multi-task\nmetric learning (MTML), which can be regarded as a special case of TML,\nperforms transfer across all related domains. Current TML tools usually assume\nthat the same feature representation is exploited for different domains.\nHowever, in real-world applications, data may be drawn from heterogeneous\ndomains. Heterogeneous transfer learning approaches can be adopted to remedy\nthis drawback by deriving a metric from the learned transformation across\ndifferent domains. But they are often limited in that only two domains can be\nhandled. To appropriately handle multiple domains, we develop a novel\nheterogeneous multi-task metric learning (HMTML) framework. In HMTML, the\nmetrics of all different domains are learned together. The transformations\nderived from the metrics are utilized to induce a common subspace, and the\nhigh-order covariance among the predictive structures of these domains is\nmaximized in this subspace. There do exist a few heterogeneous transfer\nlearning approaches that deal with multiple domains, but the high-order\nstatistics (correlation information), which can only be exploited by\nsimultaneously examining all domains, is ignored in these approaches. Compared\nwith them, the proposed HMTML can effectively explore such high-order\ninformation, thus obtaining more reliable feature transformations and metrics.\nEffectiveness of our method is validated by the extensive and intensive\nexperiments on text categorization, scene classification, and social image\nannotation.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 13:59:36 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Luo", "Yong", ""], ["Wen", "Yonggang", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.04088", "submitter": "Yong Luo", "authors": "Yong Luo, Yonggang Wen, Dacheng Tao, Jie Gui, Chao Xu", "title": "Large Margin Multi-modal Multi-task Feature Extraction for Image\n  Classification", "comments": null, "journal-ref": "IEEE Transactions on Image Processing (Volume: 25, Issue: 1, Jan.\n  2016)", "doi": "10.1109/TIP.2015.2495116", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The features used in many image analysis-based applications are frequently of\nvery high dimension. Feature extraction offers several advantages in\nhigh-dimensional cases, and many recent studies have used multi-task feature\nextraction approaches, which often outperform single-task feature extraction\napproaches. However, most of these methods are limited in that they only\nconsider data represented by a single type of feature, even though features\nusually represent images from multiple modalities. We therefore propose a novel\nlarge margin multi-modal multi-task feature extraction (LM3FE) framework for\nhandling multi-modal features for image classification. In particular, LM3FE\nsimultaneously learns the feature extraction matrix for each modality and the\nmodality combination coefficients. In this way, LM3FE not only handles\ncorrelated and noisy features, but also utilizes the complementarity of\ndifferent modalities to further help reduce feature redundancy in each\nmodality. The large margin principle employed also helps to extract strongly\npredictive features so that they are more suitable for prediction (e.g.,\nclassification). An alternating algorithm is developed for problem optimization\nand each sub-problem can be efficiently solved. Experiments on two challenging\nreal-world image datasets demonstrate the effectiveness and superiority of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 14:14:19 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Luo", "Yong", ""], ["Wen", "Yonggang", ""], ["Tao", "Dacheng", ""], ["Gui", "Jie", ""], ["Xu", "Chao", ""]]}, {"id": "1904.04096", "submitter": "Fatma Nasoz", "authors": "Nishit Shrestha and Fatma Nasoz", "title": "Deep Learning Sentiment Analysis of Amazon.com Reviews and Ratings", "comments": "15 pages, 10 figures, 3 tables, journal article", "journal-ref": "International Journal on Soft Computing, Artificial Intelligence\n  and Applications (IJSCAI), Vol.8, No.1, February 2019", "doi": "10.5121/ijscai.2019.8101", "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our study employs sentiment analysis to evaluate the compatibility of\nAmazon.com reviews with their corresponding ratings. Sentiment analysis is the\ntask of identifying and classifying the sentiment expressed in a piece of text\nas being positive or negative. On e-commerce websites such as Amazon.com,\nconsumers can submit their reviews along with a specific polarity rating. In\nsome instances, there is a mismatch between the review and the rating. To\nidentify the reviews with mismatched ratings we performed sentiment analysis\nusing deep learning on Amazon.com product review data. Product reviews were\nconverted to vectors using paragraph vector, which then was used to train a\nrecurrent neural network with gated recurrent unit. Our model incorporated both\nsemantic relationship of review text and product information. We also developed\na web service application that predicts the rating score for a submitted review\nusing the trained model and if there is a mismatch between predicted rating\nscore and submitted rating score, it provides feedback to the reviewer.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 21:34:45 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Shrestha", "Nishit", ""], ["Nasoz", "Fatma", ""]]}, {"id": "1904.04116", "submitter": "Tasnim Mohiuddin", "authors": "Tasnim Mohiuddin and Shafiq Joty", "title": "Revisiting Adversarial Autoencoder for Unsupervised Word Translation\n  with Cycle Consistency and Improved Training", "comments": "Published in NAACL-HLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training has shown impressive success in learning bilingual\ndictionary without any parallel data by mapping monolingual embeddings to a\nshared space. However, recent work has shown superior performance for\nnon-adversarial methods in more challenging language pairs. In this work, we\nrevisit adversarial autoencoder for unsupervised word translation and propose\ntwo novel extensions to it that yield more stable training and improved\nresults. Our method includes regularization terms to enforce cycle consistency\nand input reconstruction, and puts the target encoders as an adversary against\nthe corresponding discriminator. Extensive experimentations with European,\nnon-European and low-resource languages show that our method is more robust and\nachieves better performance than recently proposed adversarial and\nnon-adversarial approaches.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 12:46:07 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Mohiuddin", "Tasnim", ""], ["Joty", "Shafiq", ""]]}, {"id": "1904.04123", "submitter": "Asaf Noy", "authors": "Asaf Noy, Niv Nayman, Tal Ridnik, Nadav Zamir, Sivan Doveh, Itamar\n  Friedman, Raja Giryes, and Lihi Zelnik-Manor", "title": "ASAP: Architecture Search, Anneal and Prune", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic methods for Neural Architecture Search (NAS) have been shown to\nproduce state-of-the-art network models. Yet, their main drawback is the\ncomputational complexity of the search process. As some primal methods\noptimized over a discrete search space, thousands of days of GPU were required\nfor convergence. A recent approach is based on constructing a differentiable\nsearch space that enables gradient-based optimization, which reduces the search\ntime to a few days. While successful, it still includes some noncontinuous\nsteps, e.g., the pruning of many weak connections at once. In this paper, we\npropose a differentiable search space that allows the annealing of architecture\nweights, while gradually pruning inferior operations. In this way, the search\nconverges to a single output network in a continuous manner. Experiments on\nseveral vision datasets demonstrate the effectiveness of our method with\nrespect to the search cost and accuracy of the achieved model. Specifically,\nwith $0.2$ GPU search days we achieve an error rate of $1.68\\%$ on CIFAR-10.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 15:16:16 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 08:59:52 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Noy", "Asaf", ""], ["Nayman", "Niv", ""], ["Ridnik", "Tal", ""], ["Zamir", "Nadav", ""], ["Doveh", "Sivan", ""], ["Friedman", "Itamar", ""], ["Giryes", "Raja", ""], ["Zelnik-Manor", "Lihi", ""]]}, {"id": "1904.04153", "submitter": "Han Guo", "authors": "Han Guo, Ramakanth Pasunuru, Mohit Bansal", "title": "AutoSeM: Automatic Task Selection and Mixing in Multi-Task Learning", "comments": "NAACL 2019 (12 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning (MTL) has achieved success over a wide range of problems,\nwhere the goal is to improve the performance of a primary task using a set of\nrelevant auxiliary tasks. However, when the usefulness of the auxiliary tasks\nw.r.t. the primary task is not known a priori, the success of MTL models\ndepends on the correct choice of these auxiliary tasks and also a balanced\nmixing ratio of these tasks during alternate training. These two problems could\nbe resolved via manual intuition or hyper-parameter tuning over all\ncombinatorial task choices, but this introduces inductive bias or is not\nscalable when the number of candidate auxiliary tasks is very large. To address\nthese issues, we present AutoSeM, a two-stage MTL pipeline, where the first\nstage automatically selects the most useful auxiliary tasks via a\nBeta-Bernoulli multi-armed bandit with Thompson Sampling, and the second stage\nlearns the training mixing ratio of these selected auxiliary tasks via a\nGaussian Process based Bayesian optimization framework. We conduct several MTL\nexperiments on the GLUE language understanding tasks, and show that our AutoSeM\nframework can successfully find relevant auxiliary tasks and automatically\nlearn their mixing ratio, achieving significant performance boosts on several\nprimary tasks. Finally, we present ablations for each stage of AutoSeM and\nanalyze the learned auxiliary task choices.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 16:05:43 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Guo", "Han", ""], ["Pasunuru", "Ramakanth", ""], ["Bansal", "Mohit", ""]]}, {"id": "1904.04154", "submitter": "Robert Baldock", "authors": "Robert J. N. Baldock, Nicola Marzari", "title": "Bayesian Neural Networks at Finite Temperature", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We recapitulate the Bayesian formulation of neural network based classifiers\nand show that, while sampling from the posterior does indeed lead to better\ngeneralisation than is obtained by standard optimisation of the cost function,\neven better performance can in general be achieved by sampling finite\ntemperature ($T$) distributions derived from the posterior. Taking the example\nof two different deep (3 hidden layers) classifiers for MNIST data, we find\nquite different $T$ values to be appropriate in each case. In particular, for a\ntypical neural network classifier a clear minimum of the test error is observed\nat $T>0$. This suggests an early stopping criterion for full batch simulated\nannealing: cool until the average validation error starts to increase, then\nrevert to the parameters with the lowest validation error. As $T$ is increased\nclassifiers transition from accurate classifiers to classifiers that have\nhigher training error than assigning equal probability to each class. Efficient\nstudies of these temperature-induced effects are enabled using a\nreplica-exchange Hamiltonian Monte Carlo simulation technique. Finally, we show\nhow thermodynamic integration can be used to perform model selection for deep\nneural networks. Similar to the Laplace approximation, this approach assumes\nthat the posterior is dominated by a single mode. Crucially, however, no\nassumption is made about the shape of that mode and it is not required to\nprecisely compute and invert the Hessian.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 16:06:13 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Baldock", "Robert J. N.", ""], ["Marzari", "Nicola", ""]]}, {"id": "1904.04161", "submitter": "Jayaraman J. Thiagarajan", "authors": "Vivek Sivaraman Narayanaswamy, Sameeksha Katoch, Jayaraman J.\n  Thiagarajan, Huan Song and Andreas Spanias", "title": "Audio Source Separation via Multi-Scale Learning with Dilated Dense\n  U-Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern audio source separation techniques rely on optimizing sequence model\narchitectures such as, 1D-CNNs, on mixture recordings to generalize well to\nunseen mixtures. Specifically, recent focus is on time-domain based\narchitectures such as Wave-U-Net which exploit temporal context by extracting\nmulti-scale features. However, the optimality of the feature extraction process\nin these architectures has not been well investigated. In this paper, we\nexamine and recommend critical architectural changes that forge an optimal\nmulti-scale feature extraction process. To this end, we replace regular $1-$D\nconvolutions with adaptive dilated convolutions that have innate capability of\ncapturing increased context by using large temporal receptive fields. We also\ninvestigate the impact of dense connections on the extraction process that\nencourage feature reuse and better gradient flow. The dense connections between\nthe downsampling and upsampling paths of a U-Net architecture capture\nmulti-resolution information leading to improved temporal modelling. We\nevaluate the proposed approaches on the MUSDB test dataset. In addition to\nproviding an improved performance over the state-of-the-art, we also provide\ninsights on the impact of different architectural choices on complex\ndata-driven solutions for source separation.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 16:13:16 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Narayanaswamy", "Vivek Sivaraman", ""], ["Katoch", "Sameeksha", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Song", "Huan", ""], ["Spanias", "Andreas", ""]]}, {"id": "1904.04203", "submitter": "Marcos Oliveira", "authors": "Lydia Taw, Nishant Gurrapadi, Mariana Macedo, Marcos Oliveira, Diego\n  Pinheiro, Carmelo Bastos-Filho, Ronaldo Menezes", "title": "Characterizing the Social Interactions in the Artificial Bee Colony\n  Algorithm", "comments": "9 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational swarm intelligence consists of multiple artificial simple\nagents exchanging information while exploring a search space. Despite a rich\nliterature in the field, with works improving old approaches and proposing new\nones, the mechanism by which complex behavior emerges in these systems is still\nnot well understood. This literature gap hinders the researchers' ability to\ndeal with known problems in swarms intelligence such as premature convergence,\nand the balance of coordination and diversity among agents. Recent advances in\nthe literature, however, have proposed to study these systems via the network\nthat emerges from the social interactions within the swarm (i.e., the\ninteraction network). In our work, we propose a definition of the interaction\nnetwork for the Artificial Bee Colony (ABC) algorithm. With our approach, we\ncaptured striking idiosyncrasies of the algorithm. We uncovered the different\npatterns of social interactions that emerge from each type of bee, revealing\nthe importance of the bees variations throughout the iterations of the\nalgorithm. We found that ABC exhibits a dynamic information flow through the\nuse of different bees but lacks continuous coordination between the agents.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 17:21:14 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Taw", "Lydia", ""], ["Gurrapadi", "Nishant", ""], ["Macedo", "Mariana", ""], ["Oliveira", "Marcos", ""], ["Pinheiro", "Diego", ""], ["Bastos-Filho", "Carmelo", ""], ["Menezes", "Ronaldo", ""]]}, {"id": "1904.04204", "submitter": "Fahrettin Ay", "authors": "F. Ay, G. \\.Ince, M. E. Kama\\c{s}ak, K. Y. Ek\\c{s}i", "title": "Classification of pulsars with Dirichlet process Gaussian mixture model", "comments": "Accepted by MNRAS (14 January 2020). 11 pages, 4 figures, 7 tables", "journal-ref": null, "doi": "10.1093/mnras/staa154", "report-no": null, "categories": "astro-ph.HE cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Young isolated neutron stars (INS) most commonly manifest themselves as\nrotationally powered pulsars (RPPs) which involve conventional radio pulsars as\nwell as gamma-ray pulsars (GRPs) and rotating radio transients (RRATs). Some\nother young INS families manifest themselves as anomalous X-ray pulsars (AXPs)\nand soft gamma-ray repeaters (SGRs) which are commonly accepted as magnetars,\ni.e. magnetically powered neutron stars with decaying superstrong fields. Yet\nsome other young INS are identified as central compact objects (CCOs) and X-ray\ndim isolated neutron stars (XDINSs) which are cooling objects powered by their\nthermal energy. Older pulsars, as a result of a previous long episode of\naccretion from a companion, manifest themselves as millisecond pulsars and more\ncommonly appear in binary systems. We use Dirichlet process Gaussian mixture\nmodel (DPGMM), an unsupervised machine learning algorithm, for analyzing the\ndistribution of these pulsar families in the parameter space of period and\nperiod derivative. We compare the average values of the characteristic age,\nmagnetic dipole field strength, surface temperature and transverse velocity of\nall discovered clusters. We verify that DPGMM is robust and provides hints for\ninferring relations between different classes of pulsars. We discuss the\nimplications of our findings for the magneto-thermal spin evolution models and\nfallback discs.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 17:23:35 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 19:42:04 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Ay", "F.", ""], ["\u0130nce", "G.", ""], ["Kama\u015fak", "M. E.", ""], ["Ek\u015fi", "K. Y.", ""]]}, {"id": "1904.04206", "submitter": "Shervin Minaee", "authors": "Shervin Minaee, Elham Azimi, AmirAli Abdolrashidi", "title": "Deep-Sentiment: Sentiment Analysis Using Ensemble of CNN and Bi-LSTM\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the popularity of social networks, and e-commerce websites, sentiment\nanalysis has become a more active area of research in the past few years. On a\nhigh level, sentiment analysis tries to understand the public opinion about a\nspecific product or topic, or trends from reviews or tweets. Sentiment analysis\nplays an important role in better understanding customer/user opinion, and also\nextracting social/political trends. There has been a lot of previous works for\nsentiment analysis, some based on hand-engineering relevant textual features,\nand others based on different neural network architectures. In this work, we\npresent a model based on an ensemble of long-short-term-memory (LSTM), and\nconvolutional neural network (CNN), one to capture the temporal information of\nthe data, and the other one to extract the local structure thereof. Through\nexperimental results, we show that using this ensemble model we can outperform\nboth individual models. We are also able to achieve a very high accuracy rate\ncompared to the previous works.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 17:26:20 GMT"}], "update_date": "2019-04-14", "authors_parsed": [["Minaee", "Shervin", ""], ["Azimi", "Elham", ""], ["Abdolrashidi", "AmirAli", ""]]}, {"id": "1904.04221", "submitter": "Mohammad Esmaeilpour", "authors": "Mohammad Esmaeilpour, Patrick Cardinal, Alessandro Lameiras Koerich", "title": "Unsupervised Feature Learning for Environmental Sound Classification\n  Using Weighted Cycle-Consistent Generative Adversarial Network", "comments": "Paper Accepted for Publication in Elsevier Applied Soft Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel environmental sound classification approach\nincorporating unsupervised feature learning from codebook via spherical\n$K$-Means++ algorithm and a new architecture for high-level data augmentation.\nThe audio signal is transformed into a 2D representation using a discrete\nwavelet transform (DWT). The DWT spectrograms are then augmented by a novel\narchitecture for cycle-consistent generative adversarial network. This\nhigh-level augmentation bootstraps generated spectrograms in both intra and\ninter class manners by translating structural features from sample to sample. A\ncodebook is built by coding the DWT spectrograms with the speeded-up robust\nfeature detector (SURF) and the K-Means++ algorithm. The Random Forest is our\nfinal learning algorithm which learns the environmental sound classification\ntask from the clustered codewords in the codebook. Experimental results in four\nbenchmarking environmental sound datasets (ESC-10, ESC-50, UrbanSound8k, and\nDCASE-2017) have shown that the proposed classification approach outperforms\nthe state-of-the-art classifiers in the scope, including advanced and dense\nconvolutional neural networks such as AlexNet and GoogLeNet, improving the\nclassification rate between 3.51% and 14.34%, depending on the dataset.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 17:44:14 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 17:43:32 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Esmaeilpour", "Mohammad", ""], ["Cardinal", "Patrick", ""], ["Koerich", "Alessandro Lameiras", ""]]}, {"id": "1904.04238", "submitter": "Lu Bai", "authors": "Lu Bail, Lixin Cui, Yuhang Jiao, Luca Rossi, Edwin R. Hancock", "title": "Learning Backtrackless Aligned-Spatial Graph Convolutional Networks for\n  Graph Classification", "comments": "This is an extension work for journals based on the previous work of\n  arXiv:1904.04238v1", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2020", "doi": "10.1109/TPAMI.2020.3011866", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a novel Backtrackless Aligned-Spatial Graph\nConvolutional Network (BASGCN) model to learn effective features for graph\nclassification. Our idea is to transform arbitrary-sized graphs into\nfixed-sized backtrackless aligned grid structures and define a new spatial\ngraph convolution operation associated with the grid structures. We show that\nthe proposed BASGCN model not only reduces the problems of information loss and\nimprecise information representation arising in existing spatially-based Graph\nConvolutional Network (GCN) models, but also bridges the theoretical gap\nbetween traditional Convolutional Neural Network (CNN) models and\nspatially-based GCN models. Furthermore, the proposed BASGCN model can both\nadaptively discriminate the importance between specified vertices during the\nconvolution process and reduce the notorious tottering problem of existing\nspatially-based GCNs related to the Weisfeiler-Lehman algorithm, explaining the\neffectiveness of the proposed model. Experiments on standard graph datasets\ndemonstrate the effectiveness of the proposed model.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 09:50:54 GMT"}, {"version": "v2", "created": "Sun, 17 Nov 2019 17:18:09 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Bail", "Lu", ""], ["Cui", "Lixin", ""], ["Jiao", "Yuhang", ""], ["Rossi", "Luca", ""], ["Hancock", "Edwin R.", ""]]}, {"id": "1904.04272", "submitter": "Martin Engilberge", "authors": "Martin Engilberge, Louis Chevallier, Patrick P\\'erez, Matthieu Cord", "title": "SoDeep: a Sorting Deep net to learn ranking loss surrogates", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several tasks in machine learning are evaluated using non-differentiable\nmetrics such as mean average precision or Spearman correlation. However, their\nnon-differentiability prevents from using them as objective functions in a\nlearning framework. Surrogate and relaxation methods exist but tend to be\nspecific to a given metric.\n  In the present work, we introduce a new method to learn approximations of\nsuch non-differentiable objective functions. Our approach is based on a deep\narchitecture that approximates the sorting of arbitrary sets of scores. It is\ntrained virtually for free using synthetic data. This sorting deep (SoDeep) net\ncan then be combined in a plug-and-play manner with existing deep\narchitectures. We demonstrate the interest of our approach in three different\ntasks that require ranking: Cross-modal text-image retrieval, multi-label image\nclassification and visual memorability ranking. Our approach yields very\ncompetitive results on these three tasks, which validates the merit and the\nflexibility of SoDeep as a proxy for sorting operation in ranking-based losses.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 18:02:43 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Engilberge", "Martin", ""], ["Chevallier", "Louis", ""], ["P\u00e9rez", "Patrick", ""], ["Cord", "Matthieu", ""]]}, {"id": "1904.04276", "submitter": "Lin Liu L", "authors": "Lin Liu, Rajarshi Mukherjee, James M. Robins", "title": "On nearly assumption-free tests of nominal confidence interval coverage\n  for causal parameters estimated by machine learning", "comments": "Significant updates from the previous version. In press in\n  Statistical Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many causal effect parameters of interest, doubly robust machine learning\n(DRML) estimators $\\hat{\\psi}_{1}$ are the state-of-the-art, incorporating the\ngood prediction performance of machine learning; the decreased bias of doubly\nrobust estimators; and the analytic tractability and bias reduction of sample\nsplitting with cross fitting. Nonetheless, even in the absence of confounding\nby unmeasured factors, the nominal $(1 - \\alpha)$ Wald confidence interval\n$\\hat{\\psi}_{1} \\pm z_{\\alpha / 2} \\widehat{\\mathsf{se}} [\\hat{\\psi}_{1}]$ may\nstill undercover even in large samples, because the bias of $\\hat{\\psi}_{1}$\nmay be of the same or even larger order than its standard error of order\n$n^{-1/2}$.\n  In this paper, we introduce essentially assumption-free tests that (i) can\nfalsify the null hypothesis that the bias of $\\hat{\\psi}_{1}$ is of smaller\norder than its standard error, (ii) can provide an upper confidence bound on\nthe true coverage of the Wald interval, and (iii) are valid under the null\nunder no smoothness/sparsity assumptions on the nuisance parameters. The tests,\nwhich we refer to as \\underline{A}ssumption \\underline{F}ree\n\\underline{E}mpirical \\underline{C}overage \\underline{T}ests (AFECTs), are\nbased on a U-statistic that estimates part of the bias of $\\hat{\\psi}_{1}$.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 18:09:45 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2020 12:47:27 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Liu", "Lin", ""], ["Mukherjee", "Rajarshi", ""], ["Robins", "James M.", ""]]}, {"id": "1904.04309", "submitter": "Marcin Sendera", "authors": "Marcin Sendera", "title": "Data adaptation in HANDY economy-ideology model", "comments": "172 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of mathematical modeling is widespread across almost all of the\nfields of contemporary science and engineering. Because of the existing\nnecessity of predictions the behavior of natural phenomena, the researchers\ndevelop more and more complex models. However, despite their ability to better\nforecasting, the problem of an appropriate fitting ground truth data to those,\nhigh-dimensional and nonlinear models seems to be inevitable. In order to deal\nwith this demanding problem the entire discipline of data assimilation has been\ndeveloped. Basing on the Human and Nature Dynamics (HANDY) model, we have\npresented a detailed and comprehensive comparison of Approximate Bayesian\nComputation (classic data assimilation method) and a novelty approach of\nSupermodeling. Furthermore, with the usage of Sensitivity Analysis, we have\nproposed the methodology to reduce the number of coupling coefficients between\nsubmodels and as a consequence to increase the speed of the Supermodel\nconverging. In addition, we have demonstrated that usage of Approximate\nBayesian Computation method with the knowledge about parameters' sensitivities\ncould result with satisfactory estimation of the initial parameters. However,\nwe have also presented the mentioned methodology as unable to achieve similar\npredictions to Approximate Bayesian Computation. Finally, we have proved that\nSupermodeling with synchronization via the most sensitive variable could effect\nwith the better forecasting for chaotic as well as more stable systems than the\nApproximate Bayesian Computation. What is more, we have proposed the adequate\nmethodologies.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 19:19:59 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Sendera", "Marcin", ""]]}, {"id": "1904.04326", "submitter": "Lei Wu", "authors": "Weinan E, Chao Ma, Lei Wu", "title": "A Comparative Analysis of the Optimization and Generalization Property\n  of Two-layer Neural Network and Random Feature Models Under Gradient Descent\n  Dynamics", "comments": "Published version", "journal-ref": "Science China Mathematics (2020)", "doi": "10.1007/s11425-019-1628-5", "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fairly comprehensive analysis is presented for the gradient descent\ndynamics for training two-layer neural network models in the situation when the\nparameters in both layers are updated. General initialization schemes as well\nas general regimes for the network width and training data size are considered.\nIn the over-parametrized regime, it is shown that gradient descent dynamics can\nachieve zero training loss exponentially fast regardless of the quality of the\nlabels. In addition, it is proved that throughout the training process the\nfunctions represented by the neural network model are uniformly close to that\nof a kernel method. For general values of the network width and training data\nsize, sharp estimates of the generalization error is established for target\nfunctions in the appropriate reproducing kernel Hilbert space.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 19:43:09 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 04:48:07 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["E", "Weinan", ""], ["Ma", "Chao", ""], ["Wu", "Lei", ""]]}, {"id": "1904.04334", "submitter": "Shahbaz Rezaei", "authors": "Shahbaz Rezaei and Xin Liu", "title": "A Target-Agnostic Attack on Deep Models: Exploiting Security\n  Vulnerabilities of Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to insufficient training data and the high computational cost to train a\ndeep neural network from scratch, transfer learning has been extensively used\nin many deep-neural-network-based applications. A commonly used transfer\nlearning approach involves taking a part of a pre-trained model, adding a few\nlayers at the end, and re-training the new layers with a small dataset. This\napproach, while efficient and widely used, imposes a security vulnerability\nbecause the pre-trained model used in transfer learning is usually publicly\navailable, including to potential attackers. In this paper, we show that\nwithout any additional knowledge other than the pre-trained model, an attacker\ncan launch an effective and efficient brute force attack that can craft\ninstances of input to trigger each target class with high confidence. We assume\nthat the attacker has no access to any target-specific information, including\nsamples from target classes, re-trained model, and probabilities assigned by\nSoftmax to each class, and thus making the attack target-agnostic. These\nassumptions render all previous attack models inapplicable, to the best of our\nknowledge. To evaluate the proposed attack, we perform a set of experiments on\nface recognition and speech recognition tasks and show the effectiveness of the\nattack. Our work reveals a fundamental security weakness of the Softmax layer\nwhen used in transfer learning settings.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 20:03:28 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 02:18:10 GMT"}, {"version": "v3", "created": "Wed, 29 Jan 2020 06:51:13 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Rezaei", "Shahbaz", ""], ["Liu", "Xin", ""]]}, {"id": "1904.04352", "submitter": "Pramit Saha", "authors": "Pramit Saha and Sidney Fels", "title": "Hierarchical Deep Feature Learning For Decoding Imagined Speech From EEG", "comments": "Accepted in AAAI 2019 under Student Abstract and Poster Program", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a mixed deep neural network strategy, incorporating parallel\ncombination of Convolutional (CNN) and Recurrent Neural Networks (RNN),\ncascaded with deep autoencoders and fully connected layers towards automatic\nidentification of imagined speech from EEG. Instead of utilizing raw EEG\nchannel data, we compute the joint variability of the channels in the form of a\ncovariance matrix that provide spatio-temporal representations of EEG. The\nnetworks are trained hierarchically and the extracted features are passed onto\nthe next network hierarchy until the final classification. Using a publicly\navailable EEG based speech imagery database we demonstrate around 23.45%\nimprovement of accuracy over the baseline method. Our approach demonstrates the\npromise of a mixed DNN approach for complex spatial-temporal classification\nproblems.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 20:56:15 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Saha", "Pramit", ""], ["Fels", "Sidney", ""]]}, {"id": "1904.04354", "submitter": "Neslisah Torosdagli", "authors": "Neslisah Torosdagli, Mary McIntosh, Denise K. Liberton, Payal Verma,\n  Murat Sincan, Wade W. Han, Janice S. Lee, and Ulas Bagci", "title": "Relational Reasoning Network (RRN) for Anatomical Landmarking", "comments": "10 pages, 6 Figures, 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately identifying anatomical landmarks is a crucial step in deformation\nanalysis and surgical planning for craniomaxillofacial (CMF) bones. Available\nmethods require segmentation of the object of interest for precise landmarking.\nUnlike those, our purpose in this study is to perform anatomical landmarking\nusing the inherent relation of CMF bones without explicitly segmenting them. We\npropose a new deep network architecture, called relational reasoning network\n(RRN), to accurately learn the local and the global relations of the landmarks.\nSpecifically, we are interested in learning landmarks in CMF region: mandible,\nmaxilla, and nasal bones. The proposed RRN works in an end-to-end manner,\nutilizing learned relations of the landmarks based on dense-block units and\nwithout the need for segmentation. For a given a few landmarks as input, the\nproposed system accurately and efficiently localizes the remaining landmarks on\nthe aforementioned bones. For a comprehensive evaluation of RRN, we used\ncone-beam computed tomography (CBCT) scans of 250 patients. The proposed system\nidentifies the landmark locations very accurately even when there are severe\npathologies or deformations in the bones. The proposed RRN has also revealed\nunique relationships among the landmarks that help us infer several reasoning\nabout informativeness of the landmark points. RRN is invariant to order of\nlandmarks and it allowed us to discover the optimal configurations (number and\nlocation) for landmarks to be localized within the object of interest\n(mandible) or nearby objects (maxilla and nasal). To the best of our knowledge,\nthis is the first of its kind algorithm finding anatomical relations of the\nobjects using deep learning.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 20:58:47 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Torosdagli", "Neslisah", ""], ["McIntosh", "Mary", ""], ["Liberton", "Denise K.", ""], ["Verma", "Payal", ""], ["Sincan", "Murat", ""], ["Han", "Wade W.", ""], ["Lee", "Janice S.", ""], ["Bagci", "Ulas", ""]]}, {"id": "1904.04358", "submitter": "Pramit Saha", "authors": "Pramit Saha, Muhammad Abdul-Mageed and Sidney Fels", "title": "Deep Learning the EEG Manifold for Phonological Categorization from\n  Active Thoughts", "comments": "Accepted for publication in IEEE ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech-related Brain Computer Interfaces (BCI) aim primarily at finding an\nalternative vocal communication pathway for people with speaking disabilities.\nAs a step towards full decoding of imagined speech from active thoughts, we\npresent a BCI system for subject-independent classification of phonological\ncategories exploiting a novel deep learning based hierarchical feature\nextraction scheme. To better capture the complex representation of\nhigh-dimensional electroencephalography (EEG) data, we compute the joint\nvariability of EEG electrodes into a channel cross-covariance matrix. We then\nextract the spatio-temporal information encoded within the matrix using a mixed\ndeep neural network strategy. Our model framework is composed of a\nconvolutional neural network (CNN), a long-short term network (LSTM), and a\ndeep autoencoder. We train the individual networks hierarchically, feeding\ntheir combined outputs in a final gradient boosting classification step. Our\nbest models achieve an average accuracy of 77.9% across five different binary\nclassification tasks, providing a significant 22.5% improvement over previous\nmethods. As we also show visually, our work demonstrates that the speech\nimagery EEG possesses significant discriminative information about the intended\narticulatory movements responsible for natural speech synthesis.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 21:11:40 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Saha", "Pramit", ""], ["Abdul-Mageed", "Muhammad", ""], ["Fels", "Sidney", ""]]}, {"id": "1904.04378", "submitter": "Yuqi Gu", "authors": "Yuqi Gu and Gongjun Xu", "title": "Learning Attribute Patterns in High-Dimensional Structured Latent\n  Attribute Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured latent attribute models (SLAMs) are a special family of discrete\nlatent variable models widely used in social and biological sciences. This\npaper considers the problem of learning significant attribute patterns from a\nSLAM with potentially high-dimensional configurations of the latent attributes.\nWe address the theoretical identifiability issue, propose a penalized\nlikelihood method for the selection of the attribute patterns, and further\nestablish the selection consistency in such an overfitted SLAM with diverging\nnumber of latent patterns. The good performance of the proposed methodology is\nillustrated by simulation studies and two real datasets in educational\nassessment.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 22:04:50 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 03:50:16 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Gu", "Yuqi", ""], ["Xu", "Gongjun", ""]]}, {"id": "1904.04432", "submitter": "Yang Li", "authors": "Yang Li, Shihao Ji", "title": "$L_0$-ARM: Network Sparsification via Stochastic Binary Optimization", "comments": "Published as a conference paper at ECML 2019", "journal-ref": null, "doi": "10.1007/978-3-030-46147-8_26", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider network sparsification as an $L_0$-norm regularized binary\noptimization problem, where each unit of a neural network (e.g., weight,\nneuron, or channel, etc.) is attached with a stochastic binary gate, whose\nparameters are jointly optimized with original network parameters. The\nAugment-Reinforce-Merge (ARM), a recently proposed unbiased gradient estimator,\nis investigated for this binary optimization problem. Compared to the hard\nconcrete gradient estimator from Louizos et al., ARM demonstrates superior\nperformance of pruning network architectures while retaining almost the same\naccuracies of baseline methods. Similar to the hard concrete estimator, ARM\nalso enables conditional computation during model training but with improved\neffectiveness due to the exact binary stochasticity. Thanks to the flexibility\nof ARM, many smooth or non-smooth parametric functions, such as scaled sigmoid\nor hard sigmoid, can be used to parameterize this binary optimization problem\nand the unbiasness of the ARM estimator is retained, while the hard concrete\nestimator has to rely on the hard sigmoid function to achieve conditional\ncomputation and thus accelerated training. Extensive experiments on multiple\npublic datasets demonstrate state-of-the-art pruning rates with almost the same\naccuracies of baseline methods. The resulting algorithm $L_0$-ARM sparsifies\nthe Wide-ResNet models on CIFAR-10 and CIFAR-100 while the hard concrete\nestimator cannot. The code is public available at\nhttps://github.com/leo-yangli/l0-arm.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 02:43:31 GMT"}, {"version": "v2", "created": "Sat, 7 Sep 2019 18:08:27 GMT"}, {"version": "v3", "created": "Wed, 11 Sep 2019 13:38:21 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Li", "Yang", ""], ["Ji", "Shihao", ""]]}, {"id": "1904.04478", "submitter": "Raghav Singhal", "authors": "Raghav Singhal, Xintian Han, Saad Lahlou, Rajesh Ranganath", "title": "Kernelized Complete Conditional Stein Discrepancy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of machine learning relies on comparing distributions with discrepancy\nmeasures. Stein's method creates discrepancy measures between two distributions\nthat require only the unnormalized density of one and samples from the other.\nStein discrepancies can be combined with kernels to define kernelized Stein\ndiscrepancies (KSDs). While kernels make Stein discrepancies tractable, they\npose several challenges in high dimensions. We introduce kernelized complete\nconditional Stein discrepancies (KCC-SDs). Complete conditionals turn a\nmultivariate distribution into multiple univariate distributions. We show that\nKCC-SDs distinguish distributions. To show the efficacy of KCC-SDs in\ndistinguishing distributions, we introduce a goodness-of-fit test using\nKCC-SDs. We empirically show that KCC-SDs have higher power over baselines and\nuse KCC-SDs to assess sample quality in Markov chain Monte Carlo.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 06:06:23 GMT"}, {"version": "v2", "created": "Sat, 13 Apr 2019 07:09:09 GMT"}, {"version": "v3", "created": "Tue, 21 Jan 2020 03:21:14 GMT"}, {"version": "v4", "created": "Sat, 18 Jul 2020 03:06:14 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Singhal", "Raghav", ""], ["Han", "Xintian", ""], ["Lahlou", "Saad", ""], ["Ranganath", "Rajesh", ""]]}, {"id": "1904.04480", "submitter": "Lihua Lei", "authors": "Lihua Lei and Michael I. Jordan", "title": "On the Adaptivity of Stochastic Gradient-Based Optimization", "comments": "Accepted by SIAM Journal on Optimization; 54 pages", "journal-ref": null, "doi": "10.1137/19M1256919", "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic-gradient-based optimization has been a core enabling methodology\nin applications to large-scale problems in machine learning and related areas.\nDespite the progress, the gap between theory and practice remains significant,\nwith theoreticians pursuing mathematical optimality at a cost of obtaining\nspecialized procedures in different regimes (e.g., modulus of strong convexity,\nmagnitude of target accuracy, signal-to-noise ratio), and with practitioners\nnot readily able to know which regime is appropriate to their problem, and\nseeking broadly applicable algorithms that are reasonably close to optimality.\nTo bridge these perspectives it is necessary to study algorithms that are\nadaptive to different regimes. We present the stochastically controlled\nstochastic gradient (SCSG) method for composite convex finite-sum optimization\nproblems and show that SCSG is adaptive to both strong convexity and target\naccuracy. The adaptivity is achieved by batch variance reduction with adaptive\nbatch sizes and a novel technique, which we referred to as geometrization,\nwhich sets the length of each epoch as a geometric random variable. The\nalgorithm achieves strictly better theoretical complexity than other existing\nadaptive algorithms, while the tuning parameters of the algorithm only depend\non the smoothness parameter of the objective.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 06:07:55 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 03:57:08 GMT"}, {"version": "v3", "created": "Thu, 31 Dec 2020 18:57:20 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Lei", "Lihua", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1904.04516", "submitter": "Matthias Rottmann", "authors": "Matthias Rottmann and Marius Schubert", "title": "Uncertainty Measures and Prediction Quality Rating for the Semantic\n  Segmentation of Nested Multi Resolution Street Scene Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the semantic segmentation of street scenes the reliability of the\nprediction and therefore uncertainty measures are of highest interest. We\npresent a method that generates for each input image a hierarchy of nested\ncrops around the image center and presents these, all re-scaled to the same\nsize, to a neural network for semantic segmentation. The resulting softmax\noutputs are then post processed such that we can investigate mean and variance\nover all image crops as well as mean and variance of uncertainty heat maps\nobtained from pixel-wise uncertainty measures, like the entropy, applied to\neach crop's softmax output. In our tests, we use the publicly available\nDeepLabv3+ MobilenetV2 network (trained on the Cityscapes dataset) and\ndemonstrate that the incorporation of crops improves the quality of the\nprediction and that we obtain more reliable uncertainty measures. These are\nthen aggregated over predicted segments for either classifying between IoU=0\nand IoU>0 (meta classification) or predicting the IoU via linear regression\n(meta regression). The latter yields reliable performance estimates for\nsegmentation networks, in particular useful in the absence of ground truth. For\nthe task of meta classification we obtain a classification accuracy of\n$81.93\\%$ and an AUROC of $89.89\\%$. For meta regression we obtain an $R^2$\nvalue of $84.77\\%$. These results yield significant improvements compared to\nother approaches.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 08:14:09 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Rottmann", "Matthias", ""], ["Schubert", "Marius", ""]]}, {"id": "1904.04520", "submitter": "Mara Graziani Miss", "authors": "Mara Graziani, Vincent Andrearczyk and Henning M\\\"uller", "title": "Regression Concept Vectors for Bidirectional Explanations in\n  Histopathology", "comments": "9 pages, 3 figures, 3 tables", "journal-ref": "Understanding and Interpreting Machine Learning in Medical Image\n  Computing Applications: First International Workshops, Proceedings. Vol.\n  11038. Springer, 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explanations for deep neural network predictions in terms of domain-related\nconcepts can be valuable in medical applications, where justifications are\nimportant for confidence in the decision-making. In this work, we propose a\nmethodology to exploit continuous concept measures as Regression Concept\nVectors (RCVs) in the activation space of a layer. The directional derivative\nof the decision function along the RCVs represents the network sensitivity to\nincreasing values of a given concept measure. When applied to breast cancer\ngrading, nuclei texture emerges as a relevant concept in the detection of tumor\ntissue in breast lymph node samples. We evaluate score robustness and\nconsistency by statistical analysis.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 08:26:02 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Graziani", "Mara", ""], ["Andrearczyk", "Vincent", ""], ["M\u00fcller", "Henning", ""]]}, {"id": "1904.04540", "submitter": "Hirokazu Kameoka", "authors": "Hirokazu Kameoka, Kou Tanaka, Aaron Valero Puche, Yasunori Ohishi, and\n  Takuhiro Kaneko", "title": "Crossmodal Voice Conversion", "comments": "Submitted to Interspeech2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are able to imagine a person's voice from the person's appearance and\nimagine the person's appearance from his/her voice. In this paper, we make the\nfirst attempt to develop a method that can convert speech into a voice that\nmatches an input face image and generate a face image that matches the voice of\nthe input speech by leveraging the correlation between faces and voices. We\npropose a model, consisting of a speech converter, a face encoder/decoder and a\nvoice encoder. We use the latent code of an input face image encoded by the\nface encoder as the auxiliary input into the speech converter and train the\nspeech converter so that the original latent code can be recovered from the\ngenerated speech by the voice encoder. We also train the face decoder along\nwith the face encoder to ensure that the latent code will contain sufficient\ninformation to reconstruct the input face image. We confirmed experimentally\nthat a speech converter trained in this way was able to convert input speech\ninto a voice that matched an input face image and that the voice encoder and\nface decoder can be used to generate a face image that matches the voice of the\ninput speech.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 08:53:10 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Kameoka", "Hirokazu", ""], ["Tanaka", "Kou", ""], ["Puche", "Aaron Valero", ""], ["Ohishi", "Yasunori", ""], ["Kaneko", "Takuhiro", ""]]}, {"id": "1904.04564", "submitter": "Art\\\"ur Manukyan", "authors": "Art\\\"ur Manukyan and Elvan Ceyhan", "title": "Classification of Imbalanced Data with a Geometric Digraph Family", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use a geometric digraph family called class cover catch digraphs (CCCDs)\nto tackle the class imbalance problem in statistical classification. CCCDs\nprovide graph theoretic solutions to the class cover problem and have been\nemployed in classification. We assess the classification performance of CCCD\nclassifiers by extensive Monte Carlo simulations, comparing them with other\nclassifiers commonly used in the literature. In particular, we show that CCCD\nclassifiers perform relatively well when one class is more frequent than the\nother in a two-class setting, an example of the class imbalance problem. We\nalso point out the relationship between class imbalance and class overlapping\nproblems, and their influence on the performance of CCCD classifiers and other\nclassification methods as well as some state-of-the-art algorithms which are\nrobust to class imbalance by construction. Experiments on both simulated and\nreal data sets indicate that CCCD classifiers are robust to the class imbalance\nproblem. CCCDs substantially undersample from the majority class while\npreserving the information on the discarded points during the undersampling\nprocess. Many state-of-the-art methods, however, keep this information by means\nof ensemble classifiers, but CCCDs yield only a single classifier with the same\nproperty, making it both appealing and fast.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 09:45:24 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Manukyan", "Art\u00fcr", ""], ["Ceyhan", "Elvan", ""]]}, {"id": "1904.04573", "submitter": "Guillaume Staerman", "authors": "Guillaume Staerman, Pavlo Mozharovskyi, Stephan Cl\\'emen\\c{c}on,\n  Florence d'Alch\\'e-Buc", "title": "Functional Isolation Forest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For the purpose of monitoring the behavior of complex infrastructures (e.g.\naircrafts, transport or energy networks), high-rate sensors are deployed to\ncapture multivariate data, generally unlabeled, in quasi continuous-time to\ndetect quickly the occurrence of anomalies that may jeopardize the smooth\noperation of the system of interest. The statistical analysis of such massive\ndata of functional nature raises many challenging methodological questions. The\nprimary goal of this paper is to extend the popular Isolation Forest (IF)\napproach to Anomaly Detection, originally dedicated to finite dimensional\nobservations, to functional data. The major difficulty lies in the wide variety\nof topological structures that may equip a space of functions and the great\nvariety of patterns that may characterize abnormal curves. We address the issue\nof (randomly) splitting the functional space in a flexible manner in order to\nisolate progressively any trajectory from the others, a key ingredient to the\nefficiency of the algorithm. Beyond a detailed description of the algorithm,\ncomputational complexity and stability issues are investigated at length. From\nthe scoring function measuring the degree of abnormality of an observation\nprovided by the proposed variant of the IF algorithm, a Functional Statistical\nDepth function is defined and discussed as well as a multivariate functional\nextension. Numerical experiments provide strong empirical evidence of the\naccuracy of the extension proposed.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 10:04:18 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 07:30:09 GMT"}, {"version": "v3", "created": "Wed, 9 Oct 2019 16:06:43 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Staerman", "Guillaume", ""], ["Mozharovskyi", "Pavlo", ""], ["Cl\u00e9men\u00e7on", "Stephan", ""], ["d'Alch\u00e9-Buc", "Florence", ""]]}, {"id": "1904.04631", "submitter": "Takuhiro Kaneko", "authors": "Takuhiro Kaneko, Hirokazu Kameoka, Kou Tanaka, Nobukatsu Hojo", "title": "CycleGAN-VC2: Improved CycleGAN-based Non-parallel Voice Conversion", "comments": "Accepted to ICASSP 2019. Project page:\n  http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/cyclegan-vc2/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-parallel voice conversion (VC) is a technique for learning the mapping\nfrom source to target speech without relying on parallel data. This is an\nimportant task, but it has been challenging due to the disadvantages of the\ntraining conditions. Recently, CycleGAN-VC has provided a breakthrough and\nperformed comparably to a parallel VC method without relying on any extra data,\nmodules, or time alignment procedures. However, there is still a large gap\nbetween the real target and converted speech, and bridging this gap remains a\nchallenge. To reduce this gap, we propose CycleGAN-VC2, which is an improved\nversion of CycleGAN-VC incorporating three new techniques: an improved\nobjective (two-step adversarial losses), improved generator (2-1-2D CNN), and\nimproved discriminator (PatchGAN). We evaluated our method on a non-parallel VC\ntask and analyzed the effect of each technique in detail. An objective\nevaluation showed that these techniques help bring the converted feature\nsequence closer to the target in terms of both global and local structures,\nwhich we assess by using Mel-cepstral distortion and modulation spectra\ndistance, respectively. A subjective evaluation showed that CycleGAN-VC2\noutperforms CycleGAN-VC in terms of naturalness and similarity for every\nspeaker pair, including intra-gender and inter-gender pairs.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 12:55:35 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Kaneko", "Takuhiro", ""], ["Kameoka", "Hirokazu", ""], ["Tanaka", "Kou", ""], ["Hojo", "Nobukatsu", ""]]}, {"id": "1904.04645", "submitter": "Thiago Jose Marques Moura", "authors": "Thiago J. M. Moura, George D. C. Cavalcanti, and Luiz S. Oliveira", "title": "Evaluating Competence Measures for Dynamic Regressor Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic regressor selection (DRS) systems work by selecting the most\ncompetent regressors from an ensemble to estimate the target value of a given\ntest pattern. This competence is usually quantified using the performance of\nthe regressors in local regions of the feature space around the test pattern.\nHowever, choosing the best measure to calculate the level of competence\ncorrectly is not straightforward. The literature of dynamic classifier\nselection presents a wide variety of competence measures, which cannot be used\nor adapted for DRS. In this paper, we review eight measures used with\nregression problems, and adapt them to test the performance of the DRS\nalgorithms found in the literature. Such measures are extracted from a local\nregion of the feature space around the test pattern, called region of\ncompetence, therefore competence measures.To better compare the competence\nmeasures, we perform a set of comprehensive experiments of 15 regression\ndatasets. Three DRS systems were compared against individual regressor and\nstatic systems that use the Mean and the Median to combine the outputs of the\nregressors from the ensemble. The DRS systems were assessed varying the\ncompetence measures. Our results show that DRS systems outperform individual\nregressors and static systems but the choice of the competence measure is\nproblem-dependent.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 13:18:12 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Moura", "Thiago J. M.", ""], ["Cavalcanti", "George D. C.", ""], ["Oliveira", "Luiz S.", ""]]}, {"id": "1904.04668", "submitter": "Reza Moradinezhad", "authors": "J. Ghasemi, R. Moradinezhad, M. A. Hosseini", "title": "Kinematic Synthesis of Parallel Manipulator via Neural Network Approach", "comments": null, "journal-ref": "IJE TRANSACTIONS C: Aspects Vol. 30, No. 9 (September 2017)\n  1319-1325", "doi": null, "report-no": null, "categories": "cs.RO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this research, Artificial Neural Networks (ANNs) have been used as a\npowerful tool to solve the inverse kinematic equations of a parallel robot. For\nthis purpose, we have developed the kinematic equations of a Tricept parallel\nkinematic mechanism with two rotational and one translational degrees of\nfreedom (DoF). Using the analytical method, the inverse kinematic equations are\nsolved for specific trajectory, and used as inputs for the applied ANNs. The\nresults of both applied networks (Multi-Layer Perceptron and Redial Basis\nFunction) satisfied the required performance in solving complex inverse\nkinematics with proper accuracy and speed.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 03:48:57 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Ghasemi", "J.", ""], ["Moradinezhad", "R.", ""], ["Hosseini", "M. A.", ""]]}, {"id": "1904.04671", "submitter": "Selim Arikan", "authors": "Selim Arikan, Kiran Varanasi, Didier Stricker", "title": "Surface Defect Classification in Real-Time Using Convolutional Neural\n  Networks", "comments": "Supplementary material will follow", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surface inspection systems are an important application domain for computer\nvision, as they are used for defect detection and classification in the\nmanufacturing industry. Existing systems use hand-crafted features which\nrequire extensive domain knowledge to create. Even though Convolutional neural\nnetworks (CNNs) have proven successful in many large-scale challenges,\nindustrial inspection systems have yet barely realized their potential due to\ntwo significant challenges: real-time processing speed requirements and\nspecialized narrow domain-specific datasets which are sometimes limited in\nsize. In this paper, we propose CNN models that are specifically designed to\nhandle capacity and real-time speed requirements of surface inspection systems.\nTo train and evaluate our network models, we created a surface image dataset\ncontaining more than 22000 labeled images with many types of surface materials\nand achieved 98.0% accuracy in binary defect classification. To solve the class\nimbalance problem in our datasets, we introduce neural data augmentation\nmethods which are also applicable to similar domains that suffer from the same\nproblem. Our results show that deep learning based methods are feasible to be\nused in surface inspection systems and outperform traditional methods in\naccuracy and inference time by considerable margins.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 21:22:38 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Arikan", "Selim", ""], ["Varanasi", "Kiran", ""], ["Stricker", "Didier", ""]]}, {"id": "1904.04676", "submitter": "Nicola De Cao", "authors": "Nicola De Cao, Ivan Titov and Wilker Aziz", "title": "Block Neural Autoregressive Flow", "comments": "12 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalising flows (NFS) map two density functions via a differentiable\nbijection whose Jacobian determinant can be computed efficiently. Recently, as\nan alternative to hand-crafted bijections, Huang et al. (2018) proposed neural\nautoregressive flow (NAF) which is a universal approximator for density\nfunctions. Their flow is a neural network (NN) whose parameters are predicted\nby another NN. The latter grows quadratically with the size of the former and\nthus an efficient technique for parametrization is needed. We propose block\nneural autoregressive flow (B-NAF), a much more compact universal approximator\nof density functions, where we model a bijection directly using a single\nfeed-forward network. Invertibility is ensured by carefully designing each\naffine transformation with block matrices that make the flow autoregressive and\n(strictly) monotone. We compare B-NAF to NAF and other established flows on\ndensity estimation and approximate inference for latent variable models. Our\nproposed flow is competitive across datasets while using orders of magnitude\nfewer parameters.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 13:54:55 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["De Cao", "Nicola", ""], ["Titov", "Ivan", ""], ["Aziz", "Wilker", ""]]}, {"id": "1904.04700", "submitter": "Edouard Leurent", "authors": "Edouard Leurent and Odalric-Ambrym Maillard", "title": "Practical Open-Loop Optimistic Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of online planning in a Markov Decision Process when\ngiven only access to a generative model, restricted to open-loop policies -\ni.e. sequences of actions - and under budget constraint. In this setting, the\nOpen-Loop Optimistic Planning (OLOP) algorithm enjoys good theoretical\nguarantees but is overly conservative in practice, as we show in numerical\nexperiments. We propose a modified version of the algorithm with tighter\nupper-confidence bounds, KLOLOP, that leads to better practical performances\nwhile retaining the sample complexity bound. Finally, we propose an efficient\nimplementation that significantly improves the time complexity of both\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 14:29:53 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Leurent", "Edouard", ""], ["Maillard", "Odalric-Ambrym", ""]]}, {"id": "1904.04732", "submitter": "Daniel Russo", "authors": "Daniel Russo", "title": "A Note on the Equivalence of Upper Confidence Bounds and Gittins Indices\n  for Patient Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note gives a short, self-contained, proof of a sharp connection between\nGittins indices and Bayesian upper confidence bound algorithms. I consider a\nGaussian multi-armed bandit problem with discount factor $\\gamma$. The Gittins\nindex of an arm is shown to equal the $\\gamma$-quantile of the posterior\ndistribution of the arm's mean plus an error term that vanishes as $\\gamma\\to\n1$. In this sense, for sufficiently patient agents, a Gittins index measures\nthe highest plausible mean-reward of an arm in a manner equivalent to an upper\nconfidence bound.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 15:30:52 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Russo", "Daniel", ""]]}, {"id": "1904.04751", "submitter": "Aibek Alanov", "authors": "Aibek Alanov, Max Kochurov, Denis Volkhonskiy, Daniil Yashkov, Evgeny\n  Burnaev, Dmitry Vetrov", "title": "User-Controllable Multi-Texture Synthesis with Generative Adversarial\n  Networks", "comments": "8 pages paper, 17 pages supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel multi-texture synthesis model based on generative\nadversarial networks (GANs) with a user-controllable mechanism. The user\ncontrol ability allows to explicitly specify the texture which should be\ngenerated by the model. This property follows from using an encoder part which\nlearns a latent representation for each texture from the dataset. To ensure a\ndataset coverage, we use an adversarial loss function that penalizes for\nincorrect reproductions of a given texture. In experiments, we show that our\nmodel can learn descriptive texture manifolds for large datasets and from raw\ndata such as a collection of high-resolution photos. Moreover, we apply our\nmethod to produce 3D textures and show that it outperforms existing baselines.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 15:59:16 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 12:07:22 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Alanov", "Aibek", ""], ["Kochurov", "Max", ""], ["Volkhonskiy", "Denis", ""], ["Yashkov", "Daniil", ""], ["Burnaev", "Evgeny", ""], ["Vetrov", "Dmitry", ""]]}, {"id": "1904.04755", "submitter": "Satyen Kale", "authors": "Dylan J. Foster and Spencer Greenberg and Satyen Kale and Haipeng Luo\n  and Mehryar Mohri and Karthik Sridharan", "title": "Hypothesis Set Stability and Generalization", "comments": "Published in NeurIPS 2019. This version is equivalent to the\n  camera-ready version but also includes the supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a study of generalization for data-dependent hypothesis sets. We\ngive a general learning guarantee for data-dependent hypothesis sets based on a\nnotion of transductive Rademacher complexity. Our main result is a\ngeneralization bound for data-dependent hypothesis sets expressed in terms of a\nnotion of hypothesis set stability and a notion of Rademacher complexity for\ndata-dependent hypothesis sets that we introduce. This bound admits as special\ncases both standard Rademacher complexity bounds and algorithm-dependent\nuniform stability bounds. We also illustrate the use of these learning bounds\nin the analysis of several scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 16:08:25 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 02:46:07 GMT"}, {"version": "v3", "created": "Mon, 5 Oct 2020 14:38:37 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Foster", "Dylan J.", ""], ["Greenberg", "Spencer", ""], ["Kale", "Satyen", ""], ["Luo", "Haipeng", ""], ["Mohri", "Mehryar", ""], ["Sridharan", "Karthik", ""]]}, {"id": "1904.04765", "submitter": "Song Fang", "authors": "Song Fang, Mikael Skoglund, Karl Henrik Johansson, Hideaki Ishii,\n  Quanyan Zhu", "title": "Generic Variance Bounds on Estimation and Prediction Errors in Time\n  Series Analysis: An Entropy Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG eess.SP math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we obtain generic bounds on the variances of estimation and\nprediction errors in time series analysis via an information-theoretic\napproach. It is seen in general that the error bounds are determined by the\nconditional entropy of the data point to be estimated or predicted given the\nside information or past observations. Additionally, we discover that in order\nto achieve the prediction error bounds asymptotically, the necessary and\nsufficient condition is that the \"innovation\" is asymptotically white Gaussian.\nWhen restricted to Gaussian processes and 1-step prediction, our bounds are\nshown to reduce to the Kolmogorov-Szeg\\\"o formula and Wiener-Masani formula\nknown from linear prediction theory.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 16:22:14 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 09:17:02 GMT"}, {"version": "v3", "created": "Tue, 3 Sep 2019 15:57:47 GMT"}, {"version": "v4", "created": "Fri, 11 Oct 2019 23:12:57 GMT"}, {"version": "v5", "created": "Tue, 11 May 2021 14:48:31 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Fang", "Song", ""], ["Skoglund", "Mikael", ""], ["Johansson", "Karl Henrik", ""], ["Ishii", "Hideaki", ""], ["Zhu", "Quanyan", ""]]}, {"id": "1904.04780", "submitter": "Sheng Liu", "authors": "Sheng Liu, Mark Cheng, Hayley Brooks, Wayne Mackey, David J. Heeger,\n  Esteban G. Tabak, Carlos Fernandez-Granda", "title": "Time-Series Analysis via Low-Rank Matrix Factorization Applied to\n  Infant-Sleep Data", "comments": "Machine Learning for Health (ML4H) at NeurIPS 2019 - Extended\n  Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a nonparametric model for time series with missing data based on\nlow-rank matrix factorization. The model expresses each instance in a set of\ntime series as a linear combination of a small number of shared basis\nfunctions. Constraining the functions and the corresponding coefficients to be\nnonnegative yields an interpretable low-dimensional representation of the data.\nA time-smoothing regularization term ensures that the model captures meaningful\ntrends in the data, instead of overfitting short-term fluctuations. The\nlow-dimensional representation makes it possible to detect outliers and cluster\nthe time series according to the interpretable features extracted by the model,\nand also to perform forecasting via kernel regression. We apply our methodology\nto a large real-world dataset of infant-sleep data gathered by caregivers with\na mobile-phone app. Our analysis automatically extracts daily-sleep patterns\nconsistent with the existing literature. This allows us to compute\nsleep-development trends for the cohort, which characterize the emergence of\ncircadian sleep and different napping habits. We apply our methodology to\ndetect anomalous individuals, to cluster the cohort into groups with different\nsleeping tendencies, and to obtain improved predictions of future sleep\nbehavior.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 16:51:01 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 01:55:14 GMT"}, {"version": "v3", "created": "Wed, 6 Nov 2019 21:40:41 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Liu", "Sheng", ""], ["Cheng", "Mark", ""], ["Brooks", "Hayley", ""], ["Mackey", "Wayne", ""], ["Heeger", "David J.", ""], ["Tabak", "Esteban G.", ""], ["Fernandez-Granda", "Carlos", ""]]}, {"id": "1904.04849", "submitter": "Matthias Fey", "authors": "Matthias Fey", "title": "Just Jump: Dynamic Neighborhood Aggregation in Graph Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a dynamic neighborhood aggregation (DNA) procedure guided by\n(multi-head) attention for representation learning on graphs. In contrast to\ncurrent graph neural networks which follow a simple neighborhood aggregation\nscheme, our DNA procedure allows for a selective and node-adaptive aggregation\nof neighboring embeddings of potentially differing locality. In order to avoid\noverfitting, we propose to control the channel-wise connections between input\nand output by making use of grouped linear projections. In a number of\ntransductive node-classification experiments, we demonstrate the effectiveness\nof our approach.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 18:02:54 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 17:10:23 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Fey", "Matthias", ""]]}, {"id": "1904.04861", "submitter": "Todd Huster", "authors": "Jeremy E.J. Cohen, Todd Huster, Ra Cohen", "title": "Universal Lipschitz Approximation in Bounded Depth Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks against machine learning models are a rather hefty\nobstacle to our increasing reliance on these models. Due to this, provably\nrobust (certified) machine learning models are a major topic of interest.\nLipschitz continuous models present a promising approach to solving this\nproblem. By leveraging the expressive power of a variant of neural networks\nwhich maintain low Lipschitz constants, we prove that three layer neural\nnetworks using the FullSort activation function are Universal Lipschitz\nfunction Approximators (ULAs). This both explains experimental results and\npaves the way for the creation of better certified models going forward. We\nconclude by presenting experimental results that suggest that ULAs are a not\njust a novelty, but a competitive approach to providing certified classifiers,\nusing these results to motivate several potential topics of further research.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 18:39:43 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Cohen", "Jeremy E. J.", ""], ["Huster", "Todd", ""], ["Cohen", "Ra", ""]]}, {"id": "1904.04862", "submitter": "Mojan Javaheripi", "authors": "Mojan Javaheripi, Bita Darvish Rouhani, Farinaz Koushanfar", "title": "SWNet: Small-World Neural Networks and Rapid Convergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training large and highly accurate deep learning (DL) models is\ncomputationally costly. This cost is in great part due to the excessive number\nof trained parameters, which are well-known to be redundant and compressible\nfor the execution phase. This paper proposes a novel transformation which\nchanges the topology of the DL architecture such that it reaches an optimal\ncross-layer connectivity. This transformation leverages our important\nobservation that for a set level of accuracy, convergence is fastest when\nnetwork topology reaches the boundary of a Small-World Network. Small-world\ngraphs are known to possess a specific connectivity structure that enables\nenhanced signal propagation among nodes. Our small-world models, called SWNets,\nprovide several intriguing benefits: they facilitate data (gradient) flow\nwithin the network, enable feature-map reuse by adding long-range connections\nand accommodate various network architectures/datasets. Compared to densely\nconnected networks (e.g., DenseNets), SWNets require a substantially fewer\nnumber of training parameters while maintaining a similar level of\nclassification accuracy. We evaluate our networks on various DL model\narchitectures and image classification datasets, namely, CIFAR10, CIFAR100, and\nILSVRC (ImageNet). Our experiments demonstrate an average of ~2.1x improvement\nin convergence speed to the desired accuracy\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 18:41:26 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Javaheripi", "Mojan", ""], ["Rouhani", "Bita Darvish", ""], ["Koushanfar", "Farinaz", ""]]}, {"id": "1904.04912", "submitter": "Bryan Lim", "authors": "Bryan Lim, Stefan Zohren, Stephen Roberts", "title": "Enhancing Time Series Momentum Strategies Using Deep Neural Networks", "comments": null, "journal-ref": "The Journal of Financial Data Science, Fall 2019", "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While time series momentum is a well-studied phenomenon in finance, common\nstrategies require the explicit definition of both a trend estimator and a\nposition sizing rule. In this paper, we introduce Deep Momentum Networks -- a\nhybrid approach which injects deep learning based trading rules into the\nvolatility scaling framework of time series momentum. The model also\nsimultaneously learns both trend estimation and position sizing in a\ndata-driven manner, with networks directly trained by optimising the Sharpe\nratio of the signal. Backtesting on a portfolio of 88 continuous futures\ncontracts, we demonstrate that the Sharpe-optimised LSTM improved traditional\nmethods by more than two times in the absence of transactions costs, and\ncontinue outperforming when considering transaction costs up to 2-3 basis\npoints. To account for more illiquid assets, we also propose a turnover\nregularisation term which trains the network to factor in costs at run-time.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 21:06:55 GMT"}, {"version": "v2", "created": "Sun, 17 Nov 2019 21:16:45 GMT"}, {"version": "v3", "created": "Sun, 27 Sep 2020 14:23:03 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Lim", "Bryan", ""], ["Zohren", "Stefan", ""], ["Roberts", "Stephen", ""]]}, {"id": "1904.04917", "submitter": "Tal Kachman", "authors": "Tal Kachman, Michal Moshkovitz, Michal Rosen-Zvi", "title": "Novel Uncertainty Framework for Deep Learning Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Deep neural networks have become the default choice for many of the machine\nlearning tasks such as classification and regression. Dropout, a method\ncommonly used to improve the convergence of deep neural networks, generates an\nensemble of thinned networks with extensive weight sharing. Recent studies that\ndropout can be viewed as an approximate variational inference in Gaussian\nprocesses, and used as a practical tool to obtain uncertainty estimates of the\nnetwork. We propose a novel statistical mechanics based framework to dropout\nand use this framework to propose a new generic algorithm that focuses on\nestimates of the variance of the loss as measured by the ensemble of thinned\nnetworks. Our approach can be applied to a wide range of deep neural network\narchitectures and machine learning tasks. In classification, this algorithm\nallows the generation of a don't-know answer to be generated, which can\nincrease the reliability of the classifier. Empirically we demonstrate\nstate-of-the-art AUC results on publicly available benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 21:31:08 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Kachman", "Tal", ""], ["Moshkovitz", "Michal", ""], ["Rosen-Zvi", "Michal", ""]]}, {"id": "1904.04940", "submitter": "Maria Bampa", "authors": "Maria Bampa", "title": "Handling temporality of clinical events with application to Adverse Drug\n  Event detection in Electronic Health Records: A scoping review", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increased adoption of Electronic Health Records(EHRs) has brought changes\nto the way the patient care is carried out. The rich heterogeneous and temporal\ndata space stored in EHRs can be leveraged by machine learning models to\ncapture the underlying information and make clinically relevant predictions.\nThis can be exploited to support public health activities such as\npharmacovigilance and specifically mitigate the public health issue of Adverse\nDrug Events(ADEs). The aim of this article is, therefore, to investigate the\nvarious ways of handling temporal data for the purpose of detecting ADEs. Based\non a review of the existing literature, 11 articles from the last 10 years were\nchosen to be studied. According to the literature retrieved the main methods\nwere found to fall into 5 different approaches: based on temporal abstraction,\ngraph-based, learning weights and data tables containing time series of\ndifferent length. To that end, EHRs are a valuable source that has led current\nresearch to the automatic detection of ADEs. Yet there still exists a great\ndeal of challenges that concerns the exploitation of the heterogeneous, data\ntypes with temporal information included in EHRs for predicting ADEs.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 22:46:20 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Bampa", "Maria", ""]]}, {"id": "1904.04956", "submitter": "Wei Zhang", "authors": "Wei Zhang, Xiaodong Cui, Ulrich Finkler, Brian Kingsbury, George Saon,\n  David Kung, Michael Picheny", "title": "Distributed Deep Learning Strategies For Automatic Speech Recognition", "comments": "Published in ICASSP'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose and investigate a variety of distributed deep\nlearning strategies for automatic speech recognition (ASR) and evaluate them\nwith a state-of-the-art Long short-term memory (LSTM) acoustic model on the\n2000-hour Switchboard (SWB2000), which is one of the most widely used datasets\nfor ASR performance benchmark. We first investigate what are the proper\nhyper-parameters (e.g., learning rate) to enable the training with sufficiently\nlarge batch size without impairing the model accuracy. We then implement\nvarious distributed strategies, including Synchronous (SYNC), Asynchronous\nDecentralized Parallel SGD (ADPSGD) and the hybrid of the two HYBRID, to study\ntheir runtime/accuracy trade-off. We show that we can train the LSTM model\nusing ADPSGD in 14 hours with 16 NVIDIA P100 GPUs to reach a 7.6% WER on the\nHub5- 2000 Switchboard (SWB) test set and a 13.1% WER on the CallHome (CH) test\nset. Furthermore, we can train the model using HYBRID in 11.5 hours with 32\nNVIDIA V100 GPUs without loss in accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 01:00:26 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Zhang", "Wei", ""], ["Cui", "Xiaodong", ""], ["Finkler", "Ulrich", ""], ["Kingsbury", "Brian", ""], ["Saon", "George", ""], ["Kung", "David", ""], ["Picheny", "Michael", ""]]}, {"id": "1904.04973", "submitter": "Yoshiharu Sato", "authors": "Yoshiharu Sato", "title": "Model-Free Reinforcement Learning for Financial Portfolios: A Brief\n  Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial portfolio management is one of the problems that are most\nfrequently encountered in the investment industry. Nevertheless, it is not\nwidely recognized that both Kelly Criterion and Risk Parity collapse into Mean\nVariance under some conditions, which implies that a universal solution to the\nportfolio optimization problem could potentially exist. In fact, the process of\nsequential computation of optimal component weights that maximize the\nportfolio's expected return subject to a certain risk budget can be\nreformulated as a discrete-time Markov Decision Process (MDP) and hence as a\nstochastic optimal control, where the system being controlled is a portfolio\nconsisting of multiple investment components, and the control is its component\nweights. Consequently, the problem could be solved using model-free\nReinforcement Learning (RL) without knowing specific component dynamics. By\nexamining existing methods of both value-based and policy-based model-free RL\nfor the portfolio optimization problem, we identify some of the key unresolved\nquestions and difficulties facing today's portfolio managers of applying\nmodel-free RL to their investment portfolios.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 01:48:52 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 09:29:20 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Sato", "Yoshiharu", ""]]}, {"id": "1904.04990", "submitter": "Zhenxing Xu", "authors": "Zhenxing Xu, Jingyuan Chou, Xi Sheryl Zhang, Yuan Luo, Tamara Isakova,\n  Prakash Adekkanattu, Jessica S. Ancker, Guoqian Jiang, Richard C. Kiefer,\n  Jennifer A. Pacheco, Luke V. Rasmussen, Jyotishman Pathak, Fei Wang", "title": "Identifying Sub-Phenotypes of Acute Kidney Injury using Structured and\n  Unstructured Electronic Health Record Data with Memory Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acute Kidney Injury (AKI) is a common clinical syndrome characterized by the\nrapid loss of kidney excretory function, which aggravates the clinical severity\nof other diseases in a large number of hospitalized patients. Accurate early\nprediction of AKI can enable in-time interventions and treatments. However, AKI\nis highly heterogeneous, thus identification of AKI sub-phenotypes can lead to\nan improved understanding of the disease pathophysiology and development of\nmore targeted clinical interventions. This study used a memory network-based\ndeep learning approach to discover AKI sub-phenotypes using structured and\nunstructured electronic health record (EHR) data of patients before AKI\ndiagnosis. We leveraged a real world critical care EHR corpus including 37,486\nICU stays. Our approach identified three distinct sub-phenotypes: sub-phenotype\nI is with an average age of 63.03$ \\pm 17.25 $ years, and is characterized by\nmild loss of kidney excretory function (Serum Creatinine (SCr) $1.55\\pm 0.34$\nmg/dL, estimated Glomerular Filtration Rate Test (eGFR) $107.65\\pm 54.98$\nmL/min/1.73$m^2$). These patients are more likely to develop stage I AKI.\nSub-phenotype II is with average age 66.81$ \\pm 10.43 $ years, and was\ncharacterized by severe loss of kidney excretory function (SCr $1.96\\pm 0.49$\nmg/dL, eGFR $82.19\\pm 55.92$ mL/min/1.73$m^2$). These patients are more likely\nto develop stage III AKI. Sub-phenotype III is with average age 65.07$ \\pm\n11.32 $ years, and was characterized moderate loss of kidney excretory function\nand thus more likely to develop stage II AKI (SCr $1.69\\pm 0.32$ mg/dL, eGFR\n$93.97\\pm 56.53$ mL/min/1.73$m^2$). Both SCr and eGFR are significantly\ndifferent across the three sub-phenotypes with statistical testing plus postdoc\nanalysis, and the conclusion still holds after age adjustment.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 03:22:34 GMT"}, {"version": "v2", "created": "Sun, 22 Dec 2019 17:00:52 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Xu", "Zhenxing", ""], ["Chou", "Jingyuan", ""], ["Zhang", "Xi Sheryl", ""], ["Luo", "Yuan", ""], ["Isakova", "Tamara", ""], ["Adekkanattu", "Prakash", ""], ["Ancker", "Jessica S.", ""], ["Jiang", "Guoqian", ""], ["Kiefer", "Richard C.", ""], ["Pacheco", "Jennifer A.", ""], ["Rasmussen", "Luke V.", ""], ["Pathak", "Jyotishman", ""], ["Wang", "Fei", ""]]}, {"id": "1904.04994", "submitter": "Mert Ozer", "authors": "Mert Ozer, Anna Sapienza, Andr\\'es Abeliuk, Goran Muric, Emilio\n  Ferrara", "title": "Discovering patterns of online popularity from time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How is popularity gained online? Is being successful strictly related to\nrapidly becoming viral in an online platform or is it possible to acquire\npopularity in a steady and disciplined fashion? What are other temporal\ncharacteristics that can unveil the popularity of online content? To answer\nthese questions, we leverage a multi-faceted temporal analysis of the evolution\nof popular online contents. Here, we present dipm-SC: a multi-dimensional\nshape-based time-series clustering algorithm with a heuristic to find the\noptimal number of clusters. First, we validate the accuracy of our algorithm on\nsynthetic datasets generated from benchmark time series models. Second, we show\nthat dipm-SC can uncover meaningful clusters of popularity behaviors in a\nreal-world Twitter dataset. By clustering the multidimensional time-series of\nthe popularity of contents coupled with other domain-specific dimensions, we\nuncover two main patterns of popularity: bursty and steady temporal behaviors.\nMoreover, we find that the way popularity is gained over time has no\nsignificant impact on the final cumulative popularity.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 03:47:49 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Ozer", "Mert", ""], ["Sapienza", "Anna", ""], ["Abeliuk", "Andr\u00e9s", ""], ["Muric", "Goran", ""], ["Ferrara", "Emilio", ""]]}, {"id": "1904.05052", "submitter": "Enrique Fernandez-Blanco", "authors": "Carlos Fernandez-Lozano, Ruben F. Cuinas, Jose A. Seoane, Enrique\n  Fernandez-Blanco, Julian Dorado, Cristian R. Munteanu", "title": "Classification of signaling proteins based on molecular star graph\n  descriptors using Machine Learning models", "comments": "19 pages, 6 figures, 3 tables", "journal-ref": "Journal of theoretical biology 384 (2015): 50-58", "doi": "10.1016/j.jtbi.2015.07.038", "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signaling proteins are an important topic in drug development due to the\nincreased importance of finding fast, accurate and cheap methods to evaluate\nnew molecular targets involved in specific diseases. The complexity of the\nprotein structure hinders the direct association of the signaling activity with\nthe molecular structure. Therefore, the proposed solution involves the use of\nprotein star graphs for the peptide sequence information encoding into specific\ntopological indices calculated with S2SNet tool. The Quantitative\nStructure-Activity Relationship classification model obtained with Machine\nLearning techniques is able to predict new signaling peptides. The best\nclassification model is the first signaling prediction model, which is based on\neleven descriptors and it was obtained using the Support Vector Machines -\nRecursive Feature Elimination (SVM-RFE) technique with the Laplacian kernel\n(RFE-LAP) and an AUROC of 0.961. Testing a set of 3114 proteins of unknown\nfunction from the PDB database assessed the prediction performance of the\nmodel. Important signaling pathways are presented for three UniprotIDs (34\nPDBs) with a signaling prediction greater than 98.0%.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 08:22:35 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Fernandez-Lozano", "Carlos", ""], ["Cuinas", "Ruben F.", ""], ["Seoane", "Jose A.", ""], ["Fernandez-Blanco", "Enrique", ""], ["Dorado", "Julian", ""], ["Munteanu", "Cristian R.", ""]]}, {"id": "1904.05098", "submitter": "Marco Frasca", "authors": "Marco Frasca, Giuliano Grossi, Giorgio Valentini", "title": "Multitask Hopfield Networks", "comments": "16 pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multitask algorithms typically use task similarity information as a bias to\nspeed up and improve the performance of learning processes. Tasks are learned\njointly, sharing information across them, in order to construct models more\naccurate than those learned separately over single tasks. In this contribution,\nwe present the first multitask model, to our knowledge, based on Hopfield\nNetworks (HNs), named HoMTask. We show that by appropriately building a unique\nHN embedding all tasks, a more robust and effective classification model can be\nlearned. HoMTask is a transductive semi-supervised parametric HN, that\nminimizes an energy function extended to all nodes and to all tasks under\nstudy. We provide theoretical evidence that the optimal parameters\nautomatically estimated by HoMTask make coherent the model itself with the\nprior knowledge (connection weights and node labels). The convergence\nproperties of HNs are preserved, and the fixed point reached by the network\ndynamics gives rise to the prediction of unlabeled nodes. The proposed model\nimproves the classification abilities of singletask HNs on a preliminary\nbenchmark comparison, and achieves competitive performance with\nstate-of-the-art semi-supervised graph-based algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 10:25:19 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Frasca", "Marco", ""], ["Grossi", "Giuliano", ""], ["Valentini", "Giorgio", ""]]}, {"id": "1904.05100", "submitter": "Yuan Xie", "authors": "Shu Changyong and Li Peng and Xie Yuan and Qu Yanyun and Dai Longquan\n  and Ma Lizhuang", "title": "Knowledge Squeezed Adversarial Network Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep network compression has been achieved notable progress via knowledge\ndistillation, where a teacher-student learning manner is adopted by using\npredetermined loss. Recently, more focuses have been transferred to employ the\nadversarial training to minimize the discrepancy between distributions of\noutput from two networks. However, they always emphasize on result-oriented\nlearning while neglecting the scheme of process-oriented learning, leading to\nthe loss of rich information contained in the whole network pipeline. Inspired\nby the assumption that, the small network can not perfectly mimic a large one\ndue to the huge gap of network scale, we propose a knowledge transfer method,\ninvolving effective intermediate supervision, under the adversarial training\nframework to learn the student network. To achieve powerful but highly compact\nintermediate information representation, the squeezed knowledge is realized by\ntask-driven attention mechanism. Then, the transferred knowledge from teacher\nnetwork could accommodate the size of student network. As a result, the\nproposed method integrates merits from both process-oriented and\nresult-oriented learning. Extensive experimental results on three typical\nbenchmark datasets, i.e., CIFAR-10, CIFAR-100, and ImageNet, demonstrate that\nour method achieves highly superior performances against other state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 10:42:33 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 07:58:47 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Changyong", "Shu", ""], ["Peng", "Li", ""], ["Yuan", "Xie", ""], ["Yanyun", "Qu", ""], ["Longquan", "Dai", ""], ["Lizhuang", "Ma", ""]]}, {"id": "1904.05146", "submitter": "Micha\\\"el Defferrard", "authors": "Micha\\\"el Defferrard, Nathana\\\"el Perraudin, Tomasz Kacprzak, Raphael\n  Sgier", "title": "DeepSphere: towards an equivariant graph-based spherical CNN", "comments": "published at the ICLR 2019 Workshop on Representation Learning on\n  Graphs and Manifolds. arXiv admin note: text overlap with arXiv:1810.12186", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spherical data is found in many applications. By modeling the discretized\nsphere as a graph, we can accommodate non-uniformly distributed, partial, and\nchanging samplings. Moreover, graph convolutions are computationally more\nefficient than spherical convolutions. As equivariance is desired to exploit\nrotational symmetries, we discuss how to approach rotation equivariance using\nthe graph neural network introduced in Defferrard et al. (2016). Experiments\nshow good performance on rotation-invariant learning problems. Code and\nexamples are available at https://github.com/SwissDataScienceCenter/DeepSphere\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 18:01:04 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Defferrard", "Micha\u00ebl", ""], ["Perraudin", "Nathana\u00ebl", ""], ["Kacprzak", "Tomasz", ""], ["Sgier", "Raphael", ""]]}, {"id": "1904.05181", "submitter": "Linxi Jiang", "authors": "Linxi Jiang, Xingjun Ma, Shaoxiang Chen, James Bailey, Yu-Gang Jiang", "title": "Black-box Adversarial Attacks on Video Recognition Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are known for their vulnerability to adversarial\nexamples. These are examples that have undergone small, carefully crafted\nperturbations, and which can easily fool a DNN into making misclassifications\nat test time. Thus far, the field of adversarial research has mainly focused on\nimage models, under either a white-box setting, where an adversary has full\naccess to model parameters, or a black-box setting where an adversary can only\nquery the target model for probabilities or labels. Whilst several white-box\nattacks have been proposed for video models, black-box video attacks are still\nunexplored. To close this gap, we propose the first black-box video attack\nframework, called V-BAD. V-BAD utilizes tentative perturbations transferred\nfrom image models, and partition-based rectifications found by the NES on\npartitions (patches) of tentative perturbations, to obtain good adversarial\ngradient estimates with fewer queries to the target model. V-BAD is equivalent\nto estimating the projection of an adversarial gradient on a selected subspace.\nUsing three benchmark video datasets, we demonstrate that V-BAD can craft both\nuntargeted and targeted attacks to fool two state-of-the-art deep video\nrecognition models. For the targeted attack, it achieves $>$93\\% success rate\nusing only an average of $3.4 \\sim 8.4 \\times 10^4$ queries, a similar number\nof queries to state-of-the-art black-box image attacks. This is despite the\nfact that videos often have two orders of magnitude higher dimensionality than\nstatic images. We believe that V-BAD is a promising new tool to evaluate and\nimprove the robustness of video recognition models to black-box adversarial\nattacks.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 13:41:02 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2019 06:22:54 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Jiang", "Linxi", ""], ["Ma", "Xingjun", ""], ["Chen", "Shaoxiang", ""], ["Bailey", "James", ""], ["Jiang", "Yu-Gang", ""]]}, {"id": "1904.05187", "submitter": "Tamara Fernandez", "authors": "Tamara Fernandez and Nicolas Rivera", "title": "A Reproducing Kernel Hilbert Space log-rank test for the two-sample\n  problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weighted log-rank tests are arguably the most widely used tests by\npractitioners for the two-sample problem in the context of right-censored data.\nMany approaches have been considered to make weighted log-rank tests more\nrobust against a broader family of alternatives, among them, considering linear\ncombinations of weighted log-rank tests, and taking the maximum among a finite\ncollection of them. In this paper, we propose as test statistic the supremum of\na collection of (potentially infinite) weight-indexed log-rank tests where the\nindex space is the unit ball in a reproducing kernel Hilbert space (RKHS). By\nusing some desirable properties of RKHSs we provide an exact and simple\nevaluation of the test statistic and establish connections with previous tests\nin the literature. Additionally, we show that for a special family of RKHSs,\nthe proposed test is omnibus. We finalise by performing an empirical evaluation\nof the proposed methodology and show an application to a real data scenario.\nOur theoretical results are proved using techniques for double integrals with\nrespect to martingales that may be of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 13:44:36 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 18:28:43 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Fernandez", "Tamara", ""], ["Rivera", "Nicolas", ""]]}, {"id": "1904.05207", "submitter": "Arno Solin", "authors": "Arno Solin and Manon Kok", "title": "Know Your Boundaries: Constraining Gaussian Processes by Variational\n  Harmonic Features", "comments": "Appearing in Proceedings of AISTATS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs) provide a powerful framework for extrapolation,\ninterpolation, and noise removal in regression and classification. This paper\nconsiders constraining GPs to arbitrarily-shaped domains with boundary\nconditions. We solve a Fourier-like generalised harmonic feature representation\nof the GP prior in the domain of interest, which both constrains the GP and\nattains a low-rank representation that is used for speeding up inference. The\nmethod scales as $\\mathcal{O}(nm^2)$ in prediction and $\\mathcal{O}(m^3)$ in\nhyperparameter learning for regression, where $n$ is the number of data points\nand $m$ the number of features. Furthermore, we make use of the variational\napproach to allow the method to deal with non-Gaussian likelihoods. The\nexperiments cover both simulated and empirical data in which the boundary\nconditions allow for inclusion of additional physical information.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 14:24:26 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Solin", "Arno", ""], ["Kok", "Manon", ""]]}, {"id": "1904.05233", "submitter": "Alexey Romanov", "authors": "Alexey Romanov, Maria De-Arteaga, Hanna Wallach, Jennifer Chayes,\n  Christian Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnaram Kenthapadi,\n  Anna Rumshisky, Adam Tauman Kalai", "title": "What's in a Name? Reducing Bias in Bios without Access to Protected\n  Attributes", "comments": "Accepted at NAACL 2019; Best Thematic Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing body of work that proposes methods for mitigating bias in\nmachine learning systems. These methods typically rely on access to protected\nattributes such as race, gender, or age. However, this raises two significant\nchallenges: (1) protected attributes may not be available or it may not be\nlegal to use them, and (2) it is often desirable to simultaneously consider\nmultiple protected attributes, as well as their intersections. In the context\nof mitigating bias in occupation classification, we propose a method for\ndiscouraging correlation between the predicted probability of an individual's\ntrue occupation and a word embedding of their name. This method leverages the\nsocietal biases that are encoded in word embeddings, eliminating the need for\naccess to protected attributes. Crucially, it only requires access to\nindividuals' names at training time and not at deployment time. We evaluate two\nvariations of our proposed method using a large-scale dataset of online\nbiographies. We find that both variations simultaneously reduce race and gender\nbiases, with almost no reduction in the classifier's overall true positive\nrate.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 15:10:37 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Romanov", "Alexey", ""], ["De-Arteaga", "Maria", ""], ["Wallach", "Hanna", ""], ["Chayes", "Jennifer", ""], ["Borgs", "Christian", ""], ["Chouldechova", "Alexandra", ""], ["Geyik", "Sahin", ""], ["Kenthapadi", "Krishnaram", ""], ["Rumshisky", "Anna", ""], ["Kalai", "Adam Tauman", ""]]}, {"id": "1904.05254", "submitter": "Hristo Inouzhe Valdes", "authors": "Eustasio del Barrio, Hristo Inouzhe, Jean-Michel Loubes", "title": "Attraction-Repulsion clustering with applications to fairness", "comments": "29 pages, 9 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the framework of fair learning, we consider clustering methods that avoid\nor limit the influence of a set of protected attributes, $S$, (race, sex, etc)\nover the resulting clusters, with the goal of producing a {\\it fair\nclustering}. For this, we introduce perturbations to the Euclidean distance\nthat take into account $S$ in a way that resembles attraction-repulsion in\ncharged particles in Physics and results in dissimilarities with an easy\ninterpretation. Cluster analysis based on these dissimilarities penalizes\nhomogeneity of the clusters in the attributes $S$, and leads to an improvement\nin fairness. We illustrate the use of our procedures with both synthetic and\nreal data. Our procedures are implemented in an R package freely available at\nhttps://github.com/HristoInouzhe/AttractionRepulsionClustering.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 15:52:11 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 11:57:58 GMT"}, {"version": "v3", "created": "Wed, 29 Apr 2020 12:34:49 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["del Barrio", "Eustasio", ""], ["Inouzhe", "Hristo", ""], ["Loubes", "Jean-Michel", ""]]}, {"id": "1904.05263", "submitter": "Lei Wu", "authors": "Weinan E, Chao Ma, Qingcan Wang, Lei Wu", "title": "Analysis of the Gradient Descent Algorithm for a Deep Neural Network\n  Model with Skip-connections", "comments": "29 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The behavior of the gradient descent (GD) algorithm is analyzed for a deep\nneural network model with skip-connections. It is proved that in the\nover-parametrized regime, for a suitable initialization, with high probability\nGD can find a global minimum exponentially fast. Generalization error estimates\nalong the GD path are also established. As a consequence, it is shown that when\nthe target function is in the reproducing kernel Hilbert space (RKHS) with a\nkernel defined by the initialization, there exist generalizable early-stopping\nsolutions along the GD path. In addition, it is also shown that the GD path is\nuniformly close to the functions given by the related random feature model.\nConsequently, in this \"implicit regularization\" setting, the deep neural\nnetwork model deteriorates to a random feature model. Our results hold for\nneural networks of any width larger than the input dimension.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 16:05:17 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 01:22:36 GMT"}, {"version": "v3", "created": "Sun, 14 Apr 2019 17:08:40 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["E", "Weinan", ""], ["Ma", "Chao", ""], ["Wang", "Qingcan", ""], ["Wu", "Lei", ""]]}, {"id": "1904.05268", "submitter": "Iiris Sundin", "authors": "Iiris Sundin, Peter Schulam, Eero Siivola, Aki Vehtari, Suchi Saria,\n  Samuel Kaski", "title": "Active Learning for Decision-Making from Imbalanced Observational Data", "comments": "Published in Proceedings of the 36th International Conference on\n  Machine Learning (ICML) 2019. 15 pages (10 paper + 5 supplementary), 7\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning can help personalized decision support by learning models to\npredict individual treatment effects (ITE). This work studies the reliability\nof prediction-based decision-making in a task of deciding which action $a$ to\ntake for a target unit after observing its covariates $\\tilde{x}$ and predicted\noutcomes $\\hat{p}(\\tilde{y} \\mid \\tilde{x}, a)$. An example case is\npersonalized medicine and the decision of which treatment to give to a patient.\nA common problem when learning these models from observational data is\nimbalance, that is, difference in treated/control covariate distributions,\nwhich is known to increase the upper bound of the expected ITE estimation\nerror. We propose to assess the decision-making reliability by estimating the\nITE model's Type S error rate, which is the probability of the model inferring\nthe sign of the treatment effect wrong. Furthermore, we use the estimated\nreliability as a criterion for active learning, in order to collect new\n(possibly expensive) observations, instead of making a forced choice based on\nunreliable predictions. We demonstrate the effectiveness of this\ndecision-making aware active learning in two decision-making tasks: in\nsimulated data with binary outcomes and in a medical dataset with synthetic and\ncontinuous treatment outcomes.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 16:10:32 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 12:21:44 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Sundin", "Iiris", ""], ["Schulam", "Peter", ""], ["Siivola", "Eero", ""], ["Vehtari", "Aki", ""], ["Saria", "Suchi", ""], ["Kaski", "Samuel", ""]]}, {"id": "1904.05325", "submitter": "Shameem Puthiya Parambath Mr.", "authors": "Shameem A Puthiya Parambath, Nishant Vijayakumar, Sanjay Chawla", "title": "Risk Aware Ranking for Top-$k$ Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Given an incomplete ratings data over a set of users and items, the\npreference completion problem aims to estimate a personalized total preference\norder over a subset of the items. In practical settings, a ranked list of\ntop-$k$ items from the estimated preference order is recommended to the end\nuser in the decreasing order of preference for final consumption. We analyze\nthis model and observe that such a ranking model results in suboptimal\nperformance when the payoff associated with the recommended items is different.\nWe propose a novel and very efficient algorithm for the preference ranking\nconsidering the uncertainty regarding the payoffs of the items. Once the\npreference scores for the users are obtained using any preference learning\nalgorithm, we show that ranking the items using a risk seeking utility function\nresults in the best ranking performance.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 08:01:26 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 19:34:36 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Parambath", "Shameem A Puthiya", ""], ["Vijayakumar", "Nishant", ""], ["Chawla", "Sanjay", ""]]}, {"id": "1904.05329", "submitter": "Jaewon Chung", "authors": "Jaewon Chung, Benjamin D. Pedigo, Eric W. Bridgeford, Bijan K.\n  Varjavand, Hayden S. Helm, Joshua T. Vogelstein", "title": "GraSPy: Graph Statistics in Python", "comments": null, "journal-ref": "Journal of Machine Learning Research 20.158 (2019): 1-7", "doi": null, "report-no": null, "categories": "cs.SI stat.ML stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce GraSPy, a Python library devoted to statistical inference,\nmachine learning, and visualization of random graphs and graph populations.\nThis package provides flexible and easy-to-use algorithms for analyzing and\nunderstanding graphs with a scikit-learn compliant API. GraSPy can be\ndownloaded from Python Package Index (PyPi), and is released under the Apache\n2.0 open-source license. The documentation and all releases are available at\nhttps://neurodata.io/graspy.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 18:58:31 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 03:20:16 GMT"}, {"version": "v3", "created": "Wed, 14 Aug 2019 22:37:02 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Chung", "Jaewon", ""], ["Pedigo", "Benjamin D.", ""], ["Bridgeford", "Eric W.", ""], ["Varjavand", "Bijan K.", ""], ["Helm", "Hayden S.", ""], ["Vogelstein", "Joshua T.", ""]]}, {"id": "1904.05330", "submitter": "Arash Ali Amini", "authors": "Marina S. Paez, Arash A. Amini and Lizhen Lin", "title": "Hierarchical Stochastic Block Model for Community Detection in Multiplex\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiplex networks have become increasingly more prevalent in many fields,\nand have emerged as a powerful tool for modeling the complexity of real\nnetworks. There is a critical need for developing inference models for\nmultiplex networks that can take into account potential dependencies across\ndifferent layers, particularly when the aim is community detection. We add to a\nlimited literature by proposing a novel and efficient Bayesian model for\ncommunity detection in multiplex networks. A key feature of our approach is the\nability to model varying communities at different network layers. In contrast,\nmany existing models assume the same communities for all layers. Moreover, our\nmodel automatically picks up the necessary number of communities at each layer\n(as validated by real data examples). This is appealing, since deciding the\nnumber of communities is a challenging aspect of community detection, and\nespecially so in the multiplex setting, if one allows the communities to change\nacross layers. Borrowing ideas from hierarchical Bayesian modeling, we use a\nhierarchical Dirichlet prior to model community labels across layers, allowing\ndependency in their structure. Given the community labels, a stochastic block\nmodel (SBM) is assumed for each layer. We develop an efficient slice sampler\nfor sampling the posterior distribution of the community labels as well as the\nlink probabilities between communities. In doing so, we address some unique\nchallenges posed by coupling the complex likelihood of SBM with the\nhierarchical nature of the prior on the labels. An extensive empirical\nvalidation is performed on simulated and real data, demonstrating the superior\nperformance of the model over single-layer alternatives, as well as the ability\nto uncover interesting structures in real networks.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 02:01:09 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Paez", "Marina S.", ""], ["Amini", "Arash A.", ""], ["Lin", "Lizhen", ""]]}, {"id": "1904.05332", "submitter": "Guilherme Gomes", "authors": "Guilherme Gomes, Vinayak Rao, Jennifer Neville", "title": "Community detection over a heterogeneous population of non-aligned\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering and community detection with multiple graphs have typically\nfocused on aligned graphs, where there is a mapping between nodes across the\ngraphs (e.g., multi-view, multi-layer, temporal graphs). However, there are\nnumerous application areas with multiple graphs that are only partially\naligned, or even unaligned. These graphs are often drawn from the same\npopulation, with communities of potentially different sizes that exhibit\nsimilar structure. In this paper, we develop a joint stochastic blockmodel\n(Joint SBM) to estimate shared communities across sets of heterogeneous\nnon-aligned graphs. We derive an efficient spectral clustering approach to\nlearn the parameters of the joint SBM. We evaluate the model on both synthetic\nand real-world datasets and show that the joint model is able to exploit\ncross-graph information to better estimate the communities compared to learning\nseparate SBMs on each individual graph.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 13:30:05 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Gomes", "Guilherme", ""], ["Rao", "Vinayak", ""], ["Neville", "Jennifer", ""]]}, {"id": "1904.05333", "submitter": "Francesco Sanna Passino", "authors": "Francesco Sanna Passino and Nicholas A. Heard", "title": "Bayesian estimation of the latent dimension and communities in\n  stochastic blockmodels", "comments": null, "journal-ref": "Statistics and Computing 30(5), 1291-1307 (2020)", "doi": "10.1007/s11222-020-09946-6", "report-no": null, "categories": "cs.SI cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral embedding of adjacency or Laplacian matrices of undirected graphs is\na common technique for representing a network in a lower dimensional latent\nspace, with optimal theoretical guarantees. The embedding can be used to\nestimate the community structure of the network, with strong consistency\nresults in the stochastic blockmodel framework. One of the main practical\nlimitations of standard algorithms for community detection from spectral\nembeddings is that the number of communities and the latent dimension of the\nembedding must be specified in advance. In this article, a novel Bayesian model\nfor simultaneous and automatic selection of the appropriate dimension of the\nlatent space and the number of blocks is proposed. Extensions to directed and\nbipartite graphs are discussed. The model is tested on simulated and real world\nnetwork data, showing promising performance for recovering latent community\nstructure.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 08:22:15 GMT"}, {"version": "v2", "created": "Sat, 13 Apr 2019 09:55:51 GMT"}, {"version": "v3", "created": "Thu, 28 May 2020 08:54:28 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Passino", "Francesco Sanna", ""], ["Heard", "Nicholas A.", ""]]}, {"id": "1904.05335", "submitter": "Maoying Qiao", "authors": "Maoying Qiao, Jun Yu, Wei Bian, Qiang Li, Dacheng Tao", "title": "Adapting Stochastic Block Models to Power-Law Degree Distributions", "comments": "13 pages, 13 figures", "journal-ref": "IEEE Transactions on Cybernetics, 49 (2019) 626-637", "doi": "10.1109/TCYB.2017.2783325", "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic block models (SBMs) have been playing an important role in\nmodeling clusters or community structures of network data. But, it is incapable\nof handling several complex features ubiquitously exhibited in real-world\nnetworks, one of which is the power-law degree characteristic. To this end, we\npropose a new variant of SBM, termed power-law degree SBM (PLD-SBM), by\nintroducing degree decay variables to explicitly encode the varying degree\ndistribution over all nodes. With an exponential prior, it is proved that\nPLD-SBM approximately preserves the scale-free feature in real networks. In\naddition, from the inference of variational E-Step, PLD-SBM is indeed to\ncorrect the bias inherited in SBM with the introduced degree decay factors.\nFurthermore, experiments conducted on both synthetic networks and two\nreal-world datasets including Adolescent Health Data and the political blogs\nnetwork verify the effectiveness of the proposed model in terms of cluster\nprediction accuracies.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 15:49:06 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Qiao", "Maoying", ""], ["Yu", "Jun", ""], ["Bian", "Wei", ""], ["Li", "Qiang", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.05338", "submitter": "Amin Jalali", "authors": "Amin Jalali, Adel Javanmard and Maryam Fazel", "title": "New Computational and Statistical Aspects of Regularized Regression with\n  Application to Rare Feature Selection and Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior knowledge on properties of a target model often come as discrete or\ncombinatorial descriptions. This work provides a unified computational\nframework for defining norms that promote such structures. More specifically,\nwe develop associated tools for optimization involving such norms given only\nthe orthogonal projection oracle onto the non-convex set of desired models. As\nan example, we study a norm, which we term the doubly-sparse norm, for\npromoting vectors with few nonzero entries taking only a few distinct values.\nWe further discuss how the K-means algorithm can serve as the underlying\nprojection oracle in this case and how it can be efficiently represented as a\nquadratically constrained quadratic program. Our motivation for the study of\nthis norm is regularized regression in the presence of rare features which\nposes a challenge to various methods within high-dimensional statistics, and in\nmachine learning in general. The proposed estimation procedure is designed to\nperform automatic feature selection and aggregation for which we develop\nstatistical bounds. The bounds are general and offer a statistical framework\nfor norm-based regularization. The bounds rely on novel geometric quantities on\nwhich we attempt to elaborate as well.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 17:44:25 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Jalali", "Amin", ""], ["Javanmard", "Adel", ""], ["Fazel", "Maryam", ""]]}, {"id": "1904.05351", "submitter": "Yunchao He", "authors": "Yunchao He, Haitong Zhang, Yujun Wang", "title": "RawNet: Fast End-to-End Neural Vocoder", "comments": "Submitted to Interspeech 2019, Graz, Austria", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks based vocoders have recently demonstrated the powerful\nability to synthesize high quality speech. These models usually generate\nsamples by conditioning on some spectrum features, such as Mel-spectrum.\nHowever, these features are extracted by using speech analysis module including\nsome processing based on the human knowledge. In this work, we proposed RawNet,\na truly end-to-end neural vocoder, which use a coder network to learn the\nhigher representation of signal, and an autoregressive voder network to\ngenerate speech sample by sample. The coder and voder together act like an\nauto-encoder network, and could be jointly trained directly on raw waveform\nwithout any human-designed features. The experiments on the Copy-Synthesis\ntasks show that RawNet can achieve the comparative synthesized speech quality\nwith LPCNet, with a smaller model architecture and faster speech generation at\nthe inference step.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 10:25:25 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["He", "Yunchao", ""], ["Zhang", "Haitong", ""], ["Wang", "Yujun", ""]]}, {"id": "1904.05375", "submitter": "Daniel Moyer", "authors": "Daniel Moyer, Greg Ver Steeg, Chantal M. W. Tax, Paul M. Thompson", "title": "Scanner Invariant Representations for Diffusion MRI Harmonization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG eess.IV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: In the present work we describe the correction of diffusion-weighted\nMRI for site and scanner biases using a novel method based on invariant\nrepresentation.\n  Theory and Methods: Pooled imaging data from multiple sources are subject to\nvariation between the sources. Correcting for these biases has become very\nimportant as imaging studies increase in size and multi-site cases become more\ncommon. We propose learning an intermediate representation invariant to\nsite/protocol variables, a technique adapted from information theory-based\nalgorithmic fairness; by leveraging the data processing inequality, such a\nrepresentation can then be used to create an image reconstruction that is\nuninformative of its original source, yet still faithful to underlying\nstructures. To implement this, we use a deep learning method based on\nvariational auto-encoders (VAE) to construct scanner invariant encodings of the\nimaging data.\n  Results: To evaluate our method, we use training data from the 2018 MICCAI\nComputational Diffusion MRI (CDMRI) Challenge Harmonization dataset. Our\nproposed method shows improvements on independent test data relative to a\nrecently published baseline method on each subtask, mapping data from three\ndifferent scanning contexts to and from one separate target scanning context.\n  Conclusion: As imaging studies continue to grow, the use of pooled multi-site\nimaging will similarly increase. Invariant representation presents a strong\ncandidate for the harmonization of these data.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 18:10:19 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 19:11:39 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Moyer", "Daniel", ""], ["Steeg", "Greg Ver", ""], ["Tax", "Chantal M. W.", ""], ["Thompson", "Paul M.", ""]]}, {"id": "1904.05381", "submitter": "Xudong Sun", "authors": "Xudong Sun, Jiali Lin, Bernd Bischl", "title": "ReinBo: Machine Learning pipeline search and configuration with Bayesian\n  Optimization embedded Reinforcement Learning", "comments": null, "journal-ref": "ECML PKDD 2019: Machine Learning and Knowledge Discovery in\n  Databases pp 68-84", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Machine learning pipeline potentially consists of several stages of\noperations like data preprocessing, feature engineering and machine learning\nmodel training. Each operation has a set of hyper-parameters, which can become\nirrelevant for the pipeline when the operation is not selected. This gives rise\nto a hierarchical conditional hyper-parameter space. To optimize this mixed\ncontinuous and discrete conditional hierarchical hyper-parameter space, we\npropose an efficient pipeline search and configuration algorithm which combines\nthe power of Reinforcement Learning and Bayesian Optimization. Empirical\nresults show that our method performs favorably compared to state of the art\nmethods like Auto-sklearn , TPOT, Tree Parzen Window, and Random Search.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 18:26:16 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Sun", "Xudong", ""], ["Lin", "Jiali", ""], ["Bischl", "Bernd", ""]]}, {"id": "1904.05391", "submitter": "Mohamed Akrout", "authors": "Mohamed Akrout, Collin Wilson, Peter C. Humphreys, Timothy Lillicrap,\n  Douglas Tweed", "title": "Deep Learning without Weight Transport", "comments": "Accepted for the Conference on Neural Information Processing Systems\n  (NeurIPS) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current algorithms for deep learning probably cannot run in the brain because\nthey rely on weight transport, where forward-path neurons transmit their\nsynaptic weights to a feedback path, in a way that is likely impossible\nbiologically. An algorithm called feedback alignment achieves deep learning\nwithout weight transport by using random feedback weights, but it performs\npoorly on hard visual-recognition tasks. Here we describe two mechanisms - a\nneural circuit called a weight mirror and a modification of an algorithm\nproposed by Kolen and Pollack in 1994 - both of which let the feedback path\nlearn appropriate synaptic weights quickly and accurately even in large\nnetworks, without weight transport or complex wiring.Tested on the ImageNet\nvisual-recognition task, these mechanisms outperform both feedback alignment\nand the newer sign-symmetry method, and nearly match backprop, the standard\nalgorithm of deep learning, which uses weight transport.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 18:55:59 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 19:54:05 GMT"}, {"version": "v3", "created": "Fri, 31 May 2019 18:54:57 GMT"}, {"version": "v4", "created": "Sun, 27 Oct 2019 22:30:52 GMT"}, {"version": "v5", "created": "Thu, 9 Jan 2020 21:36:57 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Akrout", "Mohamed", ""], ["Wilson", "Collin", ""], ["Humphreys", "Peter C.", ""], ["Lillicrap", "Timothy", ""], ["Tweed", "Douglas", ""]]}, {"id": "1904.05394", "submitter": "Marco Huber", "authors": "Nina Schaaf, Marco F. Huber, and Johannes Maucher", "title": "Enhancing Decision Tree based Interpretation of Deep Neural Networks\n  through L1-Orthogonal Regularization", "comments": "8 pages, 18th IEEE International Conference on Machine Learning and\n  Applications (ICMLA) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One obstacle that so far prevents the introduction of machine learning models\nprimarily in critical areas is the lack of explainability. In this work, a\npracticable approach of gaining explainability of deep artificial neural\nnetworks (NN) using an interpretable surrogate model based on decision trees is\npresented. Simply fitting a decision tree to a trained NN usually leads to\nunsatisfactory results in terms of accuracy and fidelity. Using L1-orthogonal\nregularization during training, however, preserves the accuracy of the NN,\nwhile it can be closely approximated by small decision trees. Tests with\ndifferent data sets confirm that L1-orthogonal regularization yields models of\nlower complexity and at the same time higher fidelity compared to other\nregularizers.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 19:11:47 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 19:57:24 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Schaaf", "Nina", ""], ["Huber", "Marco F.", ""], ["Maucher", "Johannes", ""]]}, {"id": "1904.05411", "submitter": "Ilia Sucholutsky", "authors": "Ilia Sucholutsky, Apurva Narayan, Matthias Schonlau, Sebastian\n  Fischmeister", "title": "Deep Learning for System Trace Restoration", "comments": "Pre-print (accepted to IJCNN 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most real-world datasets, and particularly those collected from physical\nsystems, are full of noise, packet loss, and other imperfections. However, most\nspecification mining, anomaly detection and other such algorithms assume, or\neven require, perfect data quality to function properly. Such algorithms may\nwork in lab conditions when given clean, controlled data, but will fail in the\nfield when given imperfect data. We propose a method for accurately\nreconstructing discrete temporal or sequential system traces affected by data\nloss, using Long Short-Term Memory Networks (LSTMs). The model works by\nlearning to predict the next event in a sequence of events, and uses its own\noutput as an input to continue predicting future events. As a result, this\nmethod can be used for data restoration even with streamed data. Such a method\ncan reconstruct even long sequence of missing events, and can also help\nvalidate and improve data quality for noisy data. The output of the model will\nbe a close reconstruction of the true data, and can be fed to algorithms that\nrely on clean data. We demonstrate our method by reconstructing automotive CAN\ntraces consisting of long sequences of discrete events. We show that given even\nsmall parts of a CAN trace, our LSTM model can predict future events with an\naccuracy of almost 90%, and can successfully reconstruct large portions of the\noriginal trace, greatly outperforming a Markov Model benchmark. We separately\nfeed the original, lossy, and reconstructed traces into a specification mining\nframework to perform downstream analysis of the effect of our method on\nstate-of-the-art models that use these traces for understanding the behavior of\ncomplex systems.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 19:53:20 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Sucholutsky", "Ilia", ""], ["Narayan", "Apurva", ""], ["Schonlau", "Matthias", ""], ["Fischmeister", "Sebastian", ""]]}, {"id": "1904.05417", "submitter": "Leah Bar", "authors": "Leah Bar and Nir Sochen", "title": "Unsupervised Deep Learning Algorithm for PDE-based Forward and Inverse\n  Problems", "comments": "15 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a neural network-based algorithm for solving forward and inverse\nproblems for partial differential equations in unsupervised fashion. The\nsolution is approximated by a deep neural network which is the minimizer of a\ncost function, and satisfies the PDE, boundary conditions, and additional\nregularizations. The method is mesh free and can be easily applied to an\narbitrary regular domain. We focus on 2D second order elliptical system with\nnon-constant coefficients, with application to Electrical Impedance Tomography.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 20:01:48 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Bar", "Leah", ""], ["Sochen", "Nir", ""]]}, {"id": "1904.05419", "submitter": "Will Epperson", "authors": "\\'Angel Alexander Cabrera, Will Epperson, Fred Hohman, Minsuk Kahng,\n  Jamie Morgenstern, Duen Horng Chau", "title": "FairVis: Visual Analytics for Discovering Intersectional Bias in Machine\n  Learning", "comments": "Accepted as a VAST conference paper to IEEE VIS'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing capability and accessibility of machine learning has led to its\napplication to many real-world domains and data about people. Despite the\nbenefits algorithmic systems may bring, models can reflect, inject, or\nexacerbate implicit and explicit societal biases into their outputs,\ndisadvantaging certain demographic subgroups. Discovering which biases a\nmachine learning model has introduced is a great challenge, due to the numerous\ndefinitions of fairness and the large number of potentially impacted subgroups.\nWe present FairVis, a mixed-initiative visual analytics system that integrates\na novel subgroup discovery technique for users to audit the fairness of machine\nlearning models. Through FairVis, users can apply domain knowledge to generate\nand investigate known subgroups, and explore suggested and similar subgroups.\nFairVis' coordinated views enable users to explore a high-level overview of\nsubgroup performance and subsequently drill down into detailed investigation of\nspecific subgroups. We show how FairVis helps to discover biases in two real\ndatasets used in predicting income and recidivism. As a visual analytics system\ndevoted to discovering bias in machine learning, FairVis demonstrates how\ninteractive visualization may help data scientists and the general public\nunderstand and create more equitable algorithmic systems.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 20:07:35 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 01:34:48 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2019 20:30:36 GMT"}, {"version": "v4", "created": "Sun, 1 Sep 2019 19:54:28 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Cabrera", "\u00c1ngel Alexander", ""], ["Epperson", "Will", ""], ["Hohman", "Fred", ""], ["Kahng", "Minsuk", ""], ["Morgenstern", "Jamie", ""], ["Chau", "Duen Horng", ""]]}, {"id": "1904.05421", "submitter": "Romain Aza\\\"is", "authors": "Romain Aza\\\"is and Florian Ingels", "title": "The Weight Function in the Subtree Kernel is Decisive", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree data are ubiquitous because they model a large variety of situations,\ne.g., the architecture of plants, the secondary structure of RNA, or the\nhierarchy of XML files. Nevertheless, the analysis of these non-Euclidean data\nis difficult per se. In this paper, we focus on the subtree kernel that is a\nconvolution kernel for tree data introduced by Vishwanathan and Smola in the\nearly 2000's. More precisely, we investigate the influence of the weight\nfunction from a theoretical perspective and in real data applications. We\nestablish on a 2-classes stochastic model that the performance of the subtree\nkernel is improved when the weight of leaves vanishes, which motivates the\ndefinition of a new weight function, learned from the data and not fixed by the\nuser as usually done. To this end, we define a unified framework for computing\nthe subtree kernel from ordered or unordered trees, that is particularly\nsuitable for tuning parameters. We show through eight real data classification\nproblems the great efficiency of our approach, in particular for small\ndatasets, which also states the high importance of the weight function.\nFinally, a visualization tool of the significant features is derived.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 20:11:13 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 13:40:51 GMT"}, {"version": "v3", "created": "Tue, 25 Feb 2020 08:55:07 GMT"}, {"version": "v4", "created": "Wed, 15 Apr 2020 14:14:42 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Aza\u00efs", "Romain", ""], ["Ingels", "Florian", ""]]}, {"id": "1904.05453", "submitter": "Yifei Xu", "authors": "Yifei Xu, Jianwen Xie, Tianyang Zhao, Chris Baker, Yibiao Zhao, and\n  Ying Nian Wu", "title": "Energy-Based Continuous Inverse Optimal Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of continuous optimal control (over finite time horizon) is to\nminimize a given cost function over the sequence of continuous control\nvariables. The problem of continuous inverse optimal control is to learn the\nunknown cost function from expert demonstrations. In this article, we study\nthis fundamental problem in the framework of energy-based model, where the\nobserved expert trajectories are assumed to be random samples from a\nprobability density function defined as the exponential of the negative cost\nfunction up to a normalizing constant. The parameters of the cost function are\nlearned by maximum likelihood via an \"analysis by synthesis\" scheme, which\niterates the following two steps: (1) Synthesis step: sample the synthesized\ntrajectories from the current probability density using the Langevin dynamics\nvia back-propagation through time. (2) Analysis step: update the model\nparameters based on the statistical difference between the synthesized\ntrajectories and the observed trajectories. Given the fact that an efficient\noptimization algorithm is usually available for an optimal control problem, we\nalso consider a convenient approximation of the above learning method, where we\nreplace the sampling in the synthesis step by optimization. To make the\nsampling or optimization more efficient, we propose to train the energy-based\nmodel simultaneously with a top-down trajectory generator via cooperative\nlearning, where the trajectory generator is used to fast initialize the\nsampling step or optimization step of the energy-based model. We demonstrate\nthe proposed methods on autonomous driving tasks, and show that it can learn\nsuitable cost functions for optimal control.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 21:41:39 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 18:55:12 GMT"}, {"version": "v3", "created": "Wed, 22 Jan 2020 22:17:59 GMT"}, {"version": "v4", "created": "Sun, 1 Nov 2020 05:16:36 GMT"}, {"version": "v5", "created": "Fri, 22 Jan 2021 00:17:44 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Xu", "Yifei", ""], ["Xie", "Jianwen", ""], ["Zhao", "Tianyang", ""], ["Baker", "Chris", ""], ["Zhao", "Yibiao", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1904.05488", "submitter": "Sean Tao", "authors": "Sean Tao", "title": "Deep Neural Network Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current deep neural networks suffer from two problems; first, they are hard\nto interpret, and second, they suffer from overfitting. There have been many\nattempts to define interpretability in neural networks, but they typically lack\ncausality or generality. A myriad of regularization techniques have been\ndeveloped to prevent overfitting, and this has driven deep learning to become\nthe hot topic it is today; however, while most regularization techniques are\njustified empirically and even intuitively, there is not much underlying\ntheory. This paper argues that to extract the features used in neural networks\nto make decisions, it's important to look at the paths between clusters\nexisting in the hidden spaces of neural networks. These features are of\nparticular interest because they reflect the true decision making process of\nthe neural network. This analysis is then furthered to present an ensemble\nalgorithm for arbitrary neural networks which has guarantees for test accuracy.\nFinally, a discussion detailing the aforementioned guarantees is introduced and\nthe implications to neural networks, including an intuitive explanation for all\ncurrent regularization methods, are presented. The ensemble algorithm has\ngenerated state-of-the-art results for Wide-ResNets on CIFAR-10 (top 5 for all\nmodels) and has improved test accuracy for all models it has been applied to.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 00:52:47 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 20:48:02 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Tao", "Sean", ""]]}, {"id": "1904.05506", "submitter": "Sorami Hisamoto", "authors": "Sorami Hisamoto, Matt Post, Kevin Duh", "title": "Membership Inference Attacks on Sequence-to-Sequence Models: Is My Data\n  In Your Machine Translation System?", "comments": null, "journal-ref": "Tansactions of the Association for Computational Linguistics\n  (TACL) Volume 8, 2020 p.49-63", "doi": "10.1162/tacl_a_00299", "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data privacy is an important issue for \"machine learning as a service\"\nproviders. We focus on the problem of membership inference attacks: given a\ndata sample and black-box access to a model's API, determine whether the sample\nexisted in the model's training data. Our contribution is an investigation of\nthis problem in the context of sequence-to-sequence models, which are important\nin applications such as machine translation and video captioning. We define the\nmembership inference problem for sequence generation, provide an open dataset\nbased on state-of-the-art machine translation models, and report initial\nresults on whether these models leak private information against several kinds\nof membership inference attacks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 02:53:21 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 10:27:24 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Hisamoto", "Sorami", ""], ["Post", "Matt", ""], ["Duh", "Kevin", ""]]}, {"id": "1904.05510", "submitter": "Shiva Kasiviswanathan", "authors": "Shiva Prasad Kasiviswanathan and Mark Rudelson", "title": "Restricted Isometry Property under High Correlations", "comments": "30 pages, fixed minor typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrices satisfying the Restricted Isometry Property (RIP) play an important\nrole in the areas of compressed sensing and statistical learning. RIP matrices\nwith optimal parameters are mainly obtained via probabilistic arguments, as\nexplicit constructions seem hard. It is therefore interesting to ask whether a\nfixed matrix can be incorporated into a construction of restricted isometries.\nIn this paper, we construct a new broad ensemble of random matrices with\ndependent entries that satisfy the restricted isometry property. Our\nconstruction starts with a fixed (deterministic) matrix $X$ satisfying some\nsimple stable rank condition, and we show that the matrix $XR$, where $R$ is a\nrandom matrix drawn from various popular probabilistic models (including,\nsubgaussian, sparse, low-randomness, satisfying convex concentration property),\nsatisfies the RIP with high probability. These theorems have various\napplications in signal recovery, random matrix theory, dimensionality\nreduction, etc. Additionally, motivated by an application for understanding the\neffectiveness of word vector embeddings popular in natural language processing\nand machine learning applications, we investigate the RIP of the matrix\n$XR^{(l)}$ where $R^{(l)}$ is formed by taking all possible (disregarding\norder) $l$-way entrywise products of the columns of a random matrix $R$.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 03:14:48 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 17:34:19 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Kasiviswanathan", "Shiva Prasad", ""], ["Rudelson", "Mark", ""]]}, {"id": "1904.05514", "submitter": "Vishnu Naresh Boddeti", "authors": "Proteek Chandan Roy and Vishnu Naresh Boddeti", "title": "Mitigating Information Leakage in Image Representations: A Maximum\n  Entropy Approach", "comments": "Accepted for oral presentation at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image recognition systems have demonstrated tremendous progress over the past\nfew decades thanks, in part, to our ability of learning compact and robust\nrepresentations of images. As we witness the wide spread adoption of these\nsystems, it is imperative to consider the problem of unintended leakage of\ninformation from an image representation, which might compromise the privacy of\nthe data owner. This paper investigates the problem of learning an image\nrepresentation that minimizes such leakage of user information. We formulate\nthe problem as an adversarial non-zero sum game of finding a good embedding\nfunction with two competing goals: to retain as much task dependent\ndiscriminative image information as possible, while simultaneously minimizing\nthe amount of information, as measured by entropy, about other sensitive\nattributes of the user. We analyze the stability and convergence dynamics of\nthe proposed formulation using tools from non-linear systems theory and compare\nto that of the corresponding adversarial zero-sum game formulation that\noptimizes likelihood as a measure of information content. Numerical experiments\non UCI, Extended Yale B, CIFAR-10 and CIFAR-100 datasets indicate that our\nproposed approach is able to learn image representations that exhibit high task\nperformance while mitigating leakage of predefined sensitive information.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 03:34:27 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Roy", "Proteek Chandan", ""], ["Boddeti", "Vishnu Naresh", ""]]}, {"id": "1904.05526", "submitter": "Cong Ma", "authors": "Jianqing Fan, Cong Ma, Yiqiao Zhong", "title": "A Selective Overview of Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has arguably achieved tremendous success in recent years. In\nsimple words, deep learning uses the composition of many nonlinear functions to\nmodel the complex dependency between input features and labels. While neural\nnetworks have a long history, recent advances have greatly improved their\nperformance in computer vision, natural language processing, etc. From the\nstatistical and scientific perspective, it is natural to ask: What is deep\nlearning? What are the new characteristics of deep learning, compared with\nclassical methods? What are the theoretical foundations of deep learning? To\nanswer these questions, we introduce common neural network models (e.g.,\nconvolutional neural nets, recurrent neural nets, generative adversarial nets)\nand training techniques (e.g., stochastic gradient descent, dropout, batch\nnormalization) from a statistical point of view. Along the way, we highlight\nnew characteristics of deep learning (including depth and over-parametrization)\nand explain their practical and theoretical benefits. We also sample recent\nresults on theories of deep learning, many of which are only suggestive. While\na complete understanding of deep learning remains elusive, we hope that our\nperspectives and discussions serve as a stimulus for new statistical research.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 17:53:15 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 13:59:45 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Fan", "Jianqing", ""], ["Ma", "Cong", ""], ["Zhong", "Yiqiao", ""]]}, {"id": "1904.05530", "submitter": "Xiang Ren", "authors": "Woojeong Jin, Meng Qu, Xisen Jin, Xiang Ren", "title": "Recurrent Event Network: Autoregressive Structure Inference over\n  Temporal Knowledge Graphs", "comments": "15 pages, 8 figures, accepted at as full paper in EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph reasoning is a critical task in natural language processing.\nThe task becomes more challenging on temporal knowledge graphs, where each fact\nis associated with a timestamp. Most existing methods focus on reasoning at\npast timestamps and they are not able to predict facts happening in the future.\nThis paper proposes Recurrent Event Network (RE-NET), a novel autoregressive\narchitecture for predicting future interactions. The occurrence of a fact\n(event) is modeled as a probability distribution conditioned on temporal\nsequences of past knowledge graphs. Specifically, our RE-NET employs a\nrecurrent event encoder to encode past facts and uses a neighborhood aggregator\nto model the connection of facts at the same timestamp. Future facts can then\nbe inferred in a sequential manner based on the two modules. We evaluate our\nproposed method via link prediction at future times on five public datasets.\nThrough extensive experiments, we demonstrate the strength of RENET, especially\non multi-step inference over future timestamps, and achieve state-of-the-art\nperformance on all five datasets. Code and data can be found at\nhttps://github.com/INK-USC/RE-Net.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 04:45:42 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 19:06:37 GMT"}, {"version": "v3", "created": "Tue, 8 Oct 2019 03:32:40 GMT"}, {"version": "v4", "created": "Tue, 6 Oct 2020 18:40:59 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Jin", "Woojeong", ""], ["Qu", "Meng", ""], ["Jin", "Xisen", ""], ["Ren", "Xiang", ""]]}, {"id": "1904.05576", "submitter": "Galina Lavrentyeva", "authors": "Galina Lavrentyeva, Sergey Novoselov, Andzhukaev Tseren, Marina\n  Volkova, Artem Gorlanov, Alexandr Kozlov", "title": "STC Antispoofing Systems for the ASVspoof2019 Challenge", "comments": "Submitted to Interspeech 2019, Graz, Austria", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.CR cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the Speech Technology Center (STC) antispoofing systems\nsubmitted to the ASVspoof 2019 challenge. The ASVspoof2019 is the extended\nversion of the previous challenges and includes 2 evaluation conditions:\nlogical access use-case scenario with speech synthesis and voice conversion\nattack types and physical access use-case scenario with replay attacks. During\nthe challenge we developed anti-spoofing solutions for both scenarios. The\nproposed systems are implemented using deep learning approach and are based on\ndifferent types of acoustic features. We enhanced Light CNN architecture\npreviously considered by the authors for replay attacks detection and which\nperformed high spoofing detection quality during the ASVspoof2017 challenge. In\nparticular here we investigate the efficiency of angular margin based softmax\nactivation for training robust deep Light CNN classifier to solve the\nmentioned-above tasks. Submitted systems achieved EER of 1.86% in logical\naccess scenario and 0.54% in physical access scenario on the evaluation part of\nthe Challenge corpora. High performance obtained for the unknown types of\nspoofing attacks demonstrates the stability of the offered approach in both\nevaluation conditions.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 08:37:43 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Lavrentyeva", "Galina", ""], ["Novoselov", "Sergey", ""], ["Tseren", "Andzhukaev", ""], ["Volkova", "Marina", ""], ["Gorlanov", "Artem", ""], ["Kozlov", "Alexandr", ""]]}, {"id": "1904.05584", "submitter": "Jorge Balazs", "authors": "Jorge A. Balazs and Yutaka Matsuo", "title": "Gating Mechanisms for Combining Character and Word-level Word\n  Representations: An Empirical Study", "comments": "Proceedings of the 2019 Conference of the North American Chapter of\n  the Association for Computational Linguistics: Student Research Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study how different ways of combining character and\nword-level representations affect the quality of both final word and sentence\nrepresentations. We provide strong empirical evidence that modeling characters\nimproves the learned representations at the word and sentence levels, and that\ndoing so is particularly useful when representing less frequent words. We\nfurther show that a feature-wise sigmoid gating mechanism is a robust method\nfor creating representations that encode semantic similarity, as it performed\nreasonably well in several word similarity datasets. Finally, our findings\nsuggest that properly capturing semantic similarity at the word level does not\nconsistently yield improved performance in downstream sentence-level tasks. Our\ncode is available at https://github.com/jabalazs/gating\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 08:56:48 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Balazs", "Jorge A.", ""], ["Matsuo", "Yutaka", ""]]}, {"id": "1904.05626", "submitter": "Conor Durkan", "authors": "Charlie Nash, Conor Durkan", "title": "Autoregressive Energy Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural density estimators are flexible families of parametric models which\nhave seen widespread use in unsupervised machine learning in recent years.\nMaximum-likelihood training typically dictates that these models be constrained\nto specify an explicit density. However, this limitation can be overcome by\ninstead using a neural network to specify an energy function, or unnormalized\ndensity, which can subsequently be normalized to obtain a valid distribution.\nThe challenge with this approach lies in accurately estimating the normalizing\nconstant of the high-dimensional energy function. We propose the Autoregressive\nEnergy Machine, an energy-based model which simultaneously learns an\nunnormalized density and computes an importance-sampling estimate of the\nnormalizing constant for each conditional in an autoregressive decomposition.\nThe Autoregressive Energy Machine achieves state-of-the-art performance on a\nsuite of density-estimation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 11:11:01 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Nash", "Charlie", ""], ["Durkan", "Conor", ""]]}, {"id": "1904.05633", "submitter": "Charanjeet Singh", "authors": "Charanjeet, Anuj Sharma", "title": "Modified online Newton step based on element wise multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The second order method as Newton Step is a suitable technique in Online\nLearning to guarantee regret bound. The large data is a challenge in Newton\nmethod to store second order matrices as hessian. In this paper, we have\nproposed an modified online Newton step that store first and second order\nmatrices of dimension m (classes) by d (features). we have used element wise\narithmetic operation to retain matrices size same. The modified second order\nmatrix size results in faster computations. Also, the mistake rate is at par\nwith respect to popular methods in literature. The experiments outcome indicate\nthat proposed method could be helpful to handle large multi class datasets in\ncommon desktop machines using second order method as Newton step.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 11:26:41 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 03:35:55 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Charanjeet", "", ""], ["Sharma", "Anuj", ""]]}, {"id": "1904.05658", "submitter": "Minseop Park", "authors": "Minseop Park, Jungtaek Kim, Saehoon Kim, Yanbin Liu, and Seungjin Choi", "title": "MxML: Mixture of Meta-Learners for Few-Shot Classification", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A meta-model is trained on a distribution of similar tasks such that it\nlearns an algorithm that can quickly adapt to a novel task with only a handful\nof labeled examples. Most of current meta-learning methods assume that the\nmeta-training set consists of relevant tasks sampled from a single\ndistribution. In practice, however, a new task is often out of the task\ndistribution, yielding a performance degradation. One way to tackle this\nproblem is to construct an ensemble of meta-learners such that each\nmeta-learner is trained on different task distribution. In this paper we\npresent a method for constructing a mixture of meta-learners (MxML), where\nmixing parameters are determined by the weight prediction network (WPN)\noptimized to improve the few-shot classification performance. Experiments on\nvarious datasets demonstrate that MxML significantly outperforms\nstate-of-the-art meta-learners, or their naive ensemble in the case of\nout-of-distribution as well as in-distribution tasks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 12:16:19 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Park", "Minseop", ""], ["Kim", "Jungtaek", ""], ["Kim", "Saehoon", ""], ["Liu", "Yanbin", ""], ["Choi", "Seungjin", ""]]}, {"id": "1904.05661", "submitter": "Paulo Hubert", "authors": "Paulo Hubert and Linilson Padovese", "title": "A machine learning approach for underwater gas leakage detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Underwater gas reservoirs are used in many situations. In particular, Carbon\nCapture and Storage (CCS) facilities that are currently being developed intend\nto store greenhouse gases inside geological formations in the deep sea. In\nthese formations, however, the gas might percolate, leaking back to the water\nand eventually to the atmosphere. The early detection of such leaks is\ntherefore tantamount to any underwater CCS project. In this work, we propose to\nuse Passive Acoustic Monitoring (PAM) and a machine learning approach to design\nefficient detectors that can signal the presence of a leakage. We use data\nobtained from simulation experiments off the Brazilian shore, and show that the\ndetection based on classification algorithms achieve good performance. We also\npropose a smoothing strategy based on Hidden Markov Models in order to\nincorporate previous knowledge about the probabilities of leakage occurrences.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 12:27:08 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Hubert", "Paulo", ""], ["Padovese", "Linilson", ""]]}, {"id": "1904.05735", "submitter": "Ekram Hossain", "authors": "Fatima Hussain, Rasheed Hussain, Syed Ali Hassan, and Ekram Hossain", "title": "Machine Learning in IoT Security: Current Solutions and Future\n  Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The future Internet of Things (IoT) will have a deep economical, commercial\nand social impact on our lives. The participating nodes in IoT networks are\nusually resource-constrained, which makes them luring targets for cyber\nattacks. In this regard, extensive efforts have been made to address the\nsecurity and privacy issues in IoT networks primarily through traditional\ncryptographic approaches. However, the unique characteristics of IoT nodes\nrender the existing solutions insufficient to encompass the entire security\nspectrum of the IoT networks. This is, at least in part, because of the\nresource constraints, heterogeneity, massive real-time data generated by the\nIoT devices, and the extensively dynamic behavior of the networks. Therefore,\nMachine Learning (ML) and Deep Learning (DL) techniques, which are able to\nprovide embedded intelligence in the IoT devices and networks, are leveraged to\ncope with different security problems. In this paper, we systematically review\nthe security requirements, attack vectors, and the current security solutions\nfor the IoT networks. We then shed light on the gaps in these security\nsolutions that call for ML and DL approaches. We also discuss in detail the\nexisting ML and DL solutions for addressing different security problems in IoT\nnetworks. At last, based on the detailed investigation of the existing\nsolutions in the literature, we discuss the future research directions for ML-\nand DL-based IoT security.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 02:46:58 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Hussain", "Fatima", ""], ["Hussain", "Rasheed", ""], ["Hassan", "Syed Ali", ""], ["Hossain", "Ekram", ""]]}, {"id": "1904.05738", "submitter": "Farzam Fanitabasi", "authors": "Farzam Fanitabasi", "title": "TG-PSM: Tunable Greedy Packet Sequence Morphing Based on Trace\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common privacy enhancing technologies fail to effectively hide certain\nstatistical aspects of encrypted traffic, namely individual packets length,\npackets direction and, packets timing. Recent researches have shown that using\nsuch attributes, an adversary is able to extract various information from the\nencrypted traffic such as the visited website and used protocol. Such attacks\nare called traffic analysis. Proposed countermeasures attempt to change the\ndistribution of such features. however, either they fail to effectively reduce\nattacker's accuracy or do so while enforcing high bandwidth overhead and timing\ndelay. In this paper, through the use of a predefined set of clustered traces\nof websites and a greedy packet morphing algorithm, we introduce a website\nfingerprinting countermeasure called TG-PSM. Firstly, this method clusters\nwebsites based on their behavior in different phases of loading. Secondly, it\nfinds a suitable target site for any visiting website based on user indicated\nimportance degree; thus providing dynamic tunability. Thirdly, this method\nmorphs the given website to the target website using a greedy algorithm\nconsidering the distance and the resulted overhead. Our evaluations show that\nTG-PSM outperforms previous countermeasures regarding attacker accuracy\nreduction and enforced bandwidth, e.g., reducing bandwidth overhead over 40%\nwhile maintaining attacker's accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 12:29:44 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Fanitabasi", "Farzam", ""]]}, {"id": "1904.05742", "submitter": "Ju-Chieh Chou", "authors": "Ju-chieh Chou, Cheng-chieh Yeh, Hung-yi Lee", "title": "One-shot Voice Conversion by Separating Speaker and Content\n  Representations with Instance Normalization", "comments": "Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, voice conversion (VC) without parallel data has been successfully\nadapted to multi-target scenario in which a single model is trained to convert\nthe input voice to many different speakers. However, such model suffers from\nthe limitation that it can only convert the voice to the speakers in the\ntraining data, which narrows down the applicable scenario of VC. In this paper,\nwe proposed a novel one-shot VC approach which is able to perform VC by only an\nexample utterance from source and target speaker respectively, and the source\nand target speaker do not even need to be seen during training. This is\nachieved by disentangling speaker and content representations with instance\nnormalization (IN). Objective and subjective evaluation shows that our model is\nable to generate the voice similar to target speaker. In addition to the\nperformance measurement, we also demonstrate that this model is able to learn\nmeaningful speaker representations without any supervision.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 16:22:18 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 14:40:52 GMT"}, {"version": "v3", "created": "Sat, 29 Jun 2019 13:42:03 GMT"}, {"version": "v4", "created": "Thu, 22 Aug 2019 17:18:16 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Chou", "Ju-chieh", ""], ["Yeh", "Cheng-chieh", ""], ["Lee", "Hung-yi", ""]]}, {"id": "1904.05746", "submitter": "Pramit Saha", "authors": "Pramit Saha, Muhammad Abdul-Mageed, Sidney Fels", "title": "SPEAK YOUR MIND! Towards Imagined Speech Recognition With Hierarchical\n  Deep Learning", "comments": "Under review in INTERSPEECH 2019. arXiv admin note: text overlap with\n  arXiv:1904.04358", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech-related Brain Computer Interface (BCI) technologies provide effective\nvocal communication strategies for controlling devices through speech commands\ninterpreted from brain signals. In order to infer imagined speech from active\nthoughts, we propose a novel hierarchical deep learning BCI system for\nsubject-independent classification of 11 speech tokens including phonemes and\nwords. Our novel approach exploits predicted articulatory information of six\nphonological categories (e.g., nasal, bilabial) as an intermediate step for\nclassifying the phonemes and words, thereby finding discriminative signal\nresponsible for natural speech synthesis. The proposed network is composed of\nhierarchical combination of spatial and temporal CNN cascaded with a deep\nautoencoder. Our best models on the KARA database achieve an average accuracy\nof 83.42% across the six different binary phonological classification tasks,\nand 53.36% for the individual token identification task, significantly\noutperforming our baselines. Ultimately, our work suggests the possible\nexistence of a brain imagery footprint for the underlying articulatory movement\nrelated to different sounds that can be used to aid imagined speech decoding.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 21:41:54 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Saha", "Pramit", ""], ["Abdul-Mageed", "Muhammad", ""], ["Fels", "Sidney", ""]]}, {"id": "1904.05747", "submitter": "Yonghong Huang Ph.D.", "authors": "Yonghong Huang, Utkarsh Verma, Celeste Fralick, Gabriel Infante-Lopez,\n  Brajesh Kumarz, Carl Woodward", "title": "Malware Evasion Attack and Defense", "comments": "Accepted by IEEE DSN-DSML2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) classifiers are vulnerable to adversarial examples. An\nadversarial example is an input sample which is slightly modified to induce\nmisclassification in an ML classifier. In this work, we investigate white-box\nand grey-box evasion attacks to an ML-based malware detector and conduct\nperformance evaluations in a real-world setting. We compare the defense\napproaches in mitigating the attacks. We propose a framework for deploying\ngrey-box and black-box attacks to malware detection systems.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 22:31:53 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 16:55:54 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Huang", "Yonghong", ""], ["Verma", "Utkarsh", ""], ["Fralick", "Celeste", ""], ["Infante-Lopez", "Gabriel", ""], ["Kumarz", "Brajesh", ""], ["Woodward", "Carl", ""]]}, {"id": "1904.05760", "submitter": "Tinkle Chugh", "authors": "Tinkle Chugh", "title": "Scalarizing Functions in Bayesian Multiobjective Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalarizing functions have been widely used to convert a multiobjective\noptimization problem into a single objective optimization problem. However,\ntheir use in solving (computationally) expensive multi- and many-objective\noptimization problems in Bayesian multiobjective optimization is scarce.\nScalarizing functions can play a crucial role on the quality and number of\nevaluations required when doing the optimization. In this article, we study and\nreview 15 different scalarizing functions in the framework of Bayesian\nmultiobjective optimization and build Gaussian process models (as surrogates,\nmetamodels or emulators) on them. We use expected improvement as infill\ncriterion (or acquisition function) to update the models. In particular, we\ncompare different scalarizing functions and analyze their performance on\nseveral benchmark problems with different number of objectives to be optimized.\nThe review and experiments on different functions provide useful insights when\nusing and selecting a scalarizing function when using a Bayesian multiobjective\noptimization method.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 15:17:38 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Chugh", "Tinkle", ""]]}, {"id": "1904.05773", "submitter": "Kamran Kowsari", "authors": "Kamran Kowsari, Rasoul Sali, Marium N. Khan, William Adorno, S. Asad\n  Ali, Sean R. Moore, Beatrice C. Amadi, Paul Kelly, Sana Syed, Donald E. Brown", "title": "Diagnosis of Celiac Disease and Environmental Enteropathy on Biopsy\n  Images Using Color Balancing on Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Celiac Disease (CD) and Environmental Enteropathy (EE) are common causes of\nmalnutrition and adversely impact normal childhood development. CD is an\nautoimmune disorder that is prevalent worldwide and is caused by an increased\nsensitivity to gluten. Gluten exposure destructs the small intestinal\nepithelial barrier, resulting in nutrient mal-absorption and childhood\nunder-nutrition. EE also results in barrier dysfunction but is thought to be\ncaused by an increased vulnerability to infections. EE has been implicated as\nthe predominant cause of under-nutrition, oral vaccine failure, and impaired\ncognitive development in low-and-middle-income countries. Both conditions\nrequire a tissue biopsy for diagnosis, and a major challenge of interpreting\nclinical biopsy images to differentiate between these gastrointestinal diseases\nis striking histopathologic overlap between them. In the current study, we\npropose a convolutional neural network (CNN) to classify duodenal biopsy images\nfrom subjects with CD, EE, and healthy controls. We evaluated the performance\nof our proposed model using a large cohort containing 1000 biopsy images. Our\nevaluations show that the proposed model achieves an area under ROC of 0.99,\n1.00, and 0.97 for CD, EE, and healthy controls, respectively. These results\ndemonstrate the discriminative power of the proposed model in duodenal biopsies\nclassification.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 15:24:32 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 02:34:20 GMT"}, {"version": "v3", "created": "Thu, 13 Jun 2019 19:22:31 GMT"}, {"version": "v4", "created": "Fri, 4 Oct 2019 16:22:32 GMT"}, {"version": "v5", "created": "Wed, 9 Oct 2019 16:24:12 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Kowsari", "Kamran", ""], ["Sali", "Rasoul", ""], ["Khan", "Marium N.", ""], ["Adorno", "William", ""], ["Ali", "S. Asad", ""], ["Moore", "Sean R.", ""], ["Amadi", "Beatrice C.", ""], ["Kelly", "Paul", ""], ["Syed", "Sana", ""], ["Brown", "Donald E.", ""]]}, {"id": "1904.05777", "submitter": "Mirko Pieropan", "authors": "Alfredo Braunstein, Anna Paola Muntoni, Andrea Pagnani and Mirko\n  Pieropan", "title": "Compressed sensing reconstruction using Expectation Propagation", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": "10.1088/1751-8121/ab3065", "report-no": null, "categories": "stat.ML cond-mat.dis-nn cond-mat.stat-mech cs.LG physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many interesting problems in fields ranging from telecommunications to\ncomputational biology can be formalized in terms of large underdetermined\nsystems of linear equations with additional constraints or regularizers. One of\nthe most studied ones, the Compressed Sensing problem (CS), consists in finding\nthe solution with the smallest number of non-zero components of a given system\nof linear equations $\\boldsymbol y = \\mathbf{F} \\boldsymbol{w}$ for known\nmeasurement vector $\\boldsymbol{y}$ and sensing matrix $\\mathbf{F}$. Here, we\nwill address the compressed sensing problem within a Bayesian inference\nframework where the sparsity constraint is remapped into a singular prior\ndistribution (called Spike-and-Slab or Bernoulli-Gauss). Solution to the\nproblem is attempted through the computation of marginal distributions via\nExpectation Propagation (EP), an iterative computational scheme originally\ndeveloped in Statistical Physics. We will show that this strategy is\ncomparatively more accurate than the alternatives in solving instances of CS\ngenerated from statistically correlated measurement matrices. For computational\nstrategies based on the Bayesian framework such as variants of Belief\nPropagation, this is to be expected, as they implicitly rely on the hypothesis\nof statistical independence among the entries of the sensing matrix. Perhaps\nsurprisingly, the method outperforms uniformly also all the other\nstate-of-the-art methods in our tests.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 15:45:32 GMT"}, {"version": "v2", "created": "Sat, 3 Aug 2019 22:20:39 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Braunstein", "Alfredo", ""], ["Muntoni", "Anna Paola", ""], ["Pagnani", "Andrea", ""], ["Pieropan", "Mirko", ""]]}, {"id": "1904.05780", "submitter": "Shankar Kumar", "authors": "Jared Lichtarge, Chris Alberti, Shankar Kumar, Noam Shazeer, Niki\n  Parmar, Simon Tong", "title": "Corpora Generation for Grammatical Error Correction", "comments": "Accepted at NAACL 2019. arXiv admin note: text overlap with\n  arXiv:1811.01710", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grammatical Error Correction (GEC) has been recently modeled using the\nsequence-to-sequence framework. However, unlike sequence transduction problems\nsuch as machine translation, GEC suffers from the lack of plentiful parallel\ndata. We describe two approaches for generating large parallel datasets for GEC\nusing publicly available Wikipedia data. The first method extracts\nsource-target pairs from Wikipedia edit histories with minimal filtration\nheuristics, while the second method introduces noise into Wikipedia sentences\nvia round-trip translation through bridge languages. Both strategies yield\nsimilar sized parallel corpora containing around 4B tokens. We employ an\niterative decoding strategy that is tailored to the loosely supervised nature\nof our constructed corpora. We demonstrate that neural GEC models trained using\neither type of corpora give similar performance. Fine-tuning these models on\nthe Lang-8 corpus and ensembling allows us to surpass the state of the art on\nboth the CoNLL-2014 benchmark and the JFLEG task. We provide systematic\nanalysis that compares the two approaches to data generation and highlights the\neffectiveness of ensembling.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 05:47:15 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Lichtarge", "Jared", ""], ["Alberti", "Chris", ""], ["Kumar", "Shankar", ""], ["Shazeer", "Noam", ""], ["Parmar", "Niki", ""], ["Tong", "Simon", ""]]}, {"id": "1904.05801", "submitter": "Mingsheng Long", "authors": "Yuchen Zhang, Tianle Liu, Mingsheng Long, Michael I. Jordan", "title": "Bridging Theory and Algorithm for Domain Adaptation", "comments": "Proceedings of the 36th International Conference on Machine Learning,\n  Long Beach, California, PMLR 97, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of unsupervised domain adaption from\ntheoretical and algorithmic perspectives. Existing domain adaptation theories\nnaturally imply minimax optimization algorithms, which connect well with the\ndomain adaptation methods based on adversarial learning. However, several\ndisconnections still exist and form the gap between theory and algorithm. We\nextend previous theories (Mansour et al., 2009c; Ben-David et al., 2010) to\nmulticlass classification in domain adaptation, where classifiers based on the\nscoring functions and margin loss are standard choices in algorithm design. We\nintroduce Margin Disparity Discrepancy, a novel measurement with rigorous\ngeneralization bounds, tailored to the distribution comparison with the\nasymmetric margin loss, and to the minimax optimization for easier training.\nOur theory can be seamlessly transformed into an adversarial learning algorithm\nfor domain adaptation, successfully bridging the gap between theory and\nalgorithm. A series of empirical studies show that our algorithm achieves the\nstate of the art accuracies on challenging domain adaptation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 16:00:10 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 18:18:17 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Zhang", "Yuchen", ""], ["Liu", "Tianle", ""], ["Long", "Mingsheng", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1904.05811", "submitter": "Dan Busbridge", "authors": "Dan Busbridge, Dane Sherburn, Pietro Cavallo and Nils Y. Hammerla", "title": "Relational Graph Attention Networks", "comments": "10 pages + 8 pages of appendices. Layer implementation available at\n  https://github.com/Babylonpartners/rgat/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate Relational Graph Attention Networks, a class of models that\nextends non-relational graph attention mechanisms to incorporate relational\ninformation, opening up these methods to a wider variety of problems. A\nthorough evaluation of these models is performed, and comparisons are made\nagainst established benchmarks. To provide a meaningful comparison, we retrain\nRelational Graph Convolutional Networks, the spectral counterpart of Relational\nGraph Attention Networks, and evaluate them under the same conditions. We find\nthat Relational Graph Attention Networks perform worse than anticipated,\nalthough some configurations are marginally beneficial for modelling molecular\nproperties. We provide insights as to why this may be, and suggest both\nmodifications to evaluation strategies, as well as directions to investigate\nfor future work.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 16:11:36 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Busbridge", "Dan", ""], ["Sherburn", "Dane", ""], ["Cavallo", "Pietro", ""], ["Hammerla", "Nils Y.", ""]]}, {"id": "1904.05815", "submitter": "MArk Hoogendoorn", "authors": "Mark Hoogendoorn, Ward van Breda, and Jeroen Ruwaard", "title": "GP-HD: Using Genetic Programming to Generate Dynamical Systems Models\n  for Health Care", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The huge wealth of data in the health domain can be exploited to create\nmodels that predict development of health states over time. Temporal learning\nalgorithms are well suited to learn relationships between health states and\nmake predictions about their future developments. However, these algorithms:\n(1) either focus on learning one generic model for all patients, providing\ngeneral insights but often with limited predictive performance, or (2) learn\nindividualized models from which it is hard to derive generic concepts. In this\npaper, we present a middle ground, namely parameterized dynamical systems\nmodels that are generated from data using a Genetic Programming (GP) framework.\nA fitness function suitable for the health domain is exploited. An evaluation\nof the approach in the mental health domain shows that performance of the model\ngenerated by the GP is on par with a dynamical systems model developed based on\ndomain knowledge, significantly outperforms a generic Long Term Short Term\nMemory (LSTM) model and in some cases also outperforms an individualized LSTM\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 16:13:22 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Hoogendoorn", "Mark", ""], ["van Breda", "Ward", ""], ["Ruwaard", "Jeroen", ""]]}, {"id": "1904.05869", "submitter": "Oleh Rybkin", "authors": "Karl Pertsch, Oleh Rybkin, Jingyun Yang, Shenghao Zhou, Konstantinos\n  G. Derpanis, Kostas Daniilidis, Joseph Lim, Andrew Jaegle", "title": "Keyframing the Future: Keyframe Discovery for Visual Prediction and\n  Planning", "comments": "Conference on Learning for Dynamics and Control, 2020. Website:\n  https://sites.google.com/view/keyin/home", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal observations such as videos contain essential information about the\ndynamics of the underlying scene, but they are often interleaved with\ninessential, predictable details. One way of dealing with this problem is by\nfocusing on the most informative moments in a sequence. We propose a model that\nlearns to discover these important events and the times when they occur and\nuses them to represent the full sequence. We do so using a hierarchical\nKeyframe-Inpainter (KeyIn) model that first generates a video's keyframes and\nthen inpaints the rest by generating the frames at the intervening times. We\npropose a fully differentiable formulation to efficiently learn this procedure.\nWe show that KeyIn finds informative keyframes in several datasets with\ndifferent dynamics and visual properties. KeyIn outperforms other recent\nhierarchical predictive models for planning. For more details, please see the\nproject website at \\url{https://sites.google.com/view/keyin}.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 17:55:09 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 00:53:23 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Pertsch", "Karl", ""], ["Rybkin", "Oleh", ""], ["Yang", "Jingyun", ""], ["Zhou", "Shenghao", ""], ["Derpanis", "Konstantinos G.", ""], ["Daniilidis", "Kostas", ""], ["Lim", "Joseph", ""], ["Jaegle", "Andrew", ""]]}, {"id": "1904.05877", "submitter": "Yuan-Ting Hu", "authors": "Ishan Deshpande, Yuan-Ting Hu, Ruoyu Sun, Ayis Pyrros, Nasir Siddiqui,\n  Sanmi Koyejo, Zhizhen Zhao, David Forsyth, Alexander Schwing", "title": "Max-Sliced Wasserstein Distance and its use for GANs", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial nets (GANs) and variational auto-encoders have\nsignificantly improved our distribution modeling capabilities, showing promise\nfor dataset augmentation, image-to-image translation and feature learning.\nHowever, to model high-dimensional distributions, sequential training and\nstacked architectures are common, increasing the number of tunable\nhyper-parameters as well as the training time. Nonetheless, the sample\ncomplexity of the distance metrics remains one of the factors affecting GAN\ntraining. We first show that the recently proposed sliced Wasserstein distance\nhas compelling sample complexity properties when compared to the Wasserstein\ndistance. To further improve the sliced Wasserstein distance we then analyze\nits `projection complexity' and develop the max-sliced Wasserstein distance\nwhich enjoys compelling sample complexity while reducing projection complexity,\nalbeit necessitating a max estimation. We finally illustrate that the proposed\ndistance trains GANs on high-dimensional images up to a resolution of 256x256\neasily.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 17:59:57 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Deshpande", "Ishan", ""], ["Hu", "Yuan-Ting", ""], ["Sun", "Ruoyu", ""], ["Pyrros", "Ayis", ""], ["Siddiqui", "Nasir", ""], ["Koyejo", "Sanmi", ""], ["Zhao", "Zhizhen", ""], ["Forsyth", "David", ""], ["Schwing", "Alexander", ""]]}, {"id": "1904.05878", "submitter": "Iou-Jen Liu", "authors": "Iou-Jen Liu and Jian Peng and Alexander G. Schwing", "title": "Knowledge Flow: Improve Upon Your Teachers", "comments": "Accepted to ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A zoo of deep nets is available these days for almost any given task, and it\nis increasingly unclear which net to start with when addressing a new task, or\nwhich net to use as an initialization for fine-tuning a new model. To address\nthis issue, in this paper, we develop knowledge flow which moves 'knowledge'\nfrom multiple deep nets, referred to as teachers, to a new deep net model,\ncalled the student. The structure of the teachers and the student can differ\narbitrarily and they can be trained on entirely different tasks with different\noutput spaces too. Upon training with knowledge flow the student is independent\nof the teachers. We demonstrate our approach on a variety of supervised and\nreinforcement learning tasks, outperforming fine-tuning and other 'knowledge\nexchange' methods.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 17:59:57 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Liu", "Iou-Jen", ""], ["Peng", "Jian", ""], ["Schwing", "Alexander G.", ""]]}, {"id": "1904.05903", "submitter": "Duccio Pappadopulo", "authors": "Kyle Cranmer, Siavash Golkar, and Duccio Pappadopulo", "title": "Inferring the quantum density matrix with machine learning", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph hep-lat physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce two methods for estimating the density matrix for a quantum\nsystem: Quantum Maximum Likelihood and Quantum Variational Inference. In these\nmethods, we construct a variational family to model the density matrix of a\nmixed quantum state. We also introduce quantum flows, the quantum analog of\nnormalizing flows, which can be used to increase the expressivity of this\nvariational family. The eigenstates and eigenvalues of interest are then\nderived by optimizing an appropriate loss function. The approach is\nqualitatively different than traditional lattice techniques that rely on the\ntime dependence of correlation functions that summarize the lattice\nconfigurations. The resulting estimate of the density matrix can then be used\nto evaluate the expectation of an arbitrary operator, which opens the door to\nnew possibilities.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 18:00:19 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Cranmer", "Kyle", ""], ["Golkar", "Siavash", ""], ["Pappadopulo", "Duccio", ""]]}, {"id": "1904.05937", "submitter": "Bingyu Wang", "authors": "Bingyu Wang, Li Chen, Wei Sun, Kechen Qin, Kefeng Li and Hui Zhou", "title": "Ranking-Based Autoencoder for Extreme Multi-label Classification", "comments": "Accepted by NAACL-HLT 2019 as a long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme Multi-label classification (XML) is an important yet challenging\nmachine learning task, that assigns to each instance its most relevant\ncandidate labels from an extremely large label collection, where the numbers of\nlabels, features and instances could be thousands or millions. XML is more and\nmore on demand in the Internet industries, accompanied with the increasing\nbusiness scale / scope and data accumulation. The extremely large label\ncollections yield challenges such as computational complexity, inter-label\ndependency and noisy labeling. Many methods have been proposed to tackle these\nchallenges, based on different mathematical formulations. In this paper, we\npropose a deep learning XML method, with a word-vector-based self-attention,\nfollowed by a ranking-based AutoEncoder architecture. The proposed method has\nthree major advantages: 1) the autoencoder simultaneously considers the\ninter-label dependencies and the feature-label dependencies, by projecting\nlabels and features onto a common embedding space; 2) the ranking loss not only\nimproves the training efficiency and accuracy but also can be extended to\nhandle noisy labeled data; 3) the efficient attention mechanism improves\nfeature representation by highlighting feature importance. Experimental results\non benchmark datasets show the proposed method is competitive to\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 19:47:01 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Wang", "Bingyu", ""], ["Chen", "Li", ""], ["Sun", "Wei", ""], ["Qin", "Kechen", ""], ["Li", "Kefeng", ""], ["Zhou", "Hui", ""]]}, {"id": "1904.05945", "submitter": "Huy Phan", "authors": "Huy Phan and Oliver Y. Ch\\'en and Philipp Koch and Alfred Mertins and\n  Maarten De Vos", "title": "Deep Transfer Learning for Single-Channel Automatic Sleep Staging with\n  Channel Mismatch", "comments": "Accepted for 27th European Signal Processing Conference (EUSIPCO\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many sleep studies suffer from the problem of insufficient data to fully\nutilize deep neural networks as different labs use different recordings set\nups, leading to the need of training automated algorithms on rather small\ndatabases, whereas large annotated databases are around but cannot be directly\nincluded into these studies for data compensation due to channel mismatch. This\nwork presents a deep transfer learning approach to overcome the channel\nmismatch problem and transfer knowledge from a large dataset to a small cohort\nto study automatic sleep staging with single-channel input. We employ the\nstate-of-the-art SeqSleepNet and train the network in the source domain, i.e.\nthe large dataset. Afterwards, the pretrained network is finetuned in the\ntarget domain, i.e. the small cohort, to complete knowledge transfer. We study\ntwo transfer learning scenarios with slight and heavy channel mismatch between\nthe source and target domains. We also investigate whether, and if so, how\nfinetuning entirely or partially the pretrained network would affect the\nperformance of sleep staging on the target domain. Using the Montreal Archive\nof Sleep Studies (MASS) database consisting of 200 subjects as the source\ndomain and the Sleep-EDF Expanded database consisting of 20 subjects as the\ntarget domain in this study, our experimental results show significant\nperformance improvement on sleep staging achieved with the proposed deep\ntransfer learning approach. Furthermore, these results also reveal the\nessential of finetuning the feature-learning parts of the pretrained network to\nbe able to bypass the channel mismatch problem.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 20:18:02 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 12:11:41 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Phan", "Huy", ""], ["Ch\u00e9n", "Oliver Y.", ""], ["Koch", "Philipp", ""], ["Mertins", "Alfred", ""], ["De Vos", "Maarten", ""]]}, {"id": "1904.05948", "submitter": "Qingyu Zhao", "authors": "Qingyu Zhao, Ehsan Adeli, Nicolas Honnorat, Tuo Leng, Kilian M. Pohl", "title": "Variational AutoEncoder For Regression: Application to Brain Aging\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While unsupervised variational autoencoders (VAE) have become a powerful tool\nin neuroimage analysis, their application to supervised learning is\nunder-explored. We aim to close this gap by proposing a unified probabilistic\nmodel for learning the latent space of imaging data and performing supervised\nregression. Based on recent advances in learning disentangled representations,\nthe novel generative process explicitly models the conditional distribution of\nlatent representations with respect to the regression target variable.\nPerforming a variational inference procedure on this model leads to joint\nregularization between the VAE and a neural-network regressor. In predicting\nthe age of 245 subjects from their structural Magnetic Resonance (MR) images,\nour model is more accurate than state-of-the-art methods when applied to either\nregion-of-interest (ROI) measurements or raw 3D volume images. More\nimportantly, unlike simple feed-forward neural-networks, disentanglement of age\nin latent representations allows for intuitive interpretation of the structural\ndevelopmental patterns of the human brain.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 20:39:29 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 18:14:47 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Zhao", "Qingyu", ""], ["Adeli", "Ehsan", ""], ["Honnorat", "Nicolas", ""], ["Leng", "Tuo", ""], ["Pohl", "Kilian M.", ""]]}, {"id": "1904.05961", "submitter": "Hanlin Lu", "authors": "Hanlin Lu, Ming-Ju Li, Ting He, Shiqiang Wang, Vijaykrishnan Narayanan\n  and Kevin S Chan", "title": "Robust Coreset Construction for Distributed Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coreset, which is a summary of the original dataset in the form of a small\nweighted set in the same sample space, provides a promising approach to enable\nmachine learning over distributed data. Although viewed as a proxy of the\noriginal dataset, each coreset is only designed to approximate the cost\nfunction of a specific machine learning problem, and thus different coresets\nare often required to solve different machine learning problems, increasing the\ncommunication overhead. We resolve this dilemma by developing robust coreset\nconstruction algorithms that can support a variety of machine learning\nproblems. Motivated by empirical evidence that suitably-weighted k-clustering\ncenters provide a robust coreset, we harden the observation by establishing\ntheoretical conditions under which the coreset provides a guaranteed\napproximation for a broad range of machine learning problems, and developing\nboth centralized and distributed algorithms to generate coresets satisfying the\nconditions. The robustness of the proposed algorithms is verified through\nextensive experiments on diverse datasets with respect to both supervised and\nunsupervised learning problems.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 21:39:10 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 02:02:03 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2020 20:11:43 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Lu", "Hanlin", ""], ["Li", "Ming-Ju", ""], ["He", "Ting", ""], ["Wang", "Shiqiang", ""], ["Narayanan", "Vijaykrishnan", ""], ["Chan", "Kevin S", ""]]}, {"id": "1904.05981", "submitter": "Yizhe Zhu", "authors": "Soumik Pal, Yizhe Zhu", "title": "Community detection in the sparse hypergraph stochastic block model", "comments": "44 pages, 5 figures", "journal-ref": null, "doi": "10.1002/rsa.21006", "report-no": null, "categories": "math.PR cs.LG cs.SI math.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the community detection problem in sparse random hypergraphs.\nAngelini et al. (2015) conjectured the existence of a sharp threshold on model\nparameters for community detection in sparse hypergraphs generated by a\nhypergraph stochastic block model. We solve the positive part of the conjecture\nfor the case of two blocks: above the threshold, there is a spectral algorithm\nwhich asymptotically almost surely constructs a partition of the hypergraph\ncorrelated with the true partition. Our method is a generalization to random\nhypergraphs of the method developed by Massouli\\'{e} (2014) for sparse random\ngraphs.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 23:23:21 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 00:23:04 GMT"}, {"version": "v3", "created": "Sat, 25 Jul 2020 23:54:37 GMT"}, {"version": "v4", "created": "Tue, 22 Dec 2020 07:40:27 GMT"}, {"version": "v5", "created": "Mon, 22 Feb 2021 21:46:48 GMT"}, {"version": "v6", "created": "Thu, 8 Jul 2021 04:33:17 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Pal", "Soumik", ""], ["Zhu", "Yizhe", ""]]}, {"id": "1904.05985", "submitter": "Chu Wang", "authors": "Chu Wang, Lei Tang, Shujun Bian, Da Zhang, Zuohua Zhang, Yongning Wu", "title": "Reference Product Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a product of interest, we propose a search method to surface a set of\nreference products. The reference products can be used as candidates to support\ndownstream modeling tasks and business applications. The search method consists\nof product representation learning and fingerprint-type vector searching. The\nproduct catalog information is transformed into a high-quality embedding of low\ndimensions via a novel attention auto-encoder neural network, and the embedding\nis further coupled with a binary encoding vector for fast retrieval. We conduct\nextensive experiments to evaluate the proposed method, and compare it with peer\nservices to demonstrate its advantage in terms of search return rate and\nprecision.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 23:47:01 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Wang", "Chu", ""], ["Tang", "Lei", ""], ["Bian", "Shujun", ""], ["Zhang", "Da", ""], ["Zhang", "Zuohua", ""], ["Wu", "Yongning", ""]]}, {"id": "1904.06022", "submitter": "Gaurav Sahu", "authors": "Gaurav Sahu", "title": "Multimodal Speech Emotion Recognition and Ambiguity Resolution", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying emotion from speech is a non-trivial task pertaining to the\nambiguous definition of emotion itself. In this work, we adopt a\nfeature-engineering based approach to tackle the task of speech emotion\nrecognition. Formalizing our problem as a multi-class classification problem,\nwe compare the performance of two categories of models. For both, we extract\neight hand-crafted features from the audio signal. In the first approach, the\nextracted features are used to train six traditional machine learning\nclassifiers, whereas the second approach is based on deep learning wherein a\nbaseline feed-forward neural network and an LSTM-based classifier are trained\nover the same features. In order to resolve ambiguity in communication, we also\ninclude features from the text domain. We report accuracy, f-score, precision,\nand recall for the different experiment settings we evaluated our models in.\nOverall, we show that lighter machine learning based models trained over a few\nhand-crafted features are able to achieve performance comparable to the current\ndeep learning based state-of-the-art method for emotion recognition.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 03:22:13 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Sahu", "Gaurav", ""]]}, {"id": "1904.06025", "submitter": "Yeping Hu", "authors": "Yeping Hu, Alireza Nakhaei, Masayoshi Tomizuka, and Kikuo Fujimura", "title": "Interaction-aware Decision Making with Adaptive Strategies under Merging\n  Scenarios", "comments": "Best Paper Finalist of IROS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to drive safely and efficiently under merging scenarios, autonomous\nvehicles should be aware of their surroundings and make decisions by\ninteracting with other road participants. Moreover, different strategies should\nbe made when the autonomous vehicle is interacting with drivers having\ndifferent level of cooperativeness. Whether the vehicle is on the merge-lane or\nmain-lane will also influence the driving maneuvers since drivers will behave\ndifferently when they have the right-of-way than otherwise. Many traditional\nmethods have been proposed to solve decision making problems under merging\nscenarios. However, these works either are incapable of modeling complicated\ninteractions or require implementing hand-designed rules which cannot properly\nhandle the uncertainties in real-world scenarios. In this paper, we proposed an\ninteraction-aware decision making with adaptive strategies (IDAS) approach that\ncan let the autonomous vehicle negotiate the road with other drivers by\nleveraging their cooperativeness under merging scenarios. A single policy is\nlearned under the multi-agent reinforcement learning (MARL) setting via the\ncurriculum learning strategy, which enables the agent to automatically infer\nother drivers' various behaviors and make decisions strategically. A masking\nmechanism is also proposed to prevent the agent from exploring states that\nviolate common sense of human judgment and increase the learning efficiency. An\nexemplar merging scenario was used to implement and examine the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 04:01:18 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 18:00:23 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Hu", "Yeping", ""], ["Nakhaei", "Alireza", ""], ["Tomizuka", "Masayoshi", ""], ["Fujimura", "Kikuo", ""]]}, {"id": "1904.06034", "submitter": "Tomoharu Iwata", "authors": "Tomoharu Iwata, Yuki Yamanaka", "title": "Supervised Anomaly Detection based on Deep Autoregressive Density\n  Estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a supervised anomaly detection method based on neural density\nestimators, where the negative log likelihood is used for the anomaly score.\nDensity estimators have been widely used for unsupervised anomaly detection. By\nthe recent advance of deep learning, the density estimation performance has\nbeen greatly improved. However, the neural density estimators cannot exploit\nanomaly label information, which would be valuable for improving the anomaly\ndetection performance. The proposed method effectively utilizes the anomaly\nlabel information by training the neural density estimator so that the\nlikelihood of normal instances is maximized and the likelihood of anomalous\ninstances is lower than that of the normal instances. We employ an\nautoregressive model for the neural density estimator, which enables us to\ncalculate the likelihood exactly. With the experiments using 16 datasets, we\ndemonstrate that the proposed method improves the anomaly detection performance\nwith a few labeled anomalous instances, and achieves better performance than\nexisting unsupervised and supervised anomaly detection methods.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 05:03:53 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Iwata", "Tomoharu", ""], ["Yamanaka", "Yuki", ""]]}, {"id": "1904.06039", "submitter": "Guiying Huang", "authors": "Huang Victoria, Chen Gang, Fu Qiang", "title": "Effective Scheduling Function Design in SDN through Deep Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research on Software-Defined Networking (SDN) strongly promotes the\nadoption of distributed controller architectures. To achieve high network\nperformance, designing a scheduling function (SF) to properly dispatch requests\nfrom each switch to suitable controllers becomes critical. However, existing\nliterature tends to design the SF targeted at specific network settings. In\nthis paper, a reinforcement-learning-based (RL) approach is proposed with the\naim to automatically learn a general, effective, and efficient SF. In\nparticular, a new dispatching system is introduced in which the SF is\nrepresented as a neural network that determines the priority of each\ncontroller. Based on the priorities, a controller is selected using our\nproposed probability selection scheme to balance the trade-off between\nexploration and exploitation during learning. In order to train a general SF,\nwe first formulate the scheduling function design problem as an RL problem.\nThen a new training approach is developed based on a state-of-the-art deep RL\nalgorithm. Our simulation results show that our RL approach can rapidly design\n(or learn) SFs with optimal performance. Apart from that, the trained SF can\ngeneralize well and outperforms commonly used scheduling heuristics under\nvarious network settings.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 05:23:24 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Victoria", "Huang", ""], ["Gang", "Chen", ""], ["Qiang", "Fu", ""]]}, {"id": "1904.06049", "submitter": "Chun Hsien Yu", "authors": "Chun-Hsien Yu, Chun-Nan Chou, Emily Chang", "title": "Distributed Layer-Partitioned Training for Privacy-Preserved Deep\n  Learning", "comments": "accepted by IEEE MIPR'19 - short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning techniques have achieved remarkable results in many domains.\nOften, training deep learning models requires large datasets, which may require\nsensitive information to be uploaded to the cloud to accelerate training. To\nadequately protect sensitive information, we propose distributed\nlayer-partitioned training with step-wise activation functions for\nprivacy-preserving deep learning. Experimental results attest our method to be\nsimple and effective.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 06:08:57 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Yu", "Chun-Hsien", ""], ["Chou", "Chun-Nan", ""], ["Chang", "Emily", ""]]}, {"id": "1904.06064", "submitter": "Martin Brossard", "authors": "Martin Brossard (CAOR), Axel Barrau, Silv\\`ere Bonnabel (CAOR)", "title": "AI-IMU Dead-Reckoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel accurate method for dead-reckoning of\nwheeled vehicles based only on an Inertial Measurement Unit (IMU). In the\ncontext of intelligent vehicles, robust and accurate dead-reckoning based on\nthe IMU may prove useful to correlate feeds from imaging sensors, to safely\nnavigate through obstructions, or for safe emergency stops in the extreme case\nof exteroceptive sensors failure. The key components of the method are the\nKalman filter and the use of deep neural networks to dynamically adapt the\nnoise parameters of the filter. The method is tested on the KITTI odometry\ndataset, and our dead-reckoning inertial method based only on the IMU\naccurately estimates 3D position, velocity, orientation of the vehicle and\nself-calibrates the IMU biases. We achieve on average a 1.10% translational\nerror and the algorithm competes with top-ranked methods which, by contrast,\nuse LiDAR or stereo vision. We make our implementation open-source at:\nhttps://github.com/mbrossar/ai-imu-dr\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 07:02:53 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Brossard", "Martin", "", "CAOR"], ["Barrau", "Axel", "", "CAOR"], ["Bonnabel", "Silv\u00e8re", "", "CAOR"]]}, {"id": "1904.06127", "submitter": "Alastair Gregory", "authors": "Alastair Gregory, Din-Houn Lau, Alex Tessier and Pan Zhang", "title": "A streaming feature-based compression method for data from instrumented\n  infrastructure", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing amount of civil engineering applications are utilising data\nacquired from infrastructure instrumented with sensing devices. This data has\nan important role in monitoring the response of these structures to excitation,\nand evaluating structural health. In this paper we seek to monitor\npedestrian-events (such as a person walking) on a footbridge using strain and\nacceleration data. The rate of this data acquisition and the number of sensing\ndevices make the storage and analysis of this data a computational challenge.\nWe introduce a streaming method to compress the sensor data, whilst preserving\nkey patterns and features (unique to different sensor types) corresponding to\npedestrian-events. Numerical demonstrations of the methodology on data obtained\nfrom strain sensors and accelerometers on the pedestrian footbridge are\nprovided to show the trade-off between compression and accuracy during and\nin-between periods of pedestrian-events.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 09:36:40 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Gregory", "Alastair", ""], ["Lau", "Din-Houn", ""], ["Tessier", "Alex", ""], ["Zhang", "Pan", ""]]}, {"id": "1904.06145", "submitter": "Ari Heljakka", "authors": "Ari Heljakka, Arno Solin, Juho Kannala", "title": "Towards Photographic Image Manipulation with Balanced Growing of\n  Generative Autoencoders", "comments": "WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generative autoencoder that provides fast encoding, faithful\nreconstructions (eg. retaining the identity of a face), sharp\ngenerated/reconstructed samples in high resolutions, and a well-structured\nlatent space that supports semantic manipulation of the inputs. There are no\ncurrent autoencoder or GAN models that satisfactorily achieve all of these. We\nbuild on the progressively growing autoencoder model PIONEER, for which we\ncompletely alter the training dynamics based on a careful analysis of recently\nintroduced normalization schemes. We show significantly improved visual and\nquantitative results for face identity conservation in CelebAHQ. Our model\nachieves state-of-the-art disentanglement of latent space, both quantitatively\nand via realistic image attribute manipulations. On the LSUN Bedrooms dataset,\nwe improve the disentanglement performance of the vanilla PIONEER, despite\nhaving a simpler model. Overall, our results indicate that the PIONEER networks\nprovide a way towards photorealistic face manipulation.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 10:31:45 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 18:36:05 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Heljakka", "Ari", ""], ["Solin", "Arno", ""], ["Kannala", "Juho", ""]]}, {"id": "1904.06151", "submitter": "Maxim Panov", "authors": "Marina Gomtsyan, Nikita Mokrov, Maxim Panov and Yury Yanovich", "title": "Geometry-Aware Maximum Likelihood Estimation of Intrinsic Dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing approaches to intrinsic dimension estimation usually are not\nreliable when the data are nonlinearly embedded in the high dimensional space.\nIn this work, we show that the explicit accounting to geometric properties of\nunknown support leads to the polynomial correction to the standard maximum\nlikelihood estimate of intrinsic dimension for flat manifolds. The proposed\nalgorithm (GeoMLE) realizes the correction by regression of standard MLEs based\non distances to nearest neighbors for different sizes of neighborhoods.\nMoreover, the proposed approach also efficiently handles the case of nonuniform\nsampling of the manifold. We perform numerous experiments on different\nsynthetic and real-world datasets. The results show that our algorithm achieves\nstate-of-the-art performance, while also being computationally efficient and\nrobust to noise in the data.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 10:44:22 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Gomtsyan", "Marina", ""], ["Mokrov", "Nikita", ""], ["Panov", "Maxim", ""], ["Yanovich", "Yury", ""]]}, {"id": "1904.06186", "submitter": "Yatie Xiao", "authors": "Yatie Xiao, Chi-Man Pun", "title": "Generating Minimal Adversarial Perturbations with Integrated Adaptive\n  Gradients", "comments": "8 pages, 7 figures. arXiv admin note: text overlap with\n  arXiv:1902.01220 The formula in Algorithm 1 lacks important representations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are easily fooled high confidence predictions for\nadversarial samples\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 12:24:25 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 07:16:55 GMT"}, {"version": "v3", "created": "Mon, 20 May 2019 01:33:58 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Xiao", "Yatie", ""], ["Pun", "Chi-Man", ""]]}, {"id": "1904.06187", "submitter": "Jingcai Guo", "authors": "Shiheng Ma, Jingcai Guo, Song Guo, Minyi Guo", "title": "Position-Aware Convolutional Networks for Traffic Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting the future traffic flow distribution in an area is an important\nissue for traffic management in an intelligent transportation system. The key\nchallenge of traffic prediction is to capture spatial and temporal relations\nbetween future traffic flows and historical traffic due to highly dynamical\npatterns of human activities. Most existing methods explore such relations by\nfusing spatial and temporal features extracted from multi-source data. However,\nthey neglect position information which helps distinguish patterns on different\npositions. In this paper, we propose a position-aware neural network that\nintegrates data features and position information. Our approach employs the\ninception backbone network to capture rich features of traffic distribution on\nthe whole area. The novelty lies in that under the backbone network, we apply\nposition embedding technique used in neural language processing to represent\nposition information as embedding vectors which are learned during the\ntraining. With these embedding vectors, we design position-aware convolution\nwhich allows different kernels to process features of different positions.\nExtensive experiments on two real-world datasets show that our approach\noutperforms previous methods even with fewer data sources.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 12:25:14 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Ma", "Shiheng", ""], ["Guo", "Jingcai", ""], ["Guo", "Song", ""], ["Guo", "Minyi", ""]]}, {"id": "1904.06211", "submitter": "Jo\\~ao Henrique Corr\\^ea", "authors": "Jo\\~ao Henrique Corr\\^ea, Patrick Marques Ciarelli, Moises R. N.\n  Ribeiro and Rodolfo da Silva Villaca", "title": "On Machine Learning DoS Attack Identification from Cloud Computing\n  Telemetry", "comments": "Abstract submit for LANCOMM 2019\n  (http://sbrc2019.sbc.org.br/en/lancomm-student-workshop-2019/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.NI stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The detection of Denial of Service (DoS) attacks remains a challenge for the\ncloud environment, affecting a massive number of services and applications\nhosted by such virtualized infrastructures. Typically, in the literature, the\ndetection of DoS attacks is performed solely by analyzing the traffic of\npackets in the network. This work advocates for the use of telemetry from the\ncloud to detect DoS attacks using Machine Learning algorithms. Our hypothesis\nis based on richness of such native data collection services, with metrics from\nboth physical and virtual hosts. Our preliminary results demonstrate that DoS\ncan be identified accurately with k-Nearest Neighbors (kNN) and decision tree\n(CART).\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 12:41:12 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Corr\u00eaa", "Jo\u00e3o Henrique", ""], ["Ciarelli", "Patrick Marques", ""], ["Ribeiro", "Moises R. N.", ""], ["Villaca", "Rodolfo da Silva", ""]]}, {"id": "1904.06253", "submitter": "Nicolas Couellan", "authors": "Nicolas Couellan", "title": "The coupling effect of Lipschitz regularization in deep neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate robustness of deep feed-forward neural networks when input\ndata are subject to random uncertainties. More specifically, we consider\nregularization of the network by its Lipschitz constant and emphasize its role.\nWe highlight the fact that this regularization is not only a way to control the\nmagnitude of the weights but has also a coupling effect on the network weights\naccross the layers. We claim and show evidence on a dataset that this coupling\neffect brings a tradeoff between robustness and expressiveness of the network.\nThis suggests that Lipschitz regularization should be carefully implemented so\nas to maintain coupling accross layers.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 14:36:21 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Couellan", "Nicolas", ""]]}, {"id": "1904.06254", "submitter": "Jingcai Guo", "authors": "Jingcai Guo, Song Guo", "title": "AMS-SFE: Towards an Alignment of Manifold Structures via Semantic\n  Feature Expansion for Zero-shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) aims at recognizing unseen classes with knowledge\ntransferred from seen classes. This is typically achieved by exploiting a\nsemantic feature space (FS) shared by both seen and unseen classes, i.e.,\nattributes or word vectors, as the bridge. However, due to the mutually\ndisjoint of training (seen) and testing (unseen) data, existing ZSL methods\neasily and commonly suffer from the domain shift problem. To address this\nissue, we propose a novel model called AMS-SFE. It considers the Alignment of\nManifold Structures by Semantic Feature Expansion. Specifically, we build up an\nautoencoder based model to expand the semantic features and joint with an\nalignment to an embedded manifold extracted from the visual FS of data. It is\nthe first attempt to align these two FSs by way of expanding semantic features.\nExtensive experiments show the remarkable performance improvement of our model\ncompared with other existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 14:38:03 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Guo", "Jingcai", ""], ["Guo", "Song", ""]]}, {"id": "1904.06258", "submitter": "Saeed Ghoorchian", "authors": "Saeed Ghoorchian, Setareh Maghsudi", "title": "Multi-Armed Bandit for Energy-Efficient and Delay-Sensitive Edge\n  Computing in Dynamic Networks with Uncertainty", "comments": "30 pages, 7 figures", "journal-ref": null, "doi": "10.1109/TCCN.2020.3012445", "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the edge computing paradigm, mobile devices offload the computational\ntasks to an edge server by routing the required data over the wireless network.\nThe full potential of edge computing becomes realized only if a smart device\nselects the most appropriate server in terms of the latency and energy\nconsumption, among many available ones. The server selection problem is\nchallenging due to the randomness of the environment and lack of prior\ninformation about the environment. Therefore, a smart device, which\nsequentially chooses a server under uncertainty, aims to improve its decision\nbased on the historical time and energy consumption. The problem becomes more\ncomplicated in a dynamic environment, where key variables might undergo abrupt\nchanges. To deal with the aforementioned problem, we first analyze the required\ntime and energy to data transmission and processing. We then use the analysis\nto cast the problem as a budget-limited multi-armed bandit problem, where each\narm is associated with a reward and cost, with time-variant statistical\ncharacteristics. We propose a policy to solve the formulated problem and prove\na regret bound. The numerical results demonstrate the superiority of the\nproposed method compared to a number of existing solutions.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 14:46:20 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 17:53:24 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Ghoorchian", "Saeed", ""], ["Maghsudi", "Setareh", ""]]}, {"id": "1904.06260", "submitter": "Eric Benhamou", "authors": "Eric Benhamou", "title": "Similarities between policy gradient methods (PGM) in Reinforcement\n  learning (RL) and supervised learning (SL)", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) is about sequential decision making and is\ntraditionally opposed to supervised learning (SL) and unsupervised learning\n(USL). In RL, given the current state, the agent makes a decision that may\ninfluence the next state as opposed to SL (and USL) where, the next state\nremains the same, regardless of the decisions taken, either in batch or online\nlearning. Although this difference is fundamental between SL and RL, there are\nconnections that have been overlooked. In particular, we prove in this paper\nthat gradient policy method can be cast as a supervised learning problem where\ntrue label are replaced with discounted rewards. We provide a new proof of\npolicy gradient methods (PGM) that emphasizes the tight link with the cross\nentropy and supervised learning. We provide a simple experiment where we\ninterchange label and pseudo rewards. We conclude that other relationships with\nSL could be made if we modify the reward functions wisely.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 14:49:28 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 07:39:36 GMT"}, {"version": "v3", "created": "Thu, 2 May 2019 17:44:44 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Benhamou", "Eric", ""]]}, {"id": "1904.06264", "submitter": "Francesco Tonolini", "authors": "Francesco Tonolini, Jack Radford, Alex Turpin, Daniele Faccio,\n  Roderick Murray-Smith", "title": "Variational Inference for Computational Imaging Inverse Problems", "comments": "29+15 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning methods for computational imaging require uncertainty\nestimation to be reliable in real settings. While Bayesian models offer a\ncomputationally tractable way of recovering uncertainty, they need large data\nvolumes to be trained, which in imaging applications implicates prohibitively\nexpensive collections with specific imaging instruments. This paper introduces\na novel framework to train variational inference for inverse problems\nexploiting in combination few experimentally collected data, domain expertise\nand existing image data sets. In such a way, Bayesian machine learning models\ncan solve imaging inverse problems with minimal data collection efforts.\nExtensive simulated experiments show the advantages of the proposed framework.\nThe approach is then applied to two real experimental optics settings:\nholographic image reconstruction and imaging through highly scattering media.\nIn both settings, state of the art reconstructions are achieved with little\ncollection of training data.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 15:10:57 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 12:43:52 GMT"}, {"version": "v3", "created": "Fri, 21 Aug 2020 15:18:09 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Tonolini", "Francesco", ""], ["Radford", "Jack", ""], ["Turpin", "Alex", ""], ["Faccio", "Daniele", ""], ["Murray-Smith", "Roderick", ""]]}, {"id": "1904.06292", "submitter": "George Kesidis", "authors": "David J. Miller, Zhen Xiang, and George Kesidis", "title": "Adversarial Learning in Statistical Classification: A Comprehensive\n  Review of Defenses Against Attacks", "comments": null, "journal-ref": "Proceedings of the IEEE, March. 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is great potential for damage from adversarial learning (AL) attacks on\nmachine-learning based systems. In this paper, we provide a contemporary survey\nof AL, focused particularly on defenses against attacks on statistical\nclassifiers. After introducing relevant terminology and the goals and range of\npossible knowledge of both attackers and defenders, we survey recent work on\ntest-time evasion (TTE), data poisoning (DP), and reverse engineering (RE)\nattacks and particularly defenses against same. In so doing, we distinguish\nrobust classification from anomaly detection (AD), unsupervised from\nsupervised, and statistical hypothesis-based defenses from ones that do not\nhave an explicit null (no attack) hypothesis; we identify the hyperparameters a\nparticular method requires, its computational complexity, as well as the\nperformance measures on which it was evaluated and the obtained quality. We\nthen dig deeper, providing novel insights that challenge conventional AL wisdom\nand that target unresolved issues, including: 1) robust classification versus\nAD as a defense strategy; 2) the belief that attack success increases with\nattack strength, which ignores susceptibility to AD; 3) small perturbations for\ntest-time evasion attacks: a fallacy or a requirement?; 4) validity of the\nuniversal assumption that a TTE attacker knows the ground-truth class for the\nexample to be attacked; 5) black, grey, or white box attacks as the standard\nfor defense evaluation; 6) susceptibility of query-based RE to an AD defense.\nWe also discuss attacks on the privacy of training data. We then present\nbenchmark comparisons of several defenses against TTE, RE, and backdoor DP\nattacks on images. The paper concludes with a discussion of future work.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 16:05:21 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 17:15:49 GMT"}, {"version": "v3", "created": "Mon, 2 Dec 2019 22:49:28 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Miller", "David J.", ""], ["Xiang", "Zhen", ""], ["Kesidis", "George", ""]]}, {"id": "1904.06307", "submitter": "Shikui Tu", "authors": "Wenjing Huang, Shikui Tu and Lei Xu", "title": "Revisit Lmser and its further development based on convolutional layers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proposed in 1991, Least Mean Square Error Reconstruction for self-organizing\nnetwork, shortly Lmser, was a further development of the traditional\nauto-encoder (AE) by folding the architecture with respect to the central\ncoding layer and thus leading to the features of symmetric weights and neurons,\nas well as jointly supervised and unsupervised learning. However, its\nadvantages were only demonstrated in a one-hidden-layer implementation due to\nthe lack of computing resources and big data at that time. In this paper, we\nrevisit Lmser from the perspective of deep learning, develop Lmser network\nbased on multiple convolutional layers, which is more suitable for\nimage-related tasks, and confirm several Lmser functions with preliminary\ndemonstrations on image recognition, reconstruction, association recall, and so\non. Experiments demonstrate that Lmser indeed works as indicated in the\noriginal paper, and it has promising performance in various applications.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 16:26:04 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Huang", "Wenjing", ""], ["Tu", "Shikui", ""], ["Xu", "Lei", ""]]}, {"id": "1904.06309", "submitter": "Yuanhao Wang", "authors": "Yuanhao Wang, Jiachen Hu, Xiaoyu Chen, Liwei Wang", "title": "Distributed Bandit Learning: Near-Optimal Regret with Efficient\n  Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of regret minimization for distributed bandits learning,\nin which $M$ agents work collaboratively to minimize their total regret under\nthe coordination of a central server. Our goal is to design communication\nprotocols with near-optimal regret and little communication cost, which is\nmeasured by the total amount of transmitted data. For distributed multi-armed\nbandits, we propose a protocol with near-optimal regret and only $O(M\\log(MK))$\ncommunication cost, where $K$ is the number of arms. The communication cost is\nindependent of the time horizon $T$, has only logarithmic dependence on the\nnumber of arms, and matches the lower bound except for a logarithmic factor.\nFor distributed $d$-dimensional linear bandits, we propose a protocol that\nachieves near-optimal regret and has communication cost of order\n$\\tilde{O}(Md)$, which has only logarithmic dependence on $T$.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 16:32:17 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 02:05:10 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Wang", "Yuanhao", ""], ["Hu", "Jiachen", ""], ["Chen", "Xiaoyu", ""], ["Wang", "Liwei", ""]]}, {"id": "1904.06312", "submitter": "Kaleigh Clary", "authors": "Kaleigh Clary, Emma Tosch, John Foley, and David Jensen", "title": "Let's Play Again: Variability of Deep Reinforcement Learning Agents in\n  Atari Environments", "comments": "NeurIPS 2018 Critiquing and Correcting Trends Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reproducibility in reinforcement learning is challenging: uncontrolled\nstochasticity from many sources, such as the learning algorithm, the learned\npolicy, and the environment itself have led researchers to report the\nperformance of learned agents using aggregate metrics of performance over\nmultiple random seeds for a single environment. Unfortunately, there are still\npernicious sources of variability in reinforcement learning agents that make\nreporting common summary statistics an unsound metric for performance. Our\nexperiments demonstrate the variability of common agents used in the popular\nOpenAI Baselines repository. We make the case for reporting post-training agent\nperformance as a distribution, rather than a point estimate.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 16:37:52 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Clary", "Kaleigh", ""], ["Tosch", "Emma", ""], ["Foley", "John", ""], ["Jensen", "David", ""]]}, {"id": "1904.06314", "submitter": "Florent Avellaneda", "authors": "Florent Avellaneda", "title": "Learning Optimal Decision Trees from Large Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring a decision tree from a given dataset is one of the classic problems\nin machine learning. This problem consists of buildings, from a labelled\ndataset, a tree such that each node corresponds to a class and a path between\nthe tree root and a leaf corresponds to a conjunction of features to be\nsatisfied in this class. Following the principle of parsimony, we want to infer\na minimal tree consistent with the dataset. Unfortunately, inferring an optimal\ndecision tree is known to be NP-complete for several definitions of optimality.\nHence, the majority of existing approaches relies on heuristics, and as for the\nfew exact inference approaches, they do not work on large data sets. In this\npaper, we propose a novel approach for inferring a decision tree of a minimum\ndepth based on the incremental generation of Boolean formula. The experimental\nresults indicate that it scales sufficiently well and the time it takes to run\ngrows slowly with the size of dataset.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 16:44:10 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Avellaneda", "Florent", ""]]}, {"id": "1904.06316", "submitter": "Felix Opolka", "authors": "Felix L. Opolka, Aaron Solomon, C\\u{a}t\\u{a}lina Cangea, Petar\n  Veli\\v{c}kovi\\'c, Pietro Li\\`o, R Devon Hjelm", "title": "Spatio-Temporal Deep Graph Infomax", "comments": "6 pages, 2 figures, Representation Learning on Graphs and Manifolds\n  Workshop of the International Conference on Learning Representations (ICLR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal graphs such as traffic networks or gene regulatory systems\npresent challenges for the existing deep learning methods due to the complexity\nof structural changes over time. To address these issues, we introduce\nSpatio-Temporal Deep Graph Infomax (STDGI)---a fully unsupervised node\nrepresentation learning approach based on mutual information maximization that\nexploits both the temporal and spatial dynamics of the graph. Our model tackles\nthe challenging task of node-level regression by training embeddings to\nmaximize the mutual information between patches of the graph, at any given time\nstep, and between features of the central nodes of patches, in the future. We\ndemonstrate through experiments and qualitative studies that the learned\nrepresentations can successfully encode relevant information about the input\ngraph and improve the predictive performance of spatio-temporal auto-regressive\nforecasting models.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 16:47:30 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Opolka", "Felix L.", ""], ["Solomon", "Aaron", ""], ["Cangea", "C\u0103t\u0103lina", ""], ["Veli\u010dkovi\u0107", "Petar", ""], ["Li\u00f2", "Pietro", ""], ["Hjelm", "R Devon", ""]]}, {"id": "1904.06330", "submitter": "Isidro Cortes-Ciriano PhD", "authors": "Isidro Cortes-Ciriano and Andreas Bender", "title": "Reliable Prediction Errors for Deep Neural Networks Using Test-Time\n  Dropout", "comments": null, "journal-ref": null, "doi": "10.1021/acs.jcim.9b00297", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While the use of deep learning in drug discovery is gaining increasing\nattention, the lack of methods to compute reliable errors in prediction for\nNeural Networks prevents their application to guide decision making in domains\nwhere identifying unreliable predictions is essential, e.g. precision medicine.\nHere, we present a framework to compute reliable errors in prediction for\nNeural Networks using Test-Time Dropout and Conformal Prediction. Specifically,\nthe algorithm consists of training a single Neural Network using dropout, and\nthen applying it N times to both the validation and test sets, also employing\ndropout in this step. Therefore, for each instance in the validation and test\nsets an ensemble of predictions were generated. The residuals and absolute\nerrors in prediction for the validation set were then used to compute\nprediction errors for test set instances using Conformal Prediction. We show\nusing 24 bioactivity data sets from ChEMBL 23 that dropout Conformal Predictors\nare valid (i.e., the fraction of instances whose true value lies within the\npredicted interval strongly correlates with the confidence level) and\nefficient, as the predicted confidence intervals span a narrower set of values\nthan those computed with Conformal Predictors generated using Random Forest\n(RF) models. Lastly, we show in retrospective virtual screening experiments\nthat dropout and RF-based Conformal Predictors lead to comparable retrieval\nrates of active compounds. Overall, we propose a computationally efficient\nframework (as only N extra forward passes are required in addition to training\na single network) to harness Test-Time Dropout and the Conformal Prediction\nframework, and to thereby generate reliable prediction errors for deep Neural\nNetworks.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 17:18:08 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Cortes-Ciriano", "Isidro", ""], ["Bender", "Andreas", ""]]}, {"id": "1904.06366", "submitter": "Ranjan Maitra", "authors": "Yifan Zhu and Fan Dai and Ranjan Maitra", "title": "Fully Three-dimensional Radial Visualization", "comments": "7 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop methodology for three-dimensional (3D) radial visualization\n(RadViz) of multidimensional datasets. Our tool is called RadViz3D and extends\nthe classical two-dimensional (2D) RadViz that visualizes multivariate data in\nthe 2D plane by mapping every observation to a point inside the unit circle. We\nshow that distributing anchor points uniformly on the 3D unit sphere provides\nthe best visualization with minimal artificial visual correlation for data with\nuncorrelated variables. However, anchor points can be placed exactly\nequi-distant from each other only for the five Platonic solids. We provide\nequi-distant anchor points for these five settings, and approximately\nequi-distant anchor points via a Fibonacci grid for the other cases. Our\nmethodology, implemented in the R package $radviz3d$, makes fully 3D RadViz\npossible and is shown to improve clarity of this nonlinear display technique on\nsimulated and real datasets.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 21:24:39 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 02:15:07 GMT"}, {"version": "v3", "created": "Mon, 28 Jun 2021 13:59:02 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Zhu", "Yifan", ""], ["Dai", "Fan", ""], ["Maitra", "Ranjan", ""]]}, {"id": "1904.06387", "submitter": "Daniel Brown", "authors": "Daniel S. Brown, Wonjoon Goo, Prabhat Nagarajan, Scott Niekum", "title": "Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement\n  Learning from Observations", "comments": "In proceedings of Thirty-sixth International Conference on Machine\n  Learning (ICML 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A critical flaw of existing inverse reinforcement learning (IRL) methods is\ntheir inability to significantly outperform the demonstrator. This is because\nIRL typically seeks a reward function that makes the demonstrator appear\nnear-optimal, rather than inferring the underlying intentions of the\ndemonstrator that may have been poorly executed in practice. In this paper, we\nintroduce a novel reward-learning-from-observation algorithm, Trajectory-ranked\nReward EXtrapolation (T-REX), that extrapolates beyond a set of (approximately)\nranked demonstrations in order to infer high-quality reward functions from a\nset of potentially poor demonstrations. When combined with deep reinforcement\nlearning, T-REX outperforms state-of-the-art imitation learning and IRL methods\non multiple Atari and MuJoCo benchmark tasks and achieves performance that is\noften more than twice the performance of the best demonstration. We also\ndemonstrate that T-REX is robust to ranking noise and can accurately\nextrapolate intention by simply watching a learner noisily improve at a task\nover time.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 19:34:43 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 13:48:50 GMT"}, {"version": "v3", "created": "Sat, 1 Jun 2019 22:39:47 GMT"}, {"version": "v4", "created": "Tue, 18 Jun 2019 17:46:49 GMT"}, {"version": "v5", "created": "Tue, 9 Jul 2019 03:51:47 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Brown", "Daniel S.", ""], ["Goo", "Wonjoon", ""], ["Nagarajan", "Prabhat", ""], ["Niekum", "Scott", ""]]}, {"id": "1904.06395", "submitter": "Luis Lastras", "authors": "Luis A. Lastras", "title": "Information Theoretic Lower Bounds on Negative Log Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we use rate-distortion theory, a branch of information theory\ndevoted to the problem of lossy compression, to shed light on an important\nproblem in latent variable modeling of data: is there room to improve the\nmodel? One way to address this question is to find an upper bound on the\nprobability (equivalently a lower bound on the negative log likelihood) that\nthe model can assign to some data as one varies the prior and/or the likelihood\nfunction in a latent variable model. The core of our contribution is to\nformally show that the problem of optimizing priors in latent variable models\nis exactly an instance of the variational optimization problem that information\ntheorists solve when computing rate-distortion functions, and then to use this\nto derive a lower bound on negative log likelihood. Moreover, we will show that\nif changing the prior can improve the log likelihood, then there is a way to\nchange the likelihood function instead and attain the same log likelihood, and\nthus rate-distortion theory is of relevance to both optimizing priors as well\nas optimizing likelihood functions. We will experimentally argue for the\nusefulness of quantities derived from rate-distortion theory in latent variable\nmodeling by applying them to a problem in image modeling.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 20:03:33 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Lastras", "Luis A.", ""]]}, {"id": "1904.06396", "submitter": "Valentin De Bortoli", "authors": "De Bortoli Valentin, Desolneux Agn\\`es, Galerne Bruno, Leclaire Arthur", "title": "Macrocanonical Models for Texture Synthesis", "comments": "Accepted to Scale Space and Variational Methods in Computer Vision\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we consider macrocanonical models for texture synthesis. In\nthese models samples are generated given an input texture image and a set of\nfeatures which should be matched in expectation. It is known that if the images\nare quantized, macrocanonical models are given by Gibbs measures, using the\nmaximum entropy principle. We study conditions under which this result extends\nto real-valued images. If these conditions hold, finding a macrocanonical model\namounts to minimizing a convex function and sampling from an associated Gibbs\nmeasure. We analyze an algorithm which alternates between sampling and\nminimizing. We present experiments with neural network features and study the\ndrawbacks and advantages of using this sampling scheme.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 20:08:39 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Valentin", "De Bortoli", ""], ["Agn\u00e8s", "Desolneux", ""], ["Bruno", "Galerne", ""], ["Arthur", "Leclaire", ""]]}, {"id": "1904.06442", "submitter": "Qiyao Wang", "authors": "Qiyao Wang, Shuai Zheng, Ahmed Farahat, Susumu Serita, Chetan Gupta", "title": "Remaining Useful Life Estimation Using Functional Data Analysis", "comments": "Accepted by IEEE International Conference on Prognostics and Health\n  Management 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remaining Useful Life (RUL) of an equipment or one of its components is\ndefined as the time left until the equipment or component reaches its end of\nuseful life. Accurate RUL estimation is exceptionally beneficial to Predictive\nMaintenance, and Prognostics and Health Management (PHM). Data driven\napproaches which leverage the power of algorithms for RUL estimation using\nsensor and operational time series data are gaining popularity. Existing\nalgorithms, such as linear regression, Convolutional Neural Network (CNN),\nHidden Markov Models (HMMs), and Long Short-Term Memory (LSTM), have their own\nlimitations for the RUL estimation task. In this work, we propose a novel\nFunctional Data Analysis (FDA) method called functional Multilayer Perceptron\n(functional MLP) for RUL estimation. Functional MLP treats time series data\nfrom multiple equipment as a sample of random continuous processes over time.\nFDA explicitly incorporates both the correlations within the same equipment and\nthe random variations across different equipment's sensor time series into the\nmodel. FDA also has the benefit of allowing the relationship between RUL and\nsensor variables to vary over time. We implement functional MLP on the\nbenchmark NASA C-MAPSS data and evaluate the performance using two\npopularly-used metrics. Results show the superiority of our algorithm over all\nthe other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 22:48:49 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Wang", "Qiyao", ""], ["Zheng", "Shuai", ""], ["Farahat", "Ahmed", ""], ["Serita", "Susumu", ""], ["Gupta", "Chetan", ""]]}, {"id": "1904.06449", "submitter": "Ryan Rossi", "authors": "John Boaz Lee, Giang Nguyen, Ryan A. Rossi, Nesreen K. Ahmed, Eunyee\n  Koh, and Sungchul Kim", "title": "Dynamic Node Embeddings from Edge Streams", "comments": "IEEE Transactions on Emerging Topics in Computational Intelligence\n  (TETIC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks evolve continuously over time with the addition, deletion, and\nchanging of links and nodes. Such temporal networks (or edge streams) consist\nof a sequence of timestamped edges and are seemingly ubiquitous. Despite the\nimportance of accurately modeling the temporal information, most embedding\nmethods ignore it entirely or approximate the temporal network using a sequence\nof static snapshot graphs. In this work, we propose using the notion of\ntemporal walks for learning dynamic embeddings from temporal networks. Temporal\nwalks capture the temporally valid interactions (e.g., flow of information,\nspread of disease) in the dynamic network in a lossless fashion. Based on the\nnotion of temporal walks, we describe a general class of embeddings called\ncontinuous-time dynamic network embeddings (CTDNEs) that completely avoid the\nissues and problems that arise when approximating the temporal network as a\nsequence of static snapshot graphs. Unlike previous work, CTDNEs learn dynamic\nnode embeddings directly from the temporal network at the finest temporal\ngranularity and thus use only temporally valid information. As such CTDNEs\nnaturally support online learning of the node embeddings in a streaming\nreal-time fashion. Finally, the experiments demonstrate the effectiveness of\nthis class of embedding methods that leverage temporal walks as it achieves an\naverage gain in AUC of 11.9% across all methods and graphs.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 23:44:25 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 15:44:02 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Lee", "John Boaz", ""], ["Nguyen", "Giang", ""], ["Rossi", "Ryan A.", ""], ["Ahmed", "Nesreen K.", ""], ["Koh", "Eunyee", ""], ["Kim", "Sungchul", ""]]}, {"id": "1904.06491", "submitter": "Chandan Gautam", "authors": "Chandan Gautam, Aruna Tiwari, M. Tanveer", "title": "Graph-Embedded Multi-layer Kernel Extreme Learning Machine for One-class\n  Classification or (Graph-Embedded Multi-layer Kernel Ridge Regression for\n  One-class Classification)", "comments": "arXiv admin note: substantial text overlap with arXiv:1805.07808", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A brain can detect outlier just by using only normal samples. Similarly,\none-class classification (OCC) also uses only normal samples to train the model\nand trained model can be used for outlier detection. In this paper, a\nmulti-layer architecture for OCC is proposed by stacking various Graph-Embedded\nKernel Ridge Regression (KRR) based Auto-Encoders in a hierarchical fashion.\nThese Auto-Encoders are formulated under two types of Graph-Embedding, namely,\nlocal and global variance-based embedding. This Graph-Embedding explores the\nrelationship between samples and multi-layers of Auto-Encoder project the input\nfeatures into new feature space. The last layer of this proposed architecture\nis Graph-Embedded regression-based one-class classifier. The Auto-Encoders use\nan unsupervised approach of learning and the final layer uses semi-supervised\n(trained by only positive samples and obtained closed-form solution) approach\nto learning. The proposed method is experimentally evaluated on 21 publicly\navailable benchmark datasets. Experimental results verify the effectiveness of\nthe proposed one-class classifiers over 11 existing state-of-the-art\nkernel-based one-class classifiers. Friedman test is also performed to verify\nthe statistical significance of the claim of the superiority of the proposed\none-class classifiers over the existing state-of-the-art methods. By using two\ntypes of Graph-Embedding, 4 variants of Graph-Embedded multi-layer KRR-based\none-class classifier has been presented in this paper. All 4 variants performed\nbetter than the existing one-class classifiers in terms of various discussed\ncriteria in this paper. Hence, it can be a viable alternative for OCC task. In\nthe future, various other types of Auto-Encoders can be explored within\nproposed architecture.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 06:37:34 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Gautam", "Chandan", ""], ["Tiwari", "Aruna", ""], ["Tanveer", "M.", ""]]}, {"id": "1904.06501", "submitter": "Badong Chen", "authors": "Badong Chen, Xin Wang, Yingsong Li, Jose C. Principe", "title": "Maximum Correntropy Criterion with Variable Center", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": "10.1109/LSP.2019.2925692", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correntropy is a local similarity measure defined in kernel space and the\nmaximum correntropy criterion (MCC) has been successfully applied in many areas\nof signal processing and machine learning in recent years. The kernel function\nin correntropy is usually restricted to the Gaussian function with center\nlocated at zero. However, zero-mean Gaussian function may not be a good choice\nfor many practical applications. In this study, we propose an extended version\nof correntropy, whose center can locate at any position. Accordingly, we\npropose a new optimization criterion called maximum correntropy criterion with\nvariable center (MCC-VC). We also propose an efficient approach to optimize the\nkernel width and center location in MCC-VC. Simulation results of regression\nwith linear in parameters (LIP) models confirm the desirable performance of the\nnew method.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 07:49:54 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Chen", "Badong", ""], ["Wang", "Xin", ""], ["Li", "Yingsong", ""], ["Principe", "Jose C.", ""]]}, {"id": "1904.06546", "submitter": "Bowen Zhao", "authors": "Bowen Zhao, Xi Xiao, Wanpeng Zhang, Bin Zhang, Shutao Xia", "title": "Self-Paced Probabilistic Principal Component Analysis for Data with\n  Outliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal Component Analysis (PCA) is a popular tool for dimensionality\nreduction and feature extraction in data analysis. There is a probabilistic\nversion of PCA, known as Probabilistic PCA (PPCA). However, standard PCA and\nPPCA are not robust, as they are sensitive to outliers. To alleviate this\nproblem, this paper introduces the Self-Paced Learning mechanism into PPCA, and\nproposes a novel method called Self-Paced Probabilistic Principal Component\nAnalysis (SP-PPCA). Furthermore, we design the corresponding optimization\nalgorithm based on the alternative search strategy and the\nexpectation-maximization algorithm. SP-PPCA looks for optimal projection\nvectors and filters out outliers iteratively. Experiments on both synthetic\nproblems and real-world datasets clearly demonstrate that SP-PPCA is able to\nreduce or eliminate the impact of outliers.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 13:32:30 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Zhao", "Bowen", ""], ["Xiao", "Xi", ""], ["Zhang", "Wanpeng", ""], ["Zhang", "Bin", ""], ["Xia", "Shutao", ""]]}, {"id": "1904.06590", "submitter": "Eliya Nachmani", "authors": "Eliya Nachmani, Lior Wolf", "title": "Unsupervised Singing Voice Conversion", "comments": "Accepted to Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep learning method for singing voice conversion. The proposed\nnetwork is not conditioned on the text or on the notes, and it directly\nconverts the audio of one singer to the voice of another. Training is performed\nwithout any form of supervision: no lyrics or any kind of phonetic features, no\nnotes, and no matching samples between singers. The proposed network employs a\nsingle CNN encoder for all singers, a single WaveNet decoder, and a classifier\nthat enforces the latent representation to be singer-agnostic. Each singer is\nrepresented by one embedding vector, which the decoder is conditioned on. In\norder to deal with relatively small datasets, we propose a new data\naugmentation scheme, as well as new training losses and protocols that are\nbased on backtranslation. Our evaluation presents evidence that the conversion\nproduces natural signing voices that are highly recognizable as the target\nsinger.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 20:07:58 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 14:56:38 GMT"}, {"version": "v3", "created": "Wed, 25 Sep 2019 14:39:49 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Nachmani", "Eliya", ""], ["Wolf", "Lior", ""]]}, {"id": "1904.06593", "submitter": "Guoliang Kang", "authors": "Guoliang Kang, Jun Li, Dacheng Tao", "title": "Shakeout: A New Approach to Regularized Deep Neural Network Training", "comments": "Appears at T-PAMI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the success of deep neural networks in dealing\nwith a plenty of practical problems. Dropout has played an essential role in\nmany successful deep neural networks, by inducing regularization in the model\ntraining. In this paper, we present a new regularized training approach:\nShakeout. Instead of randomly discarding units as Dropout does at the training\nstage, Shakeout randomly chooses to enhance or reverse each unit's contribution\nto the next layer. This minor modification of Dropout has the statistical\ntrait: the regularizer induced by Shakeout adaptively combines $L_0$, $L_1$ and\n$L_2$ regularization terms. Our classification experiments with representative\ndeep architectures on image datasets MNIST, CIFAR-10 and ImageNet show that\nShakeout deals with over-fitting effectively and outperforms Dropout. We\nempirically demonstrate that Shakeout leads to sparser weights under both\nunsupervised and supervised settings. Shakeout also leads to the grouping\neffect of the input units in a layer. Considering the weights in reflecting the\nimportance of connections, Shakeout is superior to Dropout, which is valuable\nfor the deep model compression. Moreover, we demonstrate that Shakeout can\neffectively reduce the instability of the training process of the deep\narchitecture.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 21:38:16 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Kang", "Guoliang", ""], ["Li", "Jun", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.06618", "submitter": "Md Kamrul Hasan", "authors": "Md Kamrul Hasan, Wasifur Rahman, Amir Zadeh, Jianyuan Zhong, Md\n  Iftekhar Tanveer, Louis-Philippe Morency, Mohammed (Ehsan) Hoque", "title": "UR-FUNNY: A Multimodal Language Dataset for Understanding Humor", "comments": null, "journal-ref": "EMNLP-IJCNLP, 2019, 2046-2056", "doi": "10.18653/v1/D19-1211", "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Humor is a unique and creative communicative behavior displayed during social\ninteractions. It is produced in a multimodal manner, through the usage of words\n(text), gestures (vision) and prosodic cues (acoustic). Understanding humor\nfrom these three modalities falls within boundaries of multimodal language; a\nrecent research trend in natural language processing that models natural\nlanguage as it happens in face-to-face communication. Although humor detection\nis an established research area in NLP, in a multimodal context it is an\nunderstudied area. This paper presents a diverse multimodal dataset, called\nUR-FUNNY, to open the door to understanding multimodal language used in\nexpressing humor. The dataset and accompanying studies, present a framework in\nmultimodal humor detection for the natural language processing community.\nUR-FUNNY is publicly available for research.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 03:15:38 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Hasan", "Md Kamrul", "", "Ehsan"], ["Rahman", "Wasifur", "", "Ehsan"], ["Zadeh", "Amir", "", "Ehsan"], ["Zhong", "Jianyuan", "", "Ehsan"], ["Tanveer", "Md Iftekhar", "", "Ehsan"], ["Morency", "Louis-Philippe", "", "Ehsan"], ["Mohammed", "", "", "Ehsan"], ["Hoque", "", ""]]}, {"id": "1904.06632", "submitter": "Mansoor Sheikh", "authors": "M Sheikh, A.C.C. Coolen", "title": "Analysis of overfitting in the regularized Cox model", "comments": null, "journal-ref": null, "doi": "10.1088/1751-8121/ab375c", "report-no": null, "categories": "stat.ME cond-mat.dis-nn cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Cox proportional hazards model is ubiquitous in the analysis of\ntime-to-event data. However, when the data dimension p is comparable to the\nsample size $N$, maximum likelihood estimates for its regression parameters are\nknown to be biased or break down entirely due to overfitting. This prompted the\nintroduction of the so-called regularized Cox model. In this paper we use the\nreplica method from statistical physics to investigate the relationship between\nthe true and inferred regression parameters in regularized multivariate Cox\nregression with L2 regularization, in the regime where both p and N are large\nbut with p/N ~ O(1). We thereby generalize a recent study from maximum\nlikelihood to maximum a posteriori inference. We also establish a relationship\nbetween the optimal regularization parameter and p/N, allowing for\nstraightforward overfitting corrections in time-to-event analysis.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 05:48:02 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 12:15:10 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Sheikh", "M", ""], ["Coolen", "A. C. C.", ""]]}, {"id": "1904.06646", "submitter": "Zahra Zohrevand", "authors": "Zahra Zohrevand, Uwe Gl\\\"asser", "title": "Should I Raise The Red Flag? A comprehensive survey of anomaly scoring\n  methods toward mitigating false alarms", "comments": "arXiv admin note: text overlap with arXiv:1802.04431,\n  arXiv:1503.01158 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, advanced intrusion detection systems (IDSs) rely on a combination\nof anomaly detection and signature-based methods. An IDS gathers observations,\nanalyzes behavioral patterns, and reports suspicious events for further\ninvestigation. A notorious issue anomaly detection systems (ADSs) and IDSs face\nis the possibility of high false alarms, which even state-of-the-art systems\nhave not overcome. This is especially a problem with large and complex systems.\nThe number of non-critical alarms can easily overwhelm administrators and\nincrease the likelihood of ignoring future alerts. Mitigation strategies thus\naim to avoid raising `too many' false alarms without missing potentially\ndangerous situations. There are two major categories of false alarm-mitigation\nstrategies: (1) methods that are customized to enhance the quality of anomaly\nscoring; (2) approaches acting as filtering methods in contexts that aim to\ndecrease false alarm rates. These methods have been widely utilized by many\nscholars. Herein, we review and compare the existing techniques for false alarm\nmitigation in ADSs. We also examine the use of promising techniques in\nsignature-based IDS and other relevant contexts, such as commercial security\ninformation and event management tools, which are promising for ADSs. We\nconclude by highlighting promising directions for future research.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 07:36:08 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2020 19:35:13 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Zohrevand", "Zahra", ""], ["Gl\u00e4sser", "Uwe", ""]]}, {"id": "1904.06685", "submitter": "Zengmao Wang PhD", "authors": "Bo Du, Zengmao Wang, Lefei Zhang, Liangpei Zhang, Wei Liu, Jialie\n  Shen, Dacheng Tao", "title": "Exploring Representativeness and Informativeness for Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we find a general way to choose the most suitable samples for\ntraining a classifier? Even with very limited prior information? Active\nlearning, which can be regarded as an iterative optimization procedure, plays a\nkey role to construct a refined training set to improve the classification\nperformance in a variety of applications, such as text analysis, image\nrecognition, social network modeling, etc.\n  Although combining representativeness and informativeness of samples has been\nproven promising for active sampling, state-of-the-art methods perform well\nunder certain data structures. Then can we find a way to fuse the two active\nsampling criteria without any assumption on data? This paper proposes a general\nactive learning framework that effectively fuses the two criteria. Inspired by\na two-sample discrepancy problem, triple measures are elaborately designed to\nguarantee that the query samples not only possess the representativeness of the\nunlabeled data but also reveal the diversity of the labeled data. Any\nappropriate similarity measure can be employed to construct the triple\nmeasures. Meanwhile, an uncertain measure is leveraged to generate the\ninformativeness criterion, which can be carried out in different ways.\n  Rooted in this framework, a practical active learning algorithm is proposed,\nwhich exploits a radial basis function together with the estimated\nprobabilities to construct the triple measures and a modified\nBest-versus-Second-Best strategy to construct the uncertain measure,\nrespectively. Experimental results on benchmark datasets demonstrate that our\nalgorithm consistently achieves superior performance over the state-of-the-art\nactive learning algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 12:18:42 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Du", "Bo", ""], ["Wang", "Zengmao", ""], ["Zhang", "Lefei", ""], ["Zhang", "Liangpei", ""], ["Liu", "Wei", ""], ["Shen", "Jialie", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.06703", "submitter": "Benjamin Beyret", "authors": "Benjamin Beyret, Ali Shafti, A. Aldo Faisal", "title": "Dot-to-Dot: Explainable Hierarchical Reinforcement Learning for Robotic\n  Manipulation", "comments": null, "journal-ref": null, "doi": "10.1109/IROS40897.2019.8968488", "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic systems are ever more capable of automation and fulfilment of complex\ntasks, particularly with reliance on recent advances in intelligent systems,\ndeep learning and artificial intelligence. However, as robots and humans come\ncloser in their interactions, the matter of interpretability, or explainability\nof robot decision-making processes for the human grows in importance. A\nsuccessful interaction and collaboration will only take place through mutual\nunderstanding of underlying representations of the environment and the task at\nhand. This is currently a challenge in deep learning systems. We present a\nhierarchical deep reinforcement learning system, consisting of a low-level\nagent handling the large actions/states space of a robotic system efficiently,\nby following the directives of a high-level agent which is learning the\nhigh-level dynamics of the environment and task. This high-level agent forms a\nrepresentation of the world and task at hand that is interpretable for a human\noperator. The method, which we call Dot-to-Dot, is tested on a MuJoCo-based\nmodel of the Fetch Robotics Manipulator, as well as a Shadow Hand, to test its\nperformance. Results show efficient learning of complex actions/states spaces\nby the low-level agent, and an interpretable representation of the task and\ndecision-making process learned by the high-level agent.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 14:54:14 GMT"}, {"version": "v2", "created": "Sun, 11 Aug 2019 06:37:15 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Beyret", "Benjamin", ""], ["Shafti", "Ali", ""], ["Faisal", "A. Aldo", ""]]}, {"id": "1904.06738", "submitter": "Chiranjib Bhattacharyya", "authors": "Chiranjib Bhattacharyya and Ravindran Kannan", "title": "Finding a latent k-simplex in O(k . nnz(data)) time via Subset Smoothing", "comments": "Added more discussion of special cases. The assumptions are also\n  modified", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show that a large class of Latent variable models, such as\nMixed Membership Stochastic Block(MMSB) Models, Topic Models, and Adversarial\nClustering, can be unified through a geometric perspective, replacing model\nspecific assumptions and algorithms for individual models. The geometric\nperspective leads to the formulation: \\emph{find a latent $k-$ polytope $K$ in\n${\\bf R}^d$ given $n$ data points, each obtained by perturbing a latent point\nin $K$}. This problem does not seem to have been considered in the literature.\nThe most important contribution of this paper is to show that the latent\n$k-$polytope problem admits an efficient algorithm under deterministic\nassumptions which naturally hold in Latent variable models considered in this\npaper. ur algorithm runs in time $O^*(k\\; \\mbox{nnz})$ matching the best\nrunning time of algorithms in special cases considered here and is better when\nthe data is sparse, as is the case in applications. An important novelty of the\nalgorithm is the introduction of \\emph{subset smoothed polytope}, $K'$, the\nconvex hull of the ${n\\choose \\delta n}$ points obtained by averaging all\n$\\delta n$ subsets of the data points, for a given $\\delta \\in (0,1)$. We show\nthat $K$ and $K'$ are close in Hausdroff distance. Among the consequences of\nour algorithm are the following: (a) MMSB Models and Topic Models: the first\nquasi-input-sparsity time algorithm for parameter estimation for $k \\in\nO^*(1)$, (b) Adversarial Clustering: In $k-$means, if, an adversary is allowed\nto move many data points from each cluster an arbitrary amount towards the\nconvex hull of the centers of other clusters, our algorithm still estimates\ncluster centers well.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 18:29:13 GMT"}, {"version": "v2", "created": "Fri, 19 Apr 2019 23:30:28 GMT"}, {"version": "v3", "created": "Tue, 16 Jul 2019 16:41:12 GMT"}, {"version": "v4", "created": "Sun, 5 Jan 2020 06:51:19 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Bhattacharyya", "Chiranjib", ""], ["Kannan", "Ravindran", ""]]}, {"id": "1904.06762", "submitter": "Tryphon Georgiou", "authors": "Yongxin Chen, Tryphon T. Georgiou, and Allen R. Tannenbaum", "title": "Probabilistic Kernel Support Vector Machines", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a probabilistic enhancement of standard kernel Support Vector\nMachines for binary classification, in order to address the case when, along\nwith given data sets, a description of uncertainty (e.g., error bounds) may be\navailable on each datum. In the present paper, we specifically consider\nGaussian distributions to model uncertainty. Thereby, our data consist of pairs\n$(x_i,\\Sigma_i)$, $i\\in\\{1,\\ldots,N\\}$, along with an indicator\n$y_i\\in\\{-1,1\\}$ to declare membership in one of two categories for each pair.\nThese pairs may be viewed to represent the mean and covariance, respectively,\nof random vectors $\\xi_i$ taking values in a suitable linear space (typically\n$\\mathbb R^n$). Thus, our setting may also be viewed as a modification of\nSupport Vector Machines to classify distributions, albeit, at present, only\nGaussian ones. We outline the formalism that allows computing suitable\nclassifiers via a natural modification of the standard \"kernel trick.\" The main\ncontribution of this work is to point out a suitable kernel function for\napplying Support Vector techniques to the setting of uncertain data for which a\ndetailed uncertainty description is also available (herein, \"Gaussian points\").\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 21:25:25 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 16:13:27 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Chen", "Yongxin", ""], ["Georgiou", "Tryphon T.", ""], ["Tannenbaum", "Allen R.", ""]]}, {"id": "1904.06823", "submitter": "Feng Xiao", "authors": "Feng Xiao, Dapeng Zhang, Gang Kou, Lu Li", "title": "Learning Spatiotemporal Features of Ride-sourcing Services with Fusion\n  Convolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To collectively forecast the demand for ride-sourcing services in all regions\nof a city, the deep learning approaches have been applied with commendable\nresults. However, the local statistical differences throughout the geographical\nlayout of the city make the spatial stationarity assumption of the convolution\ninvalid, which limits the performance of CNNs on the demand forecasting task.\nIn this paper, we propose a novel deep learning framework called LC-ST-FCN\n(locally connected spatiotemporal fully-convolutional neural network) to\naddress the unique challenges of the region-level demand forecasting problem\nwithin one end-to-end architecture (E2E). We first employ the 3D convolutional\nlayers to fuse the spatial and temporal information existed in the input and\nthen feed the spatiotemporal features extracted by the 3D convolutional layers\nto the subsequent 2D convolutional layers. Afterward, the prediction value of\neach region is obtained by the locally connected convolutional layers which\nrelax the parameter sharing scheme. We evaluate the proposed model on a real\ndataset from a ride-sourcing service platform (DiDiChuxing) and observe\nsignificant improvements compared with a bunch of baseline models. Besides, we\nalso illustrate the effectiveness of our proposed model by visualizing how\ndifferent types of convolutional layers transform their input and capture\nuseful features. The visualization results show that fully convolutional\narchitecture enables the model to better localize the related regions. And the\nlocally connected layers play an important role in dealing with the local\nstatistical differences and activating useful regions.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 03:10:45 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 08:48:30 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Xiao", "Feng", ""], ["Zhang", "Dapeng", ""], ["Kou", "Gang", ""], ["Li", "Lu", ""]]}, {"id": "1904.06834", "submitter": "Kartik Goyal", "authors": "Kartik Goyal, Chris Dyer and Taylor Berg-Kirkpatrick", "title": "An Empirical Investigation of Global and Local Normalization for\n  Recurrent Neural Sequence Models Using a Continuous Relaxation to Beam Search", "comments": "Long paper at NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Globally normalized neural sequence models are considered superior to their\nlocally normalized equivalents because they may ameliorate the effects of label\nbias. However, when considering high-capacity neural parametrizations that\ncondition on the whole input sequence, both model classes are theoretically\nequivalent in terms of the distributions they are capable of representing.\nThus, the practical advantage of global normalization in the context of modern\nneural methods remains unclear. In this paper, we attempt to shed light on this\nproblem through an empirical study. We extend an approach for search-aware\ntraining via a continuous relaxation of beam search (Goyal et al., 2017b) in\norder to enable training of globally normalized recurrent sequence models\nthrough simple backpropagation. We then use this technique to conduct an\nempirical study of the interaction between global normalization, high-capacity\nencoders, and search-aware optimization. We observe that in the context of\ninexact search, globally normalized neural models are still more effective than\ntheir locally normalized counterparts. Further, since our training approach is\nsensitive to warm-starting with pre-trained models, we also propose a novel\ninitialization strategy based on self-normalization for pre-training globally\nnormalized models. We perform analysis of our approach on two tasks: CCG\nsupertagging and Machine Translation, and demonstrate the importance of global\nnormalization under different conditions while using search-aware training.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 04:17:13 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Goyal", "Kartik", ""], ["Dyer", "Chris", ""], ["Berg-Kirkpatrick", "Taylor", ""]]}, {"id": "1904.06836", "submitter": "Peihua Li", "authors": "Qilong Wang and Jiangtao Xie and Wangmeng Zuo and Lei Zhang and Peihua\n  Li", "title": "Deep CNNs Meet Global Covariance Pooling: Better Representation and\n  Generalization", "comments": "Accepted to IEEE TPAMI. Code is at http://peihuali.org/MPN-COV/", "journal-ref": null, "doi": "10.1109/TPAMI.2020.2974833", "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with global average pooling in existing deep convolutional neural\nnetworks (CNNs), global covariance pooling can capture richer statistics of\ndeep features, having potential for improving representation and generalization\nabilities of deep CNNs. However, integration of global covariance pooling into\ndeep CNNs brings two challenges: (1) robust covariance estimation given deep\nfeatures of high dimension and small sample size; (2) appropriate usage of\ngeometry of covariances. To address these challenges, we propose a global\nMatrix Power Normalized COVariance (MPN-COV) Pooling. Our MPN-COV conforms to a\nrobust covariance estimator, very suitable for scenario of high dimension and\nsmall sample size. It can also be regarded as Power-Euclidean metric between\ncovariances, effectively exploiting their geometry. Furthermore, a global\nGaussian embedding network is proposed to incorporate first-order statistics\ninto MPN-COV. For fast training of MPN-COV networks, we implement an iterative\nmatrix square root normalization, avoiding GPU unfriendly eigen-decomposition\ninherent in MPN-COV. Additionally, progressive 1x1 convolutions and group\nconvolution are introduced to compress covariance representations. The proposed\nmethods are highly modular, readily plugged into existing deep CNNs. Extensive\nexperiments are conducted on large-scale object classification, scene\ncategorization, fine-grained visual recognition and texture classification,\nshowing our methods outperform the counterparts and obtain state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 04:30:01 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 02:49:48 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Wang", "Qilong", ""], ["Xie", "Jiangtao", ""], ["Zuo", "Wangmeng", ""], ["Zhang", "Lei", ""], ["Li", "Peihua", ""]]}, {"id": "1904.06887", "submitter": "Minsung Hyun", "authors": "Minsung Hyun, Junyoung Choi and Nojun Kwak", "title": "Disentangling Options with Hellinger Distance Regularizer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In reinforcement learning (RL), temporal abstraction still remains as an\nimportant and unsolved problem. The options framework provided clues to\ntemporal abstraction in the RL, and the option-critic architecture elegantly\nsolved the two problems of finding options and learning RL agents in an\nend-to-end manner. However, it is necessary to examine whether the options\nlearned through this method play a mutually exclusive role. In this paper, we\npropose a Hellinger distance regularizer, a method for disentangling options.\nIn addition, we will shed light on various indicators from the statistical\npoint of view to compare with the options learned through the existing\noption-critic architecture.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 07:43:17 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Hyun", "Minsung", ""], ["Choi", "Junyoung", ""], ["Kwak", "Nojun", ""]]}, {"id": "1904.06895", "submitter": "Markku Hinkka", "authors": "Markku Hinkka, Teemu Lehto and Keijo Heljanko", "title": "Exploiting Event Log Event Attributes in RNN Based Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In predictive process analytics, current and historical process data in event\nlogs is used to predict the future, e.g., to predict the next activity or how\nlong a process will still require to complete. Recurrent neural networks (RNN)\nand its subclasses have been demonstrated to be well suited for creating\nprediction models. Thus far, event attributes have not been fully utilized in\nthese models. The biggest challenge in exploiting them in prediction models is\nthe potentially large amount of event attributes and attribute values. We\npresent a novel clustering technique that allows for trade-offs between\nprediction accuracy and the time needed for model training and prediction. As\nan additional finding, we also find that this clustering method combined with\nhaving raw event attribute values in some cases provides even better prediction\naccuracy at the cost of additional time required for training and prediction.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 07:58:30 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 08:19:29 GMT"}, {"version": "v3", "created": "Wed, 15 Jan 2020 07:18:43 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Hinkka", "Markku", ""], ["Lehto", "Teemu", ""], ["Heljanko", "Keijo", ""]]}, {"id": "1904.06915", "submitter": "Yao Yang Leow", "authors": "Yao Yang Leow, Thomas Laurent, Xavier Bresson", "title": "GraphTSNE: A Visualization Technique for Graph-Structured Data", "comments": "Published as a workshop paper at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present GraphTSNE, a novel visualization technique for graph-structured\ndata based on t-SNE. The growing interest in graph-structured data increases\nthe importance of gaining human insight into such datasets by means of\nvisualization. Among the most popular visualization techniques, classical t-SNE\nis not suitable on such datasets because it has no mechanism to make use of\ninformation from the graph structure. On the other hand, visualization\ntechniques which operate on graphs, such as Laplacian Eigenmaps and tsNET, have\nno mechanism to make use of information from node features. Our proposed method\nGraphTSNE produces visualizations which account for both graph structure and\nnode features. It is based on scalable and unsupervised training of a graph\nconvolutional network on a modified t-SNE loss. By assembling a suite of\nevaluation metrics, we demonstrate that our method produces desirable\nvisualizations on three benchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 09:11:17 GMT"}, {"version": "v2", "created": "Sun, 21 Apr 2019 05:36:53 GMT"}, {"version": "v3", "created": "Tue, 23 Apr 2019 13:10:07 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Leow", "Yao Yang", ""], ["Laurent", "Thomas", ""], ["Bresson", "Xavier", ""]]}, {"id": "1904.06950", "submitter": "Mayukh Das", "authors": "Mayukh Das, Yang Yu, Devendra Singh Dhami, Gautam Kunapuli, Sriraam\n  Natarajan", "title": "Human-Guided Learning of Column Networks: Augmenting Deep Learning with\n  Advice", "comments": "Under Review at 'Machine Learning Journal' (MLJ)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep models have been successfully applied in several applications,\nespecially with low-level representations. However, sparse, noisy samples and\nstructured domains (with multiple objects and interactions) are some of the\nopen challenges in most deep models. Column Networks, a deep architecture, can\nsuccinctly capture such domain structure and interactions, but may still be\nprone to sub-optimal learning from sparse and noisy samples. Inspired by the\nsuccess of human-advice guided learning in AI, especially in data-scarce\ndomains, we propose Knowledge-augmented Column Networks that leverage human\nadvice/knowledge for better learning with noisy/sparse samples. Our experiments\ndemonstrate that our approach leads to either superior overall performance or\nfaster convergence (i.e., both effective and efficient).\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 10:23:10 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Das", "Mayukh", ""], ["Yu", "Yang", ""], ["Dhami", "Devendra Singh", ""], ["Kunapuli", "Gautam", ""], ["Natarajan", "Sriraam", ""]]}, {"id": "1904.06952", "submitter": "Eran Treister", "authors": "Jonathan Ephrath, Lars Ruthotto, Eldad Haber, Eran Treister", "title": "LeanResNet: A Low-cost Yet Effective Convolutional Residual Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) filter the input data using spatial\nconvolution operators with compact stencils. Commonly, the convolution\noperators couple features from all channels, which leads to immense\ncomputational cost in the training of and prediction with CNNs. To improve the\nefficiency of CNNs, we introduce lean convolution operators that reduce the\nnumber of parameters and computational complexity, and can be used in a wide\nrange of existing CNNs. Here, we exemplify their use in residual networks\n(ResNets), which have been very reliable for a few years now and analyzed\nintensively. In our experiments on three image classification problems, the\nproposed LeanResNet yields results that are comparable to other recently\nproposed reduced architectures using similar number of parameters.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 10:28:42 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 13:11:43 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Ephrath", "Jonathan", ""], ["Ruthotto", "Lars", ""], ["Haber", "Eldad", ""], ["Treister", "Eran", ""]]}, {"id": "1904.06960", "submitter": "Mischa Schmidt", "authors": "Mischa Schmidt, Shahd Safarani, Julia Gastinger, Tobias Jacobs,\n  Sebastien Nicolas, Anett Sch\\\"ulke", "title": "On the Performance of Differential Evolution for Hyperparameter Tuning", "comments": "2019 International Joint Conference on Neural Networks (IJCNN)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated hyperparameter tuning aspires to facilitate the application of\nmachine learning for non-experts. In the literature, different optimization\napproaches are applied for that purpose. This paper investigates the\nperformance of Differential Evolution for tuning hyperparameters of supervised\nlearning algorithms for classification tasks. This empirical study involves a\nrange of different machine learning algorithms and datasets with various\ncharacteristics to compare the performance of Differential Evolution with\nSequential Model-based Algorithm Configuration (SMAC), a reference Bayesian\nOptimization approach. The results indicate that Differential Evolution\noutperforms SMAC for most datasets when tuning a given machine learning\nalgorithm - particularly when breaking ties in a first-to-report fashion. Only\nfor the tightest of computational budgets SMAC performs better. On small\ndatasets, Differential Evolution outperforms SMAC by 19% (37% after\ntie-breaking). In a second experiment across a range of representative datasets\ntaken from the literature, Differential Evolution scores 15% (23% after\ntie-breaking) more wins than SMAC.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 10:58:14 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Schmidt", "Mischa", ""], ["Safarani", "Shahd", ""], ["Gastinger", "Julia", ""], ["Jacobs", "Tobias", ""], ["Nicolas", "Sebastien", ""], ["Sch\u00fclke", "Anett", ""]]}, {"id": "1904.06963", "submitter": "Soham De", "authors": "Karthik A. Sankararaman, Soham De, Zheng Xu, W. Ronny Huang, Tom\n  Goldstein", "title": "The Impact of Neural Network Overparameterization on Gradient Confusion\n  and Stochastic Gradient Descent", "comments": "ICML 2020 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies how neural network architecture affects the speed of\ntraining. We introduce a simple concept called gradient confusion to help\nformally analyze this. When gradient confusion is high, stochastic gradients\nproduced by different data samples may be negatively correlated, slowing down\nconvergence. But when gradient confusion is low, data samples interact\nharmoniously, and training proceeds quickly. Through theoretical and\nexperimental results, we demonstrate how the neural network architecture\naffects gradient confusion, and thus the efficiency of training. Our results\nshow that, for popular initialization techniques, increasing the width of\nneural networks leads to lower gradient confusion, and thus faster model\ntraining. On the other hand, increasing the depth of neural networks has the\nopposite effect. Our results indicate that alternate initialization techniques\nor networks using both batch normalization and skip connections help reduce the\ntraining burden of very deep networks.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 11:02:22 GMT"}, {"version": "v2", "created": "Sat, 8 Jun 2019 14:55:41 GMT"}, {"version": "v3", "created": "Wed, 16 Oct 2019 20:49:48 GMT"}, {"version": "v4", "created": "Wed, 26 Feb 2020 15:08:50 GMT"}, {"version": "v5", "created": "Mon, 6 Jul 2020 21:36:19 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Sankararaman", "Karthik A.", ""], ["De", "Soham", ""], ["Xu", "Zheng", ""], ["Huang", "W. Ronny", ""], ["Goldstein", "Tom", ""]]}, {"id": "1904.06984", "submitter": "Itay Safran", "authors": "Itay Safran, Ronen Eldan, Ohad Shamir", "title": "Depth Separations in Neural Networks: What is Actually Being Separated?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing depth separation results for constant-depth networks essentially\nshow that certain radial functions in $\\mathbb{R}^d$, which can be easily\napproximated with depth $3$ networks, cannot be approximated by depth $2$\nnetworks, even up to constant accuracy, unless their size is exponential in\n$d$. However, the functions used to demonstrate this are rapidly oscillating,\nwith a Lipschitz parameter scaling polynomially with the dimension $d$ (or\nequivalently, by scaling the function, the hardness result applies to\n$\\mathcal{O}(1)$-Lipschitz functions only when the target accuracy $\\epsilon$\nis at most $\\text{poly}(1/d)$). In this paper, we study whether such depth\nseparations might still hold in the natural setting of\n$\\mathcal{O}(1)$-Lipschitz radial functions, when $\\epsilon$ does not scale\nwith $d$. Perhaps surprisingly, we show that the answer is negative: In\ncontrast to the intuition suggested by previous work, it \\emph{is} possible to\napproximate $\\mathcal{O}(1)$-Lipschitz radial functions with depth $2$, size\n$\\text{poly}(d)$ networks, for every constant $\\epsilon$. We complement it by\nshowing that approximating such functions is also possible with depth $2$, size\n$\\text{poly}(1/\\epsilon)$ networks, for every constant $d$. Finally, we show\nthat it is not possible to have polynomial dependence in both $d,1/\\epsilon$\nsimultaneously. Overall, our results indicate that in order to show depth\nseparations for expressing $\\mathcal{O}(1)$-Lipschitz functions with constant\naccuracy -- if at all possible -- one would need fundamentally different\ntechniques than existing ones in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 12:07:38 GMT"}, {"version": "v2", "created": "Sun, 26 May 2019 11:22:43 GMT"}, {"version": "v3", "created": "Wed, 2 Jun 2021 16:35:17 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Safran", "Itay", ""], ["Eldan", "Ronen", ""], ["Shamir", "Ohad", ""]]}, {"id": "1904.06991", "submitter": "Samuli Laine", "authors": "Tuomas Kynk\\\"a\\\"anniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen,\n  Timo Aila", "title": "Improved Precision and Recall Metric for Assessing Generative Models", "comments": "NeurIPS 2019 final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to automatically estimate the quality and coverage of the samples\nproduced by a generative model is a vital requirement for driving algorithm\nresearch. We present an evaluation metric that can separately and reliably\nmeasure both of these aspects in image generation tasks by forming explicit,\nnon-parametric representations of the manifolds of real and generated data. We\ndemonstrate the effectiveness of our metric in StyleGAN and BigGAN by providing\nseveral illustrative examples where existing metrics yield uninformative or\ncontradictory results. Furthermore, we analyze multiple design variants of\nStyleGAN to better understand the relationships between the model architecture,\ntraining methods, and the properties of the resulting sample distribution. In\nthe process, we identify new variants that improve the state-of-the-art. We\nalso perform the first principled analysis of truncation methods and identify\nan improved method. Finally, we extend our metric to estimate the perceptual\nquality of individual samples, and use this to study latent space\ninterpolations.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 12:20:32 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 13:03:04 GMT"}, {"version": "v3", "created": "Wed, 30 Oct 2019 12:33:29 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Kynk\u00e4\u00e4nniemi", "Tuomas", ""], ["Karras", "Tero", ""], ["Laine", "Samuli", ""], ["Lehtinen", "Jaakko", ""], ["Aila", "Timo", ""]]}, {"id": "1904.07032", "submitter": "Sushravya Raghunath", "authors": "Sushravya Raghunath, Alvaro E. Ulloa Cerna, Linyuan Jing, David P.\n  vanMaanen, Joshua Stough, Dustin N. Hartzel, Joseph B. Leader, H. Lester\n  Kirchner, Christopher W. Good, Aalpen A. Patel, Brian P. Delisle, Amro\n  Alsaid, Dominik Beer, Christopher M. Haggerty, Brandon K. Fornwalt", "title": "Deep neural networks can predict mortality from 12-lead\n  electrocardiogram voltage data", "comments": "An updated version of this paper is now published with Nature\n  Medicine (2020)", "journal-ref": null, "doi": "10.1038/s41591-020-0870-z", "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The electrocardiogram (ECG) is a widely-used medical test, typically\nconsisting of 12 voltage versus time traces collected from surface recordings\nover the heart. Here we hypothesize that a deep neural network can predict an\nimportant future clinical event (one-year all-cause mortality) from ECG\nvoltage-time traces. We show good performance for predicting one-year mortality\nwith an average AUC of 0.85 from a model cross-validated on 1,775,926 12-lead\nresting ECGs, that were collected over a 34-year period in a large regional\nhealth system. Even within the large subset of ECGs interpreted as 'normal' by\na physician (n=297,548), the model performance to predict one-year mortality\nremained high (AUC=0.84), and Cox Proportional Hazard model revealed a hazard\nratio of 6.6 (p<0.005) for the two predicted groups (dead vs alive one year\nafter ECG) over a 30-year follow-up period. A blinded survey of three\ncardiologists suggested that the patterns captured by the model were generally\nnot visually apparent to cardiologists even after being shown 240 paired\nexamples of labeled true positives (dead) and true negatives (alive). In\nsummary, deep learning can add significant prognostic information to the\ninterpretation of 12-lead resting ECGs, even in cases that are interpreted as\n'normal' by physicians.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 13:28:35 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 17:08:02 GMT"}, {"version": "v3", "created": "Mon, 11 May 2020 18:21:11 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Raghunath", "Sushravya", ""], ["Cerna", "Alvaro E. Ulloa", ""], ["Jing", "Linyuan", ""], ["vanMaanen", "David P.", ""], ["Stough", "Joshua", ""], ["Hartzel", "Dustin N.", ""], ["Leader", "Joseph B.", ""], ["Kirchner", "H. Lester", ""], ["Good", "Christopher W.", ""], ["Patel", "Aalpen A.", ""], ["Delisle", "Brian P.", ""], ["Alsaid", "Amro", ""], ["Beer", "Dominik", ""], ["Haggerty", "Christopher M.", ""], ["Fornwalt", "Brandon K.", ""]]}, {"id": "1904.07150", "submitter": "Kolyan Ray", "authors": "Kolyan Ray and Botond Szabo", "title": "Variational Bayes for high-dimensional linear regression with sparse\n  priors", "comments": "42 pages. To appear in Journal of the American Statistical\n  Association", "journal-ref": null, "doi": "10.1080/01621459.2020.1847121", "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a mean-field spike and slab variational Bayes (VB) approximation to\nBayesian model selection priors in sparse high-dimensional linear regression.\nUnder compatibility conditions on the design matrix, oracle inequalities are\nderived for the mean-field VB approximation, implying that it converges to the\nsparse truth at the optimal rate and gives optimal prediction of the response\nvector. The empirical performance of our algorithm is studied, showing that it\nworks comparably well as other state-of-the-art Bayesian variable selection\nmethods. We also numerically demonstrate that the widely used coordinate-ascent\nvariational inference (CAVI) algorithm can be highly sensitive to the parameter\nupdating order, leading to potentially poor performance. To mitigate this, we\npropose a novel prioritized updating scheme that uses a data-driven updating\norder and performs better in simulations. The variational algorithm is\nimplemented in the R package 'sparsevb'.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 15:58:44 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 21:33:39 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2020 15:06:11 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Ray", "Kolyan", ""], ["Szabo", "Botond", ""]]}, {"id": "1904.07153", "submitter": "Marcel Hirt", "authors": "Marcel Hirt, Petros Dellaportas, Alain Durmus", "title": "Copula-like Variational Inference", "comments": "33rd Conference on Neural Information Processing Systems (NeurIPS\n  2019), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a new family of variational distributions motivated by\nSklar's theorem. This family is based on new copula-like densities on the\nhypercube with non-uniform marginals which can be sampled efficiently, i.e.\nwith a complexity linear in the dimension of state space. Then, the proposed\nvariational densities that we suggest can be seen as arising from these\ncopula-like densities used as base distributions on the hypercube with Gaussian\nquantile functions and sparse rotation matrices as normalizing flows. The\nlatter correspond to a rotation of the marginals with complexity $\\mathcal{O}(d\n\\log d)$. We provide some empirical evidence that such a variational family can\nalso approximate non-Gaussian posteriors and can be beneficial compared to\nGaussian approximations. Our method performs largely comparably to\nstate-of-the-art variational approximations on standard regression and\nclassification benchmarks for Bayesian Neural Networks.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 16:08:32 GMT"}, {"version": "v2", "created": "Sun, 22 Dec 2019 13:01:15 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Hirt", "Marcel", ""], ["Dellaportas", "Petros", ""], ["Durmus", "Alain", ""]]}, {"id": "1904.07154", "submitter": "Jaehun Kim", "authors": "Jaehun Kim, Juli\\'an Urbano, Cynthia C. S. Liem, Alan Hanjalic", "title": "Are Nearby Neighbors Relatives?: Testing Deep Music Embeddings", "comments": "this work was accepted for publication in the \"Frontiers in Applied\n  Mathematics and Statistics (Deep Learning: Status, Applications and\n  Algorithms)\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep neural networks have frequently been used to directly learn\nrepresentations useful for a given task from raw input data. In terms of\noverall performance metrics, machine learning solutions employing deep\nrepresentations frequently have been reported to greatly outperform those using\nhand-crafted feature representations. At the same time, they may pick up on\naspects that are predominant in the data, yet not actually meaningful or\ninterpretable. In this paper, we therefore propose a systematic way to test the\ntrustworthiness of deep music representations, considering musical semantics.\nThe underlying assumption is that in case a deep representation is to be\ntrusted, distance consistency between known related points should be maintained\nboth in the input audio space and corresponding latent deep space. We generate\nknown related points through semantically meaningful transformations, both\nconsidering imperceptible and graver transformations. Then, we examine within-\nand between-space distance consistencies, both considering audio space and\nlatent embedded space, the latter either being a result of a conventional\nfeature extractor or a deep encoder. We illustrate how our method, as a\ncomplement to task-specific performance, provides interpretable insight into\nwhat a network may have captured from training data signals.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 16:08:41 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 21:42:36 GMT"}, {"version": "v3", "created": "Thu, 17 Oct 2019 23:34:04 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Kim", "Jaehun", ""], ["Urbano", "Juli\u00e1n", ""], ["Liem", "Cynthia C. S.", ""], ["Hanjalic", "Alan", ""]]}, {"id": "1904.07163", "submitter": "Jalal Mirakhorli", "authors": "Jalal Mirakhorli, Hamidreza Amindavar, Mojgan Mirakhorli", "title": "Graph-Based Method for Anomaly Prediction in Brain Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resting-state functional MRI (rs-fMRI) in functional neuroimaging techniques\nhave improved in brain disorders, dysfunction studies via mapping the topology\nof the brain connections, i.e. connectopic mapping. Since, there are the slight\ndifferences between healthy and unhealthy brain regions and functions,\ninvestigation into the complex topology of functional and structural brain\nnetworks in human is a complicated task with the growth of evaluation criteria.\nIrregular graph deep learning applications have widely spread to understanding\nhuman cognitive functions that are linked to gene expression and related\ndistributed spatial patterns, because the neuronal networks of the brain can\nhold dynamically a variety of brain solutions with different activity patterns\nand functional connectivity, these applications might also be involved with\nboth node-centric and graph-centric tasks. In this paper, we performed a novel\napproach of individual generative model and high order graph analysis for the\nregion of interest recognition areas of the brain which do not have a normal\nconnection during applying certain tasks. Here, we proposed a high order\nframework of Graph Auto-Encoder (GAE) with a hypersphere distributer for\nfunctional data analysis in brain imaging studies that is underlying\nnon-Euclidean structure in the learning of strong non-rigid graphs among large\nscale data. In addition, we distinguished the possible modes of correlations in\nabnormal brain connections. Our finding will show the degree of correlation\nbetween the affected regions and their simultaneous occurrence over time that\ncan be used to diagnose brain diseases or revealing the ability of the nervous\nsystem to modify in brain topology at all angles, brain plasticity, according\nto input stimuli.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 16:22:08 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 05:30:51 GMT"}, {"version": "v3", "created": "Tue, 23 Apr 2019 10:34:06 GMT"}, {"version": "v4", "created": "Tue, 7 May 2019 10:33:25 GMT"}, {"version": "v5", "created": "Fri, 24 May 2019 09:57:47 GMT"}, {"version": "v6", "created": "Mon, 24 Jun 2019 08:58:49 GMT"}, {"version": "v7", "created": "Wed, 17 Jul 2019 07:10:21 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Mirakhorli", "Jalal", ""], ["Amindavar", "Hamidreza", ""], ["Mirakhorli", "Mojgan", ""]]}, {"id": "1904.07192", "submitter": "Maurice Schmeits", "authors": "Kilian Bakker, Kirien Whan, Wouter Knap and Maurice Schmeits", "title": "Comparison of statistical post-processing methods for probabilistic NWP\n  forecasts of solar radiation", "comments": "https://doi.org/10.1016/j.solener.2019.08.044", "journal-ref": "Solar Energy, volume 191, 2019, pages 138-150", "doi": "10.1016/j.solener.2019.08.044", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increased usage of solar energy places additional importance on forecasts\nof solar radiation. Solar panel power production is primarily driven by the\namount of solar radiation and it is therefore important to have accurate\nforecasts of solar radiation. Accurate forecasts that also give information on\nthe forecast uncertainties can help users of solar energy to make better solar\nradiation based decisions related to the stability of the electrical grid. To\nachieve this, we apply statistical post-processing techniques that determine\nrelationships between observations of global radiation (made within the KNMI\nnetwork of automatic weather stations in the Netherlands) and forecasts of\nvarious meteorological variables from the numerical weather prediction (NWP)\nmodel HARMONIE-AROME (HA) and the atmospheric composition model CAMS. Those\nrelationships are used to produce probabilistic forecasts of global radiation.\nWe compare 7 different statistical post-processing methods, consisting of two\nparametric and five non-parametric methods. We find that all methods are able\nto generate probabilistic forecasts that improve the raw global radiation\nforecast from HA according to the root mean squared error (on the median) and\nthe potential economic value. Additionally, we show how important the\npredictors are in the different regression methods. We also compare the\nregression methods using various probabilistic scoring metrics, namely the\ncontinuous ranked probability skill score, the Brier skill score and\nreliability diagrams. We find that quantile regression and generalized random\nforests generally perform best. In (near) clear sky conditions the\nnon-parametric methods have more skill than the parametric ones.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 17:08:58 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 10:29:50 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Bakker", "Kilian", ""], ["Whan", "Kirien", ""], ["Knap", "Wouter", ""], ["Schmeits", "Maurice", ""]]}, {"id": "1904.07199", "submitter": "Rob Brekelmans", "authors": "Rob Brekelmans, Daniel Moyer, Aram Galstyan, Greg Ver Steeg", "title": "Exact Rate-Distortion in Autoencoders via Echo Noise", "comments": "NeurIPS 2019; updated Gaussian baseline results, added\n  disentanglement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compression is at the heart of effective representation learning. However,\nlossy compression is typically achieved through simple parametric models like\nGaussian noise to preserve analytic tractability, and the limitations this\nimposes on learning are largely unexplored. Further, the Gaussian prior\nassumptions in models such as variational autoencoders (VAEs) provide only an\nupper bound on the compression rate in general. We introduce a new noise\nchannel, \\emph{Echo noise}, that admits a simple, exact expression for mutual\ninformation for arbitrary input distributions. The noise is constructed in a\ndata-driven fashion that does not require restrictive distributional\nassumptions. With its complex encoding mechanism and exact rate regularization,\nEcho leads to improved bounds on log-likelihood and dominates $\\beta$-VAEs\nacross the achievable range of rate-distortion trade-offs. Further, we show\nthat Echo noise can outperform flow-based methods without the need to train\nadditional distributional transformations.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 17:22:42 GMT"}, {"version": "v2", "created": "Sun, 2 Jun 2019 03:21:25 GMT"}, {"version": "v3", "created": "Thu, 14 Nov 2019 05:41:55 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Brekelmans", "Rob", ""], ["Moyer", "Daniel", ""], ["Galstyan", "Aram", ""], ["Steeg", "Greg Ver", ""]]}, {"id": "1904.07200", "submitter": "Tim Dockhorn", "authors": "Tim Dockhorn", "title": "A Discussion on Solving Partial Differential Equations using Neural\n  Networks", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can neural networks learn to solve partial differential equations (PDEs)? We\ninvestigate this question for two (systems of) PDEs, namely, the Poisson\nequation and the steady Navier--Stokes equations. The contributions of this\npaper are five-fold. (1) Numerical experiments show that small neural networks\n(< 500 learnable parameters) are able to accurately learn complex solutions for\nsystems of partial differential equations. (2) It investigates the influence of\nrandom weight initialization on the quality of the neural network approximate\nsolution and demonstrates how one can take advantage of this non-determinism\nusing ensemble learning. (3) It investigates the suitability of the loss\nfunction used in this work. (4) It studies the benefits and drawbacks of\nsolving (systems of) PDEs with neural networks compared to classical numerical\nmethods. (5) It proposes an exhaustive list of possible directions of future\nwork.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 17:23:58 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Dockhorn", "Tim", ""]]}, {"id": "1904.07272", "submitter": "Aleksandrs Slivkins", "authors": "Aleksandrs Slivkins", "title": "Introduction to Multi-Armed Bandits", "comments": "Published with Foundations and Trends(R) in Machine Learning,\n  November 2019. The present version is a revision of the \"Foundations and\n  Trends\" publication. It contains numerous edits for presentation and accuracy\n  (based in part on readers' feedback), updated and expanded literature\n  reviews, and some new exercises", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-armed bandits a simple but very powerful framework for algorithms that\nmake decisions over time under uncertainty. An enormous body of work has\naccumulated over the years, covered in several books and surveys. This book\nprovides a more introductory, textbook-like treatment of the subject. Each\nchapter tackles a particular line of work, providing a self-contained,\nteachable technical introduction and a brief review of the further\ndevelopments; many of the chapters conclude with exercises.\n  The book is structured as follows. The first four chapters are on IID\nrewards, from the basic model to impossibility results to Bayesian priors to\nLipschitz rewards. The next three chapters cover adversarial rewards, from the\nfull-feedback version to adversarial bandits to extensions with linear rewards\nand combinatorially structured actions. Chapter 8 is on contextual bandits, a\nmiddle ground between IID and adversarial bandits in which the change in reward\ndistributions is completely explained by observable contexts. The last three\nchapters cover connections to economics, from learning in repeated games to\nbandits with supply/budget constraints to exploration in the presence of\nincentives. The appendix provides sufficient background on concentration and\nKL-divergence.\n  The chapters on \"bandits with similarity information\", \"bandits with\nknapsacks\" and \"bandits and agents\" can also be consumed as standalone surveys\non the respective topics.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 18:17:01 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 20:45:01 GMT"}, {"version": "v3", "created": "Tue, 25 Jun 2019 14:39:03 GMT"}, {"version": "v4", "created": "Sun, 15 Sep 2019 02:06:22 GMT"}, {"version": "v5", "created": "Mon, 30 Sep 2019 00:15:42 GMT"}, {"version": "v6", "created": "Sat, 26 Jun 2021 20:15:32 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Slivkins", "Aleksandrs", ""]]}, {"id": "1904.07302", "submitter": "Hassan Ismail Fawaz", "authors": "Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Fran\\c{c}ois\n  Petitjean, Lhassane Idoumghar, Pierre-Alain Muller", "title": "Automatic alignment of surgical videos using kinematic data", "comments": "Accepted at AIME 2019", "journal-ref": null, "doi": "10.1007/978-3-030-21642-9_14", "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past one hundred years, the classic teaching methodology of \"see\none, do one, teach one\" has governed the surgical education systems worldwide.\nWith the advent of Operation Room 2.0, recording video, kinematic and many\nother types of data during the surgery became an easy task, thus allowing\nartificial intelligence systems to be deployed and used in surgical and medical\npractice. Recently, surgical videos has been shown to provide a structure for\npeer coaching enabling novice trainees to learn from experienced surgeons by\nreplaying those videos. However, the high inter-operator variability in\nsurgical gesture duration and execution renders learning from comparing novice\nto expert surgical videos a very difficult task. In this paper, we propose a\nnovel technique to align multiple videos based on the alignment of their\ncorresponding kinematic multivariate time series data. By leveraging the\nDynamic Time Warping measure, our algorithm synchronizes a set of videos in\norder to show the same gesture being performed at different speed. We believe\nthat the proposed approach is a valuable addition to the existing learning\ntools for surgery.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 16:46:08 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2019 12:27:13 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Fawaz", "Hassan Ismail", ""], ["Forestier", "Germain", ""], ["Weber", "Jonathan", ""], ["Petitjean", "Fran\u00e7ois", ""], ["Idoumghar", "Lhassane", ""], ["Muller", "Pierre-Alain", ""]]}, {"id": "1904.07320", "submitter": "Fang Su", "authors": "Fang Su, Hai-Yang Shang, Jing-Yan Wang", "title": "Low-Rank Deep Convolutional Neural Network for Multi-Task Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel multi-task learning method based on the\ndeep convolutional network. The proposed deep network has four convolutional\nlayers, three max-pooling layers, and two parallel fully connected layers. To\nadjust the deep network to multi-task learning problem, we propose to learn a\nlow-rank deep network so that the relation among different tasks can be\nexplored. We proposed to minimize the number of independent parameter rows of\none fully connected layer to explore the relations among different tasks, which\nis measured by the nuclear norm of the parameter of one fully connected layer,\nand seek a low-rank parameter matrix. Meanwhile, we also propose to regularize\nanother fully connected layer by sparsity penalty, so that the useful features\nlearned by the lower layers can be selected. The learning problem is solved by\nan iterative algorithm based on gradient descent and back-propagation\nalgorithms. The proposed algorithm is evaluated over benchmark data sets of\nmultiple face attribute prediction, multi-task natural language processing, and\njoint economics index predictions. The evaluation results show the advantage of\nthe low-rank deep CNN model over multi-task problems.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 14:20:01 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Su", "Fang", ""], ["Shang", "Hai-Yang", ""], ["Wang", "Jing-Yan", ""]]}, {"id": "1904.07328", "submitter": "Niki Gitinabard", "authors": "Niki Gitinabard, Yiqiao Xu, Sarah Heckman, Tiffany Barnes, Collin F.\n  Lynch", "title": "How Widely Can Prediction Models be Generalized? Performance Prediction\n  in Blended Courses", "comments": null, "journal-ref": "IEEE TLT, Special Issue on Early Prediction 2019", "doi": "10.1109/TLT.2019.2911832", "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blended courses that mix in-person instruction with online platforms are\nincreasingly popular in secondary education. These tools record a rich amount\nof data on students' study habits and social interactions. Prior research has\nshown that these metrics are correlated with students' performance in face to\nface classes. However, predictive models for blended courses are still limited\nand have not yet succeeded at early prediction or cross-class predictions even\nfor repeated offerings of the same course.\n  In this work, we use data from two offerings of two different undergraduate\ncourses to train and evaluate predictive models on student performance based\nupon persistent student characteristics including study habits and social\ninteractions. We analyze the performance of these models on the same offering,\non different offerings of the same course, and across courses to see how well\nthey generalize. We also evaluate the models on different segments of the\ncourses to determine how early reliable predictions can be made. This work\ntells us in part how much data is required to make robust predictions and how\ncross-class data may be used, or not, to boost model performance. The results\nof this study will help us better understand how similar the study habits,\nsocial activities, and the teamwork styles are across semesters for students in\neach performance category. These trained models also provide an avenue to\nimprove our existing support platforms to better support struggling students\nearly in the semester with the goal of providing timely intervention.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 20:50:46 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 18:25:31 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Gitinabard", "Niki", ""], ["Xu", "Yiqiao", ""], ["Heckman", "Sarah", ""], ["Barnes", "Tiffany", ""], ["Lynch", "Collin F.", ""]]}, {"id": "1904.07346", "submitter": "Markus Wulfmeier", "authors": "Markus Wulfmeier", "title": "Efficient Supervision for Robot Learning via Imitation, Simulation, and\n  Adaptation", "comments": "Dissertation Summary", "journal-ref": null, "doi": "10.1007/s13218-019-00587-0", "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent successes in machine learning have led to a shift in the design of\nautonomous systems, improving performance on existing tasks and rendering new\napplications possible. Data-focused approaches gain relevance across diverse,\nintricate applications when developing data collection and curation pipelines\nbecomes more effective than manual behaviour design. The following work aims at\nincreasing the efficiency of this pipeline in two principal ways: by utilising\nmore powerful sources of informative data and by extracting additional\ninformation from existing data. In particular, we target three orthogonal\nfronts: imitation learning, domain adaptation, and transfer from simulation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 22:19:25 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Wulfmeier", "Markus", ""]]}, {"id": "1904.07370", "submitter": "Alesia Chernikova", "authors": "Alesia Chernikova, Alina Oprea, Cristina Nita-Rotaru, BaekGyu Kim", "title": "Are Self-Driving Cars Secure? Evasion Attacks against Deep Neural\n  Networks for Steering Angle Prediction", "comments": "Preprint of the work accepted for publication at the IEEE Workshop on\n  the Internet of Safe Things, San Francisco, CA, USA, May 23, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have tremendous potential in advancing the vision\nfor self-driving cars. However, the security of DNN models in this context\nleads to major safety implications and needs to be better understood. We\nconsider the case study of steering angle prediction from camera images, using\nthe dataset from the 2014 Udacity challenge. We demonstrate for the first time\nadversarial testing-time attacks for this application for both classification\nand regression settings. We show that minor modifications to the camera image\n(an L2 distance of 0.82 for one of the considered models) result in\nmis-classification of an image to any class of attacker's choice. Furthermore,\nour regression attack results in a significant increase in Mean Square Error\n(MSE) by a factor of 69 in the worst case.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 23:52:09 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Chernikova", "Alesia", ""], ["Oprea", "Alina", ""], ["Nita-Rotaru", "Cristina", ""], ["Kim", "BaekGyu", ""]]}, {"id": "1904.07387", "submitter": "Po-Yu Kao", "authors": "Po-Yu Kao, Angela Zhang, Michael Goebel, Jefferson W. Chen, B.S.\n  Manjunath", "title": "Predicting Fluid Intelligence of Children using T1-weighted MR Images\n  and a StackNet", "comments": "8 pages, 2 figures, 3 tables, Accepted by MICCAI ABCD-NP Challenge\n  2019; Added NDA", "journal-ref": null, "doi": "10.1007/978-3-030-31901-4_2", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we utilize T1-weighted MR images and StackNet to predict fluid\nintelligence in adolescents. Our framework includes feature extraction, feature\nnormalization, feature denoising, feature selection, training a StackNet, and\npredicting fluid intelligence. The extracted feature is the distribution of\ndifferent brain tissues in different brain parcellation regions. The proposed\nStackNet consists of three layers and 11 models. Each layer uses the\npredictions from all previous layers including the input layer. The proposed\nStackNet is tested on a public benchmark Adolescent Brain Cognitive Development\nNeurocognitive Prediction Challenge 2019 and achieves a mean squared error of\n82.42 on the combined training and validation set with 10-fold\ncross-validation. In addition, the proposed StackNet also achieves a mean\nsquared error of 94.25 on the testing data. The source code is available on\nGitHub.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 01:04:20 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 02:31:27 GMT"}, {"version": "v3", "created": "Tue, 12 May 2020 02:25:31 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Kao", "Po-Yu", ""], ["Zhang", "Angela", ""], ["Goebel", "Michael", ""], ["Chen", "Jefferson W.", ""], ["Manjunath", "B. S.", ""]]}, {"id": "1904.07404", "submitter": "Changxi Liu", "authors": "Changxi Liu, Hailong Yang, Rujun Sun, Zhongzhi Luan, Lin Gan, Guangwen\n  Yang, Depei Qian", "title": "swTVM: Exploring the Automated Compilation for Deep Learning on Sunway\n  Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The flourish of deep learning frameworks and hardware platforms has been\ndemanding an efficient compiler that can shield the diversity in both software\nand hardware in order to provide application portability. Among the exiting\ndeep learning compilers, TVM is well known for its efficiency in code\ngeneration and optimization across diverse hardware devices. In the meanwhile,\nthe Sunway many-core processor renders itself as a competitive candidate for\nits attractive computational power in both scientific and deep learning\napplications. This paper combines the trends in these two directions.\nSpecifically, we propose swTVM that extends the original TVM to support\nahead-of-time compilation for architecture requiring cross-compilation such as\nSunway. In addition, we leverage the architecture features during the\ncompilation such as core group for massive parallelism, DMA for high bandwidth\nmemory transfer and local device memory for data locality, in order to generate\nefficient code for deep learning application on Sunway. The experimental\nresults show the ability of swTVM to automatically generate code for various\ndeep neural network models on Sunway. The performance of automatically\ngenerated code for AlexNet and VGG-19 by swTVM achieves 6.71x and 2.45x speedup\non average than hand-optimized OpenACC implementations on convolution and fully\nconnected layers respectively. This work is the first attempt from the compiler\nperspective to bridge the gap of deep learning and high performance\narchitecture particularly with productivity and efficiency in mind. We would\nlike to open source the implementation so that more people can embrace the\npower of deep learning compiler and Sunway many-core processor.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 02:13:05 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 13:09:43 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Liu", "Changxi", ""], ["Yang", "Hailong", ""], ["Sun", "Rujun", ""], ["Luan", "Zhongzhi", ""], ["Gan", "Lin", ""], ["Yang", "Guangwen", ""], ["Qian", "Depei", ""]]}, {"id": "1904.07451", "submitter": "Yash Goyal", "authors": "Yash Goyal and Ziyan Wu and Jan Ernst and Dhruv Batra and Devi Parikh\n  and Stefan Lee", "title": "Counterfactual Visual Explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we develop a technique to produce counterfactual visual\nexplanations. Given a 'query' image $I$ for which a vision system predicts\nclass $c$, a counterfactual visual explanation identifies how $I$ could change\nsuch that the system would output a different specified class $c'$. To do this,\nwe select a 'distractor' image $I'$ that the system predicts as class $c'$ and\nidentify spatial regions in $I$ and $I'$ such that replacing the identified\nregion in $I$ with the identified region in $I'$ would push the system towards\nclassifying $I$ as $c'$. We apply our approach to multiple image classification\ndatasets generating qualitative results showcasing the interpretability and\ndiscriminativeness of our counterfactual explanations. To explore the\neffectiveness of our explanations in teaching humans, we present machine\nteaching experiments for the task of fine-grained bird classification. We find\nthat users trained to distinguish bird species fare better when given access to\ncounterfactual explanations in addition to training examples.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 04:16:11 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 16:49:55 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Goyal", "Yash", ""], ["Wu", "Ziyan", ""], ["Ernst", "Jan", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""], ["Lee", "Stefan", ""]]}, {"id": "1904.07457", "submitter": "Zezhou Cheng", "authors": "Zezhou Cheng, Matheus Gadelha, Subhransu Maji, Daniel Sheldon", "title": "A Bayesian Perspective on the Deep Image Prior", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deep image prior was recently introduced as a prior for natural images.\nIt represents images as the output of a convolutional network with random\ninputs. For \"inference\", gradient descent is performed to adjust network\nparameters to make the output match observations. This approach yields good\nperformance on a range of image reconstruction tasks. We show that the deep\nimage prior is asymptotically equivalent to a stationary Gaussian process prior\nin the limit as the number of channels in each layer of the network goes to\ninfinity, and derive the corresponding kernel. This informs a Bayesian approach\nto inference. We show that by conducting posterior inference using stochastic\ngradient Langevin we avoid the need for early stopping, which is a drawback of\nthe current approach, and improve results for denoising and impainting tasks.\nWe illustrate these intuitions on a number of 1D and 2D signal reconstruction\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 04:39:29 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Cheng", "Zezhou", ""], ["Gadelha", "Matheus", ""], ["Maji", "Subhransu", ""], ["Sheldon", "Daniel", ""]]}, {"id": "1904.07462", "submitter": "Nhat Ho", "authors": "Nhat Ho and Tianyi Lin and Michael I. Jordan", "title": "On Structured Filtering-Clustering: Global Error Bound and Optimal\n  First-Order Algorithms", "comments": "The first two authors contributed equally to this work. This version\n  greatly improves and expands the results in the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the filtering-clustering problems have been a central topic\nin statistics and machine learning, especially the $\\ell_1$-trend filtering and\n$\\ell_2$-convex clustering problems. In practice, such structured problems are\ntypically solved by first-order algorithms despite the extremely\nill-conditioned structures of difference operator matrices. Inspired by the\ndesire to analyze the convergence rates of these algorithms, we show that for a\nlarge class of filtering-clustering problems, a \\textit{global error bound}\ncondition is satisfied for the dual filtering-clustering problems when a\ncertain regularization is chosen. Based on this result, we show that many\nfirst-order algorithms attain the \\textit{optimal rate of convergence} in\ndifferent settings. In particular, we establish a generalized dual gradient\nascent (GDGA) algorithmic framework with several subroutines. In deterministic\nsetting when the subroutine is accelerated gradient descent (AGD), the\nresulting algorithm attains the linear convergence. This linear convergence\nalso holds for the finite-sum setting in which the subroutine is the Katyusha\nalgorithm. We also demonstrate that the GDGA with stochastic gradient descent\n(SGD) subroutine attains the optimal rate of convergence up to the logarithmic\nfactor, shedding the light to the possibility of solving the\nfiltering-clustering problems efficiently in online setting. Experiments\nconducted on $\\ell_1$-trend filtering problems illustrate the favorable\nperformance of our algorithms over other competing algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 04:54:47 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 00:13:20 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Ho", "Nhat", ""], ["Lin", "Tianyi", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1904.07464", "submitter": "Yeqi Liu", "authors": "Yeqi Liu, Chuanyang Gong, Ling Yang, Yingyi Chen", "title": "DSTP-RNN: a dual-stage two-phase attention-based recurrent neural\n  networks for long-term and multivariate time series prediction", "comments": "6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-term prediction of multivariate time series is still an important but\nchallenging problem. The key to solve this problem is to capture the spatial\ncorrelations at the same time, the spatio-temporal relationships at different\ntimes and the long-term dependence of the temporal relationships between\ndifferent series. Attention-based recurrent neural networks (RNN) can\neffectively represent the dynamic spatio-temporal relationships between\nexogenous series and target series, but it only performs well in one-step time\nprediction and short-term time prediction. In this paper, inspired by human\nattention mechanism including the dual-stage two-phase (DSTP) model and the\ninfluence mechanism of target information and non-target information, we\npropose DSTP-based RNN (DSTP-RNN) and DSTP-RNN-2 respectively for long-term\ntime series prediction. Specifically, we first propose the DSTP-based structure\nto enhance the spatial correlations between exogenous series. The first phase\nproduces violent but decentralized response weight, while the second phase\nleads to stationary and concentrated response weight. Secondly, we employ\nmultiple attentions on target series to boost the long-term dependence.\nFinally, we study the performance of deep spatial attention mechanism and\nprovide experiment and interpretation. Our methods outperform nine baseline\nmethods on four datasets in the fields of energy, finance, environment and\nmedicine, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 05:03:45 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Liu", "Yeqi", ""], ["Gong", "Chuanyang", ""], ["Yang", "Ling", ""], ["Chen", "Yingyi", ""]]}, {"id": "1904.07482", "submitter": "Guangxiang Zhu", "authors": "Guangxiang Zhu, Jianhao Wang, Zhizhou Ren, Zichuan Lin, and Chongjie\n  Zhang", "title": "Object-Oriented Dynamics Learning through Multi-Level Abstraction", "comments": "Accepted to the Thirthy-Fourth AAAI Conference On Artificial\n  Intelligence (AAAI), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object-based approaches for learning action-conditioned dynamics has\ndemonstrated promise for generalization and interpretability. However, existing\napproaches suffer from structural limitations and optimization difficulties for\ncommon environments with multiple dynamic objects. In this paper, we present a\nnovel self-supervised learning framework, called Multi-level Abstraction\nObject-oriented Predictor (MAOP), which employs a three-level learning\narchitecture that enables efficient object-based dynamics learning from raw\nvisual observations. We also design a spatial-temporal relational reasoning\nmechanism for MAOP to support instance-level dynamics learning and handle\npartial observability. Our results show that MAOP significantly outperforms\nprevious methods in terms of sample efficiency and generalization over novel\nenvironments for learning environment models. We also demonstrate that learned\ndynamics models enable efficient planning in unseen environments, comparable to\ntrue environment models. In addition, MAOP learns semantically and visually\ninterpretable disentangled representations.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 06:01:17 GMT"}, {"version": "v2", "created": "Sat, 25 May 2019 03:46:55 GMT"}, {"version": "v3", "created": "Sat, 30 Nov 2019 10:29:10 GMT"}, {"version": "v4", "created": "Thu, 5 Dec 2019 06:05:28 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Zhu", "Guangxiang", ""], ["Wang", "Jianhao", ""], ["Ren", "Zhizhou", ""], ["Lin", "Zichuan", ""], ["Zhang", "Chongjie", ""]]}, {"id": "1904.07496", "submitter": "Chong Peng", "authors": "Chong Peng, Qiang Cheng", "title": "Discriminative Ridge Machine: A Classifier for High-Dimensional Data or\n  Imbalanced Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a discriminative regression approach to supervised\nclassification in this paper. It estimates a representation model while\naccounting for discriminativeness between classes, thereby enabling accurate\nderivation of categorical information. This new type of regression models\nextends existing models such as ridge, lasso, and group lasso through\nexplicitly incorporating discriminative information. As a special case we focus\non a quadratic model that admits a closed-form analytical solution. The\ncorresponding classifier is called discriminative regression machine (DRM).\nThree iterative algorithms are further established for the DRM to enhance the\nefficiency and scalability for real applications. Our approach and the\nalgorithms are applicable to general types of data including images,\nhigh-dimensional data, and imbalanced data. We compare the DRM with currently\nstate-of-the-art classifiers. Our extensive experimental results show superior\nperformance of the DRM and confirm the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 07:07:01 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2019 17:54:33 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Peng", "Chong", ""], ["Cheng", "Qiang", ""]]}, {"id": "1904.07497", "submitter": "Chong Peng", "authors": "Chong Peng, Chenglizhao Chen, Zhao Kang, Jianbo Li, Qiang Cheng", "title": "RES-PCA: A Scalable Approach to Recovering Low-rank Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust principal component analysis (RPCA) has drawn significant attentions\ndue to its powerful capability in recovering low-rank matrices as well as\nsuccessful appplications in various real world problems. The current\nstate-of-the-art algorithms usually need to solve singular value decomposition\nof large matrices, which generally has at least a quadratic or even cubic\ncomplexity. This drawback has limited the application of RPCA in solving real\nworld problems. To combat this drawback, in this paper we propose a new type of\nRPCA method, RES-PCA, which is linearly efficient and scalable in both data\nsize and dimension. For comparison purpose, AltProj, an existing scalable\napproach to RPCA requires the precise knowlwdge of the true rank; otherwise, it\nmay fail to recover low-rank matrices. By contrast, our method works with or\nwithout knowing the true rank; even when both methods work, our method is\nfaster. Extensive experiments have been performed and testified to the\neffectiveness of proposed method quantitatively and in visual quality, which\nsuggests that our method is suitable to be employed as a light-weight, scalable\ncomponent for RPCA in any application pipelines.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 07:07:44 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Peng", "Chong", ""], ["Chen", "Chenglizhao", ""], ["Kang", "Zhao", ""], ["Li", "Jianbo", ""], ["Cheng", "Qiang", ""]]}, {"id": "1904.07568", "submitter": "Minghao Yin", "authors": "Minghao Yin, Xiu Li, Yongbing Zhang, Shiqi Wang", "title": "On the Mathematical Understanding of ResNet with Feynman Path Integral", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG hep-th stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to understand Residual Network (ResNet) in a\nscientifically sound way by providing a bridge between ResNet and Feynman path\nintegral. In particular, we prove that the effect of residual block is\nequivalent to partial differential equation, and the ResNet transforming\nprocess can be equivalently converted to Feynman path integral. These\nconclusions greatly help us mathematically understand the advantage of ResNet\nin addressing the gradient vanishing issue. More importantly, our analyses\noffer a path integral view of ResNet, and demonstrate that the output of\ncertain network can be obtained by adding contributions of all paths. Moreover,\nthe contribution of each path is proportional to e^{-S}, where S is the action\ngiven by time integral of Lagrangian L. This lays the solid foundation in the\nunderstanding of ResNet, and provides insights in the future design of\nconvolutional neural network architecture. Based on these results, we have\ndesigned the network using partial differential operators, which further\nvalidates our theoritical analyses.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 09:59:18 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Yin", "Minghao", ""], ["Li", "Xiu", ""], ["Zhang", "Yongbing", ""], ["Wang", "Shiqi", ""]]}, {"id": "1904.07577", "submitter": "Taban Eslami", "authors": "Taban Eslami, Vahid Mirjalili, Alvis Fong, Angela Laird and Fahad\n  Saeed", "title": "ASD-DiagNet: A hybrid learning approach for detection of Autism Spectrum\n  Disorder using fMRI data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mental disorders such as Autism Spectrum Disorders (ASD) are heterogeneous\ndisorders that are notoriously difficult to diagnose, especially in children.\nThe current psychiatric diagnostic process is based purely on the behavioural\nobservation of symptomology (DSM-5/ICD-10) and may be prone to over-prescribing\nof drugs due to misdiagnosis. In order to move the field towards more\nquantitative fashion, we need advanced and scalable machine learning\ninfrastructure that will allow us to identify reliable biomarkers of mental\nhealth disorders. In this paper, we propose a framework called ASD-DiagNet for\nclassifying subjects with ASD from healthy subjects by using only fMRI data. We\ndesigned and implemented a joint learning procedure using an autoencoder and a\nsingle layer perceptron which results in improved quality of extracted features\nand optimized parameters for the model. Further, we designed and implemented a\ndata augmentation strategy, based on linear interpolation on available feature\nvectors, that allows us to produce synthetic datasets needed for training of\nmachine learning models. The proposed approach is evaluated on a public dataset\nprovided by Autism Brain Imaging Data Exchange including 1035 subjects coming\nfrom 17 different brain imaging centers. Our machine learning model outperforms\nother state of the art methods from 13 imaging centers with increase in\nclassification accuracy up to 20% with maximum accuracy of 80%. The machine\nlearning technique presented in this paper, in addition to yielding better\nquality, gives enormous advantages in terms of execution time (40 minutes vs. 6\nhours on other methods). The implemented code is available as GPL license on\nGitHub portal of our lab (https://github.com/pcdslab/ASD-DiagNet).\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 10:19:58 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Eslami", "Taban", ""], ["Mirjalili", "Vahid", ""], ["Fong", "Alvis", ""], ["Laird", "Angela", ""], ["Saeed", "Fahad", ""]]}, {"id": "1904.07594", "submitter": "Fabien Lauer", "authors": "Fabien Lauer (ABC)", "title": "Risk Bounds for Learning Multiple Components with Permutation-Invariant\n  Losses", "comments": null, "journal-ref": "23rd International Conference on Artificial Intelligence and\n  Statistics (AISTATS), 2020, Palermo, Italy", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a simple approach to derive efficient error bounds for\nlearning multiple components with sparsity-inducing regularization. We show\nthat for such regularization schemes, known decompositions of the Rademacher\ncomplexity over the components can be used in a more efficient manner to result\nin tighter bounds without too much effort. We give examples of application to\nswitching regression and center-based clustering/vector quantization. Then, the\ncomplete workflow is illustrated on the problem of subspace clustering, for\nwhich decomposition results were not previously available. For all these\nproblems, the proposed approach yields risk bounds with mild dependencies on\nthe number of components and completely removes this dependence for nonconvex\nregularization schemes that could not be handled by previous methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 11:08:23 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 15:32:57 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Lauer", "Fabien", "", "ABC"]]}, {"id": "1904.07612", "submitter": "Michael Michelashvili", "authors": "Michael Michelashvili, Lior Wolf", "title": "Speech Denoising by Accumulating Per-Frequency Modeling Fluctuations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a method for audio denoising that combines processing done in both\nthe time domain and the time-frequency domain. Given a noisy audio clip, the\nmethod trains a deep neural network to fit this signal. Since the fitting is\nonly partly successful and is able to better capture the underlying clean\nsignal than the noise, the output of the network helps to disentangle the clean\naudio from the rest of the signal. This is done by accumulating a fitting score\nper time-frequency bin and applying the time-frequency domain filtering based\non the obtained scores. The method is completely unsupervised and only trains\non the specific audio clip that is being denoised. Our experiments demonstrate\nfavorable performance in comparison to the literature methods. Our code and\nsamples are available at github.com/mosheman5/DNP and as supplementary. Index\nTerms: Audio denoising; Unsupervised learning\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 12:06:58 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 09:43:44 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2020 20:38:08 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Michelashvili", "Michael", ""], ["Wolf", "Lior", ""]]}, {"id": "1904.07686", "submitter": "Anahid Naghibzadeh-Jalali", "authors": "Anahid Jalali, Clemens Heistracher, Alexander Schindler, Bernhard\n  Haslhofer, Tanja Nemeth, Robert Glawar, Wilfried Sihn and Peter De Boer", "title": "Predicting Time-to-Failure of Plasma Etching Equipment using Machine\n  Learning", "comments": "8 pages, 10 figures, accepted in IEEEE/PHM 2019 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Predicting unscheduled breakdowns of plasma etching equipment can reduce\nmaintenance costs and production losses in the semiconductor industry. However,\nplasma etching is a complex procedure and it is hard to capture all relevant\nequipment properties and behaviors in a single physical model. Machine learning\noffers an alternative for predicting upcoming machine failures based on\nrelevant data points. In this paper, we describe three different machine\nlearning tasks that can be used for that purpose: (i) predicting\nTime-To-Failure (TTF), (ii) predicting health state, and (iii) predicting TTF\nintervals of an equipment. Our results show that trained machine learning\nmodels can outperform benchmarks resembling human judgments in all three tasks.\nThis suggests that machine learning offers a viable alternative to currently\ndeployed plasma etching equipment maintenance strategies and decision making\nprocesses.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 14:07:17 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Jalali", "Anahid", ""], ["Heistracher", "Clemens", ""], ["Schindler", "Alexander", ""], ["Haslhofer", "Bernhard", ""], ["Nemeth", "Tanja", ""], ["Glawar", "Robert", ""], ["Sihn", "Wilfried", ""], ["De Boer", "Peter", ""]]}, {"id": "1904.07688", "submitter": "Prateek Bansal", "authors": "Prateek Bansal, Rico Krueger, Michel Bierlaire, Ricardo A. Daziano,\n  Taha H. Rashidi", "title": "P\\'olygamma Data Augmentation to address Non-conjugacy in the Bayesian\n  Estimation of Mixed Multinomial Logit Models", "comments": "arXiv admin note: text overlap with arXiv:1904.03647", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard Gibbs sampler of Mixed Multinomial Logit (MMNL) models involves\nsampling from conditional densities of utility parameters using\nMetropolis-Hastings (MH) algorithm due to unavailability of conjugate prior for\nlogit kernel. To address this non-conjugacy concern, we propose the application\nof P\\'olygamma data augmentation (PG-DA) technique for the MMNL estimation. The\nposterior estimates of the augmented and the default Gibbs sampler are similar\nfor two-alternative scenario (binary choice), but we encounter empirical\nidentification issues in the case of more alternatives ($J \\geq 3$).\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 16:38:18 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Bansal", "Prateek", ""], ["Krueger", "Rico", ""], ["Bierlaire", "Michel", ""], ["Daziano", "Ricardo A.", ""], ["Rashidi", "Taha H.", ""]]}, {"id": "1904.07698", "submitter": "Fahad Sohrab", "authors": "Fahad Sohrab, Jenni Raitoharju, Alexandros Iosifidis, Moncef Gabbouj", "title": "Multimodal Subspace Support Vector Data Description", "comments": "26 pages manuscript (6 tables, 2 figures), 24 pages supplementary\n  material (27 tables, 10 figures). The manuscript and supplementary material\n  are combined as a single .pdf (50 pages) file", "journal-ref": "Pattern Recognition, 2020", "doi": "10.1016/j.patcog.2020.107648", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel method for projecting data from multiple\nmodalities to a new subspace optimized for one-class classification. The\nproposed method iteratively transforms the data from the original feature space\nof each modality to a new common feature space along with finding a joint\ncompact description of data coming from all the modalities. For data in each\nmodality, we define a separate transformation to map the data from the\ncorresponding feature space to the new optimized subspace by exploiting the\navailable information from the class of interest only. We also propose\ndifferent regularization strategies for the proposed method and provide both\nlinear and non-linear formulations. The proposed Multimodal Subspace Support\nVector Data Description outperforms all the competing methods using data from a\nsingle modality or fusing data from all modalities in four out of five\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 14:16:09 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 13:31:50 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Sohrab", "Fahad", ""], ["Raitoharju", "Jenni", ""], ["Iosifidis", "Alexandros", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "1904.07701", "submitter": "Alessandra Cabassi", "authors": "Alessandra Cabassi, Paul D. W. Kirk", "title": "Multiple kernel learning for integrative consensus clustering of 'omic\n  datasets", "comments": "Manuscript: 18 pages, 6 figures. Supplement: 29 pages, 19 figures.\n  This version contains additional simulation studies and comparisons to other\n  methods. For associated R code, see https://CRAN.R-project.org/package=klic\n  and https://github.com/acabassi/klic-pancancer-analysis", "journal-ref": null, "doi": "10.1093/bioinformatics/btaa593", "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Diverse applications - particularly in tumour subtyping - have demonstrated\nthe importance of integrative clustering techniques for combining information\nfrom multiple data sources. Cluster-Of-Clusters Analysis (COCA) is one such\napproach that has been widely applied in the context of tumour subtyping.\nHowever, the properties of COCA have never been systematically explored, and\nits robustness to the inclusion of noisy datasets, or datasets that define\nconflicting clustering structures, is unclear. We rigorously benchmark COCA,\nand present Kernel Learning Integrative Clustering (KLIC) as an alternative\nstrategy. KLIC frames the challenge of combining clustering structures as a\nmultiple kernel learning problem, in which different datasets each provide a\nweighted contribution to the final clustering. This allows the contribution of\nnoisy datasets to be down-weighted relative to more informative datasets. We\ncompare the performances of KLIC and COCA in a variety of situations through\nsimulation studies. We also present the output of KLIC and COCA in real data\napplications to cancer subtyping and transcriptional module discovery. R\npackages \"klic\" and \"coca\" are available on the Comprehensive R Archive\nNetwork.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 12:33:32 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 16:15:23 GMT"}, {"version": "v3", "created": "Mon, 27 Apr 2020 09:35:40 GMT"}, {"version": "v4", "created": "Mon, 18 May 2020 18:12:07 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Cabassi", "Alessandra", ""], ["Kirk", "Paul D. W.", ""]]}, {"id": "1904.07704", "submitter": "Tzeviya Sylvia Fuchs", "authors": "Yael Segal, Tzeviya Sylvia Fuchs, Joseph Keshet", "title": "SpeechYOLO: Detection and Localization of Speech Objects", "comments": null, "journal-ref": "Interspeech 2019, pp. 4210-4214", "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to apply object detection methods from the vision\ndomain on the speech recognition domain, by treating audio fragments as\nobjects. More specifically, we present SpeechYOLO, which is inspired by the\nYOLO algorithm for object detection in images. The goal of SpeechYOLO is to\nlocalize boundaries of utterances within the input signal, and to correctly\nclassify them. Our system is composed of a convolutional neural network, with a\nsimple least-mean-squares loss function. We evaluated the system on several\nkeyword spotting tasks, that include corpora of read speech and spontaneous\nspeech. Our system compares favorably with other algorithms trained for both\nlocalization and classification.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 15:47:14 GMT"}, {"version": "v2", "created": "Sun, 30 Jun 2019 09:14:46 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Segal", "Yael", ""], ["Fuchs", "Tzeviya Sylvia", ""], ["Keshet", "Joseph", ""]]}, {"id": "1904.07734", "submitter": "Gido van de Ven", "authors": "Gido M. van de Ven, Andreas S. Tolias", "title": "Three scenarios for continual learning", "comments": "Extended version of work presented at the NeurIPS Continual Learning\n  workshop (2018); 18 pages, 5 figures, 6 tables. Related to arXiv:1809.10635", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard artificial neural networks suffer from the well-known issue of\ncatastrophic forgetting, making continual or lifelong learning difficult for\nmachine learning. In recent years, numerous methods have been proposed for\ncontinual learning, but due to differences in evaluation protocols it is\ndifficult to directly compare their performance. To enable more structured\ncomparisons, we describe three continual learning scenarios based on whether at\ntest time task identity is provided and--in case it is not--whether it must be\ninferred. Any sequence of well-defined tasks can be performed according to each\nscenario. Using the split and permuted MNIST task protocols, for each scenario\nwe carry out an extensive comparison of recently proposed continual learning\nmethods. We demonstrate substantial differences between the three scenarios in\nterms of difficulty and in terms of how efficient different methods are. In\nparticular, when task identity must be inferred (i.e., class incremental\nlearning), we find that regularization-based approaches (e.g., elastic weight\nconsolidation) fail and that replaying representations of previous experiences\nseems required for solving this scenario.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 12:22:36 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["van de Ven", "Gido M.", ""], ["Tolias", "Andreas S.", ""]]}, {"id": "1904.07752", "submitter": "Stefan Klus", "authors": "Stefan Klus, Brooke E. Husic, Mattes Mollenhauer, Frank No\\'e", "title": "Kernel methods for detecting coherent structures in dynamical data", "comments": null, "journal-ref": null, "doi": "10.1063/1.5100267", "report-no": null, "categories": "math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We illustrate relationships between classical kernel-based dimensionality\nreduction techniques and eigendecompositions of empirical estimates of\nreproducing kernel Hilbert space (RKHS) operators associated with dynamical\nsystems. In particular, we show that kernel canonical correlation analysis\n(CCA) can be interpreted in terms of kernel transfer operators and that it can\nbe obtained by optimizing the variational approach for Markov processes (VAMP)\nscore. As a result, we show that coherent sets of particle trajectories can be\ncomputed by kernel CCA. We demonstrate the efficiency of this approach with\nseveral examples, namely the well-known Bickley jet, ocean drifter data, and a\nmolecular dynamics problem with a time-dependent potential. Finally, we propose\na straightforward generalization of dynamic mode decomposition (DMD) called\ncoherent mode decomposition (CMD). Our results provide a generic machine\nlearning approach to the computation of coherent sets with an objective score\nthat can be used for cross-validation and the comparison of different methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 15:11:20 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 03:30:03 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Klus", "Stefan", ""], ["Husic", "Brooke E.", ""], ["Mollenhauer", "Mattes", ""], ["No\u00e9", "Frank", ""]]}, {"id": "1904.07773", "submitter": "Junhao Wen", "authors": "Junhao Wen, Elina Thibeau-Sutre, Mauricio Diaz-Melo, Jorge\n  Samper-Gonzalez, Alexandre Routier, Simona Bottani, Didier Dormont, Stanley\n  Durrleman, Ninon Burgos and Olivier Colliot", "title": "Convolutional Neural Networks for Classification of Alzheimer's Disease:\n  Overview and Reproducible Evaluation", "comments": null, "journal-ref": "Medical Image Analysis, p.101694. 2020", "doi": "10.1016/j.media.2020.101694", "report-no": null, "categories": "cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over 30 papers have proposed to use convolutional neural network (CNN) for AD\nclassification from anatomical MRI. However, the classification performance is\ndifficult to compare across studies due to variations in components such as\nparticipant selection, image preprocessing or validation procedure. Moreover,\nthese studies are hardly reproducible because their frameworks are not publicly\naccessible and because implementation details are lacking. Lastly, some of\nthese papers may report a biased performance due to inadequate or unclear\nvalidation or model selection procedures. In the present work, we aim to\naddress these limitations through three main contributions. First, we performed\na systematic literature review and found that more than half of the surveyed\npapers may have suffered from data leakage. Our second contribution is the\nextension of our open-source framework for classification of AD using CNN and\nT1-weighted MRI. Finally, we used this framework to rigorously compare\ndifferent CNN architectures. The data was split into training/validation/test\nsets at the very beginning and only the training/validation sets were used for\nmodel selection. To avoid any overfitting, the test sets were left untouched\nuntil the end of the peer-review process. Overall, the different 3D approaches\n(3D-subject, 3D-ROI, 3D-patch) achieved similar performances while that of the\n2D slice approach was lower. Of note, the different CNN approaches did not\nperform better than a SVM with voxel-based features. The different approaches\ngeneralized well to similar populations but not to datasets with different\ninclusion criteria or demographical characteristics.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 15:46:06 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 15:25:01 GMT"}, {"version": "v3", "created": "Thu, 17 Oct 2019 00:35:50 GMT"}, {"version": "v4", "created": "Mon, 23 Mar 2020 17:52:55 GMT"}, {"version": "v5", "created": "Mon, 4 May 2020 15:16:23 GMT"}, {"version": "v6", "created": "Mon, 1 Jun 2020 01:39:56 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Wen", "Junhao", ""], ["Thibeau-Sutre", "Elina", ""], ["Diaz-Melo", "Mauricio", ""], ["Samper-Gonzalez", "Jorge", ""], ["Routier", "Alexandre", ""], ["Bottani", "Simona", ""], ["Dormont", "Didier", ""], ["Durrleman", "Stanley", ""], ["Burgos", "Ninon", ""], ["Colliot", "Olivier", ""]]}, {"id": "1904.07774", "submitter": "Basura Fernando", "authors": "Basura Fernando and Cheston Tan Yin Chet and Hakan Bilen", "title": "Weakly Supervised Gaussian Networks for Action Detection", "comments": "Accepted in WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting temporal extents of human actions in videos is a challenging\ncomputer vision problem that requires detailed manual supervision including\nframe-level labels. This expensive annotation process limits deploying action\ndetectors to a limited number of categories. We propose a novel method, called\nWSGN, that learns to detect actions from \\emph{weak supervision}, using only\nvideo-level labels. WSGN learns to exploit both video-specific and dataset-wide\nstatistics to predict relevance of each frame to an action category. This\nstrategy leads to significant gains in action detection for two standard\nbenchmarks THUMOS14 and Charades. Our method obtains excellent results compared\nto state-of-the-art methods that uses similar features and loss functions on\nTHUMOS14 dataset. Similarly, our weakly supervised method is only 0.3% mAP\nbehind a state-of-the-art supervised method on challenging Charades dataset for\naction localization.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 15:48:36 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 03:09:52 GMT"}, {"version": "v3", "created": "Thu, 2 Jan 2020 03:59:18 GMT"}, {"version": "v4", "created": "Mon, 6 Jan 2020 02:46:05 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Fernando", "Basura", ""], ["Chet", "Cheston Tan Yin", ""], ["Bilen", "Hakan", ""]]}, {"id": "1904.07785", "submitter": "Bingbing Xu", "authors": "Bingbing Xu, Huawei Shen, Qi Cao, Yunqi Qiu, Xueqi Cheng", "title": "Graph Wavelet Neural Network", "comments": null, "journal-ref": "ICLR(2019)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present graph wavelet neural network (GWNN), a novel graph convolutional\nneural network (CNN), leveraging graph wavelet transform to address the\nshortcomings of previous spectral graph CNN methods that depend on graph\nFourier transform. Different from graph Fourier transform, graph wavelet\ntransform can be obtained via a fast algorithm without requiring matrix\neigendecomposition with high computational cost. Moreover, graph wavelets are\nsparse and localized in vertex domain, offering high efficiency and good\ninterpretability for graph convolution. The proposed GWNN significantly\noutperforms previous spectral graph CNNs in the task of graph-based\nsemi-supervised classification on three benchmark datasets: Cora, Citeseer and\nPubmed.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 08:20:08 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Xu", "Bingbing", ""], ["Shen", "Huawei", ""], ["Cao", "Qi", ""], ["Qiu", "Yunqi", ""], ["Cheng", "Xueqi", ""]]}, {"id": "1904.07787", "submitter": "Yoram Louzoun", "authors": "Idan Benami, Keren Cohen, Oved Nagar, Yoram Louzoun", "title": "Topological based classification of paper domains using graph\n  convolutional networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main approaches for node classification in graphs are information\npropagation and the association of the class of the node with external\ninformation. State of the art methods merge these approaches through Graph\nConvolutional Networks. We here use the association of topological features of\nthe nodes with their class to predict this class. Moreover, combining\ntopological information with information propagation improves classification\naccuracy on the standard CiteSeer and Cora paper classification task.\nTopological features and information propagation produce results almost as good\nas text-based classification, without no textual or content information. We\npropose to represent the topology and information propagation through a GCN\nwith the neighboring training node classification as an input and the current\nnode classification as output. Such a formalism outperforms state of the art\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 14:04:11 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Benami", "Idan", ""], ["Cohen", "Keren", ""], ["Nagar", "Oved", ""], ["Louzoun", "Yoram", ""]]}, {"id": "1904.07830", "submitter": "Timothy Coleman", "authors": "Tim Coleman, Wei Peng, and Lucas Mentch", "title": "Scalable and Efficient Hypothesis Testing with Random Forests", "comments": "52 pages, 10 figures [fixed some critical typo's with Algorithm 1]", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Throughout the last decade, random forests have established themselves as\namong the most accurate and popular supervised learning methods. While their\nblack-box nature has made their mathematical analysis difficult, recent work\nhas established important statistical properties like consistency and\nasymptotic normality by considering subsampling in lieu of bootstrapping.\nThough such results open the door to traditional inference procedures, all\nformal methods suggested thus far place severe restrictions on the testing\nframework and their computational overhead precludes their practical scientific\nuse. Here we propose a permutation-style testing approach to formally assess\nfeature significance. We establish asymptotic validity of the test via\nexchangeability arguments and show that the test maintains high power with\norders of magnitude fewer computations. As importantly, the procedure scales\neasily to big data settings where large training and testing sets may be\nemployed without the need to construct additional models. Simulations and\napplications to ecological data where random forests have recently shown\npromise are provided.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 17:15:15 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 21:51:07 GMT"}, {"version": "v3", "created": "Fri, 6 Dec 2019 20:31:43 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Coleman", "Tim", ""], ["Peng", "Wei", ""], ["Mentch", "Lucas", ""]]}, {"id": "1904.07845", "submitter": "Gene-Ping Yang", "authors": "Gene-Ping Yang, Chao-I Tuan, Hung-Yi Lee, Lin-shan Lee", "title": "Improved Speech Separation with Time-and-Frequency Cross-domain Joint\n  Embedding and Clustering", "comments": "Submitted to Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech separation has been very successful with deep learning techniques.\nSubstantial effort has been reported based on approaches over spectrogram,\nwhich is well known as the standard time-and-frequency cross-domain\nrepresentation for speech signals. It is highly correlated to the phonetic\nstructure of speech, or \"how the speech sounds\" when perceived by human, but\nprimarily frequency domain features carrying temporal behaviour. Very\nimpressive work achieving speech separation over time domain was reported\nrecently, probably because waveforms in time domain may describe the different\nrealizations of speech in a more precise way than spectrogram. In this paper,\nwe propose a framework properly integrating the above two directions, hoping to\nachieve both purposes. We construct a time-and-frequency feature map by\nconcatenating the 1-dim convolution encoded feature map (for time domain) and\nthe spectrogram (for frequency domain), which was then processed by an\nembedding network and clustering approaches very similar to those used in time\nand frequency domain prior works. In this way, the information in the time and\nfrequency domains, as well as the interactions between them, can be jointly\nconsidered during embedding and clustering. Very encouraging results\n(state-of-the-art to our knowledge) were obtained with WSJ0-2mix dataset in\npreliminary experiments.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 17:48:59 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Yang", "Gene-Ping", ""], ["Tuan", "Chao-I", ""], ["Lee", "Hung-Yi", ""], ["Lee", "Lin-shan", ""]]}, {"id": "1904.07854", "submitter": "Avi Singh", "authors": "Avi Singh, Larry Yang, Kristian Hartikainen, Chelsea Finn, Sergey\n  Levine", "title": "End-to-End Robotic Reinforcement Learning without Reward Engineering", "comments": "Accepted to RSS 2019. 14 pages and 13 figures including references\n  and appendix. Website: https://sites.google.com/view/reward-learning-rl/home", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of deep neural network models and reinforcement learning\nalgorithms can make it possible to learn policies for robotic behaviors that\ndirectly read in raw sensory inputs, such as camera images, effectively\nsubsuming both estimation and control into one model. However, real-world\napplications of reinforcement learning must specify the goal of the task by\nmeans of a manually programmed reward function, which in practice requires\neither designing the very same perception pipeline that end-to-end\nreinforcement learning promises to avoid, or else instrumenting the environment\nwith additional sensors to determine if the task has been performed\nsuccessfully. In this paper, we propose an approach for removing the need for\nmanual engineering of reward specifications by enabling a robot to learn from a\nmodest number of examples of successful outcomes, followed by actively\nsolicited queries, where the robot shows the user a state and asks for a label\nto determine whether that state represents successful completion of the task.\nWhile requesting labels for every single state would amount to asking the user\nto manually provide the reward signal, our method requires labels for only a\ntiny fraction of the states seen during training, making it an efficient and\npractical approach for learning skills without manually engineered rewards. We\nevaluate our method on real-world robotic manipulation tasks where the\nobservations consist of images viewed by the robot's camera. In our\nexperiments, our method effectively learns to arrange objects, place books, and\ndrape cloth, directly from images and without any manually specified reward\nfunctions, and with only 1-4 hours of interaction with the real world.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 17:59:23 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 00:00:22 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Singh", "Avi", ""], ["Yang", "Larry", ""], ["Hartikainen", "Kristian", ""], ["Finn", "Chelsea", ""], ["Levine", "Sergey", ""]]}, {"id": "1904.07935", "submitter": "Gordon Moon", "authors": "Gordon E. Moon, Aravind Sukumaran-Rajam, Srinivasan Parthasarathy and\n  P. Sadayappan", "title": "PL-NMF: Parallel Locality-Optimized Non-negative Matrix Factorization", "comments": "11 pages, 5 tables, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-negative Matrix Factorization (NMF) is a key kernel for unsupervised\ndimension reduction used in a wide range of applications, including topic\nmodeling, recommender systems and bioinformatics. Due to the compute-intensive\nnature of applications that must perform repeated NMF, several parallel\nimplementations have been developed in the past. However, existing parallel NMF\nalgorithms have not addressed data locality optimizations, which are critical\nfor high performance since data movement costs greatly exceed the cost of\narithmetic/logic operations on current computer systems. In this paper, we\ndevise a parallel NMF algorithm based on the HALS (Hierarchical Alternating\nLeast Squares) scheme that incorporates algorithmic transformations to enhance\ndata locality. Efficient realizations of the algorithm on multi-core CPUs and\nGPUs are developed, demonstrating significant performance improvement over\nexisting state-of-the-art parallel NMF algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 19:18:37 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Moon", "Gordon E.", ""], ["Sukumaran-Rajam", "Aravind", ""], ["Parthasarathy", "Srinivasan", ""], ["Sadayappan", "P.", ""]]}, {"id": "1904.07964", "submitter": "Wentai Zhang", "authors": "Wentai Zhang, Zhangsihao Yang, Haoliang Jiang, Suyash Nigam, Soji\n  Yamakawa, Tomotake Furuhata, Kenji Shimada, Levent Burak Kara", "title": "3D Shape Synthesis for Conceptual Design and Optimization Using\n  Variational Autoencoders", "comments": "Preprint accepted by ASME IDETC/CIE 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a data-driven 3D shape design method that can learn a generative\nmodel from a corpus of existing designs, and use this model to produce a wide\nrange of new designs. The approach learns an encoding of the samples in the\ntraining corpus using an unsupervised variational autoencoder-decoder\narchitecture, without the need for an explicit parametric representation of the\noriginal designs. To facilitate the generation of smooth final surfaces, we\ndevelop a 3D shape representation based on a distance transformation of the\noriginal 3D data, rather than using the commonly utilized binary voxel\nrepresentation. Once established, the generator maps the latent space\nrepresentations to the high-dimensional distance transformation fields, which\nare then automatically surfaced to produce 3D representations amenable to\nphysics simulations or other objective function evaluation modules. We\ndemonstrate our approach for the computational design of gliders that are\noptimized to attain prescribed performance scores. Our results show that when\ncombined with genetic optimization, the proposed approach can generate a rich\nset of candidate concept designs that achieve prescribed functional goals, even\nwhen the original dataset has only a few or no solutions that achieve these\ngoals.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 20:26:53 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Zhang", "Wentai", ""], ["Yang", "Zhangsihao", ""], ["Jiang", "Haoliang", ""], ["Nigam", "Suyash", ""], ["Yamakawa", "Soji", ""], ["Furuhata", "Tomotake", ""], ["Shimada", "Kenji", ""], ["Kara", "Levent Burak", ""]]}, {"id": "1904.07965", "submitter": "Fabrizio Sebastiani", "authors": "Andrea Esuli, Alejandro Moreo, Fabrizio Sebastiani", "title": "Cross-Lingual Sentiment Quantification", "comments": "Identical to previous version, but for the abstract, which is now\n  identical to the one in the published version", "journal-ref": "Published in IEEE Intelligent Systems 35(3):106-114, 2020. The\n  present version is identical to the published one but for formatting", "doi": "10.1109/MIS.2020.2979203", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \\emph{Sentiment Quantification} (i.e., the task of estimating the relative\nfrequency of sentiment-related classes -- such as \\textsf{Positive} and\n\\textsf{Negative} -- in a set of unlabelled documents) is an important topic in\nsentiment analysis, as the study of sentiment-related quantities and trends\nacross a population is often of higher interest than the analysis of individual\ninstances. In this work we propose a method for \\emph{Cross-Lingual Sentiment\nQuantification}, the task of performing sentiment quantification when training\ndocuments are available for a source language $\\mathcal{S}$ but not for the\ntarget language $\\mathcal{T}$ for which sentiment quantification needs to be\nperformed. Cross-lingual sentiment quantification (and cross-lingual\n\\emph{text} quantification in general) has never been discussed before in the\nliterature; we establish baseline results for the binary case by combining\nstate-of-the-art quantification methods with methods capable of generating\ncross-lingual vectorial representations of the source and target documents\ninvolved. We present experimental results obtained on publicly available\ndatasets for cross-lingual sentiment classification; the results show that the\npresented methods can perform cross-lingual sentiment quantification with a\nsurprising level of accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 20:32:02 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 13:50:58 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Esuli", "Andrea", ""], ["Moreo", "Alejandro", ""], ["Sebastiani", "Fabrizio", ""]]}, {"id": "1904.07969", "submitter": "Lana Sinapayen", "authors": "Lana Sinapayen, Atsushi Noda", "title": "DNN Architecture for High Performance Prediction on Natural Videos Loses\n  Submodule's Ability to Learn Discrete-World Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is cognition a collection of loosely connected functions tuned to different\ntasks, or can there be a general learning algorithm? If such an hypothetical\ngeneral algorithm did exist, tuned to our world, could it adapt seamlessly to a\nworld with different laws of nature? We consider the theory that predictive\ncoding is such a general rule, and falsify it for one specific neural\narchitecture known for high-performance predictions on natural videos and\nreplication of human visual illusions: PredNet. Our results show that PredNet's\nhigh performance generalizes without retraining on a completely different\nnatural video dataset. Yet PredNet cannot be trained to reach even mediocre\naccuracy on an artificial video dataset created with the rules of the Game of\nLife (GoL). We also find that a submodule of PredNet, a Convolutional Neural\nNetwork trained alone, reaches perfect accuracy on the GoL while being mediocre\nfor natural videos, showing that PredNet's architecture itself is responsible\nfor both the high performance on natural videos and the loss of performance on\nthe GoL. Just as humans cannot predict the dynamics of the GoL, our results\nsuggest that there might be a trade-off between high performance on sensory\ninputs with different sets of rules.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 20:35:09 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Sinapayen", "Lana", ""], ["Noda", "Atsushi", ""]]}, {"id": "1904.07976", "submitter": "Asim Darwaish", "authors": "Asim Darwaish, Farid Na\\\"it-Abdesselam, Ashfaq Khokhar", "title": "Detection and Prediction of Cardiac Anomalies Using Wireless Body\n  Sensors and Bayesian Belief Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Intricating cardiac complexities are the primary factor associated with\nhealthcare costs and the highest cause of death rate in the world. However,\npreventive measures like the early detection of cardiac anomalies can prevent\nsevere cardiovascular arrests of varying complexities and can impose a\nsubstantial impact on healthcare cost. Encountering such scenarios usually the\nelectrocardiogram (ECG or EKG) is the first diagnostic choice of a medical\npractitioner or clinical staff to measure the electrical and muscular fitness\nof an individual heart. This paper presents a system which is capable of\nreading the recorded ECG and predict the cardiac anomalies without the\nintervention of a human expert. The paper purpose an algorithm which read and\nperform analysis on electrocardiogram datasets. The proposed architecture uses\nthe Discrete Wavelet Transform (DWT) at first place to perform preprocessing of\nECG data followed by undecimated Wavelet transform (UWT) to extract nine\nrelevant features which are of high interest to a cardiologist. The\nprobabilistic mode named Bayesian Network Classifier is trained using the\nextracted nine parameters on UCL arrhythmia dataset. The proposed system\nclassifies a recorded heartbeat into four classes using Bayesian Network\nclassifier and Tukey's box analysis. The four classes for the prediction of a\nheartbeat are (a) Normal Beat, (b) Premature Ventricular Contraction (PVC) (c)\nPremature Atrial Contraction (PAC) and (d) Myocardial Infarction. The results\nof experimental setup depict that the proposed system has achieved an average\naccuracy of 96.6 for PAC\\% 92.8\\% for MI and 87\\% for PVC, with an average\nerror rate of 3.3\\% for PAC, 6\\% for MI and 12.5\\% for PVC on real\nelectrocardiogram datasets including Physionet and European ST-T Database\n(EDB).\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 20:44:43 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Darwaish", "Asim", ""], ["Na\u00eft-Abdesselam", "Farid", ""], ["Khokhar", "Ashfaq", ""]]}, {"id": "1904.07980", "submitter": "George Adam", "authors": "George Adam, Petr Smirnov, Benjamin Haibe-Kains, Anna Goldenberg", "title": "Reducing Adversarial Example Transferability Using Gradient\n  Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning algorithms have increasingly been shown to lack robustness to\nsimple adversarial examples (AdvX). An equally troubling observation is that\nthese adversarial examples transfer between different architectures trained on\ndifferent datasets. We investigate the transferability of adversarial examples\nbetween models using the angle between the input-output Jacobians of different\nmodels. To demonstrate the relevance of this approach, we perform case studies\nthat involve jointly training pairs of models. These case studies empirically\njustify the theoretical intuitions for why the angle between gradients is a\nfundamental quantity in AdvX transferability. Furthermore, we consider the\nasymmetry of AdvX transferability between two models of the same architecture\nand explain it in terms of differences in gradient norms between the models.\nLastly, we provide a simple modification to existing training setups that\nreduces transferability of adversarial examples between pairs of models.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 20:49:49 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Adam", "George", ""], ["Smirnov", "Petr", ""], ["Haibe-Kains", "Benjamin", ""], ["Goldenberg", "Anna", ""]]}, {"id": "1904.07990", "submitter": "Gunnar R\\\"atsch", "authors": "Stephanie L. Hyland and Martin Faltys and Matthias H\\\"user and Xinrui\n  Lyu and Thomas Gumbsch and Crist\\'obal Esteban and Christian Bock and Max\n  Horn and Michael Moor and Bastian Rieck and Marc Zimmermann and Dean Bodenham\n  and Karsten Borgwardt and Gunnar R\\\"atsch and Tobias M. Merz", "title": "Machine learning for early prediction of circulatory failure in the\n  intensive care unit", "comments": "5 main figures, 1 main table, 13 supplementary figures, 5\n  supplementary tables; 250ppi images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intensive care clinicians are presented with large quantities of patient\ninformation and measurements from a multitude of monitoring systems. The\nlimited ability of humans to process such complex information hinders\nphysicians to readily recognize and act on early signs of patient\ndeterioration. We used machine learning to develop an early warning system for\ncirculatory failure based on a high-resolution ICU database with 240 patient\nyears of data. This automatic system predicts 90.0% of circulatory failure\nevents (prevalence 3.1%), with 81.8% identified more than two hours in advance,\nresulting in an area under the receiver operating characteristic curve of 94.0%\nand area under the precision-recall curve of 63.0%. The model was externally\nvalidated in a large independent patient cohort.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 21:18:17 GMT"}, {"version": "v2", "created": "Fri, 19 Apr 2019 16:02:18 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Hyland", "Stephanie L.", ""], ["Faltys", "Martin", ""], ["H\u00fcser", "Matthias", ""], ["Lyu", "Xinrui", ""], ["Gumbsch", "Thomas", ""], ["Esteban", "Crist\u00f3bal", ""], ["Bock", "Christian", ""], ["Horn", "Max", ""], ["Moor", "Michael", ""], ["Rieck", "Bastian", ""], ["Zimmermann", "Marc", ""], ["Bodenham", "Dean", ""], ["Borgwardt", "Karsten", ""], ["R\u00e4tsch", "Gunnar", ""], ["Merz", "Tobias M.", ""]]}, {"id": "1904.07998", "submitter": "Yue Zhao", "authors": "Colin Wan, Zheng Li, Alicia Guo, Yue Zhao", "title": "SynC: A Unified Framework for Generating Synthetic Population with\n  Gaussian Copula", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic population generation is the process of combining multiple\nsocioeconomic and demographic datasets from different sources and/or\ngranularity levels, and downscaling them to an individual level. Although it is\na fundamental step for many data science tasks, an efficient and standard\nframework is absent. In this study, we propose a multi-stage framework called\nSynC (Synthetic Population via Gaussian Copula) to fill the gap. SynC first\nremoves potential outliers in the data and then fits the filtered data with a\nGaussian copula model to correctly capture dependencies and marginal\ndistributions of sampled survey data. Finally, SynC leverages predictive models\nto merge datasets into one and then scales them accordingly to match the\nmarginal constraints. We make three key contributions in this work: 1) propose\na novel framework for generating individual level data from aggregated data\nsources by combining state-of-the-art machine learning and statistical\ntechniques, 2) demonstrate its value as a feature engineering tool, as well as\nan alternative to data collection in situations where gathering is difficult\nthrough two real-world datasets, 3) release an easy-to-use framework\nimplementation for reproducibility, and 4) ensure the methodology is scalable\nat the production level and can easily incorporate new data.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 22:10:19 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 01:48:59 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Wan", "Colin", ""], ["Li", "Zheng", ""], ["Guo", "Alicia", ""], ["Zhao", "Yue", ""]]}, {"id": "1904.08030", "submitter": "Chao Li", "authors": "Chao Li, Zhiyuan Liu, Mengmeng Wu, Yuchi Xu, Pipei Huang, Huan Zhao,\n  Guoliang Kang, Qiwei Chen, Wei Li, Dik Lun Lee", "title": "Multi-Interest Network with Dynamic Routing for Recommendation at Tmall", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industrial recommender systems usually consist of the matching stage and the\nranking stage, in order to handle the billion-scale of users and items. The\nmatching stage retrieves candidate items relevant to user interests, while the\nranking stage sorts candidate items by user interests. Thus, the most critical\nability is to model and represent user interests for either stage. Most of the\nexisting deep learning-based models represent one user as a single vector which\nis insufficient to capture the varying nature of user's interests. In this\npaper, we approach this problem from a different view, to represent one user\nwith multiple vectors encoding the different aspects of the user's interests.\nWe propose the Multi-Interest Network with Dynamic routing (MIND) for dealing\nwith user's diverse interests in the matching stage. Specifically, we design a\nmulti-interest extractor layer based on capsule routing mechanism, which is\napplicable for clustering historical behaviors and extracting diverse\ninterests. Furthermore, we develop a technique named label-aware attention to\nhelp learn a user representation with multiple vectors. Through extensive\nexperiments on several public benchmarks and one large-scale industrial dataset\nfrom Tmall, we demonstrate that MIND can achieve superior performance than\nstate-of-the-art methods for recommendation. Currently, MIND has been deployed\nfor handling major online traffic at the homepage on Mobile Tmall App.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 00:39:24 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Li", "Chao", ""], ["Liu", "Zhiyuan", ""], ["Wu", "Mengmeng", ""], ["Xu", "Yuchi", ""], ["Huang", "Pipei", ""], ["Zhao", "Huan", ""], ["Kang", "Guoliang", ""], ["Chen", "Qiwei", ""], ["Li", "Wei", ""], ["Lee", "Dik Lun", ""]]}, {"id": "1904.08034", "submitter": "Brenden Lake", "authors": "Brenden M. Lake and Steven T. Piantadosi", "title": "People infer recursive visual concepts from just a few examples", "comments": "In press at \"Computational Brain & Behavior\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has made major advances in categorizing objects in images,\nyet the best algorithms miss important aspects of how people learn and think\nabout categories. People can learn richer concepts from fewer examples,\nincluding causal models that explain how members of a category are formed.\nHere, we explore the limits of this human ability to infer causal \"programs\" --\nlatent generating processes with nontrivial algorithmic properties -- from one,\ntwo, or three visual examples. People were asked to extrapolate the programs in\nseveral ways, for both classifying and generating new examples. As a theory of\nthese inductive abilities, we present a Bayesian program learning model that\nsearches the space of programs for the best explanation of the observations.\nAlthough variable, people's judgments are broadly consistent with the model and\ninconsistent with several alternatives, including a pre-trained deep neural\nnetwork for object recognition, indicating that people can learn and reason\nwith rich algorithmic abstractions from sparse input data.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 00:45:05 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 15:26:40 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Lake", "Brenden M.", ""], ["Piantadosi", "Steven T.", ""]]}, {"id": "1904.08035", "submitter": "Binxuan Huang", "authors": "Binxuan Huang, Kathleen M. Carley", "title": "Residual or Gate? Towards Deeper Graph Neural Networks for Inductive\n  Graph Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of node representation learning with\ngraph neural networks. We present a graph neural network class named recurrent\ngraph neural network (RGNN), that address the shortcomings of prior methods. By\nusing recurrent units to capture the long-term dependency across layers, our\nmethods can successfully identify important information during recursive\nneighborhood expansion. In our experiments, we show that our model class\nachieves state-of-the-art results on three benchmarks: the Pubmed, Reddit, and\nPPI network datasets. Our in-depth analyses also demonstrate that incorporating\nrecurrent units is a simple yet effective method to prevent noisy information\nin graphs, which enables a deeper graph neural network.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 00:46:20 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 02:38:19 GMT"}, {"version": "v3", "created": "Mon, 26 Aug 2019 17:45:11 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Huang", "Binxuan", ""], ["Carley", "Kathleen M.", ""]]}, {"id": "1904.08049", "submitter": "Yanjun  Qi Dr.", "authors": "Jack Lanchantin, Arshdeep Sekhon and Yanjun Qi", "title": "Neural Message Passing for Multi-Label Classification", "comments": "19pages. We provide our code and datasets at\n  https://github.com/QData/LaMP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label classification (MLC) is the task of assigning a set of target\nlabels for a given sample. Modeling the combinatorial label interactions in MLC\nhas been a long-haul challenge. We propose Label Message Passing (LaMP) Neural\nNetworks to efficiently model the joint prediction of multiple labels. LaMP\ntreats labels as nodes on a label-interaction graph and computes the hidden\nrepresentation of each label node conditioned on the input using\nattention-based neural message passing. Attention enables LaMP to assign\ndifferent importance to neighbor nodes per label, learning how labels interact\n(implicitly). The proposed models are simple, accurate, interpretable,\nstructure-agnostic, and applicable for predicting dense labels since LaMP is\nincredibly parallelizable. We validate the benefits of LaMP on seven real-world\nMLC datasets, covering a broad spectrum of input/output types and outperforming\nthe state-of-the-art results. Notably, LaMP enables intuitive interpretation of\nhow classifying each label depends on the elements of a sample and at the same\ntime rely on its interaction with other labels. We provide our code and\ndatasets at https://github.com/QData/LaMP\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 01:58:17 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Lanchantin", "Jack", ""], ["Sekhon", "Arshdeep", ""], ["Qi", "Yanjun", ""]]}, {"id": "1904.08050", "submitter": "Najeeb Khan", "authors": "Najeeb Khan and Ian Stavness", "title": "Sparseout: Controlling Sparsity in Deep Networks", "comments": "Code: https://github.com/najeebkhan/sparseout", "journal-ref": null, "doi": "10.1007/978-3-030-18305-9_24", "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dropout is commonly used to help reduce overfitting in deep neural networks.\nSparsity is a potentially important property of neural networks, but is not\nexplicitly controlled by Dropout-based regularization. In this work, we propose\nSparseout a simple and efficient variant of Dropout that can be used to control\nthe sparsity of the activations in a neural network. We theoretically prove\nthat Sparseout is equivalent to an $L_q$ penalty on the features of a\ngeneralized linear model and that Dropout is a special case of Sparseout for\nneural networks. We empirically demonstrate that Sparseout is computationally\ninexpensive and is able to control the desired level of sparsity in the\nactivations. We evaluated Sparseout on image classification and language\nmodelling tasks to see the effect of sparsity on these tasks. We found that\nsparsity of the activations is favorable for language modelling performance\nwhile image classification benefits from denser activations. Sparseout provides\na way to investigate sparsity in state-of-the-art deep learning models. Source\ncode for Sparseout could be found at\n\\url{https://github.com/najeebkhan/sparseout}.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 02:10:25 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Khan", "Najeeb", ""], ["Stavness", "Ian", ""]]}, {"id": "1904.08064", "submitter": "Feng Li", "authors": "Xixi Li, Yanfei Kang, Feng Li", "title": "Forecasting with time series imaging", "comments": null, "journal-ref": "Expert Systems with Applications (2020)", "doi": "10.1016/j.eswa.2020.113680", "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature-based time series representations have attracted substantial\nattention in a wide range of time series analysis methods. Recently, the use of\ntime series features for forecast model averaging has been an emerging research\nfocus in the forecasting community. Nonetheless, most of the existing\napproaches depend on the manual choice of an appropriate set of features.\nExploiting machine learning methods to extract features from time series\nautomatically becomes crucial in state-of-the-art time series analysis. In this\npaper, we introduce an automated approach to extract time series features based\non time series imaging. We first transform time series into recurrence plots,\nfrom which local features can be extracted using computer vision algorithms.\nThe extracted features are used for forecast model averaging. Our experiments\nshow that forecasting based on automatically extracted features, with less\nhuman intervention and a more comprehensive view of the raw time series data,\nyields highly comparable performances with the best methods in the largest\nforecasting competition dataset (M4) and outperforms the top methods in the\nTourism forecasting competition dataset.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 03:18:45 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 03:33:57 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2020 03:13:12 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Li", "Xixi", ""], ["Kang", "Yanfei", ""], ["Li", "Feng", ""]]}, {"id": "1904.08067", "submitter": "Kamran Kowsari", "authors": "Kamran Kowsari, Kiana Jafari Meimandi, Mojtaba Heidarysafa, Sanjana\n  Mendu, Laura E. Barnes, Donald E. Brown", "title": "Text Classification Algorithms: A Survey", "comments": null, "journal-ref": null, "doi": "10.3390/info10040150", "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, there has been an exponential growth in the number of\ncomplex documents and texts that require a deeper understanding of machine\nlearning methods to be able to accurately classify texts in many applications.\nMany machine learning approaches have achieved surpassing results in natural\nlanguage processing. The success of these learning algorithms relies on their\ncapacity to understand complex models and non-linear relationships within data.\nHowever, finding suitable structures, architectures, and techniques for text\nclassification is a challenge for researchers. In this paper, a brief overview\nof text classification algorithms is discussed. This overview covers different\ntext feature extractions, dimensionality reduction methods, existing algorithms\nand techniques, and evaluations methods. Finally, the limitations of each\ntechnique and their application in the real-world problem are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 03:29:05 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 01:20:53 GMT"}, {"version": "v3", "created": "Thu, 25 Apr 2019 18:28:33 GMT"}, {"version": "v4", "created": "Tue, 25 Jun 2019 22:51:18 GMT"}, {"version": "v5", "created": "Wed, 20 May 2020 16:27:00 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Kowsari", "Kamran", ""], ["Meimandi", "Kiana Jafari", ""], ["Heidarysafa", "Mojtaba", ""], ["Mendu", "Sanjana", ""], ["Barnes", "Laura E.", ""], ["Brown", "Donald E.", ""]]}, {"id": "1904.08082", "submitter": "InYeop Lee", "authors": "Junhyun Lee, Inyeop Lee, Jaewoo Kang", "title": "Self-Attention Graph Pooling", "comments": "10 pages, 3 figures, 4 tables. Accepted to ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced methods of applying deep learning to structured data such as graphs\nhave been proposed in recent years. In particular, studies have focused on\ngeneralizing convolutional neural networks to graph data, which includes\nredefining the convolution and the downsampling (pooling) operations for\ngraphs. The method of generalizing the convolution operation to graphs has been\nproven to improve performance and is widely used. However, the method of\napplying downsampling to graphs is still difficult to perform and has room for\nimprovement. In this paper, we propose a graph pooling method based on\nself-attention. Self-attention using graph convolution allows our pooling\nmethod to consider both node features and graph topology. To ensure a fair\ncomparison, the same training procedures and model architectures were used for\nthe existing pooling methods and our method. The experimental results\ndemonstrate that our method achieves superior graph classification performance\non the benchmark datasets using a reasonable number of parameters.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 05:02:24 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 01:38:18 GMT"}, {"version": "v3", "created": "Wed, 24 Apr 2019 06:55:43 GMT"}, {"version": "v4", "created": "Thu, 13 Jun 2019 05:54:07 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Lee", "Junhyun", ""], ["Lee", "Inyeop", ""], ["Kang", "Jaewoo", ""]]}, {"id": "1904.08089", "submitter": "Yuxian Qiu", "authors": "Yuxian Qiu, Jingwen Leng, Cong Guo, Quan Chen, Chao Li, Minyi Guo and\n  Yuhao Zhu", "title": "Adversarial Defense Through Network Profiling Based Path Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, researchers have started decomposing deep neural network models\naccording to their semantics or functions. Recent work has shown the\neffectiveness of decomposed functional blocks for defending adversarial\nattacks, which add small input perturbation to the input image to fool the DNN\nmodels. This work proposes a profiling-based method to decompose the DNN models\nto different functional blocks, which lead to the effective path as a new\napproach to exploring DNNs' internal organization. Specifically, the per-image\neffective path can be aggregated to the class-level effective path, through\nwhich we observe that adversarial images activate effective path different from\nnormal images. We propose an effective path similarity-based method to detect\nadversarial images with an interpretable model, which achieve better accuracy\nand broader applicability than the state-of-the-art technique.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 05:51:24 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 04:40:40 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Qiu", "Yuxian", ""], ["Leng", "Jingwen", ""], ["Guo", "Cong", ""], ["Chen", "Quan", ""], ["Li", "Chao", ""], ["Guo", "Minyi", ""], ["Zhu", "Yuhao", ""]]}, {"id": "1904.08092", "submitter": "Siddharth Srivastava", "authors": "Siddharth Srivastava, Sumit Soman, Astha Rai", "title": "An Online Learning Approach for Dengue Fever Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel approach for dengue fever classification based\non online learning paradigms. The proposed approach is suitable for practical\nimplementation as it enables learning using only a few training samples. With\ntime, the proposed approach is capable of learning incrementally from the data\ncollected without need for retraining the model or redeployment of the\nprediction engine. Additionally, we also provide a comprehensive evaluation of\nmachine learning methods for prediction of dengue fever. The input to the\nproposed pipeline comprises of recorded patient symptoms and diagnostic\ninvestigations. Offline classifier models have been employed to obtain baseline\nscores to establish that the feature set is optimal for classification of\ndengue. The primary benefit of the online detection model presented in the\npaper is that it has been established to effectively identify patients with\nhigh likelihood of dengue disease, and experiments on scalability in terms of\nnumber of training and test samples validate the use of the proposed model.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 05:57:52 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Srivastava", "Siddharth", ""], ["Soman", "Sumit", ""], ["Rai", "Astha", ""]]}, {"id": "1904.08102", "submitter": "Kevin Yang", "authors": "Kevin K. Yang, Yuxin Chen, Alycia Lee, Yisong Yue", "title": "Batched Stochastic Bayesian Optimization via Combinatorial Constraints\n  Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many high-throughput experimental design settings, such as those common in\nbiochemical engineering, batched queries are more cost effective than\none-by-one sequential queries. Furthermore, it is often not possible to\ndirectly choose items to query. Instead, the experimenter specifies a set of\nconstraints that generates a library of possible items, which are then selected\nstochastically. Motivated by these considerations, we investigate \\emph{Batched\nStochastic Bayesian Optimization} (BSBO), a novel Bayesian optimization scheme\nfor choosing the constraints in order to guide exploration towards items with\ngreater utility. We focus on \\emph{site-saturation mutagenesis}, a prototypical\nsetting of BSBO in biochemical engineering, and propose a natural objective\nfunction for this problem. Importantly, we show that our objective function can\nbe efficiently decomposed as a difference of submodular functions (DS), which\nallows us to employ DS optimization tools to greedily identify sets of\nconstraints that increase the likelihood of finding items with high utility.\nOur experimental results show that our algorithm outperforms common heuristics\non both synthetic and two real protein datasets.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 06:30:58 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Yang", "Kevin K.", ""], ["Chen", "Yuxin", ""], ["Lee", "Alycia", ""], ["Yue", "Yisong", ""]]}, {"id": "1904.08117", "submitter": "Hua Wei", "authors": "Hua Wei, Guanjie Zheng, Vikash Gayah, Zhenhui Li", "title": "A Survey on Traffic Signal Control Methods", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic signal control is an important and challenging real-world problem,\nwhich aims to minimize the travel time of vehicles by coordinating their\nmovements at the road intersections. Current traffic signal control systems in\nuse still rely heavily on oversimplified information and rule-based methods,\nalthough we now have richer data, more computing power and advanced methods to\ndrive the development of intelligent transportation. With the growing interest\nin intelligent transportation using machine learning methods like reinforcement\nlearning, this survey covers the widely acknowledged transportation approaches\nand a comprehensive list of recent literature on reinforcement for traffic\nsignal control. We hope this survey can foster interdisciplinary research on\nthis important topic.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 08:07:29 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 22:23:49 GMT"}, {"version": "v3", "created": "Thu, 16 Jan 2020 16:29:35 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Wei", "Hua", ""], ["Zheng", "Guanjie", ""], ["Gayah", "Vikash", ""], ["Li", "Zhenhui", ""]]}, {"id": "1904.08129", "submitter": "Yuji Kanagawa", "authors": "Yuji Kanagawa and Tomoyuki Kaneko", "title": "Rogue-Gym: A New Challenge for Generalization in Reinforcement Learning", "comments": "8 pages, 14 figures, 4 tables, accepted to IEEE COG 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Rogue-Gym, a simple and classic style roguelike\ngame built for evaluating generalization in reinforcement learning (RL).\nCombined with the recent progress of deep neural networks, RL has successfully\ntrained human-level agents without human knowledge in many games such as those\nfor Atari 2600. However, it has been pointed out that agents trained with RL\nmethods often overfit the training environment, and they work poorly in\nslightly different environments. To investigate this problem, some research\nenvironments with procedural content generation have been proposed. Following\nthese studies, we propose the use of roguelikes as a benchmark for evaluating\nthe generalization ability of RL agents. In our Rogue-Gym, agents need to\nexplore dungeons that are structured differently each time they start a new\ngame. Thanks to the very diverse structures of the dungeons, we believe that\nthe generalization benchmark of Rogue-Gym is sufficiently fair. In our\nexperiments, we evaluate a standard reinforcement learning method, PPO, with\nand without enhancements for generalization. The results show that some\nenhancements believed to be effective fail to mitigate the overfitting in\nRogue-Gym, although others slightly improve the generalization ability.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 08:31:06 GMT"}, {"version": "v2", "created": "Sat, 1 Jun 2019 03:39:07 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Kanagawa", "Yuji", ""], ["Kaneko", "Tomoyuki", ""]]}, {"id": "1904.08144", "submitter": "Jaechang Lim", "authors": "Jaechang Lim, Seongok Ryu, Kyubyong Park, Yo Joong Choe, Jiyeon Ham,\n  and Woo Youn Kim", "title": "Predicting drug-target interaction using 3D structure-embedded graph\n  representations from graph neural networks", "comments": "20 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate prediction of drug-target interaction (DTI) is essential for in\nsilico drug design. For the purpose, we propose a novel approach for predicting\nDTI using a GNN that directly incorporates the 3D structure of a protein-ligand\ncomplex. We also apply a distance-aware graph attention algorithm with gate\naugmentation to increase the performance of our model. As a result, our model\nshows better performance than docking and other deep learning methods for both\nvirtual screening and pose prediction. In addition, our model can reproduce the\nnatural population distribution of active molecules and inactive molecules.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 09:03:54 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Lim", "Jaechang", ""], ["Ryu", "Seongok", ""], ["Park", "Kyubyong", ""], ["Choe", "Yo Joong", ""], ["Ham", "Jiyeon", ""], ["Kim", "Woo Youn", ""]]}, {"id": "1904.08155", "submitter": "Dominique Vaufreydaz", "authors": "Justin Le Louedec (PERVASIVE), Thomas Guntz (PERVASIVE), James Crowley\n  (PERVASIVE), Dominique Vaufreydaz (PERVASIVE)", "title": "Deep learning investigation for chess player attention prediction using\n  eye-tracking and game data", "comments": null, "journal-ref": "ACM Symposium On Eye Tracking Research & Applications (ETRA 2019),\n  Jun 2019, Denver, United States.", "doi": "10.1145/3314111.3319827", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article reports on an investigation of the use of convolutional neural\nnetworks to predict the visual attention of chess players. The visual attention\nmodel described in this article has been created to generate saliency maps that\ncapture hierarchical and spatial features of chessboard, in order to predict\nthe probability fixation for individual pixels Using a skip-layer architecture\nof an autoencoder, with a unified decoder, we are able to use multiscale\nfeatures to predict saliency of part of the board at different scales, showing\nmultiple relations between pieces. We have used scan path and fixation data\nfrom players engaged in solving chess problems, to compute 6600 saliency maps\nassociated to the corresponding chess piece configurations. This corpus is\ncompleted with synthetically generated data from actual games gathered from an\nonline chess platform. Experiments realized using both scan-paths from chess\nplayers and the CAT2000 saliency dataset of natural images, highlights several\nresults. Deep features, pretrained on natural images, were found to be helpful\nin training visual attention prediction for chess. The proposed neural network\narchitecture is able to generate meaningful saliency maps on unseen chess\nconfigurations with good scores on standard metrics. This work provides a\nbaseline for future work on visual attention prediction in similar contexts.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 09:31:37 GMT"}], "update_date": "2019-04-21", "authors_parsed": [["Louedec", "Justin Le", "", "PERVASIVE"], ["Guntz", "Thomas", "", "PERVASIVE"], ["Crowley", "James", "", "PERVASIVE"], ["Vaufreydaz", "Dominique", "", "PERVASIVE"]]}, {"id": "1904.08157", "submitter": "Tianshu Lyu", "authors": "Tianshu Lyu, Fei Sun, Peng Jiang, Wenwu Ou, Yan Zhang", "title": "Compositional Network Embedding", "comments": "Accepted By RecSys 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network embedding has proved extremely useful in a variety of network\nanalysis tasks such as node classification, link prediction, and network\nvisualization. Almost all the existing network embedding methods learn to map\nthe node IDs to their corresponding node embeddings. This design principle,\nhowever, hinders the existing methods from being applied in real cases. Node ID\nis not generalizable and, thus, the existing methods have to pay great effort\nin cold-start problem. The heterogeneous network usually requires extra work to\nencode node types, as node type is not able to be identified by node ID. Node\nID carries rare information, resulting in the criticism that the existing\nmethods are not robust to noise.\n  To address this issue, we introduce Compositional Network Embedding, a\ngeneral inductive network representation learning framework that generates node\nembeddings by combining node features based on the principle of\ncompositionally. Instead of directly optimizing an embedding lookup based on\narbitrary node IDs, we learn a composition function that infers node embeddings\nby combining the corresponding node attribute embeddings through a graph-based\nloss. For evaluation, we conduct the experiments on link prediction under four\ndifferent settings. The results verified the effectiveness and generalization\nability of compositional network embeddings, especially on unseen nodes.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 09:39:43 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 02:24:31 GMT"}, {"version": "v3", "created": "Tue, 13 Aug 2019 01:25:16 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Lyu", "Tianshu", ""], ["Sun", "Fei", ""], ["Jiang", "Peng", ""], ["Ou", "Wenwu", ""], ["Zhang", "Yan", ""]]}, {"id": "1904.08205", "submitter": "L\\'eonard Torossian", "authors": "L\\'eonard Torossian, Aur\\'elien Garivier, Victor Picheny", "title": "X-Armed Bandits: Optimizing Quantiles, CVaR and Other Risks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze StoROO, an algorithm for risk optimization on\nstochastic black-box functions derived from StoOO. Motivated by risk-averse\ndecision making fields like agriculture, medicine, biology or finance, we do\nnot focus on the mean payoff but on generic functionals of the return\ndistribution. We provide a generic regret analysis of StoROO and illustrate its\napplicability with two examples: the optimization of quantiles and CVaR.\nInspired by the bandit literature and black-box mean optimizers, StoROO relies\non the possibility to construct confidence intervals for the targeted\nfunctional based on random-size samples. We detail their construction in the\ncase of quantiles, providing tight bounds based on Kullback-Leibler divergence.\nWe finally present numerical experiments that show a dramatic impact of tight\nbounds for the optimization of quantiles and CVaR.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 11:52:11 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 08:33:59 GMT"}, {"version": "v3", "created": "Wed, 4 Mar 2020 16:48:34 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Torossian", "L\u00e9onard", ""], ["Garivier", "Aur\u00e9lien", ""], ["Picheny", "Victor", ""]]}, {"id": "1904.08248", "submitter": "Luca Pasa PhD", "authors": "Luca Pasa, Giovanni Morrone, Leonardo Badino", "title": "An Analysis of Speech Enhancement and Recognition Losses in Limited\n  Resources Multi-talker Single Channel Audio-Visual ASR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyzed how audio-visual speech enhancement can help to\nperform the ASR task in a cocktail party scenario. Therefore we considered two\nsimple end-to-end LSTM-based models that perform single-channel audio-visual\nspeech enhancement and phone recognition respectively. Then, we studied how the\ntwo models interact, and how to train them jointly affects the final result. We\nanalyzed different training strategies that reveal some interesting and\nunexpected behaviors. The experiments show that during optimization of the ASR\ntask the speech enhancement capability of the model significantly decreases and\nvice-versa. Nevertheless the joint optimization of the two tasks shows a\nremarkable drop of the Phone Error Rate (PER) compared to the audio-visual\nbaseline models trained only to perform phone recognition. We analyzed the\nbehaviors of the proposed models by using two limited-size datasets, and in\nparticular we used the mixed-speech versions of GRID and TCD-TIMIT.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 14:43:19 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 16:37:24 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Pasa", "Luca", ""], ["Morrone", "Giovanni", ""], ["Badino", "Leonardo", ""]]}, {"id": "1904.08249", "submitter": "Rohit Babbar", "authors": "Sujay Khandagale, Han Xiao, Rohit Babbar", "title": "Bonsai -- Diverse and Shallow Trees for Extreme Multi-label\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme multi-label classification (XMC) refers to supervised multi-label\nlearning involving hundreds of thousand or even millions of labels. In this\npaper, we develop a suite of algorithms, called Bonsai, which generalizes the\nnotion of label representation in XMC, and partitions the labels in the\nrepresentation space to learn shallow trees. We show three concrete\nrealizations of this label representation space including : (i) the input space\nwhich is spanned by the input features, (ii) the output space spanned by label\nvectors based on their co-occurrence with other labels, and (iii) the joint\nspace by combining the input and output representations. Furthermore, the\nconstraint-free multi-way partitions learnt iteratively in these spaces lead to\nshallow trees. By combining the effect of shallow trees and generalized label\nrepresentation, Bonsai achieves the best of both worlds - fast training which\nis comparable to state-of-the-art tree-based methods in XMC, and much better\nprediction accuracy, particularly on tail-labels. On a benchmark Amazon-3M\ndataset with 3 million labels, \\bonsai outperforms a state-of-the-art\none-vs-rest method in terms of prediction accuracy, while being approximately\n200 times faster to train. The code for Bonsai is available at\n\\url{https://github.com/xmc-aalto/bonsai}\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 12:57:09 GMT"}, {"version": "v2", "created": "Sat, 10 Aug 2019 15:08:30 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Khandagale", "Sujay", ""], ["Xiao", "Han", ""], ["Babbar", "Rohit", ""]]}, {"id": "1904.08338", "submitter": "Chandan Gautam", "authors": "Chandan Gautam, Aruna Tiwari, M. Tanveer", "title": "OCKELM+: Kernel Extreme Learning Machine based One-class Classification\n  using Privileged Information (or KOC+: Kernel Ridge Regression or Least\n  Square SVM with zero bias based One-class Classification using Privileged\n  Information)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel method-based one-class classifier is mainly used for outlier or\nnovelty detection. In this letter, kernel ridge regression (KRR) based\none-class classifier (KOC) has been extended for learning using privileged\ninformation (LUPI). LUPI-based KOC method is referred to as KOC+. This\nprivileged information is available as a feature with the dataset but only for\ntraining (not for testing). KOC+ utilizes the privileged information\ndifferently compared to normal feature information by using a so-called\ncorrection function. Privileged information helps KOC+ in achieving better\ngeneralization performance which is exhibited in this letter by testing the\nclassifiers with and without privileged information. Existing and proposed\nclassifiers are evaluated on the datasets from UCI machine learning repository\nand also on MNIST dataset. Moreover, experimental results evince the advantage\nof KOC+ over KOC and support vector machine (SVM) based one-class classifiers.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 08:07:52 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Gautam", "Chandan", ""], ["Tiwari", "Aruna", ""], ["Tanveer", "M.", ""]]}, {"id": "1904.08353", "submitter": "Filipe Rodrigues", "authors": "Filipe Rodrigues and Carlos Lima Azevedo", "title": "Towards Robust Deep Reinforcement Learning for Traffic Signal Control:\n  Demand Surges, Incidents and Sensor Failures", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) constitutes a promising solution for alleviating\nthe problem of traffic congestion. In particular, deep RL algorithms have been\nshown to produce adaptive traffic signal controllers that outperform\nconventional systems. However, in order to be reliable in highly dynamic urban\nareas, such controllers need to be robust with the respect to a series of\nexogenous sources of uncertainty. In this paper, we develop an open-source\ncallback-based framework for promoting the flexible evaluation of different\ndeep RL configurations under a traffic simulation environment. With this\nframework, we investigate how deep RL-based adaptive traffic controllers\nperform under different scenarios, namely under demand surges caused by special\nevents, capacity reductions from incidents and sensor failures. We extract\nseveral key insights for the development of robust deep RL algorithms for\ntraffic control and propose concrete designs to mitigate the impact of the\nconsidered exogenous uncertainties.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 16:39:22 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 13:23:48 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Rodrigues", "Filipe", ""], ["Azevedo", "Carlos Lima", ""]]}, {"id": "1904.08361", "submitter": "Dileep Kalathil", "authors": "Ran Wang, Karthikeya Parunandi, Dan Yu, Dileep Kalathil, Suman\n  Chakravorty", "title": "Decoupled Data Based Approach for Learning to Control Nonlinear\n  Dynamical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of learning the optimal control policy for a\nnonlinear stochastic dynamical system with continuous state space, continuous\naction space and unknown dynamics. This class of problems are typically\naddressed in stochastic adaptive control and reinforcement learning literature\nusing model-based and model-free approaches respectively. Both methods rely on\nsolving a dynamic programming problem, either directly or indirectly, for\nfinding the optimal closed loop control policy. The inherent `curse of\ndimensionality' associated with dynamic programming method makes these\napproaches also computationally difficult.\n  This paper proposes a novel decoupled data-based control (D2C) algorithm that\naddresses this problem using a decoupled, `open loop - closed loop', approach.\nFirst, an open-loop deterministic trajectory optimization problem is solved\nusing a black-box simulation model of the dynamical system. Then, a closed loop\ncontrol is developed around this open loop trajectory by linearization of the\ndynamics about this nominal trajectory. By virtue of linearization, a linear\nquadratic regulator based algorithm can be used for this closed loop control.\nWe show that the performance of D2C algorithm is approximately optimal.\nMoreover, simulation performance suggests significant reduction in training\ntime compared to other state of the art algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 16:58:18 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Wang", "Ran", ""], ["Parunandi", "Karthikeya", ""], ["Yu", "Dan", ""], ["Kalathil", "Dileep", ""], ["Chakravorty", "Suman", ""]]}, {"id": "1904.08368", "submitter": "Jared Roesch", "authors": "Jared Roesch, Steven Lyubomirsky, Marisa Kirisame, Logan Weber, Josh\n  Pollock, Luis Vega, Ziheng Jiang, Tianqi Chen, Thierry Moreau, Zachary\n  Tatlock", "title": "Relay: A High-Level Compiler for Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frameworks for writing, compiling, and optimizing deep learning (DL) models\nhave recently enabled progress in areas like computer vision and natural\nlanguage processing. Extending these frameworks to accommodate the rapidly\ndiversifying landscape of DL models and hardware platforms presents challenging\ntradeoffs between expressivity, composability, and portability. We present\nRelay, a new compiler framework for DL. Relay's functional, statically typed\nintermediate representation (IR) unifies and generalizes existing DL IRs to\nexpress state-of-the-art models. The introduction of Relay's expressive IR\nrequires careful design of domain-specific optimizations, addressed via Relay's\nextension mechanisms. Using these extension mechanisms, Relay supports a\nunified compiler that can target a variety of hardware platforms. Our\nevaluation demonstrates Relay's competitive performance for a broad class of\nmodels and devices (CPUs, GPUs, and emerging accelerators). Relay's design\ndemonstrates how a unified IR can provide expressivity, composability, and\nportability without compromising performance.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 17:08:23 GMT"}, {"version": "v2", "created": "Sat, 24 Aug 2019 23:30:04 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Roesch", "Jared", ""], ["Lyubomirsky", "Steven", ""], ["Kirisame", "Marisa", ""], ["Weber", "Logan", ""], ["Pollock", "Josh", ""], ["Vega", "Luis", ""], ["Jiang", "Ziheng", ""], ["Chen", "Tianqi", ""], ["Moreau", "Thierry", ""], ["Tatlock", "Zachary", ""]]}, {"id": "1904.08378", "submitter": "Benjamin Krause", "authors": "Ben Krause, Emmanuel Kahembwe, Iain Murray, Steve Renals", "title": "Dynamic Evaluation of Transformer Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research note combines two methods that have recently improved the state\nof the art in language modeling: Transformers and dynamic evaluation.\nTransformers use stacked layers of self-attention that allow them to capture\nlong range dependencies in sequential data. Dynamic evaluation fits models to\nthe recent sequence history, allowing them to assign higher probabilities to\nre-occurring sequential patterns. By applying dynamic evaluation to\nTransformer-XL models, we improve the state of the art on enwik8 from 0.99 to\n0.94 bits/char, text8 from 1.08 to 1.04 bits/char, and WikiText-103 from 18.3\nto 16.4 perplexity points.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 17:26:01 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Krause", "Ben", ""], ["Kahembwe", "Emmanuel", ""], ["Murray", "Iain", ""], ["Renals", "Steve", ""]]}, {"id": "1904.08379", "submitter": "Oran Gafni", "authors": "Oran Gafni, Lior Wolf, Yaniv Taigman", "title": "Vid2Game: Controllable Characters Extracted from Real-World Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are given a video of a person performing a certain activity, from which we\nextract a controllable model. The model generates novel image sequences of that\nperson, according to arbitrary user-defined control signals, typically marking\nthe displacement of the moving body. The generated video can have an arbitrary\nbackground, and effectively capture both the dynamics and appearance of the\nperson.\n  The method is based on two networks. The first network maps a current pose,\nand a single-instance control signal to the next pose. The second network maps\nthe current pose, the new pose, and a given background, to an output frame.\nBoth networks include multiple novelties that enable high-quality performance.\nThis is demonstrated on multiple characters extracted from various videos of\ndancers and athletes.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 17:26:14 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Gafni", "Oran", ""], ["Wolf", "Lior", ""], ["Taigman", "Yaniv", ""]]}, {"id": "1904.08386", "submitter": "Shufan Wang", "authors": "Shufan Wang, Mohit Iyyer", "title": "Casting Light on Invisible Cities: Computationally Engaging with\n  Literary Criticism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Literary critics often attempt to uncover meaning in a single work of\nliterature through careful reading and analysis. Applying natural language\nprocessing methods to aid in such literary analyses remains a challenge in\ndigital humanities. While most previous work focuses on \"distant reading\" by\nalgorithmically discovering high-level patterns from large collections of\nliterary works, here we sharpen the focus of our methods to a single literary\ntheory about Italo Calvino's postmodern novel Invisible Cities, which consists\nof 55 short descriptions of imaginary cities. Calvino has provided a\nclassification of these cities into eleven thematic groups, but literary\nscholars disagree as to how trustworthy his categorization is. Due to the\nunique structure of this novel, we can computationally weigh in on this debate:\nwe leverage pretrained contextualized representations to embed each city's\ndescription and use unsupervised methods to cluster these embeddings.\nAdditionally, we compare results of our computational approach to similarity\njudgments generated by human readers. Our work is a first step towards\nincorporating natural language processing into literary criticism.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 17:37:33 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Wang", "Shufan", ""], ["Iyyer", "Mohit", ""]]}, {"id": "1904.08397", "submitter": "Wolfgang Konen K", "authors": "Samineh Bagheri and Wolfgang Konen and Thomas B\\\"ack", "title": "SACOBRA with Online Whitening for Solving Optimization Problems with\n  High Conditioning", "comments": "20 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world optimization problems often have expensive objective functions in\nterms of cost and time. It is desirable to find near-optimal solutions with\nvery few function evaluations. Surrogate-assisted optimizers tend to reduce the\nrequired number of function evaluations by replacing the real function with an\nefficient mathematical model built on few evaluated points. Problems with a\nhigh condition number are a challenge for many surrogate-assisted optimizers\nincluding SACOBRA. To address such problems we propose a new online whitening\noperating in the black-box optimization paradigm. We show on a set of\nhigh-conditioning functions that online whitening tackles SACOBRA's early\nstagnation issue and reduces the optimization error by a factor between 10 to\n1e12 as compared to the plain SACOBRA, though it imposes many extra function\nevaluations. Covariance matrix adaptation evolution strategy (CMA-ES) has for\nvery high numbers of function evaluations even lower errors, whereas SACOBRA\nperforms better in the expensive setting (less than 1e03 function evaluations).\nIf we count all parallelizable function evaluations (population evaluation in\nCMA-ES, online whitening in our approach) as one iteration, then both\nalgorithms have comparable strength even on the long run. This holds for\nproblems with dimension D <= 20.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 17:53:42 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Bagheri", "Samineh", ""], ["Konen", "Wolfgang", ""], ["B\u00e4ck", "Thomas", ""]]}, {"id": "1904.08408", "submitter": "Benjamin Deguerre", "authors": "Benjamin Deguerre, Cl\\'ement Chatelain, Gilles Gasso", "title": "Fast object detection in compressed JPEG Images", "comments": null, "journal-ref": null, "doi": "10.1109/ITSC.2019.8916937", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection in still images has drawn a lot of attention over past few\nyears, and with the advent of Deep Learning impressive performances have been\nachieved with numerous industrial applications. Most of these deep learning\nmodels rely on RGB images to localize and identify objects in the image.\nHowever in some application scenarii, images are compressed either for storage\nsavings or fast transmission. Therefore a time consuming image decompression\nstep is compulsory in order to apply the aforementioned deep models. To\nalleviate this drawback, we propose a fast deep architecture for object\ndetection in JPEG images, one of the most widespread compression format. We\ntrain a neural network to detect objects based on the blockwise DCT (discrete\ncosine transform) coefficients {issued from} the JPEG compression algorithm. We\nmodify the well-known Single Shot multibox Detector (SSD) by replacing its\nfirst layers with one convolutional layer dedicated to process the DCT inputs.\nExperimental evaluations on PASCAL VOC and industrial dataset comprising images\nof road traffic surveillance show that the model is about $2\\times$ faster than\nregular SSD with promising detection performances. To the best of our\nknowledge, this paper is the first to address detection in compressed JPEG\nimages.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 22:10:53 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Deguerre", "Benjamin", ""], ["Chatelain", "Cl\u00e9ment", ""], ["Gasso", "Gilles", ""]]}, {"id": "1904.08410", "submitter": "Reiichiro Nakano", "authors": "Reiichiro Nakano", "title": "Neural Painters: A learned differentiable constraint for generating\n  brushstroke paintings", "comments": "Added more references and acknowledgments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore neural painters, a generative model for brushstrokes learned from\na real non-differentiable and non-deterministic painting program. We show that\nwhen training an agent to \"paint\" images using brushstrokes, using a\ndifferentiable neural painter leads to much faster convergence. We propose a\nmethod for encouraging this agent to follow human-like strokes when\nreconstructing digits. We also explore the use of a neural painter as a\ndifferentiable image parameterization. By directly optimizing brushstrokes to\nactivate neurons in a pre-trained convolutional network, we can directly\nvisualize ImageNet categories and generate \"ideal\" paintings of each class.\nFinally, we present a new concept called intrinsic style transfer. By\nminimizing only the content loss from neural style transfer, we allow the\nartistic medium, in this case, brushstrokes, to naturally dictate the resulting\nstyle.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 03:03:49 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2019 17:14:53 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Nakano", "Reiichiro", ""]]}, {"id": "1904.08414", "submitter": "Andreas Danzer", "authors": "Andreas Danzer, Thomas Griebel, Martin Bach, and Klaus Dietmayer", "title": "2D Car Detection in Radar Data with PointNets", "comments": null, "journal-ref": "IEEE Intelligent Transportation Systems Conference (ITSC),\n  Auckland, New Zealand, 2019, pp. 61-66", "doi": "10.1109/ITSC.2019.8917000", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many automated driving functions, a highly accurate perception of the\nvehicle environment is a crucial prerequisite. Modern high-resolution radar\nsensors generate multiple radar targets per object, which makes these sensors\nparticularly suitable for the 2D object detection task. This work presents an\napproach to detect 2D objects solely depending on sparse radar data using\nPointNets. In literature, only methods are presented so far which perform\neither object classification or bounding box estimation for objects. In\ncontrast, this method facilitates a classification together with a bounding box\nestimation of objects using a single radar sensor. To this end, PointNets are\nadjusted for radar data performing 2D object classification with segmentation,\nand 2D bounding box regression in order to estimate an amodal 2D bounding box.\nThe algorithm is evaluated using an automatically created dataset which consist\nof various realistic driving maneuvers. The results show the great potential of\nobject detection in high-resolution radar data using PointNets.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 09:27:56 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2019 11:39:44 GMT"}, {"version": "v3", "created": "Mon, 2 Dec 2019 10:54:34 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Danzer", "Andreas", ""], ["Griebel", "Thomas", ""], ["Bach", "Martin", ""], ["Dietmayer", "Klaus", ""]]}, {"id": "1904.08421", "submitter": "Lambert Schomaker", "authors": "Lambert Schomaker", "title": "A large-scale field test on word-image classification in large\n  historical document collections using a traditional and two deep-learning\n  methods", "comments": "Field test of a large operational image search system, comparing BOVW\n  and end-to-end CNN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report describes a practical field test on word-image\nclassification in a very large collection of more than 300 diverse handwritten\nhistorical manuscripts, with 1.6 million unique labeled images and more than 11\nmillion images used in testing. Results indicate that several deep-learning\ntests completely failed (mean accuracy 83%). In the tests with more than 1000\noutput units (lexical words) in one-hot encoding for classification,\nperformance steeply drops to almost zero percent accuracy, even with a modest\nsize of the pre-final (i.e., penultimate) layer (150 units). A traditional\nfeature method (BOVW) displays a consistent performance over numbers of classes\nand numbers of training examples (mean accuracy 87%). Additional tests using\nnearest mean on the output of the pre-final layer of an Inception V3 network,\nfor each book, only yielded mediocre results (mean accuracy 49\\%), but was not\nsensitive to high numbers of classes. Notably, this experiment was only\npossible on the basis of labels that were harvested on the basis of a\ntraditional method which already works starting from a single labeled image per\nclass. It is expected that the performance of the failed deep learning tests\ncan be repaired, but only on the basis of human handcrafting (sic) of network\narchitecture and hyperparameters. When the failed problematic books are not\nconsidered, end-to-end CNN training yields about 95% accuracy. This average is\ndominated by a large subset of Chinese characters, performances for other\nscript styles being lower.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 16:03:14 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Schomaker", "Lambert", ""]]}, {"id": "1904.08444", "submitter": "Ji Lin", "authors": "Ji Lin, Chuang Gan, Song Han", "title": "Defensive Quantization: When Efficiency Meets Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network quantization is becoming an industry standard to efficiently\ndeploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and\nFPGAs. However, we observe that the conventional quantization approaches are\nvulnerable to adversarial attacks. This paper aims to raise people's awareness\nabout the security of the quantized models, and we designed a novel\nquantization methodology to jointly optimize the efficiency and robustness of\ndeep learning models. We first conduct an empirical study to show that vanilla\nquantization suffers more from adversarial attacks. We observe that the\ninferior robustness comes from the error amplification effect, where the\nquantization operation further enlarges the distance caused by amplified noise.\nThen we propose a novel Defensive Quantization (DQ) method by controlling the\nLipschitz constant of the network during quantization, such that the magnitude\nof the adversarial noise remains non-expansive during inference. Extensive\nexperiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization\nmethod can defend neural networks against adversarial examples, and even\nachieves superior robustness than their full-precision counterparts while\nmaintaining the same hardware efficiency as vanilla quantization approaches. As\na by-product, DQ can also improve the accuracy of quantized models without\nadversarial attack.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 18:23:24 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Lin", "Ji", ""], ["Gan", "Chuang", ""], ["Han", "Song", ""]]}, {"id": "1904.08459", "submitter": "Hieu Nguyen", "authors": "Hieu Quang Nguyen, Abdul Hasib Rahimyar, Xiaodi Wang", "title": "Stock Forecasting using M-Band Wavelet-Based SVR and RNN-LSTMs Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of predicting future stock values has always been one that is\nheavily desired albeit very difficult. This difficulty arises from stocks with\nnon-stationary behavior, and without any explicit form. Hence, predictions are\nbest made through analysis of financial stock data. To handle big data sets,\ncurrent convention involves the use of the Moving Average. However, by\nutilizing the Wavelet Transform in place of the Moving Average to denoise stock\nsignals, financial data can be smoothened and more accurately broken down. This\nnewly transformed, denoised, and more stable stock data can be followed up by\nnon-parametric statistical methods, such as Support Vector Regression (SVR) and\nRecurrent Neural Network (RNN) based Long Short-Term Memory (LSTM) networks to\npredict future stock prices. Through the implementation of these methods, one\nis left with a more accurate stock forecast, and in turn, increased profits.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 19:10:10 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Nguyen", "Hieu Quang", ""], ["Rahimyar", "Abdul Hasib", ""], ["Wang", "Xiaodi", ""]]}, {"id": "1904.08473", "submitter": "Yao Liu", "authors": "Yao Liu, Adith Swaminathan, Alekh Agarwal, Emma Brunskill", "title": "Off-Policy Policy Gradient with State Distribution Correction", "comments": "to appear at UAI 18; camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of off-policy policy optimization in Markov decision\nprocesses, and develop a novel off-policy policy gradient method. Prior\noff-policy policy gradient approaches have generally ignored the mismatch\nbetween the distribution of states visited under the behavior policy used to\ncollect data, and what would be the distribution of states under the learned\npolicy. Here we build on recent progress for estimating the ratio of the state\ndistributions under behavior and evaluation policies for policy evaluation, and\npresent an off-policy policy gradient optimization technique that can account\nfor this mismatch in distributions. We present an illustrative example of why\nthis is important and a theoretical convergence guarantee for our approach.\nEmpirically, we compare our method in simulations to several strong baselines\nwhich do not correct for this mismatch, significantly improving in the quality\nof the policy discovered.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 19:46:02 GMT"}, {"version": "v2", "created": "Sat, 6 Jul 2019 05:30:14 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Liu", "Yao", ""], ["Swaminathan", "Adith", ""], ["Agarwal", "Alekh", ""], ["Brunskill", "Emma", ""]]}, {"id": "1904.08477", "submitter": "Negin Musavi", "authors": "Negin Musavi", "title": "A Game Theoretical Framework for the Evaluation of Unmanned Aircraft\n  Systems Airspace Integration Concepts", "comments": "This thesis is submitted in partial fulfillment of the requirements\n  for the degree of Master of Science in Mechanical Engineering in the Graduate\n  School of Engineering and Science of Bilkent University.\n  http://repository.bilkent.edu.tr/handle/11693/33526", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the outcomes of integrating Unmanned Aerial Systems (UAS) into the\nNational Aerospace (NAS) is a complex problem which is required to be addressed\nby simulation studies before allowing the routine access of UAS into the NAS.\nThis thesis focuses on providing 2D and 3D simulation frameworks using a game\ntheoretical methodology to evaluate integration concepts in scenarios where\nmanned and unmanned air vehicles co-exist. The fundamental gap in the\nliterature is that the models of interaction between manned and unmanned\nvehicles are insufficient: a) they assume that pilot behavior is known a priori\nand b) they disregard decision making processes. The contribution of this work\nis to propose a modeling framework, in which, human pilot reactions are modeled\nusing reinforcement learning and a game theoretical concept called level-k\nreasoning to fill this gap. The level-k reasoning concept is based on the\nassumption that humans have various levels of decision making. Reinforcement\nlearning is a mathematical learning method that is rooted in human learning. In\nthis work, a classical and an approximate reinforcement learning (Neural Fitted\nQ Iteration) methods are used to model time-extended decisions of pilots with\n2D and 3D maneuvers. An analysis of UAS integration is conducted using example\nscenarios in the presence of manned aircraft and fully autonomous UAS equipped\nwith sense and avoid algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 19:58:45 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Musavi", "Negin", ""]]}, {"id": "1904.08479", "submitter": "Yaoyao Liu", "authors": "Yaoyao Liu, Bernt Schiele, Qianru Sun", "title": "An Ensemble of Epoch-wise Empirical Bayes for Few-shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Few-shot learning aims to train efficient predictive models with a few\nexamples. The lack of training data leads to poor models that perform\nhigh-variance or low-confidence predictions. In this paper, we propose to\nmeta-learn the ensemble of epoch-wise empirical Bayes models (E3BM) to achieve\nrobust predictions. \"Epoch-wise\" means that each training epoch has a Bayes\nmodel whose parameters are specifically learned and deployed. \"Empirical\" means\nthat the hyperparameters, e.g., used for learning and ensembling the epoch-wise\nmodels, are generated by hyperprior learners conditional on task-specific data.\nWe introduce four kinds of hyperprior learners by considering inductive vs.\ntransductive, and epoch-dependent vs. epoch-independent, in the paradigm of\nmeta-learning. We conduct extensive experiments for five-class few-shot tasks\non three challenging benchmarks: miniImageNet, tieredImageNet, and FC100, and\nachieve top performance using the epoch-dependent transductive hyperprior\nlearner, which captures the richest information. Our ablation study shows that\nboth \"epoch-wise ensemble\" and \"empirical\" encourage high efficiency and\nrobustness in the model performance.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 20:02:24 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 15:04:44 GMT"}, {"version": "v3", "created": "Wed, 11 Dec 2019 11:51:59 GMT"}, {"version": "v4", "created": "Mon, 23 Dec 2019 14:06:29 GMT"}, {"version": "v5", "created": "Mon, 16 Mar 2020 07:54:13 GMT"}, {"version": "v6", "created": "Fri, 17 Jul 2020 09:31:15 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Liu", "Yaoyao", ""], ["Schiele", "Bernt", ""], ["Sun", "Qianru", ""]]}, {"id": "1904.08484", "submitter": "Robin Yancey", "authors": "Robin Elizabeth Yancey, Norman Matloff, Paul Thompson", "title": "Multi-linear Faster RCNN with ELA for Image Tampering Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With technological advances leading to an increase in mechanisms for image\ntampering, fraud detection methods must continue to be upgraded to match their\nsophistication. One problem with current methods is that they require prior\nknowledge of the method of forgery in order to determine which features to\nextract from the image to localize the region of interest. When a machine\nlearning algorithm is used to learn different types of tampering from a large\nset of various image types, with a large enough database we can easily classify\nwhich images are tampered (by training on the entire image feature map for each\nimage). However, we still are left with the question of which features to train\non, and how to localize the manipulation. To solve this, object detection\nnetworks such as Faster R-CNN, which combine an RPN (Region Proposal Network)\nwith a CNN, have recently been adapted to fraud detection by utilizing their\nability to propose bounding boxes for objects of interest to localize the\ntampering artifacts. By making use of the computational powers of today's GPUs\nthis method also achieves a fast run-time and higher accuracy than the top\ncurrent methods such as noise analysis, ELA (Error Level Analysis), or CFA\n(Color Filter Array). In this work, a multi-linear Faster RCNN network will be\napplied similarly but with the second stream having an input of the ELA JPEG\ncompression level mask. This is shown to provide even higher accuracy by adding\ntraining features from the segmented image map to the network.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 19:39:17 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 17:37:44 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Yancey", "Robin Elizabeth", ""], ["Matloff", "Norman", ""], ["Thompson", "Paul", ""]]}, {"id": "1904.08486", "submitter": "Martin Mundt", "authors": "Martin Mundt, Sagnik Majumder, Sreenivas Murali, Panagiotis Panetsos,\n  Visvanathan Ramesh", "title": "Meta-learning Convolutional Neural Architectures for Multi-target\n  Concrete Defect Classification with the COncrete DEfect BRidge IMage Dataset", "comments": "Accepted for publication at CVPR 2019. Version includes supplementary\n  material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition of defects in concrete infrastructure, especially in bridges, is\na costly and time consuming crucial first step in the assessment of the\nstructural integrity. Large variation in appearance of the concrete material,\nchanging illumination and weather conditions, a variety of possible surface\nmarkings as well as the possibility for different types of defects to overlap,\nmake it a challenging real-world task. In this work we introduce the novel\nCOncrete DEfect BRidge IMage dataset (CODEBRIM) for multi-target classification\nof five commonly appearing concrete defects. We investigate and compare two\nreinforcement learning based meta-learning approaches, MetaQNN and efficient\nneural architecture search, to find suitable convolutional neural network\narchitectures for this challenging multi-class multi-target task. We show that\nlearned architectures have fewer overall parameters in addition to yielding\nbetter multi-target accuracy in comparison to popular neural architectures from\nthe literature evaluated in the context of our application.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 13:08:33 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Mundt", "Martin", ""], ["Majumder", "Sagnik", ""], ["Murali", "Sreenivas", ""], ["Panetsos", "Panagiotis", ""], ["Ramesh", "Visvanathan", ""]]}, {"id": "1904.08487", "submitter": "Zihao Liu", "authors": "Zihao Liu, Xiaowei Xu, Tao Liu, Qi Liu, Yanzhi Wang, Yiyu Shi, Wujie\n  Wen, Meiping Huang, Haiyun Yuan, Jian Zhuang", "title": "Machine Vision Guided 3D Medical Image Compression for Efficient\n  Transmission and Accurate Segmentation in the Clouds", "comments": "IEEE Computer Society Conference on Computer Vision and Pattern\n  Recognition(CVPR), Long Beach, CA, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud based medical image analysis has become popular recently due to the\nhigh computation complexities of various deep neural network (DNN) based\nframeworks and the increasingly large volume of medical images that need to be\nprocessed. It has been demonstrated that for medical images the transmission\nfrom local to clouds is much more expensive than the computation in the clouds\nitself. Towards this, 3D image compression techniques have been widely applied\nto reduce the data traffic. However, most of the existing image compression\ntechniques are developed around human vision, i.e., they are designed to\nminimize distortions that can be perceived by human eyes. In this paper we will\nuse deep learning based medical image segmentation as a vehicle and demonstrate\nthat interestingly, machine and human view the compression quality differently.\nMedical images compressed with good quality w.r.t. human vision may result in\ninferior segmentation accuracy. We then design a machine vision oriented 3D\nimage compression framework tailored for segmentation using DNNs. Our method\nautomatically extracts and retains image features that are most important to\nthe segmentation. Comprehensive experiments on widely adopted segmentation\nframeworks with HVSMR 2016 challenge dataset show that our method can achieve\nsignificantly higher segmentation accuracy at the same compression rate, or\nmuch better compression rate under the same segmentation accuracy, when\ncompared with the existing JPEG 2000 method. To the best of the authors'\nknowledge, this is the first machine vision guided medical image compression\nframework for segmentation in the clouds.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 13:34:25 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Liu", "Zihao", ""], ["Xu", "Xiaowei", ""], ["Liu", "Tao", ""], ["Liu", "Qi", ""], ["Wang", "Yanzhi", ""], ["Shi", "Yiyu", ""], ["Wen", "Wujie", ""], ["Huang", "Meiping", ""], ["Yuan", "Haiyun", ""], ["Zhuang", "Jian", ""]]}, {"id": "1904.08491", "submitter": "Shadi Albarqouni Ph.D.", "authors": "Mhd Hasan Sarhan, Abouzar Eslami, Nassir Navab, Shadi Albarqouni", "title": "Learning Interpretable Disentangled Representations using Adversarial\n  VAEs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning Interpretable representation in medical applications is becoming\nessential for adopting data-driven models into clinical practice. It has been\nrecently shown that learning a disentangled feature representation is important\nfor a more compact and explainable representation of the data. In this paper,\nwe introduce a novel adversarial variational autoencoder with a total\ncorrelation constraint to enforce independence on the latent representation\nwhile preserving the reconstruction fidelity. Our proposed method is validated\non a publicly available dataset showing that the learned disentangled\nrepresentation is not only interpretable, but also superior to the\nstate-of-the-art methods. We report a relative improvement of 81.50% in terms\nof disentanglement, 11.60% in clustering, and 2% in supervised classification\nwith a few amounts of labeled data.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 20:43:51 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Sarhan", "Mhd Hasan", ""], ["Eslami", "Abouzar", ""], ["Navab", "Nassir", ""], ["Albarqouni", "Shadi", ""]]}, {"id": "1904.08495", "submitter": "Behzad Ghazanfari", "authors": "Behzad Ghazanfari, Fatemeh Afghah, Kayvan Najarian, Sajad Mousavi,\n  Jonathan Gryak, James Todd", "title": "An Unsupervised Feature Learning Approach to Reduce False Alarm Rate in\n  ICUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high rate of false alarms in intensive care units (ICUs) is one of the\ntop challenges of using medical technology in hospitals. These false alarms are\noften caused by patients' movements, detachment of monitoring sensors, or\ndifferent sources of noise and interference that impact the collected signals\nfrom different monitoring devices. In this paper, we propose a novel set of\nhigh-level features based on unsupervised feature learning technique in order\nto effectively capture the characteristics of different arrhythmia in\nelectrocardiogram (ECG) signal and differentiate them from irregularity in\nsignals due to different sources of signal disturbances. This unsupervised\nfeature learning technique, first extracts a set of low-level features from all\nexisting heart cycles of a patient, and then clusters these segments for each\nindividual patient to provide a set of prominent high-level features. The\nobjective of the clustering phase is to enable the classification method to\ndifferentiate between the high-level features extracted from normal and\nabnormal cycles (i.e., either due to arrhythmia or different sources of\ndistortions in signal) in order to put more attention to the features extracted\nfrom abnormal portion of the signal that contribute to the alarm. The\nperformance of this method is evaluated using the 2015 PhysioNet/Computing in\nCardiology Challenge dataset for reducing false arrhythmia alarms in the ICUs.\nAs confirmed by the experimental results, the proposed method offers a\nconsiderable performance in terms of accuracy, sensitivity and specificity of\nalarm detection only using a few high-level features that are extracted from\none single lead ECG signal.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 20:49:29 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Ghazanfari", "Behzad", ""], ["Afghah", "Fatemeh", ""], ["Najarian", "Kayvan", ""], ["Mousavi", "Sajad", ""], ["Gryak", "Jonathan", ""], ["Todd", "James", ""]]}, {"id": "1904.08496", "submitter": "Loc Tran H", "authors": "Loc Hoang Tran, Linh Hoang Tran", "title": "Tensor Sparse PCA and Face Recognition: A Novel Approach", "comments": "It has some errors in the experimental section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition is the important field in machine learning and pattern\nrecognition research area. It has a lot of applications in military, finance,\npublic security, to name a few. In this paper, the combination of the tensor\nsparse PCA with the nearest-neighbor method (and with the kernel ridge\nregression method) will be proposed and applied to the face dataset.\nExperimental results show that the combination of the tensor sparse PCA with\nany classification system does not always reach the best accuracy performance\nmeasures. However, the accuracy of the combination of the sparse PCA method and\none specific classification system is always better than the accuracy of the\ncombination of the PCA method and one specific classification system and is\nalways better than the accuracy of the classification system itself.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 03:43:57 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 08:22:03 GMT"}, {"version": "v3", "created": "Sat, 7 Mar 2020 03:14:49 GMT"}, {"version": "v4", "created": "Tue, 11 Aug 2020 08:08:48 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Tran", "Loc Hoang", ""], ["Tran", "Linh Hoang", ""]]}, {"id": "1904.08497", "submitter": "Pedro Ribeiro Mendes J\\'unior", "authors": "Pedro Ribeiro Mendes J\\'unior, Luca Bondi, Paolo Bestagini, Stefano\n  Tubaro, Anderson Rocha", "title": "An In-Depth Study on Open-Set Camera Model Identification", "comments": "Published through IEEE Access journal", "journal-ref": null, "doi": "10.1109/ACCESS.2019.2921436", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera model identification refers to the problem of linking a picture to the\ncamera model used to shoot it. As this might be an enabling factor in different\nforensic applications to single out possible suspects (e.g., detecting the\nauthor of child abuse or terrorist propaganda material), many accurate camera\nmodel attribution methods have been developed in the literature. One of their\nmain drawbacks, however, is the typical closed-set assumption of the problem.\nThis means that an investigated photograph is always assigned to one camera\nmodel within a set of known ones present during investigation, i.e., training\ntime, and the fact that the picture can come from a completely unrelated camera\nmodel during actual testing is usually ignored. Under realistic conditions, it\nis not possible to assume that every picture under analysis belongs to one of\nthe available camera models. To deal with this issue, in this paper, we present\nthe first in-depth study on the possibility of solving the camera model\nidentification problem in open-set scenarios. Given a photograph, we aim at\ndetecting whether it comes from one of the known camera models of interest or\nfrom an unknown one. We compare different feature extraction algorithms and\nclassifiers specially targeting open-set recognition. We also evaluate possible\nopen-set training protocols that can be applied along with any open-set\nclassifier, observing that a simple of those alternatives obtains best results.\nThorough testing on independent datasets shows that it is possible to leverage\na recently proposed convolutional neural network as feature extractor paired\nwith a properly trained open-set classifier aiming at solving the open-set\ncamera model attribution problem even to small-scale image patches, improving\nover state-of-the-art available solutions.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 14:48:55 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 19:53:21 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["J\u00fanior", "Pedro Ribeiro Mendes", ""], ["Bondi", "Luca", ""], ["Bestagini", "Paolo", ""], ["Tubaro", "Stefano", ""], ["Rocha", "Anderson", ""]]}, {"id": "1904.08502", "submitter": "Davis Wertheimer", "authors": "Davis Wertheimer and Bharath Hariharan", "title": "Few-Shot Learning with Localization in Realistic Settings", "comments": "Appearing in CVPR 2019; added references in covariance pooling\n  sections, added link to code in supplementary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional recognition methods typically require large,\nartificially-balanced training classes, while few-shot learning methods are\ntested on artificially small ones. In contrast to both extremes, real world\nrecognition problems exhibit heavy-tailed class distributions, with cluttered\nscenes and a mix of coarse and fine-grained class distinctions. We show that\nprior methods designed for few-shot learning do not work out of the box in\nthese challenging conditions, based on a new \"meta-iNat\" benchmark. We\nintroduce three parameter-free improvements: (a) better training procedures\nbased on adapting cross-validation to meta-learning, (b) novel architectures\nthat localize objects using limited bounding box annotations before\nclassification, and (c) simple parameter-free expansions of the feature space\nbased on bilinear pooling. Together, these improvements double the accuracy of\nstate-of-the-art models on meta-iNat while generalizing to prior benchmarks,\ncomplex neural architectures, and settings with substantial domain shift.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 20:20:38 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 18:12:02 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Wertheimer", "Davis", ""], ["Hariharan", "Bharath", ""]]}, {"id": "1904.08503", "submitter": "Assaf Arbelle", "authors": "Assaf Arbelle, Eliav Elul and Tammy Riklin Raviv", "title": "QANet -- Quality Assurance Network for Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel Deep Learning framework, which quantitatively estimates\nimage segmentation quality without the need for human inspection or labeling.\nWe refer to this method as a Quality Assurance Network -- QANet. Specifically,\ngiven an image and a `proposed' corresponding segmentation, obtained by any\nmethod including manual annotation, the QANet solves a regression problem in\norder to estimate a predefined quality measure with respect to the unknown\nground truth. The QANet is by no means yet another segmentation method.\nInstead, it performs a multi-level, multi-feature comparison of an\nimage-segmentation pair based on a unique network architecture, called the\nRibCage.\n  To demonstrate the strength of the QANet, we addressed the evaluation of\ninstance segmentation using two different datasets from different domains,\nnamely, high throughput live cell microscopy images from the Cell Segmentation\nBenchmark and natural images of plants from the Leaf Segmentation Challenge.\nWhile synthesized segmentations were used to train the QANet, it was tested on\nsegmentations obtained by publicly available methods that participated in the\ndifferent challenges. We show that the QANet accurately estimates the scores of\nthe evaluated segmentations with respect to the hidden ground truth, as\npublished by the challenges' organizers.\n  The code is available at: TBD.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 08:38:57 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 12:57:36 GMT"}, {"version": "v3", "created": "Wed, 3 Jul 2019 08:39:52 GMT"}, {"version": "v4", "created": "Wed, 18 Sep 2019 08:43:24 GMT"}, {"version": "v5", "created": "Tue, 5 Nov 2019 19:09:56 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Arbelle", "Assaf", ""], ["Elul", "Eliav", ""], ["Raviv", "Tammy Riklin", ""]]}, {"id": "1904.08504", "submitter": "Takashi Matsubara", "authors": "Kenta Hama, Takashi Matsubara, Kuniaki Uehara, Jianfei Cai", "title": "Exploring Uncertainty Measures for Image-Caption Embedding-and-Retrieval\n  Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the wide development of black-box machine learning algorithms,\nparticularly deep neural network (DNN), the practical demand for the\nreliability assessment is rapidly rising. On the basis of the concept that\n`Bayesian deep learning knows what it does not know,' the uncertainty of DNN\noutputs has been investigated as a reliability measure for the classification\nand regression tasks. However, in the image-caption retrieval task, well-known\nsamples are not always easy-to-retrieve samples. This study investigates two\naspects of image-caption embedding-and-retrieval systems. On one hand, we\nquantify feature uncertainty by considering image-caption embedding as a\nregression task, and use it for model averaging, which can improve the\nretrieval performance. On the other hand, we further quantify posterior\nuncertainty by considering the retrieval as a classification task, and use it\nas a reliability measure, which can greatly improve the retrieval performance\nby rejecting uncertain queries. The consistent performance of two uncertainty\nmeasures is observed with different datasets (MS COCO and Flickr30k), different\ndeep learning architectures (dropout and batch normalization), and different\nsimilarity functions.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 12:19:09 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Hama", "Kenta", ""], ["Matsubara", "Takashi", ""], ["Uehara", "Kuniaki", ""], ["Cai", "Jianfei", ""]]}, {"id": "1904.08514", "submitter": "Rui Qiao", "authors": "Rui Qiao, Ngoc Hieu Tran, Lei Xin, Baozhen Shan, Ming Li, Ali Ghodsi", "title": "DeepNovoV2: Better de novo peptide sequencing with deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.BM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized cancer vaccines are envisioned as the next generation rational\ncancer immunotherapy. The key step in developing personalized therapeutic\ncancer vaccines is to identify tumor-specific neoantigens that are on the\nsurface of tumor cells. A promising method for this is through de novo peptide\nsequencing from mass spectrometry data. In this paper we introduce DeepNovoV2,\nthe state-of-the-art model for peptide sequencing. In DeepNovoV2, a spectrum is\ndirectly represented as a set of (m/z, intensity) pairs, therefore it does not\nsuffer from the accuracy-speed/memory trade-off problem. The model combines an\norder invariant network structure (T-Net) and recurrent neural networks and\nprovides a complete end-to-end training and prediction framework to sequence\npatterns of peptides. Our experiments on a wide variety of data from different\nspecies show that DeepNovoV2 outperforms previous state-of-the-art methods,\nachieving 13.01-23.95\\% higher accuracy at the peptide level.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 21:50:03 GMT"}, {"version": "v2", "created": "Wed, 22 May 2019 15:49:15 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Qiao", "Rui", ""], ["Tran", "Ngoc Hieu", ""], ["Xin", "Lei", ""], ["Shan", "Baozhen", ""], ["Li", "Ming", ""], ["Ghodsi", "Ali", ""]]}, {"id": "1904.08516", "submitter": "Guanxiong Liu", "authors": "Guanxiong Liu, Issa Khalil, Abdallah Khreishah", "title": "ZK-GanDef: A GAN based Zero Knowledge Adversarial Training Defense for\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Network classifiers have been used successfully in a wide range of\napplications. However, their underlying assumption of attack free environment\nhas been defied by adversarial examples. Researchers tried to develop defenses;\nhowever, existing approaches are still far from providing effective solutions\nto this evolving problem. In this paper, we design a generative adversarial net\n(GAN) based zero knowledge adversarial training defense, dubbed ZK-GanDef,\nwhich does not consume adversarial examples during training. Therefore,\nZK-GanDef is not only efficient in training but also adaptive to new\nadversarial examples. This advantage comes at the cost of small degradation in\ntest accuracy compared to full knowledge approaches. Our experiments show that\nZK-GanDef enhances test accuracy on adversarial examples by up-to 49.17%\ncompared to zero knowledge approaches. More importantly, its test accuracy is\nclose to that of the state-of-the-art full knowledge approaches (maximum\ndegradation of 8.46%), while taking much less training time.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 21:52:20 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Liu", "Guanxiong", ""], ["Khalil", "Issa", ""], ["Khreishah", "Abdallah", ""]]}, {"id": "1904.08528", "submitter": "Reazul Hasan Russel", "authors": "Reazul H. Russel and Tianyi Gu and Marek Petrik", "title": "Robust Exploration with Tight Bayesian Plausibility Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimism about the poorly understood states and actions is the main driving\nforce of exploration for many provably-efficient reinforcement learning\nalgorithms. We propose optimism in the face of sensible value functions (OFVF)-\na novel data-driven Bayesian algorithm to constructing Plausibility sets for\nMDPs to explore robustly minimizing the worst case exploration cost. The method\ncomputes policies with tighter optimistic estimates for exploration by\nintroducing two new ideas. First, it is based on Bayesian posterior\ndistributions rather than distribution-free bounds. Second, OFVF does not\nconstruct plausibility sets as simple confidence intervals. Confidence\nintervals as plausibility sets are a sufficient but not a necessary condition.\nOFVF uses the structure of the value function to optimize the location and\nshape of the plausibility set to guarantee upper bounds directly without\nnecessarily enforcing the requirement for the set to be a confidence interval.\nOFVF proceeds in an episodic manner, where the duration of the episode is fixed\nand known. Our algorithm is inherently Bayesian and can leverage prior\ninformation. Our theoretical analysis shows the robustness of OFVF, and the\nempirical results demonstrate its practical promise.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 22:54:50 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Russel", "Reazul H.", ""], ["Gu", "Tianyi", ""], ["Petrik", "Marek", ""]]}, {"id": "1904.08540", "submitter": "Christian Parkinson", "authors": "Christian Parkinson, Kevin Huynh, Deanna Needell", "title": "Matrix Completion With Selective Sampling", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion is a classical problem in data science wherein one attempts\nto reconstruct a low-rank matrix while only observing some subset of the\nentries. Previous authors have phrased this problem as a nuclear norm\nminimization problem. Almost all previous work assumes no explicit structure of\nthe matrix and uses uniform sampling to decide the observed entries. We suggest\nmethods for selective sampling in the case where we have some knowledge about\nthe structure of the matrix and are allowed to design the observation set.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 23:57:19 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Parkinson", "Christian", ""], ["Huynh", "Kevin", ""], ["Needell", "Deanna", ""]]}, {"id": "1904.08542", "submitter": "Vinay Verma Kumar", "authors": "Vinay Kumar Verma, Aakansha Mishra, Ashish Mishra and Piyush Rai", "title": "Generative Model for Zero-Shot Sketch-Based Image Retrieval", "comments": "Accepted at CVPR-Workshop 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a probabilistic model for Sketch-Based Image Retrieval (SBIR)\nwhere, at retrieval time, we are given sketches from novel classes, that were\nnot present at training time. Existing SBIR methods, most of which rely on\nlearning class-wise correspondences between sketches and images, typically work\nwell only for previously seen sketch classes, and result in poor retrieval\nperformance on novel classes. To address this, we propose a generative model\nthat learns to generate images, conditioned on a given novel class sketch. This\nenables us to reduce the SBIR problem to a standard image-to-image search\nproblem. Our model is based on an inverse auto-regressive flow based\nvariational autoencoder, with a feedback mechanism to ensure robust image\ngeneration. We evaluate our model on two very challenging datasets, Sketchy,\nand TU Berlin, with novel train-test split. The proposed approach significantly\noutperforms various baselines on both the datasets.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 00:11:04 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Verma", "Vinay Kumar", ""], ["Mishra", "Aakansha", ""], ["Mishra", "Ashish", ""], ["Rai", "Piyush", ""]]}, {"id": "1904.08544", "submitter": "Vatsal Sharan", "authors": "Vatsal Sharan, Aaron Sidford, Gregory Valiant", "title": "Memory-Sample Tradeoffs for Linear Regression with Small Error", "comments": "A few minor edits over previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of performing linear regression over a stream of\n$d$-dimensional examples, and show that any algorithm that uses a subquadratic\namount of memory exhibits a slower rate of convergence than can be achieved\nwithout memory constraints. Specifically, consider a sequence of labeled\nexamples $(a_1,b_1), (a_2,b_2)\\ldots,$ with $a_i$ drawn independently from a\n$d$-dimensional isotropic Gaussian, and where $b_i = \\langle a_i, x\\rangle +\n\\eta_i,$ for a fixed $x \\in \\mathbb{R}^d$ with $\\|x\\|_2 = 1$ and with\nindependent noise $\\eta_i$ drawn uniformly from the interval\n$[-2^{-d/5},2^{-d/5}].$ We show that any algorithm with at most $d^2/4$ bits of\nmemory requires at least $\\Omega(d \\log \\log \\frac{1}{\\epsilon})$ samples to\napproximate $x$ to $\\ell_2$ error $\\epsilon$ with probability of success at\nleast $2/3$, for $\\epsilon$ sufficiently small as a function of $d$. In\ncontrast, for such $\\epsilon$, $x$ can be recovered to error $\\epsilon$ with\nprobability $1-o(1)$ with memory $O\\left(d^2 \\log(1/\\epsilon)\\right)$ using $d$\nexamples. This represents the first nontrivial lower bounds for regression with\nsuper-linear memory, and may open the door for strong memory/sample tradeoffs\nfor continuous optimization.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 00:24:17 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 04:20:20 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Sharan", "Vatsal", ""], ["Sidford", "Aaron", ""], ["Valiant", "Gregory", ""]]}, {"id": "1904.08547", "submitter": "Qiaoyu Tan", "authors": "Qiaoyu Tan, Ninghao Liu, Xia Hu", "title": "Deep Representation Learning for Social Network Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social network analysis is an important problem in data mining. A fundamental\nstep for analyzing social networks is to encode network data into\nlow-dimensional representations, i.e., network embeddings, so that the network\ntopology structure and other attribute information can be effectively\npreserved. Network representation leaning facilitates further applications such\nas classification, link prediction, anomaly detection and clustering. In\naddition, techniques based on deep neural networks have attracted great\ninterests over the past a few years. In this survey, we conduct a comprehensive\nreview of current literature in network representation learning utilizing\nneural network models. First, we introduce the basic models for learning node\nrepresentations in homogeneous networks. Meanwhile, we will also introduce some\nextensions of the base models in tackling more complex scenarios, such as\nanalyzing attributed networks, heterogeneous networks and dynamic networks.\nThen, we introduce the techniques for embedding subgraphs. After that, we\npresent the applications of network representation learning. At the end, we\ndiscuss some promising research directions for future work.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 00:42:11 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Tan", "Qiaoyu", ""], ["Liu", "Ninghao", ""], ["Hu", "Xia", ""]]}, {"id": "1904.08548", "submitter": "Michael Minyi Zhang", "authors": "Sinead A. Williamson, Michael Minyi Zhang, Paul Damien", "title": "A New Class of Time Dependent Latent Factor Models with Applications", "comments": null, "journal-ref": "Journal of Machine Learning Research 21(27):1-24, 2020", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, observed data are influenced by some combination of\nlatent causes. For example, suppose sensors are placed inside a building to\nrecord responses such as temperature, humidity, power consumption and noise\nlevels. These random, observed responses are typically affected by many\nunobserved, latent factors (or features) within the building such as the number\nof individuals, the turning on and off of electrical devices, power surges,\netc. These latent factors are usually present for a contiguous period of time\nbefore disappearing; further, multiple factors could be present at a time. This\npaper develops new probabilistic methodology and inference methods for random\nobject generation influenced by latent features exhibiting temporal\npersistence. Every datum is associated with subsets of a potentially infinite\nnumber of hidden, persistent features that account for temporal dynamics in an\nobservation. The ensuing class of dynamic models constructed by adapting the\nIndian Buffet Process --- a probability measure on the space of random,\nunbounded binary matrices --- finds use in a variety of applications arising in\noperations, signal processing, biomedicine, marketing, image analysis, etc.\nIllustrations using synthetic and real data are provided.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 00:43:00 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Williamson", "Sinead A.", ""], ["Zhang", "Michael Minyi", ""], ["Damien", "Paul", ""]]}, {"id": "1904.08554", "submitter": "Shawn Shan", "authors": "Shawn Shan, Emily Wenger, Bolun Wang, Bo Li, Haitao Zheng, Ben Y. Zhao", "title": "Gotta Catch 'Em All: Using Honeypots to Catch Adversarial Attacks on\n  Neural Networks", "comments": null, "journal-ref": "Proceedings of the 2020 ACM SIGSAC Conference on Computer and\n  Communications Security", "doi": "10.1145/3372297.3417231", "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks (DNN) are known to be vulnerable to adversarial attacks.\nNumerous efforts either try to patch weaknesses in trained models, or try to\nmake it difficult or costly to compute adversarial examples that exploit them.\nIn our work, we explore a new \"honeypot\" approach to protect DNN models. We\nintentionally inject trapdoors, honeypot weaknesses in the classification\nmanifold that attract attackers searching for adversarial examples. Attackers'\noptimization algorithms gravitate towards trapdoors, leading them to produce\nattacks similar to trapdoors in the feature space. Our defense then identifies\nattacks by comparing neuron activation signatures of inputs to those of\ntrapdoors. In this paper, we introduce trapdoors and describe an implementation\nof a trapdoor-enabled defense. First, we analytically prove that trapdoors\nshape the computation of adversarial attacks so that attack inputs will have\nfeature representations very similar to those of trapdoors. Second, we\nexperimentally show that trapdoor-protected models can detect, with high\naccuracy, adversarial examples generated by state-of-the-art attacks (PGD,\noptimization-based CW, Elastic Net, BPDA), with negligible impact on normal\nclassification. These results generalize across classification domains,\nincluding image, facial, and traffic-sign recognition. We also present\nsignificant results measuring trapdoors' robustness against customized adaptive\nattacks (countermeasures).\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 01:17:23 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 17:31:04 GMT"}, {"version": "v3", "created": "Mon, 4 Nov 2019 01:38:59 GMT"}, {"version": "v4", "created": "Sat, 22 Feb 2020 17:42:21 GMT"}, {"version": "v5", "created": "Wed, 25 Mar 2020 14:43:21 GMT"}, {"version": "v6", "created": "Mon, 28 Sep 2020 04:32:23 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Shan", "Shawn", ""], ["Wenger", "Emily", ""], ["Wang", "Bolun", ""], ["Li", "Bo", ""], ["Zheng", "Haitao", ""], ["Zhao", "Ben Y.", ""]]}, {"id": "1904.08558", "submitter": "Ying Wang", "authors": "Ying Wang, Xiao Xu, Tao Jin, Xiang Li, Guotong Xie, Jianmin Wang", "title": "Inpatient2Vec: Medical Representation Learning for Inpatients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning (RL) plays an important role in extracting proper\nrepresentations from complex medical data for various analyzing tasks, such as\npatient grouping, clinical endpoint prediction and medication recommendation.\nMedical data can be divided into two typical categories, outpatient and\ninpatient, that have different data characteristics. However, few of existing\nRL methods are specially designed for inpatients data, which have strong\ntemporal relations and consistent diagnosis. In addition, for unordered medical\nactivity set, existing medical RL methods utilize a simple pooling strategy,\nwhich would result in indistinguishable contributions among the activities for\nlearning. In this work, weproposeInpatient2Vec, anovelmodel for learning three\nkinds of representations for inpatient, including medical activity, hospital\nday and diagnosis. A multi-layer self-attention mechanism with two training\ntasks is designed to capture the inpatient data characteristics and process the\nunordered set. Using a real-world dataset, we demonstrate that the proposed\napproach outperforms the competitive baselines on semantic similarity\nmeasurement and clinical events prediction tasks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 01:32:24 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 07:14:53 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Wang", "Ying", ""], ["Xu", "Xiao", ""], ["Jin", "Tao", ""], ["Li", "Xiang", ""], ["Xie", "Guotong", ""], ["Wang", "Jianmin", ""]]}, {"id": "1904.08575", "submitter": "Mihai Cucuringu", "authors": "Mihai Cucuringu, Peter Davies, Aldo Glielmo, and Hemant Tyagi", "title": "SPONGE: A generalized eigenproblem for clustering signed networks", "comments": "33 pages, 18 figures", "journal-ref": "AISTATS 2019", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a principled and theoretically sound spectral method for $k$-way\nclustering in signed graphs, where the affinity measure between nodes takes\neither positive or negative values. Our approach is motivated by social balance\ntheory, where the task of clustering aims to decompose the network into\ndisjoint groups, such that individuals within the same group are connected by\nas many positive edges as possible, while individuals from different groups are\nconnected by as many negative edges as possible. Our algorithm relies on a\ngeneralized eigenproblem formulation inspired by recent work on constrained\nclustering. We provide theoretical guarantees for our approach in the setting\nof a signed stochastic block model, by leveraging tools from matrix\nperturbation theory and random matrix theory. An extensive set of numerical\nexperiments on both synthetic and real data shows that our approach compares\nfavorably with state-of-the-art methods for signed clustering, especially for\nlarge number of clusters and sparse measurement graphs.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 02:55:22 GMT"}, {"version": "v2", "created": "Sun, 19 May 2019 12:22:00 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Cucuringu", "Mihai", ""], ["Davies", "Peter", ""], ["Glielmo", "Aldo", ""], ["Tyagi", "Hemant", ""]]}, {"id": "1904.08576", "submitter": "Mohsen Bayati", "authors": "Nima Hamidi and Mohsen Bayati", "title": "On Low-rank Trace Regression under General Sampling Distribution", "comments": "32 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing number of modern statistical learning problems involve estimating a\nlarge number of parameters from a (smaller) number of noisy observations. In a\nsubset of these problems (matrix completion, matrix compressed sensing, and\nmulti-task learning) the unknown parameters form a high-dimensional matrix B*,\nand two popular approaches for the estimation are convex relaxation of\nrank-penalized regression or non-convex optimization. It is also known that\nthese estimators satisfy near optimal error bounds under assumptions on rank,\ncoherence, or spikiness of the unknown matrix.\n  In this paper, we introduce a unifying technique for analyzing all of these\nproblems via both estimators that leads to short proofs for the existing\nresults as well as new results. Specifically, first we introduce a general\nnotion of spikiness for B* and consider a general family of estimators and\nprove non-asymptotic error bounds for the their estimation error. Our approach\nrelies on a generic recipe to prove restricted strong convexity for the\nsampling operator of the trace regression. Second, and most notably, we prove\nsimilar error bounds when the regularization parameter is chosen via K-fold\ncross-validation. This result is significant in that existing theory on\ncross-validated estimators do not apply to our setting since our estimators are\nnot known to satisfy their required notion of stability. Third, we study\napplications of our general results to four subproblems of (1) matrix\ncompletion, (2) multi-task learning, (3) compressed sensing with Gaussian\nensembles, and (4) compressed sensing with factored measurements. For (1), (3),\nand (4) we recover matching error bounds as those found in the literature, and\nfor (2) we obtain (to the best of our knowledge) the first such error bound. We\nalso demonstrate how our frameworks applies to the exact recovery problem in\n(3) and (4).\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 02:56:00 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 04:28:00 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Hamidi", "Nima", ""], ["Bayati", "Mohsen", ""]]}, {"id": "1904.08594", "submitter": "Sriram Ravula", "authors": "Sriram Ravula, Alexandros G. Dimakis", "title": "One-dimensional Deep Image Prior for Time Series Inverse Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the Deep Image Prior (DIP) framework to one-dimensional signals.\nDIP is using a randomly initialized convolutional neural network (CNN) to solve\nlinear inverse problems by optimizing over weights to fit the observed\nmeasurements. Our main finding is that properly tuned one-dimensional\nconvolutional architectures provide an excellent Deep Image Prior for various\ntypes of temporal signals including audio, biological signals, and sensor\nmeasurements. We show that our network can be used in a variety of recovery\ntasks including missing value imputation, blind denoising, and compressed\nsensing from random Gaussian projections. The key challenge is how to avoid\noverfitting by carefully tuning early stopping, total variation, and weight\ndecay regularization. Our method requires up to 4 times fewer measurements than\nLasso and outperforms NLM-VAMP for random Gaussian measurements on audio\nsignals, has similar imputation performance to a Kalman state-space model on a\nvariety of data, and outperforms wavelet filtering in removing additive noise\nfrom air-quality sensor readings.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 05:29:54 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Ravula", "Sriram", ""], ["Dimakis", "Alexandros G.", ""]]}, {"id": "1904.08598", "submitter": "Tatjana Chavdarova", "authors": "Tatjana Chavdarova, Gauthier Gidel, Fran\\c{c}ois Fleuret and Simon\n  Lacoste-Julien", "title": "Reducing Noise in GAN Training with Variance Reduced Extragradient", "comments": "latest NeurIPS'19 version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the effect of the stochastic gradient noise on the training of\ngenerative adversarial networks (GANs) and show that it can prevent the\nconvergence of standard game optimization methods, while the batch version\nconverges. We address this issue with a novel stochastic variance-reduced\nextragradient (SVRE) optimization algorithm, which for a large class of games\nimproves upon the previous convergence rates proposed in the literature. We\nobserve empirically that SVRE performs similarly to a batch method on MNIST\nwhile being computationally cheaper, and that SVRE yields more stable GAN\ntraining on standard datasets.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 06:02:24 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 21:37:18 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2020 18:40:55 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Chavdarova", "Tatjana", ""], ["Gidel", "Gauthier", ""], ["Fleuret", "Fran\u00e7ois", ""], ["Lacoste-Julien", "Simon", ""]]}, {"id": "1904.08613", "submitter": "Kazi Nazmul Haque", "authors": "Kazi Nazmul Haque, Siddique Latif, and Rajib Rana", "title": "Disentangled Representation Learning with Information Maximizing\n  Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning disentangled representation from any unlabelled data is a\nnon-trivial problem. In this paper we propose Information Maximising\nAutoencoder (InfoAE) where the encoder learns powerful disentangled\nrepresentation through maximizing the mutual information between the\nrepresentation and given information in an unsupervised fashion. We have\nevaluated our model on MNIST dataset and achieved 98.9 ($\\pm .1$) $\\%$ test\naccuracy while using complete unsupervised training.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 07:25:19 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Haque", "Kazi Nazmul", ""], ["Latif", "Siddique", ""], ["Rana", "Rajib", ""]]}, {"id": "1904.08622", "submitter": "Andreas Bittracher", "authors": "Andreas Bittracher, Stefan Klus, Boumediene Hamzi, P\\'eter Koltai,\n  Christof Sch\\\"utte", "title": "Dimensionality Reduction of Complex Metastable Systems via Kernel\n  Embeddings of Transition Manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel kernel-based machine learning algorithm for identifying\nthe low-dimensional geometry of the effective dynamics of high-dimensional\nmultiscale stochastic systems. Recently, the authors developed a mathematical\nframework for the computation of optimal reaction coordinates of such systems\nthat is based on learning a parametrization of a low-dimensional transition\nmanifold in a certain function space. In this article, we enhance this approach\nby embedding and learning this transition manifold in a reproducing kernel\nHilbert space, exploiting the favorable properties of kernel embeddings. Under\nmild assumptions on the kernel, the manifold structure is shown to be preserved\nunder the embedding, and distortion bounds can be derived. This leads to a more\nrobust and more efficient algorithm compared to previous parametrization\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 07:48:19 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 10:54:56 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Bittracher", "Andreas", ""], ["Klus", "Stefan", ""], ["Hamzi", "Boumediene", ""], ["Koltai", "P\u00e9ter", ""], ["Sch\u00fctte", "Christof", ""]]}, {"id": "1904.08693", "submitter": "Niki Kilbertus", "authors": "Timothy D. Gebhard, Niki Kilbertus, Ian Harry, Bernhard Sch\\\"olkopf", "title": "Convolutional neural networks: a magic bullet for gravitational-wave\n  detection?", "comments": "First two authors contributed equally; appeared at Phys. Rev. D", "journal-ref": "Phys. Rev. D 100, 063015 (2019)", "doi": "10.1103/PhysRevD.100.063015", "report-no": null, "categories": "astro-ph.IM astro-ph.HE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, machine learning techniques, in particular\nconvolutional neural networks, have been investigated as a method to replace or\ncomplement traditional matched filtering techniques that are used to detect the\ngravitational-wave signature of merging black holes. However, to date, these\nmethods have not yet been successfully applied to the analysis of long\nstretches of data recorded by the Advanced LIGO and Virgo gravitational-wave\nobservatories. In this work, we critically examine the use of convolutional\nneural networks as a tool to search for merging black holes. We identify the\nstrengths and limitations of this approach, highlight some common pitfalls in\ntranslating between machine learning and gravitational-wave astronomy, and\ndiscuss the interdisciplinary challenges. In particular, we explain in detail\nwhy convolutional neural networks alone cannot be used to claim a statistically\nsignificant gravitational-wave detection. However, we demonstrate how they can\nstill be used to rapidly flag the times of potential signals in the data for a\nmore detailed follow-up. Our convolutional neural network architecture as well\nas the proposed performance metrics are better suited for this task than a\nstandard binary classifications scheme. A detailed evaluation of our approach\non Advanced LIGO data demonstrates the potential of such systems as trigger\ngenerators. Finally, we sound a note of caution by constructing adversarial\nexamples, which showcase interesting \"failure modes\" of our model, where inputs\nwith no visible resemblance to real gravitational-wave signals are identified\nas such by the network with high confidence.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 11:13:43 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 07:39:52 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Gebhard", "Timothy D.", ""], ["Kilbertus", "Niki", ""], ["Harry", "Ian", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1904.08745", "submitter": "An-Phi Nguyen", "authors": "Guillaume Jaume, An-phi Nguyen, Mar\\'ia Rodr\\'iguez Mart\\'inez,\n  Jean-Philippe Thiran, Maria Gabrani", "title": "edGNN: a Simple and Powerful GNN for Directed Labeled Graphs", "comments": "Representation Learning on Graphs and Manifolds @ ICLR19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of a graph neural network (GNN) to leverage both the graph\ntopology and graph labels is fundamental to building discriminative node and\ngraph embeddings. Building on previous work, we theoretically show that edGNN,\nour model for directed labeled graphs, is as powerful as the Weisfeiler-Lehman\nalgorithm for graph isomorphism. Our experiments support our theoretical\nfindings, confirming that graph neural networks can be used effectively for\ninference problems on directed graphs with both node and edge labels. Code\navailable at https://github.com/guillaumejaume/edGNN.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 13:04:56 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 12:03:18 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Jaume", "Guillaume", ""], ["Nguyen", "An-phi", ""], ["Mart\u00ednez", "Mar\u00eda Rodr\u00edguez", ""], ["Thiran", "Jean-Philippe", ""], ["Gabrani", "Maria", ""]]}, {"id": "1904.08764", "submitter": "Jaakko Sahlsten", "authors": "Jaakko Sahlsten, Joel Jaskari, Jyri Kivinen, Lauri Turunen, Esa\n  Jaanio, Kustaa Hietala and Kimmo Kaski", "title": "Deep Learning Fundus Image Analysis for Diabetic Retinopathy and Macular\n  Edema Grading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Diabetes is a globally prevalent disease that can cause visible microvascular\ncomplications such as diabetic retinopathy and macular edema in the human eye\nretina, the images of which are today used for manual disease screening. This\nlabor-intensive task could greatly benefit from automatic detection using deep\nlearning technique. Here we present a deep learning system that identifies\nreferable diabetic retinopathy comparably or better than presented in the\nprevious studies, although we use only a small fraction of images (<1/4) in\ntraining but are aided with higher image resolutions. We also provide novel\nresults for five different screening and clinical grading systems for diabetic\nretinopathy and macular edema classification, including results for accurately\nclassifying images according to clinical five-grade diabetic retinopathy and\nfour-grade diabetic macular edema scales. These results suggest, that a deep\nlearning system could increase the cost-effectiveness of screening while\nattaining higher than recommended performance, and that the system could be\napplied in clinical examinations requiring finer grading.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 08:00:40 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Sahlsten", "Jaakko", ""], ["Jaskari", "Joel", ""], ["Kivinen", "Jyri", ""], ["Turunen", "Lauri", ""], ["Jaanio", "Esa", ""], ["Hietala", "Kustaa", ""], ["Kaski", "Kimmo", ""]]}, {"id": "1904.08779", "submitter": "Daniel Park", "authors": "Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph,\n  Ekin D. Cubuk, Quoc V. Le", "title": "SpecAugment: A Simple Data Augmentation Method for Automatic Speech\n  Recognition", "comments": "5 pages, 3 figures, 6 tables; v3: references added", "journal-ref": "Proc. Interspeech 2019, 2613-2617", "doi": "10.21437/Interspeech.2019-2680", "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SpecAugment, a simple data augmentation method for speech\nrecognition. SpecAugment is applied directly to the feature inputs of a neural\nnetwork (i.e., filter bank coefficients). The augmentation policy consists of\nwarping the features, masking blocks of frequency channels, and masking blocks\nof time steps. We apply SpecAugment on Listen, Attend and Spell networks for\nend-to-end speech recognition tasks. We achieve state-of-the-art performance on\nthe LibriSpeech 960h and Swichboard 300h tasks, outperforming all prior work.\nOn LibriSpeech, we achieve 6.8% WER on test-other without the use of a language\nmodel, and 5.8% WER with shallow fusion with a language model. This compares to\nthe previous state-of-the-art hybrid system of 7.5% WER. For Switchboard, we\nachieve 7.2%/14.6% on the Switchboard/CallHome portion of the Hub5'00 test set\nwithout the use of a language model, and 6.8%/14.1% with shallow fusion, which\ncompares to the previous state-of-the-art hybrid system at 8.3%/17.3% WER.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 17:53:38 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 21:56:06 GMT"}, {"version": "v3", "created": "Tue, 3 Dec 2019 18:19:07 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Park", "Daniel S.", ""], ["Chan", "William", ""], ["Zhang", "Yu", ""], ["Chiu", "Chung-Cheng", ""], ["Zoph", "Barret", ""], ["Cubuk", "Ekin D.", ""], ["Le", "Quoc V.", ""]]}, {"id": "1904.08784", "submitter": "Tianyu Shi", "authors": "Chenyang Xi, Tianyu Shi, Yuankai Wu, Lijun Sun", "title": "Efficient Motion Planning for Automated Lane Change based on Imitation\n  Learning and Mixed-Integer Optimization", "comments": "Accepted by IEEE ITSC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent motion planning is one of the core components in automated\nvehicles, which has received extensive interests. Traditional motion planning\nmethods suffer from several drawbacks in terms of optimality, efficiency and\ngeneralization capability. Sampling based methods cannot guarantee the\noptimality of the generated trajectories. Whereas the optimization-based\nmethods are not able to perform motion planning in real-time, and limited by\nthe simplified formalization. In this work, we propose a learning-based\napproach to handle those shortcomings. Mixed Integer Quadratic Problem based\noptimization (MIQP) is used to generate the optimal lane-change trajectories\nwhich served as the training dataset for learning-based action generation\nalgorithms. A hierarchical supervised learning model is devised to make the\nfast lane-change decision. Numerous experiments have been conducted to evaluate\nthe optimality, efficiency, and generalization capability of the proposed\napproach. The experimental results indicate that the proposed model outperforms\nseveral commonly used motion planning baselines.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 13:47:17 GMT"}, {"version": "v2", "created": "Sun, 1 Sep 2019 11:48:35 GMT"}, {"version": "v3", "created": "Sat, 7 Mar 2020 14:29:52 GMT"}, {"version": "v4", "created": "Fri, 8 May 2020 21:26:02 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Xi", "Chenyang", ""], ["Shi", "Tianyu", ""], ["Wu", "Yuankai", ""], ["Sun", "Lijun", ""]]}, {"id": "1904.08804", "submitter": "George Panagopoulos", "authors": "George Panagopoulos, Fragkiskos D. Malliaros, Michalis Vazirgiannis", "title": "Multi-task Learning for Influence Estimation and Maximization", "comments": "IEEE TKDE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of influence maximization when the social network is\naccompanied by diffusion cascades. In prior works, such information is used to\ncompute influence probabilities, which is utilized by stochastic diffusion\nmodels in influence maximization. Motivated by the recent criticism on the\neffectiveness of diffusion models as well as the galloping advancements in\ninfluence learning, we propose IMINFECTOR (Influence Maximization with\nINFluencer vECTORs), a unified approach that uses representations learned from\ndiffusion cascades to perform model-independent influence maximization that\nscales in real-world datasets. The first part of our methodology is a\nmulti-task neural network that learns embeddings of nodes that initiate\ncascades (influencer vectors) and embeddings of nodes that participate in them\n(susceptible vectors). The norm of an influencer vector captures the ability of\nthe node to create lengthy cascades and is used to estimate the expected\ninfluence spread and reduce the number of candidate seeds. In addition, the\ncombination of influencer and susceptible vectors form the diffusion\nprobabilities between nodes. These are used to reformulate the network as a\nbipartite graph and propose a greedy solution to influence maximization that\nretains the theoretical guarantees.We a pply our method in three sizable\nnetworks with diffusion cascades and evaluate it using cascades from future\ntime steps. IMINFECTOR is able to scale in all of them and outperforms various\ncompetitive algorithms and metrics from the diverse landscape of influence\nmaximization in terms of efficiency and seed set quality.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 14:30:54 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 12:15:22 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2020 14:46:19 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Panagopoulos", "George", ""], ["Malliaros", "Fragkiskos D.", ""], ["Vazirgiannis", "Michalis", ""]]}, {"id": "1904.08827", "submitter": "Bahareh Tolooshams", "authors": "Bahareh Tolooshams, Sourav Dey, Demba Ba", "title": "Deep Residual Autoencoders for Expectation Maximization-inspired\n  Dictionary Learning", "comments": null, "journal-ref": "in IEEE Transactions on Neural Networks and Learning Systems, pp.\n  1-15, 2020", "doi": "10.1109/TNNLS.2020.3005348", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a neural-network architecture, termed the constrained recurrent\nsparse autoencoder (CRsAE), that solves convolutional dictionary learning\nproblems, thus establishing a link between dictionary learning and neural\nnetworks. Specifically, we leverage the interpretation of the\nalternating-minimization algorithm for dictionary learning as an approximate\nExpectation-Maximization algorithm to develop autoencoders that enable the\nsimultaneous training of the dictionary and regularization parameter (ReLU\nbias). The forward pass of the encoder approximates the sufficient statistics\nof the E-step as the solution to a sparse coding problem, using an iterative\nproximal gradient algorithm called FISTA. The encoder can be interpreted either\nas a recurrent neural network or as a deep residual network, with two-sided\nReLU non-linearities in both cases. The M-step is implemented via a two-stage\nback-propagation. The first stage relies on a linear decoder applied to the\nencoder and a norm-squared loss. It parallels the dictionary update step in\ndictionary learning. The second stage updates the regularization parameter by\napplying a loss function to the encoder that includes a prior on the parameter\nmotivated by Bayesian statistics. We demonstrate in an image-denoising task\nthat CRsAE learns Gabor-like filters, and that the EM-inspired approach for\nlearning biases is superior to the conventional approach. In an application to\nrecordings of electrical activity from the brain, we demonstrate that CRsAE\nlearns realistic spike templates and speeds up the process of identifying spike\ntimes by 900x compared to algorithms based on convex optimization.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 15:08:10 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 15:33:31 GMT"}, {"version": "v3", "created": "Sun, 18 Oct 2020 16:17:46 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Tolooshams", "Bahareh", ""], ["Dey", "Sourav", ""], ["Ba", "Demba", ""]]}, {"id": "1904.08831", "submitter": "Matthew A. Wright", "authors": "Matthew A. Wright, Simon F. G. Ehlers, Roberto Horowitz", "title": "Neural-Attention-Based Deep Learning Architectures for Modeling Traffic\n  Dynamics on Lane Graphs", "comments": "To appear at 2019 IEEE Conference on Intelligent Transportation\n  Systems", "journal-ref": null, "doi": "10.1109/ITSC.2019.8917174", "report-no": null, "categories": "cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks can be powerful tools, but require careful\napplication-specific design to ensure that the most informative relationships\nin the data are learnable. In this paper, we apply deep neural networks to the\nnonlinear spatiotemporal physics problem of vehicle traffic dynamics. We\nconsider problems of estimating macroscopic quantities (e.g., the queue at an\nintersection) at a lane level. First-principles modeling at the lane scale has\nbeen a challenge due to complexities in modeling social behaviors like lane\nchanges, and those behaviors' resultant macro-scale effects. Following domain\nknowledge that upstream/downstream lanes and neighboring lanes affect each\nothers' traffic flows in distinct ways, we apply a form of neural attention\nthat allows the neural network layers to aggregate information from different\nlanes in different manners. Using a microscopic traffic simulator as a testbed,\nwe obtain results showing that an attentional neural network model can use\ninformation from nearby lanes to improve predictions, and, that explicitly\nencoding the lane-to-lane relationship types significantly improves\nperformance. We also demonstrate the transfer of our learned neural network to\na more complex road network, discuss how its performance degradation may be\nattributable to new traffic behaviors induced by increased topological\ncomplexity, and motivate learning dynamics models from many road network\ntopologies.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 15:14:10 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 20:43:43 GMT"}, {"version": "v3", "created": "Sun, 14 Jul 2019 19:03:37 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Wright", "Matthew A.", ""], ["Ehlers", "Simon F. G.", ""], ["Horowitz", "Roberto", ""]]}, {"id": "1904.08915", "submitter": "Steven Kearnes", "authors": "Steven Kearnes, Li Li, Patrick Riley", "title": "Decoding Molecular Graph Embeddings with Reinforcement Learning", "comments": "Presented at the ICML 2019 Workshop on Learning and Reasoning with\n  Graph-Structured Data. Copyright 2019 by the author(s)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present RL-VAE, a graph-to-graph variational autoencoder that uses\nreinforcement learning to decode molecular graphs from latent embeddings.\nMethods have been described previously for graph-to-graph autoencoding, but\nthese approaches require sophisticated decoders that increase the complexity of\ntraining and evaluation (such as requiring parallel encoders and decoders or\nnon-trivial graph matching). Here, we repurpose a simple graph generator to\nenable efficient decoding and generation of molecular graphs.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 17:50:57 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 23:31:26 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Kearnes", "Steven", ""], ["Li", "Li", ""], ["Riley", "Patrick", ""]]}, {"id": "1904.08930", "submitter": "Joie Yeahuay Wu", "authors": "Surya Teja Devarakonda and Joie Yeahuay Wu and Yi Ren Fung and\n  Madalina Fiterau", "title": "FLARe: Forecasting by Learning Anticipated Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": "PMLR 106:53-65", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational models that forecast the progression of Alzheimer's disease at\nthe patient level are extremely useful tools for identifying high risk cohorts\nfor early intervention and treatment planning. The state-of-the-art work in\nthis area proposes models that forecast by using latent representations\nextracted from the longitudinal data across multiple modalities, including\nvolumetric information extracted from medical scans and demographic info. These\nmodels incorporate the time horizon, which is the amount of time between the\nlast recorded visit and the future visit, by directly concatenating a\nrepresentation of it to the data latent representation. In this paper, we\npresent a model which generates a sequence of latent representations of the\npatient status across the time horizon, providing more informative modeling of\nthe temporal relationships between the patient's history and future visits. Our\nproposed model outperforms the baseline in terms of forecasting accuracy and F1\nscore with the added benefit of robustly handling missing visits.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 18:04:38 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2019 04:12:07 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Devarakonda", "Surya Teja", ""], ["Wu", "Joie Yeahuay", ""], ["Fung", "Yi Ren", ""], ["Fiterau", "Madalina", ""]]}, {"id": "1904.08933", "submitter": "Ali Yazdizadeh", "authors": "Ali Yazdizadeh, Zachary Patterson, Bilal Farooq", "title": "Ensemble Convolutional Neural Networks for Mode Inference in Smartphone\n  Travel Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop ensemble Convolutional Neural Networks (CNNs) to classify the\ntransportation mode of trip data collected as part of a large-scale smartphone\ntravel survey in Montreal, Canada. Our proposed ensemble library is composed of\na series of CNN models with different hyper-parameter values and CNN\narchitectures. In our final model, we combine the output of CNN models using\n\"average voting\", \"majority voting\" and \"optimal weights\" methods. Furthermore,\nwe exploit the ensemble library by deploying a Random Forest model as a\nmeta-learner. The ensemble method with random forest as meta-learner shows an\naccuracy of 91.8% which surpasses the other three ensemble combination methods,\nas well as other comparable models reported in the literature. The \"majority\nvoting\" and \"optimal weights\" combination methods result in prediction accuracy\nrates around 89%, while \"average voting\" is able to achieve an accuracy of only\n85%.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 00:24:21 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Yazdizadeh", "Ali", ""], ["Patterson", "Zachary", ""], ["Farooq", "Bilal", ""]]}, {"id": "1904.08935", "submitter": "Alan Gee", "authors": "Alan H. Gee, Diego Garcia-Olano, Joydeep Ghosh, and David Paydarfar", "title": "Explaining Deep Classification of Time-Series Data with Learned\n  Prototypes", "comments": "The first two authors contributed equally. Accepted May 20, Presented\n  Jun 14, 2019 at the ICML Time-series Workshop in Long Beach, CA, USA.\n  Accepted June 15, Presented Aug 11, 2019 at the IJCAI Workshop on Knowledge\n  Discovery in Healthcare Data in Macao, China. Formal proceedings available in\n  the CEUR Workshop Proceedings (http://ceur-ws.org/Vol-2429/)", "journal-ref": "Proceedings of the 4th International Workshop on Knowledge\n  Discovery in Healthcare Data, co-located with the 28th International Joint\n  Conference on Artificial Intelligence (IJCAI 2019)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of deep learning networks raises a need for explainable AI so\nthat users and domain experts can be confident applying them to high-risk\ndecisions. In this paper, we leverage data from the latent space induced by\ndeep learning models to learn stereotypical representations or \"prototypes\"\nduring training to elucidate the algorithmic decision-making process. We study\nhow leveraging prototypes effect classification decisions of two dimensional\ntime-series data in a few different settings: (1) electrocardiogram (ECG)\nwaveforms to detect clinical bradycardia, a slowing of heart rate, in preterm\ninfants, (2) respiration waveforms to detect apnea of prematurity, and (3)\naudio waveforms to classify spoken digits. We improve upon existing models by\noptimizing for increased prototype diversity and robustness, visualize how\nthese prototypes in the latent space are used by the model to distinguish\nclasses, and show that prototypes are capable of learning features on two\ndimensional time-series data to produce explainable insights during\nclassification tasks. We show that the prototypes are capable of learning\nreal-world features - bradycardia in ECG, apnea in respiration, and\narticulation in speech - as well as features within sub-classes. Our novel work\nleverages learned prototypical framework on two dimensional time-series data to\nproduce explainable insights during classification tasks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 07:14:45 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 04:48:06 GMT"}, {"version": "v3", "created": "Thu, 5 Sep 2019 02:47:22 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Gee", "Alan H.", ""], ["Garcia-Olano", "Diego", ""], ["Ghosh", "Joydeep", ""], ["Paydarfar", "David", ""]]}, {"id": "1904.08936", "submitter": "Anupiya Nugaliyadde Mr", "authors": "Anupiya Nugaliyadde, Kok Wai Wong, Ferdous Sohel and Hong Xie", "title": "Language Modeling through Long Term Memory Network", "comments": "The paper is accepted to be published in IJCNN 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNN), Long Short-Term Memory Networks (LSTM), and\nMemory Networks which contain memory are popularly used to learn patterns in\nsequential data. Sequential data has long sequences that hold relationships.\nRNN can handle long sequences but suffers from the vanishing and exploding\ngradient problems. While LSTM and other memory networks address this problem,\nthey are not capable of handling long sequences (50 or more data points long\nsequence patterns). Language modelling requiring learning from longer sequences\nare affected by the need for more information in memory. This paper introduces\nLong Term Memory network (LTM), which can tackle the exploding and vanishing\ngradient problems and handles long sequences without forgetting. LTM is\ndesigned to scale data in the memory and gives a higher weight to the input in\nthe sequence. LTM avoid overfitting by scaling the cell state after achieving\nthe optimal results. The LTM is tested on Penn treebank dataset, and Text8\ndataset and LTM achieves test perplexities of 83 and 82 respectively. 650 LTM\ncells achieved a test perplexity of 67 for Penn treebank, and 600 cells\nachieved a test perplexity of 77 for Text8. LTM achieves state of the art\nresults by only using ten hidden LTM cells for both datasets.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 09:19:25 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Nugaliyadde", "Anupiya", ""], ["Wong", "Kok Wai", ""], ["Sohel", "Ferdous", ""], ["Xie", "Hong", ""]]}, {"id": "1904.08939", "submitter": "Anh Nguyen", "authors": "Anh Nguyen and Jason Yosinski and Jeff Clune", "title": "Understanding Neural Networks via Feature Visualization: A survey", "comments": "A book chapter in an Interpretable ML book\n  (http://www.interpretable-ml.org/book/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A neuroscience method to understanding the brain is to find and study the\npreferred stimuli that highly activate an individual cell or groups of cells.\nRecent advances in machine learning enable a family of methods to synthesize\npreferred stimuli that cause a neuron in an artificial or biological brain to\nfire strongly. Those methods are known as Activation Maximization (AM) or\nFeature Visualization via Optimization. In this chapter, we (1) review existing\nAM techniques in the literature; (2) discuss a probabilistic interpretation for\nAM; and (3) review the applications of AM in debugging and explaining networks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 15:46:26 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Nguyen", "Anh", ""], ["Yosinski", "Jason", ""], ["Clune", "Jeff", ""]]}, {"id": "1904.08983", "submitter": "Adam Polyak", "authors": "Adam Polyak, Lior Wolf, Yaniv Taigman", "title": "TTS Skins: Speaker Conversion via ASR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fully convolutional wav-to-wav network for converting between\nspeakers' voices, without relying on text. Our network is based on an\nencoder-decoder architecture, where the encoder is pre-trained for the task of\nAutomatic Speech Recognition, and a multi-speaker waveform decoder is trained\nto reconstruct the original signal in an autoregressive manner. We train the\nnetwork on narrated audiobooks, and demonstrate multi-voice TTS in those\nvoices, by converting the voice of a TTS robot.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 19:35:24 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 11:18:14 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Polyak", "Adam", ""], ["Wolf", "Lior", ""], ["Taigman", "Yaniv", ""]]}, {"id": "1904.08990", "submitter": "Sajjad Abdoli", "authors": "Sajjad Abdoli, Patrick Cardinal, Alessandro Lameiras Koerich", "title": "End-to-End Environmental Sound Classification using a 1D Convolutional\n  Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an end-to-end approach for environmental sound\nclassification based on a 1D Convolution Neural Network (CNN) that learns a\nrepresentation directly from the audio signal. Several convolutional layers are\nused to capture the signal's fine time structure and learn diverse filters that\nare relevant to the classification task. The proposed approach can deal with\naudio signals of any length as it splits the signal into overlapped frames\nusing a sliding window. Different architectures considering several input sizes\nare evaluated, including the initialization of the first convolutional layer\nwith a Gammatone filterbank that models the human auditory filter response in\nthe cochlea. The performance of the proposed end-to-end approach in classifying\nenvironmental sounds was assessed on the UrbanSound8k dataset and the\nexperimental results have shown that it achieves 89% of mean accuracy.\nTherefore, the propose approach outperforms most of the state-of-the-art\napproaches that use handcrafted features or 2D representations as input.\nFurthermore, the proposed approach has a small number of parameters compared to\nother architectures found in the literature, which reduces the amount of data\nrequired for training.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 20:07:03 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Abdoli", "Sajjad", ""], ["Cardinal", "Patrick", ""], ["Koerich", "Alessandro Lameiras", ""]]}, {"id": "1904.08994", "submitter": "Lilian Weng", "authors": "Lilian Weng", "title": "From GAN to WGAN", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explains the math behind a generative adversarial network (GAN)\nmodel and why it is hard to be trained. Wasserstein GAN is intended to improve\nGANs' training by adopting a smooth metric for measuring the distance between\ntwo probability distributions.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 20:15:08 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Weng", "Lilian", ""]]}, {"id": "1904.09007", "submitter": "Nico Engel", "authors": "Nico Engel, Stefan Hoermann, Markus Horn, Vasileios Belagiannis, Klaus\n  Dietmayer", "title": "DeepLocalization: Landmark-based Self-Localization with Deep Neural\n  Networks", "comments": "Accepted for publication by the IEEE Intelligent Transportation\n  Systems Conference (ITSC 2019), Auckland, New Zealand", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of vehicle self-localization from multi-modal sensor\ninformation and a reference map. The map is generated off-line by extracting\nlandmarks from the vehicle's field of view, while the measurements are\ncollected similarly on the fly. Our goal is to determine the autonomous\nvehicle's pose from the landmark measurements and map landmarks. To learn this\nmapping, we propose DeepLocalization, a deep neural network that regresses the\nvehicle's translation and rotation parameters from unordered and dynamic input\nlandmarks. The proposed network architecture is robust to changes of the\ndynamic environment and can cope with a small number of extracted landmarks.\nDuring the training process we rely on synthetically generated ground-truth. In\nour experiments, we evaluate two inference approaches in real-world scenarios.\nWe show that DeepLocalization can be combined with regular GPS signals and\nfiltering algorithms such as the extended Kalman filter. Our approach achieves\nstate-of-the-art accuracy and is about ten times faster than the related work.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 20:41:10 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 09:24:45 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Engel", "Nico", ""], ["Hoermann", "Stefan", ""], ["Horn", "Markus", ""], ["Belagiannis", "Vasileios", ""], ["Dietmayer", "Klaus", ""]]}, {"id": "1904.09014", "submitter": "Travis Dick", "authors": "Maria-Florina Balcan, Travis Dick, Wesley Pegden", "title": "Semi-bandit Optimization in the Dispersed Setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of data-driven algorithm design is to obtain high-performing\nalgorithms for specific application domains using machine learning and data.\nAcross many fields in AI, science, and engineering, practitioners will often\nfix a family of parameterized algorithms and then optimize those parameters to\nobtain good performance on example instances from the application domain. In\nthe online setting, we must choose algorithm parameters for each instance as\nthey arrive, and our goal is to be competitive with the best fixed algorithm in\nhindsight.\n  There are two major challenges in online data-driven algorithm design. First,\nit can be computationally expensive to evaluate the loss functions that map\nalgorithm parameters to performance, which often require the learner to run a\ncombinatorial algorithm to measure its performance. Second, the losses can be\nextremely volatile and have sharp discontinuities. However, we show that in\nmany applications, evaluating the loss function for one algorithm choice can\nsometimes reveal the loss for a range of similar algorithms, essentially for\nfree. We develop online optimization algorithms capable of using this kind of\nextra information by working in the semi-bandit feedback setting. Our\nalgorithms achieve regret bounds that are essentially as good as algorithms\nunder full-information feedback and are significantly more computationally\nefficient. We apply our semi-bandit results to obtain the first provable\nguarantees for data-driven algorithm design for linkage-based clustering and we\nimprove the best regret bounds for designing greedy knapsack algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 21:13:31 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 21:38:28 GMT"}, {"version": "v3", "created": "Mon, 21 Dec 2020 17:11:56 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["Dick", "Travis", ""], ["Pegden", "Wesley", ""]]}, {"id": "1904.09019", "submitter": "Ferran Alet", "authors": "Ferran Alet, Adarsh K. Jeewajee, Maria Bauza, Alberto Rodriguez, Tomas\n  Lozano-Perez, Leslie Pack Kaelbling", "title": "Graph Element Networks: adaptive, structured computation and memory", "comments": "Accepted to ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the use of graph neural networks (GNNs) to model spatial processes\nin which there is no a priori graphical structure. Similar to finite element\nanalysis, we assign nodes of a GNN to spatial locations and use a computational\nprocess defined on the graph to model the relationship between an initial\nfunction defined over a space and a resulting function in the same space. We\nuse GNNs as a computational substrate, and show that the locations of the nodes\nin space as well as their connectivity can be optimized to focus on the most\ncomplex parts of the space. Moreover, this representational strategy allows the\nlearned input-output relationship to generalize over the size of the underlying\nspace and run the same model at different levels of precision, trading\ncomputation for accuracy. We demonstrate this method on a traditional PDE\nproblem, a physical prediction problem from robotics, and learning to predict\nscene images from novel viewpoints.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 21:29:02 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 06:27:38 GMT"}, {"version": "v3", "created": "Mon, 13 May 2019 19:41:04 GMT"}, {"version": "v4", "created": "Wed, 19 Jun 2019 21:00:38 GMT"}, {"version": "v5", "created": "Sun, 17 Nov 2019 16:08:04 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Alet", "Ferran", ""], ["Jeewajee", "Adarsh K.", ""], ["Bauza", "Maria", ""], ["Rodriguez", "Alberto", ""], ["Lozano-Perez", "Tomas", ""], ["Kaelbling", "Leslie Pack", ""]]}, {"id": "1904.09024", "submitter": "Alex Kearney", "authors": "Alex Kearney, Patrick M. Pilarski", "title": "When is a Prediction Knowledge?", "comments": "Accepted to RLDM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within Reinforcement Learning, there is a growing collection of research\nwhich aims to express all of an agent's knowledge of the world through\npredictions about sensation, behaviour, and time. This work can be seen not\nonly as a collection of architectural proposals, but also as the beginnings of\na theory of machine knowledge in reinforcement learning. Recent work has\nexpanded what can be expressed using predictions, and developed applications\nwhich use predictions to inform decision-making on a variety of synthetic and\nreal-world problems. While promising, we here suggest that the notion of\npredictions as knowledge in reinforcement learning is as yet underdeveloped:\nsome work explicitly refers to predictions as knowledge, what the requirements\nare for considering a prediction to be knowledge have yet to be well explored.\nThis specification of the necessary and sufficient conditions of knowledge is\nimportant; even if claims about the nature of knowledge are left implicit in\ntechnical proposals, the underlying assumptions of such claims have\nconsequences for the systems we design. These consequences manifest in both the\nway we choose to structure predictive knowledge architectures, and how we\nevaluate them. In this paper, we take a first step to formalizing predictive\nknowledge by discussing the relationship of predictive knowledge learning\nmethods to existing theories of knowledge in epistemology. Specifically, we\nexplore the relationships between Generalized Value Functions and epistemic\nnotions of Justification and Truth.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 22:12:49 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Kearney", "Alex", ""], ["Pilarski", "Patrick M.", ""]]}, {"id": "1904.09037", "submitter": "Chu Wang", "authors": "Chu Wang and Lei Tang and Yang Lu and Shujun Bian and Hirohisa Fujita\n  and Da Zhang and Zuohua Zhang and Yongning Wu", "title": "ProductNet: a Collection of High-Quality Datasets for Product\n  Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ProductNet is a collection of high-quality product datasets for better\nproduct understanding. Motivated by ImageNet, ProductNet aims at supporting\nproduct representation learning by curating product datasets of high quality\nwith properly chosen taxonomy. In this paper, the two goals of building\nhigh-quality product datasets and learning product representation support each\nother in an iterative fashion: the product embedding is obtained via a\nmulti-modal deep neural network (master model) designed to leverage product\nimage and catalog information; and in return, the embedding is utilized via\nactive learning (local model) to vastly accelerate the annotation process. For\nthe labeled data, the proposed master model yields high categorization accuracy\n(94.7% top-1 accuracy for 1240 classes), which can be used as search indices,\npartition keys, and input features for machine learning models. The product\nembedding, as well as the fined-tuned master model for a specific business\ntask, can also be used for various transfer learning tasks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 23:17:07 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Wang", "Chu", ""], ["Tang", "Lei", ""], ["Lu", "Yang", ""], ["Bian", "Shujun", ""], ["Fujita", "Hirohisa", ""], ["Zhang", "Da", ""], ["Zhang", "Zuohua", ""], ["Wu", "Yongning", ""]]}, {"id": "1904.09056", "submitter": "Sudeep Salgia", "authors": "Boshuang Huang, Sudeep Salgia, Qing Zhao", "title": "Disagreement-based Active Learning in Online Settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study online active learning for classifying streaming instances within\nthe framework of statistical learning theory. At each time, the learner either\nqueries the label of the current instance or predicts the label based on past\nseen examples. The objective is to minimize the number of queries while\nconstraining the number of prediction errors over a horizon of length $T$. We\ndevelop a disagreement-based online learning algorithm for a general hypothesis\nspace and under the Tsybakov noise. We show that the proposed algorithm has a\nlabel complexity of $O(dT^{\\frac{2-2\\alpha}{2-\\alpha}}\\log^2 T)$ under a\nconstraint of bounded regret in terms of classification errors, where $d$ is\nthe VC dimension of the hypothesis space and $\\alpha$ is the Tsybakov noise\nparameter. We further establish a matching (up to a poly-logarithmic factor)\nlower bound, demonstrating the order optimality of the proposed algorithm. We\naddress the tradeoff between label complexity and regret and show that the\nalgorithm can be modified to operate at a different point on the tradeoff\ncurve.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 03:08:34 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 19:35:03 GMT"}, {"version": "v3", "created": "Wed, 9 Oct 2019 20:07:56 GMT"}, {"version": "v4", "created": "Fri, 7 Feb 2020 20:54:04 GMT"}, {"version": "v5", "created": "Mon, 16 Nov 2020 15:29:56 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Huang", "Boshuang", ""], ["Salgia", "Sudeep", ""], ["Zhao", "Qing", ""]]}, {"id": "1904.09061", "submitter": "Jingwei Liu", "authors": "Jingwei Liu", "title": "Random Fragments Classification of Microbial Marker Clades with\n  Multi-class SVM and N-Best Algorithm", "comments": "17 pages, 59 figurea", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microbial clades modeling is a challenging problem in biology based on\nmicroarray genome sequences, especially in new species gene isolates discovery\nand category. Marker family genome sequences play important roles in describing\nspecific microbial clades within species, a framework of support vector machine\n(SVM) based microbial species classification with N-best algorithm is\nconstructed to classify the centroid marker genome fragments randomly generated\nfrom marker genome sequences on MetaRef. A time series feature extraction\nmethod is proposed by segmenting the centroid gene sequences and mapping into\ndifferent dimensional spaces. Two ways of data splitting are investigated\naccording to random splitting fragments along genome sequence (DI) , or\nseparating genome sequences into two parts (DII).Two strategies of fragments\nrecognition tasks, dimension-by-dimension and sequence--by--sequence, are\ninvestigated. The k-mer size selection, overlap of segmentation and effects of\nrandom split percents are also discussed. Experiments on 12390 maker genome\nsequences belonging to marker families of 17 species from MetaRef show that,\nboth for DI and DII in dimension-by-dimension and sequence-by-sequence\nrecognition, the recognition accuracy rates can achieve above 28\\% in top-1\ncandidate, and above 91\\% in top-10 candidate both on training and testing sets\noverall.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 03:21:48 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Liu", "Jingwei", ""]]}, {"id": "1904.09062", "submitter": "Hao Li", "authors": "Honglin Chen, Hao Li, Alexander Song, Matt Haberland, Osman Akar, Adam\n  Dhillon, Tiankuang Zhou, Andrea L. Bertozzi, P. Jeffrey Brantingham", "title": "Semi-Supervised First-Person Activity Recognition in Body-Worn Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Body-worn cameras are now commonly used for logging daily life, sports, and\nlaw enforcement activities, creating a large volume of archived footage. This\npaper studies the problem of classifying frames of footage according to the\nactivity of the camera-wearer with an emphasis on application to real-world\npolice body-worn video. Real-world datasets pose a different set of challenges\nfrom existing egocentric vision datasets: the amount of footage of different\nactivities is unbalanced, the data contains personally identifiable\ninformation, and in practice it is difficult to provide substantial training\nfootage for a supervised approach. We address these challenges by extracting\nfeatures based exclusively on motion information then segmenting the video\nfootage using a semi-supervised classification algorithm. On publicly available\ndatasets, our method achieves results comparable to, if not better than,\nsupervised and/or deep learning methods using a fraction of the training data.\nIt also shows promising results on real-world police body-worn video.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 03:22:10 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Chen", "Honglin", ""], ["Li", "Hao", ""], ["Song", "Alexander", ""], ["Haberland", "Matt", ""], ["Akar", "Osman", ""], ["Dhillon", "Adam", ""], ["Zhou", "Tiankuang", ""], ["Bertozzi", "Andrea L.", ""], ["Brantingham", "P. Jeffrey", ""]]}, {"id": "1904.09067", "submitter": "Michael Cogswell", "authors": "Michael Cogswell, Jiasen Lu, Stefan Lee, Devi Parikh, Dhruv Batra", "title": "Emergence of Compositional Language with Deep Generational Transmission", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has studied the emergence of language among deep reinforcement\nlearning agents that must collaborate to solve a task. Of particular interest\nare the factors that cause language to be compositional -- i.e., express\nmeaning by combining words which themselves have meaning. Evolutionary\nlinguists have found that in addition to structural priors like those already\nstudied in deep learning, the dynamics of transmitting language from generation\nto generation contribute significantly to the emergence of compositionality. In\nthis paper, we introduce these cultural evolutionary dynamics into language\nemergence by periodically replacing agents in a population to create a\nknowledge gap, implicitly inducing cultural transmission of language. We show\nthat this implicit cultural transmission encourages the resulting languages to\nexhibit better compositional generalization.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 04:09:12 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 19:54:23 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Cogswell", "Michael", ""], ["Lu", "Jiasen", ""], ["Lee", "Stefan", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "1904.09078", "submitter": "Jun-Ho Choi", "authors": "Jun-Ho Choi, Jong-Seok Lee", "title": "EmbraceNet: A robust deep learning architecture for multimodal\n  classification", "comments": "Code available at https://github.com/idearibosome/embracenet", "journal-ref": "Information Fusion 51 (2019) 259-270", "doi": "10.1016/j.inffus.2019.02.010", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification using multimodal data arises in many machine learning\napplications. It is crucial not only to model cross-modal relationship\neffectively but also to ensure robustness against loss of part of data or\nmodalities. In this paper, we propose a novel deep learning-based multimodal\nfusion architecture for classification tasks, which guarantees compatibility\nwith any kind of learning models, deals with cross-modal information carefully,\nand prevents performance degradation due to partial absence of data. We employ\ntwo datasets for multimodal classification tasks, build models based on our\narchitecture and other state-of-the-art models, and analyze their performance\non various situations. The results show that our architecture outperforms the\nother multimodal fusion architectures when some parts of data are not\navailable.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 04:46:29 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Choi", "Jun-Ho", ""], ["Lee", "Jong-Seok", ""]]}, {"id": "1904.09080", "submitter": "Paul Valiant", "authors": "Guy Blanc, Neha Gupta, Gregory Valiant, Paul Valiant", "title": "Implicit regularization for deep neural networks driven by an\n  Ornstein-Uhlenbeck like process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider networks, trained via stochastic gradient descent to minimize\n$\\ell_2$ loss, with the training labels perturbed by independent noise at each\niteration. We characterize the behavior of the training dynamics near any\nparameter vector that achieves zero training error, in terms of an implicit\nregularization term corresponding to the sum over the data points, of the\nsquared $\\ell_2$ norm of the gradient of the model with respect to the\nparameter vector, evaluated at each data point. This holds for networks of any\nconnectivity, width, depth, and choice of activation function. We interpret\nthis implicit regularization term for three simple settings: matrix sensing,\ntwo layer ReLU networks trained on one-dimensional data, and two layer networks\nwith sigmoid activations trained on a single datapoint. For these settings, we\nshow why this new and general implicit regularization effect drives the\nnetworks towards \"simple\" models.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 04:58:36 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 17:59:18 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Blanc", "Guy", ""], ["Gupta", "Neha", ""], ["Valiant", "Gregory", ""], ["Valiant", "Paul", ""]]}, {"id": "1904.09081", "submitter": "Yingtian Zou", "authors": "Yingtian Zou, Jiashi Feng", "title": "Hierarchical Meta Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta learning is a promising solution to few-shot learning problems. However,\nexisting meta learning methods are restricted to the scenarios where training\nand application tasks share the same out-put structure. To obtain a meta model\napplicable to the tasks with new structures, it is required to collect new\ntraining data and repeat the time-consuming meta training procedure. This makes\nthem inefficient or even inapplicable in learning to solve heterogeneous\nfew-shot learning tasks. We thus develop a novel and principled\nHierarchicalMeta Learning (HML) method. Different from existing methods that\nonly focus on optimizing the adaptability of a meta model to similar tasks, HML\nalso explicitly optimizes its generalizability across heterogeneous tasks. To\nthis end, HML first factorizes a set of similar training tasks into\nheterogeneous ones and trains the meta model over them at two levels to\nmaximize adaptation and generalization performance respectively. The resultant\nmodel can then directly generalize to new tasks. Extensive experiments on\nfew-shot classification and regression problems clearly demonstrate the\nsuperiority of HML over fine-tuning and state-of-the-art meta learning\napproaches in terms of generalization across heterogeneous tasks.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 05:12:57 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Zou", "Yingtian", ""], ["Feng", "Jiashi", ""]]}, {"id": "1904.09096", "submitter": "Ricardo Pio Monti", "authors": "Ricardo Pio Monti, Kun Zhang, Aapo Hyvarinen", "title": "Causal Discovery with General Non-Linear Relationships Using Non-Linear\n  ICA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of inferring causal relationships between two or more\npassively observed variables. While the problem of such causal discovery has\nbeen extensively studied especially in the bivariate setting, the majority of\ncurrent methods assume a linear causal relationship, and the few methods which\nconsider non-linear dependencies usually make the assumption of additive noise.\nHere, we propose a framework through which we can perform causal discovery in\nthe presence of general non-linear relationships. The proposed method is based\non recent progress in non-linear independent component analysis and exploits\nthe non-stationarity of observations in order to recover the underlying sources\nor latent disturbances. We show rigorously that in the case of bivariate causal\ndiscovery, such non-linear ICA can be used to infer the causal direction via a\nseries of independence tests. We further propose an alternative measure of\ncausal direction based on asymptotic approximations to the likelihood ratio, as\nwell as an extension to multivariate causal discovery. We demonstrate the\ncapabilities of the proposed method via a series of simulation studies and\nconclude with an application to neuroimaging data.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 06:44:53 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Monti", "Ricardo Pio", ""], ["Zhang", "Kun", ""], ["Hyvarinen", "Aapo", ""]]}, {"id": "1904.09098", "submitter": "Ashutosh Pednekar", "authors": "Ashutosh Mahesh Pednekar", "title": "Optimal initialization of K-means using Particle Swarm Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the use of an optimization algorithm, namely PSO to\ndecide the initial centroids in K-means, to eventually get better accuracy. The\nvectorized notation of the optimal centroids can be thought of as entities in\nan optimization space, where the accuracy of K-means over a random subset of\nthe data could act as a fitness measure. The resultant optimal vector can be\nused as the initial centroids for K-means.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 06:59:10 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Pednekar", "Ashutosh Mahesh", ""]]}, {"id": "1904.09109", "submitter": "Hye Won Chung", "authors": "Youngjae Min and Hye Won Chung", "title": "Shallow Neural Network can Perfectly Classify an Object following\n  Separable Probability Distribution", "comments": "5 pages. To be presented at the 2019 IEEE International Symposium on\n  Information Theory (ISIT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Guiding the design of neural networks is of great importance to save enormous\nresources consumed on empirical decisions of architectural parameters. This\npaper constructs shallow sigmoid-type neural networks that achieve 100%\naccuracy in classification for datasets following a linear separability\ncondition. The separability condition in this work is more relaxed than the\nwidely used linear separability. Moreover, the constructed neural network\nguarantees perfect classification for any datasets sampled from a separable\nprobability distribution. This generalization capability comes from the\nsaturation of sigmoid function that exploits small margins near the boundaries\nof intervals formed by the separable probability distribution.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 08:00:11 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Min", "Youngjae", ""], ["Chung", "Hye Won", ""]]}, {"id": "1904.09135", "submitter": "Claus Aranha", "authors": "Fabio Henrique Kiyoiti dos Santos Tanaka, Claus Aranha", "title": "Data Augmentation Using GANs", "comments": "Submitted for ACML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose the use of Generative Adversarial Networks (GAN) to\ngenerate artificial training data for machine learning tasks. The generation of\nartificial training data can be extremely useful in situations such as\nimbalanced data sets, performing a role similar to SMOTE or ADASYN. It is also\nuseful when the data contains sensitive information, and it is desirable to\navoid using the original data set as much as possible (example: medical data).\nWe test our proposal on benchmark data sets using different network\narchitectures, and show that a Decision Tree (DT) classifier trained using the\ntraining data generated by the GAN reached the same, (and surprisingly\nsometimes better), accuracy and recall than a DT trained on the original data\nset.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 10:05:33 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Tanaka", "Fabio Henrique Kiyoiti dos Santos", ""], ["Aranha", "Claus", ""]]}, {"id": "1904.09162", "submitter": "Tong Mu", "authors": "Tong Mu, Karan Goel, Emma Brunskill", "title": "PLOTS: Procedure Learning from Observations using Subtask Structure", "comments": "To appear in the proceedings of AAMAS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many cases an intelligent agent may want to learn how to mimic a single\nobserved demonstrated trajectory. In this work we consider how to perform such\nprocedural learning from observation, which could help to enable agents to\nbetter use the enormous set of video data on observation sequences. Our\napproach exploits the properties of this setting to incrementally build an open\nloop action plan that can yield the desired subsequence, and can be used in\nboth Markov and partially observable Markov domains. In addition, procedures\ncommonly involve repeated extended temporal action subsequences. Our method\noptimistically explores actions to leverage potential repeated structure in the\nprocedure. In comparing to some state-of-the-art approaches we find that our\nexplicit procedural learning from observation method is about 100 times faster\nthan policy-gradient based approaches that learn a stochastic policy and is\nfaster than model based approaches as well. We also find that performing\noptimistic action selection yields substantial speed ups when latent dynamical\nstructure is present.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 20:57:11 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Mu", "Tong", ""], ["Goel", "Karan", ""], ["Brunskill", "Emma", ""]]}, {"id": "1904.09187", "submitter": "Tianlin Liu", "authors": "Tianlin Liu, Lyle Ungar, Jo\\~ao Sedoc", "title": "Continual Learning for Sentence Representations Using Conceptors", "comments": "Accepted by NAACL-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed representations of sentences have become ubiquitous in natural\nlanguage processing tasks. In this paper, we consider a continual learning\nscenario for sentence representations: Given a sequence of corpora, we aim to\noptimize the sentence encoder with respect to the new corpus while maintaining\nits accuracy on the old corpora. To address this problem, we propose to\ninitialize sentence encoders with the help of corpus-independent features, and\nthen sequentially update sentence encoders using Boolean operations of\nconceptor matrices to learn corpus-dependent features. We evaluate our approach\non semantic textual similarity tasks and show that our proposed sentence\nencoder can continually learn features from new corpora while retaining its\ncompetence on previously encountered corpora.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 06:37:15 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Liu", "Tianlin", ""], ["Ungar", "Lyle", ""], ["Sedoc", "Jo\u00e3o", ""]]}, {"id": "1904.09211", "submitter": "Philippe Lacaille", "authors": "Philippe Lacaille", "title": "Analyzing the benefits of communication channels between deep learning\n  models", "comments": "77 pages, 7 figures, Master's Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As artificial intelligence systems spread to more diverse and larger tasks in\nmany domains, the machine learning algorithms, and in particular the deep\nlearning models and the databases required to train them are getting bigger\nthemselves. Some algorithms do allow for some scaling of large computations by\nleveraging data parallelism. However, they often require a large amount of data\nto be exchanged in order to ensure the shared knowledge throughout the compute\nnodes is accurate.\n  In this work, the effect of different levels of communications between deep\nlearning models is studied, in particular how it affects performance. The first\napproach studied looks at decentralizing the numerous computations that are\ndone in parallel in training procedures such as synchronous and asynchronous\nstochastic gradient descent. In this setting, a simplified communication that\nconsists of exchanging low bandwidth outputs between compute nodes can be\nbeneficial. In the following chapter, the communication protocol is slightly\nmodified to further include training instructions. Indeed, this is studied in a\nsimplified setup where a pre-trained model, analogous to a teacher, can\ncustomize a randomly initialized model's training procedure to accelerate\nlearning. Finally, a communication channel where two deep learning models can\nexchange a purposefully crafted language is explored while allowing for\ndifferent ways of optimizing that language.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 14:33:57 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Lacaille", "Philippe", ""]]}, {"id": "1904.09212", "submitter": "Khalil Elkhalil", "authors": "Khalil Elkhalil, Abla Kammoun, Xiangliang Zhang, Mohamed-Slim Alouini,\n  Tareq Al-Naffouri", "title": "Risk Convergence of Centered Kernel Ridge Regression with Large\n  Dimensional Data", "comments": "Submitted to IEEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2020.2975939", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper carries out a large dimensional analysis of a variation of kernel\nridge regression that we call \\emph{centered kernel ridge regression} (CKRR),\nalso known in the literature as kernel ridge regression with offset. This\nmodified technique is obtained by accounting for the bias in the regression\nproblem resulting in the old kernel ridge regression but with \\emph{centered}\nkernels. The analysis is carried out under the assumption that the data is\ndrawn from a Gaussian distribution and heavily relies on tools from random\nmatrix theory (RMT). Under the regime in which the data dimension and the\ntraining size grow infinitely large with fixed ratio and under some mild\nassumptions controlling the data statistics, we show that both the empirical\nand the prediction risks converge to a deterministic quantities that describe\nin closed form fashion the performance of CKRR in terms of the data statistics\nand dimensions. Inspired by this theoretical result, we subsequently build a\nconsistent estimator of the prediction risk based on the training data which\nallows to optimally tune the design parameters. A key insight of the proposed\nanalysis is the fact that asymptotically a large class of kernels achieve the\nsame minimum prediction risk. This insight is validated with both synthetic and\nreal data.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 14:35:39 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Elkhalil", "Khalil", ""], ["Kammoun", "Abla", ""], ["Zhang", "Xiangliang", ""], ["Alouini", "Mohamed-Slim", ""], ["Al-Naffouri", "Tareq", ""]]}, {"id": "1904.09228", "submitter": "Jasper C.H. Lee", "authors": "Jasper C.H. Lee, Paul Valiant", "title": "Uncertainty about Uncertainty: Optimal Adaptive Algorithms for\n  Estimating Mixtures of Unknown Coins", "comments": "Full paper updated to reflect the new result in our SODA 2021\n  proceedings version: our new sample complexity lower bound includes\n  dependence on the failure probability, and hence is simultaneously tight in\n  all of the problem parameters up to a constant multiplicative factor", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a mixture between two populations of coins, \"positive\" coins that each\nhave -- unknown and potentially different -- bias $\\geq\\frac{1}{2}+\\Delta$ and\n\"negative\" coins with bias $\\leq\\frac{1}{2}-\\Delta$, we consider the task of\nestimating the fraction $\\rho$ of positive coins to within additive error\n$\\epsilon$. We achieve an upper and lower bound of\n$\\Theta(\\frac{\\rho}{\\epsilon^2\\Delta^2}\\log\\frac{1}{\\delta})$ samples for a\n$1-\\delta$ probability of success, where crucially, our lower bound applies to\nall fully-adaptive algorithms. Thus, our sample complexity bounds have tight\ndependence for every relevant problem parameter. A crucial component of our\nlower bound proof is a decomposition lemma (see Lemmas 17 and 18) showing how\nto assemble partially-adaptive bounds into a fully-adaptive bound, which may be\nof independent interest: though we invoke it for the special case of Bernoulli\nrandom variables (coins), it applies to general distributions. We present\nsimulation results to demonstrate the practical efficacy of our approach for\nrealistic problem parameters for crowdsourcing applications, focusing on the\n\"rare events\" regime where $\\rho$ is small. The fine-grained adaptive flavor of\nboth our algorithm and lower bound contrasts with much previous work in\ndistributional testing and learning.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 15:22:04 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 04:29:50 GMT"}, {"version": "v3", "created": "Fri, 5 Feb 2021 05:07:48 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Lee", "Jasper C. H.", ""], ["Valiant", "Paul", ""]]}, {"id": "1904.09235", "submitter": "Eyke H\\\"ullermeier", "authors": "Vu-Linh Nguyen and Eyke H\\\"ullermeier", "title": "Reliable Multi-label Classification: Prediction with Partial Abstention", "comments": "19 pages, 12 figures", "journal-ref": "Proceedings AAAI-20, Thirty-Fourth AAAI Conference on Artificial\n  Intelligence, New York, USA, 2020", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In contrast to conventional (single-label) classification, the setting of\nmultilabel classification (MLC) allows an instance to belong to several classes\nsimultaneously. Thus, instead of selecting a single class label, predictions\ntake the form of a subset of all labels. In this paper, we study an extension\nof the setting of MLC, in which the learner is allowed to partially abstain\nfrom a prediction, that is, to deliver predictions on some but not necessarily\nall class labels. We propose a formalization of MLC with abstention in terms of\na generalized loss minimization problem and present first results for the case\nof the Hamming loss, rank loss, and F-measure, both theoretical and\nexperimental.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 15:33:06 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2020 08:49:16 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Nguyen", "Vu-Linh", ""], ["H\u00fcllermeier", "Eyke", ""]]}, {"id": "1904.09237", "submitter": "Sashank J. Reddi", "authors": "Sashank J. Reddi, Satyen Kale, Sanjiv Kumar", "title": "On the Convergence of Adam and Beyond", "comments": "Appeared in ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recently proposed stochastic optimization methods that have been\nsuccessfully used in training deep networks such as RMSProp, Adam, Adadelta,\nNadam are based on using gradient updates scaled by square roots of exponential\nmoving averages of squared past gradients. In many applications, e.g. learning\nwith large output spaces, it has been empirically observed that these\nalgorithms fail to converge to an optimal solution (or a critical point in\nnonconvex settings). We show that one cause for such failures is the\nexponential moving average used in the algorithms. We provide an explicit\nexample of a simple convex optimization setting where Adam does not converge to\nthe optimal solution, and describe the precise problems with the previous\nanalysis of Adam algorithm. Our analysis suggests that the convergence issues\ncan be fixed by endowing such algorithms with `long-term memory' of past\ngradients, and propose new variants of the Adam algorithm which not only fix\nthe convergence issues but often also lead to improved empirical performance.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 16:21:38 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Reddi", "Sashank J.", ""], ["Kale", "Satyen", ""], ["Kumar", "Sanjiv", ""]]}, {"id": "1904.09265", "submitter": "Zhize Li", "authors": "Zhize Li", "title": "SSRGD: Simple Stochastic Recursive Gradient Descent for Escaping Saddle\n  Points", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze stochastic gradient algorithms for optimizing nonconvex problems.\nIn particular, our goal is to find local minima (second-order stationary\npoints) instead of just finding first-order stationary points which may be some\nbad unstable saddle points. We show that a simple perturbed version of\nstochastic recursive gradient descent algorithm (called SSRGD) can find an\n$(\\epsilon,\\delta)$-second-order stationary point with\n$\\widetilde{O}(\\sqrt{n}/\\epsilon^2 + \\sqrt{n}/\\delta^4 + n/\\delta^3)$\nstochastic gradient complexity for nonconvex finite-sum problems. As a\nby-product, SSRGD finds an $\\epsilon$-first-order stationary point with\n$O(n+\\sqrt{n}/\\epsilon^2)$ stochastic gradients. These results are almost\noptimal since Fang et al. [2018] provided a lower bound\n$\\Omega(\\sqrt{n}/\\epsilon^2)$ for finding even just an $\\epsilon$-first-order\nstationary point. We emphasize that SSRGD algorithm for finding second-order\nstationary points is as simple as for finding first-order stationary points\njust by adding a uniform perturbation sometimes, while all other algorithms for\nfinding second-order stationary points with similar gradient complexity need to\ncombine with a negative-curvature search subroutine (e.g., Neon2 [Allen-Zhu and\nLi, 2018]). Moreover, the simple SSRGD algorithm gets a simpler analysis.\nBesides, we also extend our results from nonconvex finite-sum problems to\nnonconvex online (expectation) problems, and prove the corresponding\nconvergence results.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 16:59:46 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 19:56:33 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Li", "Zhize", ""]]}, {"id": "1904.09306", "submitter": "Mansur Arief", "authors": "Zhiyuan Huang, Mansur Arief, Henry Lam, Ding Zhao", "title": "Evaluation Uncertainty in Data-Driven Self-Driving Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Safety evaluation of self-driving technologies has been extensively studied.\nOne recent approach uses Monte Carlo based evaluation to estimate the\noccurrence probabilities of safety-critical events as safety measures. These\nMonte Carlo samples are generated from stochastic input models constructed\nbased on real-world data. In this paper, we propose an approach to assess the\nimpact on the probability estimates from the evaluation procedures due to the\nestimation error caused by data variability. Our proposed method merges the\nclassical bootstrap method for estimating input uncertainty with a likelihood\nratio based scheme to reuse experiment outputs. This approach is economical and\nefficient in terms of implementation costs in assessing input uncertainty for\nthe evaluation of self-driving technology. We use an example in autonomous\nvehicle (AV) safety evaluation to demonstrate the proposed approach as a\ndiagnostic tool for the quality of the fitted input model.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 18:21:48 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 02:08:25 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Huang", "Zhiyuan", ""], ["Arief", "Mansur", ""], ["Lam", "Henry", ""], ["Zhao", "Ding", ""]]}, {"id": "1904.09317", "submitter": "Christopher Kanan", "authors": "Kushal Kafle, Robik Shrestha, Christopher Kanan", "title": "Challenges and Prospects in Vision and Language Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language grounded image understanding tasks have often been proposed as a\nmethod for evaluating progress in artificial intelligence. Ideally, these tasks\nshould test a plethora of capabilities that integrate computer vision,\nreasoning, and natural language understanding. However, rather than behaving as\nvisual Turing tests, recent studies have demonstrated state-of-the-art systems\nare achieving good performance through flaws in datasets and evaluation\nprocedures. We review the current state of affairs and outline a path forward.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 19:04:12 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 22:10:33 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Kafle", "Kushal", ""], ["Shrestha", "Robik", ""], ["Kanan", "Christopher", ""]]}, {"id": "1904.09324", "submitter": "Omer Levy", "authors": "Marjan Ghazvininejad, Omer Levy, Yinhan Liu, Luke Zettlemoyer", "title": "Mask-Predict: Parallel Decoding of Conditional Masked Language Models", "comments": "EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most machine translation systems generate text autoregressively from left to\nright. We, instead, use a masked language modeling objective to train a model\nto predict any subset of the target words, conditioned on both the input text\nand a partially masked target translation. This approach allows for efficient\niterative decoding, where we first predict all of the target words\nnon-autoregressively, and then repeatedly mask out and regenerate the subset of\nwords that the model is least confident about. By applying this strategy for a\nconstant number of iterations, our model improves state-of-the-art performance\nlevels for non-autoregressive and parallel decoding translation models by over\n4 BLEU on average. It is also able to reach within about 1 BLEU point of a\ntypical left-to-right transformer model, while decoding significantly faster.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 19:53:01 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 16:31:39 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Ghazvininejad", "Marjan", ""], ["Levy", "Omer", ""], ["Liu", "Yinhan", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "1904.09339", "submitter": "Reza Mohammadi", "authors": "Reza Mohammadi, Matthew Pratola, Maurits Kaptein", "title": "Continuous-Time Birth-Death MCMC for Bayesian Regression Tree Models", "comments": "Published at http://jmlr.org/papers/v21/19-307 in the Journal of\n  Machine Learning Research (https://www.jmlr.org)", "journal-ref": "Journal of Machine Learning Research 2020, Vol. 21, No. 201, 1-26", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision trees are flexible models that are well suited for many statistical\nregression problems. In a Bayesian framework for regression trees, Markov Chain\nMonte Carlo (MCMC) search algorithms are required to generate samples of tree\nmodels according to their posterior probabilities. The critical component of\nsuch an MCMC algorithm is to construct good Metropolis-Hastings steps for\nupdating the tree topology. However, such algorithms frequently suffering from\nlocal mode stickiness and poor mixing. As a result, the algorithms are slow to\nconverge. Hitherto, authors have primarily used discrete-time birth/death\nmechanisms for Bayesian (sums of) regression tree models to explore the model\nspace. These algorithms are efficient only if the acceptance rate is high which\nis not always the case. Here we overcome this issue by developing a new search\nalgorithm which is based on a continuous-time birth-death Markov process. This\nsearch algorithm explores the model space by jumping between parameter spaces\ncorresponding to different tree structures. In the proposed algorithm, the\nmoves between models are always accepted which can dramatically improve the\nconvergence and mixing properties of the MCMC algorithm. We provide theoretical\nsupport of the algorithm for Bayesian regression tree models and demonstrate\nits performance.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 20:37:05 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 11:55:32 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Mohammadi", "Reza", ""], ["Pratola", "Matthew", ""], ["Kaptein", "Maurits", ""]]}, {"id": "1904.09347", "submitter": "Richard Samworth", "authors": "Thomas B. Berrett and Richard J. Samworth", "title": "Efficient two-sample functional estimation and the super-oracle\n  phenomenon", "comments": "82 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the estimation of two-sample integral functionals, of the type\nthat occur naturally, for example, when the object of interest is a divergence\nbetween unknown probability densities. Our first main result is that, in wide\ngenerality, a weighted nearest neighbour estimator is efficient, in the sense\nof achieving the local asymptotic minimax lower bound. Moreover, we also prove\na corresponding central limit theorem, which facilitates the construction of\nasymptotically valid confidence intervals for the functional, having\nasymptotically minimal width. One interesting consequence of our results is the\ndiscovery that, for certain functionals, the worst-case performance of our\nestimator may improve on that of the natural `oracle' estimator, which is given\naccess to the values of the unknown densities at the observations.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 17:22:51 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Berrett", "Thomas B.", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1904.09357", "submitter": "Thiago Andrade", "authors": "Thiago Andrade and Jo\\~ao Gama", "title": "Identifying Points of Interest and Similar Individuals from Raw GPS Data", "comments": "Conference paper at Mobility IoT 2018 -\n  http://mobilityiot2018.eai-conferences.org/full-program/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smartphones and portable devices have become ubiquitous and part of\neveryone's life. Due to the fact of its portability, these devices are perfect\nto record individuals' traces and life-logging generating vast amounts of data\nat low costs. These data is emerging as a new source for studies in human\nmobility patterns raising the number of research projects and techniques aiming\nto analyze and retrieve useful information from it. The aim of this paper is to\nexplore GPS raw data from different individuals in a community and apply data\nmining algorithms to identify meaningful places in a region and describe user's\nprofiles and its similarities. We evaluate the proposed method with a\nreal-world dataset. The experimental results show that the steps performed to\nidentify points of interest (POIs) and further the similarity between the users\nare quite satisfactory serving as a supplement for urban planning and social\nnetworks.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 22:24:47 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Andrade", "Thiago", ""], ["Gama", "Jo\u00e3o", ""]]}, {"id": "1904.09365", "submitter": "Jiawei Zhang", "authors": "Jiawei Zhang", "title": "Derivative-Free Global Optimization Algorithms: Bayesian Method and\n  Lipschitzian Approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we will provide an introduction to the derivative-free\noptimization algorithms which can be potentially applied to train deep learning\nmodels. Existing deep learning model training is mostly based on the back\npropagation algorithm, which updates the model variables layers by layers with\nthe gradient descent algorithm or its variants. However, the objective\nfunctions of deep learning models to be optimized are usually non-convex and\nthe gradient descent algorithms based on the first-order derivative can get\nstuck into the local optima very easily. To resolve such a problem, various\nlocal or global optimization algorithms have been proposed, which can help\nimprove the training of deep learning models greatly. The representative\nexamples include the Bayesian methods, Shubert-Piyavskii algorithm, Direct,\nLIPO, MCS, GA, SCE, DE, PSO, ES, CMA-ES, hill climbing and simulated annealing,\netc. One part of these algorithms will be introduced in this paper (including\nthe Bayesian method and Lipschitzian approaches, e.g., Shubert-Piyavskii\nalgorithm, Direct, LIPO and MCS), and the remaining algorithms (including the\npopulation based optimization algorithms, e.g., GA, SCE, DE, PSO, ES and\nCMA-ES, and random search algorithms, e.g., hill climbing and simulated\nannealing) will be introduced in the follow-up paper [18] in detail.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 23:14:01 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Zhang", "Jiawei", ""]]}, {"id": "1904.09368", "submitter": "Jiawei Zhang", "authors": "Jiawei Zhang", "title": "Derivative-Free Global Optimization Algorithms: Population based Methods\n  and Random Search Approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we will provide an introduction to the derivative-free\noptimization algorithms which can be potentially applied to train deep learning\nmodels. Existing deep learning model training is mostly based on the back\npropagation algorithm, which updates the model variables layers by layers with\nthe gradient descent algorithm or its variants. However, the objective\nfunctions of deep learning models to be optimized are usually non-convex and\nthe gradient descent algorithms based on the first-order derivative can get\nstuck into the local optima very easily. To resolve such a problem, various\nlocal or global optimization algorithms have been proposed, which can help\nimprove the training of deep learning models greatly. The representative\nexamples include the Bayesian methods, Shubert-Piyavskii algorithm, Direct,\nLIPO, MCS, GA, SCE, DE, PSO, ES, CMA-ES, hill climbing and simulated annealing,\netc. This is a follow-up paper of [18], and we will introduce the population\nbased optimization algorithms, e.g., GA, SCE, DE, PSO, ES and CMA-ES, and\nrandom search algorithms, e.g., hill climbing and simulated annealing, in this\npaper. For the introduction to the other derivative-free optimization\nalgorithms, please refer to [18] for more information.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 23:22:49 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Zhang", "Jiawei", ""]]}, {"id": "1904.09369", "submitter": "Hakan Gokcesu", "authors": "Hakan Gokcesu and Suleyman S. Kozat", "title": "Minimax Optimal Online Stochastic Learning for Sequences of Convex\n  Functions under Sub-Gradient Observation Failures", "comments": "25 pages, 6 figures, preprint, single column", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study online convex optimization under stochastic sub-gradient observation\nfaults, where we introduce adaptive algorithms with minimax optimal regret\nguarantees. We specifically study scenarios where our sub-gradient observations\ncan be noisy or even completely missing in a stochastic manner. To this end, we\npropose algorithms based on sub-gradient descent method, which achieve tight\nminimax optimal regret bounds. When necessary, these algorithms utilize\nproperties of the underlying stochastic settings to optimize their learning\nrates (step sizes). These optimizations are the main factor in providing the\nminimax optimal performance guarantees, especially when observations are\nstochastically missing. However, in real world scenarios, these properties of\nthe underlying stochastic settings may not be revealed to the optimizer. For\nsuch a scenario, we propose a blind algorithm that estimates these properties\nempirically in a generally applicable manner. Through extensive experiments, we\nshow that this empirical approach is a natural combination of regular\nstochastic gradient descent and the minimax optimal algorithms (which work best\nfor randomized and adversarial function sequences, respectively).\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 23:24:38 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Gokcesu", "Hakan", ""], ["Kozat", "Suleyman S.", ""]]}, {"id": "1904.09370", "submitter": "Oggi Rudovic", "authors": "Ognjen Rudovic, Yuria Utsumi, Ricardo Guerrero, Kelly Peterson, Daniel\n  Rueckert, Rosalind W. Picard", "title": "Meta-Weighted Gaussian Process Experts for Personalized Forecasting of\n  AD Cognitive Changes", "comments": null, "journal-ref": "Machine Learning for Healthcare Conference (ML4HC2019)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel personalized Gaussian Process Experts (pGPE) model for\npredicting per-subject ADAS-Cog13 cognitive scores -- a significant predictor\nof Alzheimer's Disease (AD) in the cognitive domain -- over the future 6, 12,\n18, and 24 months. We start by training a population-level model using\nmulti-modal data from previously seen subjects using a base Gaussian Process\n(GP) regression. Then, we personalize this model by adapting the base GP\nsequentially over time to a new (target) subject using domain adaptive GPs, and\nalso by training subject-specific GP. While we show that these models achieve\nimproved performance when selectively applied to the forecasting task (one\nperforms better than the other on different subjects/visits), the average\nperformance per model is suboptimal. To this end, we used the notion of meta\nlearning in the proposed pGPE to design a regression-based weighting of these\nexpert models, where the expert weights are optimized for each subject and\nhis/her future visit. The results on a cohort of subjects from the ADNI dataset\nshow that this newly introduced personalized weighting of the expert models\nleads to large improvements in accurately forecasting future ADAS-Cog13 scores\nand their fine-grained changes associated with the AD progression. This\napproach has potential to help identify at-risk patients early and improve the\nconstruction of clinical trials for AD.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 23:28:11 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Rudovic", "Ognjen", ""], ["Utsumi", "Yuria", ""], ["Guerrero", "Ricardo", ""], ["Peterson", "Kelly", ""], ["Rueckert", "Daniel", ""], ["Picard", "Rosalind W.", ""]]}, {"id": "1904.09378", "submitter": "Mathieu Carri\\`ere", "authors": "Mathieu Carri\\`ere and Fr\\'ed\\'eric Chazal and Yuichi Ike and Th\\'eo\n  Lacombe and Martin Royer and Yuhei Umeda", "title": "PersLay: A Neural Network Layer for Persistence Diagrams and New Graph\n  Topological Signatures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CG cs.LG math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistence diagrams, the most common descriptors of Topological Data\nAnalysis, encode topological properties of data and have already proved pivotal\nin many different applications of data science. However, since the (metric)\nspace of persistence diagrams is not Hilbert, they end up being difficult\ninputs for most Machine Learning techniques. To address this concern, several\nvectorization methods have been put forward that embed persistence diagrams\ninto either finite-dimensional Euclidean space or (implicit) infinite\ndimensional Hilbert space with kernels. In this work, we focus on persistence\ndiagrams built on top of graphs. Relying on extended persistence theory and the\nso-called heat kernel signature, we show how graphs can be encoded by\n(extended) persistence diagrams in a provably stable way. We then propose a\ngeneral and versatile framework for learning vectorizations of persistence\ndiagrams, which encompasses most of the vectorization techniques used in the\nliterature. We finally showcase the experimental strength of our setup by\nachieving competitive scores on classification tasks on real-life graph\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 00:21:59 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 02:14:57 GMT"}, {"version": "v3", "created": "Thu, 17 Oct 2019 23:16:02 GMT"}, {"version": "v4", "created": "Mon, 9 Mar 2020 03:15:26 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Carri\u00e8re", "Mathieu", ""], ["Chazal", "Fr\u00e9d\u00e9ric", ""], ["Ike", "Yuichi", ""], ["Lacombe", "Th\u00e9o", ""], ["Royer", "Martin", ""], ["Umeda", "Yuhei", ""]]}, {"id": "1904.09396", "submitter": "Salar Fattahi", "authors": "Salar Fattahi and Nikolai Matni and Somayeh Sojoudi", "title": "Learning Sparse Dynamical Systems from a Single Sample Trajectory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of identifying sparse linear time-invariant\n(LTI) systems from a single sample trajectory generated by the system dynamics.\nWe introduce a Lasso-like estimator for the parameters of the system, taking\ninto account their sparse nature. Assuming that the system is stable, or that\nit is equipped with an initial stabilizing controller, we provide sharp\nfinite-time guarantees on the accurate recovery of both the sparsity structure\nand the parameter values of the system. In particular, we show that the\nproposed estimator can correctly identify the sparsity pattern of the system\nmatrices with high probability, provided that the length of the sample\ntrajectory exceeds a threshold. Furthermore, we show that this threshold scales\npolynomially in the number of nonzero elements in the system matrices, but\nlogarithmically in the system dimensions --- this improves on existing sample\ncomplexity bounds for the sparse system identification problem. We further\nextend these results to obtain sharp bounds on the $\\ell_{\\infty}$-norm of the\nestimation error and show how different properties of the system---such as its\nstability level and \\textit{mutual incoherency}---affect this bound. Finally,\nan extensive case study on power systems is presented to illustrate the\nperformance of the proposed estimation method.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 03:52:23 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Fattahi", "Salar", ""], ["Matni", "Nikolai", ""], ["Sojoudi", "Somayeh", ""]]}, {"id": "1904.09404", "submitter": "Saied Mahdian", "authors": "Branislav Kveton, Saied Mahdian, S. Muthukrishnan, Zheng Wen, Yikun\n  Xian", "title": "Waterfall Bandits: Learning to Sell Ads Online", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular approach to selling online advertising is by a waterfall, where a\npublisher makes sequential price offers to ad networks for an inventory, and\nchooses the winner in that order. The publisher picks the order and prices to\nmaximize her revenue. A traditional solution is to learn the demand model and\nthen subsequently solve the optimization problem for the given demand model.\nThis will incur a linear regret. We design an online learning algorithm for\nsolving this problem, which interleaves learning and optimization, and prove\nthat this algorithm has sublinear regret. We evaluate the algorithm on both\nsynthetic and real-world data, and show that it quickly learns high quality\npricing strategies. This is the first principled study of learning a waterfall\ndesign online by sequential experimentation.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 05:11:56 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Kveton", "Branislav", ""], ["Mahdian", "Saied", ""], ["Muthukrishnan", "S.", ""], ["Wen", "Zheng", ""], ["Xian", "Yikun", ""]]}, {"id": "1904.09406", "submitter": "Gert-Jan Both", "authors": "Gert-Jan Both, Subham Choudhury, Pierre Sens, Remy Kusters", "title": "DeepMoD: Deep learning for Model Discovery in noisy data", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2020.109985", "report-no": null, "categories": "physics.comp-ph q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce DeepMoD, a Deep learning based Model Discovery algorithm.\nDeepMoD discovers the partial differential equation underlying a\nspatio-temporal data set using sparse regression on a library of possible\nfunctions and their derivatives. A neural network approximates the data and\nconstructs the function library, but it also performs the sparse regression.\nThis construction makes it extremely robust to noise, applicable to small data\nsets, and, contrary to other deep learning methods, does not require a training\nset. We benchmark our approach on several physical problems such as the\nBurgers', Korteweg-de Vries and Keller-Segel equations, and find that it\nrequires as few as $\\mathcal{O}(10^2)$ samples and works at noise levels up to\n$75\\%$. Motivated by these results, we apply DeepMoD directly on noisy\nexperimental time-series data from a gel electrophoresis experiment and find\nthat it discovers the advection-diffusion equation describing this system.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 06:04:20 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 12:07:12 GMT"}, {"version": "v3", "created": "Wed, 24 Feb 2021 16:20:02 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Both", "Gert-Jan", ""], ["Choudhury", "Subham", ""], ["Sens", "Pierre", ""], ["Kusters", "Remy", ""]]}, {"id": "1904.09415", "submitter": "Xiao Chen", "authors": "Xiao Chen, Thomas Navidi, Stefano Ermon, Ram Rajagopal", "title": "Distributed generation of privacy preserving data with user\n  customization", "comments": "accepted in ICLR 2019 SafeML workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Distributed devices such as mobile phones can produce and store large amounts\nof data that can enhance machine learning models; however, this data may\ncontain private information specific to the data owner that prevents the\nrelease of the data. We wish to reduce the correlation between user-specific\nprivate information and data while maintaining the useful information. Rather\nthan learning a large model to achieve privatization from end to end, we\nintroduce a decoupling of the creation of a latent representation and the\nprivatization of data that allows user-specific privatization to occur in a\ndistributed setting with limited computation and minimal disturbance on the\nutility of the data. We leverage a Variational Autoencoder (VAE) to create a\ncompact latent representation of the data; however, the VAE remains fixed for\nall devices and all possible private labels. We then train a small generative\nfilter to perturb the latent representation based on individual preferences\nregarding the private and utility information. The small filter is trained by\nutilizing a GAN-type robust optimization that can take place on a distributed\ndevice. We conduct experiments on three popular datasets: MNIST, UCI-Adult, and\nCelebA, and give a thorough evaluation including visualizing the geometry of\nthe latent embeddings and estimating the empirical mutual information to show\nthe effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 07:58:37 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Chen", "Xiao", ""], ["Navidi", "Thomas", ""], ["Ermon", "Stefano", ""], ["Rajagopal", "Ram", ""]]}, {"id": "1904.09448", "submitter": "Vinod Kumar Chauhan", "authors": "Vinod Kumar Chauhan, Anuj Sharma, Kalpana Dahiya", "title": "LIBS2ML: A Library for Scalable Second Order Machine Learning Algorithms", "comments": "5 page JMLR library format, 4 figures. Library available as open\n  source for download at: https://github.com/jmdvinodjmd/LIBS2ML", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LIBS2ML is a library based on scalable second order learning algorithms for\nsolving large-scale problems, i.e., big data problems in machine learning.\nLIBS2ML has been developed using MEX files, i.e., C++ with MATLAB/Octave\ninterface to take the advantage of both the worlds, i.e., faster learning using\nC++ and easy I/O using MATLAB. Most of the available libraries are either in\nMATLAB/Python/R which are very slow and not suitable for large-scale learning,\nor are in C/C++ which does not have easy ways to take input and display\nresults. So LIBS2ML is completely unique due to its focus on the scalable\nsecond order methods, the hot research topic, and being based on MEX files.\nThus it provides researchers a comprehensive environment to evaluate their\nideas and it also provides machine learning practitioners an effective tool to\ndeal with the large-scale learning problems. LIBS2ML is an open-source, highly\nefficient, extensible, scalable, readable, portable and easy to use library.\nThe library can be downloaded from the URL:\n\\url{https://github.com/jmdvinodjmd/LIBS2ML}.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 14:41:05 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Chauhan", "Vinod Kumar", ""], ["Sharma", "Anuj", ""], ["Dahiya", "Kalpana", ""]]}, {"id": "1904.09489", "submitter": "Joel Ruben Antony Moniz", "authors": "Joel Ruben Antony Moniz, Barun Patra, Sarthak Garg", "title": "Compression and Localization in Reinforcement Learning for ATARI Games", "comments": "NeurIPS 2018 Deep Reinforcement Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have become commonplace in the domain of reinforcement\nlearning, but are often expensive in terms of the number of parameters needed.\nWhile compressing deep neural networks has of late assumed great importance to\novercome this drawback, little work has been done to address this problem in\nthe context of reinforcement learning agents. This work aims at making first\nsteps towards model compression in an RL agent. In particular, we compress\nnetworks to drastically reduce the number of parameters in them (to sizes less\nthan 3% of their original size), further facilitated by applying a global max\npool after the final convolution layer, and propose using Actor-Mimic in the\ncontext of compression. Finally, we show that this global max-pool allows for\nweakly supervised object localization, improving the ability to identify the\nagent's points of focus.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 19:42:50 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Moniz", "Joel Ruben Antony", ""], ["Patra", "Barun", ""], ["Garg", "Sarthak", ""]]}, {"id": "1904.09533", "submitter": "Daniel Stoller", "authors": "Saumitra Mishra, Daniel Stoller, Emmanouil Benetos, Bob L. Sturm,\n  Simon Dixon", "title": "GAN-based Generation and Automatic Selection of Explanations for Neural\n  Networks", "comments": "8 pages plus references and appendix. Accepted at the ICLR 2019\n  Workshop \"Safe Machine Learning: Specification, Robustness and Assurance\".\n  Camera-ready version. v2: Corrected page header", "journal-ref": "SafeML Workshop at the International Conference on Learning\n  Representations (ICLR) 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One way to interpret trained deep neural networks (DNNs) is by inspecting\ncharacteristics that neurons in the model respond to, such as by iteratively\noptimising the model input (e.g., an image) to maximally activate specific\nneurons. However, this requires a careful selection of hyper-parameters to\ngenerate interpretable examples for each neuron of interest, and current\nmethods rely on a manual, qualitative evaluation of each setting, which is\nprohibitively slow. We introduce a new metric that uses Fr\\'echet Inception\nDistance (FID) to encourage similarity between model activations for real and\ngenerated data. This provides an efficient way to evaluate a set of generated\nexamples for each setting of hyper-parameters. We also propose a novel\nGAN-based method for generating explanations that enables an efficient search\nthrough the input space and imposes a strong prior favouring realistic outputs.\nWe apply our approach to a classification model trained to predict whether a\nmusic audio recording contains singing voice. Our results suggest that this\nproposed metric successfully selects hyper-parameters leading to interpretable\nexamples, avoiding the need for manual evaluation. Moreover, we see that\nexamples synthesised to maximise or minimise the predicted probability of\nsinging voice presence exhibit vocal or non-vocal characteristics,\nrespectively, suggesting that our approach is able to generate suitable\nexplanations for understanding concepts learned by a neural network.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 02:54:33 GMT"}, {"version": "v2", "created": "Sat, 27 Apr 2019 09:31:26 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Mishra", "Saumitra", ""], ["Stoller", "Daniel", ""], ["Benetos", "Emmanouil", ""], ["Sturm", "Bob L.", ""], ["Dixon", "Simon", ""]]}, {"id": "1904.09559", "submitter": "Feng Yin", "authors": "Feng Yin, Lishuo Pan, Xinwei He, Tianshi Chen, Sergios Theodoridis,\n  Zhi-Quan (Tom) Luo", "title": "Linear Multiple Low-Rank Kernel Based Stationary Gaussian Processes\n  Regression for Time Series", "comments": "15 pages, 5 figures, submitted", "journal-ref": null, "doi": "10.1109/TSP.2020.3023008", "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GP) for machine learning have been studied systematically\nover the past two decades and they are by now widely used in a number of\ndiverse applications. However, GP kernel design and the associated\nhyper-parameter optimization are still hard and to a large extend open\nproblems. In this paper, we consider the task of GP regression for time series\nmodeling and analysis. The underlying stationary kernel can be approximated\narbitrarily close by a new proposed grid spectral mixture (GSM) kernel, which\nturns out to be a linear combination of low-rank sub-kernels. In the case where\na large number of the sub-kernels are used, either the Nystr\\\"{o}m or the\nrandom Fourier feature approximations can be adopted to deal efficiently with\nthe computational demands. The unknown GP hyper-parameters consist of the\nnon-negative weights of all sub-kernels as well as the noise variance; their\nestimation is performed via the maximum-likelihood (ML) estimation framework.\nTwo efficient numerical optimization methods for solving the unknown\nhyper-parameters are derived, including a sequential majorization-minimization\n(MM) method and a non-linearly constrained alternating direction of multiplier\nmethod (ADMM). The MM matches perfectly with the proven low-rank property of\nthe proposed GSM sub-kernels and turns out to be a part of efficiency, stable,\nand efficient solver, while the ADMM has the potential to generate better local\nminimum in terms of the test MSE. Experimental results, based on various\nclassic time series data sets, corroborate that the proposed GSM kernel-based\nGP regression model outperforms several salient competitors of similar kind in\nterms of prediction mean-squared-error and numerical stability.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 07:37:19 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Yin", "Feng", "", "Tom"], ["Pan", "Lishuo", "", "Tom"], ["He", "Xinwei", "", "Tom"], ["Chen", "Tianshi", "", "Tom"], ["Theodoridis", "Sergios", "", "Tom"], ["Zhi-Quan", "", "", "Tom"], ["Luo", "", ""]]}, {"id": "1904.09585", "submitter": "Serhii Havrylov", "authors": "Zhifeng Hu, Serhii Havrylov, Ivan Titov, Shay B. Cohen", "title": "Obfuscation for Privacy-preserving Syntactic Parsing", "comments": "Accepted to IWPT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of homomorphic encryption is to encrypt data such that another party\ncan operate on it without being explicitly exposed to the content of the\noriginal data. We introduce an idea for a privacy-preserving transformation on\nnatural language data, inspired by homomorphic encryption. Our primary tool is\n{\\em obfuscation}, relying on the properties of natural language. Specifically,\na given English text is obfuscated using a neural model that aims to preserve\nthe syntactic relationships of the original sentence so that the obfuscated\nsentence can be parsed instead of the original one. The model works at the word\nlevel, and learns to obfuscate each word separately by changing it into a new\nword that has a similar syntactic role. The text obfuscated by our model leads\nto better performance on three syntactic parsers (two dependency and one\nconstituency parsers) in comparison to an upper-bound random substitution\nbaseline. More specifically, the results demonstrate that as more terms are\nobfuscated (by their part of speech), the substitution upper bound\nsignificantly degrades, while the neural model maintains a relatively high\nperforming parser. All of this is done without much sacrifice of privacy\ncompared to the random substitution upper bound. We also further analyze the\nresults, and discover that the substituted words have similar syntactic\nproperties, but different semantic content, compared to the original words.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 12:09:39 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 09:38:58 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Hu", "Zhifeng", ""], ["Havrylov", "Serhii", ""], ["Titov", "Ivan", ""], ["Cohen", "Shay B.", ""]]}, {"id": "1904.09605", "submitter": "Zongqing Lu", "authors": "Jiechuan Jiang and Zongqing Lu", "title": "Generative Exploration and Exploitation", "comments": "AAAI'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse reward is one of the biggest challenges in reinforcement learning\n(RL). In this paper, we propose a novel method called Generative Exploration\nand Exploitation (GENE) to overcome sparse reward. GENE automatically generates\nstart states to encourage the agent to explore the environment and to exploit\nreceived reward signals. GENE can adaptively tradeoff between exploration and\nexploitation according to the varying distributions of states experienced by\nthe agent as the learning progresses. GENE relies on no prior knowledge about\nthe environment and can be combined with any RL algorithm, no matter on-policy\nor off-policy, single-agent or multi-agent. Empirically, we demonstrate that\nGENE significantly outperforms existing methods in three tasks with only binary\nrewards, including Maze, Maze Ant, and Cooperative Navigation. Ablation studies\nverify the emergence of progressive exploration and automatic reversing.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 14:15:24 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 11:56:23 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Jiang", "Jiechuan", ""], ["Lu", "Zongqing", ""]]}, {"id": "1904.09609", "submitter": "Ranjan Maitra", "authors": "Nicholas S. Berry and Ranjan Maitra", "title": "TiK-means: $K$-means clustering for skewed groups", "comments": "15 pages, 6 figures, to appear in Statistical Analysis and Data\n  Mining - The ASA Data Science Journal", "journal-ref": "Statistical Analysis and Data Mining -- The ASA Data Science\n  Journal, 2019, volume 12, number 3, pages 223-233", "doi": "10.1002/sam11416", "report-no": null, "categories": "stat.ML astro-ph.HE cs.CV cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $K$-means algorithm is extended to allow for partitioning of skewed\ngroups. Our algorithm is called TiK-Means and contributes a $K$-means type\nalgorithm that assigns observations to groups while estimating their\nskewness-transformation parameters. The resulting groups and transformation\nreveal general-structured clusters that can be explained by inverting the\nestimated transformation. Further, a modification of the jump statistic chooses\nthe number of groups. Our algorithm is evaluated on simulated and real-life\ndatasets and then applied to a long-standing astronomical dispute regarding the\ndistinct kinds of gamma ray bursts.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 14:32:42 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Berry", "Nicholas S.", ""], ["Maitra", "Ranjan", ""]]}, {"id": "1904.09615", "submitter": "Cosimo Izzo", "authors": "Cosimo Izzo", "title": "Explaining a prediction in some nonlinear models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we will analyse how to compute the contribution of each input\nvalue to its aggregate output in some nonlinear models. Regression and\nclassification applications, together with related algorithms for deep neural\nnetworks are presented. The proposed approach merges two methods currently\npresent in the literature: integrated gradient and deep Taylor decomposition.\nCompared to DeepLIFT and Deep SHAP, it provides a natural choice of the\nreference point peculiar to the model at use.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 15:23:52 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 14:00:55 GMT"}, {"version": "v3", "created": "Sat, 22 Jun 2019 19:09:19 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Izzo", "Cosimo", ""]]}, {"id": "1904.09631", "submitter": "Vidyasagar Sadhu", "authors": "Vidyasagar Sadhu, Saman Zonouz, Vincent Sritapan, Dario Pompili", "title": "HCFContext: Smartphone Context Inference via Sequential History-based\n  Collaborative Filtering", "comments": "Mobile context, collaborative filtering, privacy-preserving,\n  personalized model, sensors, location, prediction, hidden markov models,\n  google now, apple siri, cortana, alexa", "journal-ref": "IEEE International Conference on Pervasive Computing and\n  Communications (PerCom), Kyoto, Japan, 2019, pp. 1-9", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile context determination is an important step for many context aware\nservices such as location-based services, enterprise policy enforcement,\nbuilding or room occupancy detection for power or HVAC operation, etc.\nEspecially in enterprise scenarios where policies (e.g., attending a\nconfidential meeting only when the user is in \"Location X\") are defined based\non mobile context, it is paramount to verify the accuracy of the mobile\ncontext. To this end, two stochastic models based on the theory of Hidden\nMarkov Models (HMMs) to obtain mobile context are proposed-personalized model\n(HPContext) and collaborative filtering model (HCFContext). The former predicts\nthe current context using sequential history of the user's past context\nobservations, the latter enhances HPContext with collaborative filtering\nfeatures, which enables it to predict the current context of the primary user\nbased on the context observations of users related to the primary user, e.g.,\nsame team colleagues in company, gym friends, family members, etc. Each of the\nproposed models can also be used to enhance or complement the context obtained\nfrom sensors. Furthermore, since privacy is a concern in collaborative\nfiltering, a privacy-preserving method is proposed to derive HCFContext model\nparameters based on the concepts of homomorphic encryption. Finally, these\nmodels are thoroughly validated on a real-life dataset.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 17:09:35 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 02:59:14 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Sadhu", "Vidyasagar", ""], ["Zonouz", "Saman", ""], ["Sritapan", "Vincent", ""], ["Pompili", "Dario", ""]]}, {"id": "1904.09633", "submitter": "Devinder Kumar", "authors": "Devinder Kumar, Ibrahim Ben-Daya, Kanav Vats, Jeffery Feng, Graham\n  Taylor and, Alexander Wong", "title": "Beyond Explainability: Leveraging Interpretability for Improved\n  Adversarial Learning", "comments": "CVPR 2019 XAI Workshop accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we propose the leveraging of interpretability for tasks beyond\npurely the purpose of explainability. In particular, this study puts forward a\nnovel strategy for leveraging gradient-based interpretability in the realm of\nadversarial examples, where we use insights gained to aid adversarial learning.\nMore specifically, we introduce the concept of spatially constrained one-pixel\nadversarial perturbations, where we guide the learning of such adversarial\nperturbations towards more susceptible areas identified via gradient-based\ninterpretability. Experimental results using different benchmark datasets show\nthat such a spatially constrained one-pixel adversarial perturbation strategy\ncan noticeably improve the speed of convergence as well as produce successful\nattacks that were also visually difficult to perceive, thus illustrating an\neffective use of interpretability methods for tasks outside of the purpose of\npurely explainability.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 17:32:03 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Kumar", "Devinder", ""], ["Ben-Daya", "Ibrahim", ""], ["Vats", "Kanav", ""], ["Feng", "Jeffery", ""], ["and", "Graham Taylor", ""], ["Wong", "Alexander", ""]]}, {"id": "1904.09635", "submitter": "Yingjie Fei", "authors": "Yingjie Fei and Yudong Chen", "title": "Achieving the Bayes Error Rate in Synchronization and Block Models by\n  SDP, Robustly", "comments": "Partial preliminary results to appear in the Conference on Learning\n  Theory (COLT) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the statistical performance of semidefinite programming (SDP)\nrelaxations for clustering under random graph models. Under the\n$\\mathbb{Z}_{2}$ Synchronization model, Censored Block Model and Stochastic\nBlock Model, we show that SDP achieves an error rate of the form \\[\n\\exp\\Big[-\\big(1-o(1)\\big)\\bar{n} I^* \\Big]. \\] Here $\\bar{n}$ is an\nappropriate multiple of the number of nodes and $I^*$ is an\ninformation-theoretic measure of the signal-to-noise ratio. We provide matching\nlower bounds on the Bayes error for each model and therefore demonstrate that\nthe SDP approach is Bayes optimal. As a corollary, our results imply that SDP\nachieves the optimal exact recovery threshold under each model. Furthermore, we\nshow that SDP is robust: the above bound remains valid under semirandom\nversions of the models in which the observed graph is modified by a monotone\nadversary. Our proof is based on a novel primal-dual analysis of SDP under a\nunified framework for all three models, and the analysis shows that SDP tightly\napproximates a joint majority voting procedure.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 17:44:48 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Fei", "Yingjie", ""], ["Chen", "Yudong", ""]]}, {"id": "1904.09639", "submitter": "Mustafa Hajij", "authors": "Yunhao Zhang, Haowen Liu, Paul Rosen, Mustafa Hajij", "title": "Mesh Learning Using Persistent Homology on the Laplacian Eigenfunctions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use persistent homology along with the eigenfunctions of the Laplacian to\nstudy similarity amongst triangulated 2-manifolds. Our method relies on\nstudying the lower-star filtration induced by the eigenfunctions of the\nLaplacian. This gives us a shape descriptor that inherits the rich information\nencoded in the eigenfunctions of the Laplacian. Moreover, the similarity\nbetween these descriptors can be easily computed using tools that are readily\navailable in Topological Data Analysis. We provide experiments to illustrate\nthe effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 18:18:40 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 22:12:39 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Zhang", "Yunhao", ""], ["Liu", "Haowen", ""], ["Rosen", "Paul", ""], ["Hajij", "Mustafa", ""]]}, {"id": "1904.09644", "submitter": "Seulki Lee", "authors": "Seulki Lee, Bashima Islam, Yubo Luo, Shahriar Nirjon", "title": "Intermittent Learning: On-Device Machine Learning on Intermittently\n  Powered System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces intermittent learning - the goal of which is to enable\nenergy harvested computing platforms capable of executing certain classes of\nmachine learning tasks effectively and efficiently. We identify unique\nchallenges to intermittent learning relating to the data and application\nsemantics of machine learning tasks, and to address these challenges, we devise\n1) an algorithm that determines a sequence of actions to achieve the desired\nlearning objective under tight energy constraints, and 2) propose three\nheuristics that help an intermittent learner decide whether to learn or discard\ntraining examples at run-time which increases the energy efficiency of the\nsystem. We implement and evaluate three intermittent learning applications that\nlearn the 1) air quality, 2) human presence, and 3) vibration using solar, RF,\nand kinetic energy harvesters, respectively. We demonstrate that the proposed\nframework improves the energy efficiency of a learner by up to 100% and cuts\ndown the number of learning examples by up to 50% when compared to\nstate-of-the-art intermittent computing systems that do not implement the\nproposed intermittent learning framework.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 19:00:43 GMT"}, {"version": "v2", "created": "Sun, 15 Dec 2019 19:55:45 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Lee", "Seulki", ""], ["Islam", "Bashima", ""], ["Luo", "Yubo", ""], ["Nirjon", "Shahriar", ""]]}, {"id": "1904.09651", "submitter": "Hritik Bansal", "authors": "Ujjwal Gupta, Hritik Bansal, Deepak Joshi", "title": "An improved sex specific and age dependent classification model for\n  Parkinson's diagnosis using handwriting measurement", "comments": "Journal of Computer Methods and Programs in Biomedicine(Accepted on\n  27 December 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate diagnosis is crucial for preventing the progression of Parkinson's,\nas well as improving the quality of life with individuals with Parkinson's\ndisease. In this paper, we develop a sex-specific and age-dependent\nclassification method to diagnose the Parkinson's disease using the online\nhandwriting recorded from individuals with\nParkinson's(n=37;m/f-19/18;age-69.3+-10.9years) and healthy\ncontrols(n=38;m/f-20/18;age-62.4+-11.3 years).The sex specific and age\ndependent classifier was observed significantly outperforming the generalized\nclassifier. An improved accuracy of 83.75%(SD+1.63) with female specific\nclassifier, and 79.55%(SD=1.58) with old age dependent classifier was observed\nin comparison to 75.76%(SD=1.17) accuracy with the generalized classifier.\nFinally, combining the age and sex information proved to be encouraging in\nclassification. We performed a rigorous analysis to observe the dominance of\nsex specific and age dependent features for Parkinson's detection and ranked\nthem using the support vector machine(SVM) ranking method. Distinct set of\nfeatures were observed to be dominating for higher classification accuracy in\ndifferent category of classification.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 19:36:26 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 11:48:47 GMT"}, {"version": "v3", "created": "Fri, 26 Apr 2019 07:49:10 GMT"}, {"version": "v4", "created": "Tue, 13 Aug 2019 11:32:50 GMT"}, {"version": "v5", "created": "Sun, 15 Dec 2019 13:02:21 GMT"}, {"version": "v6", "created": "Mon, 30 Dec 2019 14:54:26 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Gupta", "Ujjwal", ""], ["Bansal", "Hritik", ""], ["Joshi", "Deepak", ""]]}, {"id": "1904.09671", "submitter": "Rami Al-Rfou", "authors": "Rami Al-Rfou and Dustin Zelle and Bryan Perozzi", "title": "DDGK: Learning Graph Representations for Deep Divergence Graph Kernels", "comments": "www '19", "journal-ref": "Proceedings of the 2019 World Wide Web Conference (WWW '19), May\n  13--17, 2019, San Francisco, CA, USA", "doi": "10.1145/3308558.3313668", "report-no": null, "categories": "cs.LG cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can neural networks learn to compare graphs without feature engineering? In\nthis paper, we show that it is possible to learn representations for graph\nsimilarity with neither domain knowledge nor supervision (i.e.\\ feature\nengineering or labeled graphs). We propose Deep Divergence Graph Kernels, an\nunsupervised method for learning representations over graphs that encodes a\nrelaxed notion of graph isomorphism. Our method consists of three parts. First,\nwe learn an encoder for each anchor graph to capture its structure. Second, for\neach pair of graphs, we train a cross-graph attention network which uses the\nnode representations of an anchor graph to reconstruct another graph. This\napproach, which we call isomorphism attention, captures how well the\nrepresentations of one graph can encode another. We use the attention-augmented\nencoder's predictions to define a divergence score for each pair of graphs.\nFinally, we construct an embedding space for all graphs using these pair-wise\ndivergence scores.\n  Unlike previous work, much of which relies on 1) supervision, 2) domain\nspecific knowledge (e.g. a reliance on Weisfeiler-Lehman kernels), and 3) known\nnode alignment, our unsupervised method jointly learns node representations,\ngraph representations, and an attention-based alignment between graphs.\n  Our experimental results show that Deep Divergence Graph Kernels can learn an\nunsupervised alignment between graphs, and that the learned representations\nachieve competitive results when used as features on a number of challenging\ngraph classification tasks. Furthermore, we illustrate how the learned\nattention allows insight into the the alignment of sub-structures across\ngraphs.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 22:45:04 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Al-Rfou", "Rami", ""], ["Zelle", "Dustin", ""], ["Perozzi", "Bryan", ""]]}, {"id": "1904.09708", "submitter": "Jacob Russin", "authors": "Jake Russin, Jason Jo, Randall C. O'Reilly, Yoshua Bengio", "title": "Compositional generalization in a deep seq2seq model by separating\n  syntax and semantics", "comments": "18 pages, 15 figures, preprint version of submission to NeurIPS 2019,\n  under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard methods in deep learning for natural language processing fail to\ncapture the compositional structure of human language that allows for\nsystematic generalization outside of the training distribution. However, human\nlearners readily generalize in this way, e.g. by applying known grammatical\nrules to novel words. Inspired by work in neuroscience suggesting separate\nbrain systems for syntactic and semantic processing, we implement a\nmodification to standard approaches in neural machine translation, imposing an\nanalogous separation. The novel model, which we call Syntactic Attention,\nsubstantially outperforms standard methods in deep learning on the SCAN\ndataset, a compositional generalization task, without any hand-engineered\nfeatures or additional supervision. Our work suggests that separating syntactic\nfrom semantic learning may be a useful heuristic for capturing compositional\nstructure.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 03:12:09 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2019 16:05:35 GMT"}, {"version": "v3", "created": "Thu, 23 May 2019 20:59:12 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Russin", "Jake", ""], ["Jo", "Jason", ""], ["O'Reilly", "Randall C.", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1904.09712", "submitter": "Qiuwei Li", "authors": "Qiuwei Li, Zhihui Zhu, Gongguo Tang, Michael B. Wakin", "title": "Provable Bregman-divergence based Methods for Nonconvex and\n  Non-Lipschitz Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The (global) Lipschitz smoothness condition is crucial in establishing the\nconvergence theory for most optimization methods. Unfortunately, most machine\nlearning and signal processing problems are not Lipschitz smooth. This\nmotivates us to generalize the concept of Lipschitz smoothness condition to the\nrelative smoothness condition, which is satisfied by any finite-order\npolynomial objective function. Further, this work develops new\nBregman-divergence based algorithms that are guaranteed to converge to a\nsecond-order stationary point for any relatively smooth problem. In addition,\nthe proposed optimization methods cover both the proximal alternating\nminimization and the proximal alternating linearized minimization when we\nspecialize the Bregman divergence to the Euclidian distance. Therefore, this\nwork not only develops guaranteed optimization methods for non-Lipschitz smooth\nproblems but also solves an open problem of showing the second-order\nconvergence guarantees for these alternating minimization methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 03:53:56 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Li", "Qiuwei", ""], ["Zhu", "Zhihui", ""], ["Tang", "Gongguo", ""], ["Wakin", "Michael B.", ""]]}, {"id": "1904.09743", "submitter": "Yu-Feng Li", "authors": "Lan-Zhe Guo, Yu-Feng Li, Ming Li, Jin-Feng Yi, Bo-Wen Zhou, Zhi-Hua\n  Zhou", "title": "Reliable Weakly Supervised Learning: Maximize Gain and Maintain Safeness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised data are widespread and have attracted much attention.\nHowever, since label quality is often difficult to guarantee, sometimes the use\nof weakly supervised data will lead to unsatisfactory performance, i.e.,\nperformance degradation or poor performance gains. Moreover, it is usually not\nfeasible to manually increase the label quality, which results in weakly\nsupervised learning being somewhat difficult to rely on. In view of this\ncrucial issue, this paper proposes a simple and novel weakly supervised\nlearning framework. We guide the optimization of label quality through a small\namount of validation data, and to ensure the safeness of performance while\nmaximizing performance gain. As validation set is a good approximation for\ndescribing generalization risk, it can effectively avoid the unsatisfactory\nperformance caused by incorrect data distribution assumptions. We formalize\nthis underlying consideration into a novel Bi-Level optimization and give an\neffective solution. Extensive experimental results verify that the new\nframework achieves impressive performance on weakly supervised learning with a\nsmall amount of validation data.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 06:50:39 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Guo", "Lan-Zhe", ""], ["Li", "Yu-Feng", ""], ["Li", "Ming", ""], ["Yi", "Jin-Feng", ""], ["Zhou", "Bo-Wen", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1904.09765", "submitter": "Pradeep Rengaswamy", "authors": "Pradeep Rengaswamy, Gurunath Reddy M and Krothapalli Sreenivasa Rao", "title": "hf0: A hybrid pitch extraction method for multimodal voice", "comments": "Pitch Extraction, F0 extraction, harmonic signals, speech, monophonic\n  songs, Convolutional Neural Network, 5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Pitch or fundamental frequency (f0) extraction is a fundamental problem\nstudied extensively for its potential applications in speech and clinical\napplications. In literature, explicit mode specific (modal speech or singing\nvoice or emotional/ expressive speech or noisy speech) signal processing and\ndeep learning f0 extraction methods that exploit the quasi periodic nature of\nthe signal in time, harmonic property in spectral or combined form to extract\nthe pitch is developed. Hence, there is no single unified method which can\nreliably extract the pitch from various modes of the acoustic signal. In this\nwork, we propose a hybrid f0 extraction method which seamlessly extracts the\npitch across modes of speech production with very high accuracy required for\nmany applications. The proposed hybrid model exploits the advantages of deep\nlearning and signal processing methods to minimize the pitch detection error\nand adopts to various modes of acoustic signal. Specifically, we propose an\nordinal regression convolutional neural networks to map the periodicity rich\ninput representation to obtain the nominal pitch classes which drastically\nreduces the number of classes required for pitch detection unlike other deep\nlearning approaches. Further, the accurate f0 is estimated from the nominal\npitch class labels by filtering and autocorrelation. We show that the proposed\nmethod generalizes to the unseen modes of voice production and various noises\nfor large scale datasets. Also, the proposed hybrid model significantly reduces\nthe learning parameters required to train the deep model compared to other\nmethods. Furthermore,the evaluation measures showed that the proposed method is\nsignificantly better than the state-of-the-art signal processing and deep\nlearning approaches.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 08:08:12 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Rengaswamy", "Pradeep", ""], ["M", "Gurunath Reddy", ""], ["Rao", "Krothapalli Sreenivasa", ""]]}, {"id": "1904.09770", "submitter": "Erik Nijkamp", "authors": "Erik Nijkamp, Mitch Hill, Song-Chun Zhu, Ying Nian Wu", "title": "Learning Non-Convergent Non-Persistent Short-Run MCMC Toward\n  Energy-Based Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a curious phenomenon in learning energy-based model (EBM)\nusing MCMC. In each learning iteration, we generate synthesized examples by\nrunning a non-convergent, non-mixing, and non-persistent short-run MCMC toward\nthe current model, always starting from the same initial distribution such as\nuniform noise distribution, and always running a fixed number of MCMC steps.\nAfter generating synthesized examples, we then update the model parameters\naccording to the maximum likelihood learning gradient, as if the synthesized\nexamples are fair samples from the current model. We treat this non-convergent\nshort-run MCMC as a learned generator model or a flow model. We provide\narguments for treating the learned non-convergent short-run MCMC as a valid\nmodel. We show that the learned short-run MCMC is capable of generating\nrealistic images. More interestingly, unlike traditional EBM or MCMC, the\nlearned short-run MCMC is capable of reconstructing observed images and\ninterpolating between images, like generator or flow models. The code can be\nfound in the Appendix.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 08:48:52 GMT"}, {"version": "v2", "created": "Sat, 18 May 2019 02:47:13 GMT"}, {"version": "v3", "created": "Mon, 27 May 2019 00:16:34 GMT"}, {"version": "v4", "created": "Mon, 25 Nov 2019 22:54:43 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Nijkamp", "Erik", ""], ["Hill", "Mitch", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1904.09775", "submitter": "Babak Barazandeh", "authors": "Babak Barazandeh, Meisam Razaviyayn and Maziar Sanjabi", "title": "Training generative networks using random discriminators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Generative Adversarial Networks (GANs) have drawn a lot of\nattentions for learning the underlying distribution of data in various\napplications. Despite their wide applicability, training GANs is notoriously\ndifficult. This difficulty is due to the min-max nature of the resulting\noptimization problem and the lack of proper tools of solving general\n(non-convex, non-concave) min-max optimization problems. In this paper, we try\nto alleviate this problem by proposing a new generative network that relies on\nthe use of random discriminators instead of adversarial design. This design\nhelps us to avoid the min-max formulation and leads to an optimization problem\nthat is stable and could be solved efficiently. The performance of the proposed\nmethod is evaluated using handwritten digits (MNIST) and Fashion products\n(Fashion-MNIST) data sets. While the resulting images are not as sharp as\nadversarial training, the use of random discriminator leads to a much faster\nalgorithm as compared to the adversarial counterpart. This observation, at the\nminimum, illustrates the potential of the random discriminator approach for\nwarm-start in training GANs.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 08:53:18 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Barazandeh", "Babak", ""], ["Razaviyayn", "Meisam", ""], ["Sanjabi", "Maziar", ""]]}, {"id": "1904.09792", "submitter": "Sandeep Kumar", "authors": "Sandeep Kumar, Jiaxi Ying, Jos\\'e Vin\\'icius de M. Cardoso, and Daniel\n  Palomar", "title": "A Unified Framework for Structured Graph Learning via Spectral\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI math.OC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Graph learning from data represents a canonical problem that has received\nsubstantial attention in the literature. However, insufficient work has been\ndone in incorporating prior structural knowledge onto the learning of\nunderlying graphical models from data. Learning a graph with a specific\nstructure is essential for interpretability and identification of the\nrelationships among data. Useful structured graphs include the multi-component\ngraph, bipartite graph, connected graph, sparse graph, and regular graph. In\ngeneral, structured graph learning is an NP-hard combinatorial problem,\ntherefore, designing a general tractable optimization method is extremely\nchallenging. In this paper, we introduce a unified graph learning framework\nlying at the integration of Gaussian graphical models and spectral graph\ntheory. To impose a particular structure on a graph, we first show how to\nformulate the combinatorial constraints as an analytical property of the graph\nmatrix. Then we develop an optimization framework that leverages graph learning\nwith specific structures via spectral constraints on graph matrices. The\nproposed algorithms are provably convergent, computationally efficient, and\npractically amenable for numerous graph-based tasks. Extensive numerical\nexperiments with both synthetic and real data sets illustrate the effectiveness\nof the proposed algorithms. The code for all the simulations is made available\nas an open source repository.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 10:19:58 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Kumar", "Sandeep", ""], ["Ying", "Jiaxi", ""], ["Cardoso", "Jos\u00e9 Vin\u00edcius de M.", ""], ["Palomar", "Daniel", ""]]}, {"id": "1904.09807", "submitter": "Christian H\\\"ager", "authors": "Christian H\\\"ager, Henry D. Pfister, Rick M. B\\\"utler, Gabriele Liga,\n  Alex Alvarado", "title": "Revisiting Multi-Step Nonlinearity Compensation with Machine Learning", "comments": "4 pages, 3 figures, This is a preprint of a paper submitted to the\n  2019 European Conference on Optical Communication", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the efficient compensation of fiber nonlinearity, one of the guiding\nprinciples appears to be: fewer steps are better and more efficient. We\nchallenge this assumption and show that carefully designed multi-step\napproaches can lead to better performance-complexity trade-offs than their\nfew-step counterparts.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 12:01:39 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["H\u00e4ger", "Christian", ""], ["Pfister", "Henry D.", ""], ["B\u00fctler", "Rick M.", ""], ["Liga", "Gabriele", ""], ["Alvarado", "Alex", ""]]}, {"id": "1904.09816", "submitter": "Sungrae Park", "authors": "Sungrae Park, Kyungwoo Song, Mingi Ji, Wonsung Lee, Il-Chul Moon", "title": "Adversarial Dropout for Recurrent Neural Networks", "comments": "published in AAAI19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successful application processing sequential data, such as text and speech,\nrequires an improved generalization performance of recurrent neural networks\n(RNNs). Dropout techniques for RNNs were introduced to respond to these\ndemands, but we conjecture that the dropout on RNNs could have been improved by\nadopting the adversarial concept. This paper investigates ways to improve the\ndropout for RNNs by utilizing intentionally generated dropout masks.\nSpecifically, the guided dropout used in this research is called as adversarial\ndropout, which adversarially disconnects neurons that are dominantly used to\npredict correct targets over time. Our analysis showed that our regularizer,\nwhich consists of a gap between the original and the reconfigured RNNs, was the\nupper bound of the gap between the training and the inference phases of the\nrandom dropout. We demonstrated that minimizing our regularizer improved the\neffectiveness of the dropout for RNNs on sequential MNIST tasks,\nsemi-supervised text classification tasks, and language modeling tasks.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 12:16:08 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Park", "Sungrae", ""], ["Song", "Kyungwoo", ""], ["Ji", "Mingi", ""], ["Lee", "Wonsung", ""], ["Moon", "Il-Chul", ""]]}, {"id": "1904.09848", "submitter": "Amirreza Khodadadian", "authors": "Amirreza Khodadadian, Benjamin Stadlbauer and Clemens Heitzinger", "title": "Bayesian inversion for nanowire field-effect sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nanowire field-effect sensors have recently been developed for label-free\ndetection of biomolecules. In this work, we introduce a computational technique\nbased on Bayesian estimation to determine the physical parameters of the sensor\nand, more importantly, the properties of the analyte molecules. To that end, we\nfirst propose a PDE based model to simulate the device charge transport and\nelectrochemical behavior. Then, the adaptive Metropolis algorithm with delayed\nrejection (DRAM) is applied to estimate the posterior distribution of unknown\nparameters, namely molecule charge density, molecule density, doping\nconcentration, and electron and hole mobilities. We determine the device and\nmolecules properties simultaneously, and we also calculate the molecule density\nas the only parameter after having determined the device parameters. This\napproach makes it possible not only to determine unknown parameters, but it\nalso shows how well each parameter can be determined by yielding the\nprobability density function (pdf).\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 12:06:06 GMT"}, {"version": "v2", "created": "Sat, 26 Oct 2019 09:59:17 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Khodadadian", "Amirreza", ""], ["Stadlbauer", "Benjamin", ""], ["Heitzinger", "Clemens", ""]]}, {"id": "1904.09858", "submitter": "Hlynur Dav\\'i{\\dh} Hlynsson", "authors": "Hlynur Dav\\'i{\\dh} Hlynsson, Laurenz Wiskott", "title": "Learning gradient-based ICA by neurally estimating mutual information", "comments": "6 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several methods of estimating the mutual information of random variables have\nbeen developed in recent years. They can prove valuable for novel approaches to\nlearning statistically independent features. In this paper, we use one of these\nmethods, a mutual information neural estimation (MINE) network, to present a\nproof-of-concept of how a neural network can perform linear ICA. We minimize\nthe mutual information, as estimated by a MINE network, between the output\nunits of a differentiable encoder network. This is done by simple alternate\noptimization of the two networks. The method is shown to get a qualitatively\nequal solution to FastICA on blind-source-separation of noisy sources.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 13:26:13 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Hlynsson", "Hlynur Dav\u00ed\u00f0", ""], ["Wiskott", "Laurenz", ""]]}, {"id": "1904.09942", "submitter": "Michael P. Kim", "authors": "Sumegha Garg and Michael P. Kim and Omer Reingold", "title": "Tracking and Improving Information in the Service of Fairness", "comments": "Appeared at EC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As algorithmic prediction systems have become widespread, fears that these\nsystems may inadvertently discriminate against members of underrepresented\npopulations have grown. With the goal of understanding fundamental principles\nthat underpin the growing number of approaches to mitigating algorithmic\ndiscrimination, we investigate the role of information in fair prediction. A\ncommon strategy for decision-making uses a predictor to assign individuals a\nrisk score; then, individuals are selected or rejected on the basis of this\nscore. In this work, we study a formal framework for measuring the information\ncontent of predictors. Central to this framework is the notion of a refinement,\nfirst studied by Degroot and Fienberg. Intuitively, a refinement of a predictor\n$z$ increases the overall informativeness of the predictions without losing the\ninformation already contained in $z$. We show that increasing information\ncontent through refinements improves the downstream selection rules across a\nwide range of fairness measures (e.g. true positive rates, false positive\nrates, selection rates). In turn, refinements provide a simple but effective\ntool for reducing disparity in treatment and impact without sacrificing the\nutility of the predictions. Our results suggest that in many applications, the\nperceived \"cost of fairness\" results from an information disparity across\npopulations, and thus, may be avoided with improved information.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 16:35:24 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 16:39:16 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Garg", "Sumegha", ""], ["Kim", "Michael P.", ""], ["Reingold", "Omer", ""]]}, {"id": "1904.09948", "submitter": "Naresh Manwani", "authors": "Kulin Shah, P. S. Sastry, Naresh Manwani", "title": "PLUME: Polyhedral Learning Using Mixture of Experts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel mixture of expert architecture for learning\npolyhedral classifiers. We learn the parameters of the classifierusing an\nexpectation maximization algorithm. Wederive the generalization bounds of the\nproposedapproach. Through an extensive simulation study, we show that the\nproposed method performs comparably to other state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 16:50:04 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Shah", "Kulin", ""], ["Sastry", "P. S.", ""], ["Manwani", "Naresh", ""]]}, {"id": "1904.09981", "submitter": "Yang Gao", "authors": "Yang Gao, Hong Yang, Peng Zhang, Chuan Zhou, Yue Hu", "title": "GraphNAS: Graph Neural Architecture Search with Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs) have been popularly used for analyzing\nnon-Euclidean data such as social network data and biological data. Despite\ntheir success, the design of graph neural networks requires a lot of manual\nwork and domain knowledge. In this paper, we propose a Graph Neural\nArchitecture Search method (GraphNAS for short) that enables automatic search\nof the best graph neural architecture based on reinforcement learning.\nSpecifically, GraphNAS first uses a recurrent network to generate\nvariable-length strings that describe the architectures of graph neural\nnetworks, and then trains the recurrent network with reinforcement learning to\nmaximize the expected accuracy of the generated architectures on a validation\ndata set. Extensive experimental results on node classification tasks in both\ntransductive and inductive learning settings demonstrate that GraphNAS can\nachieve consistently better performance on the Cora, Citeseer, Pubmed citation\nnetwork, and protein-protein interaction network. On node classification tasks,\nGraphNAS can design a novel network architecture that rivals the best\nhuman-invented architecture in terms of test set accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 07:13:10 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 03:00:40 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Gao", "Yang", ""], ["Yang", "Hong", ""], ["Zhang", "Peng", ""], ["Zhou", "Chuan", ""], ["Hu", "Yue", ""]]}, {"id": "1904.10030", "submitter": "Davood Karimi", "authors": "Davood Karimi and Septimiu E. Salcudean", "title": "Reducing the Hausdorff Distance in Medical Image Segmentation with\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hausdorff Distance (HD) is widely used in evaluating medical image\nsegmentation methods. However, existing segmentation methods do not attempt to\nreduce HD directly. In this paper, we present novel loss functions for training\nconvolutional neural network (CNN)-based segmentation methods with the goal of\nreducing HD directly. We propose three methods to estimate HD from the\nsegmentation probability map produced by a CNN. One method makes use of the\ndistance transform of the segmentation boundary. Another method is based on\napplying morphological erosion on the difference between the true and estimated\nsegmentation maps. The third method works by applying circular/spherical\nconvolution kernels of different radii on the segmentation probability maps.\nBased on these three methods for estimating HD, we suggest three loss functions\nthat can be used for training to reduce HD. We use these loss functions to\ntrain CNNs for segmentation of the prostate, liver, and pancreas in ultrasound,\nmagnetic resonance, and computed tomography images and compare the results with\ncommonly-used loss functions. Our results show that the proposed loss functions\ncan lead to approximately 18-45 % reduction in HD without degrading other\nsegmentation performance criteria such as the Dice similarity coefficient. The\nproposed loss functions can be used for training medical image segmentation\nmethods in order to reduce the large segmentation errors.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 18:55:05 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Karimi", "Davood", ""], ["Salcudean", "Septimiu E.", ""]]}, {"id": "1904.10040", "submitter": "Djallel Bouneffouf", "authors": "Djallel Bouneffouf and Irina Rish", "title": "A Survey on Practical Applications of Multi-Armed and Contextual Bandits", "comments": "under review by IJCAI 2019 Survey", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, multi-armed bandit (MAB) framework has attracted a lot of\nattention in various applications, from recommender systems and information\nretrieval to healthcare and finance, due to its stellar performance combined\nwith certain attractive properties, such as learning from less feedback. The\nmulti-armed bandit field is currently flourishing, as novel problem settings\nand algorithms motivated by various practical applications are being\nintroduced, building on top of the classical bandit problem. This article aims\nto provide a comprehensive review of top recent developments in multiple\nreal-life applications of the multi-armed bandit. Specifically, we introduce a\ntaxonomy of common MAB-based applications and summarize state-of-art for each\nof those domains. Furthermore, we identify important current trends and provide\nnew perspectives pertaining to the future of this exciting and fast-growing\nfield.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 14:17:40 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Bouneffouf", "Djallel", ""], ["Rish", "Irina", ""]]}, {"id": "1904.10059", "submitter": "Hafiz Imtiaz", "authors": "Hafiz Imtiaz, Jafar Mohammadi, Anand D. Sarwate", "title": "Distributed Differentially Private Computation of Functions with\n  Correlated Noise", "comments": "The manuscript is partially subsumed by arXiv:1910.12913", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications of machine learning, such as human health research, involve\nprocessing private or sensitive information. Privacy concerns may impose\nsignificant hurdles to collaboration in scenarios where there are multiple\nsites holding data and the goal is to estimate properties jointly across all\ndatasets. Differentially private decentralized algorithms can provide strong\nprivacy guarantees. However, the accuracy of the joint estimates may be poor\nwhen the datasets at each site are small. This paper proposes a new framework,\nCorrelation Assisted Private Estimation (CAPE), for designing\nprivacy-preserving decentralized algorithms with better accuracy guarantees in\nan honest-but-curious model. CAPE can be used in conjunction with the\nfunctional mechanism for statistical and machine learning optimization\nproblems. A tighter characterization of the functional mechanism is provided\nthat allows CAPE to achieve the same performance as a centralized algorithm in\nthe decentralized setting using all datasets. Empirical results on regression\nand neural network problems for both synthetic and real datasets show that\ndifferentially private methods can be competitive with non-private algorithms\nin many scenarios of interest.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 20:38:09 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 19:08:54 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2021 02:41:21 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Imtiaz", "Hafiz", ""], ["Mohammadi", "Jafar", ""], ["Sarwate", "Anand D.", ""]]}, {"id": "1904.10079", "submitter": "William Guss", "authors": "William H. Guss, Cayden Codel, Katja Hofmann, Brandon Houghton, Noboru\n  Kuno, Stephanie Milani, Sharada Mohanty, Diego Perez Liebana, Ruslan\n  Salakhutdinov, Nicholay Topin, Manuela Veloso, Phillip Wang", "title": "The MineRL 2019 Competition on Sample Efficient Reinforcement Learning\n  using Human Priors", "comments": "accepted at NeurIPS 2019, 28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Though deep reinforcement learning has led to breakthroughs in many difficult\ndomains, these successes have required an ever-increasing number of samples. As\nstate-of-the-art reinforcement learning (RL) systems require an exponentially\nincreasing number of samples, their development is restricted to a continually\nshrinking segment of the AI community. Likewise, many of these systems cannot\nbe applied to real-world problems, where environment samples are expensive.\nResolution of these limitations requires new, sample-efficient methods. To\nfacilitate research in this direction, we introduce the MineRL Competition on\nSample Efficient Reinforcement Learning using Human Priors.\n  The primary goal of the competition is to foster the development of\nalgorithms which can efficiently leverage human demonstrations to drastically\nreduce the number of samples needed to solve complex, hierarchical, and sparse\nenvironments. To that end, we introduce: (1) the Minecraft ObtainDiamond task,\na sequential decision making environment requiring long-term planning,\nhierarchical control, and efficient exploration methods; and (2) the MineRL-v0\ndataset, a large-scale collection of over 60 million state-action pairs of\nhuman demonstrations that can be resimulated into embodied trajectories with\narbitrary modifications to game state and visuals.\n  Participants will compete to develop systems which solve the ObtainDiamond\ntask with a limited number of samples from the environment simulator, Malmo.\nThe competition is structured into two rounds in which competitors are provided\nseveral paired versions of the dataset and environment with different game\ntextures. At the end of each round, competitors will submit containerized\nversions of their learning algorithms and they will then be trained/evaluated\nfrom scratch on a hold-out dataset-environment pair for a total of 4-days on a\nprespecified hardware platform.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 22:18:37 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 18:24:24 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2021 07:47:28 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Guss", "William H.", ""], ["Codel", "Cayden", ""], ["Hofmann", "Katja", ""], ["Houghton", "Brandon", ""], ["Kuno", "Noboru", ""], ["Milani", "Stephanie", ""], ["Mohanty", "Sharada", ""], ["Liebana", "Diego Perez", ""], ["Salakhutdinov", "Ruslan", ""], ["Topin", "Nicholay", ""], ["Veloso", "Manuela", ""], ["Wang", "Phillip", ""]]}, {"id": "1904.10090", "submitter": "Erwan Lecarpentier", "authors": "Erwan Lecarpentier and Emmanuel Rachelson", "title": "Non-Stationary Markov Decision Processes, a Worst-Case Approach using\n  Model-Based Reinforcement Learning, Extended version", "comments": "Published at NeurIPS 2019, 17 pages, 3 figures", "journal-ref": "year: 2019; page range: 7214--7223", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work tackles the problem of robust zero-shot planning in non-stationary\nstochastic environments. We study Markov Decision Processes (MDPs) evolving\nover time and consider Model-Based Reinforcement Learning algorithms in this\nsetting. We make two hypotheses: 1) the environment evolves continuously with a\nbounded evolution rate; 2) a current model is known at each decision epoch but\nnot its evolution. Our contribution can be presented in four points. 1) we\ndefine a specific class of MDPs that we call Non-Stationary MDPs (NSMDPs). We\nintroduce the notion of regular evolution by making an hypothesis of\nLipschitz-Continuity on the transition and reward functions w.r.t. time; 2) we\nconsider a planning agent using the current model of the environment but\nunaware of its future evolution. This leads us to consider a worst-case method\nwhere the environment is seen as an adversarial agent; 3) following this\napproach, we propose the Risk-Averse Tree-Search (RATS) algorithm, a zero-shot\nModel-Based method similar to Minimax search; 4) we illustrate the benefits\nbrought by RATS empirically and compare its performance with reference\nModel-Based algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 23:19:03 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 09:39:01 GMT"}, {"version": "v3", "created": "Fri, 10 Jan 2020 16:43:46 GMT"}, {"version": "v4", "created": "Wed, 15 Jan 2020 16:32:47 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Lecarpentier", "Erwan", ""], ["Rachelson", "Emmanuel", ""]]}, {"id": "1904.10098", "submitter": "Jie Chen", "authors": "Yue Yu, Jie Chen, Tian Gao, Mo Yu", "title": "DAG-GNN: DAG Structure Learning with Graph Neural Networks", "comments": "ICML2019. Code is available at\n  https://github.com/fishmoon1234/DAG-GNN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a faithful directed acyclic graph (DAG) from samples of a joint\ndistribution is a challenging combinatorial problem, owing to the intractable\nsearch space superexponential in the number of graph nodes. A recent\nbreakthrough formulates the problem as a continuous optimization with a\nstructural constraint that ensures acyclicity (Zheng et al., 2018). The authors\napply the approach to the linear structural equation model (SEM) and the\nleast-squares loss function that are statistically well justified but\nnevertheless limited. Motivated by the widespread success of deep learning that\nis capable of capturing complex nonlinear mappings, in this work we propose a\ndeep generative model and apply a variant of the structural constraint to learn\nthe DAG. At the heart of the generative model is a variational autoencoder\nparameterized by a novel graph neural network architecture, which we coin\nDAG-GNN. In addition to the richer capacity, an advantage of the proposed model\nis that it naturally handles discrete variables as well as vector-valued ones.\nWe demonstrate that on synthetic data sets, the proposed method learns more\naccurate graphs for nonlinearly generated samples; and on benchmark data sets\nwith discrete variables, the learned graphs are reasonably close to the global\noptima. The code is available at \\url{https://github.com/fishmoon1234/DAG-GNN}.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 23:58:49 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Yu", "Yue", ""], ["Chen", "Jie", ""], ["Gao", "Tian", ""], ["Yu", "Mo", ""]]}, {"id": "1904.10100", "submitter": "Weifeng Liu", "authors": "Weifeng Liu and Dacheng Tao", "title": "Multiview Hessian Regularization for Image Annotation", "comments": null, "journal-ref": "IEEE Transactions on Image Processing, vol. 22, no. 7, pp. 2676 -\n  2687, 2013", "doi": "10.1109/TIP.2013.2255302", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development of computer hardware and Internet technology makes\nlarge scale data dependent models computationally tractable, and opens a bright\navenue for annotating images through innovative machine learning algorithms.\nSemi-supervised learning (SSL) has consequently received intensive attention in\nrecent years and has been successfully deployed in image annotation. One\nrepresentative work in SSL is Laplacian regularization (LR), which smoothes the\nconditional distribution for classification along the manifold encoded in the\ngraph Laplacian, however, it has been observed that LR biases the\nclassification function towards a constant function which possibly results in\npoor generalization. In addition, LR is developed to handle uniformly\ndistributed data (or single view data), although instances or objects, such as\nimages and videos, are usually represented by multiview features, such as\ncolor, shape and texture. In this paper, we present multiview Hessian\nregularization (mHR) to address the above two problems in LR-based image\nannotation. In particular, mHR optimally combines multiple Hessian\nregularizations, each of which is obtained from a particular view of instances,\nand steers the classification function which varies linearly along the data\nmanifold. We apply mHR to kernel least squares and support vector machines as\ntwo examples for image annotation. Extensive experiments on the PASCAL VOC'07\ndataset validate the effectiveness of mHR by comparing it with baseline\nalgorithms, including LR and HR.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 00:08:43 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Liu", "Weifeng", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.10112", "submitter": "Yan Yan", "authors": "Yan Yan, Yi Xu, Qihang Lin, Lijun Zhang, Tianbao Yang", "title": "Stochastic Primal-Dual Algorithms with Faster Convergence than\n  $O(1/\\sqrt{T})$ for Problems without Bilinear Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous studies on stochastic primal-dual algorithms for solving min-max\nproblems with faster convergence heavily rely on the bilinear structure of the\nproblem, which restricts their applicability to a narrowed range of problems.\nThe main contribution of this paper is the design and analysis of new\nstochastic primal-dual algorithms that use a mixture of stochastic gradient\nupdates and a logarithmic number of deterministic dual updates for solving a\nfamily of convex-concave problems with no bilinear structure assumed. Faster\nconvergence rates than $O(1/\\sqrt{T})$ with $T$ being the number of stochastic\ngradient updates are established under some mild conditions of involved\nfunctions on the primal and the dual variable. For example, for a family of\nproblems that enjoy a weak strong convexity in terms of the primal variable and\nhas a strongly concave function of the dual variable, the convergence rate of\nthe proposed algorithm is $O(1/T)$. We also investigate the effectiveness of\nthe proposed algorithms for learning robust models and empirical AUC\nmaximization.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 01:04:12 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 21:40:26 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Yan", "Yan", ""], ["Xu", "Yi", ""], ["Lin", "Qihang", ""], ["Zhang", "Lijun", ""], ["Yang", "Tianbao", ""]]}, {"id": "1904.10120", "submitter": "Nathan Srebro", "authors": "Hubert Eichner and Tomer Koren and H. Brendan McMahan and Nathan\n  Srebro and Kunal Talwar", "title": "Semi-Cyclic Stochastic Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider convex SGD updates with a block-cyclic structure, i.e. where each\ncycle consists of a small number of blocks, each with many samples from a\npossibly different, block-specific, distribution. This situation arises, e.g.,\nin Federated Learning where the mobile devices available for updates at\ndifferent times during the day have different characteristics. We show that\nsuch block-cyclic structure can significantly deteriorate the performance of\nSGD, but propose a simple approach that allows prediction with the same\nperformance guarantees as for i.i.d., non-cyclic, sampling.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 02:17:15 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Eichner", "Hubert", ""], ["Koren", "Tomer", ""], ["McMahan", "H. Brendan", ""], ["Srebro", "Nathan", ""], ["Talwar", "Kunal", ""]]}, {"id": "1904.10122", "submitter": "James Freitag", "authors": "Hunter Chase and James Freitag", "title": "Bounds in Query Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.LO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce new combinatorial quantities for concept classes, and prove\nlower and upper bounds for learning complexity in several models of query\nlearning in terms of various combinatorial quantities. Our approach is flexible\nand powerful enough to enough to give new and very short proofs of the\nefficient learnability of several prominent examples (e.g. regular languages\nand regular $\\omega$-languages), in some cases also producing new bounds on the\nnumber of queries. In the setting of equivalence plus membership queries, we\ngive an algorithm which learns a class in polynomially many queries whenever\nany such algorithm exists.\n  We also study equivalence query learning in a randomized model, producing new\nbounds on the expected number of queries required to learn an arbitrary\nconcept. Many of the techniques and notions of dimension draw inspiration from\nor are related to notions from model theory, and these connections are\nexplained. We also use techniques from query learning to mildly improve a\nresult of Laskowski regarding compression schemes.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 02:39:53 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Chase", "Hunter", ""], ["Freitag", "James", ""]]}, {"id": "1904.10126", "submitter": "Mundher Al-Shabi", "authors": "Mundher Al-Shabi, Boon Leong Lan, Wai Yee Chan, Kwan-Hoong Ng, Maxine\n  Tan", "title": "Lung Nodule Classification using Deep Local-Global Networks", "comments": "Code and dataset available here\n  https://github.com/mundher/local-global", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Lung nodules have very diverse shapes and sizes, which makes\nclassifying them as benign/malignant a challenging problem. In this paper, we\npropose a novel method to predict the malignancy of nodules that have the\ncapability to analyze the shape and size of a nodule using a global feature\nextractor, as well as the density and structure of the nodule using a local\nfeature extractor. Methods: We propose to use Residual Blocks with a 3x3 kernel\nsize for local feature extraction, and Non-Local Blocks to extract the global\nfeatures. The Non-Local Block has the ability to extract global features\nwithout using a huge number of parameters. The key idea behind the Non-Local\nBlock is to apply matrix multiplications between features on the same feature\nmaps. Results: We trained and validated the proposed method on the LIDC-IDRI\ndataset which contains 1,018 computed tomography (CT) scans. We followed a\nrigorous procedure for experimental setup namely, 10-fold cross-validation and\nignored the nodules that had been annotated by less than 3 radiologists. The\nproposed method achieved state-of-the-art results with AUC=95.62%, while\nsignificantly outperforming other baseline methods. Conclusions: Our proposed\nDeep Local-Global network has the capability to accurately extract both local\nand global features. Our new method outperforms state-of-the-art architecture\nincluding Densenet and Resnet with transfer learning.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 02:49:37 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Al-Shabi", "Mundher", ""], ["Lan", "Boon Leong", ""], ["Chan", "Wai Yee", ""], ["Ng", "Kwan-Hoong", ""], ["Tan", "Maxine", ""]]}, {"id": "1904.10130", "submitter": "John Brandt", "authors": "John Brandt", "title": "Spatio-temporal crop classification of low-resolution satellite imagery\n  with capsule layers and distributed attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Land use classification of low resolution spatial imagery is one of the most\nextensively researched fields in remote sensing. Despite significant\nadvancements in satellite technology, high resolution imagery lacks global\ncoverage and can be prohibitively expensive to procure for extended time\nperiods. Accurately classifying land use change without high resolution imagery\noffers the potential to monitor vital aspects of global development agenda\nincluding climate smart agriculture, drought resistant crops, and sustainable\nland management. Utilizing a combination of capsule layers and long-short term\nmemory layers with distributed attention, the present paper achieves\nstate-of-the-art accuracy on temporal crop type classification at a 30x30m\nresolution with Sentinel 2 imagery.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 03:05:31 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Brandt", "John", ""]]}, {"id": "1904.10146", "submitter": "Xiang Gao", "authors": "Xiang Gao, Wei Hu, Zongming Guo", "title": "Exploring Structure-Adaptive Graph Learning for Robust Semi-Supervised\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Convolutional Neural Networks (GCNNs) are generalizations of CNNs to\ngraph-structured data, in which convolution is guided by the graph topology. In\nmany cases where graphs are unavailable, existing methods manually construct\ngraphs or learn task-driven adaptive graphs. In this paper, we propose Graph\nLearning Neural Networks (GLNNs), which exploit the optimization of graphs (the\nadjacency matrix in particular) from both data and tasks. Leveraging on\nspectral graph theory, we propose the objective of graph learning from a\nsparsity constraint, properties of a valid adjacency matrix as well as a graph\nLaplacian regularizer via maximum a posteriori estimation. The optimization\nobjective is then integrated into the loss function of the GCNN, which adapts\nthe graph topology to not only labels of a specific task but also the input\ndata. Experimental results show that our proposed GLNN outperforms\nstate-of-the-art approaches over widely adopted social network datasets and\ncitation network datasets for semi-supervised classification.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 04:17:41 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 05:49:39 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Gao", "Xiang", ""], ["Hu", "Wei", ""], ["Guo", "Zongming", ""]]}, {"id": "1904.10155", "submitter": "Lai Tian", "authors": "Lai Tian, Feiping Nie, Xuelong Li", "title": "Learning Feature Sparse Principal Components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents new algorithms to solve the feature-sparsity constrained\nPCA problem (FSPCA), which performs feature selection and PCA simultaneously.\nExisting optimization methods for FSPCA require data distribution assumptions\nand are lack of global convergence guarantee. Though the general FSPCA problem\nis NP-hard, we show that, for a low-rank covariance, FSPCA can be solved\nglobally (Algorithm 1). Then, we propose another strategy (Algorithm 2) to\nsolve FSPCA for the general covariance by iteratively building a carefully\ndesigned proxy. We prove theoretical guarantees on approximation and\nconvergence for the new algorithms. Experimental results show the promising\nperformance of the new algorithms compared with the state-of-the-arts on both\nsynthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 05:25:02 GMT"}, {"version": "v2", "created": "Sun, 26 May 2019 02:55:26 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Tian", "Lai", ""], ["Nie", "Feiping", ""], ["Li", "Xuelong", ""]]}, {"id": "1904.10165", "submitter": "Tao Li", "authors": "Tao Li, Jinwen Ma", "title": "T-SVD Based Non-convex Tensor Completion and Robust Principal Component\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor completion and robust principal component analysis have been widely\nused in machine learning while the key problem relies on the minimization of a\ntensor rank that is very challenging. A common way to tackle this difficulty is\nto approximate the tensor rank with the $\\ell_1-$norm of singular values based\non its Tensor Singular Value Decomposition (T-SVD). Besides, the sparsity of a\ntensor is also measured by its $\\ell_1-$norm. However, the $\\ell_1$ penalty is\nessentially biased and thus the result will deviate. In order to sidestep the\nbias, we propose a novel non-convex tensor rank surrogate function and a novel\nnon-convex sparsity measure. In this new setting by using the concavity instead\nof the convexity, a majorization minimization algorithm is further designed for\ntensor completion and robust principal component analysis. Furthermore, we\nanalyze its theoretical properties. Finally, the experiments on natural and\nhyperspectral images demonstrate the efficacy and efficiency of our proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 06:09:27 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 06:08:52 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Li", "Tao", ""], ["Ma", "Jinwen", ""]]}, {"id": "1904.10171", "submitter": "Tianyu Shi", "authors": "Tianyu Shi, Pin Wang, Xuxin Cheng, Ching-Yao Chan, Ding Huang", "title": "Driving Decision and Control for Autonomous Lane Change based on Deep\n  Reinforcement Learning", "comments": "This Paper has been submitted to ITSC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply Deep Q-network (DQN) with the consideration of safety during the\ntask for deciding whether to conduct the maneuver. Furthermore, we design two\nsimilar Deep Q learning frameworks with quadratic approximator for deciding how\nto select a comfortable gap and just follow the preceding vehicle. Finally, a\npolynomial lane change trajectory is generated and Pure Pursuit Control is\nimplemented for path tracking. We demonstrate the effectiveness of this\nframework in simulation, from both the decision-making and control layers. The\nproposed architecture also has the potential to be extended to other autonomous\ndriving scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 06:27:18 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 07:18:11 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Shi", "Tianyu", ""], ["Wang", "Pin", ""], ["Cheng", "Xuxin", ""], ["Chan", "Ching-Yao", ""], ["Huang", "Ding", ""]]}, {"id": "1904.10255", "submitter": "Ahmed Imtiaz Humayun", "authors": "Ahmed Imtiaz Humayun, Asif Shahriyar Sushmit, Taufiq Hasan and\n  Mohammed Imamul Hassan Bhuiyan", "title": "End-to-end Sleep Staging with Raw Single Channel EEG using Deep Residual\n  ConvNets", "comments": "5 pages, 3 Figures, Appendix, IEEE BHI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.SP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Humans approximately spend a third of their life sleeping, which makes\nmonitoring sleep an integral part of well-being. In this paper, a 34-layer deep\nresidual ConvNet architecture for end-to-end sleep staging is proposed. The\nnetwork takes raw single channel electroencephalogram (Fpz-Cz) signal as input\nand yields hypnogram annotations for each 30s segments as output. Experiments\nare carried out for two different scoring standards (5 and 6 stage\nclassification) on the expanded PhysioNet Sleep-EDF dataset, which contains\nmulti-source data from hospital and household polysomnography setups. The\nperformance of the proposed network is compared with that of the\nstate-of-the-art algorithms in patient independent validation tasks. The\nexperimental results demonstrate the superiority of the proposed network\ncompared to the best existing method, providing a relative improvement in\nepoch-wise average accuracy of 6.8% and 6.3% on the household data and\nmulti-source data, respectively. Codes are made publicly available on Github.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 11:32:46 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Humayun", "Ahmed Imtiaz", ""], ["Sushmit", "Asif Shahriyar", ""], ["Hasan", "Taufiq", ""], ["Bhuiyan", "Mohammed Imamul Hassan", ""]]}, {"id": "1904.10261", "submitter": "Aleksander Lukashou", "authors": "Aleksander Lukashou", "title": "Improving benchmarks for autonomous vehicles testing using synthetically\n  generated images", "comments": "4 pages, 38 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays autonomous technologies are a very heavily explored area and\nparticularly computer vision as the main component of vehicle perception. The\nquality of the whole vision system based on neural networks relies on the\ndataset it was trained on. It is extremely difficult to find traffic sign\ndatasets from most of the counties of the world. Meaning autonomous vehicle\nfrom the USA will not be able to drive though Lithuania recognizing all road\nsigns on the way. In this paper, we propose a solution on how to update model\nusing a small dataset from the country vehicle will be used in. It is important\nto mention that is not panacea, rather small upgrade which can boost autonomous\ncar development in countries with limited data access. We achieved about 10\npercent quality raise and expect even better results during future experiments.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 11:59:36 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Lukashou", "Aleksander", ""]]}, {"id": "1904.10281", "submitter": "Shuai Zhang", "authors": "Shuai Zhang and Yi Tay and Lina Yao and Qi Liu", "title": "Quaternion Knowledge Graph Embeddings", "comments": "Accepted by NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we move beyond the traditional complex-valued representations,\nintroducing more expressive hypercomplex representations to model entities and\nrelations for knowledge graph embeddings. More specifically, quaternion\nembeddings, hypercomplex-valued embeddings with three imaginary components, are\nutilized to represent entities. Relations are modelled as rotations in the\nquaternion space. The advantages of the proposed approach are: (1) Latent\ninter-dependencies (between all components) are aptly captured with Hamilton\nproduct, encouraging a more compact interaction between entities and relations;\n(2) Quaternions enable expressive rotation in four-dimensional space and have\nmore degree of freedom than rotation in complex plane; (3) The proposed\nframework is a generalization of ComplEx on hypercomplex space while offering\nbetter geometrical interpretations, concurrently satisfying the key desiderata\nof relational representation learning (i.e., modeling symmetry, anti-symmetry\nand inversion). Experimental results demonstrate that our method achieves\nstate-of-the-art performance on four well-established knowledge graph\ncompletion benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 12:36:59 GMT"}, {"version": "v2", "created": "Sat, 25 May 2019 06:11:16 GMT"}, {"version": "v3", "created": "Thu, 31 Oct 2019 12:45:00 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Zhang", "Shuai", ""], ["Tay", "Yi", ""], ["Yao", "Lina", ""], ["Liu", "Qi", ""]]}, {"id": "1904.10294", "submitter": "Zihao Wang", "authors": "Zihao Wang, Datong Zhou, Yong Zhang, Hao Wu, Chenglong Bao", "title": "Wasserstein-Fisher-Rao Document Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a fundamental problem of natural language processing, it is important to\nmeasure the distance between different documents. Among the existing methods,\nthe Word Mover's Distance (WMD) has shown remarkable success in document\nsemantic matching for its clear physical insight as a parameter-free model.\nHowever, WMD is essentially based on the classical Wasserstein metric, thus it\noften fails to robustly represent the semantic similarity between texts of\ndifferent lengths. In this paper, we apply the newly developed\nWasserstein-Fisher-Rao (WFR) metric from unbalanced optimal transport theory to\nmeasure the distance between different documents. The proposed WFR document\ndistance maintains the great interpretability and simplicity as WMD. We\ndemonstrate that the WFR document distance has significant advantages when\ncomparing the texts of different lengths. In addition, an accelerated Sinkhorn\nbased algorithm with GPU implementation has been developed for the fast\ncomputation of WFR distances. The KNN classification results on eight datasets\nhave shown its clear improvement over WMD.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 13:11:40 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 08:55:21 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Wang", "Zihao", ""], ["Zhou", "Datong", ""], ["Zhang", "Yong", ""], ["Wu", "Hao", ""], ["Bao", "Chenglong", ""]]}, {"id": "1904.10353", "submitter": "Mile Sikic", "authors": "Tomislav \\v{S}ebrek, Jan Tomljanovi\\'c, Josip Krapac, Mile \\v{S}iki\\'c", "title": "Read classification using semi-supervised deep learning", "comments": "2nd International Workshop on Deep Learning for Precision Medicine,\n  ECML-PKDD, 2017, Skopje, Nothern Macedonia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a semi-supervised deep learning method for\ndetecting the specific types of reads that impede the de novo genome assembly\nprocess. Instead of dealing directly with sequenced reads, we analyze their\ncoverage graphs converted to 1D-signals. We noticed that specific signal\npatterns occur in each relevant class of reads. Semi-supervised approach is\nchosen because manually labelling the data is a very slow and tedious process,\nso our goal was to facilitate the assembly process with as little labeled data\nas possible. We tested two models to learn patterns in the coverage graphs:\nM1+M2 and semi-GAN. We evaluated the performance of each model based on a\nmanually labeled dataset that comprises various reads from multiple reference\ngenomes with respect to the number of labeled examples that were used during\nthe training process. In addition, we embedded our detection in the assembly\nprocess which improved the quality of assemblies.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 14:25:42 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["\u0160ebrek", "Tomislav", ""], ["Tomljanovi\u0107", "Jan", ""], ["Krapac", "Josip", ""], ["\u0160iki\u0107", "Mile", ""]]}, {"id": "1904.10359", "submitter": "Moa Johansson", "authors": "Moa Johansson, Marie Korneliusson, Nickey Lizbat Lawrence", "title": "Identifying cross country skiing techniques using power meters in ski\n  poles", "comments": "Presented at the Norwegian Artificial Intelligence Symposium 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power meters are becoming a widely used tool for measuring training and\nracing effort in cycling, and are now spreading also to other sports. This\nmeans that increasing volumes of data can be collected from athletes, with the\naim of helping coaches and athletes analyse and understanding training load,\nracing efforts, technique etc. In this project, we have collaborated with\nSkisens AB, a company producing handles for cross country ski poles equipped\nwith power meters. We have conducted a pilot study in the use of machine\nlearning techniques on data from Skisens poles to identify which \"gear\" a skier\nis using (double poling or gears 2-4 in skating), based only on the sensor data\nfrom the ski poles. The dataset for this pilot study contained labelled\ntime-series data from three individual skiers using four different gears\nrecorded in varied locations and varied terrain. We systematically evaluated a\nnumber of machine learning techniques based on neural networks with best\nresults obtained by a LSTM network (accuracy of 95% correctly classified\nstrokes), when a subset of data from all three skiers was used for training. As\nexpected, accuracy dropped to 78% when the model was trained on data from only\ntwo skiers and tested on the third. To achieve better generalisation to\nindividuals not appearing in the training set more data is required, which is\nongoing work.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 14:43:10 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 13:27:28 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Johansson", "Moa", ""], ["Korneliusson", "Marie", ""], ["Lawrence", "Nickey Lizbat", ""]]}, {"id": "1904.10367", "submitter": "Gabriel de Souza Pereira Moreira", "authors": "Gabriel de Souza Pereira Moreira, Dietmar Jannach, Adilson Marques da\n  Cunha", "title": "Contextual Hybrid Session-based News Recommendation with Recurrent\n  Neural Networks", "comments": "20 pgs. Published at IEEE Access, Volume 7, 2019.\n  https://ieeexplore.ieee.org/document/8908688", "journal-ref": "IEEE Access 7 (2019): 169185-169203", "doi": "10.1109/ACCESS.2019.2954957", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems help users deal with information overload by providing\ntailored item suggestions to them. The recommendation of news is often\nconsidered to be challenging, since the relevance of an article for a user can\ndepend on a variety of factors, including the user's short-term reading\ninterests, the reader's context, or the recency or popularity of an article.\nPrevious work has shown that the use of Recurrent Neural Networks is promising\nfor the next-in-session prediction task, but has certain limitations when only\nrecorded item click sequences are used as input. In this work, we present a\ncontextual hybrid, deep learning based approach for session-based news\nrecommendation that is able to leverage a variety of information types. We\nevaluated our approach on two public datasets, using a temporal evaluation\nprotocol that simulates the dynamics of a news portal in a realistic way. Our\nresults confirm the benefits of considering additional types of information,\nincluding article popularity and recency, in the proposed way, resulting in\nsignificantly higher recommendation accuracy and catalog coverage than other\nsession-based algorithms. Additional experiments show that the proposed\nparameterizable loss function used in our method also allows us to balance two\nusually conflicting quality factors, accuracy and novelty.\n  Keywords: Artificial Neural Networks, Context-Aware Recommender Systems,\nHybrid Recommender Systems, News Recommender Systems, Session-based\nRecommendation\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 21:47:43 GMT"}, {"version": "v2", "created": "Sun, 8 Dec 2019 16:56:09 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Moreira", "Gabriel de Souza Pereira", ""], ["Jannach", "Dietmar", ""], ["da Cunha", "Adilson Marques", ""]]}, {"id": "1904.10387", "submitter": "C\\'edric B\\'eny", "authors": "C\\'edric B\\'eny", "title": "Learning relevant features for statistical inference", "comments": "Changes resulting from ICLR2020 submission and review. The\n  presentation now accounts for the close connection to previous work on deep\n  CCA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two views of data, we consider the problem of finding the features of\none view which can be most faithfully inferred from the other. We find that\nthese are also the most correlated variables in the sense of deep canonical\ncorrelation analysis (DCCA). Moreover, we show that these variables can be used\nto construct a non-parametric representation of the implied joint probability\ndistribution, which can be thought of as a classical version of the Schmidt\ndecomposition of quantum states. This representation can be used to compute the\nexpectations of functions over one view of data conditioned on the other, such\nas Bayesian estimators and their standard deviations. We test the approach\nusing inference on occluded MNIST images, and show that our representation\ncontains multiple modes. Surprisingly, when applied to supervised learning (one\ndataset consists of labels), this approach automatically provides\nregularization and faster convergence compared to the cross-entropy objective.\nWe also explore using this approach to discover salient independent variables\nof a single dataset.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 15:29:04 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 03:27:20 GMT"}, {"version": "v3", "created": "Sun, 2 Jun 2019 02:56:39 GMT"}, {"version": "v4", "created": "Tue, 24 Mar 2020 13:47:56 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["B\u00e9ny", "C\u00e9dric", ""]]}, {"id": "1904.10400", "submitter": "Abeegithan Jeyasothy", "authors": "Abeegithan Jeyasothy, Savitha Ramasamy, Suresh Sundaram", "title": "Efficient single input-output layer spiking neural classifier with\n  time-varying weight model", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a supervised learning algorithm, namely, the Synaptic\nEfficacy Function with Meta-neuron based learning algorithm (SEF-M) for a\nspiking neural network with a time-varying weight model. For a given pattern,\nSEF-M uses the learning algorithm derived from meta-neuron based learning\nalgorithm to determine the change in weights corresponding to each presynaptic\nspike times. The changes in weights modulate the amplitude of a Gaussian\nfunction centred at the same presynaptic spike times. The sum of amplitude\nmodulated Gaussian functions represents the synaptic efficacy functions (or\ntime-varying weight models). The performance of SEF-M is evaluated against\nstate-of-the-art spiking neural network learning algorithms on 10 benchmark\ndatasets from UCI machine learning repository. Performance studies show\nsuperior generalization ability of SEF-M. An ablation study on time-varying\nweight model is conducted using JAFFE dataset. The results of the ablation\nstudy indicate that using a time-varying weight model instead of single weight\nmodel improves the classification accuracy by 14%. Thus, it can be inferred\nthat a single input-output layer spiking neural network with time-varying\nweight model is computationally more efficient than a multi-layer spiking\nneural network with long-term or short-term weight model.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 06:49:01 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Jeyasothy", "Abeegithan", ""], ["Ramasamy", "Savitha", ""], ["Sundaram", "Suresh", ""]]}, {"id": "1904.10416", "submitter": "Haozhe Zhang", "authors": "Haozhe Zhang, Dan Nettleton, Zhengyuan Zhu", "title": "Regression-Enhanced Random Forests", "comments": "12 pages, 5 figures", "journal-ref": "In JSM Proceedings (2017), Section on Statistical Learning and\n  Data Science, Alexandria, VA: American Statistical Association. 636 -- 647", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random forest (RF) methodology is one of the most popular machine learning\ntechniques for prediction problems. In this article, we discuss some cases\nwhere random forests may suffer and propose a novel generalized RF method,\nnamely regression-enhanced random forests (RERFs), that can improve on RFs by\nborrowing the strength of penalized parametric regression. The algorithm for\nconstructing RERFs and selecting its tuning parameters is described. Both\nsimulation study and real data examples show that RERFs have better predictive\nperformance than RFs in important situations often encountered in practice.\nMoreover, RERFs may incorporate known relationships between the response and\nthe predictors, and may give reliable predictions in extrapolation problems\nwhere predictions are required at points out of the domain of the training\ndataset. Strategies analogous to those described here can be used to improve\nother machine learning methods via combination with penalized parametric\nregression techniques.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 16:45:07 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Zhang", "Haozhe", ""], ["Nettleton", "Dan", ""], ["Zhu", "Zhengyuan", ""]]}, {"id": "1904.10446", "submitter": "Jason Chou", "authors": "Jason Chou, Gautam Hathi", "title": "Generated Loss, Augmented Training, and Multiscale VAE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The variational autoencoder (VAE) framework remains a popular option for\ntraining unsupervised generative models, especially for discrete data where\ngenerative adversarial networks (GANs) require workaround to create gradient\nfor the generator. In our work modeling US postal addresses, we show that our\ndiscrete VAE with tree recursive architecture demonstrates limited capability\nof capturing field correlations within structured data, even after overcoming\nthe challenge of posterior collapse with scheduled sampling and tuning of the\nKL-divergence weight $\\beta$. Worse, VAE seems to have difficulty mapping its\ngenerated samples to the latent space, as their VAE loss lags behind or even\nincreases during the training process. Motivated by this observation, we show\nthat augmenting training data with generated variants (augmented training) and\ntraining a VAE with multiple values of $\\beta$ simultaneously (multiscale VAE)\nboth improve the generation quality of VAE. Despite their differences in\nmotivation and emphasis, we show that augmented training and multiscale VAE are\nactually connected and have similar effects on the model.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 17:53:59 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Chou", "Jason", ""], ["Hathi", "Gautam", ""]]}, {"id": "1904.10450", "submitter": "Lijiang Guo", "authors": "Lijiang Guo", "title": "Latent Variable Algorithms for Multimodal Learning and Sensor Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal learning has been lacking principled ways of combining information\nfrom different modalities and learning a low-dimensional manifold of meaningful\nrepresentations. We study multimodal learning and sensor fusion from a latent\nvariable perspective. We first present a regularized recurrent attention filter\nfor sensor fusion. This algorithm can dynamically combine information from\ndifferent types of sensors in a sequential decision making task. Each sensor is\nbonded with a modular neural network to maximize utility of its own\ninformation. A gating modular neural network dynamically generates a set of\nmixing weights for outputs from sensor networks by balancing utility of all\nsensors' information. We design a co-learning mechanism to encourage\nco-adaption and independent learning of each sensor at the same time, and\npropose a regularization based co-learning method. In the second part, we focus\non recovering the manifold of latent representation. We propose a co-learning\napproach using probabilistic graphical model which imposes a structural prior\non the generative model: multimodal variational RNN (MVRNN) model, and derive a\nvariational lower bound for its objective functions. In the third part, we\nextend the siamese structure to sensor fusion for robust acoustic event\ndetection. We perform experiments to investigate the latent representations\nthat are extracted; works will be done in the following months. Our experiments\nshow that the recurrent attention filter can dynamically combine different\nsensor inputs according to the information carried in the inputs. We consider\nMVRNN can identify latent representations that are useful for many downstream\ntasks such as speech synthesis, activity recognition, and control and planning.\nBoth algorithms are general frameworks which can be applied to other tasks\nwhere different types of sensors are jointly used for decision making.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 17:58:19 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Guo", "Lijiang", ""]]}, {"id": "1904.10509", "submitter": "Rewon Child", "authors": "Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever", "title": "Generating Long Sequences with Sparse Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers are powerful sequence models, but require time and memory that\ngrows quadratically with the sequence length. In this paper we introduce sparse\nfactorizations of the attention matrix which reduce this to $O(n \\sqrt{n})$. We\nalso introduce a) a variation on architecture and initialization to train\ndeeper networks, b) the recomputation of attention matrices to save memory, and\nc) fast attention kernels for training. We call networks with these changes\nSparse Transformers, and show they can model sequences tens of thousands of\ntimesteps long using hundreds of layers. We use the same architecture to model\nimages, audio, and text from raw bytes, setting a new state of the art for\ndensity modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate\nunconditional samples that demonstrate global coherence and great diversity,\nand show it is possible in principle to use self-attention to model sequences\nof length one million or more.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 19:29:47 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Child", "Rewon", ""], ["Gray", "Scott", ""], ["Radford", "Alec", ""], ["Sutskever", "Ilya", ""]]}, {"id": "1904.10522", "submitter": "Hyunsu Cho", "authors": "Theodore Vasiloudis, Hyunsu Cho, Henrik Bostr\\\"om", "title": "Block-distributed Gradient Boosted Trees", "comments": "SIGIR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gradient Boosted Tree (GBT) algorithm is one of the most popular machine\nlearning algorithms used in production, for tasks that include Click-Through\nRate (CTR) prediction and learning-to-rank. To deal with the massive datasets\navailable today, many distributed GBT methods have been proposed. However, they\nall assume a row-distributed dataset, addressing scalability only with respect\nto the number of data points and not the number of features, and increasing\ncommunication cost for high-dimensional data. In order to allow for scalability\nacross both the data point and feature dimensions, and reduce communication\ncost, we propose block-distributed GBTs. We achieve communication efficiency by\nmaking full use of the data sparsity and adapting the Quickscorer algorithm to\nthe block-distributed setting. We evaluate our approach using datasets with\nmillions of features, and demonstrate that we are able to achieve multiple\norders of magnitude reduction in communication cost for sparse data, with no\nloss in accuracy, while providing a more scalable design. As a result, we are\nable to reduce the training time for high-dimensional data, and allow more\ncost-effective scale-out without the need for expensive network communication.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 20:10:36 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 19:32:35 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Vasiloudis", "Theodore", ""], ["Cho", "Hyunsu", ""], ["Bostr\u00f6m", "Henrik", ""]]}, {"id": "1904.10551", "submitter": "Arjun Pakrashi", "authors": "Arjun Pakrashi, Brian Mac Namee", "title": "CascadeML: An Automatic Neural Network Architecture Evolution and\n  Training Algorithm for Multi-label Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label classification is an approach which allows a datapoint to be\nlabelled with more than one class at the same time. A common but trivial\napproach is to train individual binary classifiers per label, but the\nperformance can be improved by considering associations within the labels. Like\nwith any machine learning algorithm, hyperparameter tuning is important to\ntrain a good multi-label classifier model. The task of selecting the best\nhyperparameter settings for an algorithm is an optimisation problem. Very\nlimited work has been done on automatic hyperparameter tuning and AutoML in the\nmulti-label domain. This paper attempts to fill this gap by proposing a neural\nnetwork algorithm, CascadeML, to train multi-label neural network based on\ncascade neural networks. This method requires minimal or no hyperparameter\ntuning and also considers pairwise label associations. The cascade algorithm\ngrows the network architecture incrementally in a two phase process as it\nlearns the weights using adaptive first order gradient algorithm, therefore\nomitting the requirement of preselecting the number of hidden layers, nodes and\nthe learning rate. The method was tested on 10 multi-label datasets and\ncompared with other multi-label classification algorithms. Results show that\nCascadeML performs very well without hyperparameter tuning.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 22:05:51 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Pakrashi", "Arjun", ""], ["Mac Namee", "Brian", ""]]}, {"id": "1904.10552", "submitter": "Arjun Pakrashi", "authors": "Arjun Pakrashi, Brian Mac Namee", "title": "ML-KFHE: Multi-label ensemble classification algorithm exploiting sensor\n  fusion properties of the Kalman filter", "comments": "The paper is under consideration at Information Fusion, Elsevier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of ensemble classification methods in multi-class\nclassification problems, ensemble methods based on approaches other than\nbagging have not been widely explored for multi-label classification problems.\nThe Kalman Filter-based Heuristic Ensemble (KFHE) is a recent ensemble method\nthat exploits the sensor fusion properties of the Kalman filter to combine\nseveral classifier models, and that has been shown to be very effective. This\nwork proposes a multi-label version of KFHE, ML-KFHE, demonstrating the\neffectiveness of the KFHE method on multi-label datasets. Two variants are\nintroduced based on the underlying component classifier algorithm,\nML-KFHE-HOMER, and ML-KFHE-CC which uses HOMER and Classifier Chain (CC) as the\nunderlying multi-label algorithms respectively. ML-KFHE-HOMER and ML-KFHE-CC\nsequentially trains multiple HOMER and CC multi-label classifiers and\naggregates their outputs using the sensor fusion properties of the Kalman\nfilter. Experiments and detailed analysis performed on thirteen multi-label\ndatasets and eight other algorithms, including state-of-the-art ensemble\nmethods, show that for both versions, the ML-KFHE framework improves the\nensembling process significantly with respect to bagging based combinations of\nHOMER and CC, thus demonstrating the effectiveness of ML-KFHE. Also, the\nML-KFHE-HOMER variant was found to perform consistently and significantly\nbetter than existing multi-label methods including existing approaches based on\nensembles.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 22:10:50 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 20:56:54 GMT"}, {"version": "v3", "created": "Wed, 10 Mar 2021 15:49:56 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Pakrashi", "Arjun", ""], ["Mac Namee", "Brian", ""]]}, {"id": "1904.10554", "submitter": "Sebastian Jaimungal", "authors": "Philippe Casgrain, Brian Ning, Sebastian Jaimungal", "title": "Deep Q-Learning for Nash Equilibria: Nash-DQN", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT q-fin.CP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-free learning for multi-agent stochastic games is an active area of\nresearch. Existing reinforcement learning algorithms, however, are often\nrestricted to zero-sum games, and are applicable only in small state-action\nspaces or other simplified settings. Here, we develop a new data efficient\nDeep-Q-learning methodology for model-free learning of Nash equilibria for\ngeneral-sum stochastic games. The algorithm uses a local linear-quadratic\nexpansion of the stochastic game, which leads to analytically solvable optimal\nactions. The expansion is parametrized by deep neural networks to give it\nsufficient flexibility to learn the environment without the need to experience\nall state-action pairs. We study symmetry properties of the algorithm stemming\nfrom label-invariant stochastic games and as a proof of concept, apply our\nalgorithm to learning optimal trading strategies in competitive electronic\nmarkets.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 22:18:59 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Casgrain", "Philippe", ""], ["Ning", "Brian", ""], ["Jaimungal", "Sebastian", ""]]}, {"id": "1904.10573", "submitter": "Max Wilson", "authors": "Max Wilson, Thomas Vandal, Tad Hogg, Eleanor Rieffel", "title": "Quantum-assisted associative adversarial network: Applying quantum\n  annealing in deep learning", "comments": null, "journal-ref": "Quantum Machine Intelligence 3:19 (2021)", "doi": "10.1007/s42484-021-00047-9", "report-no": null, "categories": "cs.LG quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for learning a latent variable generative model via\ngenerative adversarial learning where the canonical uniform noise input is\nreplaced by samples from a graphical model. This graphical model is learned by\na Boltzmann machine which learns low-dimensional feature representation of data\nextracted by the discriminator. A quantum annealer, the D-Wave 2000Q, is used\nto sample from this model. This algorithm joins a growing family of algorithms\nthat use a quantum annealing subroutine in deep learning, and provides a\nframework to test the advantages of quantum-assisted learning in GANs. Fully\nconnected, symmetric bipartite and Chimera graph topologies are compared on a\nreduced stochastically binarized MNIST dataset, for both classical and quantum\nannealing sampling methods. The quantum-assisted associative adversarial\nnetwork successfully learns a generative model of the MNIST dataset for all\ntopologies, and is also applied to the LSUN dataset bedrooms class for the\nChimera topology. Evaluated using the Fr\\'{e}chet inception distance and\ninception score, the quantum and classical versions of the algorithm are found\nto have equivalent performance for learning an implicit generative model of the\nMNIST dataset.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 23:57:40 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Wilson", "Max", ""], ["Vandal", "Thomas", ""], ["Hogg", "Tad", ""], ["Rieffel", "Eleanor", ""]]}, {"id": "1904.10574", "submitter": "Hasan Manzour", "authors": "Hasan Manzour, Simge K\\\"u\\c{c}\\\"ukyavuz, Ali Shojaie", "title": "Integer Programming for Learning Directed Acyclic Graphs from Continuous\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning directed acyclic graphs (DAGs) from data is a challenging task both\nin theory and in practice, because the number of possible DAGs scales\nsuperexponentially with the number of nodes. In this paper, we study the\nproblem of learning an optimal DAG from continuous observational data. We cast\nthis problem in the form of a mathematical programming model which can\nnaturally incorporate a super-structure in order to reduce the set of possible\ncandidate DAGs. We use the penalized negative log-likelihood score function\nwith both $\\ell_0$ and $\\ell_1$ regularizations and propose a new mixed-integer\nquadratic optimization (MIQO) model, referred to as a layered network (LN)\nformulation. The LN formulation is a compact model, which enjoys as tight an\noptimal continuous relaxation value as the stronger but larger formulations\nunder a mild condition. Computational results indicate that the proposed\nformulation outperforms existing mathematical formulations and scales better\nthan available algorithms that can solve the same problem with only $\\ell_1$\nregularization. In particular, the LN formulation clearly outperforms existing\nmethods in terms of computational time needed to find an optimal DAG in the\npresence of a sparse super-structure.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 23:58:40 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Manzour", "Hasan", ""], ["K\u00fc\u00e7\u00fckyavuz", "Simge", ""], ["Shojaie", "Ali", ""]]}, {"id": "1904.10583", "submitter": "Thomas Uriot Tu", "authors": "Thomas Uriot", "title": "Kernel Mean Embedding of Instance-wise Predictions in Multiple Instance\n  Regression", "comments": "KDD 2019, FEED Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an extension to an existing algorithm\n(instance-MIR) which tackles the multiple instance regression (MIR) problem,\nalso known as distribution regression. The MIR setting arises when the data is\na collection of bags, where each bag consists of several instances which\ncorrespond to the same and unique real-valued label. The goal of a MIR\nalgorithm is to find a mapping from the instances of an unseen bag to its\ntarget value. The instance-MIR algorithm treats all the instances separately\nand maps each instance to a label. The final bag label is then taken as the\nmean or the median of the predictions for that given bag. While it is\nconceptually simple, taking a single statistic to summarize the distribution of\nthe labels in each bag is a limitation. In spite of this performance\nbottleneck, the instance-MIR algorithm has been shown to be competitive when\ncompared to the current state-of-the-art methods. We address the aforementioned\nissue by computing the kernel mean embeddings of the distributions of the\npredicted labels, for each bag, and learn a regressor from these embeddings to\nthe bag label. We test our algorithm (instance-kme-MIR) on five real world\ndatasets and obtain better results than the baseline instance-MIR across all\nthe datasets, while achieving state-of-the-art results on two of the datasets.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 00:24:55 GMT"}, {"version": "v2", "created": "Sun, 18 Aug 2019 16:37:47 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Uriot", "Thomas", ""]]}, {"id": "1904.10584", "submitter": "Sree Hari Krishnan Parthasarathi", "authors": "Sree Hari Krishnan Parthasarathi, Nitin Sivakrishnan, Pranav Ladkat,\n  Nikko Strom", "title": "Realizing Petabyte Scale Acoustic Modeling", "comments": "2156-3357 \\copyright 2019 IEEE. Personal use is permitted, but\n  republication/redistribution requires IEEE permission. See\n  http://www.ieee.org/publications standards/publications/rights/index.html for\n  more information", "journal-ref": null, "doi": "10.1109/JETCAS.2019.2912353", "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale machine learning (ML) systems such as the Alexa automatic speech\nrecognition (ASR) system continue to improve with increasing amounts of\nmanually transcribed training data. Instead of scaling manual transcription to\nimpractical levels, we utilize semi-supervised learning (SSL) to learn acoustic\nmodels (AM) from the vast firehose of untranscribed audio data. Learning an AM\nfrom 1 Million hours of audio presents unique ML and system design challenges.\nWe present the design and evaluation of a highly scalable and resource\nefficient SSL system for AM. Employing the student/teacher learning paradigm,\nwe focus on the student learning subsystem: a scalable and robust data pipeline\nthat generates features and targets from raw audio, and an efficient model\npipeline, including the distributed trainer, that builds a student model. Our\nevaluations show that, even without extensive hyper-parameter tuning, we obtain\nrelative accuracy improvements in the 10 to 20$\\%$ range, with higher gains in\nnoisier conditions. The end-to-end processing time of this SSL system was 12\ndays, and several components in this system can trivially scale linearly with\nmore compute resources.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 00:39:25 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Parthasarathi", "Sree Hari Krishnan", ""], ["Sivakrishnan", "Nitin", ""], ["Ladkat", "Pranav", ""], ["Strom", "Nikko", ""]]}, {"id": "1904.10616", "submitter": "Song Han", "authors": "Song Han, Han Cai, Ligeng Zhu, Ji Lin, Kuan Wang, Zhijian Liu, Yujun\n  Lin", "title": "Design Automation for Efficient Deep Learning Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient deep learning computing requires algorithm and hardware co-design\nto enable specialization: we usually need to change the algorithm to reduce\nmemory footprint and improve energy efficiency. However, the extra degree of\nfreedom from the algorithm makes the design space much larger: it's not only\nabout designing the hardware but also about how to tweak the algorithm to best\nfit the hardware. Human engineers can hardly exhaust the design space by\nheuristics. It's labor consuming and sub-optimal. We propose design automation\ntechniques for efficient neural networks. We investigate automatically\ndesigning specialized fast models, auto channel pruning, and auto\nmixed-precision quantization. We demonstrate such learning-based, automated\ndesign achieves superior performance and efficiency than rule-based human\ndesign. Moreover, we shorten the design cycle by 200x than previous work, so\nthat we can afford to design specialized neural network models for different\nhardware platforms.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 02:45:44 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Han", "Song", ""], ["Cai", "Han", ""], ["Zhu", "Ligeng", ""], ["Lin", "Ji", ""], ["Wang", "Kuan", ""], ["Liu", "Zhijian", ""], ["Lin", "Yujun", ""]]}, {"id": "1904.10631", "submitter": "Nimit Sohoni", "authors": "Nimit Sharad Sohoni and Christopher Richard Aberger and Megan\n  Leszczynski and Jian Zhang and Christopher R\\'e", "title": "Low-Memory Neural Network Training: A Technical Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory is increasingly often the bottleneck when training neural network\nmodels. Despite this, techniques to lower the overall memory requirements of\ntraining have been less widely studied compared to the extensive literature on\nreducing the memory requirements of inference. In this paper we study a\nfundamental question: How much memory is actually needed to train a neural\nnetwork? To answer this question, we profile the overall memory usage of\ntraining on two representative deep learning benchmarks -- the WideResNet model\nfor image classification and the DynamicConv Transformer model for machine\ntranslation -- and comprehensively evaluate four standard techniques for\nreducing the training memory requirements: (1) imposing sparsity on the model,\n(2) using low precision, (3) microbatching, and (4) gradient checkpointing. We\nexplore how each of these techniques in isolation affects both the peak memory\nusage of training and the quality of the end model, and explore the memory,\naccuracy, and computation tradeoffs incurred when combining these techniques.\nUsing appropriate combinations of these techniques, we show that it is possible\nto the reduce the memory required to train a WideResNet-28-2 on CIFAR-10 by up\nto 60.7x with a 0.4% loss in accuracy, and reduce the memory required to train\na DynamicConv model on IWSLT'14 German to English translation by up to 8.7x\nwith a BLEU score drop of 0.15.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 03:44:58 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Sohoni", "Nimit Sharad", ""], ["Aberger", "Christopher Richard", ""], ["Leszczynski", "Megan", ""], ["Zhang", "Jian", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1904.10632", "submitter": "Nikolaj Tatti", "authors": "Nikolaj Tatti", "title": "Maximum Entropy Based Significance of Itemsets", "comments": "Journal version. The previous version is the conference paper", "journal-ref": null, "doi": "10.1007/s10115-008-0128-4", "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of defining the significance of an itemset. We say\nthat the itemset is significant if we are surprised by its frequency when\ncompared to the frequencies of its sub-itemsets. In other words, we estimate\nthe frequency of the itemset from the frequencies of its sub-itemsets and\ncompute the deviation between the real value and the estimate. For the\nestimation we use Maximum Entropy and for measuring the deviation we use\nKullback-Leibler divergence.\n  A major advantage compared to the previous methods is that we are able to use\nricher models whereas the previous approaches only measure the deviation from\nthe independence model.\n  We show that our measure of significance goes to zero for derivable itemsets\nand that we can use the rank as a statistical test. Our empirical results\ndemonstrate that for our real datasets the independence assumption is too\nstrong but applying more flexible models leads to good results.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 03:46:23 GMT"}, {"version": "v2", "created": "Sat, 27 Apr 2019 01:44:29 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Tatti", "Nikolaj", ""]]}, {"id": "1904.10642", "submitter": "Kaichun Hu", "authors": "Kai-Chun Hu, Chen-Huan Pi, Ting Han Wei, I-Chen Wu, Stone Cheng,\n  Yi-Wei Dai, Wei-Yuan Ye", "title": "Towards Combining On-Off-Policy Methods for Real-World Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we point out a fundamental property of the objective in\nreinforcement learning, with which we can reformulate the policy gradient\nobjective into a perceptron-like loss function, removing the need to\ndistinguish between on and off policy training. Namely, we posit that it is\nsufficient to only update a policy $\\pi$ for cases that satisfy the condition\n$A(\\frac{\\pi}{\\mu}-1)\\leq0$, where $A$ is the advantage, and $\\mu$ is another\npolicy. Furthermore, we show via theoretic derivation that a perceptron-like\nloss function matches the clipped surrogate objective for PPO. With our new\nformulation, the policies $\\pi$ and $\\mu$ can be arbitrarily apart in theory,\neffectively enabling off-policy training. To examine our derivations, we can\ncombine the on-policy PPO clipped surrogate (which we show to be equivalent\nwith one instance of the new reformation) with the off-policy IMPALA method. We\nfirst verify the combined method on the OpenAI Gym pendulum toy problem. Next,\nwe use our method to train a quadrotor position controller in a simulator. Our\ntrained policy is efficient and lightweight enough to perform in a low cost\nmicro-controller at a minimum update rate of 500 Hz. For the quadrotor, we show\ntwo experiments to verify our method and demonstrate performance: 1) hovering\nat a fixed position, and 2) tracking along a specific trajectory. In\npreliminary trials, we are also able to apply the method to a real-world\nquadrotor.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 05:07:49 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Hu", "Kai-Chun", ""], ["Pi", "Chen-Huan", ""], ["Wei", "Ting Han", ""], ["Wu", "I-Chen", ""], ["Cheng", "Stone", ""], ["Dai", "Yi-Wei", ""], ["Ye", "Wei-Yuan", ""]]}, {"id": "1904.10644", "submitter": "Yu Chen", "authors": "Yu Chen and Tom Diethe and Neil Lawrence", "title": "Facilitating Bayesian Continual Learning by Natural Gradients and Stein\n  Gradients", "comments": null, "journal-ref": "Continual Learning Workshop of 32nd Conference on Neural\n  Information Processing Systems (NeurIPS 2018)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning aims to enable machine learning models to learn a general\nsolution space for past and future tasks in a sequential manner. Conventional\nmodels tend to forget the knowledge of previous tasks while learning a new\ntask, a phenomenon known as catastrophic forgetting. When using Bayesian models\nin continual learning, knowledge from previous tasks can be retained in two\nways: 1). posterior distributions over the parameters, containing the knowledge\ngained from inference in previous tasks, which then serve as the priors for the\nfollowing task; 2). coresets, containing knowledge of data distributions of\nprevious tasks. Here, we show that Bayesian continual learning can be\nfacilitated in terms of these two means through the use of natural gradients\nand Stein gradients respectively.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 05:18:32 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Chen", "Yu", ""], ["Diethe", "Tom", ""], ["Lawrence", "Neil", ""]]}, {"id": "1904.10653", "submitter": "Xu Zhu", "authors": "Xu Zhu", "title": "Stochastic Lipschitz Q-Learning", "comments": "The papers have been removed and we refer the readers to\n  arXiv:1901.09277. arXiv admin note: author list truncated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an episodic Markov Decision Process (MDP) problem, an online algorithm\nchooses from a set of actions in a sequence of $H$ trials, where $H$ is the\nepisode length, in order to maximize the total payoff of the chosen actions.\nQ-learning, as the most popular model-free reinforcement learning (RL)\nalgorithm, directly parameterizes and updates value functions without\nexplicitly modeling the environment. Recently, [Jin et al. 2018] studies the\nsample complexity of Q-learning with finite states and actions. Their algorithm\nachieves nearly optimal regret, which shows that Q-learning can be made sample\nefficient. However, MDPs with large discrete states and actions [Silver et al.\n2016] or continuous spaces [Mnih et al. 2013] cannot learn efficiently in this\nway. Hence, it is critical to develop new algorithms to solve this dilemma with\nprovable guarantee on the sample complexity. With this motivation, we propose a\nnovel algorithm that works for MDPs with a more general setting, which has\ninfinitely many states and actions and assumes that the payoff function and\ntransition kernel are Lipschitz continuous. We also provide corresponding\ntheory justification for our algorithm. It achieves the regret\n$\\tilde{\\mathcal{O}}(K^{\\frac{d+1}{d+2}}\\sqrt{H^3}),$ where $K$ denotes the\nnumber of episodes and $d$ denotes the dimension of the joint space. To the\nbest of our knowledge, this is the first analysis in the model-free setting\nwhose established regret matches the lower bound up to a logarithmic factor.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 06:25:42 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 00:50:11 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Zhu", "Xu", ""]]}, {"id": "1904.10679", "submitter": "M{\\aa}ns Magnusson", "authors": "M\\r{a}ns Magnusson, Michael Riis Andersen, Johan Jonasson, Aki Vehtari", "title": "Bayesian leave-one-out cross-validation for large data", "comments": "Accepted to ICML 2019. This version is the submitted paper", "journal-ref": "Thirty-sixth International Conference on Machine Learning, PMLR\n  97:4244-4253, 2019", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Model inference, such as model comparison, model checking, and model\nselection, is an important part of model development. Leave-one-out\ncross-validation (LOO) is a general approach for assessing the generalizability\nof a model, but unfortunately, LOO does not scale well to large datasets. We\npropose a combination of using approximate inference techniques and\nprobability-proportional-to-size-sampling (PPS) for fast LOO model evaluation\nfor large datasets. We provide both theoretical and empirical results showing\ngood properties for large data.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 08:04:00 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Magnusson", "M\u00e5ns", ""], ["Andersen", "Michael Riis", ""], ["Jonasson", "Johan", ""], ["Vehtari", "Aki", ""]]}, {"id": "1904.10683", "submitter": "Peng Xu", "authors": "Peng Xu, Zhaohong Deng, Chen Cui, Te Zhang, Kup-Sze Choi, Gu Suhang,\n  Jun Wang, ShiTong Wang", "title": "Concise Fuzzy System Modeling Integrating Soft Subspace Clustering and\n  Sparse Learning", "comments": null, "journal-ref": null, "doi": "10.1109/TFUZZ.2019.2895572", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The superior interpretability and uncertainty modeling ability of\nTakagi-Sugeno-Kang fuzzy system (TSK FS) make it possible to describe complex\nnonlinear systems intuitively and efficiently. However, classical TSK FS\nusually adopts the whole feature space of the data for model construction,\nwhich can result in lengthy rules for high-dimensional data and lead to\ndegeneration in interpretability. Furthermore, for highly nonlinear modeling\ntask, it is usually necessary to use a large number of rules which further\nweakens the clarity and interpretability of TSK FS. To address these issues, a\nconcise zero-order TSK FS construction method, called ESSC-SL-CTSK-FS, is\nproposed in this paper by integrating the techniques of enhanced soft subspace\nclustering (ESSC) and sparse learning (SL). In this method, ESSC is used to\ngenerate the antecedents and various sparse subspace for different fuzzy rules,\nwhereas SL is used to optimize the consequent parameters of the fuzzy rules,\nbased on which the number of fuzzy rules can be effectively reduced. Finally,\nthe proposed ESSC-SL-CTSK-FS method is used to construct con-cise zero-order\nTSK FS that can explain the scenes in high-dimensional data modeling more\nclearly and easily. Experiments are conducted on various real-world datasets to\nconfirm the advantages.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 08:11:45 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Xu", "Peng", ""], ["Deng", "Zhaohong", ""], ["Cui", "Chen", ""], ["Zhang", "Te", ""], ["Choi", "Kup-Sze", ""], ["Suhang", "Gu", ""], ["Wang", "Jun", ""], ["Wang", "ShiTong", ""]]}, {"id": "1904.10689", "submitter": "Saurav Basu", "authors": "Saurav Basu, Koyel Mukherjee, Shrihari Vasudevan", "title": "Layer Dynamics of Linearised Neural Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the phenomenal success of deep learning in recent years, there\nremains a gap in understanding the fundamental mechanics of neural nets. More\nresearch is focussed on handcrafting complex and larger networks, and the\ndesign decisions are often ad-hoc and based on intuition. Some recent research\nhas aimed to demystify the learning dynamics in neural nets by attempting to\nbuild a theory from first principles, such as characterising the non-linear\ndynamics of specialised \\textit{linear} deep neural nets (such as orthogonal\nnetworks). In this work, we expand and derive properties of learning dynamics\nrespected by general multi-layer linear neural nets. Although an\nover-parameterisation of a single layer linear network, linear multi-layer\nneural nets offer interesting insights that explain how learning dynamics\nproceed in small pockets of the data space. We show in particular that multiple\nlayers in linear nets grow at approximately the same rate, and there are\ndistinct phases of learning with markedly different layer growth. We then apply\na linearisation process to a general RelU neural net and show how nonlinearity\nbreaks down the growth symmetry observed in liner neural nets. Overall, our\nwork can be viewed as an initial step in building a theory for understanding\nthe effect of layer design on the learning dynamics from first principles.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 08:29:43 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Basu", "Saurav", ""], ["Mukherjee", "Koyel", ""], ["Vasudevan", "Shrihari", ""]]}, {"id": "1904.10717", "submitter": "James Thorne", "authors": "James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Arpit\n  Mittal", "title": "Generating Token-Level Explanations for Natural Language Inference", "comments": "Accepted at NAACL2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of Natural Language Inference (NLI) is widely modeled as supervised\nsentence pair classification. While there has been a lot of work recently on\ngenerating explanations of the predictions of classifiers on a single piece of\ntext, there have been no attempts to generate explanations of classifiers\noperating on pairs of sentences. In this paper, we show that it is possible to\ngenerate token-level explanations for NLI without the need for training data\nexplicitly annotated for this purpose. We use a simple LSTM architecture and\nevaluate both LIME and Anchor explanations for this task. We compare these to a\nMultiple Instance Learning (MIL) method that uses thresholded attention make\ntoken-level predictions. The approach we present in this paper is a novel\nextension of zero-shot single-sentence tagging to sentence pairs for NLI. We\nconduct our experiments on the well-studied SNLI dataset that was recently\naugmented with manually annotation of the tokens that explain the entailment\nrelation. We find that our white-box MIL-based method, while orders of\nmagnitude faster, does not reach the same accuracy as the black-box methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 09:41:14 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Thorne", "James", ""], ["Vlachos", "Andreas", ""], ["Christodoulopoulos", "Christos", ""], ["Mittal", "Arpit", ""]]}, {"id": "1904.10743", "submitter": "Jiyu Chen", "authors": "Jiyu Chen, Karin Verspoor, Zenan Zhai", "title": "A bag-of-concepts model improves relation extraction in a narrow\n  knowledge domain with limited data", "comments": "To appear in Proceedings of the Student Research Workshop at the\n  North American Association for Computational Linguistics (NAACL) meeting 2019", "journal-ref": "In Proceedings of the Student Research Workshop at North American\n  Association for Computational Linguistics (NAACL) 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on a traditional relation extraction task in the context\nof limited annotated data and a narrow knowledge domain. We explore this task\nwith a clinical corpus consisting of 200 breast cancer follow-up treatment\nletters in which 16 distinct types of relations are annotated. We experiment\nwith an approach to extracting typed relations called window-bounded\nco-occurrence (WBC), which uses an adjustable context window around entity\nmentions of a relevant type, and compare its performance with a more typical\nintra-sentential co-occurrence baseline. We further introduce a new\nbag-of-concepts (BoC) approach to feature engineering based on the\nstate-of-the-art word embeddings and word synonyms. We demonstrate the\ncompetitiveness of BoC by comparing with methods of higher complexity, and\nexplore its effectiveness on this small dataset.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 11:06:54 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Chen", "Jiyu", ""], ["Verspoor", "Karin", ""], ["Zhai", "Zenan", ""]]}, {"id": "1904.10748", "submitter": "Kaito Fujii", "authors": "Kaito Fujii, Shinsaku Sakaue", "title": "Beyond Adaptive Submodularity: Approximation Guarantees of Greedy Policy\n  with Adaptive Submodularity Ratio", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new concept named adaptive submodularity ratio to study the\ngreedy policy for sequential decision making. While the greedy policy is known\nto perform well for a wide variety of adaptive stochastic optimization problems\nin practice, its theoretical properties have been analyzed only for a limited\nclass of problems. We narrow the gap between theory and practice by using\nadaptive submodularity ratio, which enables us to prove approximation\nguarantees of the greedy policy for a substantially wider class of problems.\nExamples of newly analyzed problems include important applications such as\nadaptive influence maximization and adaptive feature selection. Our adaptive\nsubmodularity ratio also provides bounds of adaptivity gaps. Experiments\nconfirm that the greedy policy performs well with the applications being\nconsidered compared to standard heuristics.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 11:18:47 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Fujii", "Kaito", ""], ["Sakaue", "Shinsaku", ""]]}, {"id": "1904.10753", "submitter": "Aysun Urhan", "authors": "Aysun Urhan, Burak Alakent", "title": "An Exploratory Analysis of Biased Learners in Soft-Sensing Frames", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data driven soft sensor design has recently gained immense popularity, due to\nadvances in sensory devices, and a growing interest in data mining. While\npartial least squares (PLS) is traditionally used in the process literature for\ndesigning soft sensors, the statistical literature has focused on sparse\nlearners, such as Lasso and relevance vector machine (RVM), to solve the high\ndimensional data problem. In the current study, predictive performances of\nthree regression techniques, PLS, Lasso and RVM were assessed and compared\nunder various offline and online soft sensing scenarios applied on datasets\nfrom five real industrial plants, and a simulated process. In offline learning,\npredictions of RVM and Lasso were found to be superior to those of PLS when a\nlarge number of time-lagged predictors were used. Online prediction results\ngave a slightly more complicated picture. It was found that the minimum\nprediction error achieved by PLS under moving window (MW), or just-in-time\nlearning scheme was decreased up to ~5-10% using Lasso, or RVM. However, when a\nsmall MW size was used, or the optimum number of PLS components was as low as\n~1, prediction performance of PLS surpassed RVM, which was found to yield\noccasional unstable predictions. PLS and Lasso models constructed via online\nparameter tuning generally did not yield better predictions compared to those\nconstructed via offline tuning. We present evidence to suggest that retaining a\nlarge portion of the available process measurement data in the predictor\nmatrix, instead of preselecting variables, would be more advantageous for\nsparse learners in increasing prediction accuracy. As a result, Lasso is\nrecommended as a better substitute for PLS in soft sensors; while performance\nof RVM should be validated before online application.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 11:45:23 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Urhan", "Aysun", ""], ["Alakent", "Burak", ""]]}, {"id": "1904.10762", "submitter": "Linsen Dong", "authors": "Linsen Dong, Guanyu Gao, Xinyi Zhang, Liangyu Chen, and Yonggang Wen", "title": "Baconian: A Unified Open-source Framework for Model-Based Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-Based Reinforcement Learning (MBRL) is one category of Reinforcement\nLearning (RL) algorithms which can improve sampling efficiency by modeling and\napproximating system dynamics. It has been widely adopted in the research of\nrobotics, autonomous driving, etc. Despite its popularity, there still lacks\nsome sophisticated and reusable open-source frameworks to facilitate MBRL\nresearch and experiments. To fill this gap, we develop a flexible and\nmodularized framework, Baconian, which allows researchers to easily implement a\nMBRL testbed by customizing or building upon our provided modules and\nalgorithms. Our framework can free users from re-implementing popular MBRL\nalgorithms from scratch thus greatly save users' efforts on MBRL experiments.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 05:35:50 GMT"}, {"version": "v2", "created": "Sun, 24 May 2020 10:55:55 GMT"}, {"version": "v3", "created": "Mon, 15 Mar 2021 15:47:28 GMT"}, {"version": "v4", "created": "Tue, 16 Mar 2021 03:40:36 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Dong", "Linsen", ""], ["Gao", "Guanyu", ""], ["Zhang", "Xinyi", ""], ["Chen", "Liangyu", ""], ["Wen", "Yonggang", ""]]}, {"id": "1904.10778", "submitter": "Abhishek Gupta", "authors": "Abhishek Gupta and Hao Chen and Jianzong Pi and Gaurav Tendolkar", "title": "Some Limit Properties of Markov Chains Induced by Stochastic Recursive\n  Algorithms", "comments": "Accepted in SIMODS, 37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY math.OC math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recursive stochastic algorithms have gained significant attention in the\nrecent past due to data driven applications. Examples include stochastic\ngradient descent for solving large-scale optimization problems and empirical\ndynamic programming algorithms for solving Markov decision problems. These\nrecursive stochastic algorithms approximate certain contraction operators and\ncan be viewed within the framework of iterated random operators. Accordingly,\nwe consider iterated random operators over a Polish space that simulate\niterated contraction operator over that Polish space. Assume that the iterated\nrandom operators are indexed by certain batch sizes such that as batch sizes\ngrow to infinity, each realization of the random operator converges (in some\nsense) to the contraction operator it is simulating. We show that starting from\nthe same initial condition, the distribution of the random sequence generated\nby the iterated random operators converges weakly to the trajectory generated\nby the contraction operator. We further show that under certain conditions, the\ntime average of the random sequence converges to the spatial mean of the\ninvariant distribution. We then apply these results to logistic regression,\nempirical value iteration, and empirical Q value iteration for finite state\nfinite action MDPs to illustrate the general theory develop here.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 12:51:46 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 20:35:37 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Gupta", "Abhishek", ""], ["Chen", "Hao", ""], ["Pi", "Jianzong", ""], ["Tendolkar", "Gaurav", ""]]}, {"id": "1904.10797", "submitter": "Julius Walln\\\"ofer", "authors": "Julius Walln\\\"ofer, Alexey A. Melnikov, Wolfgang D\\\"ur, and Hans J.\n  Briegel", "title": "Machine learning for long-distance quantum communication", "comments": "13+7 pages, 6+3 figures, 1+3 tables; v2: significantly extended scope\n  and updated figures", "journal-ref": "PRX Quantum 1, 010301 (2020)", "doi": "10.1103/PRXQuantum.1.010301", "report-no": null, "categories": "quant-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning can help us in solving problems in the context big data\nanalysis and classification, as well as in playing complex games such as Go.\nBut can it also be used to find novel protocols and algorithms for applications\nsuch as large-scale quantum communication? Here we show that machine learning\ncan be used to identify central quantum protocols, including teleportation,\nentanglement purification and the quantum repeater. These schemes are of\nimportance in long-distance quantum communication, and their discovery has\nshaped the field of quantum information processing. However, the usefulness of\nlearning agents goes beyond the mere re-production of known protocols; the same\napproach allows one to find improved solutions to long-distance communication\nproblems, in particular when dealing with asymmetric situations where channel\nnoise and segment distance are non-uniform. Our findings are based on the use\nof projective simulation, a model of a learning agent that combines\nreinforcement learning and decision making in a physically motivated framework.\nThe learning agent is provided with a universal gate set, and the desired task\nis specified via a reward scheme. From a technical perspective, the learning\nagent has to deal with stochastic environments and reactions. We utilize an\nidea reminiscent of hierarchical skill acquisition, where solutions to\nsub-problems are learned and re-used in the overall scheme. This is of\nparticular importance in the development of long-distance communication\nschemes, and opens the way for using machine learning in the design and\nimplementation of quantum networks.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 13:20:55 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 12:22:43 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Walln\u00f6fer", "Julius", ""], ["Melnikov", "Alexey A.", ""], ["D\u00fcr", "Wolfgang", ""], ["Briegel", "Hans J.", ""]]}, {"id": "1904.10799", "submitter": "David Rohde", "authors": "Dmytro Mykhaylov, David Rohde, Flavian Vasile, Martin Bompaire,\n  Olivier Jeunen", "title": "Three Methods for Training on Bandit Feedback", "comments": "5 pages 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are three quite distinct ways to train a machine learning model on\nrecommender system logs. The first method is to model the reward prediction for\neach possible recommendation to the user, at the scoring time the best\nrecommendation is found by computing an argmax over the personalized\nrecommendations. This method obeys principles such as the conditionality\nprinciple and the likelihood principle. A second method is useful when the\nmodel does not fit reality and underfits. In this case, we can use the fact\nthat we know the distribution of historical recommendations (concentrated on\npreviously identified good actions with some exploration) to adjust the errors\nin the fit to be evenly distributed over all actions. Finally, the inverse\npropensity score can be used to produce an estimate of the decision rules\nexpected performance. The latter two methods violate the conditionality and\nlikelihood principle but are shown to have good performance in certain\nsettings. In this paper we review the literature around this fundamental, yet\noften overlooked choice and do some experiments using the RecoGym simulation\nenvironment.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 13:23:18 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 16:26:20 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Mykhaylov", "Dmytro", ""], ["Rohde", "David", ""], ["Vasile", "Flavian", ""], ["Bompaire", "Martin", ""], ["Jeunen", "Olivier", ""]]}, {"id": "1904.10818", "submitter": "Michael Unser", "authors": "Michael Unser and Julien Fageot", "title": "Native Banach spaces for splines and variational inverse problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a systematic construction of native Banach spaces for general\nspline-admissible operators ${\\rm L}$. In short, the native space for ${\\rm L}$\nand the (dual) norm $\\|\\cdot\\|_{\\mathcal{X}'}$ is the largest space of\nfunctions $f: \\mathbb{R}^d \\to \\mathbb{R}$ such that $\\|{\\rm L}\nf\\|_{\\mathcal{X}'}<\\infty$, subject to the constraint that the\ngrowth-restricted null space of ${\\rm L}$be finite-dimensional. This space,\ndenoted by $\\mathcal{X}'_{\\rm L}$, is specified as the dual of the pre-native\nspace $\\mathcal{X}_{\\rm L}$, which is itself obtained through a suitable\ncompletion process. The main difference with prior constructions (e.g.,\nreproducing kernel Hilbert spaces) is that our approach involves test functions\nrather than sums of atoms (e.g, kernels), which makes it applicable to a much\nbroader class of norms, including total variation. Under specific admissibility\nand compatibility hypotheses, we lay out the direct-sum topology of\n$\\mathcal{X}_{\\rm L}$ and $\\mathcal{X}'_{\\rm L}$, and identify the whole family\nof equivalent norms. Our construction ensures that the native space and its\npre-dual are endowed with a fundamental Schwartz-Banach property. In practical\nterms, this means that $\\mathcal{X}'_{\\rm L}$ is rich enough to reproduce any\nfunction with an arbitrary degree of precision.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 13:53:58 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Unser", "Michael", ""], ["Fageot", "Julien", ""]]}, {"id": "1904.10824", "submitter": "Chongyang Wang", "authors": "Chongyang Wang, Min Peng, Temitayo A. Olugbade, Nicholas D. Lane,\n  Amanda C. De C. Williams, Nadia Bianchi-Berthouze", "title": "Learning Bodily and Temporal Attention in Protective Movement Behavior\n  Detection", "comments": "7 pages, 3 figures, 2 tables, code available, accepted in ACII 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For people with chronic pain, the assessment of protective behavior during\nphysical functioning is essential to understand their subjective pain-related\nexperiences (e.g., fear and anxiety toward pain and injury) and how they deal\nwith such experiences (avoidance or reliance on specific body joints), with the\nultimate goal of guiding intervention. Advances in deep learning (DL) can\nenable the development of such intervention. Using the EmoPain MoCap dataset,\nwe investigate how attention-based DL architectures can be used to improve the\ndetection of protective behavior by capturing the most informative temporal and\nbody configurational cues characterizing specific movements and the strategies\nused to perform them. We propose an end-to-end deep learning architecture named\nBodyAttentionNet (BANet). BANet is designed to learn temporal and bodily parts\nthat are more informative to the detection of protective behavior. The approach\naddresses the variety of ways people execute a movement (including healthy\npeople) independently of the type of movement analyzed. Through extensive\ncomparison experiments with other state-of-the-art machine learning techniques\nused with motion capture data, we show statistically significant improvements\nachieved by using these attention mechanisms. In addition, the BANet\narchitecture requires a much lower number of parameters than the state of the\nart for comparable if not higher performances.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 14:00:05 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 10:32:44 GMT"}, {"version": "v3", "created": "Wed, 17 Jul 2019 12:21:26 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Wang", "Chongyang", ""], ["Peng", "Min", ""], ["Olugbade", "Temitayo A.", ""], ["Lane", "Nicholas D.", ""], ["Williams", "Amanda C. De C.", ""], ["Bianchi-Berthouze", "Nadia", ""]]}, {"id": "1904.10829", "submitter": "Jann Goschenhofer", "authors": "Jann Goschenhofer, Franz MJ Pfister, Kamer Ali Yuksel, Bernd Bischl,\n  Urban Fietzek, Janek Thomas", "title": "Wearable-based Parkinson's Disease Severity Monitoring using Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  One major challenge in the medication of Parkinson's disease is that the\nseverity of the disease, reflected in the patients' motor state, cannot be\nmeasured using accessible biomarkers. Therefore, we develop and examine a\nvariety of statistical models to detect the motor state of such patients based\non sensor data from a wearable device. We find that deep learning models\nconsistently outperform a classical machine learning model applied on\nhand-crafted features in this time series classification task. Furthermore, our\nresults suggest that treating this problem as a regression instead of an\nordinal regression or a classification task is most appropriate. For consistent\nmodel evaluation and training, we adopt the leave-one-subject-out validation\nscheme to the training of deep learning models. We also employ a\nclass-weighting scheme to successfully mitigate the problem of high multi-class\nimbalances in this domain. In addition, we propose a customized performance\nmeasure that reflects the requirements of the involved medical staff on the\nmodel. To solve the problem of limited availability of high quality training\ndata, we propose a transfer learning technique which helps to improve model\nperformance substantially. Our results suggest that deep learning techniques\noffer a high potential to autonomously detect motor states of patients with\nParkinson's disease.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 14:05:34 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Goschenhofer", "Jann", ""], ["Pfister", "Franz MJ", ""], ["Yuksel", "Kamer Ali", ""], ["Bischl", "Bernd", ""], ["Fietzek", "Urban", ""], ["Thomas", "Janek", ""]]}, {"id": "1904.10871", "submitter": "Francesco Ortelli", "authors": "Francesco Ortelli and Sara van de Geer", "title": "Prediction bounds for higher order total variation regularized least\n  squares", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish adaptive results for trend filtering: least squares estimation\nwith a penalty on the total variation of $(k-1)^{\\rm th}$ order differences.\nOur approach is based on combining a general oracle inequality for the\n$\\ell_1$-penalized least squares estimator with \"interpolating vectors\" to\nupper-bound the \"effective sparsity\". This allows one to show that the\n$\\ell_1$-penalty on the $k^{\\text{th}}$ order differences leads to an estimator\nthat can adapt to the number of jumps in the $(k-1)^{\\text{th}}$ order\ndifferences of the underlying signal or an approximation thereof. We show the\nresult for $k \\in \\{1,2,3,4\\}$ and indicate how it could be derived for general\n$k\\in \\mathbb{N}$.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 15:26:48 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 06:18:35 GMT"}, {"version": "v3", "created": "Thu, 3 Oct 2019 19:26:40 GMT"}, {"version": "v4", "created": "Fri, 17 Jul 2020 08:08:01 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Ortelli", "Francesco", ""], ["van de Geer", "Sara", ""]]}, {"id": "1904.10873", "submitter": "Yanwei  Fu", "authors": "Yanwei Fu, Donghao Li, Xinwei Sun, Shun Zhang, Yizhou Wang, Yuan Yao", "title": "$S^{2}$-LBI: Stochastic Split Linearized Bregman Iterations for\n  Parsimonious Deep Learning", "comments": "technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel Stochastic Split Linearized Bregman Iteration\n($S^{2}$-LBI) algorithm to efficiently train the deep network. The $S^{2}$-LBI\nintroduces an iterative regularization path with structural sparsity. Our\n$S^{2}$-LBI combines the computational efficiency of the LBI, and model\nselection consistency in learning the structural sparsity. The computed\nsolution path intrinsically enables us to enlarge or simplify a network, which\ntheoretically, is benefited from the dynamics property of our $S^{2}$-LBI\nalgorithm. The experimental results validate our $S^{2}$-LBI on MNIST and\nCIFAR-10 dataset. For example, in MNIST, we can either boost a network with\nonly 1.5K parameters (1 convolutional layer of 5 filters, and 1 FC layer),\nachieves 98.40\\% recognition accuracy; or we simplify $82.5\\%$ of parameters in\nLeNet-5 network, and still achieves the 98.47\\% recognition accuracy. In\naddition, we also have the learning results on ImageNet, which will be added in\nthe next version of our report.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 15:31:55 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Fu", "Yanwei", ""], ["Li", "Donghao", ""], ["Sun", "Xinwei", ""], ["Zhang", "Shun", ""], ["Wang", "Yizhou", ""], ["Yao", "Yuan", ""]]}, {"id": "1904.10900", "submitter": "Qing Zhou", "authors": "Jiaying Gu and Qing Zhou", "title": "Learning big Gaussian Bayesian networks: partition, estimation, and\n  fusion", "comments": "26 pages, 2 figures, and 5 tables", "journal-ref": "Journal of Machine Learning Research, 21(158): 1-31, 2020", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structure learning of Bayesian networks has always been a challenging\nproblem. Nowadays, massive-size networks with thousands or more of nodes but\nfewer samples frequently appear in many areas. We develop a divide-and-conquer\nframework, called partition-estimation-fusion (PEF), for structure learning of\nsuch big networks. The proposed method first partitions nodes into clusters,\nthen learns a subgraph on each cluster of nodes, and finally fuses all learned\nsubgraphs into one Bayesian network. The PEF method is designed in a flexible\nway so that any structure learning method may be used in the second step to\nlearn a subgraph structure as either a DAG or a CPDAG. In the clustering step,\nwe adapt the hierarchical clustering method to automatically choose a proper\nnumber of clusters. In the fusion step, we propose a novel hybrid method that\nsequentially add edges between subgraphs. Extensive numerical experiments\ndemonstrate the competitive performance of our PEF method, in terms of both\nspeed and accuracy compared to existing methods. Our method can improve the\naccuracy of structure learning by 20% or more, while reducing running time up\nto two orders-of-magnitude.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 16:08:53 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Gu", "Jiaying", ""], ["Zhou", "Qing", ""]]}, {"id": "1904.10904", "submitter": "Peter Watson", "authors": "Peter A. G. Watson", "title": "Applying machine learning to improve simulations of a chaotic dynamical\n  system using empirical error correction", "comments": "26p, 7 figures To be published in Journal of Advances in Modeling\n  Earth Systems", "journal-ref": null, "doi": "10.1029/2018MS001597", "report-no": null, "categories": "physics.ao-ph cs.LG nlin.CD stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dynamical weather and climate prediction models underpin many studies of the\nEarth system and hold the promise of being able to make robust projections of\nfuture climate change based on physical laws. However, simulations from these\nmodels still show many differences compared with observations. Machine learning\nhas been applied to solve certain prediction problems with great success, and\nrecently it's been proposed that this could replace the role of\nphysically-derived dynamical weather and climate models to give better quality\nsimulations. Here, instead, a framework using machine learning together with\nphysically-derived models is tested, in which it is learnt how to correct the\nerrors of the latter from timestep to timestep. This maintains the physical\nunderstanding built into the models, whilst allowing performance improvements,\nand also requires much simpler algorithms and less training data. This is\ntested in the context of simulating the chaotic Lorenz '96 system, and it is\nshown that the approach yields models that are stable and that give both\nimproved skill in initialised predictions and better long-term climate\nstatistics. Improvements in long-term statistics are smaller than for single\ntime-step tendencies, however, indicating that it would be valuable to develop\nmethods that target improvements on longer time scales. Future strategies for\nthe development of this approach and possible applications to making progress\non important scientific problems are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 16:17:46 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Watson", "Peter A. G.", ""]]}, {"id": "1904.10921", "submitter": "Jaedeok Kim", "authors": "Jaedeok Kim, Chiyoun Park, Hyun-Joo Jung, Yoonsuck Choe", "title": "Plug-in, Trainable Gate for Streamlining Arbitrary Neural Networks", "comments": "Accepted to AAAI 2020 (Poster)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Architecture optimization, which is a technique for finding an efficient\nneural network that meets certain requirements, generally reduces to a set of\nmultiple-choice selection problems among alternative sub-structures or\nparameters. The discrete nature of the selection problem, however, makes this\noptimization difficult. To tackle this problem we introduce a novel concept of\na trainable gate function. The trainable gate function, which confers a\ndifferentiable property to discretevalued variables, allows us to directly\noptimize loss functions that include non-differentiable discrete values such as\n0-1 selection. The proposed trainable gate can be applied to pruning. Pruning\ncan be carried out simply by appending the proposed trainable gate functions to\neach intermediate output tensor followed by fine-tuning the overall model,\nusing any gradient-based training methods. So the proposed method can jointly\noptimize the selection of the pruned channels while fine-tuning the weights of\nthe pruned model at the same time. Our experimental results demonstrate that\nthe proposed method efficiently optimizes arbitrary neural networks in various\ntasks such as image classification, style transfer, optical flow estimation,\nand neural machine translation.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 16:57:20 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 15:11:45 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Kim", "Jaedeok", ""], ["Park", "Chiyoun", ""], ["Jung", "Hyun-Joo", ""], ["Choe", "Yoonsuck", ""]]}, {"id": "1904.10922", "submitter": "Michela Paganini", "authors": "Jessica Zosa Forde and Michela Paganini", "title": "The Scientific Method in the Science of Machine Learning", "comments": "4 pages + 1 appendix. Presented at the ICLR 2019 Debugging Machine\n  Learning Models workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the quest to align deep learning with the sciences to address calls for\nrigor, safety, and interpretability in machine learning systems, this\ncontribution identifies key missing pieces: the stages of hypothesis\nformulation and testing, as well as statistical and systematic uncertainty\nestimation -- core tenets of the scientific method. This position paper\ndiscusses the ways in which contemporary science is conducted in other domains\nand identifies potentially useful practices. We present a case study from\nphysics and describe how this field has promoted rigor through specific\nmethodological practices, and provide recommendations on how machine learning\nresearchers can adopt these practices into the research ecosystem. We argue\nthat both domain-driven experiments and application-agnostic questions of the\ninner workings of fundamental building blocks of machine learning models ought\nto be examined with the tools of the scientific method, to ensure we not only\nunderstand effect, but also begin to understand cause, which is the raison\nd'\\^{e}tre of science.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 17:01:43 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Forde", "Jessica Zosa", ""], ["Paganini", "Michela", ""]]}, {"id": "1904.10926", "submitter": "Malte Schilling", "authors": "Malte Schilling", "title": "Setup of a Recurrent Neural Network as a Body Model for Solving Inverse\n  and Forward Kinematics as well as Dynamics for a Redundant Manipulator", "comments": "Pre-print (accepted to IJCNN 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An internal model of the own body can be assumed a fundamental and\nevolutionary-early representation as it is present throughout the animal\nkingdom. Such functional models are, on the one hand, required in motor\ncontrol, for example solving the inverse kinematic or dynamic task in\ngoal-directed movements or a forward task in ballistic movements. On the other\nhand, such models are recruited in cognitive tasks as are planning ahead or\nobservation of actions of a conspecific. Here, we present a functional internal\nbody model that is based on the Mean of Multiple Computations principle. For\nthe first time such a model is completely realized in a recurrent neural\nnetwork as necessary normalization steps are integrated into the neural model\nitself. Secondly, a dynamic extension is applied to the model. It is shown how\nthe neural network solves a series of inverse tasks. Furthermore, emerging\nrepresentation in transformational layers are analyzed that show a form of\nprototypical population-coding as found in place or direction cells.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 19:31:52 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Schilling", "Malte", ""]]}, {"id": "1904.10931", "submitter": "Alex Fedorov", "authors": "Alex Fedorov, R Devon Hjelm, Anees Abrol, Zening Fu, Yuhui Du, Sergey\n  Plis, Vince D. Calhoun", "title": "Prediction of Progression to Alzheimer's disease with Deep InfoMax", "comments": "Accepted to 2019 IEEE Biomedical and Health Informatics (BHI) as a\n  conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Arguably, unsupervised learning plays a crucial role in the majority of\nalgorithms for processing brain imaging. A recently introduced unsupervised\napproach Deep InfoMax (DIM) is a promising tool for exploring brain structure\nin a flexible non-linear way. In this paper, we investigate the use of variants\nof DIM in a setting of progression to Alzheimer's disease in comparison with\nsupervised AlexNet and ResNet inspired convolutional neural networks. As a\nbenchmark, we use a classification task between four groups: patients with\nstable, and progressive mild cognitive impairment (MCI), with Alzheimer's\ndisease, and healthy controls. Our dataset is comprised of 828 subjects from\nthe Alzheimer's Disease Neuroimaging Initiative (ADNI) database. Our\nexperiments highlight encouraging evidence of the high potential utility of DIM\nin future neuroimaging studies.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 17:10:11 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 16:01:34 GMT"}, {"version": "v3", "created": "Wed, 1 May 2019 02:13:22 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Fedorov", "Alex", ""], ["Hjelm", "R Devon", ""], ["Abrol", "Anees", ""], ["Fu", "Zening", ""], ["Du", "Yuhui", ""], ["Plis", "Sergey", ""], ["Calhoun", "Vince D.", ""]]}, {"id": "1904.10937", "submitter": "Jason Chou", "authors": "Jason Chou", "title": "Generated Loss and Augmented Training of MNIST VAE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The variational autoencoder (VAE) framework is a popular option for training\nunsupervised generative models, featuring ease of training and latent\nrepresentation of data. The objective function of VAE does not guarantee to\nachieve the latter, however, and failure to do so leads to a frequent failure\nmode called posterior collapse. Even in successful cases, VAEs often result in\nlow-precision reconstructions and generated samples. The introduction of the\nKL-divergence weight $\\beta$ can help steer the model clear of posterior\ncollapse, but its tuning is often a trial-and-error process with no guiding\nmetrics. Here we test the idea of using the total VAE loss of generated samples\n(generated loss) as the proxy metric for generation quality, the related\nhypothesis that VAE reconstruction from the mean latent vector tends to be a\nmore typical example of its class than the original, and the idea of exploiting\nthis property by augmenting training data with generated variants (augmented\ntraining). The results are mixed, but repeated encoding and decoding indeed\nresult in qualitatively and quantitatively more typical examples from both\nconvolutional and fully-connected MNIST VAEs, suggesting that it may be an\ninherent property of the VAE framework.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 17:22:07 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Chou", "Jason", ""]]}, {"id": "1904.10939", "submitter": "Anindya Bhadra", "authors": "Anindya Bhadra, Jyotishka Datta, Yunfan Li, Nicholas G. Polson", "title": "Horseshoe Regularization for Machine Learning in Complex and Deep Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the advent of the horseshoe priors for regularization, global-local\nshrinkage methods have proved to be a fertile ground for the development of\nBayesian methodology in machine learning, specifically for high-dimensional\nregression and classification problems. They have achieved remarkable success\nin computation, and enjoy strong theoretical support. Most of the existing\nliterature has focused on the linear Gaussian case; see Bhadra et al. (2019b)\nfor a systematic survey. The purpose of the current article is to demonstrate\nthat the horseshoe regularization is useful far more broadly, by reviewing both\nmethodological and computational developments in complex models that are more\nrelevant to machine learning applications. Specifically, we focus on\nmethodological challenges in horseshoe regularization in nonlinear and\nnon-Gaussian models; multivariate models; and deep neural networks. We also\noutline the recent computational developments in horseshoe shrinkage for\ncomplex models along with a list of available software implementations that\nallows one to venture out beyond the comfort zone of the canonical linear\nregression problems.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 17:28:34 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 13:50:34 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Bhadra", "Anindya", ""], ["Datta", "Jyotishka", ""], ["Li", "Yunfan", ""], ["Polson", "Nicholas G.", ""]]}, {"id": "1904.10945", "submitter": "Donghwan Lee", "authors": "Donghwan Lee, Niao He", "title": "Target-Based Temporal Difference Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of target networks has been a popular and key component of recent\ndeep Q-learning algorithms for reinforcement learning, yet little is known from\nthe theory side. In this work, we introduce a new family of target-based\ntemporal difference (TD) learning algorithms and provide theoretical analysis\non their convergences. In contrast to the standard TD-learning, target-based TD\nalgorithms maintain two separate learning parameters-the target variable and\nonline variable. Particularly, we introduce three members in the family, called\nthe averaging TD, double TD, and periodic TD, where the target variable is\nupdated through an averaging, symmetric, or periodic fashion, mirroring those\ntechniques used in deep Q-learning practice.\n  We establish asymptotic convergence analyses for both averaging TD and double\nTD and a finite sample analysis for periodic TD. In addition, we also provide\nsome simulation results showing potentially superior convergence of these\ntarget-based TD algorithms compared to the standard TD-learning. While this\nwork focuses on linear function approximation and policy evaluation setting, we\nconsider this as a meaningful step towards the theoretical understanding of\ndeep Q-learning variants with target networks.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 17:41:58 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 01:25:31 GMT"}, {"version": "v3", "created": "Fri, 20 Sep 2019 20:23:44 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Lee", "Donghwan", ""], ["He", "Niao", ""]]}, {"id": "1904.10959", "submitter": "Samuel Asante Gyamerah", "authors": "Samuel Asante Gyamerah, Philip Ngare, Dennis Ikpe", "title": "Crop yield probability density forecasting via quantile random forest\n  and Epanechnikov Kernel function", "comments": "22 pages, 11 figures", "journal-ref": "Agricultural and Forest Meteorology, 280:107808 (2020)", "doi": "10.1016/j.ecolmodel.2014.01.030", "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A reliable and accurate forecasting model for crop yields is of crucial\nimportance for efficient decision-making process in the agricultural sector.\nHowever, due to weather extremes and uncertainties, most forecasting models for\ncrop yield are not reliable and accurate. For measuring the uncertainty and\nobtaining further information of future crop yields, a probability density\nforecasting model based on quantile random forest and Epanechnikov kernel\nfunction (QRF-SJ) is proposed. The nonlinear structure of random forest is\napplied to change the quantile regression model for building the probabilistic\nforecasting model. Epanechnikov kernel function and solve-the equation plug-in\napproach of Sheather and Jones are used in the kernel density estimation. A\ncase study using the annual crop yield of groundnut and millet in Ghana is\npresented to illustrate the efficiency and robustness of the proposed\ntechnique. The values of the prediction interval coverage probability and\nprediction interval normalized average width for the two crops show that the\nconstructed prediction intervals capture the observed yields with high coverage\nprobability. The probability density curves show that QRF-SJ method has a very\nhigh ability to forecast quality prediction intervals with a higher coverage\nprobability. The feature importance gave a score of the importance of each\nweather variable in building the quantile regression forest model. The farmer\nand other stakeholders are able to realize the specific weather variable that\naffect the yield of a selected crop through feature importance. The proposed\nmethod and its application on crop yield dataset are the first of its kind in\nliterature.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 22:55:08 GMT"}, {"version": "v2", "created": "Sat, 19 Oct 2019 17:07:07 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Gyamerah", "Samuel Asante", ""], ["Ngare", "Philip", ""], ["Ikpe", "Dennis", ""]]}, {"id": "1904.10990", "submitter": "Mohammad Esmaeilpour", "authors": "Mohammad Esmaeilpour, Patrick Cardinal, Alessandro Lameiras Koerich", "title": "A Robust Approach for Securing Audio Classification Against Adversarial\n  Attacks", "comments": "Paper Accepted for Publication in IEEE Transactions on Information\n  Forensics and Security", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial audio attacks can be considered as a small perturbation\nunperceptive to human ears that is intentionally added to the audio signal and\ncauses a machine learning model to make mistakes. This poses a security concern\nabout the safety of machine learning models since the adversarial attacks can\nfool such models toward the wrong predictions. In this paper we first review\nsome strong adversarial attacks that may affect both audio signals and their 2D\nrepresentations and evaluate the resiliency of the most common machine learning\nmodel, namely deep learning models and support vector machines (SVM) trained on\n2D audio representations such as short time Fourier transform (STFT), discrete\nwavelet transform (DWT) and cross recurrent plot (CRP) against several\nstate-of-the-art adversarial attacks. Next, we propose a novel approach based\non pre-processed DWT representation of audio signals and SVM to secure audio\nsystems against adversarial attacks. The proposed architecture has several\npreprocessing modules for generating and enhancing spectrograms including\ndimension reduction and smoothing. We extract features from small patches of\nthe spectrograms using speeded up robust feature (SURF) algorithm which are\nfurther used to generate a codebook using the K-Means++ algorithm. Finally,\ncodewords are used to train a SVM on the codebook of the SURF-generated\nvectors. All these steps yield to a novel approach for audio classification\nthat provides a good trade-off between accuracy and resilience. Experimental\nresults on three environmental sound datasets show the competitive performance\nof proposed approach compared to the deep neural networks both in terms of\naccuracy and robustness against strong adversarial attacks.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 18:07:52 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 17:32:06 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Esmaeilpour", "Mohammad", ""], ["Cardinal", "Patrick", ""], ["Koerich", "Alessandro Lameiras", ""]]}, {"id": "1904.10996", "submitter": "Zheng Ma", "authors": "Zheng Ma, Ming Li, Yuguang Wang", "title": "PAN: Path Integral Based Convolution for Deep Graph Neural Networks", "comments": null, "journal-ref": "ICML 2019 Workshop on Learning and Reasoning with Graph-Structured\n  Representations (Oral)", "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn cond-mat.stat-mech cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution operations designed for graph-structured data usually utilize the\ngraph Laplacian, which can be seen as message passing between the adjacent\nneighbors through a generic random walk. In this paper, we propose PAN, a new\ngraph convolution framework that involves every path linking the message sender\nand receiver with learnable weights depending on the path length, which\ncorresponds to the maximal entropy random walk. PAN generalizes the graph\nLaplacian to a new transition matrix we call \\emph{maximal entropy transition}\n(MET) matrix derived from a path integral formalism. Most previous graph\nconvolutional network architectures can be adapted to our framework, and many\nvariations and derivatives based on the path integral idea can be developed.\nExperimental results show that the path integral based graph neural networks\nhave great learnability and fast convergence rate, and achieve state-of-the-art\nperformance on benchmark tasks.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 18:20:15 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Ma", "Zheng", ""], ["Li", "Ming", ""], ["Wang", "Yuguang", ""]]}, {"id": "1904.11082", "submitter": "Xinlei Pan", "authors": "Xinlei Pan, Weiyao Wang, Xiaoshuai Zhang, Bo Li, Jinfeng Yi, Dawn Song", "title": "How You Act Tells a Lot: Privacy-Leakage Attack on Deep Reinforcement\n  Learning", "comments": "The first three authors contributed equally. Accepted by AAMAS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has been widely applied to various applications, some of\nwhich involve training with privacy-sensitive data. A modest number of data\nbreaches have been studied, including credit card information in natural\nlanguage data and identities from face dataset. However, most of these studies\nfocus on supervised learning models. As deep reinforcement learning (DRL) has\nbeen deployed in a number of real-world systems, such as indoor robot\nnavigation, whether trained DRL policies can leak private information requires\nin-depth study. To explore such privacy breaches in general, we mainly propose\ntwo methods: environment dynamics search via genetic algorithm and candidate\ninference based on shadow policies. We conduct extensive experiments to\ndemonstrate such privacy vulnerabilities in DRL under various settings. We\nleverage the proposed algorithms to infer floor plans from some trained Grid\nWorld navigation DRL agents with LiDAR perception. The proposed algorithm can\ncorrectly infer most of the floor plans and reaches an average recovery rate of\n95.83% using policy gradient trained agents. In addition, we are able to\nrecover the robot configuration in continuous control environments and an\nautonomous driving simulator with high accuracy. To the best of our knowledge,\nthis is the first work to investigate privacy leakage in DRL settings and we\nshow that DRL-based agents do potentially leak privacy-sensitive information\nfrom the trained policies.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 21:41:04 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Pan", "Xinlei", ""], ["Wang", "Weiyao", ""], ["Zhang", "Xiaoshuai", ""], ["Li", "Bo", ""], ["Yi", "Jinfeng", ""], ["Song", "Dawn", ""]]}, {"id": "1904.11088", "submitter": "Muhan Zhang", "authors": "Muhan Zhang, Shali Jiang, Zhicheng Cui, Roman Garnett, Yixin Chen", "title": "D-VAE: A Variational Autoencoder for Directed Acyclic Graphs", "comments": "Accepted by 33rd Conference on Neural Information Processing Systems\n  (NeurIPS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph structured data are abundant in the real world. Among different graph\ntypes, directed acyclic graphs (DAGs) are of particular interest to machine\nlearning researchers, as many machine learning models are realized as\ncomputations on DAGs, including neural networks and Bayesian networks. In this\npaper, we study deep generative models for DAGs, and propose a novel DAG\nvariational autoencoder (D-VAE). To encode DAGs into the latent space, we\nleverage graph neural networks. We propose an asynchronous message passing\nscheme that allows encoding the computations on DAGs, rather than using\nexisting simultaneous message passing schemes to encode local graph structures.\nWe demonstrate the effectiveness of our proposed DVAE through two tasks: neural\narchitecture search and Bayesian network structure learning. Experiments show\nthat our model not only generates novel and valid DAGs, but also produces a\nsmooth latent space that facilitates searching for DAGs with better performance\nthrough Bayesian optimization.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 22:22:57 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 21:52:21 GMT"}, {"version": "v3", "created": "Thu, 30 May 2019 17:51:04 GMT"}, {"version": "v4", "created": "Tue, 29 Oct 2019 04:28:17 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Zhang", "Muhan", ""], ["Jiang", "Shali", ""], ["Cui", "Zhicheng", ""], ["Garnett", "Roman", ""], ["Chen", "Yixin", ""]]}, {"id": "1904.11093", "submitter": "Mahdi Abavisani", "authors": "Mahdi Abavisani and Vishal M. Patel", "title": "Deep Sparse Representation-based Classification", "comments": null, "journal-ref": "IEEE Signal Processing Letters, 2019", "doi": "10.1109/LSP.2019.2913022", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a transductive deep learning-based formulation for the sparse\nrepresentation-based classification (SRC) method. The proposed network consists\nof a convolutional autoencoder along with a fully-connected layer. The role of\nthe autoencoder network is to learn robust deep features for classification. On\nthe other hand, the fully-connected layer, which is placed in between the\nencoder and the decoder networks, is responsible for finding the sparse\nrepresentation. The estimated sparse codes are then used for classification.\nVarious experiments on three different datasets show that the proposed network\nleads to sparse representations that give better classification results than\nstate-of-the-art SRC methods. The source code is available at:\ngithub.com/mahdiabavisani/DSRC.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 22:52:18 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Abavisani", "Mahdi", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1904.11094", "submitter": "Mariem Ben Fadhel", "authors": "Mariem Ben Fadhel, Kofi Nyarko", "title": "GAN Augmented Text Anomaly Detection with Sequences of Deep Statistics", "comments": "5 pages, 53rd Annual Conference on Information Sciences and Systems,\n  CISS 2019", "journal-ref": null, "doi": "10.1109/CISS.2019.8693024", "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection is the process of finding data points that deviate from a\nbaseline. In a real-life setting, anomalies are usually unknown or extremely\nrare. Moreover, the detection must be accomplished in a timely manner or the\nrisk of corrupting the system might grow exponentially. In this work, we\npropose a two level framework for detecting anomalies in sequences of discrete\nelements. First, we assess whether we can obtain enough information from the\nstatistics collected from the discriminator's layers to discriminate between\nout of distribution and in distribution samples. We then build an unsupervised\nanomaly detection module based on these statistics. As to augment the data and\nkeep track of classes of known data, we lean toward a semi-supervised\nadversarial learning applied to discrete elements.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 23:06:36 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Fadhel", "Mariem Ben", ""], ["Nyarko", "Kofi", ""]]}, {"id": "1904.11095", "submitter": "Minsu Cho", "authors": "Minsu Cho and Chinmay Hegde", "title": "Reducing The Search Space For Hyperparameter Optimization Using Group\n  Sparsity", "comments": "Published at ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm for hyperparameter selection in machine learning\nalgorithms. The algorithm is a novel modification of Harmonica, a spectral\nhyperparameter selection approach using sparse recovery methods. In particular,\nwe show that a special encoding of hyperparameter space enables a natural\ngroup-sparse recovery formulation, which when coupled with HyperBand (a\nmulti-armed bandit strategy) leads to improvement over existing hyperparameter\noptimization methods such as Successive Halving and Random Search. Experimental\nresults on image datasets such as CIFAR-10 confirm the benefits of our\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 23:09:54 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Cho", "Minsu", ""], ["Hegde", "Chinmay", ""]]}, {"id": "1904.11131", "submitter": "Xu Zhu", "authors": "Xu Zhu", "title": "Lipschitz Bandit Optimization with Improved Efficiency", "comments": "The papers have been removed and we refer the readers to\n  arXiv:1901.09277. arXiv admin note: author list truncated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Lipschitz bandit optimization problem with an emphasis on\npractical efficiency. Although there is rich literature on regret analysis of\nthis type of problem, e.g., [Kleinberg et al. 2008, Bubeck et al. 2011,\nSlivkins 2014], their proposed algorithms suffer from serious practical\nproblems including extreme time complexity and dependence on oracle\nimplementations. With this motivation, we propose a novel algorithm with an\nUpper Confidence Bound (UCB) exploration, namely Tree UCB-Hoeffding, using\nadaptive partitions. Our partitioning scheme is easy to implement and does not\nrequire any oracle settings. With a tree-based search strategy, the total\ncomputational cost can be improved to $\\mathcal{O}(T\\log T)$ for the first $T$\niterations. In addition, our algorithm achieves the regret lower bound up to a\nlogarithmic factor.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 02:38:33 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 00:49:54 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Zhu", "Xu", ""]]}, {"id": "1904.11132", "submitter": "Chapman Siu", "authors": "Chapman Siu", "title": "TreeGrad: Transferring Tree Ensembles to Neural Networks", "comments": "Technical Report on Implementation of Deep Neural Decision Forests\n  Algorithm. To accompany implementation here:\n  https://github.com/chappers/TreeGrad. Update: Please cite as: Siu, C. (2019).\n  \"Transferring Tree Ensembles to Neural Networks\". International Conference on\n  Neural Information Processing. Springer, 2019. arXiv admin note: text overlap\n  with arXiv:1909.11790", "journal-ref": null, "doi": "10.1007/978-3-030-36711-4_39", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient Boosting Decision Tree (GBDT) are popular machine learning\nalgorithms with implementations such as LightGBM and in popular machine\nlearning toolkits like Scikit-Learn. Many implementations can only produce\ntrees in an offline manner and in a greedy manner. We explore ways to convert\nexisting GBDT implementations to known neural network architectures with\nminimal performance loss in order to allow decision splits to be updated in an\nonline manner and provide extensions to allow splits points to be altered as a\nneural architecture search problem. We provide learning bounds for our neural\nnetwork.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 02:54:32 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 21:44:05 GMT"}, {"version": "v3", "created": "Mon, 9 Dec 2019 12:28:48 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Siu", "Chapman", ""]]}, {"id": "1904.11145", "submitter": "Ali Habibnia", "authors": "Ali Habibnia (1) and Esfandiar Maasoumi (2) ((1) Virginia Tech, (2)\n  Emory University)", "title": "Forecasting in Big Data Environments: an Adaptable and Automated\n  Shrinkage Estimation of Neural Networks (AAShNet)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers improved forecasting in possibly nonlinear dynamic\nsettings, with high-dimension predictors (\"big data\" environments). To overcome\nthe curse of dimensionality and manage data and model complexity, we examine\nshrinkage estimation of a back-propagation algorithm of a deep neural net with\nskip-layer connections. We expressly include both linear and nonlinear\ncomponents. This is a high-dimensional learning approach including both\nsparsity L1 and smoothness L2 penalties, allowing high-dimensionality and\nnonlinearity to be accommodated in one step. This approach selects significant\npredictors as well as the topology of the neural network. We estimate optimal\nvalues of shrinkage hyperparameters by incorporating a gradient-based\noptimization technique resulting in robust predictions with improved\nreproducibility. The latter has been an issue in some approaches. This is\nstatistically interpretable and unravels some network structure, commonly left\nto a black box. An additional advantage is that the nonlinear part tends to get\npruned if the underlying process is linear. In an application to forecasting\nequity returns, the proposed approach captures nonlinear dynamics between\nequities to enhance forecast performance. It offers an appreciable improvement\nover current univariate and multivariate models by RMSE and actual portfolio\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 03:43:02 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Habibnia", "Ali", ""], ["Maasoumi", "Esfandiar", ""]]}, {"id": "1904.11203", "submitter": "Tom Vander Aa", "authors": "Imen Chakroun, Tom Vander Aa and Tom Ashby", "title": "Reviewing Data Access Patterns and Computational Redundancy for Machine\n  Learning Algorithms", "comments": "European Commission Project: EPEEC - European joint Effort toward a\n  Highly Productive Programming Environment for Heterogeneous Exascale\n  Computing (EC-H2020-80151) An extended version of this paper titled\n  \"Guidelines for enhancing data locality in selected machine learning\n  algorithms\" has been published in the journal \"Intelligent Data Analysis\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) is probably the first and foremost used technique to\ndeal with the size and complexity of the new generation of data. In this paper,\nwe analyze one of the means to increase the performances of ML algorithms which\nis exploiting data locality. Data locality and access patterns are often at the\nheart of performance issues in computing systems due to the use of certain\nhardware techniques to improve performance. Altering the access patterns to\nincrease locality can dramatically increase performance of a given algorithm.\nBesides, repeated data access can be seen as redundancy in data movement.\nSimilarly, there can also be redundancy in the repetition of calculations. This\nwork also identifies some of the opportunities for avoiding these redundancies\nby directly reusing computation results. We document the possibilities of such\nreuse in some selected machine learning algorithms and give initial indicative\nresults from our first experiments on data access improvement and algorithm\nredesign.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 08:26:19 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 14:01:36 GMT"}, {"version": "v3", "created": "Thu, 9 Jan 2020 13:57:20 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Chakroun", "Imen", ""], ["Aa", "Tom Vander", ""], ["Ashby", "Tom", ""]]}, {"id": "1904.11223", "submitter": "Matteo Manica", "authors": "Matteo Manica, Ali Oskooei, Jannis Born, Vigneshwari Subramanian,\n  Julio S\\'aez-Rodr\\'iguez, Mar\\'ia Rodr\\'iguez Mart\\'inez", "title": "Towards Explainable Anticancer Compound Sensitivity Prediction via\n  Multimodal Attention-based Convolutional Encoders", "comments": "11 pages, 5 figures, 1 table, Workshop on Computational Biology at\n  the International Conference on Machine Learning (ICML), Long Beach, CA, 2019", "journal-ref": "Mol. Pharmaceutics 2019", "doi": "10.1021/acs.molpharmaceut.9b00520", "report-no": null, "categories": "cs.LG cs.AI q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In line with recent advances in neural drug design and sensitivity\nprediction, we propose a novel architecture for interpretable prediction of\nanticancer compound sensitivity using a multimodal attention-based\nconvolutional encoder. Our model is based on the three key pillars of drug\nsensitivity: compounds' structure in the form of a SMILES sequence, gene\nexpression profiles of tumors and prior knowledge on intracellular interactions\nfrom protein-protein interaction networks. We demonstrate that our multiscale\nconvolutional attention-based (MCA) encoder significantly outperforms a\nbaseline model trained on Morgan fingerprints, a selection of encoders based on\nSMILES as well as previously reported state of the art for multimodal drug\nsensitivity prediction (R2 = 0.86 and RMSE = 0.89). Moreover, the\nexplainability of our approach is demonstrated by a thorough analysis of the\nattention weights. We show that the attended genes significantly enrich\napoptotic processes and that the drug attention is strongly correlated with a\nstandard chemical structure similarity index. Finally, we report a case study\nof two receptor tyrosine kinase (RTK) inhibitors acting on a leukemia cell\nline, showcasing the ability of the model to focus on informative genes and\nsubmolecular regions of the two compounds. The demonstrated generalizability\nand the interpretability of our model testify its potential for in-silico\nprediction of anticancer compound efficacy on unseen cancer cells, positioning\nit as a valid solution for the development of personalized therapies as well as\nfor the evaluation of candidate compounds in de novo drug design.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 09:14:52 GMT"}, {"version": "v2", "created": "Wed, 22 May 2019 20:03:13 GMT"}, {"version": "v3", "created": "Sun, 14 Jul 2019 14:00:55 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Manica", "Matteo", ""], ["Oskooei", "Ali", ""], ["Born", "Jannis", ""], ["Subramanian", "Vigneshwari", ""], ["S\u00e1ez-Rodr\u00edguez", "Julio", ""], ["Mart\u00ednez", "Mar\u00eda Rodr\u00edguez", ""]]}, {"id": "1904.11266", "submitter": "Lei Zhu", "authors": "Yudong Han, Lei Zhu, Zhiyong Cheng, Jingjing Li, Xiaobai Liu", "title": "Discrete Optimal Graph Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph based clustering is one of the major clustering methods. Most of it\nwork in three separate steps: similarity graph construction, clustering label\nrelaxing and label discretization with k-means. Such common practice has three\ndisadvantages: 1) the predefined similarity graph is often fixed and may not be\noptimal for the subsequent clustering. 2) the relaxing process of cluster\nlabels may cause significant information loss. 3) label discretization may\ndeviate from the real clustering result since k-means is sensitive to the\ninitialization of cluster centroids. To tackle these problems, in this paper,\nwe propose an effective discrete optimal graph clustering (DOGC) framework. A\nstructured similarity graph that is theoretically optimal for clustering\nperformance is adaptively learned with a guidance of reasonable rank\nconstraint. Besides, to avoid the information loss, we explicitly enforce a\ndiscrete transformation on the intermediate continuous label, which derives a\ntractable optimization problem with discrete solution. Further, to compensate\nthe unreliability of the learned labels and enhance the clustering accuracy, we\ndesign an adaptive robust module that learns prediction function for the unseen\ndata based on the learned discrete cluster labels. Finally, an iterative\noptimization strategy guaranteed with convergence is developed to directly\nsolve the clustering results. Extensive experiments conducted on both real and\nsynthetic datasets demonstrate the superiority of our proposed methods compared\nwith several state-of-the-art clustering approaches.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 11:31:29 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Han", "Yudong", ""], ["Zhu", "Lei", ""], ["Cheng", "Zhiyong", ""], ["Li", "Jingjing", ""], ["Liu", "Xiaobai", ""]]}, {"id": "1904.11296", "submitter": "Sarah Itani", "authors": "Sarah Itani and Dorina Thanou", "title": "Combining Anatomical and Functional Networks for Neuropathology\n  Identification: A Case Study on Autism Spectrum Disorder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the prevalence of Autism Spectrum Disorder (ASD) is increasing,\nresearch continues in an effort to identify common etiological and\npathophysiological bases. In this regard, modern machine learning and network\nscience pave the way for a better understanding of the neuropathology and the\ndevelopment of diagnosis aid systems. The present work addresses the\nclassification of neurotypical and ASD subjects by combining knowledge about\nboth the structure and the functional activity of the brain. In particular, we\nmodel the brain structure as a graph, and the resting-state functional MRI\n(rs-fMRI) signals as values that live on the nodes of that graph. We then\nborrow tools from the emerging field of Graph Signal Processing (GSP) to build\nfeatures related to the frequency content of these signals. In order to make\nthese features highly discriminative, we apply an extension of the\nFukunaga-Koontz transform. Finally, we use these new markers to train a\ndecision tree, an interpretable classification scheme, which results in a final\ndiagnosis aid model. Interestingly, the resulting decision tree outperforms\nstate-of-the-art methods on the publicly available Autism Brain Imaging Data\nExchange (ABIDE) collection. Moreover, the analysis of the predictive markers\nreveals the influence of the frontal and temporal lobes in the diagnosis of the\ndisorder, which is in line with previous findings in the literature of\nneuroscience. Our results indicate that exploiting jointly structural and\nfunctional information of the brain can reveal important information about the\ncomplexity of the neuropathology.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 12:37:01 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 12:47:52 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Itani", "Sarah", ""], ["Thanou", "Dorina", ""]]}, {"id": "1904.11316", "submitter": "Yiming Ying", "authors": "Wei Shen, Zhenhuan Yang, Yiming Ying and Xiaoming Yuan", "title": "Stability and Optimization Error of Stochastic Gradient Descent for\n  Pairwise Learning", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the stability and its trade-off with optimization\nerror for stochastic gradient descent (SGD) algorithms in the pairwise learning\nsetting. Pairwise learning refers to a learning task which involves a loss\nfunction depending on pairs of instances among which notable examples are\nbipartite ranking, metric learning, area under ROC (AUC) maximization and\nminimum error entropy (MEE) principle. Our contribution is twofold. Firstly, we\nestablish the stability results of SGD for pairwise learning in the convex,\nstrongly convex and non-convex settings, from which generalization bounds can\nbe naturally derived. Secondly, we establish the trade-off between stability\nand optimization error of SGD algorithms for pairwise learning. This is\nachieved by lower-bounding the sum of stability and optimization error by the\nminimax statistical error over a prescribed class of pairwise loss functions.\nFrom this fundamental trade-off, we obtain lower bounds for the optimization\nerror of SGD algorithms and the excess expected risk over a class of pairwise\nlosses. In addition, we illustrate our stability results by giving some\nspecific examples of AUC maximization, metric learning and MEE.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 13:07:37 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2019 14:39:35 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Shen", "Wei", ""], ["Yang", "Zhenhuan", ""], ["Ying", "Yiming", ""], ["Yuan", "Xiaoming", ""]]}, {"id": "1904.11325", "submitter": "Aymeric Dieuleveut", "authors": "Kumar Kshitij Patel and Aymeric Dieuleveut", "title": "Communication trade-offs for synchronized distributed SGD with large\n  step size", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synchronous mini-batch SGD is state-of-the-art for large-scale distributed\nmachine learning. However, in practice, its convergence is bottlenecked by slow\ncommunication rounds between worker nodes. A natural solution to reduce\ncommunication is to use the \\emph{`local-SGD'} model in which the workers train\ntheir model independently and synchronize every once in a while. This algorithm\nimproves the computation-communication trade-off but its convergence is not\nunderstood very well. We propose a non-asymptotic error analysis, which enables\ncomparison to \\emph{one-shot averaging} i.e., a single communication round\namong independent workers, and \\emph{mini-batch averaging} i.e., communicating\nat every step. We also provide adaptive lower bounds on the communication\nfrequency for large step-sizes ($ t^{-\\alpha} $, $ \\alpha\\in (1/2 , 1 ) $) and\nshow that \\emph{Local-SGD} reduces communication by a factor of\n$O\\Big(\\frac{\\sqrt{T}}{P^{3/2}}\\Big)$, with $T$ the total number of gradients\nand $P$ machines.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 13:27:30 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Patel", "Kumar Kshitij", ""], ["Dieuleveut", "Aymeric", ""]]}, {"id": "1904.11367", "submitter": "Abeegithan Jeyasothy", "authors": "Abeegithan Jeyasothy, Suresh Sundaram, Savitha Ramasamy, Narasimhan\n  Sundararajan", "title": "A novel method for extracting interpretable knowledge from a spiking\n  neural classifier with time-varying synaptic weights", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel method for information interpretability in an\nMC-SEFRON classifier. To develop a method to extract knowledge stored in a\ntrained classifier, first, the binary-class SEFRON classifier developed earlier\nis extended to handle multi-class problems. MC-SEFRON uses the population\nencoding scheme to encode the real-valued input data into spike patterns.\nMC-SEFRON is trained using the same supervised learning rule used in the\nSEFRON. After training, the proposed method extracts the knowledge for a given\nclass stored in the classifier by mapping the weighted postsynaptic potential\nin the time domain to the feature domain as Feature Strength Functions (FSFs).\nA set of FSFs corresponding to each output class represents the extracted\nknowledge from the classifier. This knowledge encoding method is derived to\nmaintain consistency between the classification in the time domain and the\nfeature domain. The correctness of the FSF is quantitatively measured by using\nFSF directly for classification tasks. For a given input, each FSF is sampled\nat the input value to obtain the corresponding feature strength value (FSV).\nThen the aggregated FSVs obtained for each class are used to determine the\noutput class labels during classification. FSVs are also used to interpret the\npredictions during the classification task. Using ten UCI datasets and the\nMNIST dataset, the knowledge extraction method, interpretation and the\nreliability of the FSF are demonstrated. Based on the studies, it can be seen\nthat on an average, the difference in the classification accuracies using the\nFSF directly and those obtained by MC-SEFRON is only around 0.9% & 0.1\\% for\nthe UCI datasets and the MNIST dataset respectively. This clearly shows that\nthe knowledge represented by the FSFs has acceptable reliability and the\ninterpretability of classification using the classifier's knowledge has been\njustified.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 09:49:42 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Jeyasothy", "Abeegithan", ""], ["Sundaram", "Suresh", ""], ["Ramasamy", "Savitha", ""], ["Sundararajan", "Narasimhan", ""]]}, {"id": "1904.11376", "submitter": "Rogelio Andrade Mancisidor", "authors": "Rogelio A. Mancisidor, Michael Kampffmeyer, Kjersti Aas, Robert\n  Jenssen", "title": "Deep Generative Models for Reject Inference in Credit Scoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.RM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Credit scoring models based on accepted applications may be biased and their\nconsequences can have a statistical and economic impact. Reject inference is\nthe process of attempting to infer the creditworthiness status of the rejected\napplications. In this research, we use deep generative models to develop two\nnew semi-supervised Bayesian models for reject inference in credit scoring, in\nwhich we model the data generating process to be dependent on a Gaussian\nmixture. The goal is to improve the classification accuracy in credit scoring\nmodels by adding reject applications. Our proposed models infer the unknown\ncreditworthiness of the rejected applications by exact enumeration of the two\npossible outcomes of the loan (default or non-default). The efficient\nstochastic gradient optimization technique used in deep generative models makes\nour models suitable for large data sets. Finally, the experiments in this\nresearch show that our proposed models perform better than classical and\nalternative machine learning models for reject inference in credit scoring.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 12:16:15 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Mancisidor", "Rogelio A.", ""], ["Kampffmeyer", "Michael", ""], ["Aas", "Kjersti", ""], ["Jenssen", "Robert", ""]]}, {"id": "1904.11416", "submitter": "Nicholas Sanders", "authors": "Nicholas D. Sanders, Richard M. Everson, Jonathan E. Fieldsend, Alma\n  A. M. Rahat", "title": "A Bayesian Approach for the Robust Optimisation of Expensive-To-Evaluate\n  Functions", "comments": "Submitted to IEEE Transactions on Evolutionary Computation. 11 pages,\n  8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many expensive black-box optimisation problems are sensitive to their inputs.\nIn these problems it makes more sense to locate a region of good designs, than\na single, possible fragile, optimal design.\n  Expensive black-box functions can be optimised effectively with Bayesian\noptimisation, where a Gaussian process is a popular choice as a prior over the\nexpensive function. We propose a method for robust optimisation using Bayesian\noptimisation to find a region of design space in which the expensive function's\nperformance is insensitive to the inputs whilst retaining a good quality. This\nis achieved by sampling realisations from a Gaussian process modelling the\nexpensive function and evaluating the improvement for each realisation. The\nexpectation of these improvements can be optimised cheaply with an evolutionary\nalgorithm to determine the next location at which to evaluate the expensive\nfunction. We describe an efficient process to locate the optimum expected\nimprovement. We show empirically that evaluating the expensive function at the\nlocation in the candidate sweet spot about which the model is most uncertain or\nat random yield the best convergence in contrast to exploitative schemes.\n  We illustrate our method on six test functions in two, five, and ten\ndimensions, and demonstrate that it is able to outperform a state-of-the-art\napproach from the literature.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 15:43:37 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 16:55:14 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Sanders", "Nicholas D.", ""], ["Everson", "Richard M.", ""], ["Fieldsend", "Jonathan E.", ""], ["Rahat", "Alma A. M.", ""]]}, {"id": "1904.11419", "submitter": "Jie Chen", "authors": "Rao Fu, Jie Chen, Shutian Zeng, Yiping Zhuang, Agus Sudjianto", "title": "Time Series Simulation by Conditional Generative Adversarial Net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Net (GAN) has been proven to be a powerful machine\nlearning tool in image data analysis and generation. In this paper, we propose\nto use Conditional Generative Adversarial Net (CGAN) to learn and simulate time\nseries data. The conditions can be both categorical and continuous variables\ncontaining different kinds of auxiliary information. Our simulation studies\nshow that CGAN is able to learn different kinds of normal and heavy tail\ndistributions, as well as dependent structures of different time series and it\ncan further generate conditional predictive distributions consistent with the\ntraining data distributions. We also provide an in-depth discussion on the\nrationale of GAN and the neural network as hierarchical splines to draw a clear\nconnection with the existing statistical method for distribution generation. In\npractice, CGAN has a wide range of applications in the market risk and\ncounterparty risk analysis: it can be applied to learn the historical data and\ngenerate scenarios for the calculation of Value-at-Risk (VaR) and Expected\nShortfall (ES) and predict the movement of the market risk factors. We present\na real data analysis including a backtesting to demonstrate CGAN is able to\noutperform the Historic Simulation, a popular method in market risk analysis\nfor the calculation of VaR. CGAN can also be applied in the economic time\nseries modeling and forecasting, and an example of hypothetical shock analysis\nfor economic models and the generation of potential CCAR scenarios by CGAN is\ngiven at the end of the paper.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 15:49:23 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Fu", "Rao", ""], ["Chen", "Jie", ""], ["Zeng", "Shutian", ""], ["Zhuang", "Yiping", ""], ["Sudjianto", "Agus", ""]]}, {"id": "1904.11455", "submitter": "Tom Schaul", "authors": "Tom Schaul, Diana Borsa, Joseph Modayil, Razvan Pascanu", "title": "Ray Interference: a Source of Plateaus in Deep Reinforcement Learning", "comments": "Full version of RLDM abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rather than proposing a new method, this paper investigates an issue present\nin existing learning algorithms. We study the learning dynamics of\nreinforcement learning (RL), specifically a characteristic coupling between\nlearning and data generation that arises because RL agents control their future\ndata distribution. In the presence of function approximation, this coupling can\nlead to a problematic type of 'ray interference', characterized by learning\ndynamics that sequentially traverse a number of performance plateaus,\neffectively constraining the agent to learn one thing at a time even when\nlearning in parallel is better. We establish the conditions under which ray\ninterference occurs, show its relation to saddle points and obtain the exact\nlearning dynamics in a restricted setting. We characterize a number of its\nproperties and discuss possible remedies.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 16:54:02 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Schaul", "Tom", ""], ["Borsa", "Diana", ""], ["Modayil", "Joseph", ""], ["Pascanu", "Razvan", ""]]}, {"id": "1904.11532", "submitter": "Yi-Chun Chen", "authors": "Yi-Chun Chen, Velibor V. Mi\\v{s}i\\'c", "title": "Decision Forest: A Nonparametric Approach to Modeling Irrational Choice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Customer behavior is often assumed to follow weak rationality, which implies\nthat adding a product to an assortment will not increase the choice probability\nof another product in that assortment. However, an increasing amount of\nresearch has revealed that customers are not necessarily rational when making\ndecisions. In this paper, we propose a new nonparametric choice model that\nrelaxes this assumption and can model a wider range of customer behavior, such\nas decoy effects between products. In this model, each customer type is\nassociated with a binary decision tree, which represents a decision process for\nmaking a purchase based on checking for the existence of specific products in\nthe assortment. Together with a probability distribution over customer types,\nwe show that the resulting model -- a decision forest -- is able to represent\nany customer choice model, including models that are inconsistent with weak\nrationality. We theoretically characterize the depth of the forest needed to\nfit a data set of historical assortments and prove that with high probability,\na forest whose depth scales logarithmically in the number of assortments is\nsufficient to fit most data sets. We also propose two practical algorithms --\none based on column generation and one based on random sampling -- for\nestimating such models from data. Using synthetic data and real transaction\ndata exhibiting non-rational behavior, we show that the model outperforms both\nrational and non-rational benchmark models in out-of-sample predictive ability.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 18:41:29 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 09:47:18 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Chen", "Yi-Chun", ""], ["Mi\u0161i\u0107", "Velibor V.", ""]]}, {"id": "1904.11546", "submitter": "Mugdim Bublin", "authors": "Mugdim Bublin", "title": "Machine Learning For Distributed Acoustic Sensors, Classic versus Image\n  and Deep Neural Networks Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed Acoustic Sensing (DAS) using fiber optic cables is a promising\nnew technology for pipeline monitoring and protection. In this work, we applied\nand compared two approaches for event detection using DAS: Classic machine\nlearning approach and the approach based on image processing and deep learning.\nAlthough with both approaches acceptable performance can be achieved, the\npreliminary results show that image based deep learning is more promising\napproach, offering six times lower event detection delay and twelve times lower\nexecution time.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 19:18:31 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Bublin", "Mugdim", ""]]}, {"id": "1904.11547", "submitter": "Feiyang Pan", "authors": "Feiyang Pan, Shuokai Li, Xiang Ao, Pingzhong Tang, Qing He", "title": "Warm Up Cold-start Advertisements: Improving CTR Predictions via\n  Learning to Learn ID Embeddings", "comments": "Accepted at SIGIR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click-through rate (CTR) prediction has been one of the most central problems\nin computational advertising. Lately, embedding techniques that produce\nlow-dimensional representations of ad IDs drastically improve CTR prediction\naccuracies. However, such learning techniques are data demanding and work\npoorly on new ads with little logging data, which is known as the cold-start\nproblem.\n  In this paper, we aim to improve CTR predictions during both the cold-start\nphase and the warm-up phase when a new ad is added to the candidate pool. We\npropose Meta-Embedding, a meta-learning-based approach that learns to generate\ndesirable initial embeddings for new ad IDs. The proposed method trains an\nembedding generator for new ad IDs by making use of previously learned ads\nthrough gradient-based meta-learning. In other words, our method learns how to\nlearn better embeddings. When a new ad comes, the trained generator initializes\nthe embedding of its ID by feeding its contents and attributes. Next, the\ngenerated embedding can speed up the model fitting during the warm-up phase\nwhen a few labeled examples are available, compared to the existing\ninitialization methods.\n  Experimental results on three real-world datasets showed that Meta-Embedding\ncan significantly improve both the cold-start and warm-up performances for six\nexisting CTR prediction models, ranging from lightweight models such as\nFactorization Machines to complicated deep models such as PNN and DeepFM. All\nof the above apply to conversion rate (CVR) predictions as well.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 19:26:42 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Pan", "Feiyang", ""], ["Li", "Shuokai", ""], ["Ao", "Xiang", ""], ["Tang", "Pingzhong", ""], ["He", "Qing", ""]]}, {"id": "1904.11576", "submitter": "Zulfiqar Ali Z.ali", "authors": "Zulifqar Ali, Ijaz Hussain, Muhammad Faisal, Hafiza Mamona Nazir,\n  Tajammal Hussain, Muhammad Yousaf Shad, Alaa Mohamd Shoukry, Showkat Hussain\n  Gani", "title": "Forecasting Drought Using Multilayer Perceptron Artificial Neural\n  Network Model", "comments": null, "journal-ref": null, "doi": "10.1155/2017/5681308", "report-no": null, "categories": "physics.ao-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These days human beings are facing many environmental challenges due to\nfrequently occurring drought hazards. It may have an effect on the countrys\nenvironment, the community, and industries. Several adverse impacts of drought\nhazard are continued in Pakistan, including other hazards. However, early\nmeasurement and detection of drought can provide guidance to water resources\nmanagement for employing drought mitigation policies. In this paper, we used a\nmultilayer perceptron neural network (MLPNN) algorithm for drought forecasting.\nWe applied and tested MLPNN algorithm on monthly time series data of\nStandardized Precipitation Evapotranspiration Index (SPEI) for seventeen\nclimatological stations located in Northern Area and KPK (Pakistan). We found\nthat MLPNN has potential capability for SPEI drought forecasting based on\nperformance measures (i.e., Mean Average Error (MAE), the coefficient of\ncorrelation R, and Root Mean Square Error (RMSE). Water resources and\nmanagement planner can take necessary action in advance (e.g., in water\nscarcity areas) by using MLPNN model as part of their decision making.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 08:25:02 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Ali", "Zulifqar", ""], ["Hussain", "Ijaz", ""], ["Faisal", "Muhammad", ""], ["Nazir", "Hafiza Mamona", ""], ["Hussain", "Tajammal", ""], ["Shad", "Muhammad Yousaf", ""], ["Shoukry", "Alaa Mohamd", ""], ["Gani", "Showkat Hussain", ""]]}, {"id": "1904.11577", "submitter": "Bahram Mohammadi", "authors": "Bahram Mohammadi and Mohammad Sabokrou", "title": "End-to-End Adversarial Learning for Intrusion Detection in Computer\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a simple yet efficient method for an anomaly-based\nIntrusion Detection System (IDS). In reality, IDSs can be defined as a\none-class classification system, where the normal traffic is the target class.\nThe high diversity of network attacks in addition to the need for\ngeneralization, motivate us to propose a semi-supervised method. Inspired by\nthe successes of Generative Adversarial Networks (GANs) for training deep\nmodels in semi-unsupervised setting, we have proposed an end-to-end deep\narchitecture for IDS. The proposed architecture is composed of two deep\nnetworks, each of which trained by competing with each other to understand the\nunderlying concept of the normal traffic class. The key idea of this paper is\nto compensate the lack of anomalous traffic by approximately obtain them from\nnormal flows. In this case, our method is not biased towards the available\nintrusions in the training set leading to more accurate detection. The proposed\nmethod has been evaluated on NSL-KDD dataset. The results confirm that our\nmethod outperforms the other state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 20:40:05 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Mohammadi", "Bahram", ""], ["Sabokrou", "Mohammad", ""]]}, {"id": "1904.11608", "submitter": "Yao Ma", "authors": "Yao Ma, Alex Olshevsky, Venkatesh Saligrama, Csaba Szepesvari", "title": "Gradient Descent for Sparse Rank-One Matrix Completion for Crowd-Sourced\n  Aggregation of Sparsely Interacting Workers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider worker skill estimation for the single-coin Dawid-Skene\ncrowdsourcing model. In practice, skill-estimation is challenging because\nworker assignments are sparse and irregular due to the arbitrary and\nuncontrolled availability of workers. We formulate skill estimation as a\nrank-one correlation-matrix completion problem, where the observed components\ncorrespond to observed label correlations between workers. We show that the\ncorrelation matrix can be successfully recovered and skills are identifiable if\nand only if the sampling matrix (observed components) does not have a bipartite\nconnected component. We then propose a projected gradient descent scheme and\nshow that skill estimates converge to the desired global optima for such\nsampling matrices. Our proof is original and the results are surprising in\nlight of the fact that even the weighted rank-one matrix factorization problem\nis NP-hard in general. Next, we derive sample complexity bounds in terms of\nspectral properties of the signless Laplacian of the sampling matrix. Our\nproposed scheme achieves state-of-art performance on a number of real-world\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 22:09:58 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 10:01:19 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Ma", "Yao", ""], ["Olshevsky", "Alex", ""], ["Saligrama", "Venkatesh", ""], ["Szepesvari", "Csaba", ""]]}, {"id": "1904.11637", "submitter": "Christopher McCord", "authors": "Dimitris Bertsimas and Christopher McCord", "title": "From Predictions to Prescriptions in Multistage Optimization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a framework for solving finite-horizon multistage\noptimization problems under uncertainty in the presence of auxiliary data. We\nassume the joint distribution of the uncertain quantities is unknown, but noisy\nobservations, along with observations of auxiliary covariates, are available.\nWe utilize effective predictive methods from machine learning (ML), including\n$k$-nearest neighbors regression ($k$NN), classification and regression trees\n(CART), and random forests (RF), to develop specific methods that are\napplicable to a wide variety of problems. We demonstrate that our solution\nmethods are asymptotically optimal under mild conditions. Additionally, we\nestablish finite sample guarantees for the optimality of our method with $k$NN\nweight functions. Finally, we demonstrate the practicality of our approach with\ncomputational examples. We see a significant decrease in cost by taking into\naccount the auxiliary data in the multistage setting.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 01:21:20 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["McCord", "Christopher", ""]]}, {"id": "1904.11643", "submitter": "Toan Minh Tran Mr", "authors": "Toan Tran, Thanh-Toan Do, Ian Reid, Gustavo Carneiro", "title": "Bayesian Generative Active Deep Learning", "comments": "Accepted to ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models have demonstrated outstanding performance in several\nproblems, but their training process tends to require immense amounts of\ncomputational and human resources for training and labeling, constraining the\ntypes of problems that can be tackled. Therefore, the design of effective\ntraining methods that require small labeled training sets is an important\nresearch direction that will allow a more effective use of resources.Among\ncurrent approaches designed to address this issue, two are particularly\ninteresting: data augmentation and active learning. Data augmentation achieves\nthis goal by artificially generating new training points, while active learning\nrelies on the selection of the \"most informative\" subset of unlabeled training\nsamples to be labelled by an oracle. Although successful in practice, data\naugmentation can waste computational resources because it indiscriminately\ngenerates samples that are not guaranteed to be informative, and active\nlearning selects a small subset of informative samples (from a large\nun-annotated set) that may be insufficient for the training process. In this\npaper, we propose a Bayesian generative active deep learning approach that\ncombines active learning with data augmentation -- we provide theoretical and\nempirical evidence (MNIST, CIFAR-$\\{10,100\\}$, and SVHN) that our approach has\nmore efficient training and better classification results than data\naugmentation and active learning.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 01:55:04 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Tran", "Toan", ""], ["Do", "Thanh-Toan", ""], ["Reid", "Ian", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "1904.11648", "submitter": "Chong Ma", "authors": "Chong Ma, Wenxuan Deng, Shuangge Ma, Ray Liu, Kevin Galinsky", "title": "Structural modeling using overlapped group penalties for discovering\n  predictive biomarkers for subgroup analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of predictive biomarkers from a large scale of covariates\nfor subgroup analysis has attracted fundamental attention in medical research.\nIn this article, we propose a generalized penalized regression method with a\nnovel penalty function, for enforcing the hierarchy structure between the\nprognostic and predictive effects, such that a nonzero predictive effect must\ninduce its ancestor prognostic effects being nonzero in the model. Our method\nis able to select useful predictive biomarkers by yielding a sparse,\ninterpretable, and predictable model for subgroup analysis, and can deal with\ndifferent types of response variable such as continuous, categorical, and\ntime-to-event data. We show that our method is asymptotically consistent under\nsome regularized conditions. To minimize the generalized penalized regression\nmodel, we propose a novel integrative optimization algorithm by integrating the\nmajorization-minimization and the alternating direction method of multipliers,\nwhich is named after \\texttt{smog}. The enriched simulation study and real case\nstudy demonstrate that our method is very powerful for discovering the true\npredictive biomarkers and identifying subgroups of patients.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 02:06:26 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Ma", "Chong", ""], ["Deng", "Wenxuan", ""], ["Ma", "Shuangge", ""], ["Liu", "Ray", ""], ["Galinsky", "Kevin", ""]]}, {"id": "1904.11649", "submitter": "Alessandro Lameiras Koerich", "authors": "Alexandre Reeberg Mello, Jonathan de Matos, Marcelo R. Stemmer, Alceu\n  de Souza Britto Jr., Alessandro Lameiras Koerich", "title": "A Novel Orthogonal Direction Mesh Adaptive Direct Search Approach for\n  SVM Hyperparameter Tuning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the use of a black-box optimization method called\ndeterministic Mesh Adaptive Direct Search (MADS) algorithm with orthogonal\ndirections (Ortho-MADS) for the selection of hyperparameters of Support Vector\nMachines with a Gaussian kernel. Different from most of the methods in the\nliterature that exploit the properties of the data or attempt to minimize the\naccuracy of a validation dataset over the first quadrant of (C, gamma), the\nOrtho-MADS provides convergence proof. We present the MADS, followed by the\nOrtho-MADS, the dynamic stopping criterion defined by the MADS mesh size and\ntwo different search strategies (Nelder-Mead and Variable Neighborhood Search)\nthat contribute to a competitive convergence rate as well as a mechanism to\nescape from undesired local minima. We have investigated the practical\nselection of hyperparameters for the Support Vector Machine with a Gaussian\nkernel, i.e., properly choose the hyperparameters gamma (bandwidth) and C\n(trade-off) on several benchmark datasets. The experimental results have shown\nthat the proposed approach for hyperparameter tuning consistently finds\ncomparable or better solutions, when using a common configuration, than other\nmethods. We have also evaluated the accuracy and the number of function\nevaluations of the Ortho-MADS with the Nelder-Mead search strategy and the\nVariable Neighborhood Search strategy using the mesh size as a stopping\ncriterion, and we have achieved accuracy that no other method for\nhyperparameters optimization could reach.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 02:09:35 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Mello", "Alexandre Reeberg", ""], ["de Matos", "Jonathan", ""], ["Stemmer", "Marcelo R.", ""], ["Britto", "Alceu de Souza", "Jr."], ["Koerich", "Alessandro Lameiras", ""]]}, {"id": "1904.11652", "submitter": "Bum Chul Kwon", "authors": "Bum Chul Kwon, Vibha Anand, Kristen A Severson, Soumya Ghosh, Zhaonan\n  Sun, Brigitte I Frohnert, Markus Lundgren, Kenney Ng", "title": "DPVis: Visual Analytics with Hidden Markov Models for Disease\n  Progression Pathways", "comments": "to appear at IEEE Transactions on Visualization and Computer Graphics", "journal-ref": null, "doi": "10.1109/TVCG.2020.2985689", "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical researchers use disease progression models to understand patient\nstatus and characterize progression patterns from longitudinal health records.\nOne approach for disease progression modeling is to describe patient status\nusing a small number of states that represent distinctive distributions over a\nset of observed measures. Hidden Markov models (HMMs) and its variants are a\nclass of models that both discover these states and make inferences of health\nstates for patients. Despite the advantages of using the algorithms for\ndiscovering interesting patterns, it still remains challenging for medical\nexperts to interpret model outputs, understand complex modeling parameters, and\nclinically make sense of the patterns. To tackle these problems, we conducted a\ndesign study with clinical scientists, statisticians, and visualization\nexperts, with the goal to investigate disease progression pathways of chronic\ndiseases, namely type 1 diabetes (T1D), Huntington's disease, Parkinson's\ndisease, and chronic obstructive pulmonary disease (COPD). As a result, we\nintroduce DPVis which seamlessly integrates model parameters and outcomes of\nHMMs into interpretable and interactive visualizations. In this study, we\ndemonstrate that DPVis is successful in evaluating disease progression models,\nvisually summarizing disease states, interactively exploring disease\nprogression patterns, and building, analyzing, and comparing clinically\nrelevant patient subgroups.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 02:30:32 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 16:26:43 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Kwon", "Bum Chul", ""], ["Anand", "Vibha", ""], ["Severson", "Kristen A", ""], ["Ghosh", "Soumya", ""], ["Sun", "Zhaonan", ""], ["Frohnert", "Brigitte I", ""], ["Lundgren", "Markus", ""], ["Ng", "Kenney", ""]]}, {"id": "1904.11681", "submitter": "Lijun Zhang", "authors": "Lijun Zhang, Tie-Yan Liu, Zhi-Hua Zhou", "title": "Adaptive Regret of Convex and Smooth Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate online convex optimization in changing environments, and\nchoose the adaptive regret as the performance measure. The goal is to achieve a\nsmall regret over every interval so that the comparator is allowed to change\nover time. Different from previous works that only utilize the convexity\ncondition, this paper further exploits smoothness to improve the adaptive\nregret. To this end, we develop novel adaptive algorithms for convex and smooth\nfunctions, and establish problem-dependent regret bounds over any interval. Our\nregret bounds are comparable to existing results in the worst case, and become\nmuch tighter when the comparator has a small loss.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 06:01:55 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 07:31:06 GMT"}, {"version": "v3", "created": "Sat, 15 Jun 2019 07:38:05 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Zhang", "Lijun", ""], ["Liu", "Tie-Yan", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1904.11682", "submitter": "Yongqi Zhang", "authors": "Yongqi Zhang and Quanming Yao and Wenyuan Dai and Lei Chen", "title": "AutoSF: Searching Scoring Functions for Knowledge Graph Embedding", "comments": "accepted by ICDE 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scoring functions (SFs), which measure the plausibility of triplets in\nknowledge graph (KG), have become the crux of KG embedding. Lots of SFs, which\ntarget at capturing different kinds of relations in KGs, have been designed by\nhumans in recent years. However, as relations can exhibit complex patterns that\nare hard to infer before training, none of them can consistently perform better\nthan others on existing benchmark data sets. In this paper, inspired by the\nrecent success of automated machine learning (AutoML), we propose to\nautomatically design SFs (AutoSF) for distinct KGs by the AutoML techniques.\nHowever, it is non-trivial to explore domain-specific information here to make\nAutoSF efficient and effective. We firstly identify a unified representation\nover popularly used SFs, which helps to set up a search space for AutoSF. Then,\nwe propose a greedy algorithm to search in such a space efficiently. The\nalgorithm is further sped up by a filter and a predictor, which can avoid\nrepeatedly training SFs with same expressive ability and help removing bad\ncandidates during the search before model training. Finally, we perform\nextensive experiments on benchmark data sets. Results on link prediction and\ntriplets classification show that the searched SFs by AutoSF, are KG dependent,\nnew to the literature, and outperform the state-of-the-art SFs designed by\nhumans.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 06:04:10 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 17:21:46 GMT"}, {"version": "v3", "created": "Fri, 28 Feb 2020 09:45:17 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Zhang", "Yongqi", ""], ["Yao", "Quanming", ""], ["Dai", "Wenyuan", ""], ["Chen", "Lei", ""]]}, {"id": "1904.11694", "submitter": "Jiayuan Mao", "authors": "Honghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, Denny Zhou", "title": "Neural Logic Machines", "comments": "ICLR 2019. Project page:\n  https://sites.google.com/view/neural-logic-machines", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for\nboth inductive learning and logic reasoning. NLMs exploit the power of both\nneural networks---as function approximators, and logic programming---as a\nsymbolic processor for objects with properties, relations, logic connectives,\nand quantifiers. After being trained on small-scale tasks (such as sorting\nshort arrays), NLMs can recover lifted rules, and generalize to large-scale\ntasks (such as sorting longer arrays). In our experiments, NLMs achieve perfect\ngeneralization in a number of tasks, from relational reasoning tasks on the\nfamily tree and general graphs, to decision making tasks including sorting\narrays, finding shortest paths, and playing the blocks world. Most of these\ntasks are hard to accomplish for neural networks or inductive logic programming\nalone.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 06:52:53 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Dong", "Honghua", ""], ["Mao", "Jiayuan", ""], ["Lin", "Tian", ""], ["Wang", "Chong", ""], ["Li", "Lihong", ""], ["Zhou", "Denny", ""]]}, {"id": "1904.11707", "submitter": "Henri Gerard", "authors": "Emilie Chouzenoux, Henri G\\'erard, Jean-Christophe Pesquet", "title": "General risk measures for robust machine learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide array of machine learning problems are formulated as the minimization\nof the expectation of a convex loss function on some parameter space. Since the\nprobability distribution of the data of interest is usually unknown, it is is\noften estimated from training sets, which may lead to poor out-of-sample\nperformance. In this work, we bring new insights in this problem by using the\nframework which has been developed in quantitative finance for risk measures.\nWe show that the original min-max problem can be recast as a convex\nminimization problem under suitable assumptions. We discuss several important\nexamples of robust formulations, in particular by defining ambiguity sets based\non $\\varphi$-divergences and the Wasserstein metric.We also propose an\nefficient algorithm for solving the corresponding convex optimization problems\ninvolving complex convex constraints. Through simulation examples, we\ndemonstrate that this algorithm scales well on real data sets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 08:08:31 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 10:52:09 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Chouzenoux", "Emilie", ""], ["G\u00e9rard", "Henri", ""], ["Pesquet", "Jean-Christophe", ""]]}, {"id": "1904.11711", "submitter": "Davood Zabihzadeh", "authors": "Sumia Abdulhussien Razooqi Al-Obaidi, Davood Zabihzadeh, and Hamideh\n  Hajiabadi", "title": "Robust Metric Learning based on the Rescaled Hinge Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance/Similarity learning is a fundamental problem in machine learning.\nFor example, kNN classifier or clustering methods are based on a\ndistance/similarity measure. Metric learning algorithms enhance the efficiency\nof these methods by learning an optimal distance function from data. Most\nmetric learning methods need training information in the form of pair or\ntriplet sets. Nowadays, this training information often is obtained from the\nInternet via crowdsourcing methods. Therefore, this information may contain\nlabel noise or outliers leading to the poor performance of the learned metric.\nIt is even possible that the learned metric functions perform worse than the\ngeneral metrics such as Euclidean distance. To address this challenge, this\npaper presents a new robust metric learning method based on the Rescaled Hinge\nloss. This loss function is a general case of the popular Hinge loss and\ninitially introduced in (Xu et al. 2017) to develop a new robust SVM algorithm.\nIn this paper, we formulate the metric learning problem using the Rescaled\nHinge loss function and then develop an efficient algorithm based on HQ\n(Half-Quadratic) to solve the problem. Experimental results on a variety of\nboth real and synthetic datasets confirm that our new robust algorithm\nconsiderably outperforms state-of-the-art metric learning methods in the\npresence of label noise and outliers.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 08:20:59 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 14:36:54 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Al-Obaidi", "Sumia Abdulhussien Razooqi", ""], ["Zabihzadeh", "Davood", ""], ["Hajiabadi", "Hamideh", ""]]}, {"id": "1904.11717", "submitter": "Takuya Shimada", "authors": "Takuya Shimada, Han Bao, Issei Sato, Masashi Sugiyama", "title": "Classification from Pairwise Similarities/Dissimilarities and Unlabeled\n  Data via Empirical Risk Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pairwise similarities and dissimilarities between data points might be easier\nto obtain than fully labeled data in real-world classification problems, e.g.,\nin privacy-aware situations. To handle such pairwise information, an empirical\nrisk minimization approach has been proposed, giving an unbiased estimator of\nthe classification risk that can be computed only from pairwise similarities\nand unlabeled data. However, this direction cannot handle pairwise\ndissimilarities so far. On the other hand, semi-supervised clustering is one of\nthe methods which can use both similarities and dissimilarities. Nevertheless,\nthey typically require strong geometrical assumptions on the data distribution\nsuch as the manifold assumption, which may deteriorate the performance. In this\npaper, we derive an unbiased risk estimator which can handle all of\nsimilarities/dissimilarities and unlabeled data. We theoretically establish\nestimation error bounds and experimentally demonstrate the practical usefulness\nof our empirical risk minimization method.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 08:43:53 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Shimada", "Takuya", ""], ["Bao", "Han", ""], ["Sato", "Issei", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1904.11738", "submitter": "Chun Kit Yeung", "authors": "Chun-Kit Yeung", "title": "Deep-IRT: Make Deep Learning Based Knowledge Tracing Explainable Using\n  Item Response Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based knowledge tracing model has been shown to outperform\ntraditional knowledge tracing model without the need for human-engineered\nfeatures, yet its parameters and representations have long been criticized for\nnot being explainable. In this paper, we propose Deep-IRT which is a synthesis\nof the item response theory (IRT) model and a knowledge tracing model that is\nbased on the deep neural network architecture called dynamic key-value memory\nnetwork (DKVMN) to make deep learning based knowledge tracing explainable.\nSpecifically, we use the DKVMN model to process the student's learning\ntrajectory and estimate the student ability level and the item difficulty level\nover time. Then, we use the IRT model to estimate the probability that a\nstudent will answer an item correctly using the estimated student ability and\nthe item difficulty. Experiments show that the Deep-IRT model retains the\nperformance of the DKVMN model, while it provides a direct psychological\ninterpretation of both students and items.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 09:38:37 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Yeung", "Chun-Kit", ""]]}, {"id": "1904.11761", "submitter": "Robert Pinsler", "authors": "Robert Pinsler, Peter Karkus, Andras Kupcsik, David Hsu, Wee Sun Lee", "title": "Factored Contextual Policy Search with Bayesian Optimization", "comments": "To appear in ICRA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scarce data is a major challenge to scaling robot learning to truly complex\ntasks, as we need to generalize locally learned policies over different task\ncontexts. Contextual policy search offers data-efficient learning and\ngeneralization by explicitly conditioning the policy on a parametric context\nspace. In this paper, we further structure the contextual policy\nrepresentation. We propose to factor contexts into two components: target\ncontexts that describe the task objectives, e.g. target position for throwing a\nball; and environment contexts that characterize the environment, e.g. initial\nposition or mass of the ball. Our key observation is that experience can be\ndirectly generalized over target contexts. We show that this can be easily\nexploited in contextual policy search algorithms. In particular, we apply\nfactorization to a Bayesian optimization approach to contextual policy search\nboth in sampling-based and active learning settings. Our simulation results\nshow faster learning and better generalization in various robotic domains. See\nour supplementary video: https://youtu.be/MNTbBAOufDY.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 11:04:26 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Pinsler", "Robert", ""], ["Karkus", "Peter", ""], ["Kupcsik", "Andras", ""], ["Hsu", "David", ""], ["Lee", "Wee Sun", ""]]}, {"id": "1904.11798", "submitter": "Sara Morsy", "authors": "Sara Morsy and George Karypis", "title": "Will this Course Increase or Decrease Your GPA? Towards Grade-aware\n  Course Recommendation", "comments": "Under revision for Journal of Educational Data Mining (JEDM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to help undergraduate students towards successfully completing their\ndegrees, developing tools that can assist students during the course selection\nprocess is a significant task in the education domain. The optimal set of\ncourses for each student should include courses that help him/her graduate in a\ntimely fashion and for which he/she is well-prepared for so as to get a good\ngrade in. To this end, we propose two different grade-aware course\nrecommendation approaches to recommend to each student his/her optimal set of\ncourses. The first approach ranks the courses by using an objective function\nthat differentiates between courses that are expected to increase or decrease a\nstudent's GPA. The second approach combines the grades predicted by grade\nprediction methods with the rankings produced by course recommendation methods\nto improve the final course rankings. To obtain the course rankings in the\nfirst approach, we adapt two widely-used representation learning techniques to\nlearn the optimal temporal ordering between courses. Our experiments on a large\ndataset obtained from the University of Minnesota that includes students from\n23 different majors show that the grade-aware course recommendation methods can\ndo better on recommending more courses in which the students are expected to\nperform well and recommending fewer courses in which they are expected not to\nperform well in than grade-unaware course recommendation methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 21:27:42 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Morsy", "Sara", ""], ["Karypis", "George", ""]]}, {"id": "1904.11799", "submitter": "Mohit Sharma", "authors": "Mohit Sharma, Jiayu Zhou, Junling Hu, George Karypis", "title": "Feature-based factorized Bilinear Similarity Model for Cold-Start Top-n\n  Item Recommendation", "comments": "9 pages, Proceedings of the 2015 SIAM International Conference on\n  Data Mining", "journal-ref": null, "doi": "10.1137/1.9781611974010.22", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommending new items to existing users has remained a challenging problem\ndue to absence of user's past preferences for these items. The user\npersonalized non-collaborative methods based on item features can be used to\naddress this item cold-start problem. These methods rely on similarities\nbetween the target item and user's previous preferred items. While computing\nsimilarities based on item features, these methods overlook the interactions\namong the features of the items and consider them independently. Modeling\ninteractions among features can be helpful as some features, when considered\ntogether, provide a stronger signal on the relevance of an item when compared\nto case where features are considered independently. To address this important\nissue, in this work we introduce the Feature-based factorized Bilinear\nSimilarity Model (FBSM), which learns factorized bilinear similarity model for\nTOP-n recommendation of new items, given the information about items preferred\nby users in past as well as the features of these items. We carry out extensive\nempirical evaluations on benchmark datasets, and we find that the proposed FBSM\napproach improves upon traditional non-collaborative methods in terms of\nrecommendation performance. Moreover, the proposed approach also learns\ninsightful interactions among item features from data, which lead to deep\nunderstanding on how these interactions contribute to personalized\nrecommendation.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 05:10:48 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Sharma", "Mohit", ""], ["Zhou", "Jiayu", ""], ["Hu", "Junling", ""], ["Karypis", "George", ""]]}, {"id": "1904.11800", "submitter": "Mohit Sharma", "authors": "Mohit Sharma, and George Karypis", "title": "Adaptive Matrix Completion for the Users and the Items in Tail", "comments": "7 pages, 3 figures, ACM WWW'19", "journal-ref": null, "doi": "10.1145/3308558.3313736", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are widely used to recommend the most appealing items to\nusers. These recommendations can be generated by applying collaborative\nfiltering methods. The low-rank matrix completion method is the\nstate-of-the-art collaborative filtering method. In this work, we show that the\nskewed distribution of ratings in the user-item rating matrix of real-world\ndatasets affects the accuracy of matrix-completion-based approaches. Also, we\nshow that the number of ratings that an item or a user has positively\ncorrelates with the ability of low-rank matrix-completion-based approaches to\npredict the ratings for the item or the user accurately. Furthermore, we use\nthese insights to develop four matrix completion-based approaches, i.e.,\nFrequency Adaptive Rating Prediction (FARP), Truncated Matrix Factorization\n(TMF), Truncated Matrix Factorization with Dropout (TMF + Dropout) and Inverse\nFrequency Weighted Matrix Factorization (IFWMF), that outperforms traditional\nmatrix-completion-based approaches for the users and the items with few ratings\nin the user-item rating matrix.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 04:55:10 GMT"}, {"version": "v2", "created": "Sun, 5 Jan 2020 00:58:20 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Sharma", "Mohit", ""], ["Karypis", "George", ""]]}, {"id": "1904.11803", "submitter": "Francesco Ranzato", "authors": "Francesco Ranzato and Marco Zanella", "title": "Robustness Verification of Support Vector Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of formally verifying the robustness to adversarial\nexamples of support vector machines (SVMs), a major machine learning model for\nclassification and regression tasks. Following a recent stream of works on\nformal robustness verification of (deep) neural networks, our approach relies\non a sound abstract version of a given SVM classifier to be used for checking\nits robustness. This methodology is parametric on a given numerical abstraction\nof real values and, analogously to the case of neural networks, needs neither\nabstract least upper bounds nor widening operators on this abstraction. The\nstandard interval domain provides a simple instantiation of our abstraction\ntechnique, which is enhanced with the domain of reduced affine forms, which is\nan efficient abstraction of the zonotope abstract domain. This robustness\nverification technique has been fully implemented and experimentally evaluated\non SVMs based on linear and nonlinear (polynomial and radial basis function)\nkernels, which have been trained on the popular MNIST dataset of images and on\nthe recent and more challenging Fashion-MNIST dataset. The experimental results\nof our prototype SVM robustness verifier appear to be encouraging: this\nautomated verification is fast, scalable and shows significantly high\npercentages of provable robustness on the test set of MNIST, in particular\ncompared to the analogous provable robustness of neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 12:38:11 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Ranzato", "Francesco", ""], ["Zanella", "Marco", ""]]}, {"id": "1904.11829", "submitter": "Leila Arras", "authors": "Leila Arras, Ahmed Osman, Klaus-Robert M\\\"uller, Wojciech Samek", "title": "Evaluating Recurrent Neural Network Explanations", "comments": "14 pages, accepted for ACL'19 Workshop BlackboxNLP: Analyzing and\n  Interpreting Neural Networks for NLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several methods have been proposed to explain the predictions of\nrecurrent neural networks (RNNs), in particular of LSTMs. The goal of these\nmethods is to understand the network's decisions by assigning to each input\nvariable, e.g., a word, a relevance indicating to which extent it contributed\nto a particular prediction. In previous works, some of these methods were not\nyet compared to one another, or were evaluated only qualitatively. We close\nthis gap by systematically and quantitatively comparing these methods in\ndifferent settings, namely (1) a toy arithmetic task which we use as a sanity\ncheck, (2) a five-class sentiment prediction of movie reviews, and besides (3)\nwe explore the usefulness of word relevances to build sentence-level\nrepresentations. Lastly, using the method that performed best in our\nexperiments, we show how specific linguistic phenomena such as the negation in\nsentiment analysis reflect in terms of relevance patterns, and how the\nrelevance visualization can help to understand the misclassification of\nindividual samples.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 13:08:43 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 07:49:19 GMT"}, {"version": "v3", "created": "Tue, 4 Jun 2019 13:52:09 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Arras", "Leila", ""], ["Osman", "Ahmed", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Samek", "Wojciech", ""]]}, {"id": "1904.11830", "submitter": "Chunguang Li", "authors": "Xiaokun Pu, Chunguang Li", "title": "Online Learning Algorithms for Quaternion ARMA Model", "comments": "17 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of adaptive learning for autoregressive\nmoving average (ARMA) model in the quaternion domain. By transforming the\noriginal learning problem into a full information optimization task without\nexplicit noise terms, and then solving the optimization problem using the\ngradient descent and the Newton analogues, we obtain two online learning\nalgorithms for the quaternion ARMA. Furthermore, regret bound analysis\naccounting for the specific properties of quaternion algebra is presented,\nwhich proves that the performance of the online algorithms asymptotically\napproaches that of the best quaternion ARMA model in hindsight.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 13:10:14 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Pu", "Xiaokun", ""], ["Li", "Chunguang", ""]]}, {"id": "1904.11834", "submitter": "Artur Souza", "authors": "Artur Souza, Leonardo B. Oliveira, Sabine Hollatz, Matt Feldman, Kunle\n  Olukotun, James M. Holton, Aina E. Cohen, Luigi Nardi", "title": "DeepFreak: Learning Crystallography Diffraction Patterns with Automated\n  Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serial crystallography is the field of science that studies the structure and\nproperties of crystals via diffraction patterns. In this paper, we introduce a\nnew serial crystallography dataset comprised of real and synthetic images; the\nsynthetic images are generated through the use of a simulator that is both\nscalable and accurate. The resulting dataset is called DiffraNet, and it is\ncomposed of 25,457 512x512 grayscale labeled images. We explore several\ncomputer vision approaches for classification on DiffraNet such as standard\nfeature extraction algorithms associated with Random Forests and Support Vector\nMachines but also an end-to-end CNN topology dubbed DeepFreak tailored to work\non this new dataset. All implementations are publicly available and have been\nfine-tuned using off-the-shelf AutoML optimization tools for a fair comparison.\nOur best model achieves 98.5% accuracy on synthetic images and 94.51% accuracy\non real images. We believe that the DiffraNet dataset and its classification\nmethods will have in the long term a positive impact in accelerating\ndiscoveries in many disciplines, including chemistry, geology, biology,\nmaterials science, metallurgy, and physics.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 13:12:40 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 15:11:32 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Souza", "Artur", ""], ["Oliveira", "Leonardo B.", ""], ["Hollatz", "Sabine", ""], ["Feldman", "Matt", ""], ["Olukotun", "Kunle", ""], ["Holton", "James M.", ""], ["Cohen", "Aina E.", ""], ["Nardi", "Luigi", ""]]}, {"id": "1904.11838", "submitter": "Marco Roberti", "authors": "Marco Roberti, Giovanni Bonetta, Rossella Cancelliere, Patrick\n  Gallinari", "title": "Copy mechanism and tailored training for character-based data-to-text\n  generation", "comments": "ECML-PKDD 2019 (Camera ready version)", "journal-ref": null, "doi": "10.1007/978-3-030-46147-8_39", "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, many different methods have been focusing on using\ndeep recurrent neural networks for natural language generation. The most widely\nused sequence-to-sequence neural methods are word-based: as such, they need a\npre-processing step called delexicalization (conversely, relexicalization) to\ndeal with uncommon or unknown words. These forms of processing, however, give\nrise to models that depend on the vocabulary used and are not completely\nneural.\n  In this work, we present an end-to-end sequence-to-sequence model with\nattention mechanism which reads and generates at a character level, no longer\nrequiring delexicalization, tokenization, nor even lowercasing. Moreover, since\ncharacters constitute the common \"building blocks\" of every text, it also\nallows a more general approach to text generation, enabling the possibility to\nexploit transfer learning for training. These skills are obtained thanks to two\nmajor features: (i) the possibility to alternate between the standard\ngeneration mechanism and a copy one, which allows to directly copy input facts\nto produce outputs, and (ii) the use of an original training pipeline that\nfurther improves the quality of the generated texts.\n  We also introduce a new dataset called E2E+, designed to highlight the\ncopying capabilities of character-based models, that is a modified version of\nthe well-known E2E dataset used in the E2E Challenge. We tested our model\naccording to five broadly accepted metrics (including the widely used BLEU),\nshowing that it yields competitive performance with respect to both\ncharacter-based and word-based approaches.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 13:33:56 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2019 11:38:07 GMT"}, {"version": "v3", "created": "Tue, 2 Jul 2019 15:35:14 GMT"}, {"version": "v4", "created": "Mon, 11 May 2020 12:48:10 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Roberti", "Marco", ""], ["Bonetta", "Giovanni", ""], ["Cancelliere", "Rossella", ""], ["Gallinari", "Patrick", ""]]}, {"id": "1904.11857", "submitter": "Manie Tadayon", "authors": "Manie Tadayon, Greg Pottie", "title": "Predicting Student Performance in an Educational Game Using a Hidden\n  Markov Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contributions: Prior studies on education have mostly followed the model of\nthe cross sectional study, namely, examining the pretest and the posttest\nscores. This paper shows that students' knowledge throughout the intervention\ncan be estimated by time series analysis using a hidden Markov model.\nBackground: Analyzing time series and the interaction between the students and\nthe game data can result in valuable information that cannot be gained by only\ncross sectional studies of the exams. Research Questions: Can a hidden Markov\nmodel be used to analyze the educational games? Can a hidden Markov model be\nused to make a prediction of the students' performance? Methodology: The study\nwas conducted on (N=854) students who played the Save Patch game. Students were\ndivided into class 1 and class 2. Class 1 students are those who scored lower\nin the test than class 2 students. The analysis is done by choosing various\nfeatures of the game as the observations. Findings: The state trajectories can\npredict the students' performance accurately for both class 1 and class 2.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 06:55:50 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Tadayon", "Manie", ""], ["Pottie", "Greg", ""]]}, {"id": "1904.11858", "submitter": "Sara Morsy", "authors": "Sara Morsy and George Karypis", "title": "Sparse Neural Attentive Knowledge-based Models for Grade Prediction", "comments": "accepted for publication in EDM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grade prediction for future courses not yet taken by students is important as\nit can help them and their advisers during the process of course selection as\nwell as for designing personalized degree plans and modifying them based on\ntheir performance. One of the successful approaches for accurately predicting a\nstudent's grades in future courses is Cumulative Knowledge-based Regression\nModels (CKRM). CKRM learns shallow linear models that predict a student's\ngrades as the similarity between his/her knowledge state and the target course.\nA student's knowledge state is built by linearly accumulating the learned\nprovided knowledge components of the courses he/she has taken in the past,\nweighted by his/her grades in them. However, not all the prior courses\ncontribute equally to the target course. In this paper, we propose a novel\nNeural Attentive Knowledge-based model (NAK) that learns the importance of each\nhistorical course in predicting the grade of a target course. Compared to CKRM\nand other competing approaches, our experiments on a large real-world dataset\nconsisting of $\\sim$1.5 grades show the effectiveness of the proposed NAK model\nin accurately predicting the students' grades. Moreover, the attention weights\nlearned by the model can be helpful in better designing their degree plans.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 21:16:17 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Morsy", "Sara", ""], ["Karypis", "George", ""]]}, {"id": "1904.11874", "submitter": "Silvija Kokalj-Filipovic", "authors": "Silvija Kokalj-Filipovic, Rob Miller, Joshua Morman", "title": "AutoEncoders for Training Compact Deep Learning RF Classifiers for\n  Wireless Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that compact fully connected (FC) deep learning networks trained to\nclassify wireless protocols using a hierarchy of multiple denoising\nautoencoders (AEs) outperform reference FC networks trained in a typical way,\ni.e., with a stochastic gradient based optimization of a given FC architecture.\nNot only is the complexity of such FC network, measured in number of trainable\nparameters and scalar multiplications, much lower than the reference FC and\nresidual models, its accuracy also outperforms both models for nearly all\ntested SNR values (0 dB to 50dB). Such AE-trained networks are suited for\nin-situ protocol inference performed by simple mobile devices based on noisy\nsignal measurements. Training is based on the data transmitted by real devices,\nand collected in a controlled environment, and systematically augmented by a\npolicy-based data synthesis process by adding to the signal any subset of\nimpairments commonly seen in a wireless receiver.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 00:13:01 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Kokalj-Filipovic", "Silvija", ""], ["Miller", "Rob", ""], ["Morman", "Joshua", ""]]}, {"id": "1904.11875", "submitter": "Ellen Vitercik", "authors": "Daniel Alabi, Adam Tauman Kalai, Katrina Ligett, Cameron Musco,\n  Christos Tzamos, and Ellen Vitercik", "title": "Learning to Prune: Speeding up Repeated Computations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common to encounter situations where one must solve a sequence of\nsimilar computational problems. Running a standard algorithm with worst-case\nruntime guarantees on each instance will fail to take advantage of valuable\nstructure shared across the problem instances. For example, when a commuter\ndrives from work to home, there are typically only a handful of routes that\nwill ever be the shortest path. A naive algorithm that does not exploit this\ncommon structure may spend most of its time checking roads that will never be\nin the shortest path. More generally, we can often ignore large swaths of the\nsearch space that will likely never contain an optimal solution.\n  We present an algorithm that learns to maximally prune the search space on\nrepeated computations, thereby reducing runtime while provably outputting the\ncorrect solution each period with high probability. Our algorithm employs a\nsimple explore-exploit technique resembling those used in online algorithms,\nthough our setting is quite different. We prove that, with respect to our model\nof pruning search spaces, our approach is optimal up to constant factors.\nFinally, we illustrate the applicability of our model and algorithm to three\nclassic problems: shortest-path routing, string search, and linear programming.\nWe present experiments confirming that our simple algorithm is effective at\nsignificantly reducing the runtime of solving repeated computations.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 14:52:03 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Alabi", "Daniel", ""], ["Kalai", "Adam Tauman", ""], ["Ligett", "Katrina", ""], ["Musco", "Cameron", ""], ["Tzamos", "Christos", ""], ["Vitercik", "Ellen", ""]]}, {"id": "1904.11876", "submitter": "Jakub Tomczak", "authors": "Jakub M. Tomczak and Romain Lepert and Auke Wiggers", "title": "Simulating Execution Time of Tensor Programs using Graph Neural Networks", "comments": "All authors contributed equally. Accepted as a workshop paper at\n  Representation Learning on Graphs and Manifolds @ ICLR 2019. Fixed values in\n  Table 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing the execution time of tensor program, e.g., a convolution,\ninvolves finding its optimal configuration. Searching the configuration space\nexhaustively is typically infeasible in practice. In line with recent research\nusing TVM, we propose to learn a surrogate model to overcome this issue. The\nmodel is trained on an acyclic graph called an abstract syntax tree, and\nutilizes a graph convolutional network to exploit structure in the graph. We\nclaim that a learnable graph-based data processing is a strong competitor to\nheuristic-based feature extraction. We present a new dataset of graphs\ncorresponding to configurations and their execution time for various tensor\nprograms. We provide baselines for a runtime prediction task.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 14:53:30 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2019 11:49:49 GMT"}, {"version": "v3", "created": "Wed, 27 Nov 2019 14:38:08 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Tomczak", "Jakub M.", ""], ["Lepert", "Romain", ""], ["Wiggers", "Auke", ""]]}, {"id": "1904.11914", "submitter": "Saeedreza Shehnepoor", "authors": "Mohammad Adiban, Bagher BabaAli, Saeedreza Shehnepoor", "title": "Statistical feature embedding for heart sound classification", "comments": null, "journal-ref": "Journal of Electrical Engineering, 70(4), 259-272 (2019)", "doi": "10.2478/jee-2019-0056", "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiovascular Disease (CVD) is considered as one of the principal causes of\ndeath in the world. Over recent years, this field of study has attracted\nresearchers' attention to investigate heart sounds' patterns for disease\ndiagnostics. In this study, an approach is proposed for normal/abnormal heart\nsound classification on the Physionet challenge 2016 dataset. For the first\ntime, a fixed-length feature vector; called i-vector; is extracted from each\nheart sound using Mel Frequency Cepstral Coefficient (MFCC) features.\nAfterwards, Principal Component Analysis (PCA) transform and Variational\nAutoencoder (VAE) are applied on the i-vector to achieve dimension reduction.\nEventually, the reduced size vector is fed to Gaussian Mixture Models (GMMs)\nand Support Vector Machine (SVM) for classification purpose. Experimental\nresults demonstrate the proposed method could achieve a performance improvement\nof 16% based on Modified Accuracy (MAcc) compared with the baseline system on\nthe Physoinet dataset.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 16:07:18 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 10:31:29 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2020 06:43:56 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Adiban", "Mohammad", ""], ["BabaAli", "Bagher", ""], ["Shehnepoor", "Saeedreza", ""]]}, {"id": "1904.11930", "submitter": "Michael Schaub", "authors": "Michael T. Schaub and Santiago Segarra and Hoi-To Wai", "title": "Spectral partitioning of time-varying networks with unobserved edges", "comments": "5 pages, 2 figures", "journal-ref": "2019 IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP 2019)", "doi": "10.1109/ICASSP.2019.8682815", "report-no": null, "categories": "cs.SI cs.SY physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a variant of `blind' community detection, in which we aim to\npartition an unobserved network from the observation of a (dynamical) graph\nsignal defined on the network. We consider a scenario where our observed graph\nsignals are obtained by filtering white noise input, and the underlying network\nis different for every observation. In this fashion, the filtered graph signals\ncan be interpreted as defined on a time-varying network. We model each of the\nunderlying network realizations as generated by an independent draw from a\nlatent stochastic blockmodel (SBM). To infer the partition of the latent SBM,\nwe propose a simple spectral algorithm for which we provide a theoretical\nanalysis and establish consistency guarantees for the recovery. We illustrate\nour results using numerical experiments on synthetic and real data,\nhighlighting the efficacy of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 16:57:52 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Schaub", "Michael T.", ""], ["Segarra", "Santiago", ""], ["Wai", "Hoi-To", ""]]}, {"id": "1904.11943", "submitter": "Guandao Yang", "authors": "Guandao Yang, Tianyi Zhang, Polina Kirichenko, Junwen Bai, Andrew\n  Gordon Wilson, Christopher De Sa", "title": "SWALP : Stochastic Weight Averaging in Low-Precision Training", "comments": "Published at ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low precision operations can provide scalability, memory savings,\nportability, and energy efficiency. This paper proposes SWALP, an approach to\nlow precision training that averages low-precision SGD iterates with a modified\nlearning rate schedule. SWALP is easy to implement and can match the\nperformance of full-precision SGD even with all numbers quantized down to 8\nbits, including the gradient accumulators. Additionally, we show that SWALP\nconverges arbitrarily close to the optimal solution for quadratic objectives,\nand to a noise ball asymptotically smaller than low precision SGD in strongly\nconvex settings.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 17:22:06 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 16:00:08 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Yang", "Guandao", ""], ["Zhang", "Tianyi", ""], ["Kirichenko", "Polina", ""], ["Bai", "Junwen", ""], ["Wilson", "Andrew Gordon", ""], ["De Sa", "Christopher", ""]]}, {"id": "1904.11949", "submitter": "Nunzio Alexandro Letizia Mr", "authors": "Andrea M. Tonello, Nunzio A. Letizia, Davide Righini and Francesco\n  Marcuzzi", "title": "Machine Learning Tips and Tricks for Power Line Communications", "comments": "Accepted for publication in IEEE Access. 19 pages, 15 figures, 142\n  references. Added Sec. II-C", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A great deal of attention has been recently given to Machine Learning (ML)\ntechniques in many different application fields. This paper provides a vision\nof what ML can do in Power Line Communications (PLC). We firstly and briefly\ndescribe classical formulations of ML, and distinguish deterministic from\nstatistical learning models with relevance to communications. We then discuss\nML applications in PLC for each layer, namely, for characterization and\nmodeling, for the development of physical layer algorithms, for media access\ncontrol and networking. Finally, other applications of PLC that can benefit\nfrom the usage of ML, as grid diagnostics, are analyzed. Illustrative numerical\nexamples are reported to serve the purpose of validating the ideas and motivate\nfuture research endeavors in this stimulating signal/data processing field.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 15:07:31 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 07:53:51 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Tonello", "Andrea M.", ""], ["Letizia", "Nunzio A.", ""], ["Righini", "Davide", ""], ["Marcuzzi", "Francesco", ""]]}, {"id": "1904.11955", "submitter": "Simon Du", "authors": "Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov,\n  Ruosong Wang", "title": "On Exact Computation with an Infinitely Wide Neural Net", "comments": "In NeurIPS 2019. Code available: https://github.com/ruosongwang/cntk", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How well does a classic deep net architecture like AlexNet or VGG19 classify\non a standard dataset such as CIFAR-10 when its width --- namely, number of\nchannels in convolutional layers, and number of nodes in fully-connected\ninternal layers --- is allowed to increase to infinity? Such questions have\ncome to the forefront in the quest to theoretically understand deep learning\nand its mysteries about optimization and generalization. They also connect deep\nlearning to notions such as Gaussian processes and kernels. A recent paper\n[Jacot et al., 2018] introduced the Neural Tangent Kernel (NTK) which captures\nthe behavior of fully-connected deep nets in the infinite width limit trained\nby gradient descent; this object was implicit in some other recent papers. An\nattraction of such ideas is that a pure kernel-based method is used to capture\nthe power of a fully-trained deep net of infinite width.\n  The current paper gives the first efficient exact algorithm for computing the\nextension of NTK to convolutional neural nets, which we call Convolutional NTK\n(CNTK), as well as an efficient GPU implementation of this algorithm. This\nresults in a significant new benchmark for the performance of a pure\nkernel-based method on CIFAR-10, being $10\\%$ higher than the methods reported\nin [Novak et al., 2019], and only $6\\%$ lower than the performance of the\ncorresponding finite deep net architecture (once batch normalization, etc. are\nturned off). Theoretically, we also give the first non-asymptotic proof showing\nthat a fully-trained sufficiently wide net is indeed equivalent to the kernel\nregression predictor using NTK.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 17:29:37 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 15:10:47 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Arora", "Sanjeev", ""], ["Du", "Simon S.", ""], ["Hu", "Wei", ""], ["Li", "Zhiyuan", ""], ["Salakhutdinov", "Ruslan", ""], ["Wang", "Ruosong", ""]]}, {"id": "1904.11968", "submitter": "David Wehr", "authors": "David Wehr, Halley Fede, Eleanor Pence, Bo Zhang, Guilherme Ferreira,\n  John Walczyk, Joseph Hughes", "title": "Learning Semantic Vector Representations of Source Code via a Siamese\n  Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abundance of open-source code, coupled with the success of recent\nadvances in deep learning for natural language processing, has given rise to a\npromising new application of machine learning to source code. In this work, we\nexplore the use of a Siamese recurrent neural network model on Python source\ncode to create vectors which capture the semantics of code. We evaluate the\nquality of embeddings by identifying which problem from a programming\ncompetition the code solves. Our model significantly outperforms a\nbag-of-tokens embedding, providing promising results for improving code\nembeddings that can be used in future software engineering tasks.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 17:52:06 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Wehr", "David", ""], ["Fede", "Halley", ""], ["Pence", "Eleanor", ""], ["Zhang", "Bo", ""], ["Ferreira", "Guilherme", ""], ["Walczyk", "John", ""], ["Hughes", "Joseph", ""]]}, {"id": "1904.12004", "submitter": "Chenglong Wang", "authors": "Chenglong Wang, Rudy Bunel, Krishnamurthy Dvijotham, Po-Sen Huang,\n  Edward Grefenstette, Pushmeet Kohli", "title": "Knowing When to Stop: Evaluation and Verification of Conformity to\n  Output-size Specifications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models such as Sequence-to-Sequence and Image-to-Sequence are widely used in\nreal world applications. While the ability of these neural architectures to\nproduce variable-length outputs makes them extremely effective for problems\nlike Machine Translation and Image Captioning, it also leaves them vulnerable\nto failures of the form where the model produces outputs of undesirable length.\nThis behavior can have severe consequences such as usage of increased\ncomputation and induce faults in downstream modules that expect outputs of a\ncertain length. Motivated by the need to have a better understanding of the\nfailures of these models, this paper proposes and studies the novel output-size\nmodulation problem and makes two key technical contributions. First, to\nevaluate model robustness, we develop an easy-to-compute differentiable proxy\nobjective that can be used with gradient-based algorithms to find\noutput-lengthening inputs. Second and more importantly, we develop a\nverification approach that can formally verify whether a network always\nproduces outputs within a certain length. Experimental results on Machine\nTranslation and Image Captioning show that our output-lengthening approach can\nproduce outputs that are 50 times longer than the input, while our verification\napproach can, given a model and input domain, prove that the output length is\nbelow a certain size.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 18:12:56 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Wang", "Chenglong", ""], ["Bunel", "Rudy", ""], ["Dvijotham", "Krishnamurthy", ""], ["Huang", "Po-Sen", ""], ["Grefenstette", "Edward", ""], ["Kohli", "Pushmeet", ""]]}, {"id": "1904.12017", "submitter": "Jonathan Tuck", "authors": "Jonathan Tuck, Shane Barratt, Stephen Boyd", "title": "A Distributed Method for Fitting Laplacian Regularized Stratified Models", "comments": "37 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stratified models are models that depend in an arbitrary way on a set of\nselected categorical features, and depend linearly on the other features. In a\nbasic and traditional formulation a separate model is fit for each value of the\ncategorical feature, using only the data that has the specific categorical\nvalue. To this formulation we add Laplacian regularization, which encourages\nthe model parameters for neighboring categorical values to be similar.\nLaplacian regularization allows us to specify one or more weighted graphs on\nthe stratification feature values. For example, stratifying over the days of\nthe week, we can specify that the Sunday model parameter should be close to the\nSaturday and Monday model parameters. The regularization improves the\nperformance of the model over the traditional stratified model, since the model\nfor each value of the categorical `borrows strength' from its neighbors. In\nparticular, it produces a model even for categorical values that did not appear\nin the training data set.\n  We propose an efficient distributed method for fitting stratified models,\nbased on the alternating direction method of multipliers (ADMM). When the\nfitting loss functions are convex, the stratified model fitting problem is\nconvex, and our method computes the global minimizer of the loss plus\nregularization; in other cases it computes a local minimizer. The method is\nvery efficient, and naturally scales to large data sets or numbers of\nstratified feature values. We illustrate our method with a variety of examples.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 19:05:59 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 22:39:16 GMT"}, {"version": "v3", "created": "Sun, 10 Nov 2019 20:22:18 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Tuck", "Jonathan", ""], ["Barratt", "Shane", ""], ["Boyd", "Stephen", ""]]}, {"id": "1904.12043", "submitter": "Haibin Lin", "authors": "Haibin Lin, Hang Zhang, Yifei Ma, Tong He, Zhi Zhang, Sheng Zha, Mu Li", "title": "Dynamic Mini-batch SGD for Elastic Distributed Training: Learning in the\n  Limbo of Resources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With an increasing demand for training powers for deep learning algorithms\nand the rapid growth of computation resources in data centers, it is desirable\nto dynamically schedule different distributed deep learning tasks to maximize\nresource utilization and reduce cost. In this process, different tasks may\nreceive varying numbers of machines at different time, a setting we call\nelastic distributed training. Despite the recent successes in large mini-batch\ndistributed training, these methods are rarely tested in elastic distributed\ntraining environments and suffer degraded performance in our experiments, when\nwe adjust the learning rate linearly immediately with respect to the batch\nsize. One difficulty we observe is that the noise in the stochastic momentum\nestimation is accumulated over time and will have delayed effects when the\nbatch size changes. We therefore propose to smoothly adjust the learning rate\nover time to alleviate the influence of the noisy momentum estimation. Our\nexperiments on image classification, object detection and semantic segmentation\nhave demonstrated that our proposed Dynamic SGD method achieves stabilized\nperformance when varying the number of GPUs from 8 to 128. We also provide\ntheoretical understanding on the optimality of linear learning rate scheduling\nand the effects of stochastic momentum.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 20:45:28 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 06:48:24 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Lin", "Haibin", ""], ["Zhang", "Hang", ""], ["Ma", "Yifei", ""], ["He", "Tong", ""], ["Zhang", "Zhi", ""], ["Zha", "Sheng", ""], ["Li", "Mu", ""]]}, {"id": "1904.12052", "submitter": "Hengtong Zhang", "authors": "Hengtong Zhang, Tianhang Zheng, Jing Gao, Chenglin Miao, Lu Su,\n  Yaliang Li, Kui Ren", "title": "Data Poisoning Attack against Knowledge Graph Embedding", "comments": "Fix typos and version conflicts", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph embedding (KGE) is a technique for learning continuous\nembeddings for entities and relations in the knowledge graph.Due to its benefit\nto a variety of downstream tasks such as knowledge graph completion, question\nanswering and recommendation, KGE has gained significant attention recently.\nDespite its effectiveness in a benign environment, KGE' robustness to\nadversarial attacks is not well-studied. Existing attack methods on graph data\ncannot be directly applied to attack the embeddings of knowledge graph due to\nits heterogeneity. To fill this gap, we propose a collection of data poisoning\nattack strategies, which can effectively manipulate the plausibility of\narbitrary targeted facts in a knowledge graph by adding or deleting facts on\nthe graph. The effectiveness and efficiency of the proposed attack strategies\nare verified by extensive evaluations on two widely-used benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 21:12:19 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2019 04:06:02 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Zhang", "Hengtong", ""], ["Zheng", "Tianhang", ""], ["Gao", "Jing", ""], ["Miao", "Chenglin", ""], ["Su", "Lu", ""], ["Li", "Yaliang", ""], ["Ren", "Kui", ""]]}, {"id": "1904.12053", "submitter": "Shivam Garg", "authors": "Brian Axelrod, Shivam Garg, Vatsal Sharan, Gregory Valiant", "title": "Sample Amplification: Increasing Dataset Size even when Learning is\n  Impossible", "comments": "Added discussion about potential applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given data drawn from an unknown distribution, $D$, to what extent is it\npossible to ``amplify'' this dataset and output an even larger set of samples\nthat appear to have been drawn from $D$? We formalize this question as follows:\nan $(n,m)$ $\\text{amplification procedure}$ takes as input $n$ independent\ndraws from an unknown distribution $D$, and outputs a set of $m > n$\n``samples''. An amplification procedure is valid if no algorithm can\ndistinguish the set of $m$ samples produced by the amplifier from a set of $m$\nindependent draws from $D$, with probability greater than $2/3$. Perhaps\nsurprisingly, in many settings, a valid amplification procedure exists, even\nwhen the size of the input dataset, $n$, is significantly less than what would\nbe necessary to learn $D$ to non-trivial accuracy. Specifically we consider two\nfundamental settings: the case where $D$ is an arbitrary discrete distribution\nsupported on $\\le k$ elements, and the case where $D$ is a $d$-dimensional\nGaussian with unknown mean, and fixed covariance. In the first case, we show\nthat an $\\left(n, n + \\Theta(\\frac{n}{\\sqrt{k}})\\right)$ amplifier exists. In\nparticular, given $n=O(\\sqrt{k})$ samples from $D$, one can output a set of\n$m=n+1$ datapoints, whose total variation distance from the distribution of $m$\ni.i.d. draws from $D$ is a small constant, despite the fact that one would need\nquadratically more data, $n=\\Theta(k)$, to learn $D$ up to small constant total\nvariation distance. In the Gaussian case, we show that an\n$\\left(n,n+\\Theta(\\frac{n}{\\sqrt{d}} )\\right)$ amplifier exists, even though\nlearning the distribution to small constant total variation distance requires\n$\\Theta(d)$ samples. In both the discrete and Gaussian settings, we show that\nthese results are tight, to constant factors. Beyond these results, we\nformalize a number of curious directions for future research along this vein.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 21:42:44 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 01:40:28 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Axelrod", "Brian", ""], ["Garg", "Shivam", ""], ["Sharan", "Vatsal", ""], ["Valiant", "Gregory", ""]]}, {"id": "1904.12054", "submitter": "Marc Z\\\"oller", "authors": "Marc-Andr\\'e Z\\\"oller and Marco F. Huber", "title": "Benchmark and Survey of Automated Machine Learning Frameworks", "comments": "Revised version accepted for publication at Journal of Artificial\n  Intelligence Research (JAIR)", "journal-ref": "Journal of Artificial Intelligence Research 70 (2021) 409-472", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) has become a vital part in many aspects of our daily\nlife. However, building well performing machine learning applications requires\nhighly specialized data scientists and domain experts. Automated machine\nlearning (AutoML) aims to reduce the demand for data scientists by enabling\ndomain experts to build machine learning applications automatically without\nextensive knowledge of statistics and machine learning. This paper is a\ncombination of a survey on current AutoML methods and a benchmark of popular\nAutoML frameworks on real data sets. Driven by the selected frameworks for\nevaluation, we summarize and review important AutoML techniques and methods\nconcerning every step in building an ML pipeline. The selected AutoML\nframeworks are evaluated on 137 data sets from established AutoML benchmark\nsuits.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 21:42:56 GMT"}, {"version": "v2", "created": "Wed, 8 Jan 2020 11:19:24 GMT"}, {"version": "v3", "created": "Sun, 30 Aug 2020 09:49:10 GMT"}, {"version": "v4", "created": "Thu, 14 Jan 2021 15:09:26 GMT"}, {"version": "v5", "created": "Tue, 26 Jan 2021 15:52:33 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Z\u00f6ller", "Marc-Andr\u00e9", ""], ["Huber", "Marco F.", ""]]}, {"id": "1904.12058", "submitter": "Muhan Zhang", "authors": "Muhan Zhang, Yixin Chen", "title": "Inductive Matrix Completion Based on Graph Neural Networks", "comments": "Accepted as a spotlight presentation at ICLR-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an inductive matrix completion model without using side\ninformation. By factorizing the (rating) matrix into the product of\nlow-dimensional latent embeddings of rows (users) and columns (items), a\nmajority of existing matrix completion methods are transductive, since the\nlearned embeddings cannot generalize to unseen rows/columns or to new matrices.\nTo make matrix completion inductive, most previous works use content (side\ninformation), such as user's age or movie's genre, to make predictions.\nHowever, high-quality content is not always available, and can be hard to\nextract. Under the extreme setting where not any side information is available\nother than the matrix to complete, can we still learn an inductive matrix\ncompletion model? In this paper, we propose an Inductive Graph-based Matrix\nCompletion (IGMC) model to address this problem. IGMC trains a graph neural\nnetwork (GNN) based purely on 1-hop subgraphs around (user, item) pairs\ngenerated from the rating matrix and maps these subgraphs to their\ncorresponding ratings. It achieves highly competitive performance with\nstate-of-the-art transductive baselines. In addition, IGMC is inductive -- it\ncan generalize to users/items unseen during the training (given that their\ninteractions exist), and can even transfer to new tasks. Our transfer learning\nexperiments show that a model trained out of the MovieLens dataset can be\ndirectly used to predict Douban movie ratings with surprisingly good\nperformance. Our work demonstrates that: 1) it is possible to train inductive\nmatrix completion models without using side information while achieving similar\nor better performances than state-of-the-art transductive methods; 2) local\ngraph patterns around a (user, item) pair are effective predictors of the\nrating this user gives to the item; and 3) Long-range dependencies might not be\nnecessary for modeling recommender systems.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 21:58:46 GMT"}, {"version": "v2", "created": "Sun, 6 Oct 2019 03:08:54 GMT"}, {"version": "v3", "created": "Sun, 16 Feb 2020 04:27:14 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Zhang", "Muhan", ""], ["Chen", "Yixin", ""]]}, {"id": "1904.12064", "submitter": "Adam Sykulski Dr", "authors": "Jeffrey J. Early and Adam M. Sykulski", "title": "Smoothing and Interpolating Noisy GPS Data with Smoothing Splines", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": "10.1175/JTECH-D-19-0087.1", "report-no": null, "categories": "stat.ME physics.data-an stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A comprehensive methodology is provided for smoothing noisy, irregularly\nsampled data with non-Gaussian noise using smoothing splines. We demonstrate\nhow the spline order and tension parameter can be chosen a priori from physical\nreasoning. We also show how to allow for non-Gaussian noise and outliers which\nare typical in GPS signals. We demonstrate the effectiveness of our methods on\nGPS trajectory data obtained from oceanographic floating instruments known as\ndrifters.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 22:28:58 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 23:14:02 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Early", "Jeffrey J.", ""], ["Sykulski", "Adam M.", ""]]}, {"id": "1904.12083", "submitter": "Bo Dai", "authors": "Bo Dai, Zhen Liu, Hanjun Dai, Niao He, Arthur Gretton, Le Song, Dale\n  Schuurmans", "title": "Exponential Family Estimation via Adversarial Dynamics Embedding", "comments": "Appearing in NeurIPS 2019 Vancouver, Canada; a preliminary version\n  published in NeurIPS2018 Bayesian Deep Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient algorithm for maximum likelihood estimation (MLE) of\nexponential family models, with a general parametrization of the energy\nfunction that includes neural networks. We exploit the primal-dual view of the\nMLE with a kinetics augmented model to obtain an estimate associated with an\nadversarial dual sampler. To represent this sampler, we introduce a novel\nneural architecture, dynamics embedding, that generalizes Hamiltonian\nMonte-Carlo (HMC). The proposed approach inherits the flexibility of HMC while\nenabling tractable entropy estimation for the augmented model. By learning both\na dual sampler and the primal model simultaneously, and sharing parameters\nbetween them, we obviate the requirement to design a separate sampling\nprocedure once the model has been trained, leading to more effective learning.\nWe show that many existing estimators, such as contrastive divergence,\npseudo/composite-likelihood, score matching, minimum Stein discrepancy\nestimator, non-local contrastive objectives, noise-contrastive estimation, and\nminimum probability flow, are special cases of the proposed approach, each\nexpressed by a different (fixed) dual sampler. An empirical investigation shows\nthat adapting the sampler during MLE can significantly improve on\nstate-of-the-art estimators.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 01:20:21 GMT"}, {"version": "v2", "created": "Sun, 1 Dec 2019 06:36:27 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2020 20:20:43 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Dai", "Bo", ""], ["Liu", "Zhen", ""], ["Dai", "Hanjun", ""], ["He", "Niao", ""], ["Gretton", "Arthur", ""], ["Song", "Le", ""], ["Schuurmans", "Dale", ""]]}, {"id": "1904.12088", "submitter": "Xin Wang", "authors": "Xin Wang, Shinji Takaki, Junichi Yamagishi", "title": "Neural source-filter waveform models for statistical parametric speech\n  synthesis", "comments": "Accepted to IEEE/ACM TASLP. Note: this paper is on a follow-up work\n  of our ICASSP paper. Based on the h-NSF introduced in this work, we proposed\n  a h-sinc-NSF model and published the third paper in SSW 10\n  (https://www.isca-speech.org/archive/SSW_2019/pdfs/SSW10_O_1-1.pdf)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural waveform models such as WaveNet have demonstrated better performance\nthan conventional vocoders for statistical parametric speech synthesis. As an\nautoregressive (AR) model, WaveNet is limited by a slow sequential waveform\ngeneration process. Some new models that use the inverse-autoregressive flow\n(IAF) can generate a whole waveform in a one-shot manner. However, these\nIAF-based models require sequential transformation during training, which\nseverely slows down the training speed. Other models such as Parallel WaveNet\nand ClariNet bring together the benefits of AR and IAF-based models and train\nan IAF model by transferring the knowledge from a pre-trained AR teacher to an\nIAF student without any sequential transformation. However, both models require\nadditional training criteria, and their implementation is prohibitively\ncomplicated.\n  We propose a framework for neural source-filter (NSF) waveform modeling\nwithout AR nor IAF-based approaches. This framework requires only three\ncomponents for waveform generation: a source module that generates a sine-based\nsignal as excitation, a non-AR dilated-convolution-based filter module that\ntransforms the excitation into a waveform, and a conditional module that\npre-processes the acoustic features for the source and filer modules. This\nframework minimizes spectral-amplitude distances for model training, which can\nbe efficiently implemented by using short-time Fourier transform routines.\nUnder this framework, we designed three NSF models and compared them with\nWaveNet. It was demonstrated that the NSF models generated waveforms at least\n100 times faster than WaveNet, and the quality of the synthetic speech from the\nbest NSF model was better than or equally good as that from WaveNet.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 02:08:20 GMT"}, {"version": "v2", "created": "Sun, 17 Nov 2019 13:02:29 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Wang", "Xin", ""], ["Takaki", "Shinji", ""], ["Yamagishi", "Junichi", ""]]}, {"id": "1904.12118", "submitter": "Gopi Sanghani Dr.", "authors": "Gopi Sanghani, Ketan Kotecha", "title": "Incremental personalized E-mail spam filter using novel TFDCR feature\n  selection with dynamic feature update", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication through e-mails remains to be highly formalized, conventional\nand indispensable method for the exchange of information over the Internet. An\never-increasing ratio and adversary nature of spam e-mails have posed a great\nmany challenges such as uneven class distribution, unequal error cost, frequent\nchange of content and personalized context-sensitive discrimination. In this\nresearch, we propose a novel and distinctive approach to develop an incremental\npersonalized e-mail spam filter. The proposed work is described using three\nsignificant contributions. First, we applied a novel term frequency difference\nand category ratio based feature selection function TFDCR to select the most\ndiscriminating features irrespective of the number of samples in each class.\nSecond, an incremental learning model is used which enables the classifier to\nupdate the discriminant function dynamically. Third, a heuristic function\ncalled selectionRankWeight is introduced to upgrade the existing feature set\nthat determines new features carrying strong discriminating ability from an\nincoming set of e-mails. Three public e-mail datasets possessing different\ncharacteristics are used to evaluate the filter performance. Experiments are\nconducted to compare the feature selection efficiency of TFDCR and to observe\nthe filter performance under both the batch and the incremental learning mode.\nThe results demonstrate the superiority of TFDCR as the most effective f eature\nselection function. The incremental learning model incorporating dynamic\nfeature update function overcomes the problem of drifting concepts. The\nproposed filter validates its efficiency and feasibility by substantially\nimproving the classification accuracy and reducing the false positive error of\nmisclassifying legitimate e-mail as spam.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 07:00:18 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Sanghani", "Gopi", ""], ["Kotecha", "Ketan", ""]]}, {"id": "1904.12171", "submitter": "Zhi-Hua Zhou", "authors": "Bo-Jian Hou and Lijun Zhang and Zhi-Hua Zhou", "title": "Prediction with Unpredictable Feature Evolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning with feature evolution studies the scenario where the features of\nthe data streams can evolve, i.e., old features vanish and new features emerge.\nIts goal is to keep the model always performing well even when the features\nhappen to evolve. To tackle this problem, canonical methods assume that the old\nfeatures will vanish simultaneously and the new features themselves will emerge\nsimultaneously as well. They also assume there is an overlapping period where\nold and new features both exist when the feature space starts to change.\nHowever, in reality, the feature evolution could be unpredictable, which means\nthe features can vanish or emerge arbitrarily, causing the overlapping period\nincomplete. In this paper, we propose a novel paradigm: Prediction with\nUnpredictable Feature Evolution (PUFE) where the feature evolution is\nunpredictable. To address this problem, we fill the incomplete overlapping\nperiod and formulate it as a new matrix completion problem. We give a\ntheoretical bound on the least number of observed entries to make the\noverlapping period intact. With this intact overlapping period, we leverage an\nensemble method to take the advantage of both the old and new feature spaces\nwithout manually deciding which base models should be incorporated. Theoretical\nand experimental results validate that our method can always follow the best\nbase models and thus realize the goal of learning with feature evolution.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 16:08:24 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 08:24:40 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Hou", "Bo-Jian", ""], ["Zhang", "Lijun", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1904.12200", "submitter": "Anmol Sharma", "authors": "Anmol Sharma, Ghassan Hamarneh", "title": "Missing MRI Pulse Sequence Synthesis using Multi-Modal Generative\n  Adversarial Network", "comments": "Accepted for publication in IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance imaging (MRI) is being increasingly utilized to assess,\ndiagnose, and plan treatment for a variety of diseases. The ability to\nvisualize tissue in varied contrasts in the form of MR pulse sequences in a\nsingle scan provides valuable insights to physicians, as well as enabling\nautomated systems performing downstream analysis. However many issues like\nprohibitive scan time, image corruption, different acquisition protocols, or\nallergies to certain contrast materials may hinder the process of acquiring\nmultiple sequences for a patient. This poses challenges to both physicians and\nautomated systems since complementary information provided by the missing\nsequences is lost. In this paper, we propose a variant of generative\nadversarial network (GAN) capable of leveraging redundant information contained\nwithin multiple available sequences in order to generate one or more missing\nsequences for a patient scan. The proposed network is designed as a\nmulti-input, multi-output network which combines information from all the\navailable pulse sequences, implicitly infers which sequences are missing, and\nsynthesizes the missing ones in a single forward pass. We demonstrate and\nvalidate our method on two brain MRI datasets each with four sequences, and\nshow the applicability of the proposed method in simultaneously synthesizing\nall missing sequences in any possible scenario where either one, two, or three\nof the four sequences may be missing. We compare our approach with competing\nunimodal and multi-modal methods, and show that we outperform both\nquantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 20:15:15 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2019 19:08:43 GMT"}, {"version": "v3", "created": "Wed, 2 Oct 2019 00:20:13 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Sharma", "Anmol", ""], ["Hamarneh", "Ghassan", ""]]}, {"id": "1904.12206", "submitter": "Mohammad Taha Bahadori", "authors": "Mohammad Taha Bahadori, Zachary Chase Lipton", "title": "Temporal-Clustering Invariance in Irregular Healthcare Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Electronic records contain sequences of events, some of which take place all\nat once in a single visit, and others that are dispersed over multiple visits,\neach with a different timestamp. We postulate that fine temporal detail, e.g.,\nwhether a series of blood tests are completed at once or in rapid succession\nshould not alter predictions based on this data. Motivated by this intuition,\nwe propose models for analyzing sequences of multivariate clinical time series\ndata that are invariant to this temporal clustering. We propose an efficient\ndata augmentation technique that exploits the postulated temporal-clustering\ninvariance to regularize deep neural networks optimized for several clinical\nprediction tasks. We introduce two techniques to temporally coarsen\n(downsample) irregular time series: (i) grouping the data points based on\nregularly-spaced timestamps; and (ii) clustering them, yielding\nirregularly-paced timestamps. Moreover, we propose a MultiResolution Ensemble\n(MRE) model, improving predictive accuracy by ensembling predictions based on\ninputs sequences transformed by different coarsening operators. Our experiments\nshow that MRE improves the mAP on the benchmark mortality prediction task from\n51.53% to 53.92%.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 20:30:26 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Bahadori", "Mohammad Taha", ""], ["Lipton", "Zachary Chase", ""]]}, {"id": "1904.12218", "submitter": "Giannis Nikolentzos", "authors": "Giannis Nikolentzos and Giannis Siglidis and Michalis Vazirgiannis", "title": "Graph Kernels: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph kernels have attracted a lot of attention during the last decade, and\nhave evolved into a rapidly developing branch of learning on structured data.\nDuring the past 20 years, the considerable research activity that occurred in\nthe field resulted in the development of dozens of graph kernels, each focusing\non specific structural properties of graphs. Graph kernels have proven\nsuccessful in a wide range of domains, ranging from social networks to\nbioinformatics. The goal of this survey is to provide a unifying view of the\nliterature on graph kernels. In particular, we present a comprehensive overview\nof a wide range of graph kernels. Furthermore, we perform an experimental\nevaluation of several of those kernels on publicly available datasets, and\nprovide a comparative study. Finally, we discuss key applications of graph\nkernels, and outline some challenges that remain to be addressed.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 22:20:40 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Nikolentzos", "Giannis", ""], ["Siglidis", "Giannis", ""], ["Vazirgiannis", "Michalis", ""]]}, {"id": "1904.12220", "submitter": "Sachin Vernekar", "authors": "Sachin Vernekar, Ashish Gaurav, Taylor Denouden, Buu Phan, Vahdat\n  Abdelzad, Rick Salay, Krzysztof Czarnecki", "title": "Analysis of Confident-Classifiers for Out-of-distribution Detection", "comments": "SafeML 2019 ICLR workshop paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminatively trained neural classifiers can be trusted, only when the\ninput data comes from the training distribution (in-distribution). Therefore,\ndetecting out-of-distribution (OOD) samples is very important to avoid\nclassification errors. In the context of OOD detection for image\nclassification, one of the recent approaches proposes training a classifier\ncalled \"confident-classifier\" by minimizing the standard cross-entropy loss on\nin-distribution samples and minimizing the KL divergence between the predictive\ndistribution of OOD samples in the low-density regions of in-distribution and\nthe uniform distribution (maximizing the entropy of the outputs). Thus, the\nsamples could be detected as OOD if they have low confidence or high entropy.\nIn this paper, we analyze this setting both theoretically and experimentally.\nWe conclude that the resulting confident-classifier still yields arbitrarily\nhigh confidence for OOD samples far away from the in-distribution. We instead\nsuggest training a classifier by adding an explicit \"reject\" class for OOD\nsamples.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 22:33:34 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Vernekar", "Sachin", ""], ["Gaurav", "Ashish", ""], ["Denouden", "Taylor", ""], ["Phan", "Buu", ""], ["Abdelzad", "Vahdat", ""], ["Salay", "Rick", ""], ["Czarnecki", "Krzysztof", ""]]}, {"id": "1904.12222", "submitter": "Hema Venkata Krishna Giri Narra", "authors": "Krishna Giri Narra, Zhifeng Lin, Ganesh Ananthanarayanan, Salman\n  Avestimehr, Murali Annavaram", "title": "Collage Inference: Using Coded Redundancy for Low Variance Distributed\n  Image Classification", "comments": "10 pages, Under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MLaaS (ML-as-a-Service) offerings by cloud computing platforms are becoming\nincreasingly popular. Hosting pre-trained machine learning models in the cloud\nenables elastic scalability as the demand grows. But providing low latency and\nreducing the latency variance is a key requirement. Variance is harder to\ncontrol in a cloud deployment due to uncertainties in resource allocations\nacross many virtual instances. We propose the collage inference technique which\nuses a novel convolutional neural network model, collage-cnn, to provide\nlow-cost redundancy. A collage-cnn model takes a collage image formed by\ncombining multiple images and performs multi-image classification in one shot,\nalbeit at slightly lower accuracy. We augment a collection of traditional\nsingle image classifier models with a single collage-cnn classifier which acts\nas their low-cost redundant backup. Collage-cnn provides backup classification\nresults if any single image classification requests experience slowdown.\nDeploying the collage-cnn models in the cloud, we demonstrate that the 99th\npercentile tail latency of inference can be reduced by 1.2x to 2x compared to\nreplication based approaches while providing high accuracy. Variation in\ninference latency can be reduced by 1.8x to 15x.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 22:56:10 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 17:25:42 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Narra", "Krishna Giri", ""], ["Lin", "Zhifeng", ""], ["Ananthanarayanan", "Ganesh", ""], ["Avestimehr", "Salman", ""], ["Annavaram", "Murali", ""]]}, {"id": "1904.12225", "submitter": "Oh-Hyun Kwon", "authors": "Oh-Hyun Kwon and Kwan-Liu Ma", "title": "A Deep Generative Model for Graph Layout", "comments": "To appear in IEEE Transactions on Visualization and Computer\n  Graphics. In Proc. IEEE VIS 2019 (InfoVis)", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934396", "report-no": null, "categories": "cs.SI cs.GR cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different layouts can characterize different aspects of the same graph.\nFinding a \"good\" layout of a graph is thus an important task for graph\nvisualization. In practice, users often visualize a graph in multiple layouts\nby using different methods and varying parameter settings until they find a\nlayout that best suits the purpose of the visualization. However, this\ntrial-and-error process is often haphazard and time-consuming. To provide users\nwith an intuitive way to navigate the layout design space, we present a\ntechnique to systematically visualize a graph in diverse layouts using deep\ngenerative models. We design an encoder-decoder architecture to learn a model\nfrom a collection of example layouts, where the encoder represents training\nexamples in a latent space and the decoder produces layouts from the latent\nspace. In particular, we train the model to construct a two-dimensional latent\nspace for users to easily explore and generate various layouts. We demonstrate\nour approach through quantitative and qualitative evaluations of the generated\nlayouts. The results of our evaluations show that our model is capable of\nlearning and generalizing abstract concepts of graph layouts, not just\nmemorizing the training examples. In summary, this paper presents a\nfundamentally new approach to graph visualization where a machine learning\nmodel learns to visualize a graph from examples without manually-defined\nheuristics.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 23:19:49 GMT"}, {"version": "v2", "created": "Sat, 13 Jul 2019 23:44:11 GMT"}, {"version": "v3", "created": "Sat, 27 Jul 2019 21:45:55 GMT"}, {"version": "v4", "created": "Tue, 30 Jul 2019 02:57:51 GMT"}, {"version": "v5", "created": "Thu, 29 Aug 2019 00:33:59 GMT"}, {"version": "v6", "created": "Mon, 2 Sep 2019 07:04:25 GMT"}, {"version": "v7", "created": "Tue, 15 Oct 2019 17:22:25 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Kwon", "Oh-Hyun", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "1904.12232", "submitter": "Hanchen Xu", "authors": "Hanchen Xu and Xiao Li and Xiangyu Zhang and Junbo Zhang", "title": "Arbitrage of Energy Storage in Electricity Markets with Deep\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter, we address the problem of controlling energy storage systems\n(ESSs) for arbitrage in real-time electricity markets under price uncertainty.\nWe first formulate this problem as a Markov decision process, and then develop\na deep reinforcement learning based algorithm to learn a stochastic control\npolicy that maps a set of available information processed by a recurrent neural\nnetwork to ESSs' charging/discharging actions. Finally, we verify the\neffectiveness of our algorithm using real-time electricity prices from PJM.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 00:08:07 GMT"}, {"version": "v2", "created": "Sat, 4 May 2019 05:18:04 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Xu", "Hanchen", ""], ["Li", "Xiao", ""], ["Zhang", "Xiangyu", ""], ["Zhang", "Junbo", ""]]}, {"id": "1904.12233", "submitter": "Sebastien Bubeck", "authors": "S\\'ebastien Bubeck, Yuanzhi Li, Yuval Peres, Mark Sellke", "title": "Non-Stochastic Multi-Player Multi-Armed Bandits: Optimal Rate With\n  Collision Information, Sublinear Without", "comments": "27 pages, v2 adds a pseudorandom generator construction to remove the\n  shared randomness assumption in the $\\sqrt{T}$-regret result (Section 3.9)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the non-stochastic version of the (cooperative) multi-player\nmulti-armed bandit problem. The model assumes no communication at all between\nthe players, and furthermore when two (or more) players select the same action\nthis results in a maximal loss. We prove the first $\\sqrt{T}$-type regret\nguarantee for this problem, under the feedback model where collisions are\nannounced to the colliding players. Such a bound was not known even for the\nsimpler stochastic version. We also prove the first sublinear guarantee for the\nfeedback model where collision information is not available, namely\n$T^{1-\\frac{1}{2m}}$ where $m$ is the number of players.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 00:21:04 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 19:05:21 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Li", "Yuanzhi", ""], ["Peres", "Yuval", ""], ["Sellke", "Mark", ""]]}, {"id": "1904.12286", "submitter": "Koby Bibas", "authors": "Koby Bibas, Yaniv Fogel and Meir Feder", "title": "Deep pNML: Predictive Normalized Maximum Likelihood for Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Predictive Normalized Maximum Likelihood (pNML) scheme has been recently\nsuggested for universal learning in the individual setting, where both the\ntraining and test samples are individual data. The goal of universal learning\nis to compete with a ``genie'' or reference learner that knows the data values,\nbut is restricted to use a learner from a given model class. The pNML minimizes\nthe associated regret for any possible value of the unknown label. Furthermore,\nits min-max regret can serve as a pointwise measure of learnability for the\nspecific training and data sample. In this work we examine the pNML and its\nassociated learnability measure for the Deep Neural Network (DNN) model class.\nAs shown, the pNML outperforms the commonly used Empirical Risk Minimization\n(ERM) approach and provides robustness against adversarial attacks. Together\nwith its learnability measure it can detect out of distribution test examples,\nbe tolerant to noisy labels and serve as a confidence measure for the ERM.\nFinally, we extend the pNML to a ``twice universal'' solution, that provides\nuniversality for model class selection and generates a learner competing with\nthe best one from all model classes.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 09:35:50 GMT"}, {"version": "v2", "created": "Wed, 8 Jan 2020 12:46:19 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Bibas", "Koby", ""], ["Fogel", "Yaniv", ""], ["Feder", "Meir", ""]]}, {"id": "1904.12303", "submitter": "Ke Han", "authors": "Jun Song, Ke Han", "title": "Deep-MAPS: Machine Learning based Mobile Air Pollution Sensing", "comments": "10 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile and ubiquitous sensing of urban air quality has received increased\nattention as an economically and operationally viable means to survey\natmospheric environment with high spatial-temporal resolution. This paper\nproposes a machine learning based mobile air pollution sensing framework,\ncalled Deep-MAPS, and demonstrates its scientific and financial values in the\nfollowing aspects. (1) Based on a network of fixed and mobile air quality\nsensors, we perform spatial inference of PM2.5 concentrations in Beijing (3,025\nkm2, 19 Jun-16 Jul 2018) for a spatial-temporal resolution of 1km-by-1km and 1\nhour, with over 85% accuracy. (2) We leverage urban big data to generate\ninsights regarding the potential cause of pollution, which facilitates\nevidence-based sustainable urban management. (3) To achieve such\nspatial-temporal coverage and accuracy, Deep-MAPS can save up to 90% hardware\ninvestment, compared with ubiquitous sensing that relies primarily on fixed\nsensors.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 11:07:24 GMT"}, {"version": "v2", "created": "Sun, 1 Mar 2020 08:26:21 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Song", "Jun", ""], ["Han", "Ke", ""]]}, {"id": "1904.12320", "submitter": "Laurent Bou\\'e", "authors": "Laurent Bou\\'e", "title": "Real numbers, data science and chaos: How to fit any dataset with a\n  single parameter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DM cs.GL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how any dataset of any modality (time-series, images, sound...) can\nbe approximated by a well-behaved (continuous, differentiable...) scalar\nfunction with a single real-valued parameter. Building upon elementary concepts\nfrom chaos theory, we adopt a pedagogical approach demonstrating how to adjust\nthis parameter in order to achieve arbitrary precision fit to all samples of\nthe data. Targeting an audience of data scientists with a taste for the curious\nand unusual, the results presented here expand on previous similar observations\nregarding expressiveness power and generalization of machine learning models.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 13:29:49 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Bou\u00e9", "Laurent", ""]]}, {"id": "1904.12331", "submitter": "Pritam Anand South Asian University", "authors": "Pritam Anand, Reshma Rastogi (nee Khemchandani), Suresh Chandra", "title": "Support Vector Regression via a Combined Reward Cum Penalty Loss\n  Function", "comments": "For any assistance , reader can contact on email with Pritam Anand.\n  Email id - ltpritamanand@gmail.com. The valuable opinion/comments on the work\n  are welcomed. Looking for collaboration especially for speeding up the\n  solution of optimization problems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel combined reward cum penalty loss function\nto handle the regression problem. The proposed combined reward cum penalty loss\nfunction penalizes the data points which lie outside the $\\epsilon$-tube of the\nregressor and also assigns reward for the data points which lie inside of the\n$\\epsilon$-tube of the regressor. The combined reward cum penalty loss function\nbased regression (RP-$\\epsilon$-SVR) model has several interesting properties\nwhich are investigated in this paper and are also supported with the\nexperimental results.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 14:50:19 GMT"}, {"version": "v2", "created": "Sun, 3 May 2020 17:34:24 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Anand", "Pritam", "", "nee Khemchandani"], ["Rastogi", "Reshma", "", "nee Khemchandani"], ["Chandra", "Suresh", ""]]}, {"id": "1904.12335", "submitter": "Cyrille W. Combettes", "authors": "Cyrille W. Combettes and Sebastian Pokutta", "title": "Blended Matching Pursuit", "comments": "30 pages and 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching pursuit algorithms are an important class of algorithms in signal\nprocessing and machine learning. We present a blended matching pursuit\nalgorithm, combining coordinate descent-like steps with stronger gradient\ndescent steps, for minimizing a smooth convex function over a linear space\nspanned by a set of atoms. We derive sublinear to linear convergence rates\naccording to the smoothness and sharpness orders of the function and\ndemonstrate computational superiority of our approach. In particular, we derive\nlinear rates for a wide class of non-strongly convex functions, and we\ndemonstrate in experiments that our algorithm enjoys very fast rates of\nconvergence and wall-clock speed while maintaining a sparsity of iterates very\ncomparable to that of the (much slower) orthogonal matching pursuit.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 15:28:13 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 15:46:33 GMT"}, {"version": "v3", "created": "Tue, 19 Nov 2019 20:33:13 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Combettes", "Cyrille W.", ""], ["Pokutta", "Sebastian", ""]]}, {"id": "1904.12336", "submitter": "Zinan Liu", "authors": "Zinan Liu, Kai Ploeger, Svenja Stark, Elmar Rueckert and Jan Peters", "title": "Learning walk and trot from the same objective using different types of\n  exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In quadruped gait learning, policy search methods that scale high dimensional\ncontinuous action spaces are commonly used. In most approaches, it is necessary\nto introduce prior knowledge on the gaits to limit the highly non-convex search\nspace of the policies. In this work, we propose a new approach to encode the\nsymmetry properties of the desired gaits, on the initial covariance of the\nGaussian search distribution, allowing for strategic exploration. Using\nepisode-based likelihood ratio policy gradient and relative entropy policy\nsearch, we learned the gaits walk and trot on a simulated quadruped. Comparing\nthese gaits to random gaits learned by initialized diagonal covariance matrix,\nwe show that the performance can be significantly enhanced.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 15:44:24 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Liu", "Zinan", ""], ["Ploeger", "Kai", ""], ["Stark", "Svenja", ""], ["Rueckert", "Elmar", ""], ["Peters", "Jan", ""]]}, {"id": "1904.12354", "submitter": "Aydin Teyhouee", "authors": "Aydin Teyhouee and Nathaniel D. Osgood", "title": "Cough Detection Using Hidden Markov Models", "comments": "SBP-BRiMS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Respiratory infections and chronic respiratory diseases impose a heavy health\nburden worldwide. Coughing is one of the most common symptoms of many such\ninfections, and can be indicative of flare-ups of chronic respiratory diseases.\nWhether at a clinical or public health level, the capacity to identify bouts of\ncoughing can aid understanding of population and individual health status.\nDeveloping health monitoring models in the context of respiratory diseases and\nalso seasonal diseases with symptoms such as cough has the potential to improve\nquality of life, help clinicians and public health authorities with their\ndecisions and decrease the cost of health services. In this paper, we\ninvestigated the ability to which a simple machine learning approach in the\nform of Hidden Markov Models (HMMs) could be used to classify different states\nof coughing using univariate (with a single energy band as the input feature)\nand multivariate (with a multiple energy band as the input features) binned\ntime series using both of cough data. We further used the model to distinguish\ncough events from other events and environmental noise. Our Hidden Markov\nalgorithm achieved 92% AUR (Area Under Receiver Operating Characteristic Curve)\nin classifying coughing events in noisy environments. Moreover, comparison of\nunivariate with multivariate HMMs suggest a high accuracy of multivariate HMMs\nfor cough event classifications.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 17:47:31 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Teyhouee", "Aydin", ""], ["Osgood", "Nathaniel D.", ""]]}, {"id": "1904.12360", "submitter": "Qing Zhou", "authors": "Qiaoling Ye, Arash A. Amini, and Qing Zhou", "title": "Optimizing regularized Cholesky score for order-based learning of\n  Bayesian networks", "comments": "15 pages, 7 figures, 5 tables", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (2020)", "doi": "10.1109/TPAMI.2020.2990820", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian networks are a class of popular graphical models that encode causal\nand conditional independence relations among variables by directed acyclic\ngraphs (DAGs). We propose a novel structure learning method, annealing on\nregularized Cholesky score (ARCS), to search over topological sorts, or\npermutations of nodes, for a high-scoring Bayesian network. Our scoring\nfunction is derived from regularizing Gaussian DAG likelihood, and its\noptimization gives an alternative formulation of the sparse Cholesky\nfactorization problem from a statistical viewpoint, which is of independent\ninterest. We combine global simulated annealing over permutations with a fast\nproximal gradient algorithm, operating on triangular matrices of edge\ncoefficients, to compute the score of any permutation. Combined, the two\napproaches allow us to quickly and effectively search over the space of DAGs\nwithout the need to verify the acyclicity constraint or to enumerate possible\nparent sets given a candidate topological sort. The annealing aspect of the\noptimization is able to consistently improve the accuracy of DAGs learned by\nlocal search algorithms. In addition, we develop several techniques to\nfacilitate the structure learning, including pre-annealing data-driven tuning\nparameter selection and post-annealing constraint-based structure refinement.\nThrough extensive numerical comparisons, we show that ARCS achieves substantial\nimprovements over existing methods, demonstrating its great potential to learn\nBayesian networks from both observational and experimental data.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 18:24:15 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Ye", "Qiaoling", ""], ["Amini", "Arash A.", ""], ["Zhou", "Qing", ""]]}, {"id": "1904.12369", "submitter": "Elynn Chen", "authors": "Krishna Balasubramanian, Elynn Y. Chen, Jianqing Fan, Xiang Wu", "title": "Low-Rank Principal Eigenmatrix Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse PCA is a widely used technique for high-dimensional data analysis. In\nthis paper, we propose a new method called low-rank principal eigenmatrix\nanalysis. Different from sparse PCA, the dominant eigenvectors are allowed to\nbe dense but are assumed to have a low-rank structure when matricized\nappropriately. Such a structure arises naturally in several practical cases:\nIndeed the top eigenvector of a circulant matrix, when matricized appropriately\nis a rank-1 matrix. We propose a matricized rank-truncated power method that\ncould be efficiently implemented and establish its computational and\nstatistical properties. Extensive experiments on several synthetic data sets\ndemonstrate the competitive empirical performance of our method.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 18:51:39 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Balasubramanian", "Krishna", ""], ["Chen", "Elynn Y.", ""], ["Fan", "Jianqing", ""], ["Wu", "Xiang", ""]]}, {"id": "1904.12383", "submitter": "Seyedeh Neelufar Payrovnaziri", "authors": "Seyedeh Neelufar Payrovnaziri, Laura A. Barrett, Daniel Bis, Jiang\n  Bian, Zhe He", "title": "Enhancing Prediction Models for One-Year Mortality in Patients with\n  Acute Myocardial Infarction and Post Myocardial Infarction Syndrome", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Predicting the risk of mortality for patients with acute myocardial\ninfarction (AMI) using electronic health records (EHRs) data can help identify\nrisky patients who might need more tailored care. In our previous work, we\nbuilt computational models to predict one-year mortality of patients admitted\nto an intensive care unit (ICU) with AMI or post myocardial infarction\nsyndrome. Our prior work only used the structured clinical data from MIMIC-III,\na publicly available ICU clinical database. In this study, we enhanced our work\nby adding the word embedding features from free-text discharge summaries. Using\na richer set of features resulted in significant improvement in the performance\nof our deep learning models. The average accuracy of our deep learning models\nwas 92.89% and the average F-measure was 0.928. We further reported the impact\nof different combinations of features extracted from structured and/or\nunstructured data on the performance of the deep learning models.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 20:45:04 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Payrovnaziri", "Seyedeh Neelufar", ""], ["Barrett", "Laura A.", ""], ["Bis", "Daniel", ""], ["Bian", "Jiang", ""], ["He", "Zhe", ""]]}, {"id": "1904.12399", "submitter": "Zhong Meng", "authors": "Zhong Meng, Jinyu Li, Yong Zhao, Yifan Gong", "title": "Conditional Teacher-Student Learning", "comments": "5 pages, 1 figure, ICASSP 2019", "journal-ref": "2019 IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP), Brighton, UK", "doi": "10.1109/ICASSP.2019.8683438", "report-no": null, "categories": "cs.LG cs.CL cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The teacher-student (T/S) learning has been shown to be effective for a\nvariety of problems such as domain adaptation and model compression. One\nshortcoming of the T/S learning is that a teacher model, not always perfect,\nsporadically produces wrong guidance in form of posterior probabilities that\nmisleads the student model towards a suboptimal performance. To overcome this\nproblem, we propose a conditional T/S learning scheme, in which a \"smart\"\nstudent model selectively chooses to learn from either the teacher model or the\nground truth labels conditioned on whether the teacher can correctly predict\nthe ground truth. Unlike a naive linear combination of the two knowledge\nsources, the conditional learning is exclusively engaged with the teacher model\nwhen the teacher model's prediction is correct, and otherwise backs off to the\nground truth. Thus, the student model is able to learn effectively from the\nteacher and even potentially surpass the teacher. We examine the proposed\nlearning scheme on two tasks: domain adaptation on CHiME-3 dataset and speaker\nadaptation on Microsoft short message dictation dataset. The proposed method\nachieves 9.8% and 12.8% relative word error rate reductions, respectively, over\nT/S learning for environment adaptation and speaker-independent model for\nspeaker adaptation.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 23:43:20 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Meng", "Zhong", ""], ["Li", "Jinyu", ""], ["Zhao", "Yong", ""], ["Gong", "Yifan", ""]]}, {"id": "1904.12400", "submitter": "Zhong Meng", "authors": "Zhong Meng, Jinyu Li, Yifan Gong", "title": "Attentive Adversarial Learning for Domain-Invariant Training", "comments": "5 pages, 1 figure, ICASSP 2019", "journal-ref": "2019 IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP), Brighton, United Kingdom", "doi": "10.1109/ICASSP.2019.8683486", "report-no": null, "categories": "cs.LG cs.CL cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial domain-invariant training (ADIT) proves to be effective in\nsuppressing the effects of domain variability in acoustic modeling and has led\nto improved performance in automatic speech recognition (ASR). In ADIT, an\nauxiliary domain classifier takes in equally-weighted deep features from a deep\nneural network (DNN) acoustic model and is trained to improve their\ndomain-invariance by optimizing an adversarial loss function. In this work, we\npropose an attentive ADIT (AADIT) in which we advance the domain classifier\nwith an attention mechanism to automatically weight the input deep features\naccording to their importance in domain classification. With this attentive\nre-weighting, AADIT can focus on the domain normalization of phonetic\ncomponents that are more susceptible to domain variability and generates deep\nfeatures with improved domain-invariance and senone-discriminativity over ADIT.\nMost importantly, the attention block serves only as an external component to\nthe DNN acoustic model and is not involved in ASR, so AADIT can be used to\nimprove the acoustic modeling with any DNN architectures. More generally, the\nsame methodology can improve any adversarial learning system with an auxiliary\ndiscriminator. Evaluated on CHiME-3 dataset, the AADIT achieves 13.6% and 9.3%\nrelative WER improvements, respectively, over a multi-conditional model and a\nstrong ADIT baseline.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 23:44:29 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Meng", "Zhong", ""], ["Li", "Jinyu", ""], ["Gong", "Yifan", ""]]}, {"id": "1904.12406", "submitter": "Zhong Meng", "authors": "Zhong Meng, Yong Zhao, Jinyu Li, Yifan Gong", "title": "Adversarial Speaker Verification", "comments": "5 pages, 1 figure, ICASSP 2019", "journal-ref": "2019 IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP), Brighton, United Kingdom", "doi": "10.1109/ICASSP.2019.8682488", "report-no": null, "categories": "cs.SD cs.CL cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of deep networks to extract embeddings for speaker recognition has\nproven successfully. However, such embeddings are susceptible to performance\ndegradation due to the mismatches among the training, enrollment, and test\nconditions. In this work, we propose an adversarial speaker verification (ASV)\nscheme to learn the condition-invariant deep embedding via adversarial\nmulti-task training. In ASV, a speaker classification network and a condition\nidentification network are jointly optimized to minimize the speaker\nclassification loss and simultaneously mini-maximize the condition loss. The\ntarget labels of the condition network can be categorical (environment types)\nand continuous (SNR values). We further propose multi-factorial ASV to\nsimultaneously suppress multiple factors that constitute the condition\nvariability. Evaluated on a Microsoft Cortana text-dependent speaker\nverification task, the ASV achieves 8.8% and 14.5% relative improvements in\nequal error rates (EER) for known and unknown conditions, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 00:37:27 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Meng", "Zhong", ""], ["Zhao", "Yong", ""], ["Li", "Jinyu", ""], ["Gong", "Yifan", ""]]}, {"id": "1904.12407", "submitter": "Zhong Meng", "authors": "Zhong Meng, Jinyu Li, Yifan Gong", "title": "Adversarial Speaker Adaptation", "comments": "5 pages, 2 figures, ICASSP 2019", "journal-ref": "2019 IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP), Brighton, United Kingdom", "doi": "10.1109/ICASSP.2019.8682510", "report-no": null, "categories": "cs.LG cs.CL cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel adversarial speaker adaptation (ASA) scheme, in which\nadversarial learning is applied to regularize the distribution of deep hidden\nfeatures in a speaker-dependent (SD) deep neural network (DNN) acoustic model\nto be close to that of a fixed speaker-independent (SI) DNN acoustic model\nduring adaptation. An additional discriminator network is introduced to\ndistinguish the deep features generated by the SD model from those produced by\nthe SI model. In ASA, with a fixed SI model as the reference, an SD model is\njointly optimized with the discriminator network to minimize the senone\nclassification loss, and simultaneously to mini-maximize the SI/SD\ndiscrimination loss on the adaptation data. With ASA, a senone-discriminative\ndeep feature is learned in the SD model with a similar distribution to that of\nthe SI model. With such a regularized and adapted deep feature, the SD model\ncan perform improved automatic speech recognition on the target speaker's\nspeech. Evaluated on the Microsoft short message dictation dataset, ASA\nachieves 14.4% and 7.9% relative word error rate improvements for supervised\nand unsupervised adaptation, respectively, over an SI model trained from 2600\nhours data, with 200 adaptation utterances per speaker.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 00:38:16 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Meng", "Zhong", ""], ["Li", "Jinyu", ""], ["Gong", "Yifan", ""]]}, {"id": "1904.12413", "submitter": "Reza Asadi Mr", "authors": "Reza Asadi, Amelia Regan", "title": "A convolution recurrent autoencoder for spatio-temporal missing data\n  imputation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When sensors collect spatio-temporal data in a large geographical area, the\nexistence of missing data cannot be escaped. Missing data negatively impacts\nthe performance of data analysis and machine learning algorithms. In this\npaper, we study deep autoencoders for missing data imputation in\nspatio-temporal problems. We propose a convolution bidirectional-LSTM for\ncapturing spatial and temporal patterns. Moreover, we analyze an autoencoder's\nlatent feature representation in spatio-temporal data and illustrate its\nperformance for missing data imputation. Traffic flow data are used for\nevaluation of our models. The result shows that the proposed convolution\nrecurrent neural network outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 01:12:28 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Asadi", "Reza", ""], ["Regan", "Amelia", ""]]}, {"id": "1904.12426", "submitter": "Taesik Na", "authors": "Taesik Na, Minah Lee, Burhan A. Mudassar, Priyabrata Saha, Jong Hwan\n  Ko, Saibal Mukhopadhyay", "title": "Mixture of Pre-processing Experts Model for Noise Robust Deep Learning\n  on Resource Constrained Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 02:26:06 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Na", "Taesik", ""], ["Lee", "Minah", ""], ["Mudassar", "Burhan A.", ""], ["Saha", "Priyabrata", ""], ["Ko", "Jong Hwan", ""], ["Mukhopadhyay", "Saibal", ""]]}, {"id": "1904.12445", "submitter": "Junyu Cao", "authors": "Junyu Cao, Wei Sun", "title": "Dynamic Learning with Frequent New Product Launches: A Sequential\n  Multinomial Logit Bandit Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the phenomenon that companies introduce new products to keep\nabreast with customers' rapidly changing tastes, we consider a novel online\nlearning setting where a profit-maximizing seller needs to learn customers'\npreferences through offering recommendations, which may contain existing\nproducts and new products that are launched in the middle of a selling period.\nWe propose a sequential multinomial logit (SMNL) model to characterize\ncustomers' behavior when product recommendations are presented in tiers. For\nthe offline version with known customers' preferences, we propose a\npolynomial-time algorithm and characterize the properties of the optimal tiered\nproduct recommendation. For the online problem, we propose a learning algorithm\nand quantify its regret bound. Moreover, we extend the setting to incorporate a\nconstraint which ensures every new product is learned to a given accuracy. Our\nresults demonstrate the tier structure can be used to mitigate the risks\nassociated with learning new products.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 04:44:31 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Cao", "Junyu", ""], ["Sun", "Wei", ""]]}, {"id": "1904.12465", "submitter": "David Zimmermann", "authors": "David Zimmermann", "title": "Asymmetric Impurity Functions, Class Weighting, and Optimal Splits for\n  Binary Classification Trees", "comments": "18 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate how asymmetrizing an impurity function affects the choice of\noptimal node splits when growing a decision tree for binary classification. In\nparticular, we relax the usual axioms of an impurity function and show how\nskewing an impurity function biases the optimal splits to isolate points of a\nparticular class when splitting a node. We give a rigorous definition of this\nnotion, then give a necessary and sufficient condition for such a bias to hold.\nWe also show that the technique of class weighting is equivalent to applying a\nspecific transformation to the impurity function, and tie all these notions\ntogether for a class of impurity functions that includes the entropy and Gini\nimpurity. We also briefly discuss cost-insensitive impurity functions and give\na characterization of such functions.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 06:34:37 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Zimmermann", "David", ""]]}, {"id": "1904.12543", "submitter": "Kei Akuzawa", "authors": "Kei Akuzawa, Yusuke Iwasawa, Yutaka Matsuo", "title": "Adversarial Invariant Feature Learning with Accuracy Constraint for\n  Domain Generalization", "comments": "accepted for ECMLPKDD 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning domain-invariant representation is a dominant approach for domain\ngeneralization (DG), where we need to build a classifier that is robust toward\ndomain shifts. However, previous domain-invariance-based methods overlooked the\nunderlying dependency of classes on domains, which is responsible for the\ntrade-off between classification accuracy and domain invariance. Because the\nprimary purpose of DG is to classify unseen domains rather than the invariance\nitself, the improvement of the invariance can negatively affect DG performance\nunder this trade-off. To overcome the problem, this study first expands the\nanalysis of the trade-off by Xie et. al., and provides the notion of\naccuracy-constrained domain invariance, which means the maximum domain\ninvariance within a range that does not interfere with accuracy. We then\npropose a novel method adversarial feature learning with accuracy constraint\n(AFLAC), which explicitly leads to that invariance on adversarial training.\nEmpirical validations show that the performance of AFLAC is superior to that of\ndomain-invariance-based methods on both synthetic and three real-world\ndatasets, supporting the importance of considering the dependency and the\nefficacy of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 09:52:36 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 05:54:15 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2020 11:24:01 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Akuzawa", "Kei", ""], ["Iwasawa", "Yusuke", ""], ["Matsuo", "Yutaka", ""]]}, {"id": "1904.12546", "submitter": "Pankaj Malhotra", "authors": "Kathan Kashiparekh, Jyoti Narwariya, Pankaj Malhotra, Lovekesh Vig,\n  Gautam Shroff", "title": "ConvTimeNet: A Pre-trained Deep Convolutional Neural Network for Time\n  Series Classification", "comments": "Accepted at IJCNN 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural networks often requires careful hyper-parameter tuning\nand significant computational resources. In this paper, we propose ConvTimeNet\n(CTN): an off-the-shelf deep convolutional neural network (CNN) trained on\ndiverse univariate time series classification (TSC) source tasks. Once trained,\nCTN can be easily adapted to new TSC target tasks via a small amount of\nfine-tuning using labeled instances from the target tasks. We note that the\nlength of convolutional filters is a key aspect when building a pre-trained\nmodel that can generalize to time series of different lengths across datasets.\nTo achieve this, we incorporate filters of multiple lengths in all\nconvolutional layers of CTN to capture temporal features at multiple time\nscales. We consider all 65 datasets with time series of lengths up to 512\npoints from the UCR TSC Benchmark for training and testing transferability of\nCTN: We train CTN on a randomly chosen subset of 24 datasets using a multi-head\napproach with a different softmax layer for each training dataset, and study\ngeneralizability and transferability of the learned filters on the remaining 41\nTSC datasets. We observe significant gains in classification accuracy as well\nas computational efficiency when using pre-trained CTN as a starting point for\nsubsequent task-specific fine-tuning compared to existing state-of-the-art TSC\napproaches. We also provide qualitative insights into the working of CTN by: i)\nanalyzing the activations and filters of first convolution layer suggesting the\nfilters in CTN are generically useful, ii) analyzing the impact of the design\ndecision to incorporate multiple length decisions, and iii) finding regions of\ntime series that affect the final classification decision via occlusion\nsensitivity analysis.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 10:12:17 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 05:32:20 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Kashiparekh", "Kathan", ""], ["Narwariya", "Jyoti", ""], ["Malhotra", "Pankaj", ""], ["Vig", "Lovekesh", ""], ["Shroff", "Gautam", ""]]}, {"id": "1904.12562", "submitter": "Evgenii Ofitserov", "authors": "Evgenii Ofitserov, Vasily Tsvetkov, Vadim Nazarov", "title": "Soft edit distance for differentiable comparison of symbolic sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Edit distance, also known as Levenshtein distance, is an essential way to\ncompare two strings that proved to be particularly useful in the analysis of\ngenetic sequences and natural language processing. However, edit distance is a\ndiscrete function that is known to be hard to optimize. This fact hampers the\nuse of this metric in Machine Learning. Even as simple algorithm as K-means\nfails to cluster a set of sequences using edit distance if they are of variable\nlength and abundance. In this paper we propose a novel metric - soft edit\ndistance (SED), which is a smooth approximation of edit distance. It is\ndifferentiable and therefore it is possible to optimize it with gradient\nmethods. Similar to original edit distance, SED as well as its derivatives can\nbe calculated with recurrent formulas at polynomial time. We prove usefulness\nof the proposed metric on synthetic datasets and clustering of biological\nsequences.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 11:31:54 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Ofitserov", "Evgenii", ""], ["Tsvetkov", "Vasily", ""], ["Nazarov", "Vadim", ""]]}, {"id": "1904.12574", "submitter": "Da Xu", "authors": "Da Xu, Chuanwei Ruan, Jason Cho, Evren Korpeoglu, Sushant Kumar,\n  Kannan Achan", "title": "Knowledge-aware Complementary Product Representation Learning", "comments": null, "journal-ref": null, "doi": "10.1145/3336191.3371854", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning product representations that reflect complementary relationship\nplays a central role in e-commerce recommender system. In the absence of the\nproduct relationships graph, which existing methods rely on, there is a need to\ndetect the complementary relationships directly from noisy and sparse customer\npurchase activities. Furthermore, unlike simple relationships such as\nsimilarity, complementariness is asymmetric and non-transitive. Standard usage\nof representation learning emphasizes on only one set of embedding, which is\nproblematic for modelling such properties of complementariness. We propose\nusing knowledge-aware learning with dual product embedding to solve the above\nchallenges. We encode contextual knowledge into product representation by\nmulti-task learning, to alleviate the sparsity issue. By explicitly modelling\nwith user bias terms, we separate the noise of customer-specific preferences\nfrom the complementariness. Furthermore, we adopt the dual embedding framework\nto capture the intrinsic properties of complementariness and provide geometric\ninterpretation motivated by the classic separating hyperplane theory. Finally,\nwe propose a Bayesian network structure that unifies all the components, which\nalso concludes several popular models as special cases. The proposed method\ncompares favourably to state-of-art methods, in downstream classification and\nrecommendation tasks. We also develop an implementation that scales efficiently\nto a dataset with millions of items and customers.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 03:01:12 GMT"}, {"version": "v2", "created": "Sun, 16 Jun 2019 19:46:18 GMT"}, {"version": "v3", "created": "Thu, 28 Nov 2019 21:09:31 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Xu", "Da", ""], ["Ruan", "Chuanwei", ""], ["Cho", "Jason", ""], ["Korpeoglu", "Evren", ""], ["Kumar", "Sushant", ""], ["Achan", "Kannan", ""]]}, {"id": "1904.12575", "submitter": "Hongwei Wang", "authors": "Hongwei Wang, Miao Zhao, Xing Xie, Wenjie Li, Minyi Guo", "title": "Knowledge Graph Convolutional Networks for Recommender Systems", "comments": "Proceedings of the 2019 World Wide Web Conference", "journal-ref": null, "doi": "10.1145/3308558.3313417", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To alleviate sparsity and cold start problem of collaborative filtering based\nrecommender systems, researchers and engineers usually collect attributes of\nusers and items, and design delicate algorithms to exploit these additional\ninformation. In general, the attributes are not isolated but connected with\neach other, which forms a knowledge graph (KG). In this paper, we propose\nKnowledge Graph Convolutional Networks (KGCN), an end-to-end framework that\ncaptures inter-item relatedness effectively by mining their associated\nattributes on the KG. To automatically discover both high-order structure\ninformation and semantic information of the KG, we sample from the neighbors\nfor each entity in the KG as their receptive field, then combine neighborhood\ninformation with bias when calculating the representation of a given entity.\nThe receptive field can be extended to multiple hops away to model high-order\nproximity information and capture users' potential long-distance interests.\nMoreover, we implement the proposed KGCN in a minibatch fashion, which enables\nour model to operate on large datasets and KGs. We apply the proposed model to\nthree datasets about movie, book, and music recommendation, and experiment\nresults demonstrate that our approach outperforms strong recommender baselines.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 17:17:34 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Wang", "Hongwei", ""], ["Zhao", "Miao", ""], ["Xie", "Xing", ""], ["Li", "Wenjie", ""], ["Guo", "Minyi", ""]]}, {"id": "1904.12578", "submitter": "Ronghui You", "authors": "Ronghui You, Zihan Zhang, Suyang Dai and Shanfeng Zhu", "title": "HAXMLNet: Hierarchical Attention Network for Extreme Multi-Label Text\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme multi-label text classification (XMTC) addresses the problem of\ntagging each text with the most relevant labels from an extreme-scale label\nset. Traditional methods use bag-of-words (BOW) representations without context\ninformation as their features. The state-ot-the-art deep learning-based method,\nAttentionXML, which uses a recurrent neural network (RNN) and the multi-label\nattention, can hardly deal with extreme-scale (hundreds of thousands labels)\nproblem. To address this, we propose our HAXMLNet, which uses an efficient and\neffective hierarchical structure with the multi-label attention. Experimental\nresults show that HAXMLNet reaches a competitive performance with other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 08:09:15 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["You", "Ronghui", ""], ["Zhang", "Zihan", ""], ["Dai", "Suyang", ""], ["Zhu", "Shanfeng", ""]]}, {"id": "1904.12579", "submitter": "Furao Shen", "authors": "Yi Yang, Baile Xu, Furao Shen, Jian Zhao", "title": "Operation-aware Neural Networks for User Response Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User response prediction makes a crucial contribution to the rapid\ndevelopment of online advertising system and recommendation system. The\nimportance of learning feature interactions has been emphasized by many works.\nMany deep models are proposed to automatically learn high-order feature\ninteractions. Since most features in advertising system and recommendation\nsystem are high-dimensional sparse features, deep models usually learn a\nlow-dimensional distributed representation for each feature in the bottom\nlayer. Besides traditional fully-connected architectures, some new operations,\nsuch as convolutional operations and product operations, are proposed to learn\nfeature interactions better. In these models, the representation is shared\namong different operations. However, the best representation for different\noperations may be different. In this paper, we propose a new neural model named\nOperation-aware Neural Networks (ONN) which learns different representations\nfor different operations. Our experimental results on two large-scale\nreal-world ad click/conversion datasets demonstrate that ONN consistently\noutperforms the state-of-the-art models in both offline-training environment\nand online-training environment.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 15:04:52 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Yang", "Yi", ""], ["Xu", "Baile", ""], ["Shen", "Furao", ""], ["Zhao", "Jian", ""]]}, {"id": "1904.12580", "submitter": "Mahidhar Dwarampudi", "authors": "Dwarampudi Mahidhar Reddy, Dr. N V Subba Reddy, Dr. N V Subba Reddy", "title": "Twitter Sentiment Analysis using Distributed Word and Sentence\n  Representation", "comments": "8 pages, 5 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important part of the information gathering and data analysis is to find\nout what people think about, either a product or an entity. Twitter is an\nopinion rich social networking site. The posts or tweets from this data can be\nused for mining people's opinions. The recent surge of activity in this area\ncan be attributed to the computational treatment of data, which made opinion\nextraction and sentiment analysis easier. This paper classifies tweets into\npositive and negative sentiments, but instead of using traditional methods or\npreprocessing text data here we use the distributed representations of words\nand sentences to classify the tweets. We use Long Short Term Memory (LSTM)\nNetworks, Convolutional Neural Networks (CNNs) and Artificial Neural Networks.\nThe first two are used on Distributed Representation of words while the latter\nis used on the distributed representation of sentences. This paper achieves\naccuracies as high as 81%. It also suggests the best and optimal ways for\ncreating distributed representations of words for sentiment analysis, out of\nthe available methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 17:46:54 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Reddy", "Dwarampudi Mahidhar", ""], ["Reddy", "Dr. N V Subba", ""], ["Reddy", "Dr. N V Subba", ""]]}, {"id": "1904.12587", "submitter": "Thomas K\\\"ollmer", "authors": "Thomas K\\\"ollmer and Jens Hasselbach and Patrick Aichroth", "title": "Text Classification Components for Detecting Descriptions and Names of\n  CAD models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply text analysis approaches for a specialized search engine for 3D CAD\nmodels and associated products. The main goals are to distinguish between\nactual product descriptions and other text on a website, as well as to decide\nwhether a given text is or contains a product name.\n  For this we use paragraph vectors for text classification, a character-level\nlong short-term memory network (LSTM) for a single word classification and an\nLSTM tagger based on word embeddings for detecting product names within\nsentences. Despite the need to collect bigger datasets in our specific problem\ndomain, the first results are promising and partially fit for production use.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 15:41:26 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["K\u00f6llmer", "Thomas", ""], ["Hasselbach", "Jens", ""], ["Aichroth", "Patrick", ""]]}, {"id": "1904.12604", "submitter": "Jingxuan Yang", "authors": "Jingxuan Yang, Jun Xu, Jianzhuo Tong, Sheng Gao, Jun Guo, Jirong Wen", "title": "Pre-training of Context-aware Item Representation for Next Basket\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next basket recommendation, which aims to predict the next a few items that a\nuser most probably purchases given his historical transactions, plays a vital\nrole in market basket analysis. From the viewpoint of item, an item could be\npurchased by different users together with different items, for different\nreasons. Therefore, an ideal recommender system should represent an item\nconsidering its transaction contexts. Existing state-of-the-art deep learning\nmethods usually adopt the static item representations, which are invariant\namong all of the transactions and thus cannot achieve the full potentials of\ndeep learning. Inspired by the pre-trained representations of BERT in natural\nlanguage processing, we propose to conduct context-aware item representation\nfor next basket recommendation, called Item Encoder Representations from\nTransformers (IERT). In the offline phase, IERT pre-trains deep item\nrepresentations conditioning on their transaction contexts. In the online\nrecommendation phase, the pre-trained model is further fine-tuned with an\nadditional output layer. The output contextualized item embeddings are used to\ncapture users' sequential behaviors and general tastes to conduct\nrecommendation. Experimental results on the Ta-Feng data set show that IERT\noutperforms the state-of-the-art baseline methods, which demonstrated the\neffectiveness of IERT in next basket representation.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 14:57:57 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Yang", "Jingxuan", ""], ["Xu", "Jun", ""], ["Tong", "Jianzhuo", ""], ["Gao", "Sheng", ""], ["Guo", "Jun", ""], ["Wen", "Jirong", ""]]}, {"id": "1904.12606", "submitter": "Dongxu Zhang", "authors": "Dongxu Zhang and Subhabrata Mukherjee and Colin Lockard and Xin Luna\n  Dong and Andrew McCallum", "title": "OpenKI: Integrating Open Information Extraction and Knowledge Bases with\n  Relation Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider advancing web-scale knowledge extraction and\nalignment by integrating OpenIE extractions in the form of (subject, predicate,\nobject) triples with Knowledge Bases (KB). Traditional techniques from\nuniversal schema and from schema mapping fall in two extremes: either they\nperform instance-level inference relying on embedding for (subject, object)\npairs, thus cannot handle pairs absent in any existing triples; or they perform\npredicate-level mapping and completely ignore background evidence from\nindividual entities, thus cannot achieve satisfying quality. We propose OpenKI\nto handle sparsity of OpenIE extractions by performing instance-level\ninference: for each entity, we encode the rich information in its neighborhood\nin both KB and OpenIE extractions, and leverage this information in relation\ninference by exploring different methods of aggregation and attention. In order\nto handle unseen entities, our model is designed without creating\nentity-specific parameters. Extensive experiments show that this method not\nonly significantly improves state-of-the-art for conventional OpenIE\nextractions like ReVerb, but also boosts the performance on OpenIE from\nsemi-structured data, where new entity pairs are abundant and data are fairly\nsparse.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 14:05:38 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Zhang", "Dongxu", ""], ["Mukherjee", "Subhabrata", ""], ["Lockard", "Colin", ""], ["Dong", "Xin Luna", ""], ["McCallum", "Andrew", ""]]}, {"id": "1904.12608", "submitter": "J\\'an Dolinsk\\'y", "authors": "J\\'an Dolinsk\\'y, M\\'aria Starovsk\\'a, Robert T\\'oth", "title": "Automatic Model Building in GEFCom 2017 Qualifying Match", "comments": "10 pages, 3 figures, competition report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Tangent Works team participated in GEFCom 2017 to test its automatic\nmodel building strategy for time series known as Tangent Information Modeller\n(TIM). Model building using TIM combined with historical temperature shuffling\nresulted in winning the competition. This strategy involved one remaining\ndegree of freedom, a decision on using a trend variable. This paper describes\nour modelling efforts in the competition, and furthermore outlines a fully\nautomated scenario where the decision on using the trend variable is handled by\nTIM. The results show that such a setup would also win the competition.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 15:02:15 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Dolinsk\u00fd", "J\u00e1n", ""], ["Starovsk\u00e1", "M\u00e1ria", ""], ["T\u00f3th", "Robert", ""]]}, {"id": "1904.12615", "submitter": "Xinyu Li", "authors": "Xinyu Li, Wei Zhang, Tong Shen, Tao Mei", "title": "Everyone is a Cartoonist: Selfie Cartoonization with Attentive\n  Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selfie and cartoon are two popular artistic forms that are widely presented\nin our daily life. Despite the great progress in image translation/stylization,\nfew techniques focus specifically on selfie cartoonization, since cartoon\nimages usually contain artistic abstraction (e.g., large smoothing areas) and\nexaggeration (e.g., large/delicate eyebrows). In this paper, we address this\nproblem by proposing a selfie cartoonization Generative Adversarial Network\n(scGAN), which mainly uses an attentive adversarial network (AAN) to emphasize\nspecific facial regions and ignore low-level details. More specifically, we\nfirst design a cycle-like architecture to enable training with unpaired data.\nThen we design three losses from different aspects. A total variation loss is\nused to highlight important edges and contents in cartoon portraits. An\nattentive cycle loss is added to lay more emphasis on delicate facial areas\nsuch as eyes. In addition, a perceptual loss is included to eliminate artifacts\nand improve robustness of our method. Experimental results show that our method\nis capable of generating different cartoon styles and outperforms a number of\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 11:23:40 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Li", "Xinyu", ""], ["Zhang", "Wei", ""], ["Shen", "Tong", ""], ["Mei", "Tao", ""]]}, {"id": "1904.12624", "submitter": "Apostol Vassilev", "authors": "Apostol Vassilev", "title": "BowTie - A deep learning feedforward neural network for sentiment\n  analysis", "comments": "12 pages, 7 figures, 4 tables", "journal-ref": null, "doi": "10.1007/978-3-030-37599-7_30", "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to model and encode the semantics of human-written text and select the\ntype of neural network to process it are not settled issues in sentiment\nanalysis. Accuracy and transferability are critical issues in machine learning\nin general. These properties are closely related to the loss estimates for the\ntrained model. I present a computationally-efficient and accurate feedforward\nneural network for sentiment prediction capable of maintaining low losses. When\ncoupled with an effective semantics model of the text, it provides highly\naccurate models with low losses. Experimental results on representative\nbenchmark datasets and comparisons to other methods show the advantages of the\nnew approach.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 13:38:57 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Vassilev", "Apostol", ""]]}, {"id": "1904.12627", "submitter": "Thomas Hollis", "authors": "Antoine Viscardi, Casey Juanxi Li, Thomas Hollis", "title": "Catch Me If You Can", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As advances in signature recognition have reached a new plateau of\nperformance at around 2% error rate, it is interesting to investigate\nalternative approaches. The approach detailed in this paper looks at using\nVariational Auto-Encoders (VAEs) to learn a latent space representation of\ngenuine signatures. This is then used to pass unlabelled signatures such that\nonly the genuine ones will successfully be reconstructed by the VAE. This\nlatent space representation and the reconstruction loss is subsequently used by\nrandom forest and kNN classifiers for prediction. Subsequently, VAE\ndisentanglement and the possibility of posterior collapse are ascertained and\nanalysed. The final results suggest that while this method performs less well\nthan existing alternatives, further work may allow this to be used as part of\nan ensemble for future models.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 04:36:54 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Viscardi", "Antoine", ""], ["Li", "Casey Juanxi", ""], ["Hollis", "Thomas", ""]]}, {"id": "1904.12638", "submitter": "Eloi Zablocki", "authors": "Eloi Zablocki, Patrick Bordes, Benjamin Piwowarski, Laure Soulier,\n  Patrick Gallinari", "title": "Context-Aware Zero-Shot Learning for Object Recognition", "comments": "Accepted at ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-Shot Learning (ZSL) aims at classifying unlabeled objects by leveraging\nauxiliary knowledge, such as semantic representations. A limitation of previous\napproaches is that only intrinsic properties of objects, e.g. their visual\nappearance, are taken into account while their context, e.g. the surrounding\nobjects in the image, is ignored. Following the intuitive principle that\nobjects tend to be found in certain contexts but not others, we propose a new\nand challenging approach, context-aware ZSL, that leverages semantic\nrepresentations in a new way to model the conditional likelihood of an object\nto appear in a given context. Finally, through extensive experiments conducted\non Visual Genome, we show that contextual information can substantially improve\nthe standard ZSL approach and is robust to unbalanced classes.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 08:50:05 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 11:39:52 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Zablocki", "Eloi", ""], ["Bordes", "Patrick", ""], ["Piwowarski", "Benjamin", ""], ["Soulier", "Laure", ""], ["Gallinari", "Patrick", ""]]}, {"id": "1904.12643", "submitter": "Mohit Sharma", "authors": "Mohit Sharma, F.Maxwell Harper, and George Karypis", "title": "Learning from Sets of Items in Recommender Systems", "comments": "27 pages, 17 figures, ACM TiiS (2019), DOI provided", "journal-ref": null, "doi": "10.1145/3326128", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing recommender systems use the ratings provided by users on\nindividual items. An additional source of preference information is to use the\nratings that users provide on sets of items. The advantages of using\npreferences on sets are two-fold. First, a rating provided on a set conveys\nsome preference information about each of the set's items, which allows us to\nacquire a user's preferences for more items that the number of ratings that the\nuser provided. Second, due to privacy concerns, users may not be willing to\nreveal their preferences on individual items explicitly but may be willing to\nprovide a single rating to a set of items, since it provides some level of\ninformation hiding. This paper investigates two questions related to using\nset-level ratings in recommender systems. First, how users' item-level ratings\nrelate to their set-level ratings. Second, how collaborative filtering-based\nmodels for item-level rating prediction can take advantage of such set-level\nratings. We have collected set-level ratings from active users of Movielens on\nsets of movies that they have rated in the past. Our analysis of these ratings\nshows that though the majority of the users provide the average of the ratings\non a set's constituent items as the rating on the set, there exists a\nsignificant number of users that tend to consistently either under- or\nover-rate the sets. We have developed collaborative filtering-based methods to\nexplicitly model these user behaviors that can be used to recommend items to\nusers. Experiments on real data and on synthetic data that resembles the under-\nor over-rating behavior in the real data, demonstrate that these models can\nrecover the overall characteristics of the underlying data and predict the\nuser's ratings on individual items.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 04:42:12 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Sharma", "Mohit", ""], ["Harper", "F. Maxwell", ""], ["Karypis", "George", ""]]}, {"id": "1904.12654", "submitter": "Alberto Bailoni", "authors": "Steffen Wolf, Alberto Bailoni, Constantin Pape, Nasim Rahaman, Anna\n  Kreshuk, Ullrich K\\\"othe, Fred A. Hamprecht", "title": "The Mutex Watershed and its Objective: Efficient, Parameter-Free Graph\n  Partitioning", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (2020) 1-1", "doi": "10.1109/TPAMI.2020.2980827", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image partitioning, or segmentation without semantics, is the task of\ndecomposing an image into distinct segments, or equivalently to detect closed\ncontours. Most prior work either requires seeds, one per segment; or a\nthreshold; or formulates the task as multicut / correlation clustering, an\nNP-hard problem. Here, we propose an efficient algorithm for graph\npartitioning, the \"Mutex Watershed''. Unlike seeded watershed, the algorithm\ncan accommodate not only attractive but also repulsive cues, allowing it to\nfind a previously unspecified number of segments without the need for explicit\nseeds or a tunable threshold. We also prove that this simple algorithm solves\nto global optimality an objective function that is intimately related to the\nmulticut / correlation clustering integer linear programming formulation. The\nalgorithm is deterministic, very simple to implement, and has empirically\nlinearithmic complexity. When presented with short-range attractive and\nlong-range repulsive cues from a deep neural network, the Mutex Watershed gives\nthe best results currently known for the competitive ISBI 2012 EM segmentation\nbenchmark.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 17:29:45 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 13:06:08 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wolf", "Steffen", ""], ["Bailoni", "Alberto", ""], ["Pape", "Constantin", ""], ["Rahaman", "Nasim", ""], ["Kreshuk", "Anna", ""], ["K\u00f6the", "Ullrich", ""], ["Hamprecht", "Fred A.", ""]]}, {"id": "1904.12672", "submitter": "Kaifeng Yang", "authors": "Kaifeng Yang, Michael Emmerich, Andr\\'e Deutz, Thomas B\\\"ack", "title": "Efficient Computation of Expected Hypervolume Improvement Using Box\n  Decomposition Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of multi-objective optimization algorithms, multi-objective\nBayesian Global Optimization (MOBGO) is an important branch, in addition to\nevolutionary multi-objective optimization algorithms (EMOAs). MOBGO utilizes\nGaussian Process models learned from previous objective function evaluations to\ndecide the next evaluation site by maximizing or minimizing an infill\ncriterion. A common criterion in MOBGO is the Expected Hypervolume Improvement\n(EHVI), which shows a good performance on a wide range of problems, with\nrespect to exploration and exploitation. However, so far it has been a\nchallenge to calculate exact EHVI values efficiently. In this paper, an\nefficient algorithm for the computation of the exact EHVI for a generic case is\nproposed. This efficient algorithm is based on partitioning the integration\nvolume into a set of axis-parallel slices. Theoretically, the upper bound time\ncomplexities are improved from previously $O (n^2)$ and $O(n^3)$, for two- and\nthree-objective problems respectively, to $\\Theta(n\\log n)$, which is\nasymptotically optimal. This article generalizes the scheme in higher\ndimensional case by utilizing a new hyperbox decomposition technique, which was\nproposed by D{\\\"a}chert et al, EJOR, 2017. It also utilizes a generalization of\nthe multilayered integration scheme that scales linearly in the number of\nhyperboxes of the decomposition. The speed comparison shows that the proposed\nalgorithm in this paper significantly reduces computation time. Finally, this\ndecomposition technique is applied in the calculation of the Probability of\nImprovement (PoI).\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 11:23:26 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 07:31:07 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Yang", "Kaifeng", ""], ["Emmerich", "Michael", ""], ["Deutz", "Andr\u00e9", ""], ["B\u00e4ck", "Thomas", ""]]}, {"id": "1904.12674", "submitter": "Kyungwoo Song", "authors": "Kyungwoo Song, Mingi Ji, Sungrae Park, Il-Chul Moon", "title": "Hierarchical Context enabled Recurrent Neural Network for Recommendation", "comments": null, "journal-ref": "AAAI 2019", "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A long user history inevitably reflects the transitions of personal interests\nover time. The analyses on the user history require the robust sequential model\nto anticipate the transitions and the decays of user interests. The user\nhistory is often modeled by various RNN structures, but the RNN structures in\nthe recommendation system still suffer from the long-term dependency and the\ninterest drifts. To resolve these challenges, we suggest HCRNN with three\nhierarchical contexts of the global, the local, and the temporary interests.\nThis structure is designed to withhold the global long-term interest of users,\nto reflect the local sub-sequence interests, and to attend the temporary\ninterests of each transition. Besides, we propose a hierarchical context-based\ngate structure to incorporate our \\textit{interest drift assumption}. As we\nsuggest a new RNN structure, we support HCRNN with a complementary\n\\textit{bi-channel attention} structure to utilize hierarchical context. We\nexperimented the suggested structure on the sequential recommendation tasks\nwith CiteULike, MovieLens, and LastFM, and our model showed the best\nperformances in the sequential recommendations.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 09:07:55 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Song", "Kyungwoo", ""], ["Ji", "Mingi", ""], ["Park", "Sungrae", ""], ["Moon", "Il-Chul", ""]]}, {"id": "1904.12685", "submitter": "Ranwa Al Mallah", "authors": "Al Mallah Ranwa, Farooq Bilal, Quintero Alejandro", "title": "Distributed Classification of Urban Congestion Using VANET", "comments": "8 pages, transactions, 7 figures", "journal-ref": "IEEE Transactions on Intelligent Transportation Systems, 18(9),\n  2435-2442 (2017)", "doi": "10.1109/TITS.2016.2641903", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicular Ad-hoc NETworks (VANET) can efficiently detect traffic congestion,\nbut detection is not enough because congestion can be further classified as\nrecurrent and non-recurrent congestion (NRC). In particular, NRC in an urban\nnetwork is mainly caused by incidents, workzones, special events and adverse\nweather. We propose a framework for the real-time distributed classification of\ncongestion into its components on a heterogeneous urban road network using\nVANET. We present models built on an understanding of the spatial and temporal\ncausality measures and trained on synthetic data extended from a real case\nstudy of Cologne. Our performance evaluation shows a predictive accuracy of\n87.63\\% for the deterministic Classification Tree (CT), 88.83\\% for the Naive\nBayesian classifier (NB), 89.51\\% for Random Forest (RF) and 89.17\\% for the\nboosting technique. This framework can assist transportation agencies in\nreducing urban congestion by developing effective congestion mitigation\nstrategies knowing the root causes of congestion.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 17:08:01 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 17:05:12 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Ranwa", "Al Mallah", ""], ["Bilal", "Farooq", ""], ["Alejandro", "Quintero", ""]]}, {"id": "1904.12690", "submitter": "Ruairidh Battleday", "authors": "Ruairidh M. Battleday, Joshua C. Peterson, and Thomas L. Griffiths", "title": "Capturing human categorization of natural images at scale by combining\n  deep networks and cognitive models", "comments": "29 pages; 4 figures. arXiv admin note: text overlap with\n  arXiv:1711.04855", "journal-ref": null, "doi": "10.1038/s41467-020-18946-z", "report-no": null, "categories": "cs.CV cs.AI cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human categorization is one of the most important and successful targets of\ncognitive modeling in psychology, yet decades of development and assessment of\ncompeting models have been contingent on small sets of simple, artificial\nexperimental stimuli. Here we extend this modeling paradigm to the domain of\nnatural images, revealing the crucial role that stimulus representation plays\nin categorization and its implications for conclusions about how people form\ncategories. Applying psychological models of categorization to natural images\nrequired two significant advances. First, we conducted the first large-scale\nexperimental study of human categorization, involving over 500,000 human\ncategorization judgments of 10,000 natural images from ten non-overlapping\nobject categories. Second, we addressed the traditional bottleneck of\nrepresenting high-dimensional images in cognitive models by exploring the best\nof current supervised and unsupervised deep and shallow machine learning\nmethods. We find that selecting sufficiently expressive, data-driven\nrepresentations is crucial to capturing human categorization, and using these\nrepresentations allows simple models that represent categories with abstract\nprototypes to outperform the more complex memory-based exemplar accounts of\ncategorization that have dominated in studies using less naturalistic stimuli.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 15:47:59 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Battleday", "Ruairidh M.", ""], ["Peterson", "Joshua C.", ""], ["Griffiths", "Thomas L.", ""]]}, {"id": "1904.12691", "submitter": "Shangtong Zhang", "authors": "Shangtong Zhang, Shimon Whiteson", "title": "DAC: The Double Actor-Critic Architecture for Learning Options", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reformulate the option framework as two parallel augmented MDPs. Under\nthis novel formulation, all policy optimization algorithms can be used off the\nshelf to learn intra-option policies, option termination conditions, and a\nmaster policy over options. We apply an actor-critic algorithm on each\naugmented MDP, yielding the Double Actor-Critic (DAC) architecture.\nFurthermore, we show that, when state-value functions are used as critics, one\ncritic can be expressed in terms of the other, and hence only one critic is\nnecessary. We conduct an empirical study on challenging robot simulation tasks.\nIn a transfer learning setting, DAC outperforms both its hierarchy-free\ncounterpart and previous gradient-based option learning algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 14:57:47 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 08:38:00 GMT"}, {"version": "v3", "created": "Mon, 6 May 2019 15:23:29 GMT"}, {"version": "v4", "created": "Thu, 16 May 2019 14:54:53 GMT"}, {"version": "v5", "created": "Mon, 9 Sep 2019 15:37:19 GMT"}, {"version": "v6", "created": "Tue, 10 Sep 2019 08:29:53 GMT"}, {"version": "v7", "created": "Wed, 11 Sep 2019 08:34:22 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Zhang", "Shangtong", ""], ["Whiteson", "Shimon", ""]]}, {"id": "1904.12694", "submitter": "Bo Kang", "authors": "Bo Kang, Jefrey Lijffijt, Tijl De Bie", "title": "ExplaiNE: An Approach for Explaining Network Embedding-based Link\n  Predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks are powerful data structures, but are challenging to work with for\nconventional machine learning methods. Network Embedding (NE) methods attempt\nto resolve this by learning vector representations for the nodes, for\nsubsequent use in downstream machine learning tasks.\n  Link Prediction (LP) is one such downstream machine learning task that is an\nimportant use case and popular benchmark for NE methods. Unfortunately, while\nNE methods perform exceedingly well at this task, they are lacking in\ntransparency as compared to simpler LP approaches.\n  We introduce ExplaiNE, an approach to offer counterfactual explanations for\nNE-based LP methods, by identifying existing links in the network that explain\nthe predicted links. ExplaiNE is applicable to a broad class of NE algorithms.\nAn extensive empirical evaluation for the NE method `Conditional Network\nEmbedding' in particular demonstrates its accuracy and scalability.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 08:38:02 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Kang", "Bo", ""], ["Lijffijt", "Jefrey", ""], ["De Bie", "Tijl", ""]]}, {"id": "1904.12768", "submitter": "Roy Dong", "authors": "Tyler Westenbroek and Roy Dong and Lillian J. Ratliff and S. Shankar\n  Sastry", "title": "Competitive Statistical Estimation with Strategic Data Sources", "comments": "accepted in the IEEE Transactions on Automatic Control", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, data has played an increasingly important role in the\neconomy as a good in its own right. In many settings, data aggregators cannot\ndirectly verify the quality of the data they purchase, nor the effort exerted\nby data sources when creating the data. Recent work has explored mechanisms to\nensure that the data sources share high quality data with a single data\naggregator, addressing the issue of moral hazard. Oftentimes, there is a\nunique, socially efficient solution.\n  In this paper, we consider data markets where there is more than one data\naggregator. Since data can be cheaply reproduced and transmitted once created,\ndata sources may share the same data with more than one aggregator, leading to\nfree-riding between data aggregators. This coupling can lead to non-uniqueness\nof equilibria and social inefficiency. We examine a particular class of\nmechanisms that have received study recently in the literature, and we\ncharacterize all the generalized Nash equilibria of the resulting data market.\nWe show that, in contrast to the single-aggregator case, there is either\ninfinitely many generalized Nash equilibria or none. We also provide necessary\nand sufficient conditions for all equilibria to be socially inefficient. In our\nanalysis, we identify the components of these mechanisms which give rise to\nthese undesirable outcomes, showing the need for research into mechanisms for\ncompetitive settings with multiple data purchasers and sellers.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 15:26:05 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Westenbroek", "Tyler", ""], ["Dong", "Roy", ""], ["Ratliff", "Lillian J.", ""], ["Sastry", "S. Shankar", ""]]}, {"id": "1904.12770", "submitter": "Mohammed Amer", "authors": "Mohammed Amer, Tom\\'as Maul", "title": "A Review of Modularization Techniques in Artificial Neural Networks", "comments": "Artif Intell Rev (2019)", "journal-ref": null, "doi": "10.1007/s10462-019-09706-7", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks (ANNs) have achieved significant success in\ntackling classical and modern machine learning problems. As learning problems\ngrow in scale and complexity, and expand into multi-disciplinary territory, a\nmore modular approach for scaling ANNs will be needed. Modular neural networks\n(MNNs) are neural networks that embody the concepts and principles of\nmodularity. MNNs adopt a large number of different techniques for achieving\nmodularization. Previous surveys of modularization techniques are relatively\nscarce in their systematic analysis of MNNs, focusing mostly on empirical\ncomparisons and lacking an extensive taxonomical framework. In this review, we\naim to establish a solid taxonomy that captures the essential properties and\nrelationships of the different variants of MNNs. Based on an investigation of\nthe different levels at which modularization techniques act, we attempt to\nprovide a universal and systematic framework for theorists studying MNNs, also\ntrying along the way to emphasise the strengths and weaknesses of different\nmodularization approaches in order to highlight good practices for neural\nnetwork practitioners.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 15:28:14 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Amer", "Mohammed", ""], ["Maul", "Tom\u00e1s", ""]]}, {"id": "1904.12773", "submitter": "Zeyu Ding", "authors": "Zeyu Ding, Yuxin Wang, Danfeng Zhang, Daniel Kifer", "title": "Free Gap Information from the Differentially Private Sparse Vector and\n  Noisy Max Mechanisms", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noisy Max and Sparse Vector are selection algorithms for differential privacy\nand serve as building blocks for more complex algorithms. In this paper we show\nthat both algorithms can release additional information for free (i.e., at no\nadditional privacy cost). Noisy Max is used to return the approximate maximizer\namong a set of queries. We show that it can also release for free the noisy gap\nbetween the approximate maximizer and runner-up. This free information can\nimprove the accuracy of certain subsequent counting queries by up to 50%.\nSparse Vector is used to return a set of queries that are approximately larger\nthan a fixed threshold. We show that it can adaptively control its privacy\nbudget (use less budget for queries that are likely to be much larger than the\nthreshold) in order to increase the amount of queries it can process. These\nresults follow from a careful privacy analysis.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 15:31:13 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 02:30:23 GMT"}, {"version": "v3", "created": "Sun, 15 Sep 2019 16:34:49 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Ding", "Zeyu", ""], ["Wang", "Yuxin", ""], ["Zhang", "Danfeng", ""], ["Kifer", "Daniel", ""]]}, {"id": "1904.12774", "submitter": "Clemens Rosenbaum", "authors": "Clemens Rosenbaum, Ignacio Cases, Matthew Riemer, Tim Klinger", "title": "Routing Networks and the Challenges of Modular and Compositional\n  Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compositionality is a key strategy for addressing combinatorial complexity\nand the curse of dimensionality. Recent work has shown that compositional\nsolutions can be learned and offer substantial gains across a variety of\ndomains, including multi-task learning, language modeling, visual question\nanswering, machine comprehension, and others. However, such models present\nunique challenges during training when both the module parameters and their\ncomposition must be learned jointly. In this paper, we identify several of\nthese issues and analyze their underlying causes. Our discussion focuses on\nrouting networks, a general approach to this problem, and examines empirically\nthe interplay of these challenges and a variety of design decisions. In\nparticular, we consider the effect of how the algorithm decides on module\ncomposition, how the algorithm updates the modules, and if the algorithm uses\nregularization.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 15:32:14 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Rosenbaum", "Clemens", ""], ["Cases", "Ignacio", ""], ["Riemer", "Matthew", ""], ["Klinger", "Tim", ""]]}, {"id": "1904.12787", "submitter": "Yujia Li", "authors": "Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, Pushmeet Kohli", "title": "Graph Matching Networks for Learning the Similarity of Graph Structured\n  Objects", "comments": "Accepted as a conference paper at ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the challenging problem of retrieval and matching of\ngraph structured objects, and makes two key contributions. First, we\ndemonstrate how Graph Neural Networks (GNN), which have emerged as an effective\nmodel for various supervised prediction problems defined on structured data,\ncan be trained to produce embedding of graphs in vector spaces that enables\nefficient similarity reasoning. Second, we propose a novel Graph Matching\nNetwork model that, given a pair of graphs as input, computes a similarity\nscore between them by jointly reasoning on the pair through a new cross-graph\nattention-based matching mechanism. We demonstrate the effectiveness of our\nmodels on different domains including the challenging problem of\ncontrol-flow-graph based function similarity search that plays an important\nrole in the detection of vulnerabilities in software systems. The experimental\nanalysis demonstrates that our models are not only able to exploit structure in\nthe context of similarity learning but they can also outperform domain-specific\nbaseline systems that have been carefully hand-engineered for these problems.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 15:59:04 GMT"}, {"version": "v2", "created": "Sun, 12 May 2019 22:15:33 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Li", "Yujia", ""], ["Gu", "Chenjie", ""], ["Dullien", "Thomas", ""], ["Vinyals", "Oriol", ""], ["Kohli", "Pushmeet", ""]]}, {"id": "1904.12794", "submitter": "Jim Magiera", "authors": "Jim Magiera, Deep Ray, Jan S. Hesthaven, Christian Rohde", "title": "Constraint-Aware Neural Networks for Riemann Problems", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2020.109345", "report-no": null, "categories": "physics.comp-ph math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are increasingly used in complex (data-driven) simulations as\nsurrogates or for accelerating the computation of classical surrogates. In many\napplications physical constraints, such as mass or energy conservation, must be\nsatisfied to obtain reliable results. However, standard machine learning\nalgorithms are generally not tailored to respect such constraints. We propose\ntwo different strategies to generate constraint-aware neural networks. We test\ntheir performance in the context of front-capturing schemes for strongly\nnonlinear wave motion in compressible fluid flow. Precisely, in this context\nso-called Riemann problems have to be solved as surrogates. Their solution\ndescribes the local dynamics of the captured wave front in numerical\nsimulations. Three model problems are considered: a cubic flux model problem,\nan isothermal two-phase flow model, and the Euler equations. We demonstrate\nthat a decrease in the constraint deviation correlates with low discretization\nerrors for all model problems, in addition to the structural advantage of\nfulfilling the constraint.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 16:15:44 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Magiera", "Jim", ""], ["Ray", "Deep", ""], ["Hesthaven", "Jan S.", ""], ["Rohde", "Christian", ""]]}, {"id": "1904.12838", "submitter": "Rahul Kidambi", "authors": "Rong Ge, Sham M. Kakade, Rahul Kidambi and Praneeth Netrapalli", "title": "The Step Decay Schedule: A Near Optimal, Geometrically Decaying Learning\n  Rate Procedure For Least Squares", "comments": "Appears in the proceedings of the Conference on Neural Information\n  Processing Systems (NeurIPS), 2019. 28 pages, 4 tables, 1 Algorithm, 7\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimax optimal convergence rates for classes of stochastic convex\noptimization problems are well characterized, where the majority of results\nutilize iterate averaged stochastic gradient descent (SGD) with polynomially\ndecaying step sizes. In contrast, SGD's final iterate behavior has received\nmuch less attention despite their widespread use in practice. Motivated by this\nobservation, this work provides a detailed study of the following question:\nwhat rate is achievable using the final iterate of SGD for the streaming least\nsquares regression problem with and without strong convexity?\n  First, this work shows that even if the time horizon T (i.e. the number of\niterations SGD is run for) is known in advance, SGD's final iterate behavior\nwith any polynomially decaying learning rate scheme is highly sub-optimal\ncompared to the minimax rate (by a condition number factor in the strongly\nconvex case and a factor of $\\sqrt{T}$ in the non-strongly convex case). In\ncontrast, this paper shows that Step Decay schedules, which cut the learning\nrate by a constant factor every constant number of epochs (i.e., the learning\nrate decays geometrically) offers significant improvements over any\npolynomially decaying step sizes. In particular, the final iterate behavior\nwith a step decay schedule is off the minimax rate by only $log$ factors (in\nthe condition number for strongly convex case, and in T for the non-strongly\nconvex case). Finally, in stark contrast to the known horizon case, this paper\nshows that the anytime (i.e. the limiting) behavior of SGD's final iterate is\npoor (in that it queries iterates with highly sub-optimal function value\ninfinitely often, i.e. in a limsup sense) irrespective of the stepsizes\nemployed. These results demonstrate the subtlety in establishing optimal\nlearning rate schemes (for the final iterate) for stochastic gradient\nprocedures in fixed time horizon settings.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 17:41:27 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 04:04:19 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Ge", "Rong", ""], ["Kakade", "Sham M.", ""], ["Kidambi", "Rahul", ""], ["Netrapalli", "Praneeth", ""]]}, {"id": "1904.12840", "submitter": "Giorgio Patrini", "authors": "Tim van Elsloo, Giorgio Patrini, Hamish Ivey-Law", "title": "SEALion: a Framework for Neural Network Inference on Encrypted Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SEALion: an extensible framework for privacy-preserving machine\nlearning with homomorphic encryption. It allows one to learn deep neural\nnetworks that can be seamlessly utilized for prediction on encrypted data. The\nframework consists of two layers: the first is built upon TensorFlow and SEAL\nand exposes standard algebra and deep learning primitives; the second\nimplements a Keras-like syntax for training and inference with neural networks.\nGiven a required level of security, a user is abstracted from the details of\nthe encoding and the encryption scheme, allowing quick prototyping. We present\ntwo applications that exemplifying the extensibility of our proposal, which are\nalso of independent interest: i) improving efficiency of neural network\ninference by an activity sparsifier and ii) transfer learning by querying a\nserver-side Variational AutoEncoder that can handle encrypted data.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 17:42:25 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["van Elsloo", "Tim", ""], ["Patrini", "Giorgio", ""], ["Ivey-Law", "Hamish", ""]]}, {"id": "1904.12843", "submitter": "Mahyar Najibi", "authors": "Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John Dickerson,\n  Christoph Studer, Larry S. Davis, Gavin Taylor, Tom Goldstein", "title": "Adversarial Training for Free!", "comments": "Accepted to NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training, in which a network is trained on adversarial examples,\nis one of the few defenses against adversarial attacks that withstands strong\nattacks. Unfortunately, the high cost of generating strong adversarial examples\nmakes standard adversarial training impractical on large-scale problems like\nImageNet. We present an algorithm that eliminates the overhead cost of\ngenerating adversarial examples by recycling the gradient information computed\nwhen updating model parameters. Our \"free\" adversarial training algorithm\nachieves comparable robustness to PGD adversarial training on the CIFAR-10 and\nCIFAR-100 datasets at negligible additional cost compared to natural training,\nand can be 7 to 30 times faster than other strong adversarial training methods.\nUsing a single workstation with 4 P100 GPUs and 2 days of runtime, we can train\na robust model for the large-scale ImageNet classification task that maintains\n40% accuracy against PGD attacks. The code is available at\nhttps://github.com/ashafahi/free_adv_train.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 17:50:32 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 21:26:19 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Shafahi", "Ali", ""], ["Najibi", "Mahyar", ""], ["Ghiasi", "Amin", ""], ["Xu", "Zheng", ""], ["Dickerson", "John", ""], ["Studer", "Christoph", ""], ["Davis", "Larry S.", ""], ["Taylor", "Gavin", ""], ["Goldstein", "Tom", ""]]}, {"id": "1904.12847", "submitter": "Xiyang Hu", "authors": "Xiyang Hu, Cynthia Rudin, Margo Seltzer", "title": "Optimal Sparse Decision Trees", "comments": "33rd Conference on Neural Information Processing Systems (NeurIPS\n  2019), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision tree algorithms have been among the most popular algorithms for\ninterpretable (transparent) machine learning since the early 1980's. The\nproblem that has plagued decision tree algorithms since their inception is\ntheir lack of optimality, or lack of guarantees of closeness to optimality:\ndecision tree algorithms are often greedy or myopic, and sometimes produce\nunquestionably suboptimal models. Hardness of decision tree optimization is\nboth a theoretical and practical obstacle, and even careful mathematical\nprogramming approaches have not been able to solve these problems efficiently.\nThis work introduces the first practical algorithm for optimal decision trees\nfor binary variables. The algorithm is a co-design of analytical bounds that\nreduce the search space and modern systems techniques, including data\nstructures and a custom bit-vector library. Our experiments highlight\nadvantages in scalability, speed, and proof of optimality.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 17:56:34 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 04:40:34 GMT"}, {"version": "v3", "created": "Tue, 17 Dec 2019 01:23:37 GMT"}, {"version": "v4", "created": "Tue, 14 Jul 2020 22:42:16 GMT"}, {"version": "v5", "created": "Fri, 18 Sep 2020 00:51:41 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Hu", "Xiyang", ""], ["Rudin", "Cynthia", ""], ["Seltzer", "Margo", ""]]}, {"id": "1904.12848", "submitter": "Qizhe Xie", "authors": "Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, Quoc V. Le", "title": "Unsupervised Data Augmentation for Consistency Training", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning lately has shown much promise in improving deep\nlearning models when labeled data is scarce. Common among recent approaches is\nthe use of consistency training on a large amount of unlabeled data to\nconstrain model predictions to be invariant to input noise. In this work, we\npresent a new perspective on how to effectively noise unlabeled examples and\nargue that the quality of noising, specifically those produced by advanced data\naugmentation methods, plays a crucial role in semi-supervised learning. By\nsubstituting simple noising operations with advanced data augmentation methods\nsuch as RandAugment and back-translation, our method brings substantial\nimprovements across six language and three vision tasks under the same\nconsistency training framework. On the IMDb text classification dataset, with\nonly 20 labeled examples, our method achieves an error rate of 4.20,\noutperforming the state-of-the-art model trained on 25,000 labeled examples. On\na standard semi-supervised learning benchmark, CIFAR-10, our method outperforms\nall previous approaches and achieves an error rate of 5.43 with only 250\nexamples. Our method also combines well with transfer learning, e.g., when\nfinetuning from BERT, and yields improvements in high-data regime, such as\nImageNet, whether when there is only 10% labeled data or when a full labeled\nset with 1.3M extra unlabeled examples is used. Code is available at\nhttps://github.com/google-research/uda.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 17:56:59 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 17:53:48 GMT"}, {"version": "v3", "created": "Thu, 26 Sep 2019 15:32:11 GMT"}, {"version": "v4", "created": "Mon, 30 Sep 2019 15:40:40 GMT"}, {"version": "v5", "created": "Thu, 25 Jun 2020 17:58:43 GMT"}, {"version": "v6", "created": "Thu, 5 Nov 2020 15:11:02 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Xie", "Qizhe", ""], ["Dai", "Zihang", ""], ["Hovy", "Eduard", ""], ["Luong", "Minh-Thang", ""], ["Le", "Quoc V.", ""]]}, {"id": "1904.12857", "submitter": "Quanming Yao", "authors": "Yuanfei Luo and Mengshuo Wang and Hao Zhou and Quanming Yao and WeiWei\n  Tu and Yuqiang Chen and Qiang Yang and Wenyuan Dai", "title": "AutoCross: Automatic Feature Crossing for Tabular Data in Real-World\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature crossing captures interactions among categorical features and is\nuseful to enhance learning from tabular data in real-world businesses. In this\npaper, we present AutoCross, an automatic feature crossing tool provided by\n4Paradigm to its customers, ranging from banks, hospitals, to Internet\ncorporations. By performing beam search in a tree-structured space, AutoCross\nenables efficient generation of high-order cross features, which is not yet\nvisited by existing works. Additionally, we propose successive mini-batch\ngradient descent and multi-granularity discretization to further improve\nefficiency and effectiveness, while ensuring simplicity so that no machine\nlearning expertise or tedious hyper-parameter tuning is required. Furthermore,\nthe algorithms are designed to reduce the computational, transmitting, and\nstorage costs involved in distributed computing. Experimental results on both\nbenchmark and real-world business datasets demonstrate the effectiveness and\nefficiency of AutoCross. It is shown that AutoCross can significantly enhance\nthe performance of both linear and deep models.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 13:05:23 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 13:55:58 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Luo", "Yuanfei", ""], ["Wang", "Mengshuo", ""], ["Zhou", "Hao", ""], ["Yao", "Quanming", ""], ["Tu", "WeiWei", ""], ["Chen", "Yuqiang", ""], ["Yang", "Qiang", ""], ["Dai", "Wenyuan", ""]]}, {"id": "1904.12887", "submitter": "Allison Koenecke", "authors": "Allison Koenecke and Amita Gajewar", "title": "Curriculum Learning in Deep Neural Networks for Financial Forecasting", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-37720-5_2", "report-no": null, "categories": "cs.LG q-fin.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For any financial organization, computing accurate quarterly forecasts for\nvarious products is one of the most critical operations. As the granularity at\nwhich forecasts are needed increases, traditional statistical time series\nmodels may not scale well. We apply deep neural networks in the forecasting\ndomain by experimenting with techniques from Natural Language Processing\n(Encoder-Decoder LSTMs) and Computer Vision (Dilated CNNs), as well as\nincorporating transfer learning. A novel contribution of this paper is the\napplication of curriculum learning to neural network models built for time\nseries forecasting. We illustrate the performance of our models using\nMicrosoft's revenue data corresponding to Enterprise, and Small, Medium &\nCorporate products, spanning approximately 60 regions across the globe for 8\ndifferent business segments, and totaling in the order of tens of billions of\nUSD. We compare our models' performance to the ensemble model of traditional\nstatistics and machine learning techniques currently used by Microsoft Finance.\nWith this in-production model as a baseline, our experiments yield an\napproximately 30% improvement in overall accuracy on test data. We find that\nour curriculum learning LSTM-based model performs best, showing that it is\nreasonable to implement our proposed methods without overfitting on\nmedium-sized data.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 18:09:31 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 02:27:56 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Koenecke", "Allison", ""], ["Gajewar", "Amita", ""]]}, {"id": "1904.12901", "submitter": "Gabriel Dulac-Arnold", "authors": "Gabriel Dulac-Arnold, Daniel Mankowitz, Todd Hester", "title": "Challenges of Real-World Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) has proven its worth in a series of artificial\ndomains, and is beginning to show some successes in real-world scenarios.\nHowever, much of the research advances in RL are often hard to leverage in\nreal-world systems due to a series of assumptions that are rarely satisfied in\npractice. We present a set of nine unique challenges that must be addressed to\nproductionize RL to real world problems. For each of these challenges, we\nspecify the exact meaning of the challenge, present some approaches from the\nliterature, and specify some metrics for evaluating that challenge. An approach\nthat addresses all nine challenges would be applicable to a large number of\nreal world problems. We also present an example domain that has been modified\nto present these challenges as a testbed for practical RL research.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 18:40:15 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Dulac-Arnold", "Gabriel", ""], ["Mankowitz", "Daniel", ""], ["Hester", "Todd", ""]]}, {"id": "1904.12904", "submitter": "Nathan Wycoff", "authors": "Nathan Wycoff, Prasanna Balaprakash, Fangfang Xia", "title": "Neuromorphic Acceleration for Approximate Bayesian Inference on Neural\n  Networks via Permanent Dropout", "comments": "4 pages, 4 figures. Submitted to International Conference on\n  Neuromorphic Systems (ICONS) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As neural networks have begun performing increasingly critical tasks for\nsociety, ranging from driving cars to identifying candidates for drug\ndevelopment, the value of their ability to perform uncertainty quantification\n(UQ) in their predictions has risen commensurately. Permanent dropout, a\npopular method for neural network UQ, involves injecting stochasticity into the\ninference phase of the model and creating many predictions for each of the test\ndata. This shifts the computational and energy burden of deep neural networks\nfrom the training phase to the inference phase. Recent work has demonstrated\nnear-lossless conversion of classical deep neural networks to their spiking\ncounterparts. We use these results to demonstrate the feasibility of conducting\nthe inference phase with permanent dropout on spiking neural networks,\nmitigating the technique's computational and energy burden, which is essential\nfor its use at scale or on edge platforms. We demonstrate the proposed approach\nvia the Nengo spiking neural simulator on a combination drug therapy dataset\nfor cancer treatment, where UQ is critical. Our results indicate that the\nspiking approximation gives a predictive distribution practically\nindistinguishable from that given by the classical network.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 18:43:07 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Wycoff", "Nathan", ""], ["Balaprakash", "Prasanna", ""], ["Xia", "Fangfang", ""]]}, {"id": "1904.12933", "submitter": "Murphy Yuezhen Niu", "authors": "Murphy Yuezhen Niu, Lior Horesh, Isaac Chuang", "title": "Recurrent Neural Networks in the Eye of Differential Equations", "comments": "25pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To understand the fundamental trade-offs between training stability, temporal\ndynamics and architectural complexity of recurrent neural networks~(RNNs), we\ndirectly analyze RNN architectures using numerical methods of ordinary\ndifferential equations~(ODEs). We define a general family of RNNs--the\nODERNNs--by relating the composition rules of RNNs to integration methods of\nODEs at discrete time steps. We show that the degree of RNN's functional\nnonlinearity $n$ and the range of its temporal memory $t$ can be mapped to the\ncorresponding stage of Runge-Kutta recursion and the order of time-derivative\nof the ODEs. We prove that popular RNN architectures, such as LSTM and URNN,\nfit into different orders of $n$-$t$-ODERNNs. This exact correspondence between\nRNN and ODE helps us to establish the sufficient conditions for RNN training\nstability and facilitates more flexible top-down designs of new RNN\narchitectures using large varieties of toolboxes from numerical integration of\nODEs. We provide such an example: Quantum-inspired Universal computing Neural\nNetwork~(QUNN), which reduces the required number of training parameters from\npolynomial in both data length and temporal memory length to only linear in\ntemporal memory length.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 20:16:20 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Niu", "Murphy Yuezhen", ""], ["Horesh", "Lior", ""], ["Chuang", "Isaac", ""]]}, {"id": "1904.12935", "submitter": "Jihun Oh", "authors": "Jihun Oh, Kyunghyun Cho, and Joan Bruna", "title": "Advancing GraphSAGE with A Data-Driven Node Sampling", "comments": "6 pages, 2 tables, ICLR 2019 workshop on Representation Learning on\n  Graphs and Manifolds", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an efficient and scalable graph neural network, GraphSAGE has enabled an\ninductive capability for inferring unseen nodes or graphs by aggregating\nsubsampled local neighborhoods and by learning in a mini-batch gradient descent\nfashion. The neighborhood sampling used in GraphSAGE is effective in order to\nimprove computing and memory efficiency when inferring a batch of target nodes\nwith diverse degrees in parallel. Despite this advantage, the default uniform\nsampling suffers from high variance in training and inference, leading to\nsub-optimum accuracy. We propose a new data-driven sampling approach to reason\nabout the real-valued importance of a neighborhood by a non-linear regressor,\nand to use the value as a criterion for subsampling neighborhoods. The\nregressor is learned using a value-based reinforcement learning. The implied\nimportance for each combination of vertex and neighborhood is inductively\nextracted from the negative classification loss output of GraphSAGE. As a\nresult, in an inductive node classification benchmark using three datasets, our\nmethod enhanced the baseline using the uniform sampling, outperforming recent\nvariants of a graph neural network in accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 20:22:03 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Oh", "Jihun", ""], ["Cho", "Kyunghyun", ""], ["Bruna", "Joan", ""]]}, {"id": "1904.12952", "submitter": "Cristian Alecsa", "authors": "Cristian Daniel Alecsa, Titus Pinta and Imre Boros", "title": "New optimization algorithms for neural network training using operator\n  splitting techniques", "comments": "21 pages, 6 tables, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the following paper we present a new type of optimization algorithms\nadapted for neural network training. These algorithms are based upon sequential\noperator splitting technique for some associated dynamical systems.\nFurthermore, we investigate through numerical simulations the empirical rate of\nconvergence of these iterative schemes toward a local minimum of the loss\nfunction, with some suitable choices of the underlying hyper-parameters. We\nvalidate the convergence of these optimizers using the results of the accuracy\nand of the loss function on the MNIST, MNIST-Fashion and CIFAR 10\nclassification datasets.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 21:29:47 GMT"}, {"version": "v2", "created": "Sat, 15 Jun 2019 13:27:21 GMT"}, {"version": "v3", "created": "Wed, 29 Jan 2020 18:10:21 GMT"}, {"version": "v4", "created": "Thu, 30 Jan 2020 08:33:48 GMT"}, {"version": "v5", "created": "Sat, 21 Mar 2020 14:02:42 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Alecsa", "Cristian Daniel", ""], ["Pinta", "Titus", ""], ["Boros", "Imre", ""]]}, {"id": "1904.12969", "submitter": "Gregory Rehm", "authors": "Gregory B. Rehm, Brooks T. Kuhn, Jimmy Nguyen, Nicholas R. Anderson,\n  Chen-Nee Chuah, Jason Y. Adams", "title": "Improving Mechanical Ventilator Clinical Decision Support Systems with A\n  Machine Learning Classifier for Determining Ventilator Mode", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical decision support systems (CDSS) will play an in-creasing role in\nimproving the quality of medical care for critically ill patients. However, due\nto limitations in current informatics infrastructure, CDSS do not always have\ncom-plete information on state of supporting physiologic monitor-ing devices,\nwhich can limit the input data available to CDSS. This is especially true in\nthe use case of mechanical ventilation (MV), where current CDSS have no\nknowledge of critical ventilation settings, such as ventilation mode. To enable\nMV CDSS to make accurate recommendations related to ventilator mode, we\ndeveloped a highly performant ma-chine learning model that is able to perform\nper-breath clas-sification of 5 of the most widely used ventilation modes in\nthe USA with an average F1-score of 97.52%. We also show how our approach makes\nmethodologic improvements over previous work and that it is highly robust to\nmissing data caused by software/sensor error.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 21:59:03 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Rehm", "Gregory B.", ""], ["Kuhn", "Brooks T.", ""], ["Nguyen", "Jimmy", ""], ["Anderson", "Nicholas R.", ""], ["Chuah", "Chen-Nee", ""], ["Adams", "Jason Y.", ""]]}, {"id": "1904.12973", "submitter": "Gunnar R\\\"atsch", "authors": "Stefan G. Stark, Stephanie L. Hyland, Melanie F. Pradier, Kjong\n  Lehmann, Andreas Wicki, Fernando Perez Cruz, Julia E. Vogt, Gunnar R\\\"atsch", "title": "Unsupervised Extraction of Phenotypes from Cancer Clinical Notes for\n  Association Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent adoption of Electronic Health Records (EHRs) by health care\nproviders has introduced an important source of data that provides detailed and\nhighly specific insights into patient phenotypes over large cohorts. These\ndatasets, in combination with machine learning and statistical approaches,\ngenerate new opportunities for research and clinical care. However, many\nmethods require the patient representations to be in structured formats, while\nthe information in the EHR is often locked in unstructured texts designed for\nhuman readability. In this work, we develop the methodology to automatically\nextract clinical features from clinical narratives from large EHR corpora\nwithout the need for prior knowledge. We consider medical terms and sentences\nappearing in clinical narratives as atomic information units. We propose an\nefficient clustering strategy suitable for the analysis of large text corpora\nand to utilize the clusters to represent information about the patient\ncompactly. To demonstrate the utility of our approach, we perform an\nassociation study of clinical features with somatic mutation profiles from\n4,007 cancer patients and their tumors. We apply the proposed algorithm to a\ndataset consisting of about 65 thousand documents with a total of about 3.2\nmillion sentences. We identify 341 significant statistical associations between\nthe presence of somatic mutations and clinical features. We annotated these\nassociations according to their novelty, and report several known associations.\nWe also propose 32 testable hypotheses where the underlying biological\nmechanism does not appear to be known but plausible. These results illustrate\nthat the automated discovery of clinical features is possible and the joint\nanalysis of clinical and genetic datasets can generate appealing new\nhypotheses.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 22:15:22 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 14:13:56 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Stark", "Stefan G.", ""], ["Hyland", "Stephanie L.", ""], ["Pradier", "Melanie F.", ""], ["Lehmann", "Kjong", ""], ["Wicki", "Andreas", ""], ["Cruz", "Fernando Perez", ""], ["Vogt", "Julia E.", ""], ["R\u00e4tsch", "Gunnar", ""]]}, {"id": "1904.12991", "submitter": "Yiming Sun", "authors": "Yujia Zhang, Kuangyan Song, Yiming Sun, Sarah Tan, Madeleine Udell", "title": "\"Why Should You Trust My Explanation?\" Understanding Uncertainty in LIME\n  Explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for interpreting machine learning black-box models increase the\noutcomes' transparency and in turn generates insight into the reliability and\nfairness of the algorithms. However, the interpretations themselves could\ncontain significant uncertainty that undermines the trust in the outcomes and\nraises concern about the model's reliability. Focusing on the method \"Local\nInterpretable Model-agnostic Explanations\" (LIME), we demonstrate the presence\nof two sources of uncertainty, namely the randomness in its sampling procedure\nand the variation of interpretation quality across different input data points.\nSuch uncertainty is present even in models with high training and test\naccuracy. We apply LIME to synthetic data and two public data sets, text\nclassification in 20 Newsgroup and recidivism risk-scoring in COMPAS, to\nsupport our argument.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 23:49:19 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 07:46:54 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Zhang", "Yujia", ""], ["Song", "Kuangyan", ""], ["Sun", "Yiming", ""], ["Tan", "Sarah", ""], ["Udell", "Madeleine", ""]]}, {"id": "1904.12994", "submitter": "Lalit Jain", "authors": "Jordan S. Ellenberg and Lalit Jain", "title": "Convergence rates for ordinal embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We prove optimal bounds for the convergence rate of ordinal embedding (also\nknown as non-metric multidimensional scaling) in the 1-dimensional case. The\nexamples witnessing optimality of our bounds arise from a result in additive\nnumber theory on sets of integers with no three-term arithmetic progressions.\nWe also carry out some computational experiments aimed at developing a sense of\nwhat the convergence rate for ordinal embedding might look like in higher\ndimensions.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 00:01:18 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Ellenberg", "Jordan S.", ""], ["Jain", "Lalit", ""]]}, {"id": "1904.13000", "submitter": "Florian Tram\\`er", "authors": "Florian Tram\\`er and Dan Boneh", "title": "Adversarial Training and Robustness for Multiple Perturbations", "comments": "Accepted at NeurIPS 2019, 23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Defenses against adversarial examples, such as adversarial training, are\ntypically tailored to a single perturbation type (e.g., small\n$\\ell_\\infty$-noise). For other perturbations, these defenses offer no\nguarantees and, at times, even increase the model's vulnerability. Our aim is\nto understand the reasons underlying this robustness trade-off, and to train\nmodels that are simultaneously robust to multiple perturbation types. We prove\nthat a trade-off in robustness to different types of $\\ell_p$-bounded and\nspatial perturbations must exist in a natural and simple statistical setting.\nWe corroborate our formal analysis by demonstrating similar robustness\ntrade-offs on MNIST and CIFAR10. Building upon new multi-perturbation\nadversarial training schemes, and a novel efficient attack for finding\n$\\ell_1$-bounded adversarial examples, we show that no model trained against\nmultiple attacks achieves robustness competitive with that of models trained on\neach attack individually. In particular, we uncover a pernicious\ngradient-masking phenomenon on MNIST, which causes adversarial training with\nfirst-order $\\ell_\\infty, \\ell_1$ and $\\ell_2$ adversaries to achieve merely\n$50\\%$ accuracy. Our results question the viability and computational\nscalability of extending adversarial robustness, and adversarial training, to\nmultiple perturbation types.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 00:22:29 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 01:53:18 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Tram\u00e8r", "Florian", ""], ["Boneh", "Dan", ""]]}, {"id": "1904.13001", "submitter": "Austin Slakey", "authors": "Austin Slakey, Daniel Salas, Yoni Schamroth", "title": "Encoding Categorical Variables with Conjugate Bayesian Models for WeWork\n  Lead Scoring Engine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applied Data Scientists throughout various industries are commonly faced with\nthe challenging task of encoding high-cardinality categorical features into\ndigestible inputs for machine learning algorithms. This paper describes a\nBayesian encoding technique developed for WeWork's lead scoring engine which\noutputs the probability of a person touring one of our office spaces based on\ninteraction, enrichment, and geospatial data. We present a paradigm for\nensemble modeling which mitigates the need to build complicated preprocessing\nand encoding schemes for categorical variables. In particular, domain-specific\nconjugate Bayesian models are employed as base learners for features in a\nstacked ensemble model. For each column of a categorical feature matrix we fit\na problem-specific prior distribution, for example, the Beta distribution for a\nbinary classification problem. In order to analytically derive the moments of\nthe posterior distribution, we update the prior with the conjugate likelihood\nof the corresponding target variable for each unique value of the given\ncategorical feature. This function of column and value encodes the categorical\nfeature matrix so that the final learner in the ensemble model ingests\nlow-dimensional numerical input. Experimental results on both curated and real\nworld datasets demonstrate impressive accuracy and computational efficiency on\na variety of problem archetypes. Particularly, for the lead scoring engine at\nWeWork -- where some categorical features have as many as 300,000 levels -- we\nhave seen an AUC improvement from 0.87 to 0.97 through implementing conjugate\nBayesian model encoding.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 00:24:06 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Slakey", "Austin", ""], ["Salas", "Daniel", ""], ["Schamroth", "Yoni", ""]]}, {"id": "1904.13007", "submitter": "Zhaofei Yu", "authors": "Yichen Zhang and Shanshan Jia and Yajing Zheng and Zhaofei Yu and\n  Yonghong Tian and Siwei Ma and Tiejun Huang and Jian K. Liu", "title": "Reconstruction of Natural Visual Scenes from Neural Spikes with Deep\n  Neural Networks", "comments": "35 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural coding is one of the central questions in systems neuroscience for\nunderstanding how the brain processes stimulus from the environment, moreover,\nit is also a cornerstone for designing algorithms of brain-machine interface,\nwhere decoding incoming stimulus is highly demanded for better performance of\nphysical devices. Traditionally researchers have focused on functional magnetic\nresonance imaging (fMRI) data as the neural signals of interest for decoding\nvisual scenes. However, our visual perception operates in a fast time scale of\nmillisecond in terms of an event termed neural spike. There are few studies of\ndecoding by using spikes. Here we fulfill this aim by developing a novel\ndecoding framework based on deep neural networks, named spike-image decoder\n(SID), for reconstructing natural visual scenes, including static images and\ndynamic videos, from experimentally recorded spikes of a population of retinal\nganglion cells. The SID is an end-to-end decoder with one end as neural spikes\nand the other end as images, which can be trained directly such that visual\nscenes are reconstructed from spikes in a highly accurate fashion. Our SID also\noutperforms on the reconstruction of visual stimulus compared to existing fMRI\ndecoding models. In addition, with the aid of a spike encoder, we show that SID\ncan be generalized to arbitrary visual scenes by using the image datasets of\nMNIST, CIFAR10, and CIFAR100. Furthermore, with a pre-trained SID, one can\ndecode any dynamic videos to achieve real-time encoding and decoding of visual\nscenes by spikes. Altogether, our results shed new light on neuromorphic\ncomputing for artificial visual systems, such as event-based visual cameras and\nvisual neuroprostheses.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 01:15:24 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 13:04:24 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Zhang", "Yichen", ""], ["Jia", "Shanshan", ""], ["Zheng", "Yajing", ""], ["Yu", "Zhaofei", ""], ["Tian", "Yonghong", ""], ["Ma", "Siwei", ""], ["Huang", "Tiejun", ""], ["Liu", "Jian K.", ""]]}, {"id": "1904.13016", "submitter": "Xin Tong Thomson", "authors": "Xi Chen and Simon S. Du and Xin T. Tong", "title": "On Stationary-Point Hitting Time and Ergodicity of Stochastic Gradient\n  Langevin Dynamics", "comments": "41 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient Langevin dynamics (SGLD) is a fundamental algorithm in\nstochastic optimization. Recent work by Zhang et al. [2017] presents an\nanalysis for the hitting time of SGLD for the first and second order stationary\npoints. The proof in Zhang et al. [2017] is a two-stage procedure through\nbounding the Cheeger's constant, which is rather complicated and leads to loose\nbounds. In this paper, using intuitions from stochastic differential equations,\nwe provide a direct analysis for the hitting times of SGLD to the first and\nsecond order stationary points. Our analysis is straightforward. It only relies\non basic linear algebra and probability theory tools. Our direct analysis also\nleads to tighter bounds comparing to Zhang et al. [2017] and shows the explicit\ndependence of the hitting time on different factors, including dimensionality,\nsmoothness, noise strength, and step size effects. Under suitable conditions,\nwe show that the hitting time of SGLD to first-order stationary points can be\ndimension-independent. Moreover, we apply our analysis to study several\nimportant online estimation problems in machine learning, including linear\nregression, matrix factorization, and online PCA.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 02:03:11 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 03:37:12 GMT"}, {"version": "v3", "created": "Sun, 13 Oct 2019 06:30:29 GMT"}, {"version": "v4", "created": "Sun, 15 Mar 2020 22:42:14 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Chen", "Xi", ""], ["Du", "Simon S.", ""], ["Tong", "Xin T.", ""]]}, {"id": "1904.13032", "submitter": "Ekram Hossain", "authors": "Kazi Ishfaq Ahmed and Ekram Hossain", "title": "A Deep Q-Learning Method for Downlink Power Allocation in Multi-Cell\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal resource allocation is a fundamental challenge for dense and\nheterogeneous wireless networks with massive wireless connections. Because of\nthe non-convex nature of the optimization problem, it is computationally\ndemanding to obtain the optimal resource allocation. Recently, deep\nreinforcement learning (DRL) has emerged as a promising technique in solving\nnon-convex optimization problems. Unlike deep learning (DL), DRL does not\nrequire any optimal/ near-optimal training dataset which is either unavailable\nor computationally expensive in generating synthetic data. In this paper, we\npropose a novel centralized DRL based downlink power allocation scheme for a\nmulti-cell system intending to maximize the total network throughput.\nSpecifically, we apply a deep Q-learning (DQL) approach to achieve near-optimal\npower allocation policy. For benchmarking the proposed approach, we use a\nGenetic Algorithm (GA) to obtain near-optimal power allocation solution.\nSimulation results show that the proposed DRL-based power allocation scheme\nperforms better compared to the conventional power allocation schemes in a\nmulti-cell scenario.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 03:18:39 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Ahmed", "Kazi Ishfaq", ""], ["Hossain", "Ekram", ""]]}, {"id": "1904.13036", "submitter": "Qi Wang", "authors": "Qi Wang, Fahong Zhang and Xuelong Li", "title": "Optimal Clustering Framework for Hyperspectral Band Selection", "comments": null, "journal-ref": "IEEE Trans. Geoscience and Remote Sensing, vol. 56, no. 10, pp.\n  5910-5922, 2018", "doi": "10.1109/TGRS.2018.2828161", "report-no": null, "categories": "eess.IV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Band selection, by choosing a set of representative bands in hyperspectral\nimage (HSI), is an effective method to reduce the redundant information without\ncompromising the original contents. Recently, various unsupervised band\nselection methods have been proposed, but most of them are based on\napproximation algorithms which can only obtain suboptimal solutions toward a\nspecific objective function. This paper focuses on clustering-based band\nselection, and proposes a new framework to solve the above dilemma, claiming\nthe following contributions: 1) An optimal clustering framework (OCF), which\ncan obtain the optimal clustering result for a particular form of objective\nfunction under a reasonable constraint. 2) A rank on clusters strategy (RCS),\nwhich provides an effective criterion to select bands on existing clustering\nstructure. 3) An automatic method to determine the number of the required\nbands, which can better evaluate the distinctive information produced by\ncertain number of bands. In experiments, the proposed algorithm is compared to\nsome state-of-the-art competitors. According to the experimental results, the\nproposed algorithm is robust and significantly outperform the other methods on\nvarious data sets.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 03:26:44 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Wang", "Qi", ""], ["Zhang", "Fahong", ""], ["Li", "Xuelong", ""]]}, {"id": "1904.13052", "submitter": "Haiping Huang", "authors": "Tianqi Hou, and K.Y. Michael Wong, and Haiping Huang", "title": "Minimal model of permutation symmetry in unsupervised learning", "comments": "36 pages, 110 equations, 5 figures; a complete picture about physical\n  laws of unsupervised learning in neural networks presented", "journal-ref": "2019 J. Phys. A: Math. Theor. 52 414001", "doi": "10.1088/1751-8121/ab3f3f", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Permutation of any two hidden units yields invariant properties in typical\ndeep generative neural networks. This permutation symmetry plays an important\nrole in understanding the computation performance of a broad class of neural\nnetworks with two or more hidden units. However, a theoretical study of the\npermutation symmetry is still lacking. Here, we propose a minimal model with\nonly two hidden units in a restricted Boltzmann machine, which aims to address\nhow the permutation symmetry affects the critical learning data size at which\nthe concept-formation (or spontaneous symmetry breaking in physics language)\nstarts, and moreover semi-rigorously prove a conjecture that the critical data\nsize is independent of the number of hidden units once this number is finite.\nRemarkably, we find that the embedded correlation between two receptive fields\nof hidden units reduces the critical data size. In particular, the\nweakly-correlated receptive fields have the benefit of significantly reducing\nthe minimal data size that triggers the transition, given less noisy data.\nInspired by the theory, we also propose an efficient fully-distributed\nalgorithm to infer the receptive fields of hidden units. Furthermore, our\nminimal model reveals that the permutation symmetry can also be spontaneously\nbroken following the spontaneous symmetry breaking. Overall, our results\ndemonstrate that the unsupervised learning is a progressive combination of\nspontaneous symmetry breaking and permutation symmetry breaking which are both\nspontaneous processes driven by data streams (observations). All these effects\ncan be analytically probed based on the minimal model, providing theoretical\ninsights towards understanding unsupervised learning in a more general context.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 05:08:14 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 01:31:58 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Hou", "Tianqi", ""], ["Wong", "K. Y. Michael", ""], ["Huang", "Haiping", ""]]}, {"id": "1904.13081", "submitter": "Bhaskar Mukhoty", "authors": "Bhaskar Pratim Mukhoty, Vikas Maurya, Sandeep Kumar Shukla", "title": "Sequence to sequence deep learning models for solar irradiation\n  forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The energy output a photo voltaic(PV) panel is a function of solar\nirradiation and weather parameters like temperature and wind speed etc. A\ngeneral measure for solar irradiation called Global Horizontal Irradiance\n(GHI), customarily reported in Watt/meter$^2$, is a generic indicator for this\nintermittent energy resource. An accurate prediction of GHI is necessary for\nreliable grid integration of the renewable as well as for power market trading.\nWhile some machine learning techniques are well introduced along with the\ntraditional time-series forecasting techniques, deep-learning techniques\nremains less explored for the task at hand. In this paper we give deep learning\nmodels suitable for sequence to sequence prediction of GHI. The deep learning\nmodels are reported for short-term forecasting $\\{1-24\\}$ hour along with the\nstate-of-the art techniques like Gradient Boosted Regression Trees(GBRT) and\nFeed Forward Neural Networks(FFNN).\n  We have checked that spatio-temporal features like wind direction, wind speed\nand GHI of neighboring location improves the prediction accuracy of the deep\nlearning models significantly. Among the various sequence-to-sequence\nencoder-decoder models LSTM performed superior, handling short-comings of the\nstate-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 07:28:33 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Mukhoty", "Bhaskar Pratim", ""], ["Maurya", "Vikas", ""], ["Shukla", "Sandeep Kumar", ""]]}, {"id": "1904.13094", "submitter": "Francesco Crecchi", "authors": "Francesco Crecchi, Davide Bacciu and Battista Biggio", "title": "Detecting Adversarial Examples through Nonlinear Dimensionality\n  Reduction", "comments": "European Symposium on Artificial Neural Networks, Computational\n  Intelligence and Machine Learning (ESANN) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are vulnerable to adversarial examples, i.e.,\ncarefully-perturbed inputs aimed to mislead classification. This work proposes\na detection method based on combining non-linear dimensionality reduction and\ndensity estimation techniques. Our empirical findings show that the proposed\napproach is able to effectively detect adversarial examples crafted by\nnon-adaptive attackers, i.e., not specifically tuned to bypass the detection\nmethod. Given our promising results, we plan to extend our analysis to adaptive\nattackers in future work.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 07:59:52 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 17:30:58 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Crecchi", "Francesco", ""], ["Bacciu", "Davide", ""], ["Biggio", "Battista", ""]]}, {"id": "1904.13107", "submitter": "Yao Ma", "authors": "Yao Ma, Suhang Wang, Charu C. Aggarwal, Jiliang Tang", "title": "Graph Convolutional Networks with EigenPooling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks, which generalize deep neural network models to graph\nstructured data, have attracted increasing attention in recent years. They\nusually learn node representations by transforming, propagating and aggregating\nnode features and have been proven to improve the performance of many graph\nrelated tasks such as node classification and link prediction. To apply graph\nneural networks for the graph classification task, approaches to generate the\n\\textit{graph representation} from node representations are demanded. A common\nway is to globally combine the node representations. However, rich structural\ninformation is overlooked. Thus a hierarchical pooling procedure is desired to\npreserve the graph structure during the graph representation learning. There\nare some recent works on hierarchically learning graph representation analogous\nto the pooling step in conventional convolutional neural (CNN) networks.\nHowever, the local structural information is still largely neglected during the\npooling process. In this paper, we introduce a pooling operator $\\pooling$\nbased on graph Fourier transform, which can utilize the node features and local\nstructures during the pooling process. We then design pooling layers based on\nthe pooling operator, which are further combined with traditional GCN\nconvolutional layers to form a graph neural network framework $\\m$ for graph\nclassification. Theoretical analysis is provided to understand $\\pooling$ from\nboth local and global perspectives. Experimental results of the graph\nclassification task on $6$ commonly used benchmarks demonstrate the\neffectiveness of the proposed framework.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 08:57:54 GMT"}, {"version": "v2", "created": "Sat, 18 May 2019 04:20:13 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Ma", "Yao", ""], ["Wang", "Suhang", ""], ["Aggarwal", "Charu C.", ""], ["Tang", "Jiliang", ""]]}, {"id": "1904.13111", "submitter": "Francesco Curia", "authors": "Francesco Curia", "title": "Restricted Boltzmann Machine Assignment Algorithm: Application to solve\n  many-to-one matching problems on weighted bipartite graph", "comments": "In this version is was update the thresholds determination", "journal-ref": "Annals of Data Science (April 2019)", "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work an iterative algorithm based on unsupervised learning is\npresented, specifically on a Restricted Boltzmann Machine (RBM) to solve a\nperfect matching problem on a bipartite weighted graph. Iteratively is\ncalculated the weights $w_{ij}$ and the bias parameters $\\theta = ( a_i, b_j) $\nthat maximize the energy function and assignment element $i$ to element $j$. An\napplication of real problem is presented to show the potentiality of this\nalgorithm.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 09:06:55 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 12:28:31 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Curia", "Francesco", ""]]}, {"id": "1904.13113", "submitter": "Xu Yang", "authors": "Xu Yang, Cheng Deng, Feng Zheng, Junchi Yan, Wei Liu", "title": "Deep Spectral Clustering using Dual Autoencoder Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The clustering methods have recently absorbed even-increasing attention in\nlearning and vision. Deep clustering combines embedding and clustering together\nto obtain optimal embedding subspace for clustering, which can be more\neffective compared with conventional clustering methods. In this paper, we\npropose a joint learning framework for discriminative embedding and spectral\nclustering. We first devise a dual autoencoder network, which enforces the\nreconstruction constraint for the latent representations and their noisy\nversions, to embed the inputs into a latent space for clustering. As such the\nlearned latent representations can be more robust to noise. Then the mutual\ninformation estimation is utilized to provide more discriminative information\nfrom the inputs. Furthermore, a deep spectral clustering method is applied to\nembed the latent representations into the eigenspace and subsequently clusters\nthem, which can fully exploit the relationship between inputs to achieve\noptimal clustering results. Experimental results on benchmark datasets show\nthat our method can significantly outperform state-of-the-art clustering\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 09:12:22 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Yang", "Xu", ""], ["Deng", "Cheng", ""], ["Zheng", "Feng", ""], ["Yan", "Junchi", ""], ["Liu", "Wei", ""]]}, {"id": "1904.13127", "submitter": "Brais Cancela", "authors": "Brais Cancela, Ver\\'onica Bol\\'on-Canedo, Amparo Alonso-Betanzos,\n  Jo\\~ao Gama", "title": "A scalable saliency-based Feature selection method with instance level\n  information", "comments": null, "journal-ref": null, "doi": "10.1016/j.knosys.2020.105885", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classic feature selection techniques remove those features that are either\nirrelevant or redundant, achieving a subset of relevant features that help to\nprovide a better knowledge extraction. This allows the creation of compact\nmodels that are easier to interpret. Most of these techniques work over the\nwhole dataset, but they are unable to provide the user with successful\ninformation when only instance information is needed. In short, given any\nexample, classic feature selection algorithms do not give any information about\nwhich the most relevant information is, regarding this sample. This work aims\nto overcome this handicap by developing a novel feature selection method,\ncalled Saliency-based Feature Selection (SFS), based in deep-learning saliency\ntechniques. Our experimental results will prove that this algorithm can be\nsuccessfully used not only in Neural Networks, but also under any given\narchitecture trained by using Gradient Descent techniques.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 09:54:13 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Cancela", "Brais", ""], ["Bol\u00f3n-Canedo", "Ver\u00f3nica", ""], ["Alonso-Betanzos", "Amparo", ""], ["Gama", "Jo\u00e3o", ""]]}, {"id": "1904.13142", "submitter": "Chien-Feng Liao", "authors": "Chien-Feng Liao, Yu Tsao, Xugang Lu, Hisashi Kawai", "title": "Incorporating Symbolic Sequential Modeling for Speech Enhancement", "comments": "Accepted to Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a noisy environment, a lossy speech signal can be automatically restored\nby a listener if he/she knows the language well. That is, with the built-in\nknowledge of a \"language model\", a listener may effectively suppress noise\ninterference and retrieve the target speech signals. Accordingly, we argue that\nfamiliarity with the underlying linguistic content of spoken utterances\nbenefits speech enhancement (SE) in noisy environments. In this study, in\naddition to the conventional modeling for learning the acoustic noisy-clean\nspeech mapping, an abstract symbolic sequential modeling is incorporated into\nthe SE framework. This symbolic sequential modeling can be regarded as a\n\"linguistic constraint\" in learning the acoustic noisy-clean speech mapping\nfunction. In this study, the symbolic sequences for acoustic signals are\nobtained as discrete representations with a Vector Quantized Variational\nAutoencoder algorithm. The obtained symbols are able to capture high-level\nphoneme-like content from speech signals. The experimental results demonstrate\nthat the proposed framework can obtain notable performance improvement in terms\nof perceptual evaluation of speech quality (PESQ) and short-time objective\nintelligibility (STOI) on the TIMIT dataset.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 10:31:22 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 15:04:02 GMT"}, {"version": "v3", "created": "Tue, 2 Jul 2019 02:38:05 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Liao", "Chien-Feng", ""], ["Tsao", "Yu", ""], ["Lu", "Xugang", ""], ["Kawai", "Hisashi", ""]]}, {"id": "1904.13179", "submitter": "Shuhan Tan", "authors": "Shuhan Tan, Jiening Jiao, Wei-Shi Zheng", "title": "Weakly Supervised Open-set Domain Adaptation by Dual-domain\n  Collaboration", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In conventional domain adaptation, a critical assumption is that there exists\na fully labeled domain (source) that contains the same label space as another\nunlabeled or scarcely labeled domain (target). However, in the real world,\nthere often exist application scenarios in which both domains are partially\nlabeled and not all classes are shared between these two domains. Thus, it is\nmeaningful to let partially labeled domains learn from each other to classify\nall the unlabeled samples in each domain under an open-set setting. We consider\nthis problem as weakly supervised open-set domain adaptation. To address this\npractical setting, we propose the Collaborative Distribution Alignment (CDA)\nmethod, which performs knowledge transfer bilaterally and works collaboratively\nto classify unlabeled data and identify outlier samples. Extensive experiments\non the Office benchmark and an application on person reidentification show that\nour method achieves state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 11:54:19 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Tan", "Shuhan", ""], ["Jiao", "Jiening", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "1904.13195", "submitter": "Maxime Cordy", "authors": "Wei Ma, Mike Papadakis, Anestis Tsakmalis, Maxime Cordy, Yves Le Traon", "title": "Test Selection for Deep Learning Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing of deep learning models is challenging due to the excessive number\nand complexity of computations involved. As a result, test data selection is\nperformed manually and in an ad hoc way. This raises the question of how we can\nautomatically select candidate test data to test deep learning models. Recent\nresearch has focused on adapting test selection metrics from code-based\nsoftware testing (such as coverage) to deep learning. However, deep learning\nmodels have different attributes from code such as spread of computations\nacross the entire network reflecting training data properties, balance of\nneuron weights and redundancy (use of many more neurons than needed). Such\ndifferences make code-based metrics inappropriate to select data that can\nchallenge the models (can trigger misclassification). We thus propose a set of\ntest selection metrics based on the notion of model uncertainty (model\nconfidence on specific inputs). Intuitively, the more uncertain we are about a\ncandidate sample, the more likely it is that this sample triggers a\nmisclassification. Similarly, the samples for which we are the most uncertain,\nare the most informative and should be used to improve the model by retraining.\nWe evaluate these metrics on two widely-used image classification problems\ninvolving real and artificial (adversarial) data. We show that\nuncertainty-based metrics have a strong ability to select data that are\nmisclassified and lead to major improvement in classification accuracy during\nretraining: up to 80% more gain than random selection and other\nstate-of-the-art metrics on one dataset and up to 29% on the other.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 12:44:10 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Ma", "Wei", ""], ["Papadakis", "Mike", ""], ["Tsakmalis", "Anestis", ""], ["Cordy", "Maxime", ""], ["Traon", "Yves Le", ""]]}, {"id": "1904.13197", "submitter": "James Bocinsky", "authors": "James Bocinsky, Connor McCurley, Daniel Shats, and Alina Zare", "title": "Investigation of Initialization Strategies for the Multiple Instance\n  Adaptive Cosine Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensors which use electromagnetic induction (EMI) to excite a response in\nconducting bodies have long been investigated for subsurface explosive hazard\ndetection. In particular, EMI sensors have been used to discriminate between\ndifferent types of objects, and to detect objects with low metal content. One\nsuccessful, previously investigated approach is the Multiple Instance Adaptive\nCosine Estimator (MI-ACE). In this paper, a number of new initialization\ntechniques for MI-ACE are proposed and evaluated using their respective\nperformance and speed. The cross validated learned signatures, as well as\nlearned background statistics, are used with Adaptive Cosine Estimator (ACE) to\ngenerate confidence maps, which are clustered into alarms. Alarms are scored\nagainst a ground truth and the initialization approaches are compared.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 12:45:18 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Bocinsky", "James", ""], ["McCurley", "Connor", ""], ["Shats", "Daniel", ""], ["Zare", "Alina", ""]]}, {"id": "1904.13221", "submitter": "Emad Ul Haq Qazi", "authors": "Saeed Bamatraf, Muhammad Hussain, Emad-ul-Haq Qazi and Hatim Aboalsamh", "title": "Eigen Values Features for the Classification of Brain Signals\n  corresponding to 2D and 3D Educational Contents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we have proposed a brain signal classification method, which\nuses eigenvalues of the covariance matrix as features to classify images\n(topomaps) created from the brain signals. The signals are recorded during the\nanswering of 2D and 3D questions. The system is used to classify the correct\nand incorrect answers for both 2D and 3D questions. Using the classification\ntechnique, the impacts of 2D and 3D multimedia educational contents on\nlearning, memory retention and recall will be compared. The subjects learn\nsimilar 2D and 3D educational contents. Afterwards, subjects are asked 20\nmultiple-choice questions (MCQs) associated with the contents after thirty\nminutes (Short-Term Memory) and two months (Long-Term Memory). Eigenvalues\nfeatures extracted from topomaps images are given to K-Nearest Neighbor (KNN)\nand Support Vector Machine (SVM) classifiers, in order to identify the states\nof the brain related to incorrect and correct answers. Excellent accuracies\nobtained by both classifiers and by applying statistical analysis on the\nresults, no significant difference is indicated between 2D and 3D multimedia\neducational contents on learning, memory retention and recall in both STM and\nLTM.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 13:32:00 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Bamatraf", "Saeed", ""], ["Hussain", "Muhammad", ""], ["Qazi", "Emad-ul-Haq", ""], ["Aboalsamh", "Hatim", ""]]}, {"id": "1904.13223", "submitter": "Morteza Haghir Chehreghani", "authors": "Morteza Haghir Chehreghani", "title": "Unsupervised Representation Learning with Minimax Distance Measures", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the use of Minimax distances to extract in a nonparametric way\nthe features that capture the unknown underlying patterns and structures in the\ndata. We develop a general-purpose and computationally efficient framework to\nemploy Minimax distances with many machine learning methods that perform on\nnumerical data. We study both computing the pairwise Minimax distances for all\npairs of objects and as well as computing the Minimax distances of all the\nobjects to/from a fixed (test) object.\n  We first efficiently compute the pairwise Minimax distances between the\nobjects, using the equivalence of Minimax distances over a graph and over a\nminimum spanning tree constructed on that. Then, we perform an embedding of the\npairwise Minimax distances into a new vector space, such that their squared\nEuclidean distances in the new space equal to the pairwise Minimax distances in\nthe original space. We also study the case of having multiple pairwise Minimax\nmatrices, instead of a single one. Thereby, we propose an embedding via first\nsumming up the centered matrices and then performing an eigenvalue\ndecomposition to obtain the relevant features.\n  In the following, we study computing Minimax distances from a fixed (test)\nobject which can be used for instance in K-nearest neighbor search. Similar to\nthe case of all-pair pairwise Minimax distances, we develop an efficient and\ngeneral-purpose algorithm that is applicable with any arbitrary base distance\nmeasure. Moreover, we investigate in detail the edges selected by the Minimax\ndistances and thereby explore the ability of Minimax distances in detecting\noutlier objects.\n  Finally, for each setting, we perform several experiments to demonstrate the\neffectiveness of our framework.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 16:13:08 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 12:08:40 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Chehreghani", "Morteza Haghir", ""]]}, {"id": "1904.13228", "submitter": "Emad Ul Haq Qazi", "authors": "Emad-ul-Haq Qazi, Muhammad Hussain and Hatim Aboalsamh", "title": "An Efficient Intelligent System for the Classification of\n  Electroencephalography (EEG) Brain Signals using Nuclear Features for Human\n  Cognitive Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation and classification of Electroencephalography (EEG) brain\nsignals are critical processes for their analysis in cognitive tasks.\nParticularly, extraction of discriminative features from raw EEG signals,\nwithout any pre-processing, is a challenging task. Motivated by nuclear norm,\nwe observed that there is a significant difference between the variances of EEG\nsignals captured from the same brain region when a subject performs different\ntasks. This observation lead us to use singular value decomposition for\ncomputing dominant variances of EEG signals captured from a certain brain\nregion while performing a certain task and use them as features (nuclear\nfeatures). A simple and efficient class means based minimum distance classifier\n(CMMDC) is enough to predict brain states. This approach results in the feature\nspace of significantly small dimension and gives equally good classification\nresults on clean as well as raw data. We validated the effectiveness and\nrobustness of the technique using four datasets of different tasks: fluid\nintelligence clean data (FICD), fluid intelligence raw data (FIRD), memory\nrecall task (MRT), and eyes open / eyes closed task (EOEC). For each task, we\nanalyzed EEG signals over six (06) different brain regions with 8, 16, 20, 18,\n18 and 100 electrodes. The nuclear features from frontal brain region gave the\n100% prediction accuracy. The discriminant analysis of the nuclear features has\nbeen conducted using intra-class and inter-class variations. Comparisons with\nthe state-of-the-art techniques showed the superiority of the proposed system.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 13:38:04 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Qazi", "Emad-ul-Haq", ""], ["Hussain", "Muhammad", ""], ["Aboalsamh", "Hatim", ""]]}, {"id": "1904.13233", "submitter": "Daniel Cunnington", "authors": "Daniel Cunnington, Graham White, Geeth de Mel", "title": "Synthetic Ground Truth Generation for Evaluating Generative Policy\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Policy-based Models aim to enable a coalition of systems, be they\ndevices or services to adapt according to contextual changes such as\nenvironmental factors, user preferences and different tasks whilst adhering to\nvarious constraints and regulations as directed by a managing party or the\ncollective vision of the coalition. Recent developments have proposed new\narchitectures to realize the potential of GPMs but as the complexity of systems\nand their associated requirements increases, there is an emerging requirement\nto have scenarios and associated datasets to realistically evaluate GPMs with\nrespect to the properties of the operating environment, be it the future\nbattlespace or an autonomous organization. In order to address this\nrequirement, in this paper, we present a method of applying an agile knowledge\nrepresentation framework to model requirements, both individualistic and\ncollective that enables synthetic generation of ground truth data such that\nadvanced GPMs can be evaluated robustly in complex environments. We also\nrelease conceptual models, annotated datasets, as well as means to extend the\ndata generation approach so that similar datasets can be developed for varying\ncomplexities and different situations.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 14:41:58 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Cunnington", "Daniel", ""], ["White", "Graham", ""], ["de Mel", "Geeth", ""]]}, {"id": "1904.13234", "submitter": "Emad Ul Haq Qazi", "authors": "Emad-ul-Haq Qazi, Muhammad Hussain, Hatim AboAlsamh, Ihsan Ullah", "title": "Automatic Emotion Recognition (AER) System based on Two-Level Ensemble\n  of Lightweight Deep CNN Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotions play a crucial role in human interaction, health care and security\ninvestigations and monitoring. Automatic emotion recognition (AER) using\nelectroencephalogram (EEG) signals is an effective method for decoding the real\nemotions, which are independent of body gestures, but it is a challenging\nproblem. Several automatic emotion recognition systems have been proposed,\nwhich are based on traditional hand-engineered approaches and their\nperformances are very poor. Motivated by the outstanding performance of deep\nlearning (DL) in many recognition tasks, we introduce an AER system (Deep-AER)\nbased on EEG brain signals using DL. A DL model involves a large number of\nlearnable parameters, and its training needs a large dataset of EEG signals,\nwhich is difficult to acquire for AER problem. To overcome this problem, we\nproposed a lightweight pyramidal one-dimensional convolutional neural network\n(LP-1D-CNN) model, which involves a small number of learnable parameters. Using\nLP-1D-CNN, we build a two level ensemble model. In the first level of the\nensemble, each channel is scanned incrementally by LP-1D-CNN to generate\npredictions, which are fused using majority vote. The second level of the\nensemble combines the predictions of all channels of an EEG signal using\nmajority vote for detecting the emotion state. We validated the effectiveness\nand robustness of Deep-AER using DEAP, a benchmark dataset for emotion\nrecognition research. The results indicate that FRONT plays dominant role in\nAER and over this region, Deep-AER achieved the accuracies of 98.43% and 97.65%\nfor two AER problems, i.e., high valence vs low valence (HV vs LV) and high\narousal vs low arousal (HA vs LA), respectively. The comparison reveals that\nDeep-AER outperforms the state-of-the-art systems with large margin. The\nDeep-AER system will be helpful in monitoring for health care and security\ninvestigations.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 13:43:14 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Qazi", "Emad-ul-Haq", ""], ["Hussain", "Muhammad", ""], ["AboAlsamh", "Hatim", ""], ["Ullah", "Ihsan", ""]]}, {"id": "1904.13241", "submitter": "Soheil Mehrabkhani", "authors": "Soheil Mehrabkhani", "title": "Fourier Transform Approach to Machine Learning II: Fourier Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Fourier-based approach for optimization of several clustering\nalgorithms. Mathematically, clusters data can be described by a density\nfunction represented by the Dirac mixture distribution. The density function\ncan be smoothed by applying the Fourier transform and a Gaussian filter. The\ndetermination of the optimal standard deviation of the Gaussian filter will be\naccomplished by the use of a convergence criterion related to the correlation\nbetween the smoothed and the original density functions. In principle, the\noptimal smoothed density function exhibits local maxima, which correspond to\nthe cluster centroids. Thus, the complex task of finding the centroids of the\nclusters is simplified by the detection of the peaks of the smoothed density\nfunction. A multiple sliding windows procedure is used to detect the peaks. The\nremarkable accuracy of the proposed algorithm demonstrates its capability as a\nreliable general method for enhancement of the clustering performance, its\nglobal optimization and also removing the initialization problem in many\nclustering methods.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 03:12:32 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 16:10:03 GMT"}, {"version": "v3", "created": "Sun, 22 Sep 2019 09:37:07 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Mehrabkhani", "Soheil", ""]]}, {"id": "1904.13247", "submitter": "Durdane Kocacoban", "authors": "Durdane Kocacoban, James Cussens", "title": "Online Causal Structure Learning in the Presence of Latent Variables", "comments": "16 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two online causal structure learning algorithms which can track\nchanges in a causal structure and process data in a dynamic real-time manner.\nStandard causal structure learning algorithms assume that causal structure does\nnot change during the data collection process, but in real-world scenarios, it\ndoes often change. Therefore, it is inappropriate to handle such changes with\nexisting batch-learning approaches, and instead, a structure should be learned\nin an online manner. The online causal structure learning algorithms we present\nhere can revise correlation values without reprocessing the entire dataset and\nuse an existing model to avoid relearning the causal links in the prior model,\nwhich still fit data. Proposed algorithms are tested on synthetic and\nreal-world datasets, the latter being a seasonally adjusted commodity price\nindex dataset for the U.S. The online causal structure learning algorithms\noutperformed standard FCI by a large margin in learning the changed causal\nstructure correctly and efficiently when latent variables were present.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 13:49:43 GMT"}, {"version": "v2", "created": "Sun, 14 Jul 2019 18:17:34 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Kocacoban", "Durdane", ""], ["Cussens", "James", ""]]}, {"id": "1904.13255", "submitter": "Kacper Kielak", "authors": "Kacper Kielak", "title": "Generative Adversarial Imagination for Sample Efficient Deep\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reinforcement learning has seen great advancements in the past five years.\nThe successful introduction of deep learning in place of more traditional\nmethods allowed reinforcement learning to scale to very complex domains\nachieving super-human performance in environments like the game of Go or\nnumerous video games. Despite great successes in multiple domains, these new\nmethods suffer from their own issues that make them often inapplicable to the\nreal world problems. Extreme lack of data efficiency, together with huge\nvariance and difficulty in enforcing safety constraints, is one of the three\nmost prominent issues in the field. Usually, millions of data points sampled\nfrom the environment are necessary for these algorithms to converge to\nacceptable policies.\n  This thesis proposes novel Generative Adversarial Imaginative Reinforcement\nLearning algorithm. It takes advantage of the recent introduction of highly\neffective generative adversarial models, and Markov property that underpins\nreinforcement learning setting, to model dynamics of the real environment\nwithin the internal imagination module. Rollouts from the imagination are then\nused to artificially simulate the real environment in a standard reinforcement\nlearning process to avoid, often expensive and dangerous, trial and error in\nthe real environment. Experimental results show that the proposed algorithm\nmore economically utilises experience from the real environment than the\ncurrent state-of-the-art Rainbow DQN algorithm, and thus makes an important\nstep towards sample efficient deep reinforcement learning.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 13:53:29 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 18:34:54 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Kielak", "Kacper", ""]]}, {"id": "1904.13262", "submitter": "Gauthier Gidel", "authors": "Gauthier Gidel, Francis Bach and Simon Lacoste-Julien", "title": "Implicit Regularization of Discrete Gradient Dynamics in Linear Neural\n  Networks", "comments": "19 pages, to appear in NeurIPS 2019 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When optimizing over-parameterized models, such as deep neural networks, a\nlarge set of parameters can achieve zero training error. In such cases, the\nchoice of the optimization algorithm and its respective hyper-parameters\nintroduces biases that will lead to convergence to specific minimizers of the\nobjective. Consequently, this choice can be considered as an implicit\nregularization for the training of over-parametrized models. In this work, we\npush this idea further by studying the discrete gradient dynamics of the\ntraining of a two-layer linear network with the least-squares loss. Using a\ntime rescaling, we show that, with a vanishing initialization and a small\nenough step size, this dynamics sequentially learns the solutions of a\nreduced-rank regression with a gradually increasing rank.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 14:06:05 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 17:02:21 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Gidel", "Gauthier", ""], ["Bach", "Francis", ""], ["Lacoste-Julien", "Simon", ""]]}, {"id": "1904.13304", "submitter": "Youngjin Kim", "authors": "Youngjin Kim", "title": "A supervised-learning-based strategy for optimal demand response of an\n  HVAC System", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large thermal capacity of buildings enables heating, ventilating, and\nair-conditioning (HVAC) systems to be exploited as demand response (DR)\nresources. Optimal DR of HVAC units is challenging, particularly for multi-zone\nbuildings, because this requires detailed physics-based models of zonal\ntemperature variations for HVAC system operation and building thermal\nconditions. This paper proposes a new strategy for optimal DR of an HVAC system\nin a multi-zone building, based on supervised learning (SL). Artificial neural\nnetworks (ANNs) are trained with data obtained under normal building operating\nconditions. The ANNs are replicated using piecewise linear equations, which are\nexplicitly integrated into an optimal scheduling problem for price-based DR.\nThe optimization problem is solved for various electricity prices and building\nthermal conditions. The solutions are further used to train a deep neural\nnetwork (DNN) to directly determine the optimal DR schedule, referred to here\nas supervised-learning-aided meta-prediction (SLAMP). Case studies are\nperformed using three different methods: explicit ANN replication (EAR), SLAMP,\nand physics-based modeling. The case study results verify the effectiveness of\nthe proposed SL-based strategy, in terms of both practical applicability and\ncomputational time, while also ensuring the thermal comfort of occupants and\ncost-effective operation of the HVAC system.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 04:37:50 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Kim", "Youngjin", ""]]}, {"id": "1904.13323", "submitter": "Kamen Brestnichki", "authors": "Francisco Vargas, Kamen Brestnichki, Nils Hammerla", "title": "Model Comparison for Semantic Grouping", "comments": "Proceedings of the 36th International Conference on Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a probabilistic framework for quantifying the semantic\nsimilarity between two groups of embeddings. We formulate the task of semantic\nsimilarity as a model comparison task in which we contrast a generative model\nwhich jointly models two sentences versus one that does not. We illustrate how\nthis framework can be used for the Semantic Textual Similarity tasks using\nclear assumptions about how the embeddings of words are generated. We apply\nmodel comparison that utilises information criteria to address some of the\nshortcomings of Bayesian model comparison, whilst still penalising model\ncomplexity. We achieve competitive results by applying the proposed framework\nwith an appropriate choice of likelihood on the STS datasets.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 15:37:16 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 10:52:54 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Vargas", "Francisco", ""], ["Brestnichki", "Kamen", ""], ["Hammerla", "Nils", ""]]}, {"id": "1904.13335", "submitter": "Xin Du", "authors": "Xin Du, Lei Sun, Wouter Duivesteijn, Alexander Nikolaev, Mykola\n  Pechenizkiy", "title": "Adversarial Balancing-based Representation Learning for Causal Effect\n  Inference with Observational Data", "comments": null, "journal-ref": "Data Mining and Knowledge Discovery, (2021), 1-26", "doi": "10.1007/s10618-021-00759-3", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Learning causal effects from observational data greatly benefits a variety of\ndomains such as health care, education and sociology. For instance, one could\nestimate the impact of a new drug on specific individuals to assist the clinic\nplan and improve the survival rate. In this paper, we focus on studying the\nproblem of estimating Conditional Average Treatment Effect (CATE) from\nobservational data. The challenges for this problem are two-fold: on the one\nhand, we have to derive a causal estimator to estimate the causal quantity from\nobservational data, where there exists confounding bias; on the other hand, we\nhave to deal with the identification of CATE when the distribution of\ncovariates in treatment and control groups are imbalanced. To overcome these\nchallenges, we propose a neural network framework called Adversarial\nBalancing-based representation learning for Causal Effect Inference (ABCEI),\nbased on the recent advances in representation learning. To ensure the\nidentification of CATE, ABCEI uses adversarial learning to balance the\ndistributions of covariates in treatment and control groups in the latent\nrepresentation space, without any assumption on the form of the treatment\nselection/assignment function. In addition, during the representation learning\nand balancing process, highly predictive information from the original\ncovariate space might be lost. ABCEI can tackle this information loss problem\nby preserving useful information for predicting causal effects under the\nregularization of a mutual information estimator. The experimental results show\nthat ABCEI is robust against treatment selection bias, and matches/outperforms\nthe state-of-the-art approaches. Our experiments show promising results on\nseveral datasets, representing different health care domains among others.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 16:02:49 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 11:59:03 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2020 21:52:08 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Du", "Xin", ""], ["Sun", "Lei", ""], ["Duivesteijn", "Wouter", ""], ["Nikolaev", "Alexander", ""], ["Pechenizkiy", "Mykola", ""]]}, {"id": "1904.13341", "submitter": "Rui Feng", "authors": "Rui Feng, Yang Yang, Yuehan Lyu, Chenhao Tan, Yizhou Sun and Chunping\n  Wang", "title": "Learning Fair Representations via an Adversarial Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fairness has become a central issue for our research community as\nclassification algorithms are adopted in societally critical domains such as\nrecidivism prediction and loan approval. In this work, we consider the\npotential bias based on protected attributes (e.g., race and gender), and\ntackle this problem by learning latent representations of individuals that are\nstatistically indistinguishable between protected groups while sufficiently\npreserving other information for classification. To do that, we develop a\nminimax adversarial framework with a generator to capture the data distribution\nand generate latent representations, and a critic to ensure that the\ndistributions across different protected groups are similar. Our framework\nprovides a theoretical guarantee with respect to statistical parity and\nindividual fairness. Empirical results on four real-world datasets also show\nthat the learned representation can effectively be used for classification\ntasks such as credit risk prediction while obstructing information related to\nprotected groups, especially when removing protected attributes is not\nsufficient for fair classification.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 16:12:19 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Feng", "Rui", ""], ["Yang", "Yang", ""], ["Lyu", "Yuehan", ""], ["Tan", "Chenhao", ""], ["Sun", "Yizhou", ""], ["Wang", "Chunping", ""]]}, {"id": "1904.13349", "submitter": "Maarten Sukel", "authors": "Maarten Sukel, Stevan Rudinac and Marcel Worring", "title": "Multimodal Classification of Urban Micro-Events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we seek methods to effectively detect urban micro-events. Urban\nmicro-events are events which occur in cities, have limited geographical\ncoverage and typically affect only a small group of citizens. Because of their\nscale these are difficult to identify in most data sources. However, by using\ncitizen sensing to gather data, detecting them becomes feasible. The data\ngathered by citizen sensing is often multimodal and, as a consequence, the\ninformation required to detect urban micro-events is distributed over multiple\nmodalities. This makes it essential to have a classifier capable of combining\nthem. In this paper we explore several methods of creating such a classifier,\nincluding early, late, hybrid fusion and representation learning using\nmultimodal graphs. We evaluate performance on a real world dataset obtained\nfrom a live citizen reporting system. We show that a multimodal approach yields\nhigher performance than unimodal alternatives. Furthermore, we demonstrate that\nour hybrid combination of early and late fusion with multimodal embeddings\nperforms best in classification of urban micro-events.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 16:24:19 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Sukel", "Maarten", ""], ["Rudinac", "Stevan", ""], ["Worring", "Marcel", ""]]}, {"id": "1904.13366", "submitter": "Nagdev Amruthnath", "authors": "Nagdev Amruthnath, Tarun Gupta", "title": "Factor Analysis in Fault Diagnostics Using Random Forest", "comments": null, "journal-ref": "Industrial Engineering & Management, 2019", "doi": "10.4172/2169-0316.1000278", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factor analysis or sometimes referred to as variable analysis has been\nextensively used in classification problems for identifying specific factors\nthat are significant to particular classes. This type of analysis has been\nwidely used in application such as customer segmentation, medical research,\nnetwork traffic, image, and video classification. Today, factor analysis is\nprominently being used in fault diagnosis of machines to identify the\nsignificant factors and to study the root cause of a specific machine fault.\nThe advantage of performing factor analysis in machine maintenance is to\nperform prescriptive analysis (helps answer what actions to take?) and\npreemptive analysis (helps answer how to eliminate the failure mode?). In this\npaper, a real case of an industrial rotating machine was considered where\nvibration and ambient temperature data was collected for monitoring the health\nof the machine. Gaussian mixture model-based clustering was used to cluster the\ndata into significant groups, and spectrum analysis was used to diagnose each\ncluster to a specific state of the machine. The significant features that\nattribute to a particular mode of the machine were identified by using the\nrandom forest classification model. The significant features for specific modes\nof the machine were used to conclude that the clusters generated are distinct\nand have a unique set of significant features.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 16:54:13 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Amruthnath", "Nagdev", ""], ["Gupta", "Tarun", ""]]}, {"id": "1904.13373", "submitter": "Swanand Kadhe", "authors": "Swanand Kadhe, O. Ozan Koyluoglu, Kannan Ramchandran", "title": "Gradient Coding Based on Block Designs for Mitigating Adversarial\n  Stragglers", "comments": "Shorter version accepted in 2019 IEEE International Symposium on\n  Information Theory (ISIT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed implementations of gradient-based methods, wherein a server\ndistributes gradient computations across worker machines, suffer from slow\nrunning machines, called 'stragglers'. Gradient coding is a coding-theoretic\nframework to mitigate stragglers by enabling the server to recover the gradient\nsum in the presence of stragglers. 'Approximate gradient codes' are variants of\ngradient codes that reduce computation and storage overhead per worker by\nallowing the server to approximately reconstruct the gradient sum.\n  In this work, our goal is to construct approximate gradient codes that are\nresilient to stragglers selected by a computationally unbounded adversary. Our\nmotivation for constructing codes to mitigate adversarial stragglers stems from\nthe challenge of tackling stragglers in massive-scale elastic and serverless\nsystems, wherein it is difficult to statistically model stragglers. Towards\nthis end, we propose a class of approximate gradient codes based on balanced\nincomplete block designs (BIBDs). We show that the approximation error for\nthese codes depends only on the number of stragglers, and thus, adversarial\nstraggler selection has no advantage over random selection. In addition, the\nproposed codes admit computationally efficient decoding at the server. Next, to\ncharacterize fundamental limits of adversarial straggling, we consider the\nnotion of 'adversarial threshold' -- the smallest number of workers that an\nadversary must straggle to inflict certain approximation error. We compute a\nlower bound on the adversarial threshold, and show that codes based on\nsymmetric BIBDs maximize this lower bound among a wide class of codes, making\nthem excellent candidates for mitigating adversarial stragglers.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 17:13:32 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Kadhe", "Swanand", ""], ["Koyluoglu", "O. Ozan", ""], ["Ramchandran", "Kannan", ""]]}, {"id": "1904.13386", "submitter": "Robert Bridges", "authors": "Robert A. Bridges, Anthony D. Gruber, Christopher Felder, Miki Verma,\n  Chelsey Hoff", "title": "Active Manifolds: A non-linear analogue to Active Subspaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to analyze $C^1(\\mathbb{R}^m)$ functions that\naddresses limitations present in the Active Subspaces (AS) method of\nConstantine et al.(2015; 2014). Under appropriate hypotheses, our Active\nManifolds (AM) method identifies a 1-D curve in the domain (the active\nmanifold) on which nearly all values of the unknown function are attained, and\nwhich can be exploited for approximation or analysis, especially when $m$ is\nlarge (high-dimensional input space). We provide theorems justifying our AM\ntechnique and an algorithm permitting functional approximation and sensitivity\nanalysis. Using accessible, low-dimensional functions as initial examples, we\nshow AM reduces approximation error by an order of magnitude compared to AS, at\nthe expense of more computation. Following this, we revisit the sensitivity\nanalysis by Glaws et al. (2017), who apply AS to analyze a magnetohydrodynamic\npower generator model, and compare the performance of AM on the same data. Our\nanalysis provides detailed information not captured by AS, exhibiting the\ninfluence of each parameter individually along an active manifold. Overall, AM\nrepresents a novel technique for analyzing functional models with benefits\nincluding: reducing $m$-dimensional analysis to a 1-D analogue, permitting more\naccurate regression than AS (at more computational expense), enabling more\ninformative sensitivity analysis, and granting accessible visualizations(2-D\nplots) of parameter sensitivity along the AM.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 17:39:55 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 15:54:01 GMT"}, {"version": "v3", "created": "Mon, 13 May 2019 13:54:12 GMT"}, {"version": "v4", "created": "Tue, 14 May 2019 13:49:37 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Bridges", "Robert A.", ""], ["Gruber", "Anthony D.", ""], ["Felder", "Christopher", ""], ["Verma", "Miki", ""], ["Hoff", "Chelsey", ""]]}, {"id": "1904.13387", "submitter": "Ali Yekkehkhany", "authors": "Ali Yekkehkhany, Ebrahim Arian, Mohammad Hajiesmaili, Rakesh Nagi", "title": "Risk-Averse Explore-Then-Commit Algorithms for Finite-Time Bandits", "comments": null, "journal-ref": "2019 IEEE 58th Conference on Decision and Control (CDC)", "doi": "10.1109/CDC40024.2019.9142286", "report-no": "https://ieeexplore.ieee.org/document/9142286", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study multi-armed bandit problems in explore-then-commit\nsetting. In our proposed explore-then-commit setting, the goal is to identify\nthe best arm after a pure experimentation (exploration) phase and exploit it\nonce or for a given finite number of times. We identify that although the arm\nwith the highest expected reward is the most desirable objective for infinite\nexploitations, it is not necessarily the one that is most probable to have the\nhighest reward in a single or finite-time exploitations. Alternatively, we\nadvocate the idea of risk-aversion where the objective is to compete against\nthe arm with the best risk-return trade-off. Then, we propose two algorithms\nwhose objectives are to select the arm that is most probable to reward the\nmost. Using a new notion of finite-time exploitation regret, we find an upper\nbound for the minimum number of experiments before commitment, to guarantee an\nupper bound for the regret. As compared to existing risk-averse bandit\nalgorithms, our algorithms do not rely on hyper-parameters, resulting in a more\nrobust behavior in practice, which is verified by the numerical evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 17:40:13 GMT"}, {"version": "v2", "created": "Wed, 8 May 2019 23:34:45 GMT"}, {"version": "v3", "created": "Wed, 11 Sep 2019 18:58:30 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Yekkehkhany", "Ali", ""], ["Arian", "Ebrahim", ""], ["Hajiesmaili", "Mohammad", ""], ["Nagi", "Rakesh", ""]]}, {"id": "1904.13389", "submitter": "Lin Chen", "authors": "MohammadHossein Bateni, Lin Chen, Hossein Esfandiari, Thomas Fu, Vahab\n  S. Mirrokni, Afshin Rostamizadeh", "title": "Categorical Feature Compression via Submodular Optimization", "comments": "Accepted to ICML 2019. Authors are listed in alphabetical order", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of big data, learning from categorical features with very large\nvocabularies (e.g., 28 million for the Criteo click prediction dataset) has\nbecome a practical challenge for machine learning researchers and\npractitioners. We design a highly-scalable vocabulary compression algorithm\nthat seeks to maximize the mutual information between the compressed\ncategorical feature and the target binary labels and we furthermore show that\nits solution is guaranteed to be within a $1-1/e \\approx 63\\%$ factor of the\nglobal optimal solution. To achieve this, we introduce a novel\nre-parametrization of the mutual information objective, which we prove is\nsubmodular, and design a data structure to query the submodular function in\namortized $O(\\log n )$ time (where $n$ is the input vocabulary size). Our\ncomplete algorithm is shown to operate in $O(n \\log n )$ time. Additionally, we\ndesign a distributed implementation in which the query data structure is\ndecomposed across $O(k)$ machines such that each machine only requires $O(\\frac\nn k)$ space, while still preserving the approximation guarantee and using only\nlogarithmic rounds of computation. We also provide analysis of simple\nalternative heuristic compression methods to demonstrate they cannot achieve\nany approximation guarantee. Using the large-scale Criteo learning task, we\ndemonstrate better performance in retaining mutual information and also verify\ncompetitive learning performance compared to other baseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 17:45:13 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Bateni", "MohammadHossein", ""], ["Chen", "Lin", ""], ["Esfandiari", "Hossein", ""], ["Fu", "Thomas", ""], ["Mirrokni", "Vahab S.", ""], ["Rostamizadeh", "Afshin", ""]]}]