[{"id": "2007.00028", "submitter": "Ohad Shamir", "authors": "Ohad Shamir", "title": "Gradient Methods Never Overfit On Separable Data", "comments": "Added a short appendix on polynomially-tailed losses; some minor\n  edits", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A line of recent works established that when training linear predictors over\nseparable data, using gradient methods and exponentially-tailed losses, the\npredictors asymptotically converge in direction to the max-margin predictor. As\na consequence, the predictors asymptotically do not overfit. However, this does\nnot address the question of whether overfitting might occur non-asymptotically,\nafter some bounded number of iterations. In this paper, we formally show that\nstandard gradient methods (in particular, gradient flow, gradient descent and\nstochastic gradient descent) never overfit on separable data: If we run these\nmethods for $T$ iterations on a dataset of size $m$, both the empirical risk\nand the generalization error decrease at an essentially optimal rate of\n$\\tilde{\\mathcal{O}}(1/\\gamma^2 T)$ up till $T\\approx m$, at which point the\ngeneralization error remains fixed at an essentially optimal level of\n$\\tilde{\\mathcal{O}}(1/\\gamma^2 m)$ regardless of how large $T$ is. Along the\nway, we present non-asymptotic bounds on the number of margin violations over\nthe dataset, and prove their tightness.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 18:01:46 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 10:51:50 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Shamir", "Ohad", ""]]}, {"id": "2007.00041", "submitter": "Jay Stanley IIi", "authors": "Jay S. Stanley III, Eric C. Chi, and Gal Mishne", "title": "Multi-way Graph Signal Processing on Tensors: Integrative analysis of\n  irregular geometries", "comments": "In review for IEEE Signal Processing Magazine", "journal-ref": null, "doi": "10.1109/MSP.2020.3013555", "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph signal processing (GSP) is an important methodology for studying data\nresiding on irregular structures. As acquired data is increasingly taking the\nform of multi-way tensors, new signal processing tools are needed to maximally\nutilize the multi-way structure within the data. In this paper, we review\nmodern signal processing frameworks generalizing GSP to multi-way data,\nstarting from graph signals coupled to familiar regular axes such as time in\nsensor networks, and then extending to general graphs across all tensor modes.\nThis widely applicable paradigm motivates reformulating and improving upon\nclassical problems and approaches to creatively address the challenges in\ntensor-based data. We synthesize common themes arising from current efforts to\ncombine GSP with tensor analysis and highlight future directions in extending\nGSP to the multi-way paradigm.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 18:15:17 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 13:43:07 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Stanley", "Jay S.", "III"], ["Chi", "Eric C.", ""], ["Mishne", "Gal", ""]]}, {"id": "2007.00045", "submitter": "Abdul Mueed Hafiz Dr.", "authors": "A. M. Hafiz", "title": "K-Nearest Neighbour and Support Vector Machine Hybrid Classification", "comments": null, "journal-ref": "International Journal of Imaging and Robotics, Vol.19, No.4,\n  pp.33-41, CESER Publications (2019)", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel K-Nearest Neighbour and Support Vector Machine hybrid\nclassification technique has been proposed that is simple and robust. It is\nbased on the concept of discriminative nearest neighbourhood classification.\nThe technique consists of using K-Nearest Neighbour Classification for test\nsamples satisfying a proximity condition. The patterns which do not pass the\nproximity condition are separated. This is followed by sifting the training set\nfor a fixed number of patterns for every class which are closest to each\nseparated test pattern respectively, based on the Euclidean distance metric.\nSubsequently, for every separated test sample, a Support Vector Machine is\ntrained on the sifted training set patterns associated with it, and\nclassification for the test sample is done. The proposed technique has been\ncompared to the state of art in this research area. Three datasets viz. the\nUnited States Postal Service (USPS) Handwritten Digit Dataset, MNIST Dataset,\nand an Arabic numeral dataset, the Modified Arabic Digits Database, MADB, have\nbeen used to evaluate the performance of the algorithm. The algorithm generally\noutperforms the other algorithms with which it has been compared.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 15:26:56 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Hafiz", "A. M.", ""]]}, {"id": "2007.00051", "submitter": "Hadi Pouransari", "authors": "Hadi Pouransari, Mojan Javaheripi, Vinay Sharma, Oncel Tuzel", "title": "Extracurricular Learning: Knowledge Transfer Beyond Empirical\n  Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation has been used to transfer knowledge learned by a\nsophisticated model (teacher) to a simpler model (student). This technique is\nwidely used to compress model complexity. However, in most applications the\ncompressed student model suffers from an accuracy gap with its teacher. We\npropose extracurricular learning, a novel knowledge distillation method, that\nbridges this gap by (1) modeling student and teacher output distributions; (2)\nsampling examples from an approximation to the underlying data distribution;\nand (3) matching student and teacher output distributions over this extended\nset including uncertain samples. We conduct rigorous evaluations on regression\nand classification tasks and show that compared to the standard knowledge\ndistillation, extracurricular learning reduces the gap by 46% to 68%. This\nleads to major accuracy improvements compared to the empirical risk\nminimization-based training for various recent neural network architectures:\n16% regression error reduction on the MPIIGaze dataset, +3.4% to +9.1%\nimprovement in top-1 classification accuracy on the CIFAR100 dataset, and +2.9%\ntop-1 improvement on the ImageNet dataset.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 18:21:21 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 19:11:09 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Pouransari", "Hadi", ""], ["Javaheripi", "Mojan", ""], ["Sharma", "Vinay", ""], ["Tuzel", "Oncel", ""]]}, {"id": "2007.00067", "submitter": "Boyuan Pan", "authors": "Boyuan Pan, Yazheng Yang, Kaizhao Liang, Bhavya Kailkhura, Zhongming\n  Jin, Xian-Sheng Hua, Deng Cai, Bo Li", "title": "Adversarial Mutual Information for Text Generation", "comments": "Published at ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in maximizing mutual information (MI) between the source and\ntarget have demonstrated its effectiveness in text generation. However,\nprevious works paid little attention to modeling the backward network of MI\n(i.e., dependency from the target to the source), which is crucial to the\ntightness of the variational information maximization lower bound. In this\npaper, we propose Adversarial Mutual Information (AMI): a text generation\nframework which is formed as a novel saddle point (min-max) optimization aiming\nto identify joint interactions between the source and target. Within this\nframework, the forward and backward networks are able to iteratively promote or\ndemote each other's generated instances by comparing the real and synthetic\ndata distributions. We also develop a latent noise sampling strategy that\nleverages random variations at the high-level semantic space to enhance the\nlong term dependency in the generation process. Extensive experiments based on\ndifferent text generation tasks demonstrate that the proposed AMI framework can\nsignificantly outperform several strong baselines, and we also show that AMI\nhas potential to lead to a tighter lower bound of maximum mutual information\nfor the variational information maximization problem.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 19:11:51 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Pan", "Boyuan", ""], ["Yang", "Yazheng", ""], ["Liang", "Kaizhao", ""], ["Kailkhura", "Bhavya", ""], ["Jin", "Zhongming", ""], ["Hua", "Xian-Sheng", ""], ["Cai", "Deng", ""], ["Li", "Bo", ""]]}, {"id": "2007.00072", "submitter": "Nikoli Dryden", "authors": "Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, Torsten Hoefler", "title": "Data Movement Is All You Need: A Case Study on Optimizing Transformers", "comments": "15 pages, 6 figures; minor clarifications and style updates", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers have become widely used for language modeling and sequence\nlearning tasks, and are one of the most important machine learning workloads\ntoday. Training one is a very compute-intensive task, often taking days or\nweeks, and significant attention has been given to optimizing transformers.\nDespite this, existing implementations do not efficiently utilize GPUs. We find\nthat data movement is the key bottleneck when training. Due to Amdahl's Law and\nmassive improvements in compute performance, training has now become\nmemory-bound. Further, existing frameworks use suboptimal data layouts. Using\nthese insights, we present a recipe for globally optimizing data movement in\ntransformers. We reduce data movement by up to 22.91% and overall achieve a\n1.30x performance improvement over state-of-the-art frameworks when training\nBERT. Our approach is applicable more broadly to optimizing deep neural\nnetworks, and offers insight into how to tackle emerging performance\nbottlenecks.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 19:26:36 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 09:26:19 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Ivanov", "Andrei", ""], ["Dryden", "Nikoli", ""], ["Ben-Nun", "Tal", ""], ["Li", "Shigang", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2007.00077", "submitter": "Cody Coleman", "authors": "Cody Coleman, Edward Chou, Julian Katz-Samuels, Sean Culatana, Peter\n  Bailis, Alexander C. Berg, Robert Nowak, Roshan Sumbaly, Matei Zaharia, I.\n  Zeki Yalniz", "title": "Similarity Search for Efficient Active Learning and Search of Rare\n  Concepts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many active learning and search approaches are intractable for large-scale\nindustrial settings with billions of unlabeled examples. Existing approaches\nsearch globally for the optimal examples to label, scaling linearly or even\nquadratically with the unlabeled data. In this paper, we improve the\ncomputational efficiency of active learning and search methods by restricting\nthe candidate pool for labeling to the nearest neighbors of the currently\nlabeled set instead of scanning over all of the unlabeled data. We evaluate\nseveral selection strategies in this setting on three large-scale computer\nvision datasets: ImageNet, OpenImages, and a de-identified and aggregated\ndataset of 10 billion images provided by a large internet company. Our approach\nachieved similar mean average precision and recall as the traditional global\napproach while reducing the computational cost of selection by up to three\norders of magnitude, thus enabling web-scale active learning.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 19:46:10 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 16:54:12 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Coleman", "Cody", ""], ["Chou", "Edward", ""], ["Katz-Samuels", "Julian", ""], ["Culatana", "Sean", ""], ["Bailis", "Peter", ""], ["Berg", "Alexander C.", ""], ["Nowak", "Robert", ""], ["Sumbaly", "Roshan", ""], ["Zaharia", "Matei", ""], ["Yalniz", "I. Zeki", ""]]}, {"id": "2007.00080", "submitter": "Xiao-Yue Gong", "authors": "Xiao-Yue Gong, David Simchi-Levi", "title": "Provably More Efficient Q-Learning in the\n  One-Sided-Feedback/Full-Feedback Settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the episodic version of the classical inventory control problem,\nwe propose a new Q-learning-based algorithm, Elimination-Based Half-Q-Learning\n(HQL), that enjoys improved efficiency over existing algorithms for a wide\nvariety of problems in the one-sided-feedback setting. We also provide a\nsimpler variant of the algorithm, Full-Q-Learning (FQL), for the full-feedback\nsetting. We establish that HQL incurs $ \\tilde{\\mathcal{O}}(H^3\\sqrt{ T})$\nregret and FQL incurs $\\tilde{\\mathcal{O}}(H^2\\sqrt{ T})$ regret, where $H$ is\nthe length of each episode and $T$ is the total length of the horizon. The\nregret bounds are not affected by the possibly huge state and action space. Our\nnumerical experiments demonstrate the superior efficiency of HQL and FQL, and\nthe potential to combine reinforcement learning with richer feedback models.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 19:47:38 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 20:25:44 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Gong", "Xiao-Yue", ""], ["Simchi-Levi", "David", ""]]}, {"id": "2007.00081", "submitter": "Semih Cayci", "authors": "Semih Cayci, Atilla Eryilmaz, R. Srikant", "title": "Continuous-Time Multi-Armed Bandits with Controlled Restarts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Time-constrained decision processes have been ubiquitous in many fundamental\napplications in physics, biology and computer science. Recently, restart\nstrategies have gained significant attention for boosting the efficiency of\ntime-constrained processes by expediting the completion times. In this work, we\ninvestigate the bandit problem with controlled restarts for time-constrained\ndecision processes, and develop provably good learning algorithms. In\nparticular, we consider a bandit setting where each decision takes a random\ncompletion time, and yields a random and correlated reward at the end, with\nunknown values at the time of decision. The goal of the decision-maker is to\nmaximize the expected total reward subject to a time constraint $\\tau$. As an\nadditional control, we allow the decision-maker to interrupt an ongoing task\nand forgo its reward for a potentially more rewarding alternative. For this\nproblem, we develop efficient online learning algorithms with $O(\\log(\\tau))$\nand $O(\\sqrt{\\tau\\log(\\tau)})$ regret in a finite and continuous action space\nof restart strategies, respectively. We demonstrate an applicability of our\nalgorithm by using it to boost the performance of SAT solvers.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 19:50:39 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Cayci", "Semih", ""], ["Eryilmaz", "Atilla", ""], ["Srikant", "R.", ""]]}, {"id": "2007.00119", "submitter": "Marylens Hernandez", "authors": "Chris Lin, Gerald J. Sun, Krishna C. Bulusu, Jonathan R. Dry and\n  Marylens Hernandez", "title": "Graph Neural Networks Including Sparse Interpretability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs) are versatile, powerful machine learning methods\nthat enable graph structure and feature representation learning, and have\napplications across many domains. For applications critically requiring\ninterpretation, attention-based GNNs have been leveraged. However, these\napproaches either rely on specific model architectures or lack a joint\nconsideration of graph structure and node features in their interpretation.\nHere we present a model-agnostic framework for interpreting important graph\nstructure and node features, Graph neural networks Including SparSe\ninTerpretability (GISST). With any GNN model, GISST combines an attention\nmechanism and sparsity regularization to yield an important subgraph and node\nfeature subset related to any graph-based task. Through a single self-attention\nlayer, a GISST model learns an importance probability for each node feature and\nedge in the input graph. By including these importance probabilities in the\nmodel loss function, the probabilities are optimized end-to-end and tied to the\ntask-specific performance. Furthermore, GISST sparsifies these importance\nprobabilities with entropy and L1 regularization to reduce noise in the input\ngraph topology and node features. Our GISST models achieve superior node\nfeature and edge explanation precision in synthetic datasets, as compared to\nalternative interpretation approaches. Moreover, our GISST models are able to\nidentify important graph structure in real-world datasets. We demonstrate in\ntheory that edge feature importance and multiple edge types can be considered\nby incorporating them into the GISST edge probability computation. By jointly\naccounting for topology, node features, and edge features, GISST inherently\nprovides simple and relevant interpretations for any GNN models and tasks.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 21:35:55 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Lin", "Chris", ""], ["Sun", "Gerald J.", ""], ["Bulusu", "Krishna C.", ""], ["Dry", "Jonathan R.", ""], ["Hernandez", "Marylens", ""]]}, {"id": "2007.00140", "submitter": "Kumar Pratik", "authors": "Kumar Pratik, Bhaskar D. Rao, Max Welling", "title": "RE-MIMO: Recurrent and Permutation Equivariant Neural MIMO Detection", "comments": "copyright 2020 IEEE TSP. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": null, "doi": "10.1109/TSP.2020.3045199", "report-no": null, "categories": "eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel neural network for MIMO symbol detection.\nIt is motivated by several important considerations in wireless communication\nsystems; permutation equivariance and a variable number of users. The neural\ndetector learns an iterative decoding algorithm that is implemented as a stack\nof iterative units. Each iterative unit is a neural computation module\ncomprising of 3 sub-modules: the likelihood module, the encoder module, and the\npredictor module. The likelihood module injects information about the\ngenerative (forward) process into the neural network. The encoder-predictor\nmodules together update the state vector and symbol estimates. The encoder\nmodule updates the state vector and employs a transformer based attention\nnetwork to handle the interactions among the users in a permutation equivariant\nmanner. The predictor module refines the symbol estimates. The modular and\npermutation equivariant architecture allows for dealing with a varying number\nof users. The resulting neural detector architecture is unique and exhibits\nseveral desirable properties unseen in any of the previously proposed neural\ndetectors. We compare its performance against existing methods and the results\nshow the ability of our network to efficiently handle a variable number of\ntransmitters with high accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 22:43:01 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2021 10:43:53 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Pratik", "Kumar", ""], ["Rao", "Bhaskar D.", ""], ["Welling", "Max", ""]]}, {"id": "2007.00147", "submitter": "Eric Wong", "authors": "Eric Wong, Tim Schneider, Joerg Schmitt, Frank R. Schmidt, J. Zico\n  Kolter", "title": "Neural Network Virtual Sensors for Fuel Injection Quantities with\n  Provable Performance Specifications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that it is possible to learn neural networks with\nprovable guarantees on the output of the model when subject to input\nperturbations, however these works have focused primarily on defending against\nadversarial examples for image classifiers. In this paper, we study how these\nprovable guarantees can be naturally applied to other real world settings,\nnamely getting performance specifications for robust virtual sensors measuring\nfuel injection quantities within an engine. We first demonstrate that, in this\nsetting, even simple neural network models are highly susceptible to reasonable\nlevels of adversarial sensor noise, which are capable of increasing the mean\nrelative error of a standard neural network from 6.6% to 43.8%. We then\nleverage methods for learning provably robust networks and verifying robustness\nproperties, resulting in a robust model which we can provably guarantee has at\nmost 16.5% mean relative error under any sensor noise. Additionally, we show\nhow specific intervals of fuel injection quantities can be targeted to maximize\nrobustness for certain ranges, allowing us to train a virtual sensor for fuel\ninjection which is provably guaranteed to have at most 10.69% relative error\nunder noise while maintaining 3% relative error on non-adversarial data within\nnormalized fuel injection ranges of 0.6 to 1.0.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 23:33:17 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Wong", "Eric", ""], ["Schneider", "Tim", ""], ["Schmitt", "Joerg", ""], ["Schmidt", "Frank R.", ""], ["Kolter", "J. Zico", ""]]}, {"id": "2007.00148", "submitter": "Yingjie Fei", "authors": "Yingjie Fei, Zhuoran Yang, Zhaoran Wang, Qiaomin Xie", "title": "Dynamic Regret of Policy Optimization in Non-stationary Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider reinforcement learning (RL) in episodic MDPs with adversarial\nfull-information reward feedback and unknown fixed transition kernels. We\npropose two model-free policy optimization algorithms, POWER and POWER++, and\nestablish guarantees for their dynamic regret. Compared with the classical\nnotion of static regret, dynamic regret is a stronger notion as it explicitly\naccounts for the non-stationarity of environments. The dynamic regret attained\nby the proposed algorithms interpolates between different regimes of\nnon-stationarity, and moreover satisfies a notion of adaptive\n(near-)optimality, in the sense that it matches the (near-)optimal static\nregret under slow-changing environments. The dynamic regret bound features two\ncomponents, one arising from exploration, which deals with the uncertainty of\ntransition kernels, and the other arising from adaptation, which deals with\nnon-stationary environments. Specifically, we show that POWER++ improves over\nPOWER on the second component of the dynamic regret by actively adapting to\nnon-stationarity through prediction. To the best of our knowledge, our work is\nthe first dynamic regret analysis of model-free RL algorithms in non-stationary\nenvironments.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 23:34:37 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Fei", "Yingjie", ""], ["Yang", "Zhuoran", ""], ["Wang", "Zhaoran", ""], ["Xie", "Qiaomin", ""]]}, {"id": "2007.00151", "submitter": "Sheng Liu", "authors": "Sheng Liu, Jonathan Niles-Weed, Narges Razavian, Carlos\n  Fernandez-Granda", "title": "Early-Learning Regularization Prevents Memorization of Noisy Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel framework to perform classification via deep learning in\nthe presence of noisy annotations. When trained on noisy labels, deep neural\nnetworks have been observed to first fit the training data with clean labels\nduring an \"early learning\" phase, before eventually memorizing the examples\nwith false labels. We prove that early learning and memorization are\nfundamental phenomena in high-dimensional classification tasks, even in simple\nlinear models, and give a theoretical explanation in this setting. Motivated by\nthese findings, we develop a new technique for noisy classification tasks,\nwhich exploits the progress of the early learning phase. In contrast with\nexisting approaches, which use the model output during early learning to detect\nthe examples with clean labels, and either ignore or attempt to correct the\nfalse labels, we take a different route and instead capitalize on early\nlearning via regularization. There are two key elements to our approach. First,\nwe leverage semi-supervised learning techniques to produce target probabilities\nbased on the model outputs. Second, we design a regularization term that steers\nthe model towards these targets, implicitly preventing memorization of the\nfalse labels. The resulting framework is shown to provide robustness to noisy\nannotations on several standard benchmarks and real-world datasets, where it\nachieves results comparable to the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 23:46:33 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 22:18:22 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Liu", "Sheng", ""], ["Niles-Weed", "Jonathan", ""], ["Razavian", "Narges", ""], ["Fernandez-Granda", "Carlos", ""]]}, {"id": "2007.00155", "submitter": "Michael Teng", "authors": "Michael Teng, Tuan Anh Le, Adam Scibior, Frank Wood", "title": "Semi-supervised Sequential Generative Models", "comments": "Accepted to Uncertainty in Artificial Intelligence 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel objective for training deep generative time-series\nmodels with discrete latent variables for which supervision is only sparsely\navailable. This instance of semi-supervised learning is challenging for\nexisting methods, because the exponential number of possible discrete latent\nconfigurations results in high variance gradient estimators. We first overcome\nthis problem by extending the standard semi-supervised generative modeling\nobjective with reweighted wake-sleep. However, we find that this approach still\nsuffers when the frequency of available labels varies between training\nsequences. Finally, we introduce a unified objective inspired by\nteacher-forcing and show that this approach is robust to variable length\nsupervision. We call the resulting method caffeinated wake-sleep (CWS) to\nemphasize its additional dependence on real data. We demonstrate its\neffectiveness with experiments on MNIST, handwriting, and fruit fly trajectory\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 23:53:12 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Teng", "Michael", ""], ["Le", "Tuan Anh", ""], ["Scibior", "Adam", ""], ["Wood", "Frank", ""]]}, {"id": "2007.00163", "submitter": "Andrew Jesson D", "authors": "Andrew Jesson, S\\\"oren Mindermann, Uri Shalit and Yarin Gal", "title": "Identifying Causal-Effect Inference Failure with Uncertainty-Aware\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommending the best course of action for an individual is a major\napplication of individual-level causal effect estimation. This application is\noften needed in safety-critical domains such as healthcare, where estimating\nand communicating uncertainty to decision-makers is crucial. We introduce a\npractical approach for integrating uncertainty estimation into a class of\nstate-of-the-art neural network methods used for individual-level causal\nestimates. We show that our methods enable us to deal gracefully with\nsituations of \"no-overlap\", common in high-dimensional data, where standard\napplications of causal effect approaches fail. Further, our methods allow us to\nhandle covariate shift, where test distribution differs to train distribution,\ncommon when systems are deployed in practice. We show that when such a\ncovariate shift occurs, correctly modeling uncertainty can keep us from giving\noverconfident and potentially harmful recommendations. We demonstrate our\nmethodology with a range of state-of-the-art models. Under both covariate shift\nand lack of overlap, our uncertainty-equipped methods can alert decisions\nmakers when predictions are not to be trusted while outperforming their\nuncertainty-oblivious counterparts.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 00:37:41 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 20:52:39 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Jesson", "Andrew", ""], ["Mindermann", "S\u00f6ren", ""], ["Shalit", "Uri", ""], ["Gal", "Yarin", ""]]}, {"id": "2007.00169", "submitter": "Shuai Han", "authors": "Shuai Han and Wenbo Zhou and Shuai L\\\"u and Jiayu Yu", "title": "Regularly Updated Deterministic Policy Gradient Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Deterministic Policy Gradient (DDPG) algorithm is one of the most\nwell-known reinforcement learning methods. However, this method is inefficient\nand unstable in practical applications. On the other hand, the bias and\nvariance of the Q estimation in the target function are sometimes difficult to\ncontrol. This paper proposes a Regularly Updated Deterministic (RUD) policy\ngradient algorithm for these problems. This paper theoretically proves that the\nlearning procedure with RUD can make better use of new data in replay buffer\nthan the traditional procedure. In addition, the low variance of the Q value in\nRUD is more suitable for the current Clipped Double Q-learning strategy. This\npaper has designed a comparison experiment against previous methods, an\nablation experiment with the original DDPG, and other analytical experiments in\nMujoco environments. The experimental results demonstrate the effectiveness and\nsuperiority of RUD.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 01:18:25 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Han", "Shuai", ""], ["Zhou", "Wenbo", ""], ["L\u00fc", "Shuai", ""], ["Yu", "Jiayu", ""]]}, {"id": "2007.00178", "submitter": "Erdem B{\\i}y{\\i}k", "authors": "Zhangjie Cao, Erdem B{\\i}y{\\i}k, Woodrow Z. Wang, Allan Raventos,\n  Adrien Gaidon, Guy Rosman, Dorsa Sadigh", "title": "Reinforcement Learning based Control of Imitative Policies for\n  Near-Accident Driving", "comments": "10 pages, 7 figures. Published at Robotics: Science and Systems (RSS)\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving has achieved significant progress in recent years, but\nautonomous cars are still unable to tackle high-risk situations where a\npotential accident is likely. In such near-accident scenarios, even a minor\nchange in the vehicle's actions may result in drastically different\nconsequences. To avoid unsafe actions in near-accident scenarios, we need to\nfully explore the environment. However, reinforcement learning (RL) and\nimitation learning (IL), two widely-used policy learning methods, cannot model\nrapid phase transitions and are not scalable to fully cover all the states. To\naddress driving in near-accident scenarios, we propose a hierarchical\nreinforcement and imitation learning (H-ReIL) approach that consists of\nlow-level policies learned by IL for discrete driving modes, and a high-level\npolicy learned by RL that switches between different driving modes. Our\napproach exploits the advantages of both IL and RL by integrating them into a\nunified learning framework. Experimental results and user studies suggest our\napproach can achieve higher efficiency and safety compared to other methods.\nAnalyses of the policies demonstrate our high-level policy appropriately\nswitches between different low-level policies in near-accident driving\nsituations.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 01:41:45 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Cao", "Zhangjie", ""], ["B\u0131y\u0131k", "Erdem", ""], ["Wang", "Woodrow Z.", ""], ["Raventos", "Allan", ""], ["Gaidon", "Adrien", ""], ["Rosman", "Guy", ""], ["Sadigh", "Dorsa", ""]]}, {"id": "2007.00197", "submitter": "Mohammad Rostami", "authors": "Mohammad Rostami, Aram Galstyan", "title": "Sequential Model Adaptation Using Domain Agnostic Internal Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an algorithm for sequential adaptation of a classifier that is\ntrained for a source domain to generalize in an unannotated target domain. We\nconsider that the model has been trained on the source domain annotated data\nand then it needs to be adapted using the target domain unannotated data when\nthe source domain data is not accessible. We align the distributions of the\nsource and the target domains in a discriminative embedding space via an\nintermediate internal distribution. This distribution is estimated using the\nsource data representations in the embedding. We conduct experiments on four\nbenchmarks to demonstrate the method is effective and compares favorably\nagainst existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 03:14:17 GMT"}, {"version": "v2", "created": "Sun, 20 Sep 2020 23:37:44 GMT"}, {"version": "v3", "created": "Sat, 3 Apr 2021 07:10:25 GMT"}, {"version": "v4", "created": "Wed, 23 Jun 2021 08:01:32 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Rostami", "Mohammad", ""], ["Galstyan", "Aram", ""]]}, {"id": "2007.00203", "submitter": "Jordan Erskine", "authors": "Jordan Erskine, Chris Lehnert", "title": "Developing cooperative policies for multi-stage tasks", "comments": "This paper was submitted to RA-L on June 20th 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the Cooperative Soft Actor Critic (CSAC) method of\nenabling consecutive reinforcement learning agents to cooperatively solve a\nlong time horizon multi-stage task. This method is achieved by modifying the\npolicy of each agent to maximise both the current and next agent's critic.\nCooperatively maximising each agent's critic allows each agent to take actions\nthat are beneficial for its task as well as subsequent tasks. Using this method\nin a multi-room maze domain, the cooperative policies were able to outperform\nboth uncooperative policies as well as a single agent trained across the entire\ndomain. CSAC achieved a success rate of at least 20\\% higher than the\nuncooperative policies, and converged on a solution at least 4 times faster\nthan the single agent.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 03:32:14 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Erskine", "Jordan", ""], ["Lehnert", "Chris", ""]]}, {"id": "2007.00204", "submitter": "Wenpin Tang", "authors": "Wenpin Tang", "title": "Learning an arbitrary mixture of two multinomial logits", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CC cs.LG cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider mixtures of multinomial logistic models (MNL),\nwhich are known to $\\epsilon$-approximate any random utility model. Despite its\nlong history and broad use, rigorous results are only available for learning a\nuniform mixture of two MNLs. Continuing this line of research, we study the\nproblem of learning an arbitrary mixture of two MNLs. We show that the\nidentifiability of the mixture models may only fail on an algebraic variety of\na negligible measure. This is done by reducing the problem of learning a\nmixture of two MNLs to the problem of solving a system of univariate quartic\nequations. We also devise an algorithm to learn any mixture of two MNLs using a\npolynomial number of samples and a linear number of queries, provided that a\nmixture of two MNLs over some finite universe is identifiable. Several\nnumerical experiments and conjectures are also presented.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 03:33:52 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 09:28:56 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Tang", "Wenpin", ""]]}, {"id": "2007.00211", "submitter": "Marc Law", "authors": "Marc T. Law and Jos Stam", "title": "Ultrahyperbolic Representation Learning", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning, data is usually represented in a (flat) Euclidean space\nwhere distances between points are along straight lines. Researchers have\nrecently considered more exotic (non-Euclidean) Riemannian manifolds such as\nhyperbolic space which is well suited for tree-like data. In this paper, we\npropose a representation living on a pseudo-Riemannian manifold of constant\nnonzero curvature. It is a generalization of hyperbolic and spherical\ngeometries where the nondegenerate metric tensor need not be positive definite.\nWe provide the necessary learning tools in this geometry and extend\ngradient-based optimization techniques. More specifically, we provide\nclosed-form expressions for distances via geodesics and define a descent\ndirection to minimize some objective function. Our novel framework is applied\nto graph representations.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 03:49:24 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 15:15:44 GMT"}, {"version": "v3", "created": "Mon, 26 Oct 2020 15:56:04 GMT"}, {"version": "v4", "created": "Wed, 28 Oct 2020 14:10:44 GMT"}, {"version": "v5", "created": "Mon, 11 Jan 2021 02:49:43 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Law", "Marc T.", ""], ["Stam", "Jos", ""]]}, {"id": "2007.00218", "submitter": "Kevin Bello", "authors": "Kevin Bello and Jean Honorio", "title": "Fairness constraints can help exact inference in structured prediction", "comments": "10pages, 3 figures", "journal-ref": "Neural Information Processing Systems (NeurIPS), 2020", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many inference problems in structured prediction can be modeled as maximizing\na score function on a space of labels, where graphs are a natural\nrepresentation to decompose the total score into a sum of unary (nodes) and\npairwise (edges) scores. Given a generative model with an undirected connected\ngraph $G$ and true vector of binary labels, it has been previously shown that\nwhen $G$ has good expansion properties, such as complete graphs or $d$-regular\nexpanders, one can exactly recover the true labels (with high probability and\nin polynomial time) from a single noisy observation of each edge and node. We\nanalyze the previously studied generative model by Globerson et al. (2015)\nunder a notion of statistical parity. That is, given a fair binary node\nlabeling, we ask the question whether it is possible to recover the fair\nassignment, with high probability and in polynomial time, from single edge and\nnode observations. We find that, in contrast to the known trade-offs between\nfairness and model performance, the addition of the fairness constraint\nimproves the probability of exact recovery. We effectively explain this\nphenomenon and empirically show how graphs with poor expansion properties, such\nas grids, are now capable to achieve exact recovery with high probability.\nFinally, as a byproduct of our analysis, we provide a tighter\nminimum-eigenvalue bound than that of Weyl's inequality.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 04:11:29 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Bello", "Kevin", ""], ["Honorio", "Jean", ""]]}, {"id": "2007.00222", "submitter": "Yuma Koizumi", "authors": "Yuma Koizumi, Ryo Masumura, Kyosuke Nishida, Masahiro Yasuda,\n  Shoichiro Saito", "title": "A Transformer-based Audio Captioning Model with Keyword Estimation", "comments": "Accepted to Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the problems with automated audio captioning (AAC) is the\nindeterminacy in word selection corresponding to the audio event/scene. Since\none acoustic event/scene can be described with several words, it results in a\ncombinatorial explosion of possible captions and difficulty in training. To\nsolve this problem, we propose a Transformer-based audio-captioning model with\nkeyword estimation called TRACKE. It simultaneously solves the word-selection\nindeterminacy problem with the main task of AAC while executing the sub-task of\nacoustic event detection/acoustic scene classification (i.e., keyword\nestimation). TRACKE estimates keywords, which comprise a word set corresponding\nto audio events/scenes in the input audio, and generates the caption while\nreferring to the estimated keywords to reduce word-selection indeterminacy.\nExperimental results on a public AAC dataset indicate that TRACKE achieved\nstate-of-the-art performance and successfully estimated both the caption and\nits keywords.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 04:21:00 GMT"}, {"version": "v2", "created": "Sat, 8 Aug 2020 06:38:00 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Koizumi", "Yuma", ""], ["Masumura", "Ryo", ""], ["Nishida", "Kyosuke", ""], ["Yasuda", "Masahiro", ""], ["Saito", "Shoichiro", ""]]}, {"id": "2007.00224", "submitter": "Ching-Yao Chuang", "authors": "Ching-Yao Chuang, Joshua Robinson, Lin Yen-Chen, Antonio Torralba,\n  Stefanie Jegelka", "title": "Debiased Contrastive Learning", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems (2020)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A prominent technique for self-supervised representation learning has been to\ncontrast semantically similar and dissimilar pairs of samples. Without access\nto labels, dissimilar (negative) points are typically taken to be randomly\nsampled datapoints, implicitly accepting that these points may, in reality,\nactually have the same label. Perhaps unsurprisingly, we observe that sampling\nnegative examples from truly different labels improves performance, in a\nsynthetic setting where labels are available. Motivated by this observation, we\ndevelop a debiased contrastive objective that corrects for the sampling of\nsame-label datapoints, even without knowledge of the true labels. Empirically,\nthe proposed objective consistently outperforms the state-of-the-art for\nrepresentation learning in vision, language, and reinforcement learning\nbenchmarks. Theoretically, we establish generalization bounds for the\ndownstream classification task.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 04:25:24 GMT"}, {"version": "v2", "created": "Sun, 5 Jul 2020 18:58:44 GMT"}, {"version": "v3", "created": "Wed, 21 Oct 2020 06:39:24 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Chuang", "Ching-Yao", ""], ["Robinson", "Joshua", ""], ["Yen-Chen", "Lin", ""], ["Torralba", "Antonio", ""], ["Jegelka", "Stefanie", ""]]}, {"id": "2007.00225", "submitter": "Yuma Koizumi", "authors": "Yuma Koizumi, Daiki Takeuchi, Yasunori Ohishi, Noboru Harada, Kunio\n  Kashino", "title": "The NTT DCASE2020 Challenge Task 6 system: Automated Audio Captioning\n  with Keywords and Sentence Length Estimation", "comments": "Technical Report of DCASE2020 Challenge Task 6", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This technical report describes the system participating to the Detection and\nClassification of Acoustic Scenes and Events (DCASE) 2020 Challenge, Task 6:\nautomated audio captioning. Our submission focuses on solving two indeterminacy\nproblems in automated audio captioning: word selection indeterminacy and\nsentence length indeterminacy. We simultaneously solve the main caption\ngeneration and sub indeterminacy problems by estimating keywords and sentence\nlength through multi-task learning. We tested a simplified model of our\nsubmission using the development-testing dataset. Our model achieved 20.7\nSPIDEr score where that of the baseline system was 5.4.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 04:26:27 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Koizumi", "Yuma", ""], ["Takeuchi", "Daiki", ""], ["Ohishi", "Yasunori", ""], ["Harada", "Noboru", ""], ["Kashino", "Kunio", ""]]}, {"id": "2007.00232", "submitter": "Xiaorui Liu", "authors": "Xiaorui Liu, Yao Li, Rongrong Wang, Jiliang Tang, Ming Yan", "title": "Linear Convergent Decentralized Optimization with Compression", "comments": "ICLR 2021 (International Conference on Learning Representations)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication compression has become a key strategy to speed up distributed\noptimization. However, existing decentralized algorithms with compression\nmainly focus on compressing DGD-type algorithms. They are unsatisfactory in\nterms of convergence rate, stability, and the capability to handle\nheterogeneous data. Motivated by primal-dual algorithms, this paper proposes\nthe first \\underline{L}in\\underline{EA}r convergent \\underline{D}ecentralized\nalgorithm with compression, LEAD. Our theory describes the coupled dynamics of\nthe inexact primal and dual update as well as compression error, and we provide\nthe first consensus error bound in such settings without assuming bounded\ngradients. Experiments on convex problems validate our theoretical analysis,\nand empirical study on deep neural nets shows that LEAD is applicable to\nnon-convex problems.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 04:35:00 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 20:46:05 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Liu", "Xiaorui", ""], ["Li", "Yao", ""], ["Wang", "Rongrong", ""], ["Tang", "Jiliang", ""], ["Yan", "Ming", ""]]}, {"id": "2007.00237", "submitter": "Rohit Babbar", "authors": "Erik Schultheis, Mohammadreza Qaraei, Priyanshu Gupta, and Rohit\n  Babbar", "title": "Unbiased Loss Functions for Extreme Classification With Missing Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal in extreme multi-label classification (XMC) is to tag an instance\nwith a small subset of relevant labels from an extremely large set of possible\nlabels. In addition to the computational burden arising from large number of\ntraining instances, features and labels, problems in XMC are faced with two\nstatistical challenges, (i) large number of 'tail-labels' -- those which occur\nvery infrequently, and (ii) missing labels as it is virtually impossible to\nmanually assign every relevant label to an instance. In this work, we derive an\nunbiased estimator for general formulation of loss functions which decompose\nover labels, and then infer the forms for commonly used loss functions such as\nhinge- and squared-hinge-loss and binary cross-entropy loss. We show that the\nderived unbiased estimators, in the form of appropriate weighting factors, can\nbe easily incorporated in state-of-the-art algorithms for extreme\nclassification, thereby scaling to datasets with hundreds of thousand labels.\nHowever, empirically, we find a slightly altered version that gives more\nrelative weight to tail labels to perform even better. We suspect is due to the\nlabel imbalance in the dataset, which is not explicitly addressed by our\ntheoretically derived estimator. Minimizing the proposed loss functions leads\nto significant improvement over existing methods (up to 20% in some cases) on\nbenchmark datasets in XMC.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 04:42:12 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Schultheis", "Erik", ""], ["Qaraei", "Mohammadreza", ""], ["Gupta", "Priyanshu", ""], ["Babbar", "Rohit", ""]]}, {"id": "2007.00240", "submitter": "Dongxian Wu", "authors": "Dongxian Wu, Yisen Wang, Zhuobin Zheng, Shu-tao Xia", "title": "Temporal Calibrated Regularization for Robust Noisy Label Learning", "comments": "Published as a conference paper at IJCNN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) exhibit great success on many tasks with the help\nof large-scale well annotated datasets. However, labeling large-scale data can\nbe very costly and error-prone so that it is difficult to guarantee the\nannotation quality (i.e., having noisy labels). Training on these noisy labeled\ndatasets may adversely deteriorate their generalization performance. Existing\nmethods either rely on complex training stage division or bring too much\ncomputation for marginal performance improvement. In this paper, we propose a\nTemporal Calibrated Regularization (TCR), in which we utilize the original\nlabels and the predictions in the previous epoch together to make DNN inherit\nthe simple pattern it has learned with little overhead. We conduct extensive\nexperiments on various neural network architectures and datasets, and find that\nit consistently enhances the robustness of DNNs to label noise.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 04:48:49 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Wu", "Dongxian", ""], ["Wang", "Yisen", ""], ["Zheng", "Zhuobin", ""], ["Xia", "Shu-tao", ""]]}, {"id": "2007.00254", "submitter": "Arabin Kumar Dey", "authors": "Shankhyajyoti De, Arabin Kumar Dey, and Deepak Gauda", "title": "Construction of confidence interval for a univariate stock price signal\n  predicted through Long Short Term Memory Network", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show an innovative way to construct bootstrap confidence\ninterval of a signal estimated based on a univariate LSTM model. We take three\ndifferent types of bootstrap methods for dependent set up. We prescribe some\nuseful suggestions to select the optimal block length while performing the\nbootstrapping of the sample. We also propose a benchmark to compare the\nconfidence interval measured through different bootstrap strategies. We\nillustrate the experimental results through some stock price data set.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 05:28:20 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["De", "Shankhyajyoti", ""], ["Dey", "Arabin Kumar", ""], ["Gauda", "Deepak", ""]]}, {"id": "2007.00271", "submitter": "So Yeon Min", "authors": "So Yeon Min, Preethi Raghavan and Peter Szolovits", "title": "TransINT: Embedding Implication Rules in Knowledge Graphs with\n  Isomorphic Intersections of Linear Subspaces", "comments": "Conference Paper published in the proceedings of AKBC (Automated\n  Knowledge Base Construction) 2020\n  (https://openreview.net/forum?id=shkmWLRBXH)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Graphs (KG), composed of entities and relations, provide a\nstructured representation of knowledge. For easy access to statistical\napproaches on relational data, multiple methods to embed a KG into f(KG) $\\in$\nR^d have been introduced. We propose TransINT, a novel and interpretable KG\nembedding method that isomorphically preserves the implication ordering among\nrelations in the embedding space. Given implication rules, TransINT maps set of\nentities (tied by a relation) to continuous sets of vectors that are\ninclusion-ordered isomorphically to relation implications. With a novel\nparameter sharing scheme, TransINT enables automatic training on missing but\nimplied facts without rule grounding. On a benchmark dataset, we outperform the\nbest existing state-of-the-art rule integration embedding methods with\nsignificant margins in link Prediction and triple Classification. The angles\nbetween the continuous sets embedded by TransINT provide an interpretable way\nto mine semantic relatedness and implication rules among relations.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 06:45:27 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Min", "So Yeon", ""], ["Raghavan", "Preethi", ""], ["Szolovits", "Peter", ""]]}, {"id": "2007.00289", "submitter": "Kevin Bello", "authors": "Qiuling Xu and Kevin Bello and Jean Honorio", "title": "A Le Cam Type Bound for Adversarial Learning and Applications", "comments": "10 pages", "journal-ref": "IEEE International Symposium on Information Theory (ISIT), 2021", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robustness of machine learning methods is essential for modern practical\napplications. Given the arms race between attack and defense methods, one may\nbe curious regarding the fundamental limits of any defense mechanism. In this\nwork, we focus on the problem of learning from noise-injected data, where the\nexisting literature falls short by either assuming a specific attack method or\nby over-specifying the learning problem. We shed light on the\ninformation-theoretic limits of adversarial learning without assuming a\nparticular learning process or attacker. Finally, we apply our general bounds\nto a canonical set of non-trivial learning problems and provide examples of\ncommon types of attacks.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 07:29:16 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 21:27:30 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Xu", "Qiuling", ""], ["Bello", "Kevin", ""], ["Honorio", "Jean", ""]]}, {"id": "2007.00295", "submitter": "Jonathan Kuck", "authors": "Jonathan Kuck, Shuvam Chakraborty, Hao Tang, Rachel Luo, Jiaming Song,\n  Ashish Sabharwal, Stefano Ermon", "title": "Belief Propagation Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learned neural solvers have successfully been used to solve combinatorial\noptimization and decision problems. More general counting variants of these\nproblems, however, are still largely solved with hand-crafted solvers. To\nbridge this gap, we introduce belief propagation neural networks (BPNNs), a\nclass of parameterized operators that operate on factor graphs and generalize\nBelief Propagation (BP). In its strictest form, a BPNN layer (BPNN-D) is a\nlearned iterative operator that provably maintains many of the desirable\nproperties of BP for any choice of the parameters. Empirically, we show that by\ntraining BPNN-D learns to perform the task better than the original BP: it\nconverges 1.7x faster on Ising models while providing tighter bounds. On\nchallenging model counting problems, BPNNs compute estimates 100's of times\nfaster than state-of-the-art handcrafted methods, while returning an estimate\nof comparable quality.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 07:39:51 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Kuck", "Jonathan", ""], ["Chakraborty", "Shuvam", ""], ["Tang", "Hao", ""], ["Luo", "Rachel", ""], ["Song", "Jiaming", ""], ["Sabharwal", "Ashish", ""], ["Ermon", "Stefano", ""]]}, {"id": "2007.00306", "submitter": "Paolo Baracca", "authors": "Alessandro Brighente, Jafar Mohammadi, Paolo Baracca", "title": "Interference Distribution Prediction for Link Adaptation in\n  Ultra-Reliable Low-Latency Communications", "comments": "IEEE VTC-Spring 2020; minor notation changes in Sections III-V", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The strict latency and reliability requirements of ultra-reliable low-latency\ncommunications (URLLC) use cases are among the main drivers in fifth generation\n(5G) network design. Link adaptation (LA) is considered to be one of the\nbottlenecks to realize URLLC. In this paper, we focus on predicting the signal\nto interference plus noise ratio at the user to enhance the LA. Motivated by\nthe fact that most of the URLLC use cases with most extreme latency and\nreliability requirements are characterized by semi-deterministic traffic, we\npropose to exploit the time correlation of the interference to compute useful\nstatistics needed to predict the interference power in the next transmission.\nThis prediction is exploited in the LA context to maximize the spectral\nefficiency while guaranteeing reliability at an arbitrary level. Numerical\nresults are compared with state of the art interference prediction techniques\nfor LA. We show that exploiting time correlation of the interference is an\nimportant enabler of URLLC.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 07:59:35 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Brighente", "Alessandro", ""], ["Mohammadi", "Jafar", ""], ["Baracca", "Paolo", ""]]}, {"id": "2007.00334", "submitter": "Minhyeok Lee", "authors": "Minhyeok Lee, Junhee Seok", "title": "Estimation with Uncertainty via Conditional Generative Adversarial\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional predictive Artificial Neural Networks (ANNs) commonly employ\ndeterministic weight matrices; therefore, their prediction is a point estimate.\nSuch a deterministic nature in ANNs causes the limitations of using ANNs for\nmedical diagnosis, law problems, and portfolio management, in which discovering\nnot only the prediction but also the uncertainty of the prediction is\nessentially required. To address such a problem, we propose a predictive\nprobabilistic neural network model, which corresponds to a different manner of\nusing the generator in conditional Generative Adversarial Network (cGAN) that\nhas been routinely used for conditional sample generation. By reversing the\ninput and output of ordinary cGAN, the model can be successfully used as a\npredictive model; besides, the model is robust against noises since adversarial\ntraining is employed. In addition, to measure the uncertainty of predictions,\nwe introduce the entropy and relative entropy for regression problems and\nclassification problems, respectively. The proposed framework is applied to\nstock market data and an image classification task. As a result, the proposed\nframework shows superior estimation performance, especially on noisy data;\nmoreover, it is demonstrated that the proposed framework can properly estimate\nthe uncertainty of predictions.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 08:54:17 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Lee", "Minhyeok", ""], ["Seok", "Junhee", ""]]}, {"id": "2007.00339", "submitter": "Weizhu Qian", "authors": "Weizhu Qian, Bowei Chen, Yichao Zhang, Guanghui Wen and Franck Gechter", "title": "Multi-Task Variational Information Bottleneck", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-task learning (MTL) is an important subject in machine learning and\nartificial intelligence. Its applications to computer vision, signal\nprocessing, and speech recognition are ubiquitous. Although this subject has\nattracted considerable attention recently, the performance and robustness of\nthe existing models to different tasks have not been well balanced. This\narticle proposes an MTL model based on the architecture of the variational\ninformation bottleneck (VIB), which can provide a more effective latent\nrepresentation of the input features for the downstream tasks. Extensive\nobservations on three public data sets under adversarial attacks show that the\nproposed model is competitive to the state-of-the-art algorithms concerning the\nprediction accuracy. Experimental results suggest that combining the VIB and\nthe task-dependent uncertainties is a very effective way to abstract valid\ninformation from the input features for accomplishing multiple tasks.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 09:06:20 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 13:10:30 GMT"}, {"version": "v3", "created": "Fri, 18 Dec 2020 09:35:12 GMT"}, {"version": "v4", "created": "Mon, 1 Mar 2021 12:12:22 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Qian", "Weizhu", ""], ["Chen", "Bowei", ""], ["Zhang", "Yichao", ""], ["Wen", "Guanghui", ""], ["Gechter", "Franck", ""]]}, {"id": "2007.00346", "submitter": "Clemens Damke", "authors": "Clemens Damke, Vitalik Melnikov, Eyke H\\\"ullermeier", "title": "A Novel Higher-order Weisfeiler-Lehman Graph Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current GNN architectures use a vertex neighborhood aggregation scheme, which\nlimits their discriminative power to that of the 1-dimensional\nWeisfeiler-Lehman (WL) graph isomorphism test. Here, we propose a novel graph\nconvolution operator that is based on the 2-dimensional WL test. We formally\nshow that the resulting 2-WL-GNN architecture is more discriminative than\nexisting GNN approaches. This theoretical result is complemented by\nexperimental studies using synthetic and real data. On multiple common graph\nclassification benchmarks, we demonstrate that the proposed model is\ncompetitive with state-of-the-art graph kernels and GNNs.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 09:32:01 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 11:33:32 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Damke", "Clemens", ""], ["Melnikov", "Vitalik", ""], ["H\u00fcllermeier", "Eyke", ""]]}, {"id": "2007.00347", "submitter": "Nicolai Engelmann", "authors": "Nicolai Engelmann, Dominik Linzner, Heinz Koeppl", "title": "Continuous-Time Bayesian Networks with Clocks", "comments": "10 main pages, 7 pages appendix, 3 figures, proceedings of ICML2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured stochastic processes evolving in continuous time present a widely\nadopted framework to model phenomena occurring in nature and engineering.\nHowever, such models are often chosen to satisfy the Markov property to\nmaintain tractability. One of the more popular of such memoryless models are\nContinuous Time Bayesian Networks (CTBNs). In this work, we lift its\nrestriction to exponential survival times to arbitrary distributions. Current\nextensions achieve this via auxiliary states, which hinder tractability. To\navoid that, we introduce a set of node-wise clocks to construct a collection of\ngraph-coupled semi-Markov chains. We provide algorithms for parameter and\nstructure inference, which make use of local dependencies and conduct\nexperiments on synthetic data and a data-set generated through a benchmark tool\nfor gene regulatory networks. In doing so, we point out advantages compared to\ncurrent CTBN extensions.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 09:33:39 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 19:16:47 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Engelmann", "Nicolai", ""], ["Linzner", "Dominik", ""], ["Koeppl", "Heinz", ""]]}, {"id": "2007.00350", "submitter": "Kuan Fang", "authors": "Kuan Fang, Yuke Zhu, Silvio Savarese, Li Fei-Fei", "title": "Adaptive Procedural Task Generation for Hard-Exploration Problems", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Adaptive Procedural Task Generation (APT-Gen), an approach to\nprogressively generate a sequence of tasks as curricula to facilitate\nreinforcement learning in hard-exploration problems. At the heart of our\napproach, a task generator learns to create tasks from a parameterized task\nspace via a black-box procedural generation module. To enable curriculum\nlearning in the absence of a direct indicator of learning progress, we propose\nto train the task generator by balancing the agent's performance in the\ngenerated tasks and the similarity to the target tasks. Through adversarial\ntraining, the task similarity is adaptively estimated by a task discriminator\ndefined on the agent's experiences, allowing the generated tasks to approximate\ntarget tasks of unknown parameterization or outside of the predefined task\nspace. Our experiments on the grid world and robotic manipulation task domains\nshow that APT-Gen achieves substantially better performance than various\nexisting baselines by generating suitable tasks of rich variations.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 09:38:51 GMT"}, {"version": "v2", "created": "Sat, 3 Oct 2020 01:05:29 GMT"}, {"version": "v3", "created": "Thu, 18 Mar 2021 08:53:32 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Fang", "Kuan", ""], ["Zhu", "Yuke", ""], ["Savarese", "Silvio", ""], ["Fei-Fei", "Li", ""]]}, {"id": "2007.00360", "submitter": "Dominic Richards", "authors": "Dominic Richards, Patrick Rebeschini and Lorenzo Rosasco", "title": "Decentralised Learning with Random Features and Distributed Gradient\n  Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the generalisation performance of Distributed Gradient Descent\nwith Implicit Regularisation and Random Features in the homogenous setting\nwhere a network of agents are given data sampled independently from the same\nunknown distribution. Along with reducing the memory footprint, Random Features\nare particularly convenient in this setting as they provide a common\nparameterisation across agents that allows to overcome previous difficulties in\nimplementing Decentralised Kernel Regression. Under standard source and\ncapacity assumptions, we establish high probability bounds on the predictive\nperformance for each agent as a function of the step size, number of\niterations, inverse spectral gap of the communication matrix and number of\nRandom Features. By tuning these parameters, we obtain statistical rates that\nare minimax optimal with respect to the total number of samples in the network.\nThe algorithm provides a linear improvement over single machine Gradient\nDescent in memory cost and, when agents hold enough data with respect to the\nnetwork size and inverse spectral gap, a linear speed-up in computational\nruntime for any network topology. We present simulations that show how the\nnumber of Random Features, iterations and samples impact predictive\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 09:55:09 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Richards", "Dominic", ""], ["Rebeschini", "Patrick", ""], ["Rosasco", "Lorenzo", ""]]}, {"id": "2007.00373", "submitter": "Juanping Zhu", "authors": "Juanping Zhu, Hairong Gu", "title": "Can Global Optimization Strategy Outperform Myopic Strategy for Bayesian\n  Parameter Estimation?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian adaptive inference is widely used in psychophysics to estimate\npsychometric parameters. Most applications used myopic one-step ahead strategy\nwhich only optimizes the immediate utility. The widely held expectation is that\nglobal optimization strategies that explicitly optimize over some horizon can\nlargely improve the performance of the myopic strategy. With limited studies\nthat compared myopic and global strategies, the expectation was not challenged\nand researchers are still investing heavily to achieve global optimization. Is\nthat really worthwhile? This paper provides a discouraging answer based on\nexperimental simulations comparing the performance improvement and computation\nburden between global and myopic strategies in parameter estimation of multiple\nmodels. The finding is that the added horizon in global strategies has\nnegligible contributions to the improvement of optimal global utility other\nthan the most immediate next steps (of myopic strategy). Mathematical recursion\nis derived to prove that the contribution of utility improvement of each added\nhorizon step diminishes fast as that step moves further into the future.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 10:31:16 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Zhu", "Juanping", ""], ["Gu", "Hairong", ""]]}, {"id": "2007.00389", "submitter": "Joost van Amersfoort", "authors": "Joost van Amersfoort, Milad Alizadeh, Sebastian Farquhar, Nicholas\n  Lane, Yarin Gal", "title": "Single Shot Structured Pruning Before Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method to speed up training by 2x and inference by 3x in deep\nneural networks using structured pruning applied before training. Unlike\nprevious works on pruning before training which prune individual weights, our\nwork develops a methodology to remove entire channels and hidden units with the\nexplicit aim of speeding up training and inference. We introduce a\ncompute-aware scoring mechanism which enables pruning in units of sensitivity\nper FLOP removed, allowing even greater speed ups. Our method is fast, easy to\nimplement, and needs just one forward/backward pass on a single batch of data\nto complete pruning before training begins.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 11:27:37 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["van Amersfoort", "Joost", ""], ["Alizadeh", "Milad", ""], ["Farquhar", "Sebastian", ""], ["Lane", "Nicholas", ""], ["Gal", "Yarin", ""]]}, {"id": "2007.00399", "submitter": "Scott Pesme", "authors": "Scott Pesme and Nicolas Flammarion", "title": "Online Robust Regression via SGD on the l1 loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the robust linear regression problem in the online setting where\nwe have access to the data in a streaming manner, one data point after the\nother. More specifically, for a true parameter $\\theta^*$, we consider the\ncorrupted Gaussian linear model $y = \\langle x , \\ \\theta^* \\rangle +\n\\varepsilon + b$ where the adversarial noise $b$ can take any value with\nprobability $\\eta$ and equals zero otherwise. We consider this adversary to be\noblivious (i.e., $b$ independent of the data) since this is the only\ncontamination model under which consistency is possible. Current algorithms\nrely on having the whole data at hand in order to identify and remove the\noutliers. In contrast, we show in this work that stochastic gradient descent on\nthe $\\ell_1$ loss converges to the true parameter vector at a $\\tilde{O}( 1 /\n(1 - \\eta)^2 n )$ rate which is independent of the values of the contaminated\nmeasurements. Our proof relies on the elegant smoothing of the non-smooth\n$\\ell_1$ loss by the Gaussian data and a classical non-asymptotic analysis of\nPolyak-Ruppert averaged SGD. In addition, we provide experimental evidence of\nthe efficiency of this simple and highly scalable algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 11:38:21 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Pesme", "Scott", ""], ["Flammarion", "Nicolas", ""]]}, {"id": "2007.00411", "submitter": "Vibhor Gupta", "authors": "Vibhor Gupta, Jyoti Narwariya, Pankaj Malhotra, Lovekesh Vig, Gautam\n  Shroff", "title": "Handling Variable-Dimensional Time Series with Graph Neural Networks", "comments": "Accepted at AI4IoT@IJCAI'20 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several applications of Internet of Things (IoT) technology involve capturing\ndata from multiple sensors resulting in multi-sensor time series. Existing\nneural networks based approaches for such multi-sensor or multivariate time\nseries modeling assume fixed input dimension or number of sensors. Such\napproaches can struggle in the practical setting where different instances of\nthe same device or equipment such as mobiles, wearables, engines, etc. come\nwith different combinations of installed sensors. We consider training neural\nnetwork models from such multi-sensor time series, where the time series have\nvarying input dimensionality owing to availability or installation of a\ndifferent subset of sensors at each source of time series. We propose a novel\nneural network architecture suitable for zero-shot transfer learning allowing\nrobust inference for multivariate time series with previously unseen\ncombination of available dimensions or sensors at test time. Such a\ncombinatorial generalization is achieved by conditioning the layers of a core\nneural network-based time series model with a \"conditioning vector\" that\ncarries information of the available combination of sensors for each time\nseries. This conditioning vector is obtained by summarizing the set of learned\n\"sensor embedding vectors\" corresponding to the available sensors in a time\nseries via a graph neural network. We evaluate the proposed approach on\npublicly available activity recognition and equipment prognostics datasets, and\nshow that the proposed approach allows for better generalization in comparison\nto a deep gated recurrent neural network baseline.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 12:11:16 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 15:24:52 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2020 03:57:59 GMT"}, {"version": "v4", "created": "Tue, 7 Jul 2020 04:50:36 GMT"}, {"version": "v5", "created": "Mon, 20 Jul 2020 06:42:21 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Gupta", "Vibhor", ""], ["Narwariya", "Jyoti", ""], ["Malhotra", "Pankaj", ""], ["Vig", "Lovekesh", ""], ["Shroff", "Gautam", ""]]}, {"id": "2007.00419", "submitter": "Sylvain Courtain", "authors": "Pierre Leleux, Sylvain Courtain, Guillaume Guex and Marco Saerens", "title": "Sparse Randomized Shortest Paths Routing with Tsallis Divergence\n  Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work elaborates on the important problem of (1) designing optimal\nrandomized routing policies for reaching a target node t from a source note s\non a weighted directed graph G and (2) defining distance measures between nodes\ninterpolating between the least cost (based on optimal movements) and the\ncommute-cost (based on a random walk on G), depending on a temperature\nparameter T. To this end, the randomized shortest path formalism (RSP,\n[2,99,124]) is rephrased in terms of Tsallis divergence regularization, instead\nof Kullback-Leibler divergence. The main consequence of this change is that the\nresulting routing policy (local transition probabilities) becomes sparser when\nT decreases, therefore inducing a sparse random walk on G converging to the\nleast-cost directed acyclic graph when T tends to 0. Experimental comparisons\non node clustering and semi-supervised classification tasks show that the\nderived dissimilarity measures based on expected routing costs provide\nstate-of-the-art results. The sparse RSP is therefore a promising model of\nmovements on a graph, balancing sparse exploitation and exploration in an\noptimal way.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 12:22:26 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Leleux", "Pierre", ""], ["Courtain", "Sylvain", ""], ["Guex", "Guillaume", ""], ["Saerens", "Marco", ""]]}, {"id": "2007.00425", "submitter": "Parameswaran Kamalaruban Dr.", "authors": "Martin Troussard, Emmanuel Pignat, Parameswaran Kamalaruban, Sylvain\n  Calinon, Volkan Cevher", "title": "Interaction-limited Inverse Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an inverse reinforcement learning (IRL) framework to\naccelerate learning when the learner-teacher \\textit{interaction} is\n\\textit{limited} during training. Our setting is motivated by the realistic\nscenarios where a helpful teacher is not available or when the teacher cannot\naccess the learning dynamics of the student. We present two different training\nstrategies: Curriculum Inverse Reinforcement Learning (CIRL) covering the\nteacher's perspective, and Self-Paced Inverse Reinforcement Learning (SPIRL)\nfocusing on the learner's perspective. Using experiments in simulations and\nexperiments with a real robot learning a task from a human demonstrator, we\nshow that our training strategies can allow a faster training than a random\nteacher for CIRL and than a batch learner for SPIRL.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 12:31:52 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Troussard", "Martin", ""], ["Pignat", "Emmanuel", ""], ["Kamalaruban", "Parameswaran", ""], ["Calinon", "Sylvain", ""], ["Cevher", "Volkan", ""]]}, {"id": "2007.00426", "submitter": "Michael Tashman", "authors": "Michael Tashman, Jiayi Xie, John Hoffman, Lee Winikor, Rouzbeh Gerami", "title": "Dynamic Bidding Strategies with Multivariate Feedback Control for\n  Multiple Goals in Display Advertising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-Time Bidding (RTB) display advertising is a method for purchasing\ndisplay advertising inventory in auctions that occur within milliseconds. The\nperformance of RTB campaigns is generally measured with a series of Key\nPerformance Indicators (KPIs) - measurements used to ensure that the campaign\nis cost-effective and that it is purchasing valuable inventory. While an RTB\ncampaign should ideally meet all KPIs, simultaneous improvement tends to be\nvery challenging, as an improvement to any one KPI risks a detrimental effect\ntoward the others. Here we present an approach to simultaneously controlling\nmultiple KPIs with a PID-based feedback-control system. This method generates a\ncontrol score for each KPI, based on both the output of a PID controller module\nand a metric that quantifies the importance of each KPI for internal business\nneeds. On regular intervals, this algorithm - Sequential Control - will choose\nthe KPI with the greatest overall need for improvement. In this way, our\nalgorithm is able to continually seek the greatest marginal improvements to its\ncurrent state. Multiple methods of control can be associated with each KPI, and\ncan be triggered either simultaneously or chosen stochastically, in order to\navoid local optima. In both offline ad bidding simulations and testing on live\ntraffic, our methods proved to be effective in simultaneously controlling\nmultiple KPIs, and bringing them toward their respective goals.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 15:49:08 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Tashman", "Michael", ""], ["Xie", "Jiayi", ""], ["Hoffman", "John", ""], ["Winikor", "Lee", ""], ["Gerami", "Rouzbeh", ""]]}, {"id": "2007.00479", "submitter": "Alex Goe{\\ss}mann", "authors": "Alex Goe{\\ss}mann and Gitta Kutyniok", "title": "The Restricted Isometry of ReLU Networks: Generalization through Norm\n  Concentration", "comments": "27 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While regression tasks aim at interpolating a relation on the entire input\nspace, they often have to be solved with a limited amount of training data.\nStill, if the hypothesis functions can be sketched well with the data, one can\nhope for identifying a generalizing model.\n  In this work, we introduce with the Neural Restricted Isometry Property\n(NeuRIP) a uniform concentration event, in which all shallow $\\mathrm{ReLU}$\nnetworks are sketched with the same quality. To derive the sample complexity\nfor achieving NeuRIP, we bound the covering numbers of the networks in the\nSub-Gaussian metric and apply chaining techniques. In case of the NeuRIP event,\nwe then provide bounds on the expected risk, which hold for networks in any\nsublevel set of the empirical risk. We conclude that all networks with\nsufficiently small empirical risk generalize uniformly.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 13:36:13 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Goe\u00dfmann", "Alex", ""], ["Kutyniok", "Gitta", ""]]}, {"id": "2007.00492", "submitter": "Shaoqing Yuan", "authors": "Shaoqing Yuan, Parminder Bhatia, Busra Celikkaya, Haiyang Liu,\n  Kyunghwan Choi", "title": "Towards User Friendly Medication Mapping Using Entity-Boosted Two-Tower\n  Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in medical entity linking have been applied in the area\nof scientific literature and social media data. However, with the adoption of\ntelemedicine and conversational agents such as Alexa in healthcare settings,\nmedical name inference has become an important task. Medication name inference\nis the task of mapping user friendly medication names from a free-form text to\na concept in a normalized medication list. This is challenging due to the\ndifferences in the use of medical terminology from health care professionals\nand user conversations coming from the lay public. We begin with mapping\ndescriptive medication phrases (DMP) to standard medication names (SMN). Given\nthe prescriptions of each patient, we want to provide them with the flexibility\nof referring to the medication in their preferred ways. We approach this as a\nranking problem which maps SMN to DMP by ordering the list of medications in\nthe patient's prescription list obtained from pharmacies. Furthermore, we\nleveraged the output of intermediate layers and performed medication\nclustering. We present the Medication Inference Model (MIM) achieving\nstate-of-the-art results. By incorporating medical entities based attention, we\nhave obtained further improvement for ranking models.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 18:56:44 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 18:44:40 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Yuan", "Shaoqing", ""], ["Bhatia", "Parminder", ""], ["Celikkaya", "Busra", ""], ["Liu", "Haiyang", ""], ["Choi", "Kyunghwan", ""]]}, {"id": "2007.00523", "submitter": "Jose Jimenez-Luna", "authors": "Jos\\'e Jim\\'enez-Luna, Francesca Grisoni, Gisbert Schneider", "title": "Drug discovery with explainable artificial intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning bears promise for drug discovery, including advanced image\nanalysis, prediction of molecular structure and function, and automated\ngeneration of innovative chemical entities with bespoke properties. Despite the\ngrowing number of successful prospective applications, the underlying\nmathematical models often remain elusive to interpretation by the human mind.\nThere is a demand for 'explainable' deep learning methods to address the need\nfor a new narrative of the machine language of the molecular sciences. This\nreview summarizes the most prominent algorithmic concepts of explainable\nartificial intelligence, and dares a forecast of the future opportunities,\npotential applications, and remaining challenges.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 14:36:23 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 09:47:01 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Jim\u00e9nez-Luna", "Jos\u00e9", ""], ["Grisoni", "Francesca", ""], ["Schneider", "Gisbert", ""]]}, {"id": "2007.00533", "submitter": "Georgina Hall", "authors": "Georgina Hall and Laurent Massouli\\'e", "title": "Partial Recovery in the Graph Alignment Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG cs.SI math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the graph alignment problem, which is the problem\nof recovering, given two graphs, a one-to-one mapping between nodes that\nmaximizes edge overlap. This problem can be viewed as a noisy version of the\nwell-known graph isomorphism problem and appears in many applications,\nincluding social network deanonymization and cellular biology. Our focus here\nis on partial recovery, i.e., we look for a one-to-one mapping which is correct\non a fraction of the nodes of the graph rather than on all of them, and we\nassume that the two input graphs to the problem are correlated\nErd\\H{o}s-R\\'enyi graphs of parameters $(n,q,s)$. Our main contribution is then\nto give necessary and sufficient conditions on $(n,q,s)$ under which partial\nrecovery is possible with high probability as the number of nodes $n$ goes to\ninfinity. In particular, we show that it is possible to achieve partial\nrecovery in the $nqs=\\Theta(1)$ regime under certain additional assumptions. An\ninteresting byproduct of the analysis techniques we develop to obtain the\nsufficiency result in the partial recovery setting is a tighter analysis of the\nmaximum likelihood estimator for the graph alignment problem, which leads to\nimproved sufficient conditions for exact recovery.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 14:57:53 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 16:45:16 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2020 14:13:40 GMT"}, {"version": "v4", "created": "Wed, 28 Jul 2021 08:42:00 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Hall", "Georgina", ""], ["Massouli\u00e9", "Laurent", ""]]}, {"id": "2007.00534", "submitter": "Scott Pesme", "authors": "Scott Pesme, Aymeric Dieuleveut, Nicolas Flammarion", "title": "On Convergence-Diagnostic based Step Sizes for Stochastic Gradient\n  Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constant step-size Stochastic Gradient Descent exhibits two phases: a\ntransient phase during which iterates make fast progress towards the optimum,\nfollowed by a stationary phase during which iterates oscillate around the\noptimal point. In this paper, we show that efficiently detecting this\ntransition and appropriately decreasing the step size can lead to fast\nconvergence rates. We analyse the classical statistical test proposed by Pflug\n(1983), based on the inner product between consecutive stochastic gradients.\nEven in the simple case where the objective function is quadratic we show that\nthis test cannot lead to an adequate convergence diagnostic. We then propose a\nnovel and simple statistical procedure that accurately detects stationarity and\nwe provide experimental results showing state-of-the-art performance on\nsynthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 14:58:01 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Pesme", "Scott", ""], ["Dieuleveut", "Aymeric", ""], ["Flammarion", "Nicolas", ""]]}, {"id": "2007.00544", "submitter": "Harald Bayerlein", "authors": "Harald Bayerlein, Mirco Theile, Marco Caccamo, David Gesbert", "title": "UAV Path Planning for Wireless Data Harvesting: A Deep Reinforcement\n  Learning Approach", "comments": "Code available under\n  https://github.com/hbayerlein/uav_data_harvesting, IEEE Global Communications\n  Conference (GLOBECOM) 2020", "journal-ref": null, "doi": "10.1109/GLOBECOM42002.2020.9322234", "report-no": null, "categories": "cs.LG cs.IT cs.RO eess.SP math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous deployment of unmanned aerial vehicles (UAVs) supporting\nnext-generation communication networks requires efficient trajectory planning\nmethods. We propose a new end-to-end reinforcement learning (RL) approach to\nUAV-enabled data collection from Internet of Things (IoT) devices in an urban\nenvironment. An autonomous drone is tasked with gathering data from distributed\nsensor nodes subject to limited flying time and obstacle avoidance. While\nprevious approaches, learning and non-learning based, must perform expensive\nrecomputations or relearn a behavior when important scenario parameters such as\nthe number of sensors, sensor positions, or maximum flying time, change, we\ntrain a double deep Q-network (DDQN) with combined experience replay to learn a\nUAV control policy that generalizes over changing scenario parameters. By\nexploiting a multi-layer map of the environment fed through convolutional\nnetwork layers to the agent, we show that our proposed network architecture\nenables the agent to make movement decisions for a variety of scenario\nparameters that balance the data collection goal with flight time efficiency\nand safety constraints. Considerable advantages in learning efficiency from\nusing a map centered on the UAV's position over a non-centered map are also\nillustrated.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 15:14:16 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 12:14:45 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Bayerlein", "Harald", ""], ["Theile", "Mirco", ""], ["Caccamo", "Marco", ""], ["Gesbert", "David", ""]]}, {"id": "2007.00590", "submitter": "Mert G\\\"urb\\\"uzbalaban", "authors": "Mert G\\\"urb\\\"uzbalaban, Xuefeng Gao, Yuanhan Hu, Lingjiong Zhu", "title": "Decentralized Stochastic Gradient Langevin Dynamics and Hamiltonian\n  Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient Langevin dynamics (SGLD) and stochastic gradient\nHamiltonian Monte Carlo (SGHMC) are two popular Markov Chain Monte Carlo (MCMC)\nalgorithms for Bayesian inference that can scale to large datasets, allowing to\nsample from the posterior distribution of the parameters of a statistical model\ngiven the input data and the prior distribution over the model parameters.\nHowever, these algorithms do not apply to the decentralized learning setting,\nwhen a network of agents are working collaboratively to learn the parameters of\na statistical model without sharing their individual data due to privacy\nreasons or communication constraints. We study two algorithms: Decentralized\nSGLD (DE-SGLD) and Decentralized SGHMC (DE-SGHMC) which are adaptations of SGLD\nand SGHMC methods that allow scaleable Bayesian inference in the decentralized\nsetting for large datasets. We show that when the posterior distribution is\nstrongly log-concave and smooth, the iterates of these algorithms converge\nlinearly to a neighborhood of the target distribution in the 2-Wasserstein\ndistance if their parameters are selected appropriately. We illustrate the\nefficiency of our algorithms on decentralized Bayesian linear regression and\nBayesian logistic regression problems.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 16:26:00 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 16:19:37 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2021 22:08:52 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["G\u00fcrb\u00fczbalaban", "Mert", ""], ["Gao", "Xuefeng", ""], ["Hu", "Yuanhan", ""], ["Zhu", "Lingjiong", ""]]}, {"id": "2007.00591", "submitter": "Antonia Gogoglou", "authors": "Antonia Gogoglou, Brian Nguyen, Alan Salimov, Jonathan Rider, C. Bayan\n  Bruss", "title": "Navigating the Dynamics of Financial Embeddings over Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial transactions constitute connections between entities and through\nthese connections a large scale heterogeneous weighted graph is formulated. In\nthis labyrinth of interactions that are continuously updated, there exists a\nvariety of similarity-based patterns that can provide insights into the\ndynamics of the financial system. With the current work, we propose the\napplication of Graph Representation Learning in a scalable dynamic setting as a\nmeans of capturing these patterns in a meaningful and robust way. We proceed to\nperform a rigorous qualitative analysis of the latent trajectories to extract\nreal world insights from the proposed representations and their evolution over\ntime that is to our knowledge the first of its kind in the financial sector.\nShifts in the latent space are associated with known economic events and in\nparticular the impact of the recent Covid-19 pandemic to consumer patterns.\nCapturing such patterns indicates the value added to financial modeling through\nthe incorporation of latent graph representations.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 16:27:31 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Gogoglou", "Antonia", ""], ["Nguyen", "Brian", ""], ["Salimov", "Alan", ""], ["Rider", "Jonathan", ""], ["Bruss", "C. Bayan", ""]]}, {"id": "2007.00595", "submitter": "Zach Moshe Mr.", "authors": "Zach Moshe (1), Asher Metzger (1), Gal Elidan (1 and 2), Frederik\n  Kratzert (4), Sella Nevo (1), Ran El-Yaniv (1 and 3) ((1) Google Research,\n  (2) The Hebrew University of Jerusalem, (3) Technion - Israel Institute of\n  Technology, (4) LIT AI Lab & Institute for Machine Learning, Johannes Kepler\n  University Linz)", "title": "HydroNets: Leveraging River Structure for Hydrologic Modeling", "comments": "Presented in the \"AI for physical sciences\" workshop, ICLR2020\n  (https://ai4earthscience.github.io/iclr-2020-workshop/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurate and scalable hydrologic models are essential building blocks of\nseveral important applications, from water resource management to timely flood\nwarnings. However, as the climate changes, precipitation and rainfall-runoff\npattern variations become more extreme, and accurate training data that can\naccount for the resulting distributional shifts become more scarce. In this\nwork we present a novel family of hydrologic models, called HydroNets, which\nleverages river network structure. HydroNets are deep neural network models\ndesigned to exploit both basin specific rainfall-runoff signals, and upstream\nnetwork dynamics, which can lead to improved predictions at longer horizons.\nThe injection of the river structure prior knowledge reduces sample complexity\nand allows for scalable and more accurate hydrologic modeling even with only a\nfew years of data. We present an empirical study over two large basins in India\nthat convincingly support the proposed model and its advantages.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 16:32:07 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Moshe", "Zach", "", "1 and 2"], ["Metzger", "Asher", "", "1 and 2"], ["Elidan", "Gal", "", "1 and 2"], ["Kratzert", "Frederik", "", "1 and 3"], ["Nevo", "Sella", "", "1 and 3"], ["El-Yaniv", "Ran", "", "1 and 3"]]}, {"id": "2007.00596", "submitter": "Fan Chen", "authors": "Fan Chen and Karl Rohe", "title": "A New Basis for Sparse PCA", "comments": "33 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The statistical and computational performance of sparse principal component\nanalysis (PCA) can be dramatically improved when the principal components are\nallowed to be sparse in a rotated eigenbasis. For this, we propose a new method\nfor sparse PCA. In the simplest version of the algorithm, the component scores\nand loadings are initialized with a low-rank singular value decomposition.\nThen, the singular vectors are rotated with orthogonal rotations to make them\napproximately sparse. Finally, soft-thresholding is applied to the rotated\nsingular vectors. This approach differs from prior approaches because it uses\nan orthogonal rotation to approximate a sparse basis. Our sparse PCA framework\nis versatile; for example, it extends naturally to the two-way analysis of a\ndata matrix for simultaneous dimensionality reduction of rows and columns. We\nidentify the close relationship between sparse PCA and independent component\nanalysis for separating sparse signals. We provide empirical evidence showing\nthat for the same level of sparsity, the proposed sparse PCA method is more\nstable and can explain more variance compared to alternative methods. Through\nthree applications---sparse coding of images, analysis of transcriptome\nsequencing data, and large-scale clustering of Twitter accounts, we demonstrate\nthe usefulness of sparse PCA in exploring modern multivariate data.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 16:32:22 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Chen", "Fan", ""], ["Rohe", "Karl", ""]]}, {"id": "2007.00611", "submitter": "Sina Ghiassian", "authors": "Sina Ghiassian, Andrew Patterson, Shivam Garg, Dhawal Gupta, Adam\n  White, Martha White", "title": "Gradient Temporal-Difference Learning with Regularized Corrections", "comments": "Appeared in Proceedings of the 37th International Conference on\n  Machine Learning (ICML2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is still common to use Q-learning and temporal difference (TD)\nlearning-even though they have divergence issues and sound Gradient TD\nalternatives exist-because divergence seems rare and they typically perform\nwell. However, recent work with large neural network learning systems reveals\nthat instability is more common than previously thought. Practitioners face a\ndifficult dilemma: choose an easy to use and performant TD method, or a more\ncomplex algorithm that is more sound but harder to tune and all but unexplored\nwith non-linear function approximation or control. In this paper, we introduce\na new method called TD with Regularized Corrections (TDRC), that attempts to\nbalance ease of use, soundness, and performance. It behaves as well as TD, when\nTD performs well, but is sound in cases where TD diverges. We empirically\ninvestigate TDRC across a range of problems, for both prediction and control,\nand for both linear and non-linear function approximation, and show,\npotentially for the first time, that gradient TD methods could be a better\nalternative to TD and Q-learning.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 16:56:56 GMT"}, {"version": "v2", "created": "Sun, 5 Jul 2020 17:53:35 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 16:59:45 GMT"}, {"version": "v4", "created": "Thu, 17 Sep 2020 21:17:00 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Ghiassian", "Sina", ""], ["Patterson", "Andrew", ""], ["Garg", "Shivam", ""], ["Gupta", "Dhawal", ""], ["White", "Adam", ""], ["White", "Martha", ""]]}, {"id": "2007.00628", "submitter": "Noam Finkelstein", "authors": "Noam Finkelstein, Ilya Shpitser", "title": "Deriving Bounds and Inequality Constraints Using LogicalRelations Among\n  Counterfactuals", "comments": null, "journal-ref": "Proceedings of the Thirty Sixth Conference on Uncertainty in\n  Artificial Intelligence (UAI-36th), 2020", "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal parameters may not be point identified in the presence of unobserved\nconfounding. However, information about non-identified parameters, in the form\nof bounds, may still be recovered from the observed data in some cases. We\ndevelop a new general method for obtaining bounds on causal parameters using\nrules of probability and restrictions on counterfactuals implied by causal\ngraphical models. We additionally provide inequality constraints on functionals\nof the observed data law implied by such causal models. Our approach is\nmotivated by the observation that logical relations between identified and\nnon-identified counterfactual events often yield information about\nnon-identified events. We show that this approach is powerful enough to recover\nknown sharp bounds and tight inequality constraints, and to derive novel bounds\nand constraints.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 17:25:44 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Finkelstein", "Noam", ""], ["Shpitser", "Ilya", ""]]}, {"id": "2007.00631", "submitter": "Yunzhu Li", "authors": "Yunzhu Li, Antonio Torralba, Animashree Anandkumar, Dieter Fox,\n  Animesh Garg", "title": "Causal Discovery in Physical Systems from Videos", "comments": "NeurIPS 2020. Project page: https://yunzhuli.github.io/V-CDN/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal discovery is at the core of human cognition. It enables us to reason\nabout the environment and make counterfactual predictions about unseen\nscenarios that can vastly differ from our previous experiences. We consider the\ntask of causal discovery from videos in an end-to-end fashion without\nsupervision on the ground-truth graph structure. In particular, our goal is to\ndiscover the structural dependencies among environmental and object variables:\ninferring the type and strength of interactions that have a causal effect on\nthe behavior of the dynamical system. Our model consists of (a) a perception\nmodule that extracts a semantically meaningful and temporally consistent\nkeypoint representation from images, (b) an inference module for determining\nthe graph distribution induced by the detected keypoints, and (c) a dynamics\nmodule that can predict the future by conditioning on the inferred graph. We\nassume access to different configurations and environmental conditions, i.e.,\ndata from unknown interventions on the underlying system; thus, we can hope to\ndiscover the correct underlying causal graph without explicit interventions. We\nevaluate our method in a planar multi-body interaction environment and\nscenarios involving fabrics of different shapes like shirts and pants.\nExperiments demonstrate that our model can correctly identify the interactions\nfrom a short sequence of images and make long-term future predictions. The\ncausal structure assumed by the model also allows it to make counterfactual\npredictions and extrapolate to systems of unseen interaction graphs or graphs\nof various sizes.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 17:29:57 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 19:11:32 GMT"}, {"version": "v3", "created": "Sun, 29 Nov 2020 20:47:06 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Li", "Yunzhu", ""], ["Torralba", "Antonio", ""], ["Anandkumar", "Animashree", ""], ["Fox", "Dieter", ""], ["Garg", "Animesh", ""]]}, {"id": "2007.00642", "submitter": "Rob Brekelmans", "authors": "Rob Brekelmans, Vaden Masrani, Frank Wood, Greg Ver Steeg, Aram\n  Galstyan", "title": "All in the Exponential Family: Bregman Duality in Thermodynamic\n  Variational Inference", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed Thermodynamic Variational Objective (TVO) leverages\nthermodynamic integration to provide a family of variational inference\nobjectives, which both tighten and generalize the ubiquitous Evidence Lower\nBound (ELBO). However, the tightness of TVO bounds was not previously known, an\nexpensive grid search was used to choose a \"schedule\" of intermediate\ndistributions, and model learning suffered with ostensibly tighter bounds. In\nthis work, we propose an exponential family interpretation of the geometric\nmixture curve underlying the TVO and various path sampling methods, which\nallows us to characterize the gap in TVO likelihood bounds as a sum of KL\ndivergences. We propose to choose intermediate distributions using equal\nspacing in the moment parameters of our exponential family, which matches grid\nsearch performance and allows the schedule to adaptively update over the course\nof training. Finally, we derive a doubly reparameterized gradient estimator\nwhich improves model learning and allows the TVO to benefit from more refined\nbounds. To further contextualize our contributions, we provide a unified\nframework for understanding thermodynamic integration and the TVO using Taylor\nseries remainders.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 17:46:49 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Brekelmans", "Rob", ""], ["Masrani", "Vaden", ""], ["Wood", "Frank", ""], ["Steeg", "Greg Ver", ""], ["Galstyan", "Aram", ""]]}, {"id": "2007.00644", "submitter": "Rohan Taori", "authors": "Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin\n  Recht, Ludwig Schmidt", "title": "Measuring Robustness to Natural Distribution Shifts in Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how robust current ImageNet models are to distribution shifts\narising from natural variations in datasets. Most research on robustness\nfocuses on synthetic image perturbations (noise, simulated weather artifacts,\nadversarial examples, etc.), which leaves open how robustness on synthetic\ndistribution shift relates to distribution shift arising in real data. Informed\nby an evaluation of 204 ImageNet models in 213 different test conditions, we\nfind that there is often little to no transfer of robustness from current\nsynthetic to natural distribution shift. Moreover, most current techniques\nprovide no robustness to the natural distribution shifts in our testbed. The\nmain exception is training on larger and more diverse datasets, which in\nmultiple cases increases robustness, but is still far from closing the\nperformance gaps. Our results indicate that distribution shifts arising in real\ndata are currently an open research problem. We provide our testbed and data as\na resource for future work at https://modestyachts.github.io/imagenet-testbed/ .\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 17:53:26 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 09:55:13 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Taori", "Rohan", ""], ["Dave", "Achal", ""], ["Shankar", "Vaishaal", ""], ["Carlini", "Nicholas", ""], ["Recht", "Benjamin", ""], ["Schmidt", "Ludwig", ""]]}, {"id": "2007.00655", "submitter": "Corbin Rosset", "authors": "Corby Rosset, Chenyan Xiong, Minh Phan, Xia Song, Paul Bennett,\n  Saurabh Tiwary", "title": "Knowledge-Aware Language Model Pretraining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How much knowledge do pretrained language models hold? Recent research\nobserved that pretrained transformers are adept at modeling semantics but it is\nunclear to what degree they grasp human knowledge, or how to ensure they do so.\nIn this paper we incorporate knowledge-awareness in language model pretraining\nwithout changing the transformer architecture, inserting explicit knowledge\nlayers, or adding external storage of semantic information. Rather, we simply\nsignal the existence of entities to the input of the transformer in\npretraining, with an entity-extended tokenizer; and at the output, with an\nadditional entity prediction task. Our experiments show that solely by adding\nthese entity signals in pretraining, significantly more knowledge is packed\ninto the transformer parameters: we observe improved language modeling\naccuracy, factual correctness in LAMA knowledge probing tasks, and semantics in\nthe hidden representations through edge probing.We also show that our\nknowledge-aware language model (KALM) can serve as a drop-in replacement for\nGPT-2 models, significantly improving downstream tasks like zero-shot\nquestion-answering with no task-related training.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 06:09:59 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 06:54:39 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Rosset", "Corby", ""], ["Xiong", "Chenyan", ""], ["Phan", "Minh", ""], ["Song", "Xia", ""], ["Bennett", "Paul", ""], ["Tiwary", "Saurabh", ""]]}, {"id": "2007.00674", "submitter": "Biwei Dai", "authors": "Biwei Dai and Uros Seljak", "title": "Sliced Iterative Normalizing Flows", "comments": "19 pages, 12 figures, 7 tables. Code available at\n  https://github.com/biweidai/SINF", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an iterative (greedy) deep learning (DL) algorithm which is able\nto transform an arbitrary probability distribution function (PDF) into the\ntarget PDF. The model is based on iterative Optimal Transport of a series of 1D\nslices, matching on each slice the marginal PDF to the target. The axes of the\northogonal slices are chosen to maximize the PDF difference using Wasserstein\ndistance at each iteration, which enables the algorithm to scale well to high\ndimensions. As special cases of this algorithm, we introduce two sliced\niterative Normalizing Flow (SINF) models, which map from the data to the latent\nspace (GIS) and vice versa (SIG). We show that SIG is able to generate high\nquality samples of image datasets, which match the GAN benchmarks, while GIS\nobtains competitive results on density estimation tasks compared to the density\ntrained NFs, and is more stable, faster, and achieves higher $p(x)$ when\ntrained on small training sets. SINF approach deviates significantly from the\ncurrent DL paradigm, as it is greedy and does not use concepts such as\nmini-batching, stochastic gradient descent and gradient back-propagation\nthrough deep layers.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 18:00:04 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 21:07:01 GMT"}, {"version": "v3", "created": "Mon, 14 Jun 2021 19:11:07 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Dai", "Biwei", ""], ["Seljak", "Uros", ""]]}, {"id": "2007.00689", "submitter": "Wei Wang", "authors": "Wei Wang and Haojie Li and Zhengming Ding and Zhihui Wang", "title": "Rethink Maximum Mean Discrepancy for Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing domain adaptation methods aim to reduce the distributional\ndifference between the source and target domains and respect their specific\ndiscriminative information, by establishing the Maximum Mean Discrepancy (MMD)\nand the discriminative distances. However, they usually accumulate to consider\nthose statistics and deal with their relationships by estimating parameters\nblindly. This paper theoretically proves two essential facts: 1) minimizing the\nMMD equals to maximize the source and target intra-class distances respectively\nbut jointly minimize their variance with some implicit weights, so that the\nfeature discriminability degrades; 2) the relationship between the intra-class\nand inter-class distances is as one falls, another rises. Based on this, we\npropose a novel discriminative MMD. On one hand, we consider the intra-class\nand inter-class distances alone to remove a redundant parameter, and the\nrevealed weights provide their approximate optimal ranges. On the other hand,\nwe design two different strategies to boost the feature discriminability: 1) we\ndirectly impose a trade-off parameter on the implicit intra-class distance in\nMMD to regulate its change; 2) we impose the similar weights revealed in MMD on\ninter-class distance and maximize it, then a balanced factor could be\nintroduced to quantitatively leverage the relative importance between the\nfeature transferability and its discriminability. The experiments on several\nbenchmark datasets not only prove the validity of theoretical results but also\ndemonstrate that our approach could perform better than the comparative\nstate-of-art methods substantially.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 18:25:10 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Wang", "Wei", ""], ["Li", "Haojie", ""], ["Ding", "Zhengming", ""], ["Wang", "Zhihui", ""]]}, {"id": "2007.00699", "submitter": "Jonathan Lee", "authors": "Jonathan N. Lee, Aldo Pacchiano, Peter Bartlett, Michael I. Jordan", "title": "Accelerated Message Passing for Entropy-Regularized MAP Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum a posteriori (MAP) inference in discrete-valued Markov random fields\nis a fundamental problem in machine learning that involves identifying the most\nlikely configuration of random variables given a distribution. Due to the\ndifficulty of this combinatorial problem, linear programming (LP) relaxations\nare commonly used to derive specialized message passing algorithms that are\noften interpreted as coordinate descent on the dual LP. To achieve more\ndesirable computational properties, a number of methods regularize the LP with\nan entropy term, leading to a class of smooth message passing algorithms with\nconvergence guarantees. In this paper, we present randomized methods for\naccelerating these algorithms by leveraging techniques that underlie classical\naccelerated gradient methods. The proposed algorithms incorporate the familiar\nsteps of standard smooth message passing algorithms, which can be viewed as\ncoordinate minimization steps. We show that these accelerated variants achieve\nfaster rates for finding $\\epsilon$-optimal points of the unregularized\nproblem, and, when the LP is tight, we prove that the proposed algorithms\nrecover the true MAP solution in fewer iterations than standard message passing\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 18:43:32 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Lee", "Jonathan N.", ""], ["Pacchiano", "Aldo", ""], ["Bartlett", "Peter", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2007.00708", "submitter": "Linnan Wang", "authors": "Linnan Wang, Rodrigo Fonseca, Yuandong Tian", "title": "Learning Search Space Partition for Black-box Optimization using Monte\n  Carlo Tree Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional black-box optimization has broad applications but remains a\nchallenging problem to solve. Given a set of samples $\\{\\vx_i, y_i\\}$, building\na global model (like Bayesian Optimization (BO)) suffers from the curse of\ndimensionality in the high-dimensional search space, while a greedy search may\nlead to sub-optimality. By recursively splitting the search space into regions\nwith high/low function values, recent works like LaNAS shows good performance\nin Neural Architecture Search (NAS), reducing the sample complexity\nempirically. In this paper, we coin LA-MCTS that extends LaNAS to other\ndomains. Unlike previous approaches, LA-MCTS learns the partition of the search\nspace using a few samples and their function values in an online fashion. While\nLaNAS uses linear partition and performs uniform sampling in each region, our\nLA-MCTS adopts a nonlinear decision boundary and learns a local model to pick\ngood candidates. If the nonlinear partition function and the local model fits\nwell with ground-truth black-box function, then good partitions and candidates\ncan be reached with much fewer samples. LA-MCTS serves as a\n\\emph{meta-algorithm} by using existing black-box optimizers (e.g., BO, TuRBO)\nas its local models, achieving strong performance in general black-box\noptimization and reinforcement learning benchmarks, in particular for\nhigh-dimensional problems.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 19:17:47 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Wang", "Linnan", ""], ["Fonseca", "Rodrigo", ""], ["Tian", "Yuandong", ""]]}, {"id": "2007.00714", "submitter": "Dominik Janzing", "authors": "Dominik Janzing, Patrick Bl\\\"obaum, Lenon Minorics, Philipp Faller", "title": "Quantifying causal contributions via structure preserving interventions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a concept to quantify the 'intrinsic' causal contribution of\neach variable in a causal directed acyclic graph to the uncertainty or\ninformation of some target variable. By recursively writing each node as\nfunction of the noise terms, we separate the information added by each node\nfrom the one obtained from its ancestors. To interpret this information as a\ncausal contribution, we consider 'structure-preserving interventions' that\nrandomize each node in a way that mimics the usual dependence on the parents\nand don't perturb the observed joint distribution. Using Shapley values, the\ncontribution becomes independent of the ordering of nodes. We describe our\ncontribution analysis for variance and entropy as two important examples, but\ncontributions for other target metrics can be defined analogously.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 19:34:08 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 15:42:40 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Janzing", "Dominik", ""], ["Bl\u00f6baum", "Patrick", ""], ["Minorics", "Lenon", ""], ["Faller", "Philipp", ""]]}, {"id": "2007.00715", "submitter": "Jacky Zhang", "authors": "Jacky Y. Zhang, Rajiv Khanna, Anastasios Kyrillidis, Oluwasanmi Koyejo", "title": "Bayesian Coresets: Revisiting the Nonconvex Optimization Perspective", "comments": "AISTATS 2021 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian coresets have emerged as a promising approach for implementing\nscalable Bayesian inference. The Bayesian coreset problem involves selecting a\n(weighted) subset of the data samples, such that the posterior inference using\nthe selected subset closely approximates the posterior inference using the full\ndataset. This manuscript revisits Bayesian coresets through the lens of\nsparsity constrained optimization. Leveraging recent advances in accelerated\noptimization methods, we propose and analyze a novel algorithm for coreset\nselection. We provide explicit convergence rate guarantees and present an\nempirical evaluation on a variety of benchmark datasets to highlight our\nproposed algorithm's superior performance compared to state-of-the-art on speed\nand accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 19:34:59 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 22:04:24 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Zhang", "Jacky Y.", ""], ["Khanna", "Rajiv", ""], ["Kyrillidis", "Anastasios", ""], ["Koyejo", "Oluwasanmi", ""]]}, {"id": "2007.00717", "submitter": "Sean Sinclair", "authors": "Sean R. Sinclair, Tianyu Wang, Gauri Jain, Siddhartha Banerjee,\n  Christina Lee Yu", "title": "Adaptive Discretization for Model-Based Reinforcement Learning", "comments": "50 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the technique of adaptive discretization to design an efficient\nmodel-based episodic reinforcement learning algorithm in large (potentially\ncontinuous) state-action spaces. Our algorithm is based on optimistic one-step\nvalue iteration extended to maintain an adaptive discretization of the space.\nFrom a theoretical perspective we provide worst-case regret bounds for our\nalgorithm which are competitive compared to the state-of-the-art model-based\nalgorithms. Moreover, our bounds are obtained via a modular proof technique\nwhich can potentially extend to incorporate additional structure on the\nproblem.\n  From an implementation standpoint, our algorithm has much lower storage and\ncomputational requirements due to maintaining a more efficient partition of the\nstate and action spaces. We illustrate this via experiments on several\ncanonical control problems, which shows that our algorithm empirically performs\nsignificantly better than fixed discretization in terms of both faster\nconvergence and lower memory usage. Interestingly, we observe empirically that\nwhile fixed-discretization model-based algorithms vastly outperform their\nmodel-free counterparts, the two achieve comparable performance with adaptive\ndiscretization.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 19:36:46 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 13:16:38 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Sinclair", "Sean R.", ""], ["Wang", "Tianyu", ""], ["Jain", "Gauri", ""], ["Banerjee", "Siddhartha", ""], ["Yu", "Christina Lee", ""]]}, {"id": "2007.00720", "submitter": "Avishek Bose", "authors": "Avishek Joey Bose, Gauthier Gidel, Hugo Berard, Andre Cianflone,\n  Pascal Vincent, Simon Lacoste-Julien and William L. Hamilton", "title": "Adversarial Example Games", "comments": "Appears in: Advances in Neural Information Processing Systems 33\n  (NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existence of adversarial examples capable of fooling trained neural\nnetwork classifiers calls for a much better understanding of possible attacks\nto guide the development of safeguards against them. This includes attack\nmethods in the challenging non-interactive blackbox setting, where adversarial\nattacks are generated without any access, including queries, to the target\nmodel. Prior attacks in this setting have relied mainly on algorithmic\ninnovations derived from empirical observations (e.g., that momentum helps),\nlacking principled transferability guarantees. In this work, we provide a\ntheoretical foundation for crafting transferable adversarial examples to entire\nhypothesis classes. We introduce Adversarial Example Games (AEG), a framework\nthat models the crafting of adversarial examples as a min-max game between a\ngenerator of attacks and a classifier. AEG provides a new way to design\nadversarial examples by adversarially training a generator and a classifier\nfrom a given hypothesis class (e.g., architecture). We prove that this game has\nan equilibrium, and that the optimal generator is able to craft adversarial\nexamples that can attack any classifier from the corresponding hypothesis\nclass. We demonstrate the efficacy of AEG on the MNIST and CIFAR-10 datasets,\noutperforming prior state-of-the-art approaches with an average relative\nimprovement of $29.9\\%$ and $47.2\\%$ against undefended and robust models\n(Table 2 & 3) respectively.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 19:47:23 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 05:56:03 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 02:47:01 GMT"}, {"version": "v4", "created": "Sat, 24 Oct 2020 02:15:06 GMT"}, {"version": "v5", "created": "Fri, 20 Nov 2020 05:07:25 GMT"}, {"version": "v6", "created": "Sat, 9 Jan 2021 01:44:02 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Bose", "Avishek Joey", ""], ["Gidel", "Gauthier", ""], ["Berard", "Hugo", ""], ["Cianflone", "Andre", ""], ["Vincent", "Pascal", ""], ["Lacoste-Julien", "Simon", ""], ["Hamilton", "William L.", ""]]}, {"id": "2007.00722", "submitter": "Andrea Tirinzoni", "authors": "Andrea Tirinzoni, Riccardo Poiani, Marcello Restelli", "title": "Sequential Transfer in Reinforcement Learning with a Generative Model", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in how to design reinforcement learning agents that\nprovably reduce the sample complexity for learning new tasks by transferring\nknowledge from previously-solved ones. The availability of solutions to related\nproblems poses a fundamental trade-off: whether to seek policies that are\nexpected to achieve high (yet sub-optimal) performance in the new task\nimmediately or whether to seek information to quickly identify an optimal\nsolution, potentially at the cost of poor initial behavior. In this work, we\nfocus on the second objective when the agent has access to a generative model\nof state-action pairs. First, given a set of solved tasks containing an\napproximation of the target one, we design an algorithm that quickly identifies\nan accurate solution by seeking the state-action pairs that are most\ninformative for this purpose. We derive PAC bounds on its sample complexity\nwhich clearly demonstrate the benefits of using this kind of prior knowledge.\nThen, we show how to learn these approximate tasks sequentially by reducing our\ntransfer setting to a hidden Markov model and employing spectral methods to\nrecover its parameters. Finally, we empirically verify our theoretical findings\nin simple simulated domains.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 19:53:35 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Tirinzoni", "Andrea", ""], ["Poiani", "Riccardo", ""], ["Restelli", "Marcello", ""]]}, {"id": "2007.00728", "submitter": "Andrew Ferguson", "authors": "Hythem Sidky, Wei Chen, Andrew L. Ferguson", "title": "Molecular Latent Space Simulators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small integration time steps limit molecular dynamics (MD) simulations to\nmillisecond time scales. Markov state models (MSMs) and equation-free\napproaches learn low-dimensional kinetic models from MD simulation data by\nperforming configurational or dynamical coarse-graining of the state space. The\nlearned kinetic models enable the efficient generation of dynamical\ntrajectories over vastly longer time scales than are accessible by MD, but the\ndiscretization of configurational space and/or absence of a means to\nreconstruct molecular configurations precludes the generation of continuous\nall-atom molecular trajectories. We propose latent space simulators (LSS) to\nlearn kinetic models for continuous all-atom simulation trajectories by\ntraining three deep learning networks to (i) learn the slow collective\nvariables of the molecular system, (ii) propagate the system dynamics within\nthis slow latent space, and (iii) generatively reconstruct molecular\nconfigurations. We demonstrate the approach in an application to Trp-cage\nminiprotein to produce novel ultra-long synthetic folding trajectories that\naccurately reproduce all-atom molecular structure, thermodynamics, and kinetics\nat six orders of magnitude lower cost than MD. The dramatically lower cost of\ntrajectory generation enables greatly improved sampling and greatly reduced\nstatistical uncertainties in estimated thermodynamic averages and kinetic\nrates.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 20:05:27 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Sidky", "Hythem", ""], ["Chen", "Wei", ""], ["Ferguson", "Andrew L.", ""]]}, {"id": "2007.00736", "submitter": "Christina Lee Yu", "authors": "Christina Lee Yu", "title": "Tensor Estimation with Nearly Linear Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a conjectured computational-statistical gap in terms of the number\nof samples needed to perform tensor estimation. In particular, for a low rank\n3-order tensor with $\\Theta(n)$ parameters, Barak and Moitra conjectured that\n$\\Omega(n^{3/2})$ samples are needed for polynomial time computation based on a\nreduction of a specific hard instance of a rank 1 tensor to the random 3-XOR\ndistinguishability problem. In this paper, we take a complementary perspective\nand characterize a subclass of tensor instances that can be estimated with only\n$O(n^{1+\\kappa})$ observations for any arbitrarily small constant $\\kappa > 0$,\nnearly linear. If one considers the class of tensors with constant orthogonal\nCP-rank, the \"hardness\" of the instance can be parameterized by the minimum\nabsolute value of the sum of latent factor vectors. If the sum of each latent\nfactor vector is bounded away from zero, we present an algorithm that can\nperform tensor estimation with $O(n^{1+\\kappa})$ samples for a $t$-order\ntensor, significantly less than the previous achievable bound of $O(n^{t/2})$,\nand close to the lower bound of $\\Omega(n)$. This result suggests that amongst\nconstant orthogonal CP-rank tensors, the set of computationally hard instances\nto estimate are in fact a small subset of all possible tensors.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 20:28:22 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Yu", "Christina Lee", ""]]}, {"id": "2007.00753", "submitter": "Samuel Henrique Silva", "authors": "Samuel Henrique Silva and Peyman Najafirad", "title": "Opportunities and Challenges in Deep Learning Adversarial Robustness: A\n  Survey", "comments": "20 pages, 9 figures, submited to IEEE Transactions on Knowledge and\n  Data Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As we seek to deploy machine learning models beyond virtual and controlled\ndomains, it is critical to analyze not only the accuracy or the fact that it\nworks most of the time, but if such a model is truly robust and reliable. This\npaper studies strategies to implement adversary robustly trained algorithms\ntowards guaranteeing safety in machine learning algorithms. We provide a\ntaxonomy to classify adversarial attacks and defenses, formulate the Robust\nOptimization problem in a min-max setting and divide it into 3 subcategories,\nnamely: Adversarial (re)Training, Regularization Approach, and Certified\nDefenses. We survey the most recent and important results in adversarial\nexample generation, defense mechanisms with adversarial (re)Training as their\nmain defense against perturbations. We also survey mothods that add\nregularization terms that change the behavior of the gradient, making it harder\nfor attackers to achieve their objective. Alternatively, we've surveyed methods\nwhich formally derive certificates of robustness by exactly solving the\noptimization problem or by approximations using upper or lower bounds. In\naddition, we discuss the challenges faced by most of the recent algorithms\npresenting future research perspectives.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 21:00:32 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 20:10:20 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Silva", "Samuel Henrique", ""], ["Najafirad", "Peyman", ""]]}, {"id": "2007.00758", "submitter": "Ron Levie", "authors": "Cosmas Hei{\\ss}, Ron Levie, Cinjon Resnick, Gitta Kutyniok, Joan Bruna", "title": "In-Distribution Interpretability for Challenging Modalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely recognized that the predictions of deep neural networks are\ndifficult to parse relative to simpler approaches. However, the development of\nmethods to investigate the mode of operation of such models has advanced\nrapidly in the past few years. Recent work introduced an intuitive framework\nwhich utilizes generative models to improve on the meaningfulness of such\nexplanations. In this work, we display the flexibility of this method to\ninterpret diverse and challenging modalities: music and physical simulations of\nurban environments.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 21:08:45 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 15:10:36 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Hei\u00df", "Cosmas", ""], ["Levie", "Ron", ""], ["Resnick", "Cinjon", ""], ["Kutyniok", "Gitta", ""], ["Bruna", "Joan", ""]]}, {"id": "2007.00759", "submitter": "Asaf Cassel", "authors": "Asaf Cassel (1), Tomer Koren ((1) School of Computer Science, Tel Aviv\n  University)", "title": "Bandit Linear Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of controlling a known linear dynamical system under\nstochastic noise, adversarially chosen costs, and bandit feedback. Unlike the\nfull feedback setting where the entire cost function is revealed after each\ndecision, here only the cost incurred by the learner is observed. We present a\nnew and efficient algorithm that, for strongly convex and smooth costs, obtains\nregret that grows with the square root of the time horizon $T$. We also give\nextensions of this result to general convex, possibly non-smooth costs, and to\nnon-stochastic system noise. A key component of our algorithm is a new\ntechnique for addressing bandit optimization of loss functions with memory.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 21:12:19 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Cassel", "Asaf", ""], ["Koren", "Tomer", ""]]}, {"id": "2007.00767", "submitter": "Xuesong Wang", "authors": "Xuesong Wang, Lina Yao, Xianzhi Wang, Feiping Nie", "title": "NP-PROV: Neural Processes with Position-Relevant-Only Variances", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Processes (NPs) families encode distributions over functions to a\nlatent representation, given context data, and decode posterior mean and\nvariance at unknown locations. Since mean and variance are derived from the\nsame latent space, they may fail on out-of-domain tasks where fluctuations in\nfunction values amplify the model uncertainty. We present a new member named\nNeural Processes with Position-Relevant-Only Variances (NP-PROV). NP-PROV\nhypothesizes that a target point close to a context point has small\nuncertainty, regardless of the function value at that position. The resulting\napproach derives mean and variance from a function-value-related space and a\nposition-related-only latent space separately. Our evaluation on synthetic and\nreal-world datasets reveals that NP-PROV can achieve state-of-the-art\nlikelihood while retaining a bounded variance when drifts exist in the function\nvalue.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 06:11:21 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Wang", "Xuesong", ""], ["Yao", "Lina", ""], ["Wang", "Xianzhi", ""], ["Nie", "Feiping", ""]]}, {"id": "2007.00772", "submitter": "Yizhen Wang", "authors": "Yizhen Wang, Xiaozhu Meng, Ke Wang, Mihai Christodorescu, Somesh Jha", "title": "Robustness against Relational Adversary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Test-time adversarial attacks have posed serious challenges to the robustness\nof machine-learning models, and in many settings the adversarial perturbation\nneed not be bounded by small $\\ell_p$-norms. Motivated by the\nsemantics-preserving attacks in vision and security domain, we investigate\n$\\textit{relational adversaries}$, a broad class of attackers who create\nadversarial examples that are in a reflexive-transitive closure of a logical\nrelation. We analyze the conditions for robustness and propose\n$\\textit{normalize-and-predict}$ -- a learning framework with provable\nrobustness guarantee. We compare our approach with adversarial training and\nderive an unified framework that provides benefits of both approaches. Guided\nby our theoretical findings, we apply our framework to image classification and\nmalware detection. Results of both tasks show that attacks using relational\nadversaries frequently fool existing models, but our unified framework can\nsignificantly enhance their robustness.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 21:27:38 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 14:42:42 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Wang", "Yizhen", ""], ["Meng", "Xiaozhu", ""], ["Wang", "Ke", ""], ["Christodorescu", "Mihai", ""], ["Jha", "Somesh", ""]]}, {"id": "2007.00784", "submitter": "J. Gregory Pauloski", "authors": "J. Gregory Pauloski, Zhao Zhang, Lei Huang, Weijia Xu and Ian T.\n  Foster", "title": "Convolutional Neural Network Training with Distributed K-FAC", "comments": "To be published in the proceedings of the International Conference\n  for High Performance Computing, Networking, Storage and Analysis (SC20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training neural networks with many processors can reduce time-to-solution;\nhowever, it is challenging to maintain convergence and efficiency at large\nscales. The Kronecker-factored Approximate Curvature (K-FAC) was recently\nproposed as an approximation of the Fisher Information Matrix that can be used\nin natural gradient optimizers. We investigate here a scalable K-FAC design and\nits applicability in convolutional neural network (CNN) training at scale. We\nstudy optimization techniques such as layer-wise distribution strategies,\ninverse-free second-order gradient evaluation, and dynamic K-FAC update\ndecoupling to reduce training time while preserving convergence. We use\nresidual neural networks (ResNet) applied to the CIFAR-10 and ImageNet-1k\ndatasets to evaluate the correctness and scalability of our K-FAC gradient\npreconditioner. With ResNet-50 on the ImageNet-1k dataset, our distributed\nK-FAC implementation converges to the 75.9% MLPerf baseline in 18-25% less time\nthan does the classic stochastic gradient descent (SGD) optimizer across scales\non a GPU cluster.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 22:00:53 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Pauloski", "J. Gregory", ""], ["Zhang", "Zhao", ""], ["Huang", "Lei", ""], ["Xu", "Weijia", ""], ["Foster", "Ian T.", ""]]}, {"id": "2007.00790", "submitter": "Hao Sun", "authors": "Pu Ren and Xinyu Chen and Lijun Sun and Hao Sun", "title": "Incremental Bayesian tensor learning for structural monitoring data\n  imputation and response forecasting", "comments": "25 pages; 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been increased interest in missing sensor data imputation, which is\nubiquitous in the field of structural health monitoring (SHM) due to\ndiscontinuous sensing caused by sensor malfunction. To address this fundamental\nissue, this paper presents an incremental Bayesian tensor learning method for\nreconstruction of spatiotemporal missing data in SHM and forecasting of\nstructural response. In particular, a spatiotemporal tensor is first\nconstructed followed by Bayesian tensor factorization that extracts latent\nfeatures for missing data imputation. To enable structural response forecasting\nbased on incomplete sensing data, the tensor decomposition is further\nintegrated with vector autoregression in an incremental learning scheme. The\nperformance of the proposed approach is validated on continuous field-sensing\ndata (including strain and temperature records) of a concrete bridge, based on\nthe assumption that strain time histories are highly correlated to temperature\nrecordings. The results indicate that the proposed probabilistic tensor\nlearning approach is accurate and robust even in the presence of large rates of\nrandom missing, structured missing and their combination. The effect of rank\nselection on the imputation and prediction performance is also investigated.\nThe results show that a better estimation accuracy can be achieved with a\nhigher rank for random missing whereas a lower rank for structured missing.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 22:16:56 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 03:59:34 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 01:12:38 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Ren", "Pu", ""], ["Chen", "Xinyu", ""], ["Sun", "Lijun", ""], ["Sun", "Hao", ""]]}, {"id": "2007.00795", "submitter": "Ching-An Cheng", "authors": "Ching-An Cheng, Andrey Kolobov, Alekh Agarwal", "title": "Policy Improvement via Imitation of Multiple Oracles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite its promise, reinforcement learning's real-world adoption has been\nhampered by the need for costly exploration to learn a good policy. Imitation\nlearning (IL) mitigates this shortcoming by using an oracle policy during\ntraining as a bootstrap to accelerate the learning process. However, in many\npractical situations, the learner has access to multiple suboptimal oracles,\nwhich may provide conflicting advice in a state. The existing IL literature\nprovides a limited treatment of such scenarios. Whereas in the single-oracle\ncase, the return of the oracle's policy provides an obvious benchmark for the\nlearner to compete against, neither such a benchmark nor principled ways of\noutperforming it are known for the multi-oracle setting. In this paper, we\npropose the state-wise maximum of the oracle policies' values as a natural\nbaseline to resolve conflicting advice from multiple oracles. Using a reduction\nof policy optimization to online learning, we introduce a novel IL algorithm\nMAMBA, which can provably learn a policy competitive with this benchmark. In\nparticular, MAMBA optimizes policies by using a gradient estimator in the style\nof generalized advantage estimation (GAE). Our theoretical analysis shows that\nthis design makes MAMBA robust and enables it to outperform the oracle policies\nby a larger margin than the IL state of the art, even in the single-oracle\ncase. In an evaluation against standard policy gradient with GAE and\nAggreVaTe(D), we showcase MAMBA's ability to leverage demonstrations both from\na single and from multiple weak oracles, and significantly speed up policy\noptimization.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 22:33:28 GMT"}, {"version": "v2", "created": "Sun, 6 Dec 2020 04:02:56 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Cheng", "Ching-An", ""], ["Kolobov", "Andrey", ""], ["Agarwal", "Alekh", ""]]}, {"id": "2007.00796", "submitter": "Xiaochen Yang", "authors": "Xiaochen Yang and Jean Honorio", "title": "Information Theoretic Lower Bounds for Feed-Forward Fully-Connected Deep\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the sample complexity lower bounds for the exact\nrecovery of parameters and for a positive excess risk of a feed-forward,\nfully-connected neural network for binary classification, using\ninformation-theoretic tools. We prove these lower bounds by the existence of a\ngenerative network characterized by a backwards data generating process, where\nthe input is generated based on the binary output, and the network is\nparametrized by weight parameters for the hidden layers. The sample complexity\nlower bound for the exact recovery of parameters is $\\Omega(d r \\log(r) + p )$\nand for a positive excess risk is $\\Omega(r \\log(r) + p )$, where $p$ is the\ndimension of the input, $r$ reflects the rank of the weight matrices and $d$ is\nthe number of hidden layers. To the best of our knowledge, our results are the\nfirst information theoretic lower bounds.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 22:36:37 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 04:38:47 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Yang", "Xiaochen", ""], ["Honorio", "Jean", ""]]}, {"id": "2007.00800", "submitter": "Huanru Henry Mao", "authors": "Huanru Henry Mao", "title": "A Survey on Self-supervised Pre-training for Sequential Transfer\n  Learning in Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are typically trained under a supervised learning\nframework where a model learns a single task using labeled data. Instead of\nrelying solely on labeled data, practitioners can harness unlabeled or related\ndata to improve model performance, which is often more accessible and\nubiquitous. Self-supervised pre-training for transfer learning is becoming an\nincreasingly popular technique to improve state-of-the-art results using\nunlabeled data. It involves first pre-training a model on a large amount of\nunlabeled data, then adapting the model to target tasks of interest. In this\nreview, we survey self-supervised learning methods and their applications\nwithin the sequential transfer learning framework. We provide an overview of\nthe taxonomy for self-supervised learning and transfer learning, and highlight\nsome prominent methods for designing pre-training tasks across different\ndomains. Finally, we discuss recent trends and suggest areas for future\ninvestigation.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 22:55:48 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Mao", "Huanru Henry", ""]]}, {"id": "2007.00810", "submitter": "Diederik P. Kingma Dr.", "authors": "Geoffrey Roeder, Luke Metz and Diederik P. Kingma", "title": "On Linear Identifiability of Learned Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifiability is a desirable property of a statistical model: it implies\nthat the true model parameters may be estimated to any desired precision, given\nsufficient computational resources and data. We study identifiability in the\ncontext of representation learning: discovering nonlinear data representations\nthat are optimal with respect to some downstream task. When parameterized as\ndeep neural networks, such representation functions typically lack\nidentifiability in parameter space, because they are overparameterized by\ndesign. In this paper, building on recent advances in nonlinear ICA, we aim to\nrehabilitate identifiability by showing that a large family of discriminative\nmodels are in fact identifiable in function space, up to a linear\nindeterminacy. Many models for representation learning in a wide variety of\ndomains have been identifiable in this sense, including text, images and audio,\nstate-of-the-art at time of publication. We derive sufficient conditions for\nlinear identifiability and provide empirical support for the result on both\nsimulated and real-world data.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 23:33:37 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 18:29:22 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2020 03:51:28 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Roeder", "Geoffrey", ""], ["Metz", "Luke", ""], ["Kingma", "Diederik P.", ""]]}, {"id": "2007.00811", "submitter": "Denny Zhou", "authors": "Denny Zhou, Mao Ye, Chen Chen, Tianjian Meng, Mingxing Tan, Xiaodan\n  Song, Quoc Le, Qiang Liu, and Dale Schuurmans", "title": "Go Wide, Then Narrow: Efficient Training of Deep Thin Networks", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For deploying a deep learning model into production, it needs to be both\naccurate and compact to meet the latency and memory constraints. This usually\nresults in a network that is deep (to ensure performance) and yet thin (to\nimprove computational efficiency). In this paper, we propose an efficient\nmethod to train a deep thin network with a theoretic guarantee. Our method is\nmotivated by model compression. It consists of three stages. First, we\nsufficiently widen the deep thin network and train it until convergence. Then,\nwe use this well-trained deep wide network to warm up (or initialize) the\noriginal deep thin network. This is achieved by layerwise imitation, that is,\nforcing the thin network to mimic the intermediate outputs of the wide network\nfrom layer to layer. Finally, we further fine tune this already\nwell-initialized deep thin network. The theoretical guarantee is established by\nusing the neural mean field analysis. It demonstrates the advantage of our\nlayerwise imitation approach over backpropagation. We also conduct large-scale\nempirical experiments to validate the proposed method. By training with our\nmethod, ResNet50 can outperform ResNet101, and BERT Base can be comparable with\nBERT Large, when ResNet101 and BERT Large are trained under the standard\ntraining procedures as in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 23:34:35 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 17:43:30 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Zhou", "Denny", ""], ["Ye", "Mao", ""], ["Chen", "Chen", ""], ["Meng", "Tianjian", ""], ["Tan", "Mingxing", ""], ["Song", "Xiaodan", ""], ["Le", "Quoc", ""], ["Liu", "Qiang", ""], ["Schuurmans", "Dale", ""]]}, {"id": "2007.00812", "submitter": "Junhao Wen", "authors": "Junhao Wen, Erdem Varol, Ganesh Chand, Aristeidis Sotiras, Christos\n  Davatzikos", "title": "MAGIC: Multi-scale Heterogeneity Analysis and Clustering for Brain\n  Diseases", "comments": "11 pages, 3 figures, accepted by MICCAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing amount of clinical, anatomical and functional evidence for\nthe heterogeneous presentation of neuropsychiatric and neurodegenerative\ndiseases such as schizophrenia and Alzheimers Disease (AD). Elucidating\ndistinct subtypes of diseases allows a better understanding of\nneuropathogenesis and enables the possibility of developing targeted treatment\nprograms. Recent semi-supervised clustering techniques have provided a\ndata-driven way to understand disease heterogeneity. However, existing methods\ndo not take into account that subtypes of the disease might present themselves\nat different spatial scales across the brain. Here, we introduce a novel\nmethod, MAGIC, to uncover disease heterogeneity by leveraging multi-scale\nclustering. We first extract multi-scale patterns of structural covariance\n(PSCs) followed by a semi-supervised clustering with double cyclic block-wise\noptimization across different scales of PSCs. We validate MAGIC using simulated\nheterogeneous neuroanatomical data and demonstrate its clinical potential by\nexploring the heterogeneity of AD using T1 MRI scans of 228 cognitively normal\n(CN) and 191 patients. Our results indicate two main subtypes of AD with\ndistinct atrophy patterns that consist of both fine-scale atrophy in the\nhippocampus as well as large-scale atrophy in cortical regions. The evidence\nfor the heterogeneity is further corroborated by the clinical evaluation of two\nsubtypes, which indicates that there is a subpopulation of AD patients that\ntend to be younger and decline faster in cognitive performance relative to the\nother subpopulation, which tends to be older and maintains a relatively steady\ndecline in cognitive abilities.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 23:42:37 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 00:50:31 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Wen", "Junhao", ""], ["Varol", "Erdem", ""], ["Chand", "Ganesh", ""], ["Sotiras", "Aristeidis", ""], ["Davatzikos", "Christos", ""]]}, {"id": "2007.00816", "submitter": "Jin Jin", "authors": "Jin Jin (1), Lin Zhang (2), Ethan Leng (3), Gregory J. Metzger (4),\n  Joseph S. Koopmeiners (2) ((1) Department of Biostatistics, Bloomberg School\n  of Public Health, Johns Hopkins University, (2) Devision of Biostatistics,\n  School of Public Health, University of Minnesota, (3) Department of\n  Biomedical Engineering, University of Minnesota, (4) Department of Radiology,\n  University of Minnesota)", "title": "Multi-resolution Super Learner for Voxel-wise Classification of Prostate\n  Cancer Using Multi-parametric MRI", "comments": "23 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While current research has shown the importance of Multi-parametric MRI\n(mpMRI) in diagnosing prostate cancer (PCa), further investigation is needed\nfor how to incorporate the specific structures of the mpMRI data, such as the\nregional heterogeneity and between-voxel correlation within a subject. This\npaper proposes a machine learning-based method for improved voxel-wise PCa\nclassification by taking into account the unique structures of the data. We\npropose a multi-resolution modeling approach to account for regional\nheterogeneity, where base learners trained locally at multiple resolutions are\ncombined using the super learner, and account for between-voxel correlation by\nefficient spatial Gaussian kernel smoothing. The method is flexible in that the\nsuper learner framework allows implementation of any classifier as the base\nlearner, and can be easily extended to classifying cancer into more\nsub-categories. We describe detailed classification algorithm for the binary\nPCa status, as well as the ordinal clinical significance of PCa for which a\nweighted likelihood approach is implemented to enhance the detection of the\nless prevalent cancer categories. We illustrate the advantages of the proposed\napproach over conventional modeling and machine learning approaches through\nsimulations and application to in vivo data.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 00:14:55 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Jin", "Jin", ""], ["Zhang", "Lin", ""], ["Leng", "Ethan", ""], ["Metzger", "Gregory J.", ""], ["Koopmeiners", "Joseph S.", ""]]}, {"id": "2007.00823", "submitter": "Benjamin Lengerich", "authors": "Benjamin Lengerich, Eric P. Xing, Rich Caruana", "title": "On Dropout, Overfitting, and Interaction Effects in Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine Dropout through the perspective of interactions: learned effects\nthat combine multiple input variables. Given $N$ variables, there are $O(N^2)$\npossible pairwise interactions, $O(N^3)$ possible 3-way interactions, etc. We\nshow that Dropout implicitly sets a learning rate for interaction effects that\ndecays exponentially with the size of the interaction, corresponding to a\nregularizer that balances against the hypothesis space which grows\nexponentially with number of variables in the interaction. This understanding\nof Dropout has implications for the optimal Dropout rate: higher Dropout rates\nshould be used when we need stronger regularization against spurious high-order\ninteractions. This perspective also issues caution against using Dropout to\nmeasure term saliency because Dropout regularizes against terms for high-order\ninteractions. Finally, this view of Dropout as a regularizer of interaction\neffects provides insight into the varying effectiveness of Dropout for\ndifferent architectures and data sets. We also compare Dropout to\nregularization via weight decay and early stopping and find that it is\ndifficult to obtain the same regularization effect for high-order interactions\nwith these methods.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 01:11:52 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Lengerich", "Benjamin", ""], ["Xing", "Eric P.", ""], ["Caruana", "Rich", ""]]}, {"id": "2007.00869", "submitter": "Michael Gimelfarb Mr.", "authors": "Michael Gimelfarb, Scott Sanner, Chi-Guhn Lee", "title": "{\\epsilon}-BMC: A Bayesian Ensemble Approach to Epsilon-Greedy\n  Exploration in Model-Free Reinforcement Learning", "comments": "Published in UAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resolving the exploration-exploitation trade-off remains a fundamental\nproblem in the design and implementation of reinforcement learning (RL)\nalgorithms. In this paper, we focus on model-free RL using the epsilon-greedy\nexploration policy, which despite its simplicity, remains one of the most\nfrequently used forms of exploration. However, a key limitation of this policy\nis the specification of $\\varepsilon$. In this paper, we provide a novel\nBayesian perspective of $\\varepsilon$ as a measure of the uniformity of the\nQ-value function. We introduce a closed-form Bayesian model update based on\nBayesian model combination (BMC), based on this new perspective, which allows\nus to adapt $\\varepsilon$ using experiences from the environment in constant\ntime with monotone convergence guarantees. We demonstrate that our proposed\nalgorithm, $\\varepsilon$-\\texttt{BMC}, efficiently balances exploration and\nexploitation on different problems, performing comparably or outperforming the\nbest tuned fixed annealing schedules and an alternative data-dependent\n$\\varepsilon$ adaptation scheme proposed in the literature.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 04:30:47 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Gimelfarb", "Michael", ""], ["Sanner", "Scott", ""], ["Lee", "Chi-Guhn", ""]]}, {"id": "2007.00873", "submitter": "Kyung-Su Kim", "authors": "Kyung-Su Kim, Jung Hyun Lee, Eunho Yang", "title": "Compressed Sensing via Measurement-Conditional Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A pre-trained generator has been frequently adopted in compressed sensing\n(CS) due to its ability to effectively estimate signals with the prior of NNs.\nIn order to further refine the NN-based prior, we propose a framework that\nallows the generator to utilize additional information from a given measurement\nfor prior learning, thereby yielding more accurate prediction for signals. As\nour framework has a simple form, it is easily applied to existing CS methods\nusing pre-trained generators. We demonstrate through extensive experiments that\nour framework exhibits uniformly superior performances by large margin and can\nreduce the reconstruction error up to an order of magnitude for some\napplications. We also explain the experimental success in theory by showing\nthat our framework can slightly relax the stringent signal presence condition,\nwhich is required to guarantee the success of signal recovery.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 04:40:34 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 10:22:37 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Kim", "Kyung-Su", ""], ["Lee", "Jung Hyun", ""], ["Yang", "Eunho", ""]]}, {"id": "2007.00878", "submitter": "Jakub Kone\\v{c}n\\'y", "authors": "Zachary Charles, Jakub Kone\\v{c}n\\'y", "title": "On the Outsized Importance of Learning Rates in Local Update Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a family of algorithms, which we refer to as local update methods,\nthat generalize many federated learning and meta-learning algorithms. We prove\nthat for quadratic objectives, local update methods perform stochastic gradient\ndescent on a surrogate loss function which we exactly characterize. We show\nthat the choice of client learning rate controls the condition number of that\nsurrogate loss, as well as the distance between the minimizers of the surrogate\nand true loss functions. We use this theory to derive novel convergence rates\nfor federated averaging that showcase this trade-off between the condition\nnumber of the surrogate loss and its alignment with the true loss function. We\nvalidate our results empirically, showing that in communication-limited\nsettings, proper learning rate tuning is often sufficient to reach near-optimal\nbehavior. We also present a practical method for automatic learning rate decay\nin local update methods that helps reduce the need for learning rate tuning,\nand highlight its empirical performance on a variety of tasks and datasets.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 04:45:55 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Charles", "Zachary", ""], ["Kone\u010dn\u00fd", "Jakub", ""]]}, {"id": "2007.00882", "submitter": "Richard Barnes", "authors": "Richard Barnes and Senaka Buthpitiya and James Cook and Alex Fabrikant\n  and Andrew Tomkins and Fangzhou Xu", "title": "BusTr: Predicting Bus Travel Times from Real-Time Traffic", "comments": "14 pages, 2 figures, 5 tables. Citation: \"Richard Barnes, Senaka\n  Buthpitiya, James Cook, Alex Fabrikant, Andrew Tomkins, Fangzhou Xu (2020).\n  BusTr: Predicting Bus Travel Times from Real-Time Traffic. 26th ACM SIGKDD\n  Conference on Knowledge Discovery and Data Mining. doi:\n  10.1145/3394486.3403376\"", "journal-ref": null, "doi": "10.1145/3394486.3403376", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present BusTr, a machine-learned model for translating road traffic\nforecasts into predictions of bus delays, used by Google Maps to serve the\nmajority of the world's public transit systems where no official real-time bus\ntracking is provided. We demonstrate that our neural sequence model improves\nover DeepTTE, the state-of-the-art baseline, both in performance (-30% MAPE)\nand training stability. We also demonstrate significant generalization gains\nover simpler models, evaluated on longitudinal data to cope with a constantly\nevolving world.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 05:05:23 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Barnes", "Richard", ""], ["Buthpitiya", "Senaka", ""], ["Cook", "James", ""], ["Fabrikant", "Alex", ""], ["Tomkins", "Andrew", ""], ["Xu", "Fangzhou", ""]]}, {"id": "2007.00884", "submitter": "Kyung-Su Kim", "authors": "Kyung-Su Kim, Aur\\'elie C. Lozano, Eunho Yang", "title": "A Revision of Neural Tangent Kernel-based Approaches for Neural Networks", "comments": "We spotted an error in the proof of Lemma A.4 and are investigating\n  whether this can be corrected. Furthermore, the authors of the original paper\n  have informed us that they are fixing the lemma upon which our theorem 3.2\n  builds. Therefore, we are removing the current version of our paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent theoretical works based on the neural tangent kernel (NTK) have shed\nlight on the optimization and generalization of over-parameterized networks,\nand partially bridge the gap between their practical success and classical\nlearning theory. Especially, using the NTK-based approach, the following three\nrepresentative results were obtained: (1) A training error bound was derived to\nshow that networks can fit any finite training sample perfectly by reflecting a\ntighter characterization of training speed depending on the data complexity.\n(2) A generalization error bound invariant of network size was derived by using\na data-dependent complexity measure (CMD). It follows from this CMD bound that\nnetworks can generalize arbitrary smooth functions. (3) A simple and analytic\nkernel function was derived as indeed equivalent to a fully-trained network.\nThis kernel outperforms its corresponding network and the existing gold\nstandard, Random Forests, in few shot learning. For all of these results to\nhold, the network scaling factor $\\kappa$ should decrease w.r.t. sample size n.\nIn this case of decreasing $\\kappa$, however, we prove that the aforementioned\nresults are surprisingly erroneous. It is because the output value of trained\nnetwork decreases to zero when $\\kappa$ decreases w.r.t. n. To solve this\nproblem, we tighten key bounds by essentially removing $\\kappa$-affected\nvalues. Our tighter analysis resolves the scaling problem and enables the\nvalidation of the original NTK-based results.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 05:07:55 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 23:30:27 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Kim", "Kyung-Su", ""], ["Lozano", "Aur\u00e9lie C.", ""], ["Yang", "Eunho", ""]]}, {"id": "2007.00897", "submitter": "Siamak Mehrkanoon", "authors": "Ismail Alaoui Abdellaoui, Jesus Garcia Fernandez, Caner Sahinli and\n  Siamak Mehrkanoon", "title": "Deep brain state classification of MEG data", "comments": "11 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP q-bio.NC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neuroimaging techniques have shown to be useful when studying the brain's\nactivity. This paper uses Magnetoencephalography (MEG) data, provided by the\nHuman Connectome Project (HCP), in combination with various deep artificial\nneural network models to perform brain decoding. More specifically, here we\ninvestigate to which extent can we infer the task performed by a subject based\non its MEG data. Three models based on compact convolution, combined\nconvolutional and long short-term architecture as well as a model based on\nmulti-view learning that aims at fusing the outputs of the two stream networks\nare proposed and examined. These models exploit the spatio-temporal MEG data\nfor learning new representations that are used to decode the relevant tasks\nacross subjects. In order to realize the most relevant features of the input\nsignals, two attention mechanisms, i.e. self and global attention, are\nincorporated in all the models. The experimental results of cross subject\nmulti-class classification on the studied MEG dataset show that the inclusion\nof attention improves the generalization of the models across subjects.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 05:51:57 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2020 19:28:11 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Abdellaoui", "Ismail Alaoui", ""], ["Fernandez", "Jesus Garcia", ""], ["Sahinli", "Caner", ""], ["Mehrkanoon", "Siamak", ""]]}, {"id": "2007.00914", "submitter": "Eugenio Mart\\'inez-C\\'amara", "authors": "Nuria Rodr\\'iguez-Barroso, Goran Stipcich, Daniel Jim\\'enez-L\\'opez,\n  Jos\\'e Antonio Ruiz-Mill\\'an, Eugenio Mart\\'inez-C\\'amara, Gerardo\n  Gonz\\'alez-Seco, M. Victoria Luz\\'on, Miguel \\'Angel Veganzones, Francisco\n  Herrera", "title": "Federated Learning and Differential Privacy: Software tools analysis,\n  the Sherpa.ai FL framework and methodological guidelines for preserving data\n  privacy", "comments": "46 pages, 5 figures", "journal-ref": "Information Fusion 64 (2020) 270-292", "doi": "10.1016/j.inffus.2020.07.009", "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high demand of artificial intelligence services at the edges that also\npreserve data privacy has pushed the research on novel machine learning\nparadigms that fit those requirements. Federated learning has the ambition to\nprotect data privacy through distributed learning methods that keep the data in\ntheir data silos. Likewise, differential privacy attains to improve the\nprotection of data privacy by measuring the privacy loss in the communication\namong the elements of federated learning. The prospective matching of federated\nlearning and differential privacy to the challenges of data privacy protection\nhas caused the release of several software tools that support their\nfunctionalities, but they lack of the needed unified vision for those\ntechniques, and a methodological workflow that support their use. Hence, we\npresent the Sherpa.ai Federated Learning framework that is built upon an\nholistic view of federated learning and differential privacy. It results from\nthe study of how to adapt the machine learning paradigm to federated learning,\nand the definition of methodological guidelines for developing artificial\nintelligence services based on federated learning and differential privacy. We\nshow how to follow the methodological guidelines with the Sherpa.ai Federated\nLearning framework by means of a classification and a regression use cases.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 06:47:35 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 07:39:39 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Rodr\u00edguez-Barroso", "Nuria", ""], ["Stipcich", "Goran", ""], ["Jim\u00e9nez-L\u00f3pez", "Daniel", ""], ["Ruiz-Mill\u00e1n", "Jos\u00e9 Antonio", ""], ["Mart\u00ednez-C\u00e1mara", "Eugenio", ""], ["Gonz\u00e1lez-Seco", "Gerardo", ""], ["Luz\u00f3n", "M. Victoria", ""], ["Veganzones", "Miguel \u00c1ngel", ""], ["Herrera", "Francisco", ""]]}, {"id": "2007.00935", "submitter": "Hachem Kadri", "authors": "Hachem Kadri (QARMA), St\\'ephane Ayache (QARMA), Riikka Huusari, Alain\n  Rakotomamonjy (DocApp - LITIS), Liva Ralaivola", "title": "Partial Trace Regression and Low-Rank Kraus Decomposition", "comments": null, "journal-ref": "International Conference on Machine Learning, Jul 2020, Vienne\n  (Online), Austria", "doi": null, "report-no": null, "categories": "cs.LG quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The trace regression model, a direct extension of the well-studied linear\nregression model, allows one to map matrices to real-valued outputs. We here\nintroduce an even more general model, namely the partial-trace regression\nmodel, a family of linear mappings from matrix-valued inputs to matrix-valued\noutputs; this model subsumes the trace regression model and thus the linear\nregression model. Borrowing tools from quantum information theory, where\npartial trace operators have been extensively studied, we propose a framework\nfor learning partial trace regression models from data by taking advantage of\nthe so-called low-rank Kraus representation of completely positive maps. We\nshow the relevance of our framework with synthetic and real-world experiments\nconducted for both i) matrix-to-matrix regression and ii) positive semidefinite\nmatrix completion, two tasks which can be formulated as partial trace\nregression problems.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 07:21:22 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 08:41:59 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Kadri", "Hachem", "", "QARMA"], ["Ayache", "St\u00e9phane", "", "QARMA"], ["Huusari", "Riikka", "", "DocApp - LITIS"], ["Rakotomamonjy", "Alain", "", "DocApp - LITIS"], ["Ralaivola", "Liva", ""]]}, {"id": "2007.00939", "submitter": "Henry Moss", "authors": "Henry B. Moss, David S. Leslie, Paul Rayson", "title": "BOSH: Bayesian Optimization by Sampling Hierarchically", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deployments of Bayesian Optimization (BO) for functions with stochastic\nevaluations, such as parameter tuning via cross validation and simulation\noptimization, typically optimize an average of a fixed set of noisy\nrealizations of the objective function. However, disregarding the true\nobjective function in this manner finds a high-precision optimum of the wrong\nfunction. To solve this problem, we propose Bayesian Optimization by Sampling\nHierarchically (BOSH), a novel BO routine pairing a hierarchical Gaussian\nprocess with an information-theoretic framework to generate a growing pool of\nrealizations as the optimization progresses. We demonstrate that BOSH provides\nmore efficient and higher-precision optimization than standard BO across\nsynthetic benchmarks, simulation optimization, reinforcement learning and\nhyper-parameter tuning tasks.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 07:35:49 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Moss", "Henry B.", ""], ["Leslie", "David S.", ""], ["Rayson", "Paul", ""]]}, {"id": "2007.00953", "submitter": "R\\'emy Degenne", "authors": "R\\'emy Degenne, Pierre M\\'enard, Xuedong Shang, Michal Valko", "title": "Gamification of Pure Exploration for Linear Bandits", "comments": "11+25 pages. To be published in the proceedings of ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate an active pure-exploration setting, that includes best-arm\nidentification, in the context of linear stochastic bandits. While\nasymptotically optimal algorithms exist for standard multi-arm bandits, the\nexistence of such algorithms for the best-arm identification in linear bandits\nhas been elusive despite several attempts to address it. First, we provide a\nthorough comparison and new insight over different notions of optimality in the\nlinear case, including G-optimality, transductive optimality from optimal\nexperimental design and asymptotic optimality. Second, we design the first\nasymptotically optimal algorithm for fixed-confidence pure exploration in\nlinear bandits. As a consequence, our algorithm naturally bypasses the pitfall\ncaused by a simple but difficult instance, that most prior algorithms had to be\nengineered to deal with explicitly. Finally, we avoid the need to fully solve\nan optimal design problem by providing an approach that entails an efficient\nimplementation.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 08:20:35 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Degenne", "R\u00e9my", ""], ["M\u00e9nard", "Pierre", ""], ["Shang", "Xuedong", ""], ["Valko", "Michal", ""]]}, {"id": "2007.00961", "submitter": "Bishwo Adhikari Mr.", "authors": "Bishwo Adhikari and Heikki Huttunen", "title": "Iterative Bounding Box Annotation for Object Detection", "comments": "Accepted at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Manual annotation of bounding boxes for object detection in digital images is\ntedious, and time and resource consuming. In this paper, we propose a\nsemi-automatic method for efficient bounding box annotation. The method trains\nthe object detector iteratively on small batches of labeled images and learns\nto propose bounding boxes for the next batch, after which the human annotator\nonly needs to correct possible errors. We propose an experimental setup for\nsimulating the human actions and use it for comparing different iteration\nstrategies, such as the order in which the data is presented to the annotator.\nWe experiment on our method with three datasets and show that it can reduce the\nhuman annotation effort significantly, saving up to 75% of total manual\nannotation work.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 08:40:12 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Adhikari", "Bishwo", ""], ["Huttunen", "Heikki", ""]]}, {"id": "2007.00969", "submitter": "R\\'emy Degenne", "authors": "R\\'emy Degenne, Han Shao, Wouter M. Koolen", "title": "Structure Adaptive Algorithms for Stochastic Bandits", "comments": "10+18 pages. To be published in the proceedings of ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study reward maximisation in a wide class of structured stochastic\nmulti-armed bandit problems, where the mean rewards of arms satisfy some given\nstructural constraints, e.g. linear, unimodal, sparse, etc. Our aim is to\ndevelop methods that are flexible (in that they easily adapt to different\nstructures), powerful (in that they perform well empirically and/or provably\nmatch instance-dependent lower bounds) and efficient in that the per-round\ncomputational burden is small.\n  We develop asymptotically optimal algorithms from instance-dependent\nlower-bounds using iterative saddle-point solvers. Our approach generalises\nrecent iterative methods for pure exploration to reward maximisation, where a\nmajor challenge arises from the estimation of the sub-optimality gaps and their\nreciprocals. Still we manage to achieve all the above desiderata. Notably, our\ntechnique avoids the computational cost of the full-blown saddle point oracle\nemployed by previous work, while at the same time enabling finite-time regret\nbounds.\n  Our experiments reveal that our method successfully leverages the structural\nassumptions, while its regret is at worst comparable to that of vanilla UCB.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 08:59:54 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Degenne", "R\u00e9my", ""], ["Shao", "Han", ""], ["Koolen", "Wouter M.", ""]]}, {"id": "2007.00970", "submitter": "Ettore Randazzo", "authors": "Ettore Randazzo, Eyvind Niklasson, Alexander Mordvintsev", "title": "MPLP: Learning a Message Passing Learning Protocol", "comments": "Code at\n  https://github.com/google-research/self-organising-systems/tree/master/mplp;\n  code base link fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel method for learning the weights of an artificial neural\nnetwork - a Message Passing Learning Protocol (MPLP). In MPLP, we abstract\nevery operations occurring in ANNs as independent agents. Each agent is\nresponsible for ingesting incoming multidimensional messages from other agents,\nupdating its internal state, and generating multidimensional messages to be\npassed on to neighbouring agents. We demonstrate the viability of MPLP as\nopposed to traditional gradient-based approaches on simple feed-forward neural\nnetworks, and present a framework capable of generalizing to non-traditional\nneural network architectures. MPLP is meta learned using end-to-end\ngradient-based meta-optimisation. We further discuss the observed properties of\nMPLP and hypothesize its applicability on various fields of deep learning.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 09:03:14 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 10:28:35 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Randazzo", "Ettore", ""], ["Niklasson", "Eyvind", ""], ["Mordvintsev", "Alexander", ""]]}, {"id": "2007.00973", "submitter": "Samuel H{\\aa}kansson", "authors": "Samuel H{\\aa}kansson, Viktor Lindblom, Omer Gottesman, Fredrik D.\n  Johansson", "title": "Learning to search efficiently for causally near-optimal treatments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding an effective medical treatment often requires a search by trial and\nerror. Making this search more efficient by minimizing the number of\nunnecessary trials could lower both costs and patient suffering. We formalize\nthis problem as learning a policy for finding a near-optimal treatment in a\nminimum number of trials using a causal inference framework. We give a\nmodel-based dynamic programming algorithm which learns from observational data\nwhile being robust to unmeasured confounding. To reduce time complexity, we\nsuggest a greedy algorithm which bounds the near-optimality constraint. The\nmethods are evaluated on synthetic and real-world healthcare data and compared\nto model-free reinforcement learning. We find that our methods compare\nfavorably to the model-free baseline while offering a more transparent\ntrade-off between search time and treatment efficacy.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 09:17:48 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 08:38:14 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["H\u00e5kansson", "Samuel", ""], ["Lindblom", "Viktor", ""], ["Gottesman", "Omer", ""], ["Johansson", "Fredrik D.", ""]]}, {"id": "2007.01003", "submitter": "Fabi\\'an Latorre", "authors": "Fabian Latorre, Paul Rolland, Nadav Hallak, Volkan Cevher", "title": "Efficient Proximal Mapping of the 1-path-norm of Shallow Networks", "comments": "ICML 2020. Fabian Latorre, Paul Rolland and Nadav Hallak have\n  contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate two new important properties of the 1-path-norm of shallow\nneural networks. First, despite its non-smoothness and non-convexity it allows\na closed form proximal operator which can be efficiently computed, allowing the\nuse of stochastic proximal-gradient-type methods for regularized empirical risk\nminimization. Second, when the activation functions is differentiable, it\nprovides an upper bound on the Lipschitz constant of the network. Such bound is\ntighter than the trivial layer-wise product of Lipschitz constants, motivating\nits use for training networks robust to adversarial perturbations. In practical\nexperiments we illustrate the advantages of using the proximal mapping and we\ncompare the robustness-accuracy trade-off induced by the 1-path-norm, L1-norm\nand layer-wise constraints on the Lipschitz constant (Parseval networks).\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 10:34:06 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 09:40:23 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Latorre", "Fabian", ""], ["Rolland", "Paul", ""], ["Hallak", "Nadav", ""], ["Cevher", "Volkan", ""]]}, {"id": "2007.01012", "submitter": "Alex Nowak-Vila", "authors": "Alex Nowak-Vila, Francis Bach, Alessandro Rudi", "title": "Consistent Structured Prediction with Max-Min Margin Markov Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max-margin methods for binary classification such as the support vector\nmachine (SVM) have been extended to the structured prediction setting under the\nname of max-margin Markov networks ($M^3N$), or more generally structural SVMs.\nUnfortunately, these methods are statistically inconsistent when the\nrelationship between inputs and labels is far from deterministic. We overcome\nsuch limitations by defining the learning problem in terms of a \"max-min\"\nmargin formulation, naming the resulting method max-min margin Markov networks\n($M^4N$). We prove consistency and finite sample generalization bounds for\n$M^4N$ and provide an explicit algorithm to compute the estimator. The\nalgorithm achieves a generalization error of $O(1/\\sqrt{n})$ for a total cost\nof $O(n)$ projection-oracle calls (which have at most the same cost as the\nmax-oracle from $M^3N$). Experiments on multi-class classification, ordinal\nregression, sequence prediction and ranking demonstrate the effectiveness of\nthe proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 10:48:42 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 18:24:07 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Nowak-Vila", "Alex", ""], ["Bach", "Francis", ""], ["Rudi", "Alessandro", ""]]}, {"id": "2007.01017", "submitter": "Xabier Echeberria-Barrio", "authors": "Xabier Echeberria-Barrio, Amaia Gil-Lerchundi, Ines\n  Goicoechea-Telleria and Raul Orduna-Urrutia", "title": "Deep Learning Defenses Against Adversarial Examples for Dynamic Risk\n  Assessment", "comments": "11 pages, 10 figures, CISIS2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks were first developed decades ago, but it was not until\nrecently that they started being extensively used, due to their computing power\nrequirements. Since then, they are increasingly being applied to many fields\nand have undergone far-reaching advancements. More importantly, they have been\nutilized for critical matters, such as making decisions in healthcare\nprocedures or autonomous driving, where risk management is crucial. Any\nmistakes in the diagnostics or decision-making in these fields could entail\ngrave accidents, and even death. This is preoccupying, because it has been\nrepeatedly reported that it is straightforward to attack this type of models.\nThus, these attacks must be studied to be able to assess their risk, and\ndefenses need to be developed to make models more robust. For this work, the\nmost widely known attack was selected (adversarial attack) and several defenses\nwere implemented against it (i.e. adversarial training, dimensionality reduc\ntion and prediction similarity). The obtained outcomes make the model more\nrobust while keeping a similar accuracy. The idea was developed using a breast\ncancer dataset and a VGG16 and dense neural network model, but the solutions\ncould be applied to datasets from other areas and different convolutional and\ndense deep neural network models.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 11:01:27 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Echeberria-Barrio", "Xabier", ""], ["Gil-Lerchundi", "Amaia", ""], ["Goicoechea-Telleria", "Ines", ""], ["Orduna-Urrutia", "Raul", ""]]}, {"id": "2007.01027", "submitter": "Annabelle Redelmeier", "authors": "Annabelle Redelmeier, Martin Jullum, and Kjersti Aas", "title": "Explaining predictive models with mixed features using Shapley values\n  and conditional inference trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is becoming increasingly important to explain complex, black-box machine\nlearning models. Although there is an expanding literature on this topic,\nShapley values stand out as a sound method to explain predictions from any type\nof machine learning model. The original development of Shapley values for\nprediction explanation relied on the assumption that the features being\ndescribed were independent. This methodology was then extended to explain\ndependent features with an underlying continuous distribution. In this paper,\nwe propose a method to explain mixed (i.e. continuous, discrete, ordinal, and\ncategorical) dependent features by modeling the dependence structure of the\nfeatures using conditional inference trees. We demonstrate our proposed method\nagainst the current industry standards in various simulation studies and find\nthat our method often outperforms the other approaches. Finally, we apply our\nmethod to a real financial data set used in the 2018 FICO Explainable Machine\nLearning Challenge and show how our explanations compare to the FICO challenge\nRecognition Award winning team.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 11:25:45 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Redelmeier", "Annabelle", ""], ["Jullum", "Martin", ""], ["Aas", "Kjersti", ""]]}, {"id": "2007.01028", "submitter": "Antonio Macaluso", "authors": "Antonio Macaluso, Luca Clissa, Stefano Lodi, Claudio Sartori", "title": "Quantum Ensemble for Classification", "comments": "14 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A powerful way to improve performance in machine learning is to construct an\nensemble that combines the predictions of multiple models. Ensemble methods are\noften much more accurate and lower variance than the individual classifiers\nthat make them up but have high requirements in terms of memory and\ncomputational time. In fact, a large number of alternative algorithms is\nusually adopted, each requiring to query all available data. We propose a new\nquantum algorithm that exploits quantum superposition, entanglement and\ninterference to build an ensemble of classification models. Thanks to the\ngeneration of the several quantum trajectories in superposition, we obtain $B$\ntransformations of the quantum state which encodes the training set in only\n$log\\left(B\\right)$ operations. This implies an exponential speed-up in the\nsize of the ensemble with respect to classical methods. Furthermore, when\nconsidering the overall cost of the algorithm, we show that the training of a\nsingle weak classifier impacts additively rather than multiplicatively, as it\nusually happens. We also present small-scale experiments, defining a quantum\nversion of the cosine classifier and using the IBM qiskit environment to show\nhow the algorithm works.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 11:26:54 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 08:10:33 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Macaluso", "Antonio", ""], ["Clissa", "Luca", ""], ["Lodi", "Stefano", ""], ["Sartori", "Claudio", ""]]}, {"id": "2007.01038", "submitter": "Yaniv Blumenfeld", "authors": "Yaniv Blumenfeld, Dar Gilboa, Daniel Soudry", "title": "Beyond Signal Propagation: Is Feature Diversity Necessary in Deep Neural\n  Network Initialization?", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are typically initialized with random weights, with\nvariances chosen to facilitate signal propagation and stable gradients. It is\nalso believed that diversity of features is an important property of these\ninitializations. We construct a deep convolutional network with identical\nfeatures by initializing almost all the weights to $0$. The architecture also\nenables perfect signal propagation and stable gradients, and achieves high\naccuracy on standard benchmarks. This indicates that random, diverse\ninitializations are \\textit{not} necessary for training neural networks. An\nessential element in training this network is a mechanism of symmetry breaking;\nwe study this phenomenon and find that standard GPU operations, which are\nnon-deterministic, can serve as a sufficient source of symmetry breaking to\nenable training.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 11:49:17 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Blumenfeld", "Yaniv", ""], ["Gilboa", "Dar", ""], ["Soudry", "Daniel", ""]]}, {"id": "2007.01054", "submitter": "Daniel Wilke", "authors": "Dominic Kafka and Daniel Nicolas Wilke", "title": "Gradient-only line searches to automatically determine learning rates\n  for a variety of stochastic training algorithms", "comments": "38 pages, 12 figures, 4 tables, 7 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient-only and probabilistic line searches have recently reintroduced the\nability to adaptively determine learning rates in dynamic mini-batch\nsub-sampled neural network training. However, stochastic line searches are\nstill in their infancy and thus call for an ongoing investigation. We study the\napplication of the Gradient-Only Line Search that is Inexact (GOLS-I) to\nautomatically determine the learning rate schedule for a selection of popular\nneural network training algorithms, including NAG, Adagrad, Adadelta, Adam and\nLBFGS, with numerous shallow, deep and convolutional neural network\narchitectures trained on different datasets with various loss functions. We\nfind that GOLS-I's learning rate schedules are competitive with manually tuned\nlearning rates, over seven optimization algorithms, three types of neural\nnetwork architecture, 23 datasets and two loss functions. We demonstrate that\nalgorithms, which include dominant momentum characteristics, are not well\nsuited to be used with GOLS-I. However, we find GOLS-I to be effective in\nautomatically determining learning rate schedules over 15 orders of magnitude,\nfor most popular neural network training algorithms, effectively removing the\nneed to tune the sensitive hyperparameters of learning rate schedules in neural\nnetwork training.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 08:59:31 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Kafka", "Dominic", ""], ["Wilke", "Daniel Nicolas", ""]]}, {"id": "2007.01055", "submitter": "Zhen Long", "authors": "Zhen Long, Ce Zhu, Jiani Liu, Yipeng Liu", "title": "Bayesian Low Rank Tensor Ring Model for Image Completion", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2021.3062195", "report-no": null, "categories": "stat.ML cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low rank tensor ring model is powerful for image completion which recovers\nmissing entries in data acquisition and transformation. The recently proposed\ntensor ring (TR) based completion algorithms generally solve the low rank\noptimization problem by alternating least squares method with predefined ranks,\nwhich may easily lead to overfitting when the unknown ranks are set too large\nand only a few measurements are available. In this paper, we present a Bayesian\nlow rank tensor ring model for image completion by automatically learning the\nlow rank structure of data. A multiplicative interaction model is developed for\nthe low-rank tensor ring decomposition, where core factors are enforced to be\nsparse by assuming their entries obey Student-T distribution. Compared with\nmost of the existing methods, the proposed one is free of parameter-tuning, and\nthe TR ranks can be obtained by Bayesian inference. Numerical Experiments,\nincluding synthetic data, color images with different sizes and YaleFace\ndataset B with respect to one pose, show that the proposed approach outperforms\nstate-of-the-art ones, especially in terms of recovery accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 02:58:25 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Long", "Zhen", ""], ["Zhu", "Ce", ""], ["Liu", "Jiani", ""], ["Liu", "Yipeng", ""]]}, {"id": "2007.01072", "submitter": "Marcel Hildebrandt", "authors": "Marcel Hildebrandt, Hang Li, Rajat Koner, Volker Tresp, Stephan\n  G\\\"unnemann", "title": "Scene Graph Reasoning for Visual Question Answering", "comments": "ICML Workshop Graph Representation Learning and Beyond (GRL+)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering is concerned with answering free-form questions\nabout an image. Since it requires a deep linguistic understanding of the\nquestion and the ability to associate it with various objects that are present\nin the image, it is an ambitious task and requires techniques from both\ncomputer vision and natural language processing. We propose a novel method that\napproaches the task by performing context-driven, sequential reasoning based on\nthe objects and their semantic and spatial relationships present in the scene.\nAs a first step, we derive a scene graph which describes the objects in the\nimage, as well as their attributes and their mutual relationships. A\nreinforcement agent then learns to autonomously navigate over the extracted\nscene graph to generate paths, which are then the basis for deriving answers.\nWe conduct a first experimental study on the challenging GQA dataset with\nmanually curated scene graphs, where our method almost reaches the level of\nhuman performance.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 13:02:54 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Hildebrandt", "Marcel", ""], ["Li", "Hang", ""], ["Koner", "Rajat", ""], ["Tresp", "Volker", ""], ["G\u00fcnnemann", "Stephan", ""]]}, {"id": "2007.01073", "submitter": "Stijn de Waele", "authors": "Stijn de Waele", "title": "Accurate Characterization of Non-Uniformly Sampled Time Series using\n  Stochastic Differential Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-uniform sampling arises when an experimenter does not have full control\nover the sampling characteristics of the process under investigation. Moreover,\nit is introduced intentionally in algorithms such as Bayesian optimization and\ncompressive sensing. We argue that Stochastic Differential Equations (SDEs) are\nespecially well-suited for characterizing second order moments of such time\nseries. We introduce new initial estimates for the numerical optimization of\nthe likelihood, based on incremental estimation and initialization from\nautoregressive models. Furthermore, we introduce model truncation as a purely\ndata-driven method to reduce the order of the estimated model based on the SDE\nlikelihood. We show the increased accuracy achieved with the new estimator in\nsimulation experiments, covering all challenging circumstances that may be\nencountered in characterizing a non-uniformly sampled time series. Finally, we\napply the new estimator to experimental rainfall variability data.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 13:03:09 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["de Waele", "Stijn", ""]]}, {"id": "2007.01083", "submitter": "Zhiliang Wu", "authors": "Zhiliang Wu, Yinchong Yang, Yunpu Ma, Yushan Liu, Rui Zhao, Michael\n  Moor, Volker Tresp", "title": "Learning Individualized Treatment Rules with Estimated Translated\n  Inverse Propensity Score", "comments": null, "journal-ref": "2020 IEEE International Conference on Healthcare Informatics\n  (ICHI)", "doi": "10.1109/ICHI48887.2020.9374397", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized controlled trials typically analyze the effectiveness of\ntreatments with the goal of making treatment recommendations for patient\nsubgroups. With the advance of electronic health records, a great variety of\ndata has been collected in clinical practice, enabling the evaluation of\ntreatments and treatment policies based on observational data. In this paper,\nwe focus on learning individualized treatment rules (ITRs) to derive a\ntreatment policy that is expected to generate a better outcome for an\nindividual patient. In our framework, we cast ITRs learning as a contextual\nbandit problem and minimize the expected risk of the treatment policy. We\nconduct experiments with the proposed framework both in a simulation study and\nbased on a real-world dataset. In the latter case, we apply our proposed method\nto learn the optimal ITRs for the administration of intravenous (IV) fluids and\nvasopressors (VP). Based on various offline evaluation methods, we could show\nthat the policy derived in our framework demonstrates better performance\ncompared to both the physicians and other baselines, including a simple\ntreatment prediction approach. As a long-term goal, our derived policy might\neventually lead to better clinical guidelines for the administration of IV and\nVP.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 13:13:56 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Wu", "Zhiliang", ""], ["Yang", "Yinchong", ""], ["Ma", "Yunpu", ""], ["Liu", "Yushan", ""], ["Zhao", "Rui", ""], ["Moor", "Michael", ""], ["Tresp", "Volker", ""]]}, {"id": "2007.01097", "submitter": "Daniel Reiss Harris", "authors": "Daniel Reiss Harris", "title": "PrototypeML: A Neural Network Integrated Design and Development\n  Environment", "comments": "10 pages, 6 figures. Submitted to NeurIPS 2020. More details\n  available at https://PrototypeML.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network architectures are most often conceptually designed and\ndescribed in visual terms, but are implemented by writing error-prone code.\nPrototypeML is a machine learning development environment that bridges the\ndichotomy between the design and development processes: it provides a highly\nintuitive visual neural network design interface that supports (yet abstracts)\nthe full capabilities of the PyTorch deep learning framework, reduces model\ndesign and development time, makes debugging easier, and automates many\nframework and code writing idiosyncrasies. In this paper, we detail the deep\nlearning development deficiencies that drove the implementation of PrototypeML,\nand propose a hybrid approach to resolve these issues without limiting network\nexpressiveness or reducing code quality. We demonstrate the real-world benefits\nof a visual approach to neural network design for research, industry and\nteaching. Available at https://PrototypeML.com\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 08:47:46 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Harris", "Daniel Reiss", ""]]}, {"id": "2007.01126", "submitter": "Farzad Khalvati", "authors": "Partoo Vafaeikia, Khashayar Namdar, Farzad Khalvati", "title": "A Brief Review of Deep Multi-task Learning and Auxiliary Task Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning (MTL) optimizes several learning tasks simultaneously and\nleverages their shared information to improve generalization and the prediction\nof the model for each task. Auxiliary tasks can be added to the main task to\nultimately boost the performance. In this paper, we provide a brief review on\nthe recent deep multi-task learning (dMTL) approaches followed by methods on\nselecting useful auxiliary tasks that can be used in dMTL to improve the\nperformance of the model for the main task.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 14:23:39 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Vafaeikia", "Partoo", ""], ["Namdar", "Khashayar", ""], ["Khalvati", "Farzad", ""]]}, {"id": "2007.01127", "submitter": "Manit Mishra", "authors": "Shivaji Alaparthi (Data Scientist, CenturyLink, Bengaluru, India) and\n  Manit Mishra (Associate Professor, International Management Institute\n  Bhubaneswar, India)", "title": "Bidirectional Encoder Representations from Transformers (BERT): A\n  sentiment analysis odyssey", "comments": "15 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of the study is to investigate the relative effectiveness of four\ndifferent sentiment analysis techniques: (1) unsupervised lexicon-based model\nusing Sent WordNet; (2) traditional supervised machine learning model using\nlogistic regression; (3) supervised deep learning model using Long Short-Term\nMemory (LSTM); and, (4) advanced supervised deep learning models using\nBidirectional Encoder Representations from Transformers (BERT). We use publicly\navailable labeled corpora of 50,000 movie reviews originally posted on internet\nmovie database (IMDB) for analysis using Sent WordNet lexicon, logistic\nregression, LSTM, and BERT. The first three models were run on CPU based system\nwhereas BERT was run on GPU based system. The sentiment classification\nperformance was evaluated based on accuracy, precision, recall, and F1 score.\nThe study puts forth two key insights: (1) relative efficacy of four highly\nadvanced and widely used sentiment analysis techniques; (2) undisputed\nsuperiority of pre-trained advanced supervised deep learning BERT model in\nsentiment analysis from text data. This study provides professionals in\nanalytics industry and academicians working on text analysis key insight\nregarding comparative classification performance evaluation of key sentiment\nanalysis techniques, including the recently developed BERT. This is the first\nresearch endeavor to compare the advanced pre-trained supervised deep learning\nmodel of BERT vis-\\`a-vis other sentiment analysis models of LSTM, logistic\nregression, and Sent WordNet.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 14:23:57 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Alaparthi", "Shivaji", "", "Data Scientist, CenturyLink, Bengaluru, India"], ["Mishra", "Manit", "", "Associate Professor, International Management Institute\n  Bhubaneswar, India"]]}, {"id": "2007.01135", "submitter": "Rasheed El-Bouri", "authors": "Rasheed el-Bouri, David Eyre, Peter Watkinson, Tingting Zhu, David\n  Clifton", "title": "Student-Teacher Curriculum Learning via Reinforcement Learning:\n  Predicting Hospital Inpatient Admission Location", "comments": "16 pages, 31 figures, In Proceedings of the 37th International\n  Conference on Machine Learning", "journal-ref": "In Proceedings of the 37th International Conference on Machine\n  Learning, 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and reliable prediction of hospital admission location is important\ndue to resource-constraints and space availability in a clinical setting,\nparticularly when dealing with patients who come from the emergency department.\nIn this work we propose a student-teacher network via reinforcement learning to\ndeal with this specific problem. A representation of the weights of the student\nnetwork is treated as the state and is fed as an input to the teacher network.\nThe teacher network's action is to select the most appropriate batch of data to\ntrain the student network on from a training set sorted according to entropy.\nBy validating on three datasets, not only do we show that our approach\noutperforms state-of-the-art methods on tabular data and performs competitively\non image recognition, but also that novel curricula are learned by the teacher\nnetwork. We demonstrate experimentally that the teacher network can actively\nlearn about the student network and guide it to achieve better performance than\nif trained alone.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 15:00:43 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["el-Bouri", "Rasheed", ""], ["Eyre", "David", ""], ["Watkinson", "Peter", ""], ["Zhu", "Tingting", ""], ["Clifton", "David", ""]]}, {"id": "2007.01137", "submitter": "Nicholas Napolitano", "authors": "Nicholas Napolitano", "title": "Testing match-3 video games with Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Testing a video game is a critical step for the production process and\nrequires a great effort in terms of time and resources spent. Some software\nhouses are trying to use the artificial intelligence to reduce the need of\nhuman resources using systems able to replace a human agent. We study the\npossibility to use the Deep Reinforcement Learning to automate the testing\nprocess in match-3 video games and suggest to approach the problem in the\nframework of a Dueling Deep Q-Network paradigm. We test this kind of network on\nthe Jelly Juice game, a match-3 video game developed by the redBit Games. The\nnetwork extracts the essential information from the game environment and infers\nthe next move. We compare the results with the random player performance,\nfinding that the network shows a highest success rate. The results are in most\ncases similar with those obtained by real users, and the network also succeeds\nin learning over time the different features that distinguish the game levels\nand adapts its strategy to the increasing difficulties.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 12:41:35 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 11:30:08 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Napolitano", "Nicholas", ""]]}, {"id": "2007.01154", "submitter": "Mohammad Mahdi Kamani", "authors": "Farzin Haddadpour, Mohammad Mahdi Kamani, Aryan Mokhtari and Mehrdad\n  Mahdavi", "title": "Federated Learning with Compression: Unified Analysis and Sharp\n  Guarantees", "comments": "version 2. more experiments and comparisons", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In federated learning, communication cost is often a critical bottleneck to\nscale up distributed optimization algorithms to collaboratively learn a model\nfrom millions of devices with potentially unreliable or limited communication\nand heterogeneous data distributions. Two notable trends to deal with the\ncommunication overhead of federated algorithms are gradient compression and\nlocal computation with periodic communication. Despite many attempts,\ncharacterizing the relationship between these two approaches has proven\nelusive. We address this by proposing a set of algorithms with periodical\ncompressed (quantized or sparsified) communication and analyze their\nconvergence properties in both homogeneous and heterogeneous local data\ndistribution settings. For the homogeneous setting, our analysis improves\nexisting bounds by providing tighter convergence rates for both strongly convex\nand non-convex objective functions. To mitigate data heterogeneity, we\nintroduce a local gradient tracking scheme and obtain sharp convergence rates\nthat match the best-known communication complexities without compression for\nconvex, strongly convex, and nonconvex settings. We complement our theoretical\nresults and demonstrate the effectiveness of our proposed methods by several\nexperiments on real-world datasets.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 14:44:07 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2020 04:33:08 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Haddadpour", "Farzin", ""], ["Kamani", "Mohammad Mahdi", ""], ["Mokhtari", "Aryan", ""], ["Mahdavi", "Mehrdad", ""]]}, {"id": "2007.01160", "submitter": "Blair Bilodeau", "authors": "Blair Bilodeau, Dylan J. Foster, Daniel M. Roy", "title": "Tight Bounds on Minimax Regret under Logarithmic Loss via\n  Self-Concordance", "comments": "25 pages", "journal-ref": "Proceedings of the 37th International Conference on Machine\n  Learning, ICML 2020", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classical problem of sequential probability assignment under\nlogarithmic loss while competing against an arbitrary, potentially\nnonparametric class of experts. We obtain tight bounds on the minimax regret\nvia a new approach that exploits the self-concordance property of the\nlogarithmic loss. We show that for any expert class with (sequential) metric\nentropy $\\mathcal{O}(\\gamma^{-p})$ at scale $\\gamma$, the minimax regret is\n$\\mathcal{O}(n^{p/(p+1)})$, and that this rate cannot be improved without\nadditional assumptions on the expert class under consideration. As an\napplication of our techniques, we resolve the minimax regret for nonparametric\nLipschitz classes of experts.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 14:47:33 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 14:46:13 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Bilodeau", "Blair", ""], ["Foster", "Dylan J.", ""], ["Roy", "Daniel M.", ""]]}, {"id": "2007.01162", "submitter": "Tian Li", "authors": "Tian Li, Ahmad Beirami, Maziar Sanjabi, Virginia Smith", "title": "Tilted Empirical Risk Minimization", "comments": "Accepted by ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Empirical risk minimization (ERM) is typically designed to perform well on\nthe average loss, which can result in estimators that are sensitive to\noutliers, generalize poorly, or treat subgroups unfairly. While many methods\naim to address these problems individually, in this work, we explore them\nthrough a unified framework -- tilted empirical risk minimization (TERM). In\nparticular, we show that it is possible to flexibly tune the impact of\nindividual losses through a straightforward extension to ERM using a\nhyperparameter called the tilt. We provide several interpretations of the\nresulting framework: We show that TERM can increase or decrease the influence\nof outliers, respectively, to enable fairness or robustness; has\nvariance-reduction properties that can benefit generalization; and can be\nviewed as a smooth approximation to a superquantile method. We develop batch\nand stochastic first-order optimization methods for solving TERM, and show that\nthe problem can be efficiently solved relative to common alternatives. Finally,\nwe demonstrate that TERM can be used for a multitude of applications, such as\nenforcing fairness between subgroups, mitigating the effect of outliers, and\nhandling class imbalance. TERM is not only competitive with existing solutions\ntailored to these individual problems, but can also enable entirely new\napplications, such as simultaneously addressing outliers and promoting\nfairness.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 14:49:48 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 16:34:26 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Li", "Tian", ""], ["Beirami", "Ahmad", ""], ["Sanjabi", "Maziar", ""], ["Smith", "Virginia", ""]]}, {"id": "2007.01165", "submitter": "Anthony Nouy", "authors": "Bertrand Michel and Anthony Nouy", "title": "Learning with tree tensor networks: complexity estimates and model\n  selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree tensor networks, or tree-based tensor formats, are prominent model\nclasses for the approximation of high-dimensional functions in computational\nand data science. They correspond to sum-product neural networks with a sparse\nconnectivity associated with a dimension tree and widths given by a tuple of\ntensor ranks. The approximation power of these models has been proved to be\n(near to) optimal for classical smoothness classes. However, in an empirical\nrisk minimization framework with a limited number of observations, the\ndimension tree and ranks should be selected carefully to balance estimation and\napproximation errors. We propose and analyze a complexity-based model selection\nmethod for tree tensor networks in an empirical risk minimization framework and\nwe analyze its performance over a wide range of smoothness classes. Given a\nfamily of model classes associated with different trees, ranks, tensor product\nfeature spaces and sparsity patterns for sparse tensor networks, a model is\nselected (\\`a la Barron, Birg\\'e, Massart) by minimizing a penalized empirical\nrisk, with a penalty depending on the complexity of the model class and derived\nfrom estimates of the metric entropy of tree tensor networks. This choice of\npenalty yields a risk bound for the selected predictor. In a least-squares\nsetting, after deriving fast rates of convergence of the risk, we show that our\nstrategy is (near to) minimax adaptive to a wide range of smoothness classes\nincluding Sobolev or Besov spaces (with isotropic, anisotropic or mixed\ndominating smoothness) and analytic functions. We discuss the role of sparsity\nof the tensor network for obtaining optimal performance in several regimes. In\npractice, the amplitude of the penalty is calibrated with a slope heuristics\nmethod. Numerical experiments in a least-squares regression setting illustrate\nthe performance of the strategy.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 14:52:08 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 15:59:26 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 10:15:30 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Michel", "Bertrand", ""], ["Nouy", "Anthony", ""]]}, {"id": "2007.01174", "submitter": "Parameswaran Kamalaruban Dr.", "authors": "Luca Viano, Yu-Ting Huang, Parameswaran Kamalaruban, Adrian Weller,\n  Volkan Cevher", "title": "Robust Inverse Reinforcement Learning under Transition Dynamics Mismatch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the inverse reinforcement learning (IRL) problem under a transition\ndynamics mismatch between the expert and the learner. Specifically, we consider\nthe Maximum Causal Entropy (MCE) IRL learner model and provide a tight upper\nbound on the learner's performance degradation based on the $\\ell_1$-distance\nbetween the two transition dynamics of the expert and the learner. Then, by\nleveraging insights from the Robust RL literature, we propose a robust MCE IRL\nalgorithm, which is a principled approach to help with this mismatch. Finally,\nwe empirically demonstrate the stable performance of our algorithm compared to\nthe standard MCE IRL algorithm under transition mismatches in finite MDP\nproblems.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 14:57:13 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 09:38:32 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Viano", "Luca", ""], ["Huang", "Yu-Ting", ""], ["Kamalaruban", "Parameswaran", ""], ["Weller", "Adrian", ""], ["Cevher", "Volkan", ""]]}, {"id": "2007.01179", "submitter": "Yuge Shi", "authors": "Yuge Shi, Brooks Paige, Philip H.S. Torr, N. Siddharth", "title": "Relating by Contrasting: A Data-efficient Framework for Multimodal\n  Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multimodal learning for generative models often refers to the learning of\nabstract concepts from the commonality of information in multiple modalities,\nsuch as vision and language. While it has proven effective for learning\ngeneralisable representations, the training of such models often requires a\nlarge amount of \"related\" multimodal data that shares commonality, which can be\nexpensive to come by. To mitigate this, we develop a novel contrastive\nframework for generative model learning, allowing us to train the model not\njust by the commonality between modalities, but by the distinction between\n\"related\" and \"unrelated\" multimodal data. We show in experiments that our\nmethod enables data-efficient multimodal learning on challenging datasets for\nvarious multimodal VAE models. We also show that under our proposed framework,\nthe generative model can accurately identify related samples from unrelated\nones, making it possible to make use of the plentiful unlabeled, unpaired\nmultimodal data.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 15:08:11 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 09:58:33 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Shi", "Yuge", ""], ["Paige", "Brooks", ""], ["Torr", "Philip H. S.", ""], ["Siddharth", "N.", ""]]}, {"id": "2007.01181", "submitter": "Ellen Vitercik", "authors": "Andr\\'es Mu\\~noz Medina, Umar Syed, Sergei Vassilvitskii, Ellen\n  Vitercik", "title": "Private Optimization Without Constraint Violations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of differentially private optimization with linear\nconstraints when the right-hand-side of the constraints depends on private\ndata. This type of problem appears in many applications, especially resource\nallocation. Previous research provided solutions that retained privacy but\nsometimes violated the constraints. In many settings, however, the constraints\ncannot be violated under any circumstances. To address this hard requirement,\nwe present an algorithm that releases a nearly-optimal solution satisfying the\nconstraints with probability 1. We also prove a lower bound demonstrating that\nthe difference between the objective value of our algorithm's solution and the\noptimal solution is tight up to logarithmic factors among all differentially\nprivate algorithms. We conclude with experiments demonstrating that our\nalgorithm can achieve nearly optimal performance while preserving privacy.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 15:08:52 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 02:40:12 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Medina", "Andr\u00e9s Mu\u00f1oz", ""], ["Syed", "Umar", ""], ["Vassilvitskii", "Sergei", ""], ["Vitercik", "Ellen", ""]]}, {"id": "2007.01195", "submitter": "Mayalen Etcheverry", "authors": "Mayalen Etcheverry, Clement Moulin-Frier, Pierre-Yves Oudeyer", "title": "Hierarchically Organized Latent Modules for Exploratory Search in\n  Morphogenetic Systems", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 33 (NeurIPS 2020\n  - oral)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI nlin.CG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-organization of complex morphological patterns from local interactions\nis a fascinating phenomenon in many natural and artificial systems. In the\nartificial world, typical examples of such morphogenetic systems are cellular\nautomata. Yet, their mechanisms are often very hard to grasp and so far\nscientific discoveries of novel patterns have primarily been relying on manual\ntuning and ad hoc exploratory search. The problem of automated diversity-driven\ndiscovery in these systems was recently introduced [26, 62], highlighting that\ntwo key ingredients are autonomous exploration and unsupervised representation\nlearning to describe \"relevant\" degrees of variations in the patterns. In this\npaper, we motivate the need for what we call Meta-diversity search, arguing\nthat there is not a unique ground truth interesting diversity as it strongly\ndepends on the final observer and its motives. Using a continuous game-of-life\nsystem for experiments, we provide empirical evidences that relying on\nmonolithic architectures for the behavioral embedding design tends to bias the\nfinal discoveries (both for hand-defined and unsupervisedly-learned features)\nwhich are unlikely to be aligned with the interest of a final end-user. To\naddress these issues, we introduce a novel dynamic and modular architecture\nthat enables unsupervised learning of a hierarchy of diverse representations.\nCombined with intrinsically motivated goal exploration algorithms, we show that\nthis system forms a discovery assistant that can efficiently adapt its\ndiversity search towards preferences of a user using only a very small amount\nof user feedback.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 15:28:27 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 14:47:31 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Etcheverry", "Mayalen", ""], ["Moulin-Frier", "Clement", ""], ["Oudeyer", "Pierre-Yves", ""]]}, {"id": "2007.01200", "submitter": "Caio Davi", "authors": "Caio Davi and Ulisses Braga-Neto", "title": "A Semi-Supervised Generative Adversarial Network for Prediction of\n  Genetic Disease Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For most diseases, building large databases of labeled genetic data is an\nexpensive and time-demanding task. To address this, we introduce genetic\nGenerative Adversarial Networks (gGAN), a semi-supervised approach based on an\ninnovative GAN architecture to create large synthetic genetic data sets\nstarting with a small amount of labeled data and a large amount of unlabeled\ndata. Our goal is to determine the propensity of a new individual to develop\nthe severe form of the illness from their genetic profile alone. The proposed\nmodel achieved satisfactory results using real genetic data from different\ndatasets and populations, in which the test populations may not have the same\ngenetic profiles. The proposed model is self-aware and capable of determining\nwhether a new genetic profile has enough compatibility with the data on which\nthe network was trained and is thus suitable for prediction. The code and\ndatasets used can be found at https://github.com/caio-davi/gGAN.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 15:35:14 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Davi", "Caio", ""], ["Braga-Neto", "Ulisses", ""]]}, {"id": "2007.01208", "submitter": "Zunwei Fu", "authors": "Congcong Zhang, Sung-Kwun Oh, Witold Pedrycz, Zunwei Fu and Shanzhen\n  Lu", "title": "Exponentially Weighted l_2 Regularization Strategy in Constructing\n  Reinforced Second-order Fuzzy Rule-based Model", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the conventional Takagi-Sugeno-Kang (TSK)-type fuzzy models, constant or\nlinear functions are usually utilized as the consequent parts of the fuzzy\nrules, but they cannot effectively describe the behavior within local regions\ndefined by the antecedent parts. In this article, a theoretical and practical\ndesign methodology is developed to address this problem. First, the information\ngranulation (Fuzzy C-Means) method is applied to capture the structure in the\ndata and split the input space into subspaces, as well as form the antecedent\nparts. Second, the quadratic polynomials (QPs) are employed as the consequent\nparts. Compared with constant and linear functions, QPs can describe the\ninput-output behavior within the local regions (subspaces) by refining the\nrelationship between input and output variables. However, although QP can\nimprove the approximation ability of the model, it could lead to the\ndeterioration of the prediction ability of the model (e.g., overfitting). To\nhandle this issue, we introduce an exponential weight approach inspired by the\nweight function theory encountered in harmonic analysis. More specifically, we\nadopt the exponential functions as the targeted penalty terms, which are\nequipped with l2 regularization (l2) (i.e., exponential weighted l2, ewl_2) to\nmatch the proposed reinforced second-order fuzzy rule-based model (RSFRM)\nproperly. The advantage of el 2 compared to ordinary l2 lies in separately\nidentifying and penalizing different types of polynomial terms in the\ncoefficient estimation, and its results not only alleviate the overfitting and\nprevent the deterioration of generalization ability but also effectively\nrelease the prediction potential of the model.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 15:42:15 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Zhang", "Congcong", ""], ["Oh", "Sung-Kwun", ""], ["Pedrycz", "Witold", ""], ["Fu", "Zunwei", ""], ["Lu", "Shanzhen", ""]]}, {"id": "2007.01221", "submitter": "Mariami Gachechiladze", "authors": "Mariami Gachechiladze, Nikolai Miklin, and Rafael Chaves", "title": "Quantifying causal influences in the presence of a quantum common cause", "comments": "13 pages, 2 figures", "journal-ref": "Phys. Rev. Lett. 125, 230401 (2020)", "doi": "10.1103/PhysRevLett.125.230401", "report-no": null, "categories": "quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum mechanics challenges our intuition on the cause-effect relations in\nnature. Some fundamental concepts, including Reichenbach's common cause\nprinciple or the notion of local realism, have to be reconsidered.\nTraditionally, this is witnessed by the violation of a Bell inequality. But are\nBell inequalities the only signature of the incompatibility between quantum\ncorrelations and causality theory? Motivated by this question we introduce a\ngeneral framework able to estimate causal influences between two variables,\nwithout the need of interventions and irrespectively of the classical, quantum,\nor even post-quantum nature of a common cause. In particular, by considering\nthe simplest instrumental scenario -- for which violation of Bell inequalities\nis not possible -- we show that every pure bipartite entangled state violates\nthe classical bounds on causal influence, thus answering in negative to the\nposed question and opening a new venue to explore the role of causality within\nquantum theory.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 16:07:18 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Gachechiladze", "Mariami", ""], ["Miklin", "Nikolai", ""], ["Chaves", "Rafael", ""]]}, {"id": "2007.01229", "submitter": "Shenyang Huang", "authors": "Shenyang Huang, Yasmeen Hitti, Guillaume Rabusseau, Reihaneh Rabbany", "title": "Laplacian Change Point Detection for Dynamic Graphs", "comments": "in KDD 2020, 10 pages", "journal-ref": null, "doi": "10.1145/3394486.3403077", "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic and temporal graphs are rich data structures that are used to model\ncomplex relationships between entities over time. In particular, anomaly\ndetection in temporal graphs is crucial for many real world applications such\nas intrusion identification in network systems, detection of ecosystem\ndisturbances and detection of epidemic outbreaks. In this paper, we focus on\nchange point detection in dynamic graphs and address two main challenges\nassociated with this problem: I) how to compare graph snapshots across time,\nII) how to capture temporal dependencies. To solve the above challenges, we\npropose Laplacian Anomaly Detection (LAD) which uses the spectrum of the\nLaplacian matrix of the graph structure at each snapshot to obtain low\ndimensional embeddings. LAD explicitly models short term and long term\ndependencies by applying two sliding windows. In synthetic experiments, LAD\noutperforms the state-of-the-art method. We also evaluate our method on three\nreal dynamic networks: UCI message network, US senate co-sponsorship network\nand Canadian bill voting network. In all three datasets, we demonstrate that\nour method can more effectively identify anomalous time points according to\nsignificant real world events.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 16:24:24 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Huang", "Shenyang", ""], ["Hitti", "Yasmeen", ""], ["Rabusseau", "Guillaume", ""], ["Rabbany", "Reihaneh", ""]]}, {"id": "2007.01231", "submitter": "Kian Ahrabian", "authors": "Kian Ahrabian, Daniel Tarlow, Hehuimin Cheng, Jin L.C. Guo", "title": "Software Engineering Event Modeling using Relative Time in Temporal\n  Knowledge Graphs", "comments": "11 pages, 1 figure. 37th International Conference on Machine Learning\n  (ICML 2020) - Workshop on Graph Representation Learning and Beyond", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a multi-relational temporal Knowledge Graph based on the daily\ninteractions between artifacts in GitHub, one of the largest social coding\nplatforms. Such representation enables posing many user-activity and project\nmanagement questions as link prediction and time queries over the knowledge\ngraph. In particular, we introduce two new datasets for i) interpolated\ntime-conditioned link prediction and ii) extrapolated time-conditioned\nlink/time prediction queries, each with distinguished properties. Our\nexperiments on these datasets highlight the potential of adapting knowledge\ngraphs to answer broad software engineering questions. Meanwhile, it also\nreveals the unsatisfactory performance of existing temporal models on\nextrapolated queries and time prediction queries in general. To overcome these\nshortcomings, we introduce an extension to current temporal models using\nrelative temporal information with regards to past events.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 16:28:43 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 01:07:23 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Ahrabian", "Kian", ""], ["Tarlow", "Daniel", ""], ["Cheng", "Hehuimin", ""], ["Guo", "Jin L. C.", ""]]}, {"id": "2007.01238", "submitter": "Gustau Camps-Valls", "authors": "Gustau Camps-Valls and Dino Sejdinovic and Jakob Runge and Markus\n  Reichstein", "title": "A Perspective on Gaussian Processes for Earth Observation", "comments": "1 figure", "journal-ref": "National Science Review, Volume 6, Issue 4, July 2019, Pages\n  616-618", "doi": "10.1093/nsr/nwz028", "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Earth observation (EO) by airborne and satellite remote sensing and in-situ\nobservations play a fundamental role in monitoring our planet. In the last\ndecade, machine learning and Gaussian processes (GPs) in particular has\nattained outstanding results in the estimation of bio-geo-physical variables\nfrom the acquired images at local and global scales in a time-resolved manner.\nGPs provide not only accurate estimates but also principled uncertainty\nestimates for the predictions, can easily accommodate multimodal data coming\nfrom different sensors and from multitemporal acquisitions, allow the\nintroduction of physical knowledge, and a formal treatment of uncertainty\nquantification and error propagation. Despite great advances in forward and\ninverse modelling, GP models still have to face important challenges that are\nrevised in this perspective paper. GP models should evolve towards data-driven\nphysics-aware models that respect signal characteristics, be consistent with\nelementary laws of physics, and move from pure regression to observational\ncausal inference.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 16:44:11 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Camps-Valls", "Gustau", ""], ["Sejdinovic", "Dino", ""], ["Runge", "Jakob", ""], ["Reichstein", "Markus", ""]]}, {"id": "2007.01255", "submitter": "Toshiaki Koike-Akino", "authors": "Andac Demir, Toshiaki Koike-Akino, Ye Wang, Deniz Erdogmus", "title": "AutoBayes: Automated Bayesian Graph Exploration for Nuisance-Robust\n  Inference", "comments": "24 pages, 11 figures, under review in ICLR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning data representations that capture task-related features, but are\ninvariant to nuisance variations remains a key challenge in machine learning.\nWe introduce an automated Bayesian inference framework, called AutoBayes, that\nexplores different graphical models linking classifier, encoder, decoder,\nestimator and adversarial network blocks to optimize nuisance-invariant machine\nlearning pipelines. AutoBayes also enables learning disentangled\nrepresentations, where the latent variable is split into multiple pieces to\nimpose various relationships with the nuisance variation and task labels. We\nbenchmark the framework on several public datasets, and provide analysis of its\ncapability for subject-transfer learning with/without variational modeling and\nadversarial training. We demonstrate a significant performance improvement with\nensemble learning across explored graphical models.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 17:06:26 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 23:01:04 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2020 16:39:32 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Demir", "Andac", ""], ["Koike-Akino", "Toshiaki", ""], ["Wang", "Ye", ""], ["Erdogmus", "Deniz", ""]]}, {"id": "2007.01263", "submitter": "Matthew Cook", "authors": "Matthew Cook, Alina Zare, Paul Gader", "title": "Outlier Detection through Null Space Analysis of Neural Networks", "comments": "6 pages, 4 figures, Presented at the ICML 2020 Workshop on\n  Uncertainty and Robustness in Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Many machine learning classification systems lack competency awareness.\nSpecifically, many systems lack the ability to identify when outliers (e.g.,\nsamples that are distinct from and not represented in the training data\ndistribution) are being presented to the system. The ability to detect outliers\nis of practical significance since it can help the system behave in an\nreasonable way when encountering unexpected data. In prior work, outlier\ndetection is commonly carried out in a processing pipeline that is distinct\nfrom the classification model. Thus, for a complete system that incorporates\noutlier detection and classification, two models must be trained, increasing\nthe overall complexity of the approach. In this paper we use the concept of the\nnull space to integrate an outlier detection method directly into a neural\nnetwork used for classification. Our method, called Null Space Analysis (NuSA)\nof neural networks, works by computing and controlling the magnitude of the\nnull space projection as data is passed through a network. Using these\nprojections, we can then calculate a score that can differentiate between\nnormal and abnormal data. Results are shown that indicate networks trained with\nNuSA retain their classification performance while also being able to detect\noutliers at rates similar to commonly used outlier detection algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 17:17:21 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Cook", "Matthew", ""], ["Zare", "Alina", ""], ["Gader", "Paul", ""]]}, {"id": "2007.01276", "submitter": "Navid Ghassemi", "authors": "Afshin Shoeibi, Marjane Khodatars, Navid Ghassemi, Mahboobeh Jafari,\n  Parisa Moridian, Roohallah Alizadehsani, Maryam Panahiazar, Fahime Khozeimeh,\n  Assef Zare, Hossein Hosseini-Nejad, Abbas Khosravi, Amir F. Atiya, Diba\n  Aminshahidi, Sadiq Hussain, Modjtaba Rouhani, Saeid Nahavandi, Udyavara\n  Rajendra Acharya", "title": "Epileptic Seizures Detection Using Deep Learning Techniques: A Review", "comments": null, "journal-ref": "International Journal of Environmental Research and Public Health.\n  2021; 18(11):5780", "doi": "10.3390/ijerph18115780", "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A variety of screening approaches have been proposed to diagnose epileptic\nseizures, using electroencephalography (EEG) and magnetic resonance imaging\n(MRI) modalities. Artificial intelligence encompasses a variety of areas, and\none of its branches is deep learning (DL). Before the rise of DL, conventional\nmachine learning algorithms involving feature extraction were performed. This\nlimited their performance to the ability of those handcrafting the features.\nHowever, in DL, the extraction of features and classification are entirely\nautomated. The advent of these techniques in many areas of medicine, such as in\nthe diagnosis of epileptic seizures, has made significant advances. In this\nstudy, a comprehensive overview of works focused on automated epileptic seizure\ndetection using DL techniques and neuroimaging modalities is presented. Various\nmethods proposed to diagnose epileptic seizures automatically using EEG and MRI\nmodalities are described. In addition, rehabilitation systems developed for\nepileptic seizures using DL have been analyzed, and a summary is provided. The\nrehabilitation tools include cloud computing techniques and hardware required\nfor implementation of DL algorithms. The important challenges in accurate\ndetection of automated epileptic seizures using DL with EEG and MRI modalities\nare discussed. The advantages and limitations in employing DL-based techniques\nfor epileptic seizures diagnosis are presented. Finally, the most promising DL\nmodels proposed and possible future works on automated epileptic seizure\ndetection are delineated.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 17:34:02 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 17:50:58 GMT"}, {"version": "v3", "created": "Sat, 29 May 2021 14:18:28 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Shoeibi", "Afshin", ""], ["Khodatars", "Marjane", ""], ["Ghassemi", "Navid", ""], ["Jafari", "Mahboobeh", ""], ["Moridian", "Parisa", ""], ["Alizadehsani", "Roohallah", ""], ["Panahiazar", "Maryam", ""], ["Khozeimeh", "Fahime", ""], ["Zare", "Assef", ""], ["Hosseini-Nejad", "Hossein", ""], ["Khosravi", "Abbas", ""], ["Atiya", "Amir F.", ""], ["Aminshahidi", "Diba", ""], ["Hussain", "Sadiq", ""], ["Rouhani", "Modjtaba", ""], ["Nahavandi", "Saeid", ""], ["Acharya", "Udyavara Rajendra", ""]]}, {"id": "2007.01285", "submitter": "Navid Ghassemi", "authors": "Marjane Khodatars, Afshin Shoeibi, Navid Ghassemi, Mahboobeh Jafari,\n  Ali Khadem, Delaram Sadeghi, Parisa Moridian, Sadiq Hussain, Roohallah\n  Alizadehsani, Assef Zare, Abbas Khosravi, Saeid Nahavandi, U. Rajendra\n  Acharya, Michael Berk", "title": "Deep Learning for Neuroimaging-based Diagnosis and Rehabilitation of\n  Autism Spectrum Disorder: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate diagnosis of Autism Spectrum Disorder (ASD) is essential for its\nmanagement and rehabilitation. Neuroimaging techniques that are non-invasive\nare disease markers and may be leveraged to aid ASD diagnosis. Structural and\nfunctional neuroimaging techniques provide physicians substantial information\nabout the structure (anatomy and structural connectivity) and function\n(activity and functional connectivity) of the brain. Due to the intricate\nstructure and function of the brain, diagnosing ASD with neuroimaging data\nwithout exploiting artificial intelligence (AI) techniques is extremely\nchallenging. AI techniques comprise traditional machine learning (ML)\napproaches and deep learning (DL) techniques. Conventional ML methods employ\nvarious feature extraction and classification techniques, but in DL, the\nprocess of feature extraction and classification is accomplished intelligently\nand integrally. In this paper, studies conducted with the aid of DL networks to\ndistinguish ASD were investigated. Rehabilitation tools provided by supporting\nASD patients utilizing DL networks were also assessed. Finally, we presented\nimportant challenges in this automated detection and rehabilitation of ASD.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 17:49:19 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2020 17:23:17 GMT"}, {"version": "v3", "created": "Sun, 26 Jul 2020 17:31:24 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Khodatars", "Marjane", ""], ["Shoeibi", "Afshin", ""], ["Ghassemi", "Navid", ""], ["Jafari", "Mahboobeh", ""], ["Khadem", "Ali", ""], ["Sadeghi", "Delaram", ""], ["Moridian", "Parisa", ""], ["Hussain", "Sadiq", ""], ["Alizadehsani", "Roohallah", ""], ["Zare", "Assef", ""], ["Khosravi", "Abbas", ""], ["Nahavandi", "Saeid", ""], ["Acharya", "U. Rajendra", ""], ["Berk", "Michael", ""]]}, {"id": "2007.01290", "submitter": "Luofeng Liao", "authors": "Luofeng Liao, You-Lin Chen, Zhuoran Yang, Bo Dai, Zhaoran Wang, Mladen\n  Kolar", "title": "Provably Efficient Neural Estimation of Structural Equation Model: An\n  Adversarial Approach", "comments": "- v1: Submitted to NeurIPS 2020. Under review - v2: Revised after\n  NeurIPS reviews. Major updates: (i) clean presentation of consistency\n  results; (ii) more references for conditional moment problems - v3: Add\n  references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural equation models (SEMs) are widely used in sciences, ranging from\neconomics to psychology, to uncover causal relationships underlying a complex\nsystem under consideration and estimate structural parameters of interest. We\nstudy estimation in a class of generalized SEMs where the object of interest is\ndefined as the solution to a linear operator equation. We formulate the linear\noperator equation as a min-max game, where both players are parameterized by\nneural networks (NNs), and learn the parameters of these neural networks using\nthe stochastic gradient descent. We consider both 2-layer and multi-layer NNs\nwith ReLU activation functions and prove global convergence in an\noverparametrized regime, where the number of neurons is diverging. The results\nare established using techniques from online learning and local linearization\nof NNs, and improve in several aspects the current state-of-the-art. For the\nfirst time we provide a tractable estimation procedure for SEMs based on NNs\nwith provable convergence and without the need for sample splitting.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 17:55:47 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 17:07:08 GMT"}, {"version": "v3", "created": "Tue, 20 Oct 2020 16:56:32 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Liao", "Luofeng", ""], ["Chen", "You-Lin", ""], ["Yang", "Zhuoran", ""], ["Dai", "Bo", ""], ["Wang", "Zhaoran", ""], ["Kolar", "Mladen", ""]]}, {"id": "2007.01293", "submitter": "Zhongzheng Ren", "authors": "Zhongzheng Ren, Raymond A. Yeh, Alexander G. Schwing", "title": "Not All Unlabeled Data are Equal: Learning to Weight Data in\n  Semi-supervised Learning", "comments": "NeurIPS camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing semi-supervised learning (SSL) algorithms use a single weight to\nbalance the loss of labeled and unlabeled examples, i.e., all unlabeled\nexamples are equally weighted. But not all unlabeled data are equal. In this\npaper we study how to use a different weight for every unlabeled example.\nManual tuning of all those weights -- as done in prior work -- is no longer\npossible. Instead, we adjust those weights via an algorithm based on the\ninfluence function, a measure of a model's dependency on one training example.\nTo make the approach efficient, we propose a fast and effective approximation\nof the influence function. We demonstrate that this technique outperforms\nstate-of-the-art methods on semi-supervised image and language classification\ntasks.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 17:59:05 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 04:29:54 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Ren", "Zhongzheng", ""], ["Yeh", "Raymond A.", ""], ["Schwing", "Alexander G.", ""]]}, {"id": "2007.01327", "submitter": "Micha{\\l} Derezi\\'nski", "authors": "Micha{\\l} Derezi\\'nski, Burak Bartan, Mert Pilanci and Michael W.\n  Mahoney", "title": "Debiasing Distributed Second Order Optimization with Surrogate Sketching\n  and Scaled Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed second order optimization, a standard strategy is to average\nmany local estimates, each of which is based on a small sketch or batch of the\ndata. However, the local estimates on each machine are typically biased,\nrelative to the full solution on all of the data, and this can limit the\neffectiveness of averaging. Here, we introduce a new technique for debiasing\nthe local estimates, which leads to both theoretical and empirical improvements\nin the convergence rate of distributed second order methods. Our technique has\ntwo novel components: (1) modifying standard sketching techniques to obtain\nwhat we call a surrogate sketch; and (2) carefully scaling the global\nregularization parameter for local computations. Our surrogate sketches are\nbased on determinantal point processes, a family of distributions for which the\nbias of an estimate of the inverse Hessian can be computed exactly. Based on\nthis computation, we show that when the objective being minimized is\n$l_2$-regularized with parameter $\\lambda$ and individual machines are each\ngiven a sketch of size $m$, then to eliminate the bias, local estimates should\nbe computed using a shrunk regularization parameter given by\n$\\lambda^{\\prime}=\\lambda\\cdot(1-\\frac{d_{\\lambda}}{m})$, where $d_{\\lambda}$\nis the $\\lambda$-effective dimension of the Hessian (or, for quadratic\nproblems, the data matrix).\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 18:08:14 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Derezi\u0144ski", "Micha\u0142", ""], ["Bartan", "Burak", ""], ["Pilanci", "Mert", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "2007.01332", "submitter": "Andrew Y. K. Foong", "authors": "Andrew Y. K. Foong, Wessel P. Bruinsma, Jonathan Gordon, Yann Dubois,\n  James Requeima, Richard E. Turner", "title": "Meta-Learning Stationary Stochastic Process Prediction with\n  Convolutional Neural Processes", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stationary stochastic processes (SPs) are a key component of many\nprobabilistic models, such as those for off-the-grid spatio-temporal data. They\nenable the statistical symmetry of underlying physical phenomena to be\nleveraged, thereby aiding generalization. Prediction in such models can be\nviewed as a translation equivariant map from observed data sets to predictive\nSPs, emphasizing the intimate relationship between stationarity and\nequivariance. Building on this, we propose the Convolutional Neural Process\n(ConvNP), which endows Neural Processes (NPs) with translation equivariance and\nextends convolutional conditional NPs to allow for dependencies in the\npredictive distribution. The latter enables ConvNPs to be deployed in settings\nwhich require coherent samples, such as Thompson sampling or conditional image\ncompletion. Moreover, we propose a new maximum-likelihood objective to replace\nthe standard ELBO objective in NPs, which conceptually simplifies the framework\nand empirically improves performance. We demonstrate the strong performance and\ngeneralization capabilities of ConvNPs on 1D regression, image completion, and\nvarious tasks with real-world spatio-temporal data.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 18:25:27 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 10:52:35 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Foong", "Andrew Y. K.", ""], ["Bruinsma", "Wessel P.", ""], ["Gordon", "Jonathan", ""], ["Dubois", "Yann", ""], ["Requeima", "James", ""], ["Turner", "Richard E.", ""]]}, {"id": "2007.01346", "submitter": "Umang Varma", "authors": "Umang Varma, Lalit Jain, Anna C. Gilbert", "title": "Spectral Methods for Ranking with Scarce Data", "comments": "To appear in Proceedings of Uncertainty in Artificial Intelligence\n  (UAI) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given a number of pairwise preferences of items, a common task is to rank all\nthe items. Examples include pairwise movie ratings, New Yorker cartoon caption\ncontests, and many other consumer preferences tasks. What these settings have\nin common is two-fold: a scarcity of data (it may be costly to get comparisons\nfor all the pairs of items) and additional feature information about the items\n(e.g., movie genre, director, and cast). In this paper we modify a popular and\nwell studied method, RankCentrality for rank aggregation to account for few\ncomparisons and that incorporates additional feature information. This method\nreturns meaningful rankings even under scarce comparisons. Using diffusion\nbased methods, we incorporate feature information that outperforms\nstate-of-the-art methods in practice. We also provide improved sample\ncomplexity for RankCentrality in a variety of sampling schemes.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 19:17:35 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Varma", "Umang", ""], ["Jain", "Lalit", ""], ["Gilbert", "Anna C.", ""]]}, {"id": "2007.01350", "submitter": "Jiri Navratil", "authors": "Jiri Navratil, Matthew Arnold, Benjamin Elder", "title": "Uncertainty Prediction for Deep Sequential Regression Using Meta Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating high quality uncertainty estimates for sequential regression,\nparticularly deep recurrent networks, remains a challenging and open problem.\nExisting approaches often make restrictive assumptions (such as stationarity)\nyet still perform poorly in practice, particularly in presence of real world\nnon-stationary signals and drift. This paper describes a flexible method that\ncan generate symmetric and asymmetric uncertainty estimates, makes no\nassumptions about stationarity, and outperforms competitive baselines on both\ndrift and non drift scenarios. This work helps make sequential regression more\neffective and practical for use in real-world applications, and is a powerful\nnew addition to the modeling toolbox for sequential uncertainty quantification\nin general.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 19:27:17 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 21:59:50 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Navratil", "Jiri", ""], ["Arnold", "Matthew", ""], ["Elder", "Benjamin", ""]]}, {"id": "2007.01356", "submitter": "Yifei Wang", "authors": "Yifei Wang, Dan Peng, Furui Liu, Zhenguo Li, Zhitang Chen, Jiansheng\n  Yang", "title": "Decoder-free Robustness Disentanglement without (Additional) Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial Training (AT) is proposed to alleviate the adversarial\nvulnerability of machine learning models by extracting only robust features\nfrom the input, which, however, inevitably leads to severe accuracy reduction\nas it discards the non-robust yet useful features. This motivates us to\npreserve both robust and non-robust features and separate them with\ndisentangled representation learning. Our proposed Adversarial Asymmetric\nTraining (AAT) algorithm can reliably disentangle robust and non-robust\nrepresentations without additional supervision on robustness. Empirical results\nshow our method does not only successfully preserve accuracy by combining two\nrepresentations, but also achieve much better disentanglement than previous\nwork.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 19:51:40 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Wang", "Yifei", ""], ["Peng", "Dan", ""], ["Liu", "Furui", ""], ["Li", "Zhenguo", ""], ["Chen", "Zhitang", ""], ["Yang", "Jiansheng", ""]]}, {"id": "2007.01357", "submitter": "Yongchan Kwon", "authors": "Yongchan Kwon, Manuel A. Rivas, James Zou", "title": "Efficient computation and analysis of distributional Shapley values", "comments": "24 pages, accepted for AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributional data Shapley value (DShapley) has recently been proposed as a\nprincipled framework to quantify the contribution of individual datum in\nmachine learning. DShapley develops the foundational game theory concept of\nShapley values into a statistical framework and can be applied to identify data\npoints that are useful (or harmful) to a learning algorithm. Estimating\nDShapley is computationally expensive, however, and this can be a major\nchallenge to using it in practice. Moreover, there has been little mathematical\nanalyses of how this value depends on data characteristics. In this paper, we\nderive the first analytic expressions for DShapley for the canonical problems\nof linear regression, binary classification, and non-parametric density\nestimation. These analytic forms provide new algorithms to estimate DShapley\nthat are several orders of magnitude faster than previous state-of-the-art\nmethods. Furthermore, our formulas are directly interpretable and provide\nquantitative insights into how the value varies for different types of data. We\ndemonstrate the practical efficacy of our approach on multiple real and\nsynthetic datasets.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 19:51:54 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2020 10:30:57 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2021 21:38:22 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Kwon", "Yongchan", ""], ["Rivas", "Manuel A.", ""], ["Zou", "James", ""]]}, {"id": "2007.01380", "submitter": "Charalampos Andriotis", "authors": "C.P. Andriotis, K.G. Papakonstantinou", "title": "Deep reinforcement learning driven inspection and maintenance planning\n  under incomplete information and constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determination of inspection and maintenance policies for minimizing long-term\nrisks and costs in deteriorating engineering environments constitutes a complex\noptimization problem. Major computational challenges include the (i) curse of\ndimensionality, due to exponential scaling of state/action set cardinalities\nwith the number of components; (ii) curse of history, related to exponentially\ngrowing decision-trees with the number of decision-steps; (iii) presence of\nstate uncertainties, induced by inherent environment stochasticity and\nvariability of inspection/monitoring measurements; (iv) presence of\nconstraints, pertaining to stochastic long-term limitations, due to resource\nscarcity and other infeasible/undesirable system responses. In this work, these\nchallenges are addressed within a joint framework of constrained Partially\nObservable Markov Decision Processes (POMDP) and multi-agent Deep Reinforcement\nLearning (DRL). POMDPs optimally tackle (ii)-(iii), combining stochastic\ndynamic programming with Bayesian inference principles. Multi-agent DRL\naddresses (i), through deep function parametrizations and decentralized control\nassumptions. Challenge (iv) is herein handled through proper state augmentation\nand Lagrangian relaxation, with emphasis on life-cycle risk-based constraints\nand budget limitations. The underlying algorithmic steps are provided, and the\nproposed framework is found to outperform well-established policy baselines and\nfacilitate adept prescription of inspection and intervention actions, in cases\nwhere decisions must be made in the most resource- and risk-aware manner.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 20:44:07 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Andriotis", "C. P.", ""], ["Papakonstantinou", "K. G.", ""]]}, {"id": "2007.01386", "submitter": "James Davis", "authors": "Jim Davis", "title": "Posterior Model Adaptation With Updated Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification approaches based on the direct estimation and analysis of\nposterior probabilities will degrade if the original class priors begin to\nchange. We prove that a unique (up to scale) solution is possible to recover\nthe data likelihoods for a test example from its original class posteriors and\ndataset priors. Given the recovered likelihoods and a set of new priors, the\nposteriors can be re-computed using Bayes' Rule to reflect the influence of the\nnew priors. The method is simple to compute and allows a dynamic update of the\noriginal posteriors.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 21:07:05 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Davis", "Jim", ""]]}, {"id": "2007.01388", "submitter": "Farshid Varno", "authors": "Farshid Varno and Lucas May Petry and Lisa Di Jorio and Stan Matwin", "title": "Learn Faster and Forget Slower via Fast and Stable Task Adaptation", "comments": "52 pages, 15 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Training Deep Neural Networks (DNNs) is still highly time-consuming and\ncompute-intensive. It has been shown that adapting a pretrained model may\nsignificantly accelerate this process. With a focus on classification, we show\nthat current fine-tuning techniques make the pretrained models catastrophically\nforget the transferred knowledge even before anything about the new task is\nlearned. Such rapid knowledge loss undermines the merits of transfer learning\nand may result in a much slower convergence rate compared to when the maximum\namount of knowledge is exploited. We investigate the source of this problem\nfrom different perspectives and to alleviate it, introduce Fast And Stable\nTask-adaptation (FAST), an easy to apply fine-tuning algorithm. The paper\nprovides a novel geometric perspective on how the loss landscape of source and\ntarget tasks are linked in different transfer learning strategies. We\nempirically show that compared to prevailing fine-tuning practices, FAST learns\nthe target task faster and forgets the source task slower.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 21:13:55 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2020 16:01:50 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Varno", "Farshid", ""], ["Petry", "Lucas May", ""], ["Di Jorio", "Lisa", ""], ["Matwin", "Stan", ""]]}, {"id": "2007.01394", "submitter": "Ainesh Bakshi", "authors": "Ainesh Bakshi and Adarsh Prasad", "title": "Robust Linear Regression: Optimal Rates in Polynomial Time", "comments": "Updated exposition of sum-of-squares background and preliminaries", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain robust and computationally efficient estimators for learning\nseveral linear models that achieve statistically optimal convergence rate under\nminimal distributional assumptions. Concretely, we assume our data is drawn\nfrom a $k$-hypercontractive distribution and an $\\epsilon$-fraction is\nadversarially corrupted. We then describe an estimator that converges to the\noptimal least-squares minimizer for the true distribution at a rate\nproportional to $\\epsilon^{2-2/k}$, when the noise is independent of the\ncovariates. We note that no such estimator was known prior to our work, even\nwith access to unbounded computation. The rate we achieve is\ninformation-theoretically optimal and thus we resolve the main open question in\nKlivans, Kothari and Meka [COLT'18].\n  Our key insight is to identify an analytic condition that serves as a\npolynomial relaxation of independence of random variables. In particular, we\nshow that when the moments of the noise and covariates are\nnegatively-correlated, we obtain the same rate as independent noise. Further,\nwhen the condition is not satisfied, we obtain a rate proportional to\n$\\epsilon^{2-4/k}$, and again match the information-theoretic lower bound. Our\ncentral technical contribution is to algorithmically exploit independence of\nrandom variables in the \"sum-of-squares\" framework by formulating it as the\naforementioned polynomial inequality.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 17:22:16 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 20:53:11 GMT"}, {"version": "v3", "created": "Sat, 14 Nov 2020 00:13:24 GMT"}, {"version": "v4", "created": "Fri, 4 Dec 2020 15:18:28 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Bakshi", "Ainesh", ""], ["Prasad", "Adarsh", ""]]}, {"id": "2007.01397", "submitter": "Vitaliy Chiley", "authors": "Abhinav Venigalla and Atli Kosson and Vitaliy Chiley and Urs K\\\"oster", "title": "Adaptive Braking for Mitigating Gradient Delay", "comments": "In Beyond First Order Methods in ML Systems workshop at the 37th\n  International Conference on Machine Learning, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network training is commonly accelerated by using multiple\nsynchronized workers to compute gradient updates in parallel. Asynchronous\nmethods remove synchronization overheads and improve hardware utilization at\nthe cost of introducing gradient delay, which impedes optimization and can lead\nto lower final model performance. We introduce Adaptive Braking (AB), a\nmodification for momentum-based optimizers that mitigates the effects of\ngradient delay. AB dynamically scales the gradient based on the alignment of\nthe gradient and the velocity. This can dampen oscillations along high\ncurvature directions of the loss surface, stabilizing and accelerating\nasynchronous training. We show that applying AB on top of SGD with momentum\nenables training ResNets on CIFAR-10 and ImageNet-1k with delays $D \\geq$ 32\nupdate steps with minimal drop in final test accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 21:26:27 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 17:12:25 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Venigalla", "Abhinav", ""], ["Kosson", "Atli", ""], ["Chiley", "Vitaliy", ""], ["K\u00f6ster", "Urs", ""]]}, {"id": "2007.01419", "submitter": "Yimeng Min", "authors": "Yimeng Min", "title": "Persistent Neurons", "comments": "add some new results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks (NN)-based learning algorithms are strongly affected by the\nchoices of initialization and data distribution. Different optimization\nstrategies have been proposed for improving the learning trajectory and finding\na better optima. However, designing improved optimization strategies is a\ndifficult task under the conventional landscape view. Here, we propose\npersistent neurons, a trajectory-based strategy that optimizes the learning\ntask using information from previous converged solutions. More precisely, we\nutilize the end of trajectories and let the parameters explore new landscapes\nby penalizing the model from converging to the previous solutions under the\nsame initialization. Persistent neurons can be regarded as a stochastic\ngradient method with informed bias where individual updates are corrupted by\ndeterministic error terms. Specifically, we show that persistent neurons, under\ncertain data distribution, is able to converge to more optimal solutions while\ninitializations under popular framework find bad local minima. We further\ndemonstrate that persistent neurons helps improve the model's performance under\nboth good and poor initializations. We evaluate the full and partial persistent\nmodel and show it can be used to boost the performance on a range of NN\nstructures, such as AlexNet and residual neural network (ResNet).\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 22:36:49 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 09:16:24 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Min", "Yimeng", ""]]}, {"id": "2007.01420", "submitter": "Mohannad Elhamod", "authors": "Mohannad Elhamod, Jie Bu, Christopher Singh, Matthew Redell, Abantika\n  Ghosh, Viktor Podolskiy, Wei-Cheng Lee, Anuj Karpatne", "title": "CoPhy-PGNN: Learning Physics-guided Neural Networks with Competing Loss\n  Functions for Solving Eigenvalue Problems", "comments": "Submitted to SDM 2021, preprint version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.comp-ph quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physics-guided Neural Networks (PGNNs) represent an emerging class of neural\nnetworks that are trained using physics-guided (PG) loss functions (capturing\nviolations in network outputs with known physics), along with the supervision\ncontained in data. Existing work in PGNNs have demonstrated the efficacy of\nadding single PG loss functions in the neural network objectives, using\nconstant trade-off parameters, to ensure better generalizability. However, in\nthe presence of multiple physics loss functions with competing gradient\ndirections, there is a need to adaptively tune the contribution of competing PG\nloss functions during the course of training to arrive at generalizable\nsolutions. We demonstrate the presence of competing PG losses in the generic\nneural network problem of solving for the lowest (or highest) eigenvector of a\nphysics-based eigenvalue equation, common to many scientific problems. We\npresent a novel approach to handle competing PG losses and demonstrate its\nefficacy in learning generalizable solutions in two motivating applications of\nquantum mechanics and electromagnetic propagation. All the code and data used\nin this work is available at https://github.com/jayroxis/Cophy-PGNN.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 22:39:02 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 01:06:26 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 15:24:01 GMT"}, {"version": "v4", "created": "Thu, 5 Nov 2020 17:16:04 GMT"}, {"version": "v5", "created": "Thu, 17 Jun 2021 19:12:59 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Elhamod", "Mohannad", ""], ["Bu", "Jie", ""], ["Singh", "Christopher", ""], ["Redell", "Matthew", ""], ["Ghosh", "Abantika", ""], ["Podolskiy", "Viktor", ""], ["Lee", "Wei-Cheng", ""], ["Karpatne", "Anuj", ""]]}, {"id": "2007.01423", "submitter": "M. Maruf", "authors": "M. Maruf and Anuj Karpatne", "title": "Maximizing Cohesion and Separation in Graph Representation Learning: A\n  Distance-aware Negative Sampling Approach", "comments": "14 pages, 9 figures, 3 tables, full length version with appendix;\n  Published in Proceedings of the 2021 SIAM International Conference on Data\n  Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of unsupervised graph representation learning (GRL) is to learn\na low-dimensional space of node embeddings that reflect the structure of a\ngiven unlabeled graph. Existing algorithms for this task rely on negative\nsampling objectives that maximize the similarity in node embeddings at nearby\nnodes (referred to as \"cohesion\") by maintaining positive and negative corpus\nof node pairs. While positive samples are drawn from node pairs that co-occur\nin short random walks, conventional approaches construct negative corpus by\nuniformly sampling random pairs, thus ignoring valuable information about\nstructural dissimilarity among distant node pairs (referred to as\n\"separation\"). In this paper, we present a novel Distance-aware Negative\nSampling (DNS) which maximizes the separation of distant node-pairs while\nmaximizing cohesion at nearby node-pairs by setting the negative sampling\nprobability proportional to the pair-wise shortest distances. Our approach can\nbe used in conjunction with any GRL algorithm and we demonstrate the efficacy\nof our approach over baseline negative sampling methods over downstream node\nclassification tasks on a number of benchmark datasets and GRL algorithms. All\nour codes and datasets are available at\nhttps://github.com/Distance-awareNS/DNS/.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 22:40:38 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2021 08:27:06 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Maruf", "M.", ""], ["Karpatne", "Anuj", ""]]}, {"id": "2007.01429", "submitter": "Dawei Li", "authors": "Ruoyu Sun, Dawei Li, Shiyu Liang, Tian Ding, R Srikant", "title": "The Global Landscape of Neural Networks: An Overview", "comments": "16 pages. 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major concerns for neural network training is that the\nnon-convexity of the associated loss functions may cause bad landscape. The\nrecent success of neural networks suggests that their loss landscape is not too\nbad, but what specific results do we know about the landscape? In this article,\nwe review recent findings and results on the global landscape of neural\nnetworks. First, we point out that wide neural nets may have sub-optimal local\nminima under certain assumptions. Second, we discuss a few rigorous results on\nthe geometric properties of wide networks such as \"no bad basin\", and some\nmodifications that eliminate sub-optimal local minima and/or decreasing paths\nto infinity. Third, we discuss visualization and empirical explorations of the\nlandscape for practical neural nets. Finally, we briefly discuss some\nconvergence results and their relation to landscape results.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 22:50:20 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Sun", "Ruoyu", ""], ["Li", "Dawei", ""], ["Liang", "Shiyu", ""], ["Ding", "Tian", ""], ["Srikant", "R", ""]]}, {"id": "2007.01434", "submitter": "David Lopez-Paz", "authors": "Ishaan Gulrajani, David Lopez-Paz", "title": "In Search of Lost Domain Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of domain generalization algorithms is to predict well on\ndistributions different from those seen during training. While a myriad of\ndomain generalization algorithms exist, inconsistencies in experimental\nconditions -- datasets, architectures, and model selection criteria -- render\nfair and realistic comparisons difficult. In this paper, we are interested in\nunderstanding how useful domain generalization algorithms are in realistic\nsettings. As a first step, we realize that model selection is non-trivial for\ndomain generalization tasks. Contrary to prior work, we argue that domain\ngeneralization algorithms without a model selection strategy should be regarded\nas incomplete. Next, we implement DomainBed, a testbed for domain\ngeneralization including seven multi-domain datasets, nine baseline algorithms,\nand three model selection criteria. We conduct extensive experiments using\nDomainBed and find that, when carefully implemented, empirical risk\nminimization shows state-of-the-art performance across all datasets. Looking\nforward, we hope that the release of DomainBed, along with contributions from\nfellow researchers, will streamline reproducible and rigorous research in\ndomain generalization.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 23:08:07 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Gulrajani", "Ishaan", ""], ["Lopez-Paz", "David", ""]]}, {"id": "2007.01442", "submitter": "Ronshee Chawla", "authors": "Ronshee Chawla, Abishek Sankararaman and Sanjay Shakkottai", "title": "Multi-Agent Low-Dimensional Linear Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a multi-agent stochastic linear bandit with side information,\nparameterized by an unknown vector $\\theta^* \\in \\mathbb{R}^d$. The side\ninformation consists of a finite collection of low-dimensional subspaces, one\nof which contains $\\theta^*$. In our setting, agents can collaborate to reduce\nregret by sending recommendations across a communication graph connecting them.\nWe present a novel decentralized algorithm, where agents communicate subspace\nindices with each other and each agent plays a projected variant of LinUCB on\nthe corresponding (low-dimensional) subspace. By distributing the search for\nthe optimal subspace across users and learning of the unknown vector by each\nagent in the corresponding low-dimensional subspace, we show that the per-agent\nfinite-time regret is much smaller than the case when agents do not\ncommunicate. We finally complement these results through simulations.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 23:54:56 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 01:10:50 GMT"}, {"version": "v3", "created": "Sun, 16 May 2021 04:55:24 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Chawla", "Ronshee", ""], ["Sankararaman", "Abishek", ""], ["Shakkottai", "Sanjay", ""]]}, {"id": "2007.01447", "submitter": "Jinshan Xu", "authors": "Jinshan Xu, Zhenqin Chen, Yanpei Lu, Xi Yang, Alain Pumir", "title": "Improved Preterm Prediction Based on Optimized Synthetic Sampling of EHG\n  Signal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preterm labor is the leading cause of neonatal morbidity and mortality and\nhas attracted research efforts from many scientific areas. The\ninter-relationship between uterine contraction and the underlying electrical\nactivities makes uterine electrohysterogram (EHG) a promising direction for\npreterm detection and prediction. Due the scarcity of EHG signals, especially\nthose of preterm patients, synthetic algorithms are applied to create\nartificial samples of preterm type in order to remove prediction bias towards\nterm, at the expense of a reduction of the feature effectiveness in\nmachine-learning based automatic preterm detecting. To address such problem, we\nquantify the effect of synthetic samples (balance coefficient) on features'\neffectiveness, and form a general performance metric by utilizing multiple\nfeature scores with relevant weights that describe their contributions to class\nseparation. Combined with the activation/inactivation functions that\ncharacterizes the effect of the abundance of training samples in term and\npreterm prediction precision, we obtain an optimal sample balance coefficient\nthat compromise the effect of synthetic samples in removing bias towards the\nmajority and the side-effect of reducing features' importance. Substantial\nimprovement in prediction precision has been achieved through a set of\nnumerical tests on public available TPEHG database, and it verifies the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 01:12:31 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Xu", "Jinshan", ""], ["Chen", "Zhenqin", ""], ["Lu", "Yanpei", ""], ["Yang", "Xi", ""], ["Pumir", "Alain", ""]]}, {"id": "2007.01452", "submitter": "Cong Fang", "authors": "Cong Fang, Jason D. Lee, Pengkun Yang, Tong Zhang", "title": "Modeling from Features: a Mean-field Framework for Over-parameterized\n  Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new mean-field framework for over-parameterized deep\nneural networks (DNNs), which can be used to analyze neural network training.\nIn this framework, a DNN is represented by probability measures and functions\nover its features (that is, the function values of the hidden units over the\ntraining data) in the continuous limit, instead of the neural network\nparameters as most existing studies have done. This new representation\novercomes the degenerate situation where all the hidden units essentially have\nonly one meaningful hidden unit in each middle layer, and further leads to a\nsimpler representation of DNNs, for which the training objective can be\nreformulated as a convex optimization problem via suitable re-parameterization.\nMoreover, we construct a non-linear dynamics called neural feature flow, which\ncaptures the evolution of an over-parameterized DNN trained by Gradient\nDescent. We illustrate the framework via the standard DNN and the Residual\nNetwork (Res-Net) architectures. Furthermore, we show, for Res-Net, when the\nneural feature flow process converges, it reaches a global minimal solution\nunder suitable conditions. Our analysis leads to the first global convergence\nproof for over-parameterized neural network training with more than $3$ layers\nin the mean-field regime.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 01:37:16 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Fang", "Cong", ""], ["Lee", "Jason D.", ""], ["Yang", "Pengkun", ""], ["Zhang", "Tong", ""]]}, {"id": "2007.01458", "submitter": "Sangheum Hwang", "authors": "Jooyoung Moon, Jihyo Kim, Younghak Shin, Sangheum Hwang", "title": "Confidence-Aware Learning for Deep Neural Networks", "comments": "ICML 2020. The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the power of deep neural networks for a wide range of tasks, an\noverconfident prediction issue has limited their practical use in many\nsafety-critical applications. Many recent works have been proposed to mitigate\nthis issue, but most of them require either additional computational costs in\ntraining and/or inference phases or customized architectures to output\nconfidence estimates separately. In this paper, we propose a method of training\ndeep neural networks with a novel loss function, named Correctness Ranking\nLoss, which regularizes class probabilities explicitly to be better confidence\nestimates in terms of ordinal ranking according to confidence. The proposed\nmethod is easy to implement and can be applied to the existing architectures\nwithout any modification. Also, it has almost the same computational costs for\ntraining as conventional deep classifiers and outputs reliable predictions by a\nsingle inference. Extensive experimental results on classification benchmark\ndatasets indicate that the proposed method helps networks to produce\nwell-ranked confidence estimates. We also demonstrate that it is effective for\nthe tasks closely related to confidence estimation, out-of-distribution\ndetection and active learning.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 02:00:35 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 04:56:43 GMT"}, {"version": "v3", "created": "Thu, 13 Aug 2020 03:16:37 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Moon", "Jooyoung", ""], ["Kim", "Jihyo", ""], ["Shin", "Younghak", ""], ["Hwang", "Sangheum", ""]]}, {"id": "2007.01472", "submitter": "Zhihui Shao", "authors": "Zhihui Shao, and Jianyi Yang, and Shaolei Ren", "title": "Increasing Trustworthiness of Deep Neural Networks via Accuracy\n  Monitoring", "comments": "Accepted by the AISafety workshop co-located with IJCAI-PRICAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inference accuracy of deep neural networks (DNNs) is a crucial performance\nmetric, but can vary greatly in practice subject to actual test datasets and is\ntypically unknown due to the lack of ground truth labels. This has raised\nsignificant concerns with trustworthiness of DNNs, especially in\nsafety-critical applications. In this paper, we address trustworthiness of DNNs\nby using post-hoc processing to monitor the true inference accuracy on a user's\ndataset. Concretely, we propose a neural network-based accuracy monitor model,\nwhich only takes the deployed DNN's softmax probability output as its input and\ndirectly predicts if the DNN's prediction result is correct or not, thus\nleading to an estimate of the true inference accuracy. The accuracy monitor\nmodel can be pre-trained on a dataset relevant to the target application of\ninterest, and only needs to actively label a small portion (1% in our\nexperiments) of the user's dataset for model transfer. For estimation\nrobustness, we further employ an ensemble of monitor models based on the\nMonte-Carlo dropout method. We evaluate our approach on different deployed DNN\nmodels for image classification and traffic sign detection over multiple\ndatasets (including adversarial samples). The result shows that our accuracy\nmonitor model provides a close-to-true accuracy estimation and outperforms the\nexisting baseline methods.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 03:09:36 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Shao", "Zhihui", ""], ["Yang", "Jianyi", ""], ["Ren", "Shaolei", ""]]}, {"id": "2007.01480", "submitter": "Chih-Hsing Ho", "authors": "Chih-Hsing Ho, Shang-Ho (Lawrence) Tsai", "title": "RSAC: Regularized Subspace Approximation Classifier for Lightweight\n  Continuous Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous learning seeks to perform the learning on the data that arrives\nfrom time to time. While prior works have demonstrated several possible\nsolutions, these approaches require excessive training time as well as memory\nusage. This is impractical for applications where time and storage are\nconstrained, such as edge computing. In this work, a novel training algorithm,\nregularized subspace approximation classifier (RSAC), is proposed to achieve\nlightweight continuous learning. RSAC contains a feature reduction module and\nclassifier module with regularization. Extensive experiments show that RSAC is\nmore efficient than prior continuous learning works and outperforms these works\non various experimental settings.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 03:38:06 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Ho", "Chih-Hsing", "", "Lawrence"], ["Shang-Ho", "", "", "Lawrence"], ["Tsai", "", ""]]}, {"id": "2007.01486", "submitter": "Shibo Shen", "authors": "Shibo Shen, Rongpeng Li, Zhifeng Zhao, Honggang Zhang, Yugeng Zhou", "title": "Learning to Prune in Training via Dynamic Channel Propagation", "comments": "accepted by ICPR-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel network training mechanism called \"dynamic\nchannel propagation\" to prune the neural networks during the training period.\nIn particular, we pick up a specific group of channels in each convolutional\nlayer to participate in the forward propagation in training time according to\nthe significance level of channel, which is defined as channel utility. The\nutility values with respect to all selected channels are updated simultaneously\nwith the error back-propagation process and will adaptively change.\nFurthermore, when the training ends, channels with high utility values are\nretained whereas those with low utility values are discarded. Hence, our\nproposed scheme trains and prunes neural networks simultaneously. We\nempirically evaluate our novel training scheme on various representative\nbenchmark datasets and advanced convolutional neural network (CNN)\narchitectures, including VGGNet and ResNet. The experiment results verify the\nsuperior performance and robust effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 04:02:41 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Shen", "Shibo", ""], ["Li", "Rongpeng", ""], ["Zhao", "Zhifeng", ""], ["Zhang", "Honggang", ""], ["Zhou", "Yugeng", ""]]}, {"id": "2007.01488", "submitter": "Jianing Li", "authors": "Jianing Li, Yanyan Lan, Jiafeng Guo, Xueqi Cheng", "title": "On the Relation between Quality-Diversity Evaluation and\n  Distribution-Fitting Goal in Text Generation", "comments": "16 pages, 7 figures. ICML2020 Final Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of text generation models is to fit the underlying real probability\ndistribution of text. For performance evaluation, quality and diversity metrics\nare usually applied. However, it is still not clear to what extend can the\nquality-diversity evaluation reflect the distribution-fitting goal. In this\npaper, we try to reveal such relation in a theoretical approach. We prove that\nunder certain conditions, a linear combination of quality and diversity\nconstitutes a divergence metric between the generated distribution and the real\ndistribution. We also show that the commonly used BLEU/Self-BLEU metric pair\nfails to match any divergence metric, thus propose CR/NRR as a substitute for\nquality/diversity metric pair.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 04:06:59 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 03:37:59 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Li", "Jianing", ""], ["Lan", "Yanyan", ""], ["Guo", "Jiafeng", ""], ["Cheng", "Xueqi", ""]]}, {"id": "2007.01494", "submitter": "Andi Han", "authors": "Andi Han, Junbin Gao", "title": "Variance reduction for Riemannian non-convex optimization with batch\n  size adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variance reduction techniques are popular in accelerating gradient descent\nand stochastic gradient descent for optimization problems defined on both\nEuclidean space and Riemannian manifold. In this paper, we further improve on\nexisting variance reduction methods for non-convex Riemannian optimization,\nincluding R-SVRG and R-SRG/R-SPIDER with batch size adaptation. We show that\nthis strategy can achieve lower total complexities for optimizing both general\nnon-convex and gradient dominated functions under both finite-sum and online\nsettings. As a result, we also provide simpler convergence analysis for R-SVRG\nand improve complexity bounds for R-SRG under finite-sum setting. Specifically,\nwe prove that R-SRG achieves the same near-optimal complexity as R-SPIDER\nwithout requiring a small step size. Empirical experiments on a variety of\ntasks demonstrate effectiveness of proposed adaptive batch size scheme.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 04:34:39 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Han", "Andi", ""], ["Gao", "Junbin", ""]]}, {"id": "2007.01498", "submitter": "Yuqian Jiang", "authors": "Yuqian Jiang, Sudarshanan Bharadwaj, Bo Wu, Rishi Shah, Ufuk Topcu,\n  Peter Stone", "title": "Temporal-Logic-Based Reward Shaping for Continuing Learning Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In continuing tasks, average-reward reinforcement learning may be a more\nappropriate problem formulation than the more common discounted reward\nformulation. As usual, learning an optimal policy in this setting typically\nrequires a large amount of training experiences. Reward shaping is a common\napproach for incorporating domain knowledge into reinforcement learning in\norder to speed up convergence to an optimal policy. However, to the best of our\nknowledge, the theoretical properties of reward shaping have thus far only been\nestablished in the discounted setting. This paper presents the first reward\nshaping framework for average-reward learning and proves that, under standard\nassumptions, the optimal policy under the original reward function can be\nrecovered. In order to avoid the need for manual construction of the shaping\nfunction, we introduce a method for utilizing domain knowledge expressed as a\ntemporal logic formula. The formula is automatically translated to a shaping\nfunction that provides additional reward throughout the learning process. We\nevaluate the proposed method on three continuing tasks. In all cases, shaping\nspeeds up the average-reward learning rate without any reduction in the\nperformance of the learned policy compared to relevant baselines.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 05:06:57 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Jiang", "Yuqian", ""], ["Bharadwaj", "Sudarshanan", ""], ["Wu", "Bo", ""], ["Shah", "Rishi", ""], ["Topcu", "Ufuk", ""], ["Stone", "Peter", ""]]}, {"id": "2007.01500", "submitter": "Sapir Kaplan", "authors": "Sapir Kaplan and Raja Giryes", "title": "Self-supervised Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) has been used recently to achieve improved\nperformance in various tasks and most prominently in image classification. Yet,\ncurrent search strategies rely on large labeled datasets, which limit their\nusage in the case where only a smaller fraction of the data is annotated.\nSelf-supervised learning has shown great promise in training neural networks\nusing unlabeled data. In this work, we propose a self-supervised neural\narchitecture search (SSNAS) that allows finding novel network models without\nthe need for labeled data. We show that such a search leads to comparable\nresults to supervised training with a \"fully labeled\" NAS and that it can\nimprove the performance of self-supervised learning. Moreover, we demonstrate\nthe advantage of the proposed approach when the number of labels in the search\nis relatively small.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 05:09:30 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Kaplan", "Sapir", ""], ["Giryes", "Raja", ""]]}, {"id": "2007.01503", "submitter": "Yarema Boryshchak", "authors": "Yarema Boryshchak", "title": "Mathematical Perspective of Machine Learning", "comments": "9 pages 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We take a closer look at some theoretical challenges of Machine Learning as a\nfunction approximation, gradient descent as the default optimization algorithm,\nlimitations of fixed length and width networks and a different approach to RNNs\nfrom a mathematical perspective.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 05:26:02 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Boryshchak", "Yarema", ""]]}, {"id": "2007.01507", "submitter": "Yuting Liang", "authors": "Yuting Liang, Reza Samavi", "title": "Towards Robust Deep Learning with Ensemble Networks and Noisy Layers", "comments": "Accepted into AAAI RSEML 2021 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide an approach for deep learning that protects against\nadversarial examples in image classification-type networks. The approach relies\non two mechanisms:1) a mechanism that increases robustness at the expense of\naccuracy, and, 2) a mechanism that improves accuracy but does not always\nincrease robustness. We show that an approach combining the two mechanisms can\nprovide protection against adversarial examples while retaining accuracy. We\nformulate potential attacks on our approach with experimental results to\ndemonstrate its effectiveness. We also provide a robustness guarantee for our\napproach along with an interpretation for the guarantee.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 06:04:02 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 18:05:57 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Liang", "Yuting", ""], ["Samavi", "Reza", ""]]}, {"id": "2007.01516", "submitter": "Deepak Sharma", "authors": "Deepak Sharma, Audrey Durand, Marc-Andr\\'e Legault, Louis-Philippe\n  Lemieux Perreault, Audrey Lema\\c{c}on, Marie-Pierre Dub\\'e, Joelle Pineau", "title": "Deep interpretability for GWAS", "comments": "Accepted at ICML 2020 workshop on ML Interpretability for Scientific\n  Discovery", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.GN stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genome-Wide Association Studies are typically conducted using linear models\nto find genetic variants associated with common diseases. In these studies,\nassociation testing is done on a variant-by-variant basis, possibly missing out\non non-linear interaction effects between variants. Deep networks can be used\nto model these interactions, but they are difficult to train and interpret on\nlarge genetic datasets. We propose a method that uses the gradient based deep\ninterpretability technique named DeepLIFT to show that known diabetes genetic\nrisk factors can be identified using deep models along with possibly novel\nassociations.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 06:49:31 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Sharma", "Deepak", ""], ["Durand", "Audrey", ""], ["Legault", "Marc-Andr\u00e9", ""], ["Perreault", "Louis-Philippe Lemieux", ""], ["Lema\u00e7on", "Audrey", ""], ["Dub\u00e9", "Marie-Pierre", ""], ["Pineau", "Joelle", ""]]}, {"id": "2007.01522", "submitter": "Yasmeen George", "authors": "Yasmeen M. George, Suman Sedai, Bhavna J. Antony, Hiroshi Ishikawa,\n  Gadi Wollstein, Joel S. Schuman and Rahil Garnavi", "title": "Dueling Deep Q-Network for Unsupervised Inter-frame Eye Movement\n  Correction in Optical Coherence Tomography Volumes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In optical coherence tomography (OCT) volumes of retina, the sequential\nacquisition of the individual slices makes this modality prone to motion\nartifacts, misalignments between adjacent slices being the most noticeable. Any\ndistortion in OCT volumes can bias structural analysis and influence the\noutcome of longitudinal studies. On the other hand, presence of speckle noise\nthat is characteristic of this imaging modality, leads to inaccuracies when\ntraditional registration techniques are employed. Also, the lack of a\nwell-defined ground truth makes supervised deep-learning techniques ill-posed\nto tackle the problem. In this paper, we tackle these issues by using deep\nreinforcement learning to correct inter-frame movements in an unsupervised\nmanner. Specifically, we use dueling deep Q-network to train an artificial\nagent to find the optimal policy, i.e. a sequence of actions, that best\nimproves the alignment by maximizing the sum of reward signals. Instead of\nrelying on the ground-truth of transformation parameters to guide the rewarding\nsystem, for the first time, we use a combination of intensity based image\nsimilarity metrics. Further, to avoid the agent bias towards speckle noise, we\nensure the agent can see retinal layers as part of the interacting environment.\nFor quantitative evaluation, we simulate the eye movement artifacts by applying\n2D rigid transformations on individual B-scans. The proposed model achieves an\naverage of 0.985 and 0.914 for normalized mutual information and correlation\ncoefficient, respectively. We also compare our model with elastix intensity\nbased medical image registration approach, where significant improvement is\nachieved by our model for both noisy and denoised volumes.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 07:14:30 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["George", "Yasmeen M.", ""], ["Sedai", "Suman", ""], ["Antony", "Bhavna J.", ""], ["Ishikawa", "Hiroshi", ""], ["Wollstein", "Gadi", ""], ["Schuman", "Joel S.", ""], ["Garnavi", "Rahil", ""]]}, {"id": "2007.01547", "submitter": "Robin M. Schmidt", "authors": "Robin M. Schmidt, Frank Schneider, Philipp Hennig", "title": "Descending through a Crowded Valley -- Benchmarking Deep Learning\n  Optimizers", "comments": "Raw results: https://github.com/SirRob1997/Crowded-Valley---Results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choosing the optimizer is considered to be among the most crucial design\ndecisions in deep learning, and it is not an easy one. The growing literature\nnow lists hundreds of optimization methods. In the absence of clear theoretical\nguidance and conclusive empirical evidence, the decision is often made based on\nanecdotes. In this work, we aim to replace these anecdotes, if not with a\nconclusive ranking, then at least with evidence-backed heuristics. To do so, we\nperform an extensive, standardized benchmark of fifteen particularly popular\ndeep learning optimizers while giving a concise overview of the wide range of\npossible choices. Analyzing more than $50,000$ individual runs, we contribute\nthe following three points: (i) Optimizer performance varies greatly across\ntasks. (ii) We observe that evaluating multiple optimizers with default\nparameters works approximately as well as tuning the hyperparameters of a\nsingle, fixed optimizer. (iii) While we cannot discern an optimization method\nclearly dominating across all tested tasks, we identify a significantly reduced\nsubset of specific optimizers and parameter choices that generally lead to\ncompetitive results in our experiments: Adam remains a strong contender, with\nnewer methods failing to significantly and consistently outperform it. Our\nopen-sourced results are available as challenging and well-tuned baselines for\nmore meaningful evaluations of novel optimization methods without requiring any\nfurther computational efforts.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 08:19:36 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 10:12:48 GMT"}, {"version": "v3", "created": "Thu, 1 Oct 2020 13:27:59 GMT"}, {"version": "v4", "created": "Mon, 5 Oct 2020 17:21:01 GMT"}, {"version": "v5", "created": "Thu, 11 Feb 2021 18:17:58 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Schmidt", "Robin M.", ""], ["Schneider", "Frank", ""], ["Hennig", "Philipp", ""]]}, {"id": "2007.01570", "submitter": "Aleksandar Bojchevski", "authors": "Aleksandar Bojchevski, Johannes Klicpera, Bryan Perozzi, Amol Kapoor,\n  Martin Blais, Benedek R\\'ozemberczki, Michal Lukasik, Stephan G\\\"unnemann", "title": "Scaling Graph Neural Networks with Approximate PageRank", "comments": "Published as a Conference Paper at ACM SIGKDD 2020", "journal-ref": null, "doi": "10.1145/3394486.3403296", "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNNs) have emerged as a powerful approach for solving\nmany network mining tasks. However, learning on large graphs remains a\nchallenge - many recently proposed scalable GNN approaches rely on an expensive\nmessage-passing procedure to propagate information through the graph. We\npresent the PPRGo model which utilizes an efficient approximation of\ninformation diffusion in GNNs resulting in significant speed gains while\nmaintaining state-of-the-art prediction performance. In addition to being\nfaster, PPRGo is inherently scalable, and can be trivially parallelized for\nlarge datasets like those found in industry settings. We demonstrate that PPRGo\noutperforms baselines in both distributed and single-machine training\nenvironments on a number of commonly used academic graphs. To better analyze\nthe scalability of large-scale graph learning methods, we introduce a novel\nbenchmark graph with 12.4 million nodes, 173 million edges, and 2.8 million\nnode features. We show that training PPRGo from scratch and predicting labels\nfor all nodes in this graph takes under 2 minutes on a single machine, far\noutpacing other baselines on the same graph. We discuss the practical\napplication of PPRGo to solve large-scale node classification problems at\nGoogle.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 09:30:07 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Bojchevski", "Aleksandar", ""], ["Klicpera", "Johannes", ""], ["Perozzi", "Bryan", ""], ["Kapoor", "Amol", ""], ["Blais", "Martin", ""], ["R\u00f3zemberczki", "Benedek", ""], ["Lukasik", "Michal", ""], ["G\u00fcnnemann", "Stephan", ""]]}, {"id": "2007.01580", "submitter": "Amnon Geifman", "authors": "Amnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs,\n  Ronen Basri", "title": "On the Similarity between the Laplace and Neural Tangent Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent theoretical work has shown that massively overparameterized neural\nnetworks are equivalent to kernel regressors that use Neural Tangent\nKernels(NTK). Experiments show that these kernel methods perform similarly to\nreal neural networks. Here we show that NTK for fully connected networks is\nclosely related to the standard Laplace kernel. We show theoretically that for\nnormalized data on the hypersphere both kernels have the same eigenfunctions\nand their eigenvalues decay polynomially at the same rate, implying that their\nReproducing Kernel Hilbert Spaces (RKHS) include the same sets of functions.\nThis means that both kernels give rise to classes of functions with the same\nsmoothness properties. The two kernels differ for data off the hypersphere, but\nexperiments indicate that when data is properly normalized these differences\nare not significant. Finally, we provide experiments on real data comparing NTK\nand the Laplace kernel, along with a larger class of{\\gamma}-exponential\nkernels. We show that these perform almost identically. Our results suggest\nthat much insight about neural networks can be obtained from analysis of the\nwell-known Laplace kernel, which has a simple closed-form.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 09:48:23 GMT"}, {"version": "v2", "created": "Sat, 14 Nov 2020 10:45:23 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Geifman", "Amnon", ""], ["Yadav", "Abhay", ""], ["Kasten", "Yoni", ""], ["Galun", "Meirav", ""], ["Jacobs", "David", ""], ["Basri", "Ronen", ""]]}, {"id": "2007.01587", "submitter": "Dashan Gao", "authors": "Dashan Gao, Ben Tan, Ce Ju, Vincent W. Zheng and Qiang Yang", "title": "Privacy Threats Against Federated Matrix Factorization", "comments": "6 pages, 2 figures, 1 table, Accepted for Workshop on Federated\n  Learning for Data Privacy and Confidentiality in Conjunction with IJCAI 2020\n  (FL-IJCAI'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix Factorization has been very successful in practical recommendation\napplications and e-commerce. Due to data shortage and stringent regulations, it\ncan be hard to collect sufficient data to build performant recommender systems\nfor a single company. Federated learning provides the possibility to bridge the\ndata silos and build machine learning models without compromising privacy and\nsecurity. Participants sharing common users or items collaboratively build a\nmodel over data from all the participants. There have been some works exploring\nthe application of federated learning to recommender systems and the privacy\nissues in collaborative filtering systems. However, the privacy threats in\nfederated matrix factorization are not studied. In this paper, we categorize\nfederated matrix factorization into three types based on the partition of\nfeature space and analyze privacy threats against each type of federated matrix\nfactorization model. We also discuss privacy-preserving approaches. As far as\nwe are aware, this is the first study of privacy threats of the matrix\nfactorization method in the federated learning framework.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 09:58:52 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Gao", "Dashan", ""], ["Tan", "Ben", ""], ["Ju", "Ce", ""], ["Zheng", "Vincent W.", ""], ["Yang", "Qiang", ""]]}, {"id": "2007.01592", "submitter": "Muhammad Osama", "authors": "Muhammad Osama, Dave Zachariah, Petre Stoica", "title": "Prediction of Spatial Point Processes: Regularized Method with\n  Out-of-Sample Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A spatial point process can be characterized by an intensity function which\npredicts the number of events that occur across space. In this paper, we\ndevelop a method to infer predictive intensity intervals by learning a spatial\nmodel using a regularized criterion. We prove that the proposed method exhibits\nout-of-sample prediction performance guarantees which, unlike standard\nestimators, are valid even when the spatial model is misspecified. The method\nis demonstrated using synthetic as well as real spatial data.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 10:11:59 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Osama", "Muhammad", ""], ["Zachariah", "Dave", ""], ["Stoica", "Petre", ""]]}, {"id": "2007.01594", "submitter": "Ganqu Cui", "authors": "Ganqu Cui, Jie Zhou, Cheng Yang, Zhiyuan Liu", "title": "Adaptive Graph Encoder for Attributed Graph Embedding", "comments": "To appear in KDD 2020", "journal-ref": null, "doi": "10.1145/3394486.3403140", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attributed graph embedding, which learns vector representations from graph\ntopology and node features, is a challenging task for graph analysis. Recently,\nmethods based on graph convolutional networks (GCNs) have made great progress\non this task. However,existing GCN-based methods have three major drawbacks.\nFirstly,our experiments indicate that the entanglement of graph convolutional\nfilters and weight matrices will harm both the performance and robustness.\nSecondly, we show that graph convolutional filters in these methods reveal to\nbe special cases of generalized Laplacian smoothing filters, but they do not\npreserve optimal low-pass characteristics. Finally, the training objectives of\nexisting algorithms are usually recovering the adjacency matrix or feature\nmatrix, which are not always consistent with real-world applications. To\naddress these issues, we propose Adaptive Graph Encoder (AGE), a novel\nattributed graph embedding framework. AGE consists of two modules: (1) To\nbetter alleviate the high-frequency noises in the node features, AGE first\napplies a carefully-designed Laplacian smoothing filter. (2) AGE employs an\nadaptive encoder that iteratively strengthens the filtered features for better\nnode embeddings. We conduct experiments using four public benchmark datasets to\nvalidate AGE on node clustering and link prediction tasks. Experimental results\nshow that AGE consistently outperforms state-of-the-art graph embedding methods\nconsiderably on these tasks.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 10:20:34 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Cui", "Ganqu", ""], ["Zhou", "Jie", ""], ["Yang", "Cheng", ""], ["Liu", "Zhiyuan", ""]]}, {"id": "2007.01612", "submitter": "Julia Olkhovskaya", "authors": "Gergely Neu and Julia Olkhovskaya", "title": "Online learning in MDPs with linear function approximation and bandit\n  feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an online learning problem where the learner interacts with a\nMarkov decision process in a sequence of episodes, where the reward function is\nallowed to change between episodes in an adversarial manner and the learner\nonly gets to observe the rewards associated with its actions. We allow the\nstate space to be arbitrarily large, but we assume that all action-value\nfunctions can be represented as linear functions in terms of a known\nlow-dimensional feature map, and that the learner has access to a simulator of\nthe environment that allows generating trajectories from the true MDP dynamics.\nOur main contribution is developing a computationally efficient algorithm that\nwe call MDP-LinExp3, and prove that its regret is bounded by\n$\\widetilde{\\mathcal{O}}\\big(H^2 T^{2/3} (dK)^{1/3}\\big)$, where $T$ is the\nnumber of episodes, $H$ is the number of steps in each episode, $K$ is the\nnumber of actions, and $d$ is the dimension of the feature map. We also show\nthat the regret can be improved to $\\widetilde{\\mathcal{O}}\\big(H^2\n\\sqrt{TdK}\\big)$ under much stronger assumptions on the MDP dynamics. To our\nknowledge, MDP-LinExp3 is the first provably efficient algorithm for this\nproblem setting.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 11:06:38 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 19:28:35 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Neu", "Gergely", ""], ["Olkhovskaya", "Julia", ""]]}, {"id": "2007.01620", "submitter": "Immanuel Bayer", "authors": "Immanuel Bayer and Anastasios Zouzias", "title": "Team voyTECH: User Activity Modeling with Boosting Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our winning solution for the ECML-PKDD ChAT Discovery\nChallenge 2020. We show that whether or not a Twitch user has subscribed to a\nchannel can be well predicted by modeling user activity with boosting trees. We\nintroduce the connection between target-encodings and boosting trees in the\ncontext of high cardinality categoricals and find that modeling user activity\nis more powerful then direct modeling of content when encoded properly and\ncombined with a suitable optimization approach.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 11:29:58 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 14:54:24 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Bayer", "Immanuel", ""], ["Zouzias", "Anastasios", ""]]}, {"id": "2007.01623", "submitter": "Oleg Szehr", "authors": "Loris Cannelli, Giuseppe Nuti, Marzio Sala, Oleg Szehr", "title": "Hedging using reinforcement learning: Contextual $k$-Armed Bandit versus\n  $Q$-learning", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-fin.CP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The construction of replication strategies for contingent claims in the\npresence of risk and market friction is a key problem of financial engineering.\nIn real markets, continuous replication, such as in the model of Black, Scholes\nand Merton, is not only unrealistic but it is also undesirable due to high\ntransaction costs. Over the last decades stochastic optimal-control methods\nhave been developed to balance between effective replication and losses. More\nrecently, with the rise of artificial intelligence, temporal-difference\nReinforcement Learning, in particular variations of $Q$-learning in conjunction\nwith Deep Neural Networks, have attracted significant interest. From a\npractical point of view, however, such methods are often relatively sample\ninefficient, hard to train and lack performance guarantees. This motivates the\ninvestigation of a stable benchmark algorithm for hedging. In this article, the\nhedging problem is viewed as an instance of a risk-averse contextual $k$-armed\nbandit problem, for which a large body of theoretical results and well-studied\nalgorithms are available. We find that the $k$-armed bandit model naturally\nfits to the $P\\&L$ formulation of hedging, providing for a more accurate and\nsample efficient approach than $Q$-learning and reducing to the Black-Scholes\nmodel in the absence of transaction costs and risks.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 11:34:10 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Cannelli", "Loris", ""], ["Nuti", "Giuseppe", ""], ["Sala", "Marzio", ""], ["Szehr", "Oleg", ""]]}, {"id": "2007.01627", "submitter": "Thomas Moreau", "authors": "Marine Le Morvan (PARIETAL, IJCLab), Julie Josse (CMAP, XPOP), Thomas\n  Moreau (PARIETAL), Erwan Scornet (CMAP), Ga\\\"el Varoquaux (PARIETAL, MILA)", "title": "NeuMiss networks: differentiable programming for supervised learning\n  with missing values", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 33, Dec 2020,\n  Vancouver, Canada", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of missing values makes supervised learning much more\nchallenging. Indeed, previous work has shown that even when the response is a\nlinear function of the complete data, the optimal predictor is a complex\nfunction of the observed entries and the missingness indicator. As a result,\nthe computational or sample complexities of consistent approaches depend on the\nnumber of missing patterns, which can be exponential in the number of\ndimensions. In this work, we derive the analytical form of the optimal\npredictor under a linearity assumption and various missing data mechanisms\nincluding Missing at Random (MAR) and self-masking (Missing Not At Random).\nBased on a Neumann-series approximation of the optimal predictor, we propose a\nnew principled architecture, named NeuMiss networks. Their originality and\nstrength come from the use of a new type of non-linearity: the multiplication\nby the missingness indicator. We provide an upper bound on the Bayes risk of\nNeuMiss networks, and show that they have good predictive accuracy with both a\nnumber of parameters and a computational complexity independent of the number\nof missing data patterns. As a result they scale well to problems with many\nfeatures, and remain statistically efficient for medium-sized samples.\nMoreover, we show that, contrary to procedures using EM or imputation, they are\nrobust to the missing data mechanism, including difficult MNAR settings such as\nself-masking.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 11:42:25 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 12:29:36 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2020 14:47:18 GMT"}, {"version": "v4", "created": "Wed, 4 Nov 2020 15:39:04 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Morvan", "Marine Le", "", "PARIETAL, IJCLab"], ["Josse", "Julie", "", "CMAP, XPOP"], ["Moreau", "Thomas", "", "PARIETAL"], ["Scornet", "Erwan", "", "CMAP"], ["Varoquaux", "Ga\u00ebl", "", "PARIETAL, MILA"]]}, {"id": "2007.01636", "submitter": "Marinus Lagerwerf", "authors": "Marinus J. Lagerwerf, Allard A. Hendriksen, Jan-Willem Buurlage and K.\n  Joost Batenburg", "title": "Noise2Filter: fast, self-supervised learning and real-time\n  reconstruction for 3D Computed Tomography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At X-ray beamlines of synchrotron light sources, the achievable\ntime-resolution for 3D tomographic imaging of the interior of an object has\nbeen reduced to a fraction of a second, enabling rapidly changing structures to\nbe examined. The associated data acquisition rates require sizable\ncomputational resources for reconstruction. Therefore, full 3D reconstruction\nof the object is usually performed after the scan has completed. Quasi-3D\nreconstruction -- where several interactive 2D slices are computed instead of a\n3D volume -- has been shown to be significantly more efficient, and can enable\nthe real-time reconstruction and visualization of the interior. However,\nquasi-3D reconstruction relies on filtered backprojection type algorithms,\nwhich are typically sensitive to measurement noise. To overcome this issue, we\npropose Noise2Filter, a learned filter method that can be trained using only\nthe measured data, and does not require any additional training data. This\nmethod combines quasi-3D reconstruction, learned filters, and self-supervised\nlearning to derive a tomographic reconstruction method that can be trained in\nunder a minute and evaluated in real-time. We show limited loss of accuracy\ncompared to training with additional training data, and improved accuracy\ncompared to standard filter-based methods.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 12:12:10 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Lagerwerf", "Marinus J.", ""], ["Hendriksen", "Allard A.", ""], ["Buurlage", "Jan-Willem", ""], ["Batenburg", "K. Joost", ""]]}, {"id": "2007.01659", "submitter": "Takahiro Mimori", "authors": "Takahiro Mimori, Keiko Sasada, Hirotaka Matsui, Issei Sato", "title": "Diagnostic Uncertainty Calibration: Towards Reliable Machine Predictions\n  in Medical Domain", "comments": "31 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an evaluation framework for class probability estimates (CPEs) in\nthe presence of label uncertainty, which is commonly observed as diagnosis\ndisagreement between experts in the medical domain. We also formalize\nevaluation metrics for higher-order statistics, including inter-rater\ndisagreement, to assess predictions on label uncertainty. Moreover, we propose\na novel post-hoc method called $alpha$-calibration, that equips neural network\nclassifiers with calibrated distributions over CPEs. Using synthetic\nexperiments and a large-scale medical imaging application, we show that our\napproach significantly enhances the reliability of uncertainty estimates:\ndisagreement probabilities and posterior CPEs.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 12:54:08 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 02:07:20 GMT"}, {"version": "v3", "created": "Mon, 25 Jan 2021 04:40:25 GMT"}, {"version": "v4", "created": "Mon, 22 Mar 2021 06:23:53 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Mimori", "Takahiro", ""], ["Sasada", "Keiko", ""], ["Matsui", "Hirotaka", ""], ["Sato", "Issei", ""]]}, {"id": "2007.01669", "submitter": "Yuya Yoshikawa", "authors": "Yuya Yoshikawa, Tomoharu Iwata", "title": "Gaussian Process Regression with Local Explanation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process regression (GPR) is a fundamental model used in machine\nlearning. Owing to its accurate prediction with uncertainty and versatility in\nhandling various data structures via kernels, GPR has been successfully used in\nvarious applications. However, in GPR, how the features of an input contribute\nto its prediction cannot be interpreted. Herein, we propose GPR with local\nexplanation, which reveals the feature contributions to the prediction of each\nsample, while maintaining the predictive performance of GPR. In the proposed\nmodel, both the prediction and explanation for each sample are performed using\nan easy-to-interpret locally linear model. The weight vector of the locally\nlinear model is assumed to be generated from multivariate Gaussian process\npriors. The hyperparameters of the proposed models are estimated by maximizing\nthe marginal likelihood. For a new test sample, the proposed model can predict\nthe values of its target variable and weight vector, as well as their\nuncertainties, in a closed form. Experimental results on various benchmark\ndatasets verify that the proposed model can achieve predictive performance\ncomparable to those of GPR and superior to that of existing interpretable\nmodels, and can achieve higher interpretability than them, both quantitatively\nand qualitatively.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 13:22:24 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 13:23:07 GMT"}, {"version": "v3", "created": "Wed, 2 Dec 2020 10:13:54 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Yoshikawa", "Yuya", ""], ["Iwata", "Tomoharu", ""]]}, {"id": "2007.01671", "submitter": "Youssef Dawoud", "authors": "Youssef Dawoud, Julia Hornauer, Gustavo Carneiro, and Vasileios\n  Belagiannis", "title": "Few-Shot Microscopy Image Cell Segmentation", "comments": "16 pages, 4 figures, Accepted by ECML-PKDD 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic cell segmentation in microscopy images works well with the support\nof deep neural networks trained with full supervision. Collecting and\nannotating images, though, is not a sustainable solution for every new\nmicroscopy database and cell type. Instead, we assume that we can access a\nplethora of annotated image data sets from different domains (sources) and a\nlimited number of annotated image data sets from the domain of interest\n(target), where each domain denotes not only different image appearance but\nalso a different type of cell segmentation problem. We pose this problem as\nmeta-learning where the goal is to learn a generic and adaptable few-shot\nlearning model from the available source domain data sets and cell segmentation\ntasks. The model can be afterwards fine-tuned on the few annotated images of\nthe target domain that contains different image appearance and different cell\ntype. In our meta-learning training, we propose the combination of three\nobjective functions to segment the cells, move the segmentation results away\nfrom the classification boundary using cross-domain tasks, and learn an\ninvariant representation between tasks of the source domains. Our experiments\non five public databases show promising results from 1- to 10-shot\nmeta-learning using standard segmentation neural network architectures.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 12:12:10 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Dawoud", "Youssef", ""], ["Hornauer", "Julia", ""], ["Carneiro", "Gustavo", ""], ["Belagiannis", "Vasileios", ""]]}, {"id": "2007.01675", "submitter": "Michael Chappell", "authors": "Michael A. Chappell, Martin S. Craig, Mark W. Woolrich", "title": "Stochastic Variational Bayesian Inference for a Nonlinear Forward Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Variational Bayes (VB) has been used to facilitate the calculation of the\nposterior distribution in the context of Bayesian inference of the parameters\nof nonlinear models from data. Previously an analytical formulation of VB has\nbeen derived for nonlinear model inference on data with additive gaussian noise\nas an alternative to nonlinear least squares. Here a stochastic solution is\nderived that avoids some of the approximations required of the analytical\nformulation, offering a solution that can be more flexibly deployed for\nnonlinear model inference problems. The stochastic VB solution was used for\ninference on a biexponential toy case and the algorithmic parameter space\nexplored, before being deployed on real data from a magnetic resonance imaging\nstudy of perfusion. The new method was found to achieve comparable parameter\nrecovery to the analytic solution and be competitive in terms of computational\nspeed despite being reliant on sampling.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 13:30:50 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Chappell", "Michael A.", ""], ["Craig", "Martin S.", ""], ["Woolrich", "Mark W.", ""]]}, {"id": "2007.01720", "submitter": "Ronald Seoh", "authors": "Ronald Seoh", "title": "Qualitative Analysis of Monte Carlo Dropout", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this report, we present qualitative analysis of Monte Carlo (MC) dropout\nmethod for measuring model uncertainty in neural network (NN) models. We first\nconsider the sources of uncertainty in NNs, and briefly review Bayesian Neural\nNetworks (BNN), the group of Bayesian approaches to tackle uncertainties in\nNNs. After presenting mathematical formulation of MC dropout, we proceed to\nsuggesting potential benefits and associated costs for using MC dropout in\ntypical NN models, with the results from our experiments.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 14:40:56 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Seoh", "Ronald", ""]]}, {"id": "2007.01754", "submitter": "S\\'ebastien Lachapelle", "authors": "Philippe Brouillard, S\\'ebastien Lachapelle, Alexandre Lacoste, Simon\n  Lacoste-Julien, Alexandre Drouin", "title": "Differentiable Causal Discovery from Interventional Data", "comments": "Appears in: Advances in Neural Information Processing Systems 34\n  (NeurIPS 2020). 46 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a causal directed acyclic graph from data is a challenging task that\ninvolves solving a combinatorial problem for which the solution is not always\nidentifiable. A new line of work reformulates this problem as a continuous\nconstrained optimization one, which is solved via the augmented Lagrangian\nmethod. However, most methods based on this idea do not make use of\ninterventional data, which can significantly alleviate identifiability issues.\nThis work constitutes a new step in this direction by proposing a\ntheoretically-grounded method based on neural networks that can leverage\ninterventional data. We illustrate the flexibility of the\ncontinuous-constrained framework by taking advantage of expressive neural\narchitectures such as normalizing flows. We show that our approach compares\nfavorably to the state of the art in a variety of settings, including perfect\nand imperfect interventions for which the targeted nodes may even be unknown.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 15:19:17 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 20:43:10 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Brouillard", "Philippe", ""], ["Lachapelle", "S\u00e9bastien", ""], ["Lacoste", "Alexandre", ""], ["Lacoste-Julien", "Simon", ""], ["Drouin", "Alexandre", ""]]}, {"id": "2007.01760", "submitter": "Philipp Liznerski", "authors": "Philipp Liznerski, Lukas Ruff, Robert A. Vandermeulen, Billy Joe\n  Franks, Marius Kloft, and Klaus-Robert M\\\"uller", "title": "Explainable Deep One-Class Classification", "comments": "25 pages, published as a conference paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep one-class classification variants for anomaly detection learn a mapping\nthat concentrates nominal samples in feature space causing anomalies to be\nmapped away. Because this transformation is highly non-linear, finding\ninterpretations poses a significant challenge. In this paper we present an\nexplainable deep one-class classification method, Fully Convolutional Data\nDescription (FCDD), where the mapped samples are themselves also an explanation\nheatmap. FCDD yields competitive detection performance and provides reasonable\nexplanations on common anomaly detection benchmarks with CIFAR-10 and ImageNet.\nOn MVTec-AD, a recent manufacturing dataset offering ground-truth anomaly maps,\nFCDD sets a new state of the art in the unsupervised setting. Our method can\nincorporate ground-truth anomaly maps during training and using even a few of\nthese (~5) improves performance significantly. Finally, using FCDD's\nexplanations we demonstrate the vulnerability of deep one-class classification\nmodels to spurious image features such as image watermarks.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 15:29:06 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 16:11:43 GMT"}, {"version": "v3", "created": "Thu, 18 Mar 2021 10:35:33 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Liznerski", "Philipp", ""], ["Ruff", "Lukas", ""], ["Vandermeulen", "Robert A.", ""], ["Franks", "Billy Joe", ""], ["Kloft", "Marius", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "2007.01777", "submitter": "Dat Hong", "authors": "Dat Hong, Stephen S. Baek, Tong Wang", "title": "Interpretable Sequence Classification Via Prototype Trajectory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel interpretable recurrent neural network (RNN) model, called\nProtoryNet, in which we introduce a new concept of prototype trajectories.\nMotivated by the prototype theory in modern linguistics, ProtoryNet makes a\nprediction by finding the most similar prototype for each sentence in a text\nsequence and feeding an RNN backbone with the proximity of each of the\nsentences to the prototypes. The RNN backbone then captures the temporal\npattern of the prototypes, to which we refer as prototype trajectories. The\nprototype trajectories enable intuitive, fine-grained interpretation of how the\nmodel reached to the final prediction, resembling the process of how humans\nanalyze paragraphs. Experiments conducted on multiple public data sets reveal\nthat the proposed method not only is more interpretable but also is more\naccurate than the current state-of-the-art prototype-based method. Furthermore,\nwe report a survey result indicating that human users find ProtoryNet more\nintuitive and easier to understand, compared to the other prototype-based\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 16:00:26 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Hong", "Dat", ""], ["Baek", "Stephen S.", ""], ["Wang", "Tong", ""]]}, {"id": "2007.01790", "submitter": "Jihong Park", "authors": "Anis Elgabli, Jihong Park, Chaouki Ben Issaid, Mehdi Bennis", "title": "Harnessing Wireless Channels for Scalable and Privacy-Preserving\n  Federated Learning", "comments": "14 pages, 7 figures; This article has been submitted to IEEE for\n  possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT cs.NI math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless connectivity is instrumental in enabling scalable federated learning\n(FL), yet wireless channels bring challenges for model training, in which\nchannel randomness perturbs each worker's model update while multiple workers'\nupdates incur significant interference under limited bandwidth. To address\nthese challenges, in this work we formulate a novel constrained optimization\nproblem, and propose an FL framework harnessing wireless channel perturbations\nand interference for improving privacy, bandwidth-efficiency, and scalability.\nThe resultant algorithm is coined analog federated ADMM (A-FADMM) based on\nanalog transmissions and the alternating direction method of multipliers\n(ADMM). In A-FADMM, all workers upload their model updates to the parameter\nserver (PS) using a single channel via analog transmissions, during which all\nmodels are perturbed and aggregated over-the-air. This not only saves\ncommunication bandwidth, but also hides each worker's exact model update\ntrajectory from any eavesdropper including the honest-but-curious PS, thereby\npreserving data privacy against model inversion attacks. We formally prove the\nconvergence and privacy guarantees of A-FADMM for convex functions under\ntime-varying channels, and numerically show the effectiveness of A-FADMM under\nnoisy channels and stochastic non-convex functions, in terms of convergence\nspeed and scalability, as well as communication bandwidth and energy\nefficiency.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 16:31:15 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 09:17:13 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Elgabli", "Anis", ""], ["Park", "Jihong", ""], ["Issaid", "Chaouki Ben", ""], ["Bennis", "Mehdi", ""]]}, {"id": "2007.01807", "submitter": "Hao Wang", "authors": "Hao Wang and Hao He and Dina Katabi", "title": "Continuously Indexed Domain Adaptation", "comments": "Accepted at ICML 2020. Talk:\n  https://www.youtube.com/watch?v=KtZPSCD-WhQ Code and Project Page:\n  https://github.com/hehaodele/CIDA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing domain adaptation focuses on transferring knowledge between domains\nwith categorical indices (e.g., between datasets A and B). However, many tasks\ninvolve continuously indexed domains. For example, in medical applications, one\noften needs to transfer disease analysis and prediction across patients of\ndifferent ages, where age acts as a continuous domain index. Such tasks are\nchallenging for prior domain adaptation methods since they ignore the\nunderlying relation among domains. In this paper, we propose the first method\nfor continuously indexed domain adaptation. Our approach combines traditional\nadversarial adaptation with a novel discriminator that models the\nencoding-conditioned domain index distribution. Our theoretical analysis\ndemonstrates the value of leveraging the domain index to generate invariant\nfeatures across a continuous range of domains. Our empirical results show that\nour approach outperforms the state-of-the-art domain adaption methods on both\nsynthetic and real-world medical datasets.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 16:53:50 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2020 02:31:43 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Wang", "Hao", ""], ["He", "Hao", ""], ["Katabi", "Dina", ""]]}, {"id": "2007.01811", "submitter": "Chris von Csefalvay", "authors": "Tamas Foldi, Chris von Csefalvay and Nicolas A. Perez", "title": "JAMPI: efficient matrix multiplication in Spark using Barrier Execution\n  Mode", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new barrier mode in Apache Spark allows embedding distributed deep\nlearning training as a Spark stage to simplify the distributed training\nworkflow. In Spark, a task in a stage does not depend on any other tasks in the\nsame stage, and hence it can be scheduled independently. However, several\nalgorithms require more sophisticated inter-task communications, similar to the\nMPI paradigm. By combining distributed message passing (using asynchronous\nnetwork IO), OpenJDK's new auto-vectorization and Spark's barrier execution\nmode, we can add non-map/reduce based algorithms, such as Cannon's distributed\nmatrix multiplication to Spark. We document an efficient distributed matrix\nmultiplication using Cannon's algorithm, which improves significantly on the\nperformance of the existing MLlib implementation. Used within a barrier task,\nthe algorithm described herein results in an up to 24 percent performance\nincrease on a 10,000x10,000 square matrix with a significantly lower memory\nfootprint. Applications of efficient matrix multiplication include, among\nothers, accelerating the training and implementation of deep convolutional\nneural network based workloads, and thus such efficient algorithms can play a\nground-breaking role in faster, more efficient execution of even the most\ncomplicated machine learning tasks.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 17:31:23 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Foldi", "Tamas", ""], ["von Csefalvay", "Chris", ""], ["Perez", "Nicolas A.", ""]]}, {"id": "2007.01814", "submitter": "Soheil Sadeghi Eshkevari", "authors": "Soheil Sadeghi Eshkevari, Martin Tak\\'a\\v{c}, Shamim N. Pakzad, and\n  Majid Jahani", "title": "DynNet: Physics-based neural architecture design for linear and\n  nonlinear structural response modeling and prediction", "comments": "Submitted to Elsevier", "journal-ref": "Journal of Engineering Structures, Volume 229, 15 February 2021", "doi": "10.1016/j.engstruct.2020.111582", "report-no": "111582", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven models for predicting dynamic responses of linear and nonlinear\nsystems are of great importance due to their wide application from\nprobabilistic analysis to inverse problems such as system identification and\ndamage diagnosis. In this study, a physics-based recurrent neural network model\nis designed that is able to learn the dynamics of linear and nonlinear multiple\ndegrees of freedom systems given a ground motion. The model is able to estimate\na complete set of responses, including displacement, velocity, acceleration,\nand internal forces. Compared to the most advanced counterparts, this model\nrequires a smaller number of trainable variables while the accuracy of\npredictions is higher for long trajectories. In addition, the architecture of\nthe recurrent block is inspired by differential equation solver algorithms and\nit is expected that this approach yields more generalized solutions. In the\ntraining phase, we propose multiple novel techniques to dramatically accelerate\nthe learning process using smaller datasets, such as hardsampling, utilization\nof trajectory loss function, and implementation of a trust-region approach.\nNumerical case studies are conducted to examine the strength of the network to\nlearn different nonlinear behaviors. It is shown that the network is able to\ncapture different nonlinear behaviors of dynamic systems with very high\naccuracy and with no need for prior information or very large datasets.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 17:05:35 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Eshkevari", "Soheil Sadeghi", ""], ["Tak\u00e1\u010d", "Martin", ""], ["Pakzad", "Shamim N.", ""], ["Jahani", "Majid", ""]]}, {"id": "2007.01819", "submitter": "Swapnil Nitin Shah", "authors": "Swapnil Nitin Shah", "title": "Addressing the interpretability problem for deep learning using many\n  valued quantum logic", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models are widely used for various industrial and scientific\napplications. Even though these models have achieved considerable success in\nrecent years, there exists a lack of understanding of the rationale behind\ndecisions made by such systems in the machine learning community. This problem\nof interpretability is further aggravated by the increasing complexity of such\nmodels. This paper utilizes concepts from machine learning, quantum computation\nand quantum field theory to demonstrate how a many valued quantum logic system\nnaturally arises in a specific class of generative deep learning models called\nConvolutional Deep Belief Networks. It provides a robust theoretical framework\nfor constructing deep learning models equipped with the interpretability of\nmany valued quantum logic systems without compromising their computing\nefficiency.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 15:00:28 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Shah", "Swapnil Nitin", ""]]}, {"id": "2007.01833", "submitter": "Prakash Rajan", "authors": "Prakash Rajan, Krishna P. Miyapuram", "title": "PsychFM: Predicting your next gamble", "comments": "To be published in International Joint Conference on Neural Networks\n  (IJCNN) 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a sudden surge to model human behavior due to its vast and diverse\napplications which includes modeling public policies, economic behavior and\nconsumer behavior. Most of the human behavior itself can be modeled into a\nchoice prediction problem. Prospect theory is a theoretical model that tries to\nexplain the anomalies in choice prediction. These theories perform well in\nterms of explaining the anomalies but they lack precision. Since the behavior\nis person dependent, there is a need to build a model that predicts choices on\na per-person basis. Looking on at the average persons choice may not\nnecessarily throw light on a particular person's choice. Modeling the gambling\nproblem on a per person basis will help in recommendation systems and related\nareas. A novel hybrid model namely psychological factorisation machine (\nPsychFM ) has been proposed that involves concepts from machine learning as\nwell as psychological theories. It outperforms the popular existing models\nnamely random forest and factorisation machines for the benchmark dataset\nCPC-18. Finally,the efficacy of the proposed hybrid model has been verified by\ncomparing with the existing models.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 17:41:14 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Rajan", "Prakash", ""], ["Miyapuram", "Krishna P.", ""]]}, {"id": "2007.01839", "submitter": "Hado van Hasselt", "authors": "Hado van Hasselt, Sephora Madjiheurem, Matteo Hessel, David Silver,\n  Andr\\'e Barreto, Diana Borsa", "title": "Expected Eligibility Traces", "comments": "AAAI, distinguished paper award", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question of how to determine which states and actions are responsible for\na certain outcome is known as the credit assignment problem and remains a\ncentral research question in reinforcement learning and artificial\nintelligence. Eligibility traces enable efficient credit assignment to the\nrecent sequence of states and actions experienced by the agent, but not to\ncounterfactual sequences that could also have led to the current state. In this\nwork, we introduce expected eligibility traces. Expected traces allow, with a\nsingle update, to update states and actions that could have preceded the\ncurrent state, even if they did not do so on this occasion. We discuss when\nexpected traces provide benefits over classic (instantaneous) traces in\ntemporal-difference learning, and show that sometimes substantial improvements\ncan be attained. We provide a way to smoothly interpolate between instantaneous\nand expected traces by a mechanism similar to bootstrapping, which ensures that\nthe resulting algorithm is a strict generalisation of TD($\\lambda$). Finally,\nwe discuss possible extensions and connections to related ideas, such as\nsuccessor features.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 17:46:16 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 13:02:30 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["van Hasselt", "Hado", ""], ["Madjiheurem", "Sephora", ""], ["Hessel", "Matteo", ""], ["Silver", "David", ""], ["Barreto", "Andr\u00e9", ""], ["Borsa", "Diana", ""]]}, {"id": "2007.01850", "submitter": "Taoli Cheng", "authors": "Taoli Cheng, Jean-Fran\\c{c}ois Arguin, Julien Leissner-Martin,\n  Jacinthe Pilette, Tobias Golling", "title": "Variational Autoencoders for Anomalous Jet Tagging", "comments": "35 pages, 22 figures. Revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-ph hep-ex stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a detailed study on Variational Autoencoders (VAEs) for anomalous\njet tagging at the Large Hadron Collider. By taking in low-level jet\nconstituents' information, and training with background QCD jets in an\nunsupervised manner, the VAE is able to encode important information for\nreconstructing jets, while learning an expressive posterior distribution in the\nlatent space. When using the VAE as an anomaly detector, we present different\napproaches to detect anomalies: directly comparing in the input space or,\ninstead, working in the latent space. In order to facilitate general search\napproaches such as bump-hunt, mass-decorrelated VAEs based on distance\ncorrelation regularization are also studied. We find that the naive\nmass-decorrelated VAEs fail at maintaining proper detection performance, by\nassigning higher probabilities to some anomalous samples. To build a performant\nmass-decorrelated anomalous jet tagger, we propose the Outlier Exposed VAE\n(OE-VAE), for which some outlier samples are introduced in the training process\nto guide the learned information. OE-VAEs are employed to achieve two goals at\nthe same time: increasing sensitivity of outlier detection and decorrelating\njet mass from the anomaly score. We succeed in reaching excellent results from\nboth aspects. Code implementation of this work can be found at\n\\href{https://github.com/taolicheng/VAE-Jet}{Github}.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 17:57:52 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 17:28:43 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2021 18:54:08 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Cheng", "Taoli", ""], ["Arguin", "Jean-Fran\u00e7ois", ""], ["Leissner-Martin", "Julien", ""], ["Pilette", "Jacinthe", ""], ["Golling", "Tobias", ""]]}, {"id": "2007.01855", "submitter": "Ehsan Kazemi Dr", "authors": "Ehsan Kazemi, Thomas Kerdreux and Liqiang Wang", "title": "Trace-Norm Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  White box adversarial perturbations are sought via iterative optimization\nalgorithms most often minimizing an adversarial loss on a $l_p$ neighborhood of\nthe original image, the so-called distortion set. Constraining the adversarial\nsearch with different norms results in disparately structured adversarial\nexamples. Here we explore several distortion sets with structure-enhancing\nalgorithms. These new structures for adversarial examples, yet pervasive in\noptimization, are for instance a challenge for adversarial theoretical\ncertification which again provides only $l_p$ certificates. Because adversarial\nrobustness is still an empirical field, defense mechanisms should also\nreasonably be evaluated against differently structured attacks. Besides, these\nstructured adversarial perturbations may allow for larger distortions size than\ntheir $l_p$ counter-part while remaining imperceptible or perceptible as\nnatural slight distortions of the image. Finally, they allow some control on\nthe generation of the adversarial perturbation, like (localized) bluriness.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 13:37:19 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Kazemi", "Ehsan", ""], ["Kerdreux", "Thomas", ""], ["Wang", "Liqiang", ""]]}, {"id": "2007.01884", "submitter": "Andreas Gerhardus", "authors": "Andreas Gerhardus and Jakob Runge", "title": "High-recall causal discovery for autocorrelated time series with latent\n  confounders", "comments": "55 pages, 26 figures; added reference to related work plus\n  accompanying dicussion in section 3.3", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for linear and nonlinear, lagged and contemporaneous\nconstraint-based causal discovery from observational time series in the\npresence of latent confounders. We show that existing causal discovery methods\nsuch as FCI and variants suffer from low recall in the autocorrelated time\nseries case and identify low effect size of conditional independence tests as\nthe main reason. Information-theoretical arguments show that effect size can\noften be increased if causal parents are included in the conditioning sets. To\nidentify parents early on, we suggest an iterative procedure that utilizes\nnovel orientation rules to determine ancestral relationships already during the\nedge removal phase. We prove that the method is order-independent, and sound\nand complete in the oracle case. Extensive simulation studies for different\nnumbers of variables, time lags, sample sizes, and further cases demonstrate\nthat our method indeed achieves much higher recall than existing methods for\nthe case of autocorrelated continuous variables while keeping false positives\nat the desired level. This performance gain grows with stronger\nautocorrelation. At https://github.com/jakobrunge/tigramite we provide Python\ncode for all methods involved in the simulation studies.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 18:01:04 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 19:00:08 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2021 19:00:08 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Gerhardus", "Andreas", ""], ["Runge", "Jakob", ""]]}, {"id": "2007.01888", "submitter": "Abhishek Kaul", "authors": "Abhishek Kaul, Stergios B. Fotopoulos, Venkata K. Jandhyala, Abolfazl\n  Safikhani", "title": "Inference on the change point in high dimensional time series models via\n  plug in least squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a plug in least squares estimator for the change point parameter\nwhere change is in the mean of a high dimensional random vector under\nsubgaussian or subexponential distributions. We obtain sufficient conditions\nunder which this estimator possesses sufficient adaptivity against plug in\nestimates of mean parameters in order to yield an optimal rate of convergence\n$O_p(\\xi^{-2})$ in the integer scale. This rate is preserved while allowing\nhigh dimensionality as well as a potentially diminishing jump size $\\xi,$\nprovided $s\\log (p\\vee T)=o(\\surd(Tl_T))$ or $s\\log^{3/2}(p\\vee\nT)=o(\\surd(Tl_T))$ in the subgaussian and subexponential cases, respectively.\nHere $s,p,T$ and $l_T$ represent a sparsity parameter, model dimension,\nsampling period and the separation of the change point from its parametric\nboundary. Moreover, since the rate of convergence is free of $s,p$ and\nlogarithmic terms of $T,$ it allows the existence of limiting distributions.\nThese distributions are then derived as the {\\it argmax} of a two sided\nnegative drift Brownian motion or a two sided negative drift random walk under\nvanishing and non-vanishing jump size regimes, respectively. Thereby allowing\ninference of the change point parameter in the high dimensional setting.\nFeasible algorithms for implementation of the proposed methodology are\nprovided. Theoretical results are supported with monte-carlo simulations.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 18:08:12 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 06:15:20 GMT"}, {"version": "v3", "created": "Sat, 11 Jul 2020 05:49:14 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Kaul", "Abhishek", ""], ["Fotopoulos", "Stergios B.", ""], ["Jandhyala", "Venkata K.", ""], ["Safikhani", "Abolfazl", ""]]}, {"id": "2007.01891", "submitter": "Ciara Pike-Burke", "authors": "Gergely Neu and Ciara Pike-Burke", "title": "A Unifying View of Optimism in Episodic Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The principle of optimism in the face of uncertainty underpins many\ntheoretically successful reinforcement learning algorithms. In this paper we\nprovide a general framework for designing, analyzing and implementing such\nalgorithms in the episodic reinforcement learning problem. This framework is\nbuilt upon Lagrangian duality, and demonstrates that every model-optimistic\nalgorithm that constructs an optimistic MDP has an equivalent representation as\na value-optimistic dynamic programming algorithm. Typically, it was thought\nthat these two classes of algorithms were distinct, with model-optimistic\nalgorithms benefiting from a cleaner probabilistic analysis while\nvalue-optimistic algorithms are easier to implement and thus more practical.\nWith the framework developed in this paper, we show that it is possible to get\nthe best of both worlds by providing a class of algorithms which have a\ncomputationally efficient dynamic-programming implementation and also a simple\nprobabilistic analysis. Besides being able to capture many existing algorithms\nin the tabular setting, our framework can also address largescale problems\nunder realizable function approximation, where it enables a simple model-based\nanalysis of some recently proposed methods.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 18:10:30 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Neu", "Gergely", ""], ["Pike-Burke", "Ciara", ""]]}, {"id": "2007.01900", "submitter": "Hans Dermot Doran", "authors": "Hans Dermot Doran and Monika Reif", "title": "Examining Redundancy in the Context of Safe Machine Learning", "comments": "5 pages, 7 tables, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a set of experiments with neural network classifiers on\nthe MNIST database of digits. The purpose is to investigate na\\\"ive\nimplementations of redundant architectures as a first step towards safe and\ndependable machine learning. We report on a set of measurements using the MNIST\ndatabase which ultimately serve to underline the expected difficulties in using\nNN classifiers in safe and dependable systems.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 18:23:56 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Doran", "Hans Dermot", ""], ["Reif", "Monika", ""]]}, {"id": "2007.01903", "submitter": "Max Biggs", "authors": "Max Biggs, Wei Sun, Markus Ettl", "title": "Model Distillation for Revenue Optimization: Interpretable Personalized\n  Pricing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven pricing strategies are becoming increasingly common, where\ncustomers are offered a personalized price based on features that are\npredictive of their valuation of a product. It is desirable for this pricing\npolicy to be simple and interpretable, so it can be verified, checked for\nfairness, and easily implemented. However, efforts to incorporate machine\nlearning into a pricing framework often lead to complex pricing policies which\nare not interpretable, resulting in slow adoption in practice. We present a\ncustomized, prescriptive tree-based algorithm that distills knowledge from a\ncomplex black-box machine learning algorithm, segments customers with similar\nvaluations and prescribes prices in such a way that maximizes revenue while\nmaintaining interpretability. We quantify the regret of a resulting policy and\ndemonstrate its efficacy in applications with both synthetic and real-world\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 18:33:23 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 18:22:11 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Biggs", "Max", ""], ["Sun", "Wei", ""], ["Ettl", "Markus", ""]]}, {"id": "2007.01905", "submitter": "Chris Williams", "authors": "Christopher K I Williams", "title": "The Effect of Class Imbalance on Precision-Recall Curves", "comments": "4 pages, 1 figure. Added ref to Siblini et al (2020) and last\n  sentence. Final m/s version of paper published in Neural Computation", "journal-ref": "Neural Computation 33(4) 853-857 (2021)", "doi": "10.1162/neco_a_01362", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note I study how the precision of a classifier depends on the ratio\n$r$ of positive to negative cases in the test set, as well as the classifier's\ntrue and false positive rates. This relationship allows prediction of how the\nprecision-recall curve will change with $r$, which seems not to be well known.\nIt also allows prediction of how $F_{\\beta}$ and the Precision Gain and Recall\nGain measures of Flach and Kull (2015) vary with $r$.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 18:42:25 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 17:08:54 GMT"}, {"version": "v3", "created": "Tue, 27 Apr 2021 10:58:43 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Williams", "Christopher K I", ""]]}, {"id": "2007.01919", "submitter": "Gon\\c{c}alo M. Correia", "authors": "Gon\\c{c}alo M. Correia, Vlad Niculae, Wilker Aziz, Andr\\'e F. T.\n  Martins", "title": "Efficient Marginalization of Discrete and Structured Latent Variables\n  via Sparsity", "comments": "Accepted for spotlight presentation at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training neural network models with discrete (categorical or structured)\nlatent variables can be computationally challenging, due to the need for\nmarginalization over large or combinatorial sets. To circumvent this issue, one\ntypically resorts to sampling-based approximations of the true marginal,\nrequiring noisy gradient estimators (e.g., score function estimator) or\ncontinuous relaxations with lower-variance reparameterized gradients (e.g.,\nGumbel-Softmax). In this paper, we propose a new training strategy which\nreplaces these estimators by an exact yet efficient marginalization. To achieve\nthis, we parameterize discrete distributions over latent assignments using\ndifferentiable sparse mappings: sparsemax and its structured counterparts. In\neffect, the support of these distributions is greatly reduced, which enables\nefficient marginalization. We report successful results in three tasks covering\na range of latent variable modeling applications: a semisupervised deep\ngenerative model, a latent communication game, and a generative model with a\nbit-vector latent representation. In all cases, we obtain good performance\nwhile still achieving the practicality of sampling-based approximations.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 19:36:35 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 14:50:31 GMT"}, {"version": "v3", "created": "Mon, 28 Dec 2020 10:33:38 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Correia", "Gon\u00e7alo M.", ""], ["Niculae", "Vlad", ""], ["Aziz", "Wilker", ""], ["Martins", "Andr\u00e9 F. T.", ""]]}, {"id": "2007.01922", "submitter": "Bahram Zonooz", "authors": "Fahad Sarfraz, Elahe Arani and Bahram Zonooz", "title": "Knowledge Distillation Beyond Model Compression", "comments": "Accepted at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation (KD) is commonly deemed as an effective model\ncompression technique in which a compact model (student) is trained under the\nsupervision of a larger pretrained model or an ensemble of models (teacher).\nVarious techniques have been proposed since the original formulation, which\nmimic different aspects of the teacher such as the representation space,\ndecision boundary, or intra-data relationship. Some methods replace the one-way\nknowledge distillation from a static teacher with collaborative learning\nbetween a cohort of students. Despite the recent advances, a clear\nunderstanding of where knowledge resides in a deep neural network and an\noptimal method for capturing knowledge from teacher and transferring it to\nstudent remains an open question. In this study, we provide an extensive study\non nine different KD methods which covers a broad spectrum of approaches to\ncapture and transfer knowledge. We demonstrate the versatility of the KD\nframework on different datasets and network architectures under varying\ncapacity gaps between the teacher and student. The study provides intuition for\nthe effects of mimicking different aspects of the teacher and derives insights\nfrom the performance of the different distillation approaches to guide the\ndesign of more effective KD methods. Furthermore, our study shows the\neffectiveness of the KD framework in learning efficiently under varying\nseverity levels of label noise and class imbalance, consistently providing\ngeneralization gains over standard training. We emphasize that the efficacy of\nKD goes much beyond a model compression technique and it should be considered\nas a general-purpose training paradigm which offers more robustness to common\nchallenges in the real-world datasets compared to the standard training\nprocedure.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 19:54:04 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Sarfraz", "Fahad", ""], ["Arani", "Elahe", ""], ["Zonooz", "Bahram", ""]]}, {"id": "2007.01926", "submitter": "Yaofeng Desmond Zhong", "authors": "Yaofeng Desmond Zhong, Naomi Ehrich Leonard", "title": "Unsupervised Learning of Lagrangian Dynamics from Images for Prediction\n  and Control", "comments": "Camera-ready submission for NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent approaches for modelling dynamics of physical systems with neural\nnetworks enforce Lagrangian or Hamiltonian structure to improve prediction and\ngeneralization. However, when coordinates are embedded in high-dimensional data\nsuch as images, these approaches either lose interpretability or can only be\napplied to one particular example. We introduce a new unsupervised neural\nnetwork model that learns Lagrangian dynamics from images, with\ninterpretability that benefits prediction and control. The model infers\nLagrangian dynamics on generalized coordinates that are simultaneously learned\nwith a coordinate-aware variational autoencoder (VAE). The VAE is designed to\naccount for the geometry of physical systems composed of multiple rigid bodies\nin the plane. By inferring interpretable Lagrangian dynamics, the model learns\nphysical system properties, such as kinetic and potential energy, which enables\nlong-term prediction of dynamics in the image space and synthesis of\nenergy-based controllers.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 20:06:43 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 00:27:34 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Zhong", "Yaofeng Desmond", ""], ["Leonard", "Naomi Ehrich", ""]]}, {"id": "2007.01929", "submitter": "Niharika Shimona D'Souza", "authors": "Niharika Shimona D'Souza, Mary Beth Nebel, Nicholas Wymbs, Stewart\n  Mostofsky, and Archana Venkataraman", "title": "A Coupled Manifold Optimization Framework to Jointly Model the\n  Functional Connectomics and Behavioral Data Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of linking functional connectomics to behavior is extremely\nchallenging due to the complex interactions between the two distinct, but\nrelated, data domains. We propose a coupled manifold optimization framework\nwhich projects fMRI data onto a low dimensional matrix manifold common to the\ncohort. The patient specific loadings simultaneously map onto a behavioral\nmeasure of interest via a second, non-linear, manifold. By leveraging the\nkernel trick, we can optimize over a potentially infinite dimensional space\nwithout explicitly computing the embeddings. As opposed to conventional\nmanifold learning, which assumes a fixed input representation, our framework\ndirectly optimizes for embedding directions that predict behavior. Our\noptimization algorithm combines proximal gradient descent with the trust region\nmethod, which has good convergence guarantees. We validate our framework on\nresting state fMRI from fifty-eight patients with Autism Spectrum Disorder\nusing three distinct measures of clinical severity. Our method outperforms\ntraditional representation learning techniques in a cross validated setting,\nthus demonstrating the predictive power of our coupled objective.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 20:12:51 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["D'Souza", "Niharika Shimona", ""], ["Nebel", "Mary Beth", ""], ["Wymbs", "Nicholas", ""], ["Mostofsky", "Stewart", ""], ["Venkataraman", "Archana", ""]]}, {"id": "2007.01930", "submitter": "Niharika Shimona D'Souza", "authors": "Niharika Shimona D'Souza, Mary Beth Nebel, Nicholas Wymbs, Stewart\n  Mostofsky, and Archana Venkataraman", "title": "Integrating Neural Networks and Dictionary Learning for Multidimensional\n  Clinical Characterizations from Functional Connectomics Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a unified optimization framework that combines neural networks\nwith dictionary learning to model complex interactions between resting state\nfunctional MRI and behavioral data. The dictionary learning objective\ndecomposes patient correlation matrices into a collection of shared basis\nnetworks and subject-specific loadings. These subject-specific features are\nsimultaneously input into a neural network that predicts multidimensional\nclinical information. Our novel optimization framework combines the gradient\ninformation from the neural network with that of a conventional matrix\nfactorization objective. This procedure collectively estimates the basis\nnetworks, subject loadings, and neural network weights most informative of\nclinical severity. We evaluate our combined model on a multi-score prediction\ntask using 52 patients diagnosed with Autism Spectrum Disorder (ASD). Our\nintegrated framework outperforms state-of-the-art methods in a ten-fold cross\nvalidated setting to predict three different measures of clinical severity.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 20:14:45 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["D'Souza", "Niharika Shimona", ""], ["Nebel", "Mary Beth", ""], ["Wymbs", "Nicholas", ""], ["Mostofsky", "Stewart", ""], ["Venkataraman", "Archana", ""]]}, {"id": "2007.01931", "submitter": "Niharika Shimona D'Souza", "authors": "Niharika Shimona D'Souza, Mary Beth Nebel, Deana Crocetti, Nicholas\n  Wymbs, Joshua Robinson, Stewart Mostofsky, and Archana Venkataraman", "title": "A Deep-Generative Hybrid Model to Integrate Multimodal and Dynamic\n  Connectivity for Predicting Spectrum-Level Deficits in Autism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an integrated deep-generative framework, that jointly models\ncomplementary information from resting-state functional MRI (rs-fMRI)\nconnectivity and diffusion tensor imaging (DTI) tractography to extract\npredictive biomarkers of a disease. The generative part of our framework is a\nstructurally-regularized Dynamic Dictionary Learning (sr-DDL) model that\ndecomposes the dynamic rs-fMRI correlation matrices into a collection of shared\nbasis networks and time varying patient-specific loadings. This matrix\nfactorization is guided by the DTI tractography matrices to learn anatomically\ninformed connectivity profiles. The deep part of our framework is an LSTM-ANN\nblock, which models the temporal evolution of the patient sr-DDL loadings to\npredict multidimensional clinical severity. Our coupled optimization procedure\ncollectively estimates the basis networks, the patient-specific dynamic\nloadings, and the neural network weights. We validate our framework on a\nmulti-score prediction task in 57 patients diagnosed with Autism Spectrum\nDisorder (ASD). Our hybrid model outperforms state-of-the-art baselines in a\nfive-fold cross validated setting and extracts interpretable multimodal neural\nsignatures of brain dysfunction in ASD.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 20:18:09 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["D'Souza", "Niharika Shimona", ""], ["Nebel", "Mary Beth", ""], ["Crocetti", "Deana", ""], ["Wymbs", "Nicholas", ""], ["Robinson", "Joshua", ""], ["Mostofsky", "Stewart", ""], ["Venkataraman", "Archana", ""]]}, {"id": "2007.01932", "submitter": "Yufei Wang", "authors": "Yufei Wang, Tianwei Ni", "title": "Meta-SAC: Auto-tune the Entropy Temperature of Soft Actor-Critic via\n  Metagradient", "comments": "published at 7th ICML Workshop on Automated Machine Learning (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploration-exploitation dilemma has long been a crucial issue in\nreinforcement learning. In this paper, we propose a new approach to\nautomatically balance between these two. Our method is built upon the Soft\nActor-Critic (SAC) algorithm, which uses an \"entropy temperature\" that balances\nthe original task reward and the policy entropy, and hence controls the\ntrade-off between exploitation and exploration. It is empirically shown that\nSAC is very sensitive to this hyperparameter, and the follow-up work (SAC-v2),\nwhich uses constrained optimization for automatic adjustment, has some\nlimitations. The core of our method, namely Meta-SAC, is to use metagradient\nalong with a novel meta objective to automatically tune the entropy temperature\nin SAC. We show that Meta-SAC achieves promising performances on several of the\nMujoco benchmarking tasks, and outperforms SAC-v2 over 10% in one of the most\nchallenging tasks, humanoid-v2.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 20:26:50 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 04:34:20 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Wang", "Yufei", ""], ["Ni", "Tianwei", ""]]}, {"id": "2007.01946", "submitter": "Tomas Martin", "authors": "Tomas Martin, Guy Francoeur, Petko Valtchev", "title": "CICLAD: A Fast and Memory-efficient Closed Itemset Miner for Streams", "comments": "KDD20", "journal-ref": null, "doi": "10.1145/3394486.3403232", "report-no": null, "categories": "cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining association rules from data streams is a challenging task due to the\n(typically) limited resources available vs. the large size of the result.\nFrequent closed itemsets (FCI) enable an efficient first step, yet current FCI\nstream miners are not optimal on resource consumption, e.g. they store a large\nnumber of extra itemsets at an additional cost. In a search for a better\nstorage-efficiency trade-off, we designed Ciclad,an intersection-based\nsliding-window FCI miner. Leveraging in-depth insights into FCI evolution, it\ncombines minimal storage with quick access. Experimental results indicate\nCiclad's memory imprint is much lower and its performances globally better than\ncompetitor methods.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 21:50:35 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Martin", "Tomas", ""], ["Francoeur", "Guy", ""], ["Valtchev", "Petko", ""]]}, {"id": "2007.01965", "submitter": "Ramin Moradi", "authors": "Ramin Moradi, Katrina M. Groth", "title": "On the application of transfer learning in prognostics and health\n  management", "comments": "8 pages, 3 figures, submitted to the annual conference of the\n  prognostics and health management society 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advancements in sensing and computing technologies, the development of human\nand computer interaction frameworks, big data storage capabilities, and the\nemergence of cloud storage and could computing have resulted in an abundance of\ndata in the modern industry. This data availability has encouraged researchers\nand industry practitioners to rely on data-based machine learning, especially\ndeep learning, models for fault diagnostics and prognostics more than ever.\nThese models provide unique advantages, however, their performance is heavily\ndependent on the training data and how well that data represents the test data.\nThis issue mandates fine-tuning and even training the models from scratch when\nthere is a slight change in operating conditions or equipment. Transfer\nlearning is an approach that can remedy this issue by keeping portions of what\nis learned from previous training and transferring them to the new application.\nIn this paper, a unified definition for transfer learning and its different\ntypes is provided, Prognostics and Health Management (PHM) studies that have\nused transfer learning are reviewed in detail, and finally, a discussion on\ntransfer learning application considerations and gaps is provided for improving\nthe applicability of transfer learning in PHM.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 23:35:18 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Moradi", "Ramin", ""], ["Groth", "Katrina M.", ""]]}, {"id": "2007.01971", "submitter": "Ping Yu", "authors": "Ping Yu, Yang Zhao, Chunyuan Li, Junsong Yuan, Changyou Chen", "title": "Structure-Aware Human-Action Generation", "comments": "accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Generating long-range skeleton-based human actions has been a challenging\nproblem since small deviations of one frame can cause a malformed action\nsequence. Most existing methods borrow ideas from video generation, which\nnaively treat skeleton nodes/joints as pixels of images without considering the\nrich inter-frame and intra-frame structure information, leading to potential\ndistorted actions. Graph convolutional networks (GCNs) is a promising way to\nleverage structure information to learn structure representations. However,\ndirectly adopting GCNs to tackle such continuous action sequences both in\nspatial and temporal spaces is challenging as the action graph could be huge.\nTo overcome this issue, we propose a variant of GCNs to leverage the powerful\nself-attention mechanism to adaptively sparsify a complete action graph in the\ntemporal space. Our method could dynamically attend to important past frames\nand construct a sparse graph to apply in the GCN framework, well-capturing the\nstructure information in action sequences. Extensive experimental results\ndemonstrate the superiority of our method on two standard human action datasets\ncompared with existing methods.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 00:18:27 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 21:31:44 GMT"}, {"version": "v3", "created": "Sun, 16 Aug 2020 20:05:43 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Yu", "Ping", ""], ["Zhao", "Yang", ""], ["Li", "Chunyuan", ""], ["Yuan", "Junsong", ""], ["Chen", "Changyou", ""]]}, {"id": "2007.01972", "submitter": "Nitakshi Sood", "authors": "Nitakshi Sood and Osmar Zaiane", "title": "Building a Competitive Associative Classifier", "comments": "To be published in - The 22nd International Conference on Big Data\n  Analytics and Knowledge Discovery - DaWaK2020, Bratislava, Slovakia,\n  September 14-17, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the huge success of deep learning, other machine learning paradigms have\nhad to take back seat. Yet other models, particularly rule-based, are more\nreadable and explainable and can even be competitive when labelled data is not\nabundant. However, most of the existing rule-based classifiers suffer from the\nproduction of a large number of classification rules, affecting the model\nreadability. This hampers the classification accuracy as noisy rules might not\nadd any useful informationfor classification and also lead to longer\nclassification time. In this study, we propose SigD2 which uses a novel,\ntwo-stage pruning strategy which prunes most of the noisy, redundant and\nuninteresting rules and makes the classification model more accurate and\nreadable. To make SigDirect more competitive with the most prevalent but\nuninterpretable machine learning-based classifiers like neural networks and\nsupport vector machines, we propose bagging and boosting on the ensemble of the\nSigDirect classifier. The results of the proposed algorithms are quite\npromising and we are able to obtain a minimal set of statistically significant\nrules for classification without jeopardizing the classification accuracy. We\nuse 15 UCI datasets and compare our approach with eight existing systems.The\nSigD2 and boosted SigDirect (ACboost) ensemble model outperform various\nstate-of-the-art classifiers not only in terms of classification accuracy but\nalso in terms of the number of rules.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 00:20:27 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Sood", "Nitakshi", ""], ["Zaiane", "Osmar", ""]]}, {"id": "2007.01980", "submitter": "Yuan Zhou", "authors": "Yufei Ruan, Jiaqi Yang, Yuan Zhou", "title": "Linear Bandits with Limited Adaptivity and Learning Distributional\n  Optimal Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by practical needs such as large-scale learning, we study the\nimpact of adaptivity constraints to linear contextual bandits, a central\nproblem in online active learning. We consider two popular limited adaptivity\nmodels in literature: batch learning and rare policy switches. We show that,\nwhen the context vectors are adversarially chosen in $d$-dimensional linear\ncontextual bandits, the learner needs $O(d \\log d \\log T)$ policy switches to\nachieve the minimax-optimal regret, and this is optimal up to\n$\\mathrm{poly}(\\log d, \\log \\log T)$ factors; for stochastic context vectors,\neven in the more restricted batch learning model, only $O(\\log \\log T)$ batches\nare needed to achieve the optimal regret. Together with the known results in\nliterature, our results present a complete picture about the adaptivity\nconstraints in linear contextual bandits. Along the way, we propose the\ndistributional optimal design, a natural extension of the optimal experiment\ndesign, and provide a both statistically and computationally efficient learning\nalgorithm for the problem, which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 01:34:22 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 04:17:42 GMT"}, {"version": "v3", "created": "Fri, 23 Apr 2021 13:01:03 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Ruan", "Yufei", ""], ["Yang", "Jiaqi", ""], ["Zhou", "Yuan", ""]]}, {"id": "2007.01990", "submitter": "Yi Chen", "authors": "Yi Chen, Jinglin Chen, Jing Dong, Jian Peng, Zhaoran Wang", "title": "Accelerating Nonconvex Learning via Replica Exchange Langevin Diffusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Langevin diffusion is a powerful method for nonconvex optimization, which\nenables the escape from local minima by injecting noise into the gradient. In\nparticular, the temperature parameter controlling the noise level gives rise to\na tradeoff between ``global exploration'' and ``local exploitation'', which\ncorrespond to high and low temperatures. To attain the advantages of both\nregimes, we propose to use replica exchange, which swaps between two Langevin\ndiffusions with different temperatures. We theoretically analyze the\nacceleration effect of replica exchange from two perspectives: (i) the\nconvergence in \\chi^2-divergence, and (ii) the large deviation principle. Such\nan acceleration effect allows us to faster approach the global minima.\nFurthermore, by discretizing the replica exchange Langevin diffusion, we obtain\na discrete-time algorithm. For such an algorithm, we quantify its\ndiscretization error in theory and demonstrate its acceleration effect in\npractice.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 02:52:11 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Chen", "Yi", ""], ["Chen", "Jinglin", ""], ["Dong", "Jing", ""], ["Peng", "Jian", ""], ["Wang", "Zhaoran", ""]]}, {"id": "2007.01995", "submitter": "Hang Lai", "authors": "Hang Lai, Jian Shen, Weinan Zhang, Yong Yu", "title": "Bidirectional Model-based Policy Optimization", "comments": "Accepted at ICML2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based reinforcement learning approaches leverage a forward dynamics\nmodel to support planning and decision making, which, however, may fail\ncatastrophically if the model is inaccurate. Although there are several\nexisting methods dedicated to combating the model error, the potential of the\nsingle forward model is still limited. In this paper, we propose to\nadditionally construct a backward dynamics model to reduce the reliance on\naccuracy in forward model predictions. We develop a novel method, called\nBidirectional Model-based Policy Optimization (BMPO) to utilize both the\nforward model and backward model to generate short branched rollouts for policy\noptimization. Furthermore, we theoretically derive a tighter bound of return\ndiscrepancy, which shows the superiority of BMPO against the one using merely\nthe forward model. Extensive experiments demonstrate that BMPO outperforms\nstate-of-the-art model-based methods in terms of sample efficiency and\nasymptotic performance.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 03:34:09 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 13:58:33 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Lai", "Hang", ""], ["Shen", "Jian", ""], ["Zhang", "Weinan", ""], ["Yu", "Yong", ""]]}, {"id": "2007.02007", "submitter": "Nozomi Hata", "authors": "Nozomi Hata, Shizuo Kaji, Akihiro Yoshida, Katsuki Fujisawa", "title": "Nested Subspace Arrangement for Representation of Relational Data", "comments": "11 pages, 13 figures, ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies on acquiring appropriate continuous representations of discrete\nobjects, such as graphs and knowledge base data, have been conducted by many\nresearchers in the field of machine learning. In this study, we introduce\nNested SubSpace (NSS) arrangement, a comprehensive framework for representation\nlearning. We show that existing embedding techniques can be regarded as special\ncases of the NSS arrangement. Based on the concept of the NSS arrangement, we\nimplement a Disk-ANChor ARrangement (DANCAR), a representation learning method\nspecialized to reproducing general graphs. Numerical experiments have shown\nthat DANCAR has successfully embedded WordNet in ${\\mathbb R}^{20}$ with an F1\nscore of 0.993 in the reconstruction task. DANCAR is also suitable for\nvisualization in understanding the characteristics of graphs.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 04:07:14 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Hata", "Nozomi", ""], ["Kaji", "Shizuo", ""], ["Yoshida", "Akihiro", ""], ["Fujisawa", "Katsuki", ""]]}, {"id": "2007.02010", "submitter": "Yanwei Fu", "authors": "Yanwei Fu, Chen Liu, Donghao Li, Xinwei Sun, Jinshan Zeng, Yuan Yao", "title": "DessiLBI: Exploring Structural Sparsity of Deep Networks via\n  Differential Inclusion Paths", "comments": "conference , 23 pages https://github.com/corwinliu9669/dS2LBI", "journal-ref": "ICML 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG math.DS stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over-parameterization is ubiquitous nowadays in training neural networks to\nbenefit both optimization in seeking global optima and generalization in\nreducing prediction error. However, compressive networks are desired in many\nreal world applications and direct training of small networks may be trapped in\nlocal optima. In this paper, instead of pruning or distilling\nover-parameterized models to compressive ones, we propose a new approach based\non differential inclusions of inverse scale spaces. Specifically, it generates\na family of models from simple to complex ones that couples a pair of\nparameters to simultaneously train over-parameterized deep models and\nstructural sparsity on weights of fully connected and convolutional layers.\nSuch a differential inclusion scheme has a simple discretization, proposed as\nDeep structurally splitting Linearized Bregman Iteration (DessiLBI), whose\nglobal convergence analysis in deep learning is established that from any\ninitializations, algorithmic iterations converge to a critical point of\nempirical risks. Experimental evidence shows that DessiLBI achieve comparable\nand even better performance than the competitive optimizers in exploring the\nstructural sparsity of several widely used backbones on the benchmark datasets.\nRemarkably, with early stopping, DessiLBI unveils \"winning tickets\" in early\nepochs: the effective sparse structure with comparable test accuracy to fully\ntrained over-parameterized models.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 04:40:16 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Fu", "Yanwei", ""], ["Liu", "Chen", ""], ["Li", "Donghao", ""], ["Sun", "Xinwei", ""], ["Zeng", "Jinshan", ""], ["Yao", "Yuan", ""]]}, {"id": "2007.02040", "submitter": "Ron Amit", "authors": "Ron Amit, Ron Meir, Kamil Ciosek", "title": "Discount Factor as a Regularizer in Reinforcement Learning", "comments": "Published in ICML 2020", "journal-ref": "Published in Proceedings of the 37th International Conference on\n  Machine Learning, Vienna, Austria, PMLR 119, 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Specifying a Reinforcement Learning (RL) task involves choosing a suitable\nplanning horizon, which is typically modeled by a discount factor. It is known\nthat applying RL algorithms with a lower discount factor can act as a\nregularizer, improving performance in the limited data regime. Yet the exact\nnature of this regularizer has not been investigated. In this work, we fill in\nthis gap. For several Temporal-Difference (TD) learning methods, we show an\nexplicit equivalence between using a reduced discount factor and adding an\nexplicit regularization term to the algorithm's loss. Motivated by the\nequivalence, we empirically study this technique compared to standard $L_2$\nregularization by extensive experiments in discrete and continuous domains,\nusing tabular and functional representations. Our experiments suggest the\nregularization effectiveness is strongly related to properties of the available\ndata, such as size, distribution, and mixing rate.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 08:10:09 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Amit", "Ron", ""], ["Meir", "Ron", ""], ["Ciosek", "Kamil", ""]]}, {"id": "2007.02047", "submitter": "Haiping Huang", "authors": "Zijian Jiang, Jianwen Zhou, and Haiping Huang", "title": "Relationship between manifold smoothness and adversarial vulnerability\n  in deep learning with local errors", "comments": "10 pages, 8 figures, to appear in Chin. Phys. B (2021)", "journal-ref": "Chin. Phys. B Vol. 30, No. 4 (2021) 048702", "doi": "10.1088/1674-1056/abd68e", "report-no": null, "categories": "cs.LG cond-mat.dis-nn cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks can achieve impressive performances, and even\noutperform humans in some specific tasks. Nevertheless, unlike biological\nbrains, the artificial neural networks suffer from tiny perturbations in\nsensory input, under various kinds of adversarial attacks. It is therefore\nnecessary to study the origin of the adversarial vulnerability. Here, we\nestablish a fundamental relationship between geometry of hidden representations\n(manifold perspective) and the generalization capability of the deep networks.\nFor this purpose, we choose a deep neural network trained by local errors, and\nthen analyze emergent properties of trained networks through the manifold\ndimensionality, manifold smoothness, and the generalization capability. To\nexplore effects of adversarial examples, we consider independent Gaussian noise\nattacks and fast-gradient-sign-method (FGSM) attacks. Our study reveals that a\nhigh generalization accuracy requires a relatively fast power-law decay of the\neigen-spectrum of hidden representations. Under Gaussian attacks, the\nrelationship between generalization accuracy and power-law exponent is\nmonotonic, while a non-monotonic behavior is observed for FGSM attacks. Our\nempirical study provides a route towards a final mechanistic interpretation of\nadversarial vulnerability under adversarial attacks.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 08:47:51 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 05:27:53 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Jiang", "Zijian", ""], ["Zhou", "Jianwen", ""], ["Huang", "Haiping", ""]]}, {"id": "2007.02056", "submitter": "Chuan Ma", "authors": "Chuan Ma, Jun Li, Ming Ding, Bo Liu, Kang Wei, Jian Weng and H.\n  Vincent Poor", "title": "RDP-GAN: A R\\'enyi-Differential Privacy based Generative Adversarial\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial network (GAN) has attracted increasing attention\nrecently owing to its impressive ability to generate realistic samples with\nhigh privacy protection. Without directly interactive with training examples,\nthe generative model can be fully used to estimate the underlying distribution\nof an original dataset while the discriminative model can examine the quality\nof the generated samples by comparing the label values with the training\nexamples. However, when GANs are applied on sensitive or private training\nexamples, such as medical or financial records, it is still probable to divulge\nindividuals' sensitive and private information. To mitigate this information\nleakage and construct a private GAN, in this work we propose a\nR\\'enyi-differentially private-GAN (RDP-GAN), which achieves differential\nprivacy (DP) in a GAN by carefully adding random noises on the value of the\nloss function during training. Moreover, we derive the analytical results of\nthe total privacy loss under the subsampling method and cumulated iterations,\nwhich show its effectiveness on the privacy budget allocation. In addition, in\norder to mitigate the negative impact brought by the injecting noise, we\nenhance the proposed algorithm by adding an adaptive noise tuning step, which\nwill change the volume of added noise according to the testing accuracy.\nThrough extensive experimental results, we verify that the proposed algorithm\ncan achieve a better privacy level while producing high-quality samples\ncompared with a benchmark DP-GAN scheme based on noise perturbation on training\ngradients.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 09:51:02 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Ma", "Chuan", ""], ["Li", "Jun", ""], ["Ding", "Ming", ""], ["Liu", "Bo", ""], ["Wei", "Kang", ""], ["Weng", "Jian", ""], ["Poor", "H. Vincent", ""]]}, {"id": "2007.02103", "submitter": "Yihuang Kang", "authors": "Bowen Kuo, Yihuang Kang, Pinghsung Wu, Sheng-Tai Huang, Yajie Huang", "title": "Discovering Drug-Drug and Drug-Disease Interactions Inducing Acute\n  Kidney Injury Using Deep Rule Forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Patients with Acute Kidney Injury (AKI) increase mortality, morbidity, and\nlong-term adverse events. Therefore, early identification of AKI may improve\nrenal function recovery, decrease comorbidities, and further improve patients'\nsurvival. To control certain risk factors and develop targeted prevention\nstrategies are important to reduce the risk of AKI. Drug-drug interactions and\ndrug-disease interactions are critical issues for AKI. Typical statistical\napproaches cannot handle the complexity of drug-drug and drug-disease\ninteractions. In this paper, we propose a novel learning algorithm, Deep Rule\nForests (DRF), which discovers rules from multilayer tree models as the\ncombinations of drug usages and disease indications to help identify such\ninteractions. We found that several disease and drug usages are considered\nhaving significant impact on the occurrence of AKI. Our experimental results\nalso show that the DRF model performs comparatively better than typical\ntree-based and other state-of-the-art algorithms in terms of prediction\naccuracy and model interpretability.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 14:10:28 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Kuo", "Bowen", ""], ["Kang", "Yihuang", ""], ["Wu", "Pinghsung", ""], ["Huang", "Sheng-Tai", ""], ["Huang", "Yajie", ""]]}, {"id": "2007.02126", "submitter": "Hengguan Huang", "authors": "Hengguan Huang, Fuzhao Xue, Hao Wang, Ye Wang", "title": "Deep Graph Random Process for Relational-Thinking-Based Speech\n  Recognition", "comments": "Accepted at ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lying at the core of human intelligence, relational thinking is characterized\nby initially relying on innumerable unconscious percepts pertaining to\nrelations between new sensory signals and prior knowledge, consequently\nbecoming a recognizable concept or object through coupling and transformation\nof these percepts. Such mental processes are difficult to model in real-world\nproblems such as in conversational automatic speech recognition (ASR), as the\npercepts (if they are modelled as graphs indicating relationships among\nutterances) are supposed to be innumerable and not directly observable. In this\npaper, we present a Bayesian nonparametric deep learning method called deep\ngraph random process (DGP) that can generate an infinite number of\nprobabilistic graphs representing percepts. We further provide a closed-form\nsolution for coupling and transformation of these percept graphs for acoustic\nmodeling. Our approach is able to successfully infer relations among utterances\nwithout using any relational data during training. Experimental evaluations on\nASR tasks including CHiME-2 and CHiME-5 demonstrate the effectiveness and\nbenefits of our method.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 15:27:57 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 09:03:55 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Huang", "Hengguan", ""], ["Xue", "Fuzhao", ""], ["Wang", "Hao", ""], ["Wang", "Ye", ""]]}, {"id": "2007.02133", "submitter": "Ming Chen", "authors": "Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, Yaliang Li", "title": "Simple and Deep Graph Convolutional Networks", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph convolutional networks (GCNs) are a powerful deep learning approach for\ngraph-structured data. Recently, GCNs and subsequent variants have shown\nsuperior performance in various application areas on real-world datasets.\nDespite their success, most of the current GCN models are shallow, due to the\n{\\em over-smoothing} problem. In this paper, we study the problem of designing\nand analyzing deep graph convolutional networks. We propose the GCNII, an\nextension of the vanilla GCN model with two simple yet effective techniques:\n{\\em Initial residual} and {\\em Identity mapping}. We provide theoretical and\nempirical evidence that the two techniques effectively relieves the problem of\nover-smoothing. Our experiments show that the deep GCNII model outperforms the\nstate-of-the-art methods on various semi- and full-supervised tasks. Code is\navailable at https://github.com/chennnM/GCNII .\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 16:18:06 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Chen", "Ming", ""], ["Wei", "Zhewei", ""], ["Huang", "Zengfeng", ""], ["Ding", "Bolin", ""], ["Li", "Yaliang", ""]]}, {"id": "2007.02141", "submitter": "Kenshi Abe", "authors": "Kenshi Abe, Yusuke Kaneko", "title": "Off-Policy Exploitability-Evaluation in Two-Player Zero-Sum Markov Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Off-policy evaluation (OPE) is the problem of evaluating new policies using\nhistorical data obtained from a different policy. In the recent OPE context,\nmost studies have focused on single-player cases, and not on multi-player\ncases. In this study, we propose OPE estimators constructed by the doubly\nrobust and double reinforcement learning estimators in two-player zero-sum\nMarkov games. The proposed estimators project exploitability that is often used\nas a metric for determining how close a policy profile (i.e., a tuple of\npolicies) is to a Nash equilibrium in two-player zero-sum games. We prove the\nexploitability estimation error bounds for the proposed estimators. We then\npropose the methods to find the best candidate policy profile by selecting the\npolicy profile that minimizes the estimated exploitability from a given policy\nprofile class. We prove the regret bounds of the policy profiles selected by\nour methods. Finally, we demonstrate the effectiveness and performance of the\nproposed estimators through experiments.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 16:51:56 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2020 08:24:37 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Abe", "Kenshi", ""], ["Kaneko", "Yusuke", ""]]}, {"id": "2007.02151", "submitter": "Amrit Singh Bedi", "authors": "Junyu Zhang, Alec Koppel, Amrit Singh Bedi, Csaba Szepesvari, and\n  Mengdi Wang", "title": "Variational Policy Gradient Method for Reinforcement Learning with\n  General Utilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, reinforcement learning (RL) systems with general goals\nbeyond a cumulative sum of rewards have gained traction, such as in constrained\nproblems, exploration, and acting upon prior experiences. In this paper, we\nconsider policy optimization in Markov Decision Problems, where the objective\nis a general concave utility function of the state-action occupancy measure,\nwhich subsumes several of the aforementioned examples as special cases. Such\ngenerality invalidates the Bellman equation. As this means that dynamic\nprogramming no longer works, we focus on direct policy search. Analogously to\nthe Policy Gradient Theorem \\cite{sutton2000policy} available for RL with\ncumulative rewards, we derive a new Variational Policy Gradient Theorem for RL\nwith general utilities, which establishes that the parametrized policy gradient\nmay be obtained as the solution of a stochastic saddle point problem involving\nthe Fenchel dual of the utility function. We develop a variational Monte Carlo\ngradient estimation algorithm to compute the policy gradient based on sample\npaths. We prove that the variational policy gradient scheme converges globally\nto the optimal policy for the general objective, though the optimization\nproblem is nonconvex. We also establish its rate of convergence of the order\n$O(1/t)$ by exploiting the hidden convexity of the problem, and proves that it\nconverges exponentially when the problem admits hidden strong convexity. Our\nanalysis applies to the standard RL problem with cumulative rewards as a\nspecial case, in which case our result improves the available convergence rate.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 17:51:53 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Zhang", "Junyu", ""], ["Koppel", "Alec", ""], ["Bedi", "Amrit Singh", ""], ["Szepesvari", "Csaba", ""], ["Wang", "Mengdi", ""]]}, {"id": "2007.02156", "submitter": "Cong Mu", "authors": "Cong Mu, Angelo Mele, Lingxin Hao, Joshua Cape, Avanti Athreya, Carey\n  E. Priebe", "title": "On spectral algorithms for community detection in stochastic blockmodel\n  graphs with vertex covariates", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In network inference applications, it is often desirable to detect community\nstructure, namely to cluster vertices into groups, or blocks, according to some\nmeasure of similarity. Beyond mere adjacency matrices, many real networks also\ninvolve vertex covariates that carry key information about underlying block\nstructure in graphs. To assess the effects of such covariates on block\nrecovery, we present a comparative analysis of two model-based spectral\nalgorithms for clustering vertices in stochastic blockmodel graphs with vertex\ncovariates. The first algorithm uses only the adjacency matrix, and directly\nestimates the induced block assignments. The second algorithm incorporates both\nthe adjacency matrix and the vertex covariates into the estimation of block\nassignments, and moreover quantifies the explicit impact of the vertex\ncovariates on the resulting estimate of the block assignments. We employ\nChernoff information to analytically compare the algorithms' performance and\nderive the Chernoff ratio for certain models of interest. Analytic results and\nsimulations suggest that the second algorithm is often preferred: we can often\nbetter estimate the induced block assignments by first estimating the effect of\nvertex covariates. In addition, real data examples on diffusion MRI connectome\ndatasets and social network datasets also indicate that the second algorithm\nhas the advantages of revealing underlying block structure and taking observed\nvertex heterogeneity into account in real applications. Our findings emphasize\nthe importance of distinguishing between observed and unobserved factors that\ncan affect block structure in graphs.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 18:22:22 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 00:31:53 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Mu", "Cong", ""], ["Mele", "Angelo", ""], ["Hao", "Lingxin", ""], ["Cape", "Joshua", ""], ["Athreya", "Avanti", ""], ["Priebe", "Carey E.", ""]]}, {"id": "2007.02168", "submitter": "Yi-Ling Qiao", "authors": "Yi-Ling Qiao, Junbang Liang, Vladlen Koltun, Ming C. Lin", "title": "Scalable Differentiable Physics for Learning and Control", "comments": null, "journal-ref": "Proceedings of the 37th International Conference on Machine\n  Learning, ICML 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentiable physics is a powerful approach to learning and control\nproblems that involve physical objects and environments. While notable progress\nhas been made, the capabilities of differentiable physics solvers remain\nlimited. We develop a scalable framework for differentiable physics that can\nsupport a large number of objects and their interactions. To accommodate\nobjects with arbitrary geometry and topology, we adopt meshes as our\nrepresentation and leverage the sparsity of contacts for scalable\ndifferentiable collision handling. Collisions are resolved in localized regions\nto minimize the number of optimization variables even when the number of\nsimulated objects is high. We further accelerate implicit differentiation of\noptimization with nonlinear constraints. Experiments demonstrate that the\npresented framework requires up to two orders of magnitude less memory and\ncomputation in comparison to recent particle-based methods. We further validate\nthe approach on inverse problems and control scenarios, where it outperforms\nderivative-free and model-free baselines by at least an order of magnitude.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 19:07:51 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Qiao", "Yi-Ling", ""], ["Liang", "Junbang", ""], ["Koltun", "Vladlen", ""], ["Lin", "Ming C.", ""]]}, {"id": "2007.02192", "submitter": "Se Yoon Lee", "authors": "Se Yoon Lee, Debdeep Pati, Bani K. Mallick", "title": "Tail-adaptive Bayesian shrinkage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern genomic studies are increasingly focused on discovering more and more\ninteresting genes associated with a health response. Traditional shrinkage\npriors are primarily designed to detect a handful of signals from tens and\nthousands of predictors. Under diverse sparsity regimes, the nature of signal\ndetection is associated with a tail behaviour of a prior. A desirable tail\nbehaviour is called tail-adaptive shrinkage property where tail-heaviness of a\nprior gets adaptively larger (or smaller) as a sparsity level increases (or\ndecreases) to accommodate more (or less) signals. We propose a\nglobal-local-tail (GLT) Gaussian mixture distribution to ensure this property\nand provide accurate inference under diverse sparsity regimes. Incorporating a\npeaks-over-threshold method in extreme value theory, we develop an automated\ntail learning algorithm for the GLT prior. We compare the performance of the\nGLT prior to the Horseshoe in two gene expression datasets and numerical\nexamples. Results suggest that varying tail rule is advantageous over fixed\ntail rule under diverse sparsity domains.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 21:40:12 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 03:47:49 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Lee", "Se Yoon", ""], ["Pati", "Debdeep", ""], ["Mallick", "Bani K.", ""]]}, {"id": "2007.02196", "submitter": "Jaya Krishna Mandivarapu Mr", "authors": "Jaya Krishna Mandivarapu, Blake Camp, Rolando Estrada", "title": "Deep Active Learning via Open Set Recognition", "comments": "Withdrawn to address fundamental concerns with the text", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, data is easy to acquire but expensive and\ntime-consuming to label prominent examples include medical imaging and NLP.\nThis disparity has only grown in recent years as our ability to collect data\nimproves. Under these constraints, it makes sense to select only the most\ninformative instances from the unlabeled pool and request an oracle (e.g., a\nhuman expert) to provide labels for those samples. The goal of active learning\nis to infer the informativeness of unlabeled samples so as to minimize the\nnumber of requests to the oracle. Here, we formulate active learning as an\nopen-set recognition problem. In this paradigm, only some of the inputs belong\nto known classes; the classifier must identify the rest as unknown. More\nspecifically, we leverage variational neural networks (VNNs), which produce\nhigh-confidence (i.e., low-entropy) predictions only for inputs that closely\nresemble the training data. We use the inverse of this confidence measure to\nselect the samples that the oracle should label. Intuitively, unlabeled samples\nthat the VNN is uncertain about are more informative for future training. We\ncarried out an extensive evaluation of our novel, probabilistic formulation of\nactive learning, achieving state-of-the-art results on MNIST, CIFAR-10, and\nCIFAR-100. Additionally, unlike current active learning methods, our algorithm\ncan learn tasks without the need for task labels. As our experiments show, when\nthe unlabeled pool consists of a mixture of samples from multiple datasets, our\napproach can automatically distinguish between samples from seen vs. unseen\ntasks.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 22:09:17 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 20:56:01 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 14:27:20 GMT"}, {"version": "v4", "created": "Mon, 5 Apr 2021 18:47:17 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Mandivarapu", "Jaya Krishna", ""], ["Camp", "Blake", ""], ["Estrada", "Rolando", ""]]}, {"id": "2007.02235", "submitter": "Yu-Ting Chou", "authors": "Yu-Ting Chou, Gang Niu, Hsuan-Tien Lin, Masashi Sugiyama", "title": "Unbiased Risk Estimators Can Mislead: A Case Study of Learning with\n  Complementary Labels", "comments": "Accepted at ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In weakly supervised learning, unbiased risk estimator(URE) is a powerful\ntool for training classifiers when training and test data are drawn from\ndifferent distributions. Nevertheless, UREs lead to overfitting in many problem\nsettings when the models are complex like deep networks. In this paper, we\ninvestigate reasons for such overfitting by studying a weakly supervised\nproblem called learning with complementary labels. We argue the quality of\ngradient estimation matters more in risk minimization. Theoretically, we show\nthat a URE gives an unbiased gradient estimator(UGE). Practically, however,\nUGEs may suffer from huge variance, which causes empirical gradients to be\nusually far away from true gradients during minimization. To this end, we\npropose a novel surrogate complementary loss(SCL) framework that trades zero\nbias with reduced variance and makes empirical gradients more aligned with true\ngradients in the direction. Thanks to this characteristic, SCL successfully\nmitigates the overfitting issue and improves URE-based methods.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 04:19:37 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 08:28:25 GMT"}, {"version": "v3", "created": "Fri, 21 Aug 2020 18:11:55 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Chou", "Yu-Ting", ""], ["Niu", "Gang", ""], ["Lin", "Hsuan-Tien", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "2007.02334", "submitter": "Kyuyong Shin", "authors": "Kyuyong Shin, Young-Jin Park, Kyung-Min Kim, Sunyoung Kwon", "title": "Multi-Manifold Learning for Large-scale Targeted Advertising System", "comments": "Accepted at AdKDD 2020", "journal-ref": "AdKDD 2020", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Messenger advertisements (ads) give direct and personal user experience\nyielding high conversion rates and sales. However, people are skeptical about\nads and sometimes perceive them as spam, which eventually leads to a decrease\nin user satisfaction. Targeted advertising, which serves ads to individuals who\nmay exhibit interest in a particular advertising message, is strongly required.\nThe key to the success of precise user targeting lies in learning the accurate\nuser and ad representation in the embedding space. Most of the previous studies\nhave limited the representation learning in the Euclidean space, but recent\nstudies have suggested hyperbolic manifold learning for the distinct projection\nof complex network properties emerging from real-world datasets such as social\nnetworks, recommender systems, and advertising. We propose a framework that can\neffectively learn the hierarchical structure in users and ads on the hyperbolic\nspace, and extend to the Multi-Manifold Learning. Our method constructs\nmultiple hyperbolic manifolds with learnable curvatures and maps the\nrepresentation of user and ad to each manifold. The origin of each manifold is\nset as the centroid of each user cluster. The user preference for each ad is\nestimated using the distance between two entities in the hyperbolic space, and\nthe final prediction is determined by aggregating the values calculated from\nthe learned multiple manifolds. We evaluate our method on public benchmark\ndatasets and a large-scale commercial messenger system LINE, and demonstrate\nits effectiveness through improved performance.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 13:31:43 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 04:34:59 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Shin", "Kyuyong", ""], ["Park", "Young-Jin", ""], ["Kim", "Kyung-Min", ""], ["Kwon", "Sunyoung", ""]]}, {"id": "2007.02376", "submitter": "Zilong Bai", "authors": "Zilong Bai, Hoa Nguyen, Ian Davidson", "title": "Block Model Guided Unsupervised Feature Selection", "comments": "Published at KDD2020", "journal-ref": "Proceedings of the 26th ACM SIGKDD Conference on Knowledge\n  Discovery and Data Mining (KDD2020)", "doi": "10.1145/3394486.3403173", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is a core area of data mining with a recent innovation of\ngraph-driven unsupervised feature selection for linked data. In this setting we\nhave a dataset $\\mathbf{Y}$ consisting of $n$ instances each with $m$ features\nand a corresponding $n$ node graph (whose adjacency matrix is $\\mathbf{A}$)\nwith an edge indicating that the two instances are similar. Existing efforts\nfor unsupervised feature selection on attributed networks have explored either\ndirectly regenerating the links by solving for $f$ such that\n$f(\\mathbf{y}_i,\\mathbf{y}_j) \\approx \\mathbf{A}_{i,j}$ or finding community\nstructure in $\\mathbf{A}$ and using the features in $\\mathbf{Y}$ to predict\nthese communities. However, graph-driven unsupervised feature selection remains\nan understudied area with respect to exploring more complex guidance. Here we\ntake the novel approach of first building a block model on the graph and then\nusing the block model for feature selection. That is, we discover\n$\\mathbf{F}\\mathbf{M}\\mathbf{F}^T \\approx \\mathbf{A}$ and then find a subset of\nfeatures $\\mathcal{S}$ that induces another graph to preserve both $\\mathbf{F}$\nand $\\mathbf{M}$. We call our approach Block Model Guided Unsupervised Feature\nSelection (BMGUFS). Experimental results show that our method outperforms the\nstate of the art on several real-world public datasets in finding high-quality\nfeatures for clustering.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 16:19:47 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Bai", "Zilong", ""], ["Nguyen", "Hoa", ""], ["Davidson", "Ian", ""]]}, {"id": "2007.02382", "submitter": "Michael Chang", "authors": "Michael Chang, Sidhant Kaushik, S. Matthew Weinberg, Thomas L.\n  Griffiths, Sergey Levine", "title": "Decentralized Reinforcement Learning: Global Decision-Making via Local\n  Economic Transactions", "comments": "18 pages, 13 figures, accepted to the International Conference on\n  Machine Learning (ICML) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT cs.MA cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper seeks to establish a framework for directing a society of simple,\nspecialized, self-interested agents to solve what traditionally are posed as\nmonolithic single-agent sequential decision problems. What makes it challenging\nto use a decentralized approach to collectively optimize a central objective is\nthe difficulty in characterizing the equilibrium strategy profile of\nnon-cooperative games. To overcome this challenge, we design a mechanism for\ndefining the learning environment of each agent for which we know that the\noptimal solution for the global objective coincides with a Nash equilibrium\nstrategy profile of the agents optimizing their own local objectives. The\nsociety functions as an economy of agents that learn the credit assignment\nprocess itself by buying and selling to each other the right to operate on the\nenvironment state. We derive a class of decentralized reinforcement learning\nalgorithms that are broadly applicable not only to standard reinforcement\nlearning but also for selecting options in semi-MDPs and dynamically composing\ncomputation graphs. Lastly, we demonstrate the potential advantages of a\nsociety's inherent modular structure for more efficient transfer learning.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 16:41:09 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 05:20:29 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Chang", "Michael", ""], ["Kaushik", "Sidhant", ""], ["Weinberg", "S. Matthew", ""], ["Griffiths", "Thomas L.", ""], ["Levine", "Sergey", ""]]}, {"id": "2007.02387", "submitter": "Meng Qu", "authors": "Meng Qu, Tianyu Gao, Louis-Pascal A. C. Xhonneux, Jian Tang", "title": "Few-shot Relation Extraction via Bayesian Meta-learning on Relation\n  Graphs", "comments": "icml2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies few-shot relation extraction, which aims at predicting the\nrelation for a pair of entities in a sentence by training with a few labeled\nexamples in each relation. To more effectively generalize to new relations, in\nthis paper we study the relationships between different relations and propose\nto leverage a global relation graph. We propose a novel Bayesian meta-learning\napproach to effectively learn the posterior distribution of the prototype\nvectors of relations, where the initial prior of the prototype vectors is\nparameterized with a graph neural network on the global relation graph.\nMoreover, to effectively optimize the posterior distribution of the prototype\nvectors, we propose to use the stochastic gradient Langevin dynamics, which is\nrelated to the MAML algorithm but is able to handle the uncertainty of the\nprototype vectors. The whole framework can be effectively and efficiently\noptimized in an end-to-end fashion. Experiments on two benchmark datasets prove\nthe effectiveness of our proposed approach against competitive baselines in\nboth the few-shot and zero-shot settings.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 17:04:41 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Qu", "Meng", ""], ["Gao", "Tianyu", ""], ["Xhonneux", "Louis-Pascal A. C.", ""], ["Tang", "Jian", ""]]}, {"id": "2007.02392", "submitter": "Alkis Kalavasis", "authors": "Dimitris Fotakis, Alkis Kalavasis, Christos Tzamos", "title": "Efficient Parameter Estimation of Truncated Boolean Product\n  Distributions", "comments": "33 pages, 33rd Conference on Learning Theory (COLT 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the parameters of a Boolean product\ndistribution in $d$ dimensions, when the samples are truncated by a set $S\n\\subset \\{0, 1\\}^d$ accessible through a membership oracle. This is the first\ntime that the computational and statistical complexity of learning from\ntruncated samples is considered in a discrete setting.\n  We introduce a natural notion of fatness of the truncation set $S$, under\nwhich truncated samples reveal enough information about the true distribution.\nWe show that if the truncation set is sufficiently fat, samples from the true\ndistribution can be generated from truncated samples. A stunning consequence is\nthat virtually any statistical task (e.g., learning in total variation\ndistance, parameter estimation, uniformity or identity testing) that can be\nperformed efficiently for Boolean product distributions, can also be performed\nfrom truncated samples, with a small increase in sample complexity. We\ngeneralize our approach to ranking distributions over $d$ alternatives, where\nwe show how fatness implies efficient parameter estimation of Mallows models\nfrom truncated samples.\n  Exploring the limits of learning discrete models from truncated samples, we\nidentify three natural conditions that are necessary for efficient\nidentifiability: (i) the truncation set $S$ should be rich enough; (ii) $S$\nshould be accessible through membership queries; and (iii) the truncation by\n$S$ should leave enough randomness in all directions. By carefully adapting the\nStochastic Gradient Descent approach of (Daskalakis et al., FOCS 2018), we show\nthat these conditions are also sufficient for efficient learning of truncated\nBoolean product distributions.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 17:20:39 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Fotakis", "Dimitris", ""], ["Kalavasis", "Alkis", ""], ["Tzamos", "Christos", ""]]}, {"id": "2007.02394", "submitter": "Yulin Wang", "authors": "Yulin Wang, Jiayi Guo, Shiji Song, Gao Huang", "title": "Meta-Semi: A Meta-learning Approach for Semi-supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based semi-supervised learning (SSL) algorithms have led to\npromising results in recent years. However, they tend to introduce multiple\ntunable hyper-parameters, making them less practical in real SSL scenarios\nwhere the labeled data is scarce for extensive hyper-parameter search. In this\npaper, we propose a novel meta-learning based SSL algorithm (Meta-Semi) that\nrequires tuning only one additional hyper-parameter, compared with a standard\nsupervised deep learning algorithm, to achieve competitive performance under\nvarious conditions of SSL. We start by defining a meta optimization problem\nthat minimizes the loss on labeled data through dynamically reweighting the\nloss on unlabeled samples, which are associated with soft pseudo labels during\ntraining. As the meta problem is computationally intensive to solve directly,\nwe propose an efficient algorithm to dynamically obtain the approximate\nsolutions. We show theoretically that Meta-Semi converges to the stationary\npoint of the loss function on labeled data under mild conditions. Empirically,\nMeta-Semi outperforms state-of-the-art SSL algorithms significantly on the\nchallenging semi-supervised CIFAR-100 and STL-10 tasks, and achieves\ncompetitive performance on CIFAR-10 and SVHN.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 17:31:14 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Wang", "Yulin", ""], ["Guo", "Jiayi", ""], ["Song", "Shiji", ""], ["Huang", "Gao", ""]]}, {"id": "2007.02407", "submitter": "Ishai Rosenberg", "authors": "Ihai Rosenberg and Asaf Shabtai and Yuval Elovici and Lior Rokach", "title": "Adversarial Machine Learning Attacks and Defense Methods in the Cyber\n  Security Domain", "comments": "Accepted as a long survey paper at ACM CSUR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years machine learning algorithms, and more specifically deep\nlearning algorithms, have been widely used in many fields, including cyber\nsecurity. However, machine learning systems are vulnerable to adversarial\nattacks, and this limits the application of machine learning, especially in\nnon-stationary, adversarial environments, such as the cyber security domain,\nwhere actual adversaries (e.g., malware developers) exist. This paper\ncomprehensively summarizes the latest research on adversarial attacks against\nsecurity solutions based on machine learning techniques and illuminates the\nrisks they pose. First, the adversarial attack methods are characterized based\non their stage of occurrence, and the attacker's goals and capabilities. Then,\nwe categorize the applications of adversarial attack and defense methods in the\ncyber security domain. Finally, we highlight some characteristics identified in\nrecent research and discuss the impact of recent advancements in other\nadversarial learning domains on future research directions in the cyber\nsecurity domain. This paper is the first to discuss the unique challenges of\nimplementing end-to-end adversarial attacks in the cyber security domain, map\nthem in a unified taxonomy, and use the taxonomy to highlight future research\ndirections.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 18:22:40 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 00:38:42 GMT"}, {"version": "v3", "created": "Sat, 13 Mar 2021 19:31:59 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Rosenberg", "Ihai", ""], ["Shabtai", "Asaf", ""], ["Elovici", "Yuval", ""], ["Rokach", "Lior", ""]]}, {"id": "2007.02411", "submitter": "Hongseok Namkoong", "authors": "Sookyo Jeong, Hongseok Namkoong", "title": "Robust Causal Inference Under Covariate Shift via Worst-Case\n  Subpopulation Treatment Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the worst-case treatment effect (WTE) across all subpopulations of\na given size, a conservative notion of topline treatment effect. Compared to\nthe average treatment effect (ATE), whose validity relies on the covariate\ndistribution of collected data, WTE is robust to unanticipated covariate\nshifts, and positive findings guarantee uniformly valid treatment effects over\nsubpopulations. We develop a semiparametrically efficient estimator for the\nWTE, leveraging machine learning-based estimates of the heterogeneous treatment\neffect and propensity score. By virtue of satisfying a key (Neyman)\northogonality property, our estimator enjoys central limit behavior---oracle\nrates with true nuisance parameters---even when estimates of nuisance\nparameters converge at slower rates. For both randomized trials and\nobservational studies, we establish a semiparametric efficiency bound, proving\nthat our estimator achieves the optimal asymptotic variance. On real datasets\nwhere robustness to covariate shift is of core concern, we illustrate the\nnon-robustness of ATE under even mild distributional shift, and demonstrate\nthat the WTE guards against brittle findings that are invalidated by\nunanticipated covariate shifts.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 18:34:14 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 00:16:57 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Jeong", "Sookyo", ""], ["Namkoong", "Hongseok", ""]]}, {"id": "2007.02418", "submitter": "Zaheer Abbas", "authors": "Zaheer Abbas, Samuel Sokota, Erin J. Talvitie, Martha White", "title": "Selective Dyna-style Planning Under Limited Model Capacity", "comments": "Accepted at ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In model-based reinforcement learning, planning with an imperfect model of\nthe environment has the potential to harm learning progress. But even when a\nmodel is imperfect, it may still contain information that is useful for\nplanning. In this paper, we investigate the idea of using an imperfect model\nselectively. The agent should plan in parts of the state space where the model\nwould be helpful but refrain from using the model where it would be harmful. An\neffective selective planning mechanism requires estimating predictive\nuncertainty, which arises out of aleatoric uncertainty, parameter uncertainty,\nand model inadequacy, among other sources. Prior work has focused on parameter\nuncertainty for selective planning. In this work, we emphasize the importance\nof model inadequacy. We show that heteroscedastic regression can signal\npredictive uncertainty arising from model inadequacy that is complementary to\nthat which is detected by methods designed for parameter uncertainty,\nindicating that considering both parameter uncertainty and model inadequacy may\nbe a more promising direction for effective selective planning than either in\nisolation.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 18:51:50 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 22:32:28 GMT"}, {"version": "v3", "created": "Sun, 7 Mar 2021 21:52:28 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Abbas", "Zaheer", ""], ["Sokota", "Samuel", ""], ["Talvitie", "Erin J.", ""], ["White", "Martha", ""]]}, {"id": "2007.02422", "submitter": "Ali Siahkamari", "authors": "Ali Siahkamari, Aditya Gangrade, Brian Kulis and Venkatesh Saligrama", "title": "Piecewise Linear Regression via a Difference of Convex Functions", "comments": "Published in International Conference on Machine Learning (ICML2020)\n  Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new piecewise linear regression methodology that utilizes\nfitting a difference of convex functions (DC functions) to the data. These are\nfunctions $f$ that may be represented as the difference $\\phi_1 - \\phi_2$ for a\nchoice of convex functions $\\phi_1, \\phi_2$. The method proceeds by estimating\npiecewise-liner convex functions, in a manner similar to max-affine regression,\nwhose difference approximates the data. The choice of the function is\nregularised by a new seminorm over the class of DC functions that controls the\n$\\ell_\\infty$ Lipschitz constant of the estimate. The resulting methodology can\nbe efficiently implemented via Quadratic programming even in high dimensions,\nand is shown to have close to minimax statistical risk. We empirically validate\nthe method, showing it to be practically implementable, and to have comparable\nperformance to existing regression/classification methods on real-world\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 18:58:47 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 23:24:03 GMT"}, {"version": "v3", "created": "Fri, 13 Nov 2020 21:01:52 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Siahkamari", "Ali", ""], ["Gangrade", "Aditya", ""], ["Kulis", "Brian", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "2007.02439", "submitter": "Hui Ye", "authors": "Hui Ye, Zhiyu Chen, Da-Han Wang, Brian D. Davison", "title": "Pretrained Generalized Autoregressive Model with Adaptive Probabilistic\n  Label Clusters for Extreme Multi-label Text Classification", "comments": "Accepted by ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme multi-label text classification (XMTC) is a task for tagging a given\ntext with the most relevant labels from an extremely large label set. We\npropose a novel deep learning method called APLC-XLNet. Our approach fine-tunes\nthe recently released generalized autoregressive pretrained model (XLNet) to\nlearn a dense representation for the input text. We propose Adaptive\nProbabilistic Label Clusters (APLC) to approximate the cross entropy loss by\nexploiting the unbalanced label distribution to form clusters that explicitly\nreduce the computational time. Our experiments, carried out on five benchmark\ndatasets, show that our approach has achieved new state-of-the-art results on\nfour benchmark datasets. Our source code is available publicly at\nhttps://github.com/huiyegit/APLC_XLNet.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 20:19:29 GMT"}, {"version": "v2", "created": "Sat, 15 Aug 2020 01:41:34 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Ye", "Hui", ""], ["Chen", "Zhiyu", ""], ["Wang", "Da-Han", ""], ["Davison", "Brian D.", ""]]}, {"id": "2007.02443", "submitter": "Jary Pomponi", "authors": "Jary Pomponi, Simone Scardapane, Aurelio Uncini", "title": "Pseudo-Rehearsal for Continual Learning with Normalizing Flows", "comments": "Submitted to IEEE Transactions on Neural Networks and Learning\n  Systems. A preliminary unpublished version of this work was presented in the\n  LifelongML workshop, at ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Catastrophic forgetting (CF) happens whenever a neural network overwrites\npast knowledge while being trained on new tasks. Common techniques to handle CF\ninclude regularization of the weights (using, e.g., their importance on past\ntasks), and rehearsal strategies, where the network is constantly re-trained on\npast data. Generative models have also been applied for the latter, in order to\nhave endless sources of data. In this paper, we propose a novel method that\ncombines the strengths of regularization and generative-based rehearsal\napproaches. Our generative model consists of a normalizing flow (NF), a\nprobabilistic and invertible neural network, trained on the internal embeddings\nof the network. By keeping a single NF conditioned on the task, we show that\nour memory overhead remains constant. In addition, exploiting the invertibility\nof the NF, we propose a simple approach to regularize the network's embeddings\nwith respect to past tasks. We show that our method performs favorably with\nrespect to state-of-the-art approaches in the literature, with bounded\ncomputational power and memory overheads.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 20:43:52 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 17:33:26 GMT"}, {"version": "v3", "created": "Sat, 17 Jul 2021 09:53:28 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Pomponi", "Jary", ""], ["Scardapane", "Simone", ""], ["Uncini", "Aurelio", ""]]}, {"id": "2007.02445", "submitter": "Kirill Shevkunov", "authors": "Kirill Shevkunov and Liudmila Prokhorenkova", "title": "Overlapping Spaces for Compact Graph Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various non-trivial spaces are becoming popular for embedding structured data\nsuch as graphs, texts, or images. Following spherical and hyperbolic spaces,\nmore general product spaces have been proposed. However, searching for the best\nconfiguration of product space is a resource-intensive procedure, which reduces\nthe practical applicability of the idea. We generalize the concept of product\nspace and introduce an overlapping space that does not have the configuration\nsearch problem. The main idea is to allow subsets of coordinates to be shared\nbetween spaces of different types (Euclidean, hyperbolic, spherical). As a\nresult, parameter optimization automatically learns the optimal configuration.\nAdditionally, overlapping spaces allow for more compact representations since\ntheir geometry is more complex. Our experiments confirm that overlapping spaces\noutperform the competitors in graph embedding tasks. Here, we consider both\ndistortion setup, where the aim is to preserve distances, and ranking setup,\nwhere the relative order should be preserved. The proposed method effectively\nsolves the problem and outperforms the competitors in both settings. We also\nperform an empirical analysis in a realistic information retrieval task, where\nwe compare all spaces by incorporating them into DSSM. In this case, the\nproposed overlapping space consistently achieves nearly optimal results without\nany configuration tuning. This allows for reducing training time, which can be\nsignificant in large-scale applications.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 20:55:47 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 10:21:18 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Shevkunov", "Kirill", ""], ["Prokhorenkova", "Liudmila", ""]]}, {"id": "2007.02448", "submitter": "Mohammed Rayyan Sheriff", "authors": "Mohammed Rayyan Sheriff and Debasish Chatterjee", "title": "Novel min-max reformulations of Linear Inverse Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we dwell into the class of so-called ill-posed Linear\nInverse Problems (LIP) which simply refers to the task of recovering the entire\nsignal from its relatively few random linear measurements. Such problems arise\nin a variety of settings with applications ranging from medical image\nprocessing, recommender systems, etc. We propose a slightly generalized version\nof the error constrained linear inverse problem and obtain a novel and\nequivalent convex-concave min-max reformulation by providing an exposition to\nits convex geometry. Saddle points of the min-max problem are completely\ncharacterized in terms of a solution to the LIP, and vice versa. Applying\nsimple saddle point seeking ascend-descent type algorithms to solve the min-max\nproblems provides novel and simple algorithms to find a solution to the LIP.\nMoreover, the reformulation of an LIP as the min-max problem provided in this\narticle is crucial in developing methods to solve the dictionary learning\nproblem with almost sure recovery constraints.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 21:09:01 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Sheriff", "Mohammed Rayyan", ""], ["Chatterjee", "Debasish", ""]]}, {"id": "2007.02449", "submitter": "Marc Harper", "authors": "Marc Harper and Joshua Safyan", "title": "Momentum Accelerates Evolutionary Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.DS math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We combine momentum from machine learning with evolutionary dynamics, where\nmomentum can be viewed as a simple mechanism of intergenerational memory. Using\ninformation divergences as Lyapunov functions, we show that momentum\naccelerates the convergence of evolutionary dynamics including the replicator\nequation and Euclidean gradient descent on populations. When evolutionarily\nstable states are present, these methods prove convergence for small learning\nrates or small momentum, and yield an analytic determination of the relative\ndecrease in time to converge that agrees well with computations. The main\nresults apply even when the evolutionary dynamic is not a gradient flow. We\nalso show that momentum can alter the convergence properties of these dynamics,\nfor example by breaking the cycling associated to the rock-paper-scissors\nlandscape, leading to either convergence to the ordinarily non-absorbing\nequilibrium, or divergence, depending on the value and mechanism of momentum.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 21:09:30 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Harper", "Marc", ""], ["Safyan", "Joshua", ""]]}, {"id": "2007.02455", "submitter": "Xuekui Zhang", "authors": "Li Xing, Songwan Joun, Kurt Mackey, Mary Lesperance, and Xuekui Zhang", "title": "Handling high correlations in the feature gene selection using\n  Single-Cell RNA sequencing data", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Selecting feature genes and predicting cells' phenotype are\ntypical tasks in the analysis of scRNA-seq data. Many algorithms were developed\nfor these tasks, but high correlations among genes create challenges\nspecifically in scRNA-seq analysis, which are not well addressed. Highly\ncorrelated genes lead to unreliable prediction models due to technical\nproblems, such as multi-collinearity. Most importantly, when a causal gene\n(whose variants have a true biological effect on the phenotype) is highly\ncorrelated with other genes, most algorithms select one of them in a\ndata-driven manner. The correlation structure among genes could change\nsubstantially. Hence, it is critical to build a prediction model based on\ncausal genes.\n  Results: To address the issues discussed above, we propose a grouping\nalgorithm that can be integrated into prediction models. Using real benchmark\nscRNA-seq data and simulated cell phenotypes, we show our novel method\nsignificantly outperforms standard models in both prediction and feature\nselection. Our algorithm reports the whole group of correlated genes, allowing\nresearchers to either use their common pattern as a more robust predictor or\nconduct follow-up studies to identify the causal genes in the group.\n  Availability: An R package is being developed and will be available on the\nComprehensive R Archive Network (CRAN) when the paper is published.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 22:14:03 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 20:19:01 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Xing", "Li", ""], ["Joun", "Songwan", ""], ["Mackey", "Kurt", ""], ["Lesperance", "Mary", ""], ["Zhang", "Xuekui", ""]]}, {"id": "2007.02470", "submitter": "Guang Cheng", "authors": "Chi-Hua Wang, Zhanyu Wang, Will Wei Sun, Guang Cheng", "title": "Online Regularization for High-Dimensional Dynamic Pricing Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel \\textit{online regularization} scheme for\nrevenue-maximization in high-dimensional dynamic pricing algorithms. The online\nregularization scheme equips the proposed optimistic online regularized maximum\nlikelihood pricing (\\texttt{OORMLP}) algorithm with three major advantages:\nencode market noise knowledge into pricing process optimism; empower online\nstatistical learning with always-validity over all decision points; envelop\nprediction error process with time-uniform non-asymptotic oracle inequalities.\nThis type of non-asymptotic inference results allows us to design safer and\nmore robust dynamic pricing algorithms in practice. In theory, the proposed\n\\texttt{OORMLP} algorithm exploits the sparsity structure of high-dimensional\nmodels and obtains a logarithmic regret in a decision horizon. These\ntheoretical advances are made possible by proposing an optimistic online LASSO\nprocedure that resolves dynamic pricing problems at the \\textit{process} level,\nbased on a novel use of non-asymptotic martingale concentration. In\nexperiments, we evaluate \\texttt{OORMLP} in different synthetic pricing problem\nsettings and observe that \\texttt{OORMLP} performs better than \\texttt{RMLP}\nproposed in \\cite{javanmard2019dynamic}.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 23:52:09 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Wang", "Chi-Hua", ""], ["Wang", "Zhanyu", ""], ["Sun", "Will Wei", ""], ["Cheng", "Guang", ""]]}, {"id": "2007.02471", "submitter": "Mohammad Zalbagi Darestani", "authors": "Mohammad Zalbagi Darestani and Reinhard Heckel", "title": "Accelerated MRI with Un-trained Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are highly effective for image\nreconstruction problems. Typically, CNNs are trained on large amounts of\ntraining images. Recently, however, un-trained CNNs such as the Deep Image\nPrior and Deep Decoder have achieved excellent performance for image\nreconstruction problems such as denoising and inpainting, \\emph{without using\nany training data}. Motivated by this development, we address the\nreconstruction problem arising in accelerated MRI with un-trained neural\nnetworks. We propose a highly optimized un-trained recovery approach based on a\nvariation of the Deep Decoder and show that it significantly outperforms other\nun-trained methods, in particular sparsity-based classical compressed sensing\nmethods and naive applications of un-trained neural networks. We also compare\nperformance (both in terms of reconstruction accuracy and computational cost)\nin an ideal setup for trained methods, specifically on the fastMRI dataset,\nwhere the training and test data come from the same distribution. We find that\nour un-trained algorithm achieves similar performance to a baseline trained\nneural network, but a state-of-the-art trained network outperforms the\nun-trained one. Finally, we perform a comparison on a non-ideal setup where the\ntrain and test distributions are slightly different, and find that our\nun-trained method achieves similar performance to a state-of-the-art\naccelerated MRI reconstruction method.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 00:01:25 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 23:06:26 GMT"}, {"version": "v3", "created": "Tue, 27 Apr 2021 18:23:17 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Darestani", "Mohammad Zalbagi", ""], ["Heckel", "Reinhard", ""]]}, {"id": "2007.02486", "submitter": "Guang Cheng", "authors": "Wenjia Wang, Tianyang Hu, Cong Lin, Guang Cheng", "title": "Regularization Matters: A Nonparametric Perspective on Overparametrized\n  Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overparametrized neural networks trained by gradient descent (GD) can\nprovably overfit any training data. However, the generalization guarantee may\nnot hold for noisy data. From a nonparametric perspective, this paper studies\nhow well overparametrized neural networks can recover the true target function\nin the presence of random noises. We establish a lower bound on the $L_2$\nestimation error with respect to the GD iteration, which is away from zero\nwithout a delicate choice of early stopping. In turn, through a comprehensive\nanalysis of $\\ell_2$-regularized GD trajectories, we prove that for\noverparametrized one-hidden-layer ReLU neural network with the $\\ell_2$\nregularization: (1) the output is close to that of the kernel ridge regression\nwith the corresponding neural tangent kernel; (2) minimax {optimal} rate of\n$L_2$ estimation error is achieved. Numerical experiments confirm our theory\nand further demonstrate that the $\\ell_2$ regularization approach improves the\ntraining robustness and works for a wider range of neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 01:02:23 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Wang", "Wenjia", ""], ["Hu", "Tianyang", ""], ["Lin", "Cong", ""], ["Cheng", "Guang", ""]]}, {"id": "2007.02500", "submitter": "Guansong Pang", "authors": "Guansong Pang, Chunhua Shen, Longbing Cao, Anton van den Hengel", "title": "Deep Learning for Anomaly Detection: A Review", "comments": "Survey paper, 36 pages, 180 references, 2 figures, 4 tables", "journal-ref": "ACM Computing Surveys, 2020", "doi": "10.1145/3439950", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection, a.k.a. outlier detection or novelty detection, has been a\nlasting yet active research area in various research communities for several\ndecades. There are still some unique problem complexities and challenges that\nrequire advanced approaches. In recent years, deep learning enabled anomaly\ndetection, i.e., deep anomaly detection, has emerged as a critical direction.\nThis paper surveys the research of deep anomaly detection with a comprehensive\ntaxonomy, covering advancements in three high-level categories and 11\nfine-grained categories of the methods. We review their key intuitions,\nobjective functions, underlying assumptions, advantages and disadvantages, and\ndiscuss how they address the aforementioned challenges. We further discuss a\nset of possible future opportunities and new perspectives on addressing the\nchallenges.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 02:21:16 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 01:51:13 GMT"}, {"version": "v3", "created": "Sat, 5 Dec 2020 04:53:35 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Pang", "Guansong", ""], ["Shen", "Chunhua", ""], ["Cao", "Longbing", ""], ["Hengel", "Anton van den", ""]]}, {"id": "2007.02520", "submitter": "Xinyan Yan", "authors": "Xinyan Yan, Byron Boots, Ching-An Cheng", "title": "Explaining Fast Improvement in Online Imitation Learning", "comments": "22 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online imitation learning (IL) is an algorithmic framework that leverages\ninteractions with expert policies for efficient policy optimization. Here\npolicies are optimized by performing online learning on a sequence of loss\nfunctions that encourage the learner to mimic expert actions, and if the online\nlearning has no regret, the agent can provably learn an expert-like policy.\nOnline IL has demonstrated empirical successes in many applications and\ninterestingly, its policy improvement speed observed in practice is usually\nmuch faster than existing theory suggests. In this work, we provide an\nexplanation of this phenomenon. Let $\\xi$ denote the policy class bias and\nassume the online IL loss functions are convex, smooth, and non-negative. We\nprove that, after $N$ rounds of online IL with stochastic feedback, the policy\nimproves in $\\tilde{O}(1/N + \\sqrt{\\xi/N})$ in both expectation and high\nprobability. In other words, we show that adopting a sufficiently expressive\npolicy class in online IL has two benefits: both the policy improvement speed\nincreases and the performance bias decreases.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 04:37:03 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 01:51:50 GMT"}, {"version": "v3", "created": "Mon, 22 Feb 2021 01:26:47 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Yan", "Xinyan", ""], ["Boots", "Byron", ""], ["Cheng", "Ching-An", ""]]}, {"id": "2007.02523", "submitter": "Amrith Setlur", "authors": "Amrith Setlur, Saket Dingliwal, Barnabas Poczos", "title": "Covariate Distribution Aware Meta-learning", "comments": null, "journal-ref": "ICML 2020 Lifelong Learning Workshop", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Meta-learning has proven to be successful for few-shot learning across the\nregression, classification, and reinforcement learning paradigms. Recent\napproaches have adopted Bayesian interpretations to improve gradient-based\nmeta-learners by quantifying the uncertainty of the post-adaptation estimates.\nMost of these works almost completely ignore the latent relationship between\nthe covariate distribution $(p(x))$ of a task and the corresponding conditional\ndistribution $p(y|x)$. In this paper, we identify the need to explicitly model\nthe meta-distribution over the task covariates in a hierarchical Bayesian\nframework. We begin by introducing a graphical model that leverages the samples\nfrom the marginal $p(x)$ to better infer the posterior over the optimal\nparameters of the conditional distribution $(p(y|x))$ for each task. Based on\nthis model we propose a computationally feasible meta-learning algorithm by\nintroducing meaningful relaxations in our final objective. We demonstrate the\ngains of our algorithm over initialization based meta-learning baselines on\npopular classification benchmarks. Finally, to understand the potential benefit\nof modeling task covariates we further evaluate our method on a synthetic\nregression dataset.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 05:00:13 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 19:22:10 GMT"}, {"version": "v3", "created": "Sat, 28 Nov 2020 02:07:27 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Setlur", "Amrith", ""], ["Dingliwal", "Saket", ""], ["Poczos", "Barnabas", ""]]}, {"id": "2007.02529", "submitter": "Ziyu Liu", "authors": "Meng Zhou, Ziyu Liu, Pengwei Sui, Yixuan Li, Yuk Ying Chung", "title": "Learning Implicit Credit Assignment for Cooperative Multi-Agent\n  Reinforcement Learning", "comments": "NeurIPS 2020 Camera Ready; first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a multi-agent actor-critic method that aims to implicitly address\nthe credit assignment problem under fully cooperative settings. Our key\nmotivation is that credit assignment among agents may not require an explicit\nformulation as long as (1) the policy gradients derived from a centralized\ncritic carry sufficient information for the decentralized agents to maximize\ntheir joint action value through optimal cooperation and (2) a sustained level\nof exploration is enforced throughout training. Under the centralized training\nwith decentralized execution (CTDE) paradigm, we achieve the former by\nformulating the centralized critic as a hypernetwork such that a latent state\nrepresentation is integrated into the policy gradients through its\nmultiplicative association with the stochastic policies; to achieve the latter,\nwe derive a simple technique called adaptive entropy regularization where\nmagnitudes of the entropy gradients are dynamically rescaled based on the\ncurrent policy stochasticity to encourage consistent levels of exploration. Our\nalgorithm, referred to as LICA, is evaluated on several benchmarks including\nthe multi-agent particle environments and a set of challenging StarCraft II\nmicromanagement tasks, and we show that LICA significantly outperforms previous\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 05:25:02 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 14:18:50 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Zhou", "Meng", ""], ["Liu", "Ziyu", ""], ["Sui", "Pengwei", ""], ["Li", "Yixuan", ""], ["Chung", "Yuk Ying", ""]]}, {"id": "2007.02534", "submitter": "Pierre Humbert", "authors": "Pierre Humbert (ENS Paris Saclay, CGB, CNRS), Laurent Oudre (L2TI),\n  Nivolas Vayatis (ENS Paris Saclay, CGB, CNRS), Julien Audiffren", "title": "Tensor Convolutional Sparse Coding with Low-Rank activations, an\n  application to EEG analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been growing interest in the analysis of spectrograms of\nElectroEncephaloGram (EEG), particularly to study the neural correlates of\n(un)-consciousness during General Anesthesia (GA). Indeed, it has been shown\nthat order three tensors (channels x frequencies x times) are a natural and\nuseful representation of these signals. However this encoding entails\nsignificant difficulties, especially for convolutional sparse coding (CSC) as\nexisting methods do not take advantage of the particularities of tensor\nrepresentation, such as rank structures, and are vulnerable to the high level\nof noise and perturbations that are inherent to EEG during medical acts. To\naddress this issue, in this paper we introduce a new CSC model, named Kruskal\nCSC (K-CSC), that uses the Kruskal decomposition of the activation tensors to\nleverage the intrinsic low rank nature of these representations in order to\nextract relevant and interpretable encodings. Our main contribution, TC-FISTA,\nuses multiple tools to efficiently solve the resulting optimization problem\ndespite the increasing complexity induced by the tensor representation. We then\nevaluate TC-FISTA on both synthetic dataset and real EEG recorded during GA.\nThe results show that TC-FISTA is robust to noise and perturbations, resulting\nin accurate, sparse and interpretable encoding of the signals.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 05:42:38 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 13:00:50 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Humbert", "Pierre", "", "ENS Paris Saclay, CGB, CNRS"], ["Oudre", "Laurent", "", "L2TI"], ["Vayatis", "Nivolas", "", "ENS Paris Saclay, CGB, CNRS"], ["Audiffren", "Julien", ""]]}, {"id": "2007.02561", "submitter": "Junhyun Nam", "authors": "Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, Jinwoo Shin", "title": "Learning from Failure: Training Debiased Classifier from Biased\n  Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks often learn to make predictions that overly rely on spurious\ncorrelation existing in the dataset, which causes the model to be biased. While\nprevious work tackles this issue by using explicit labeling on the spuriously\ncorrelated attributes or presuming a particular bias type, we instead utilize a\ncheaper, yet generic form of human knowledge, which can be widely applicable to\nvarious types of bias. We first observe that neural networks learn to rely on\nthe spurious correlation only when it is \"easier\" to learn than the desired\nknowledge, and such reliance is most prominent during the early phase of\ntraining. Based on the observations, we propose a failure-based debiasing\nscheme by training a pair of neural networks simultaneously. Our main idea is\ntwofold; (a) we intentionally train the first network to be biased by\nrepeatedly amplifying its \"prejudice\", and (b) we debias the training of the\nsecond network by focusing on samples that go against the prejudice of the\nbiased network in (a). Extensive experiments demonstrate that our method\nsignificantly improves the training of the network against various types of\nbiases in both synthetic and real-world datasets. Surprisingly, our framework\neven occasionally outperforms the debiasing methods requiring explicit\nsupervision of the spuriously correlated attributes.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 07:20:29 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 07:41:57 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Nam", "Junhyun", ""], ["Cha", "Hyuntak", ""], ["Ahn", "Sungsoo", ""], ["Lee", "Jaeho", ""], ["Shin", "Jinwoo", ""]]}, {"id": "2007.02572", "submitter": "Simon Bernard", "authors": "Hongliu Cao, Simon Bernard, Robert Sabourin, Laurent Heutte", "title": "A Novel Random Forest Dissimilarity Measure for Multi-View Learning", "comments": "accepted to ICPR 2020 (22/06/2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view learning is a learning task in which data is described by several\nconcurrent representations. Its main challenge is most often to exploit the\ncomplementarities between these representations to help solve a\nclassification/regression task. This is a challenge that can be met nowadays if\nthere is a large amount of data available for learning. However, this is not\nnecessarily true for all real-world problems, where data are sometimes scarce\n(e.g. problems related to the medical environment). In these situations, an\neffective strategy is to use intermediate representations based on the\ndissimilarities between instances. This work presents new ways of constructing\nthese dissimilarity representations, learning them from data with Random Forest\nclassifiers. More precisely, two methods are proposed, which modify the Random\nForest proximity measure, to adapt it to the context of High Dimension Low\nSample Size (HDLSS) multi-view classification problems. The second method,\nbased on an Instance Hardness measurement, is significantly more accurate than\nother state-of-the-art measurements including the original RF Proximity\nmeasurement and the Large Margin Nearest Neighbor (LMNN) metric learning\nmeasurement.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 07:54:52 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Cao", "Hongliu", ""], ["Bernard", "Simon", ""], ["Sabourin", "Robert", ""], ["Heutte", "Laurent", ""]]}, {"id": "2007.02592", "submitter": "Shujaat Khan Engr", "authors": "Syed Muhammad Atif, Shujaat Khan, Imran Naseem, Roberto Togneri,\n  Mohammed Bennamoun", "title": "Multi-Kernel Fusion for RBF Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple yet effective architectural design of radial basis function neural\nnetworks (RBFNN) makes them amongst the most popular conventional neural\nnetworks. The current generation of radial basis function neural network is\nequipped with multiple kernels which provide significant performance benefits\ncompared to the previous generation using only a single kernel. In existing\nmulti-kernel RBF algorithms, multi-kernel is formed by the convex combination\nof the base/primary kernels. In this paper, we propose a novel multi-kernel\nRBFNN in which every base kernel has its own (local) weight. This novel\nflexibility in the network provides better performance such as faster\nconvergence rate, better local minima and resilience against stucking in poor\nlocal minima. These performance gains are achieved at a competitive\ncomputational complexity compared to the contemporary multi-kernel RBF\nalgorithms. The proposed algorithm is thoroughly analysed for performance gain\nusing mathematical and graphical illustrations and also evaluated on three\ndifferent types of problems namely: (i) pattern classification, (ii) system\nidentification and (iii) function approximation. Empirical results clearly show\nthe superiority of the proposed algorithm compared to the existing\nstate-of-the-art multi-kernel approaches.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 09:07:16 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Atif", "Syed Muhammad", ""], ["Khan", "Shujaat", ""], ["Naseem", "Imran", ""], ["Togneri", "Roberto", ""], ["Bennamoun", "Mohammed", ""]]}, {"id": "2007.02617", "submitter": "Maksym Andriushchenko", "authors": "Maksym Andriushchenko, Nicolas Flammarion", "title": "Understanding and Improving Fast Adversarial Training", "comments": "The camera-ready version (accepted at NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent line of work focused on making adversarial training computationally\nefficient for deep learning models. In particular, Wong et al. (2020) showed\nthat $\\ell_\\infty$-adversarial training with fast gradient sign method (FGSM)\ncan fail due to a phenomenon called \"catastrophic overfitting\", when the model\nquickly loses its robustness over a single epoch of training. We show that\nadding a random step to FGSM, as proposed in Wong et al. (2020), does not\nprevent catastrophic overfitting, and that randomness is not important per se\n-- its main role being simply to reduce the magnitude of the perturbation.\nMoreover, we show that catastrophic overfitting is not inherent to deep and\noverparametrized networks, but can occur in a single-layer convolutional\nnetwork with a few filters. In an extreme case, even a single filter can make\nthe network highly non-linear locally, which is the main reason why FGSM\ntraining fails. Based on this observation, we propose a new regularization\nmethod, GradAlign, that prevents catastrophic overfitting by explicitly\nmaximizing the gradient alignment inside the perturbation set and improves the\nquality of the FGSM solution. As a result, GradAlign allows to successfully\napply FGSM training also for larger $\\ell_\\infty$-perturbations and reduce the\ngap to multi-step adversarial training. The code of our experiments is\navailable at https://github.com/tml-epfl/understanding-fast-adv-training.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 10:16:43 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 11:20:16 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Andriushchenko", "Maksym", ""], ["Flammarion", "Nicolas", ""]]}, {"id": "2007.02639", "submitter": "Matthias Perkonigg", "authors": "Johannes Hofmanninger, Matthias Perkonigg, James A. Brink, Oleg\n  Pianykh, Christian Herold, Georg Langs", "title": "Dynamic memory to alleviate catastrophic forgetting in continuous\n  learning settings", "comments": "The first two authors contributed equally. Accepted at MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medical imaging, technical progress or changes in diagnostic procedures\nlead to a continuous change in image appearance. Scanner manufacturer,\nreconstruction kernel, dose, other protocol specific settings or administering\nof contrast agents are examples that influence image content independent of the\nscanned biology. Such domain and task shifts limit the applicability of machine\nlearning algorithms in the clinical routine by rendering models obsolete over\ntime. Here, we address the problem of data shifts in a continuous learning\nscenario by adapting a model to unseen variations in the source domain while\ncounteracting catastrophic forgetting effects. Our method uses a dynamic memory\nto facilitate rehearsal of a diverse training data subset to mitigate\nforgetting. We evaluated our approach on routine clinical CT data obtained with\ntwo different scanner protocols and synthetic classification tasks. Experiments\nshow that dynamic memory counters catastrophic forgetting in a setting with\nmultiple data shifts without the necessity for explicit knowledge about when\nthese shifts occur.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 11:02:38 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 09:05:02 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Hofmanninger", "Johannes", ""], ["Perkonigg", "Matthias", ""], ["Brink", "James A.", ""], ["Pianykh", "Oleg", ""], ["Herold", "Christian", ""], ["Langs", "Georg", ""]]}, {"id": "2007.02650", "submitter": "Hamid Eghbal-zadeh", "authors": "Hamid Eghbal-zadeh, Khaled Koutini, Paul Primus, Verena Haunschmid,\n  Michal Lewandowski, Werner Zellinger, Bernhard A. Moser, Gerhard Widmer", "title": "On Data Augmentation and Adversarial Risk: An Empirical Analysis", "comments": "21 pages, 15 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation techniques have become standard practice in deep learning,\nas it has been shown to greatly improve the generalisation abilities of models.\nThese techniques rely on different ideas such as invariance-preserving\ntransformations (e.g, expert-defined augmentation), statistical heuristics\n(e.g, Mixup), and learning the data distribution (e.g, GANs). However, in the\nadversarial settings it remains unclear under what conditions such data\naugmentation methods reduce or even worsen the misclassification risk. In this\npaper, we therefore analyse the effect of different data augmentation\ntechniques on the adversarial risk by three measures: (a) the well-known risk\nunder adversarial attacks, (b) a new measure of prediction-change stress based\non the Laplacian operator, and (c) the influence of training examples on\nprediction. The results of our empirical analysis disprove the hypothesis that\nan improvement in the classification performance induced by a data augmentation\nis always accompanied by an improvement in the risk under adversarial attack.\nFurther, our results reveal that the augmented data has more influence than the\nnon-augmented data, on the resulting models. Taken together, our results\nsuggest that general-purpose data augmentations that do not take into the\naccount the characteristics of the data and the task, must be applied with\ncare.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 11:16:18 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Eghbal-zadeh", "Hamid", ""], ["Koutini", "Khaled", ""], ["Primus", "Paul", ""], ["Haunschmid", "Verena", ""], ["Lewandowski", "Michal", ""], ["Zellinger", "Werner", ""], ["Moser", "Bernhard A.", ""], ["Widmer", "Gerhard", ""]]}, {"id": "2007.02673", "submitter": "Daniel \\v{S}tifani\\'c", "authors": "Daniel \\v{S}tifani\\'c, Jelena Musulin, Adrijana Mio\\v{c}evi\\'c, Sandi\n  Baressi \\v{S}egota, Roman \\v{S}ubi\\'c, Zlatan Car", "title": "Impact of COVID-19 on Forecasting Stock Prices: An Integration of\n  Stationary Wavelet Transform and Bidirectional Long Short-Term Memory", "comments": "26 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  COVID-19 is an infectious disease that mostly affects the respiratory system.\nAt the time of this research being performed, there were more than 1.4 million\ncases of COVID-19, and one of the biggest anxieties is not just our health, but\nour livelihoods, too. In this research, authors investigate the impact of\nCOVID-19 on the global economy, more specifically, the impact of COVID-19 on\nfinancial movement of Crude Oil price and three U.S. stock indexes: DJI, S&P\n500 and NASDAQ Composite. The proposed system for predicting commodity and\nstock prices integrates the Stationary Wavelet Transform (SWT) and\nBidirectional Long Short-Term Memory (BDLSTM) networks. Firstly, SWT is used to\ndecompose the data into approximation and detail coefficients. After\ndecomposition, data of Crude Oil price and stock market indexes along with\nCOVID-19 confirmed cases were used as input variables for future price movement\nforecasting. As a result, the proposed system BDLSTM+WT-ADA achieved\nsatisfactory results in terms of five-day Crude Oil price forecast.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 14:03:39 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["\u0160tifani\u0107", "Daniel", ""], ["Musulin", "Jelena", ""], ["Mio\u010devi\u0107", "Adrijana", ""], ["\u0160egota", "Sandi Baressi", ""], ["\u0160ubi\u0107", "Roman", ""], ["Car", "Zlatan", ""]]}, {"id": "2007.02677", "submitter": "Simon Weissmann", "authors": "Neil K. Chada, Claudia Schillings, Xin T. Tong and Simon Weissmann", "title": "Consistency analysis of bilevel data-driven learning in inverse problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.NA math.NA math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One fundamental problem when solving inverse problems is how to find\nregularization parameters. This article considers solving this problem using\ndata-driven bilevel optimization, i.e. we consider the adaptive learning of the\nregularization parameter from data by means of optimization. This approach can\nbe interpreted as solving an empirical risk minimization problem, and we\nanalyze its performance in the large data sample size limit for general\nnonlinear problems. We demonstrate how to implement our framework on linear\ninverse problems, where we can further show the inverse accuracy does not\ndepend on the ambient space dimension. To reduce the associated computational\ncost, online numerical schemes are derived using the stochastic gradient\ndescent method. We prove convergence of these numerical schemes under suitable\nassumptions on the forward problem. Numerical experiments are presented\nillustrating the theoretical results and demonstrating the applicability and\nefficiency of the proposed approaches for various linear and nonlinear inverse\nproblems, including Darcy flow, the eikonal equation, and an image denoising\nexample.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 12:23:29 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 15:37:05 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Chada", "Neil K.", ""], ["Schillings", "Claudia", ""], ["Tong", "Xin T.", ""], ["Weissmann", "Simon", ""]]}, {"id": "2007.02693", "submitter": "Aviv Navon", "authors": "Aviv Navon and Idan Achituve and Haggai Maron and Gal Chechik and\n  Ethan Fetaya", "title": "Auxiliary Learning by Implicit Differentiation", "comments": "Published at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training neural networks with auxiliary tasks is a common practice for\nimproving the performance on a main task of interest. Two main challenges arise\nin this multi-task learning setting: (i) designing useful auxiliary tasks; and\n(ii) combining auxiliary tasks into a single coherent loss. Here, we propose a\nnovel framework, AuxiLearn, that targets both challenges based on implicit\ndifferentiation. First, when useful auxiliaries are known, we propose learning\na network that combines all losses into a single coherent objective function.\nThis network can learn non-linear interactions between tasks. Second, when no\nuseful auxiliary task is known, we describe how to learn a network that\ngenerates a meaningful, novel auxiliary task. We evaluate AuxiLearn in a series\nof tasks and domains, including image segmentation and learning with attributes\nin the low data regime, and find that it consistently outperforms competing\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 19:35:07 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 07:40:34 GMT"}, {"version": "v3", "created": "Tue, 11 May 2021 06:52:59 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Navon", "Aviv", ""], ["Achituve", "Idan", ""], ["Maron", "Haggai", ""], ["Chechik", "Gal", ""], ["Fetaya", "Ethan", ""]]}, {"id": "2007.02701", "submitter": "Artemij Amiranashvili", "authors": "Artemij Amiranashvili, Nicolai Dorka, Wolfram Burgard, Vladlen Koltun,\n  Thomas Brox", "title": "Scaling Imitation Learning in Minecraft", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imitation learning is a powerful family of techniques for learning\nsensorimotor coordination in immersive environments. We apply imitation\nlearning to attain state-of-the-art performance on hard exploration problems in\nthe Minecraft environment. We report experiments that highlight the influence\nof network architecture, loss function, and data augmentation. An early version\nof our approach reached second place in the MineRL competition at NeurIPS 2019.\nHere we report stronger results that can be used as a starting point for future\ncompetition entries and related research. Our code is available at\nhttps://github.com/amiranas/minerl_imitation_learning.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 12:47:01 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Amiranashvili", "Artemij", ""], ["Dorka", "Nicolai", ""], ["Burgard", "Wolfram", ""], ["Koltun", "Vladlen", ""], ["Brox", "Thomas", ""]]}, {"id": "2007.02719", "submitter": "Praneeth Vepakomma", "authors": "Praneeth Vepakomma, Julia Balla, Ramesh Raskar", "title": "Splintering with distributions: A stochastic decoy scheme for private\n  computation", "comments": "28 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing computations while maintaining privacy is an important problem in\ntodays distributed machine learning solutions. Consider the following two set\nups between a client and a server, where in setup i) the client has a public\ndata vector $\\mathbf{x}$, the server has a large private database of data\nvectors $\\mathcal{B}$ and the client wants to find the inner products $\\langle\n\\mathbf{x,y_k} \\rangle, \\forall \\mathbf{y_k} \\in \\mathcal{B}$. The client does\nnot want the server to learn $\\mathbf{x}$ while the server does not want the\nclient to learn the records in its database. This is in contrast to another\nsetup ii) where the client would like to perform an operation solely on its\ndata, such as computation of a matrix inverse on its data matrix $\\mathbf{M}$,\nbut would like to use the superior computing ability of the server to do so\nwithout having to leak $\\mathbf{M}$ to the server. \\par We present a stochastic\nscheme for splitting the client data into privatized shares that are\ntransmitted to the server in such settings. The server performs the requested\noperations on these shares instead of on the raw client data at the server. The\nobtained intermediate results are sent back to the client where they are\nassembled by the client to obtain the final result.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 13:06:49 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 19:23:39 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Vepakomma", "Praneeth", ""], ["Balla", "Julia", ""], ["Raskar", "Ramesh", ""]]}, {"id": "2007.02723", "submitter": "Diyora Salimova", "authors": "Aritz Bercher, Lukas Gonon, Arnulf Jentzen, Diyora Salimova", "title": "Weak error analysis for stochastic gradient descent optimization\n  algorithms", "comments": "123 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA math.OC math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) type optimization schemes are fundamental\ningredients in a large number of machine learning based algorithms. In\nparticular, SGD type optimization schemes are frequently employed in\napplications involving natural language processing, object and face\nrecognition, fraud detection, computational advertisement, and numerical\napproximations of partial differential equations. In mathematical convergence\nresults for SGD type optimization schemes there are usually two types of error\ncriteria studied in the scientific literature, that is, the error in the strong\nsense and the error with respect to the objective function. In applications one\nis often not only interested in the size of the error with respect to the\nobjective function but also in the size of the error with respect to a test\nfunction which is possibly different from the objective function. The analysis\nof the size of this error is the subject of this article. In particular, the\nmain result of this article proves under suitable assumptions that the size of\nthis error decays at the same speed as in the special case where the test\nfunction coincides with the objective function.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 12:38:16 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 12:07:12 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Bercher", "Aritz", ""], ["Gonon", "Lukas", ""], ["Jentzen", "Arnulf", ""], ["Salimova", "Diyora", ""]]}, {"id": "2007.02731", "submitter": "Didrik Nielsen", "authors": "Didrik Nielsen, Priyank Jaini, Emiel Hoogeboom, Ole Winther, Max\n  Welling", "title": "SurVAE Flows: Surjections to Bridge the Gap between VAEs and Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalizing flows and variational autoencoders are powerful generative models\nthat can represent complicated density functions. However, they both impose\nconstraints on the models: Normalizing flows use bijective transformations to\nmodel densities whereas VAEs learn stochastic transformations that are\nnon-invertible and thus typically do not provide tractable estimates of the\nmarginal likelihood. In this paper, we introduce SurVAE Flows: A modular\nframework of composable transformations that encompasses VAEs and normalizing\nflows. SurVAE Flows bridge the gap between normalizing flows and VAEs with\nsurjective transformations, wherein the transformations are deterministic in\none direction -- thereby allowing exact likelihood computation, and stochastic\nin the reverse direction -- hence providing a lower bound on the corresponding\nlikelihood. We show that several recently proposed methods, including\ndequantization and augmented normalizing flows, can be expressed as SurVAE\nFlows. Finally, we introduce common operations such as the max value, the\nabsolute value, sorting and stochastic permutation as composable layers in\nSurVAE Flows.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 13:13:22 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 17:08:53 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Nielsen", "Didrik", ""], ["Jaini", "Priyank", ""], ["Hoogeboom", "Emiel", ""], ["Winther", "Ole", ""], ["Welling", "Max", ""]]}, {"id": "2007.02734", "submitter": "Hadi Mohaghegh Dolatabadi", "authors": "Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie", "title": "Black-box Adversarial Example Generation with Normalizing Flows", "comments": "Accepted to the 2nd workshop on Invertible Neural Networks,\n  Normalizing Flows, and Explicit Likelihood Models (ICML 2020), Virtual\n  Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network classifiers suffer from adversarial vulnerability:\nwell-crafted, unnoticeable changes to the input data can affect the classifier\ndecision. In this regard, the study of powerful adversarial attacks can help\nshed light on sources of this malicious behavior. In this paper, we propose a\nnovel black-box adversarial attack using normalizing flows. We show how an\nadversary can be found by searching over a pre-trained flow-based model base\ndistribution. This way, we can generate adversaries that resemble the original\ndata closely as the perturbations are in the shape of the data. We then\ndemonstrate the competitive performance of the proposed approach against\nwell-known black-box adversarial attack methods.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 13:14:21 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Dolatabadi", "Hadi M.", ""], ["Erfani", "Sarah", ""], ["Leckie", "Christopher", ""]]}, {"id": "2007.02738", "submitter": "Zhijie Zhang", "authors": "Wei Chen, Xiaoming Sun, Jialin Zhang, Zhijie Zhang", "title": "Optimization from Structured Samples for Coverage Functions", "comments": "To appear in ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the optimization from samples (OPS) model, which studies the\nproblem of optimizing objective functions directly from the sample data.\nPrevious results showed that we cannot obtain a constant approximation ratio\nfor the maximum coverage problem using polynomially many independent samples of\nthe form $\\{S_i, f(S_i)\\}_{i=1}^t$ (Balkanski et al., 2017), even if coverage\nfunctions are $(1 - \\epsilon)$-PMAC learnable using these samples (Badanidiyuru\net al., 2012), which means most of the function values can be approximately\nlearned very well with high probability. In this work, to circumvent the\nimpossibility result of OPS, we propose a stronger model called optimization\nfrom structured samples (OPSS) for coverage functions, where the data samples\nencode the structural information of the functions. We show that under three\ngeneral assumptions on the sample distributions, we can design efficient OPSS\nalgorithms that achieve a constant approximation for the maximum coverage\nproblem. We further prove a constant lower bound under these assumptions, which\nis tight when not considering computational efficiency. Moreover, we also show\nthat if we remove any one of the three assumptions, OPSS for the maximum\ncoverage problem has no constant approximation.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 13:18:11 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Chen", "Wei", ""], ["Sun", "Xiaoming", ""], ["Zhang", "Jialin", ""], ["Zhang", "Zhijie", ""]]}, {"id": "2007.02745", "submitter": "Ivana Bala\\v{z}evi\\'c", "authors": "Ivana Bala\\v{z}evi\\'c, Carl Allen, Timothy Hospedales", "title": "Learning the Prediction Distribution for Semi-Supervised Learning with\n  Normalising Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As data volumes continue to grow, the labelling process increasingly becomes\na bottleneck, creating demand for methods that leverage information from\nunlabelled data. Impressive results have been achieved in semi-supervised\nlearning (SSL) for image classification, nearing fully supervised performance,\nwith only a fraction of the data labelled. In this work, we propose a\nprobabilistically principled general approach to SSL that considers the\ndistribution over label predictions, for labels of different complexity, from\n\"one-hot\" vectors to binary vectors and images. Our method regularises an\nunderlying supervised model, using a normalising flow that learns the posterior\ndistribution over predictions for labelled data, to serve as a prior over the\npredictions on unlabelled data. We demonstrate the general applicability of\nthis approach on a range of computer vision tasks with varying output\ncomplexity: classification, attribute prediction and image-to-image\ntranslation.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 13:36:00 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Bala\u017eevi\u0107", "Ivana", ""], ["Allen", "Carl", ""], ["Hospedales", "Timothy", ""]]}, {"id": "2007.02771", "submitter": "Claudio Lucchese", "authors": "Stefano Calzavara and Pietro Ferrara and Claudio Lucchese", "title": "Certifying Decision Trees Against Evasion Attacks by Program Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has proved invaluable for a range of different tasks, yet it\nalso proved vulnerable to evasion attacks, i.e., maliciously crafted\nperturbations of input data designed to force mispredictions. In this paper we\npropose a novel technique to verify the security of decision tree models\nagainst evasion attacks with respect to an expressive threat model, where the\nattacker can be represented by an arbitrary imperative program. Our approach\nexploits the interpretability property of decision trees to transform them into\nimperative programs, which are amenable for traditional program analysis\ntechniques. By leveraging the abstract interpretation framework, we are able to\nsoundly verify the security guarantees of decision tree models trained over\npublicly available datasets. Our experiments show that our technique is both\nprecise and efficient, yielding only a minimal number of false positives and\nscaling up to cases which are intractable for a competitor approach.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 14:18:10 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Calzavara", "Stefano", ""], ["Ferrara", "Pietro", ""], ["Lucchese", "Claudio", ""]]}, {"id": "2007.02777", "submitter": "Mattia G. Bergomi", "authors": "Pietro Vertechi, Patrizio Frosini, Mattia G. Bergomi", "title": "Parametric machines: a fresh approach to architecture search", "comments": "31 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using tools from category theory, we provide a framework where artificial\nneural networks, and their architectures, can be formally described. We first\ndefine the notion of machine in a general categorical context, and show how\nsimple machines can be combined into more complex ones. We explore finite- and\ninfinite-depth machines, which generalize neural networks and neural ordinary\ndifferential equations. Borrowing ideas from functional analysis and kernel\nmethods, we build complete, normed, infinite-dimensional spaces of machines,\nand discuss how to find optimal architectures and parameters -- within those\nspaces -- to solve a given computational problem. In our numerical experiments,\nthese kernel-inspired networks can outperform classical neural networks when\nthe training dataset is small.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 14:27:06 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 16:24:55 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Vertechi", "Pietro", ""], ["Frosini", "Patrizio", ""], ["Bergomi", "Mattia G.", ""]]}, {"id": "2007.02786", "submitter": "Joshua Romoff", "authors": "Joshua Romoff, Peter Henderson, David Kanaa, Emmanuel Bengio, Ahmed\n  Touati, Pierre-Luc Bacon, Joelle Pineau", "title": "TDprop: Does Jacobi Preconditioning Help Temporal Difference Learning?", "comments": "Presented at the Theoretical Foundations of Reinforcement Learning\n  workshop at ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate whether Jacobi preconditioning, accounting for the bootstrap\nterm in temporal difference (TD) learning, can help boost performance of\nadaptive optimizers. Our method, TDprop, computes a per parameter learning rate\nbased on the diagonal preconditioning of the TD update rule. We show how this\ncan be used in both $n$-step returns and TD($\\lambda$). Our theoretical\nfindings demonstrate that including this additional preconditioning information\nis, surprisingly, comparable to normal semi-gradient TD if the optimal learning\nrate is found for both via a hyperparameter search. In Deep RL experiments\nusing Expected SARSA, TDprop meets or exceeds the performance of Adam in all\ntested games under near-optimal learning rates, but a well-tuned SGD can yield\nsimilar improvements -- matching our theory. Our findings suggest that Jacobi\npreconditioning may improve upon typical adaptive optimization methods in Deep\nRL, but despite incorporating additional information from the TD bootstrap\nterm, may not always be better than SGD.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 14:40:34 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Romoff", "Joshua", ""], ["Henderson", "Peter", ""], ["Kanaa", "David", ""], ["Bengio", "Emmanuel", ""], ["Touati", "Ahmed", ""], ["Bacon", "Pierre-Luc", ""], ["Pineau", "Joelle", ""]]}, {"id": "2007.02794", "submitter": "Tianyu Shi", "authors": "Tianyu Shi, Jiawei Wang, Yuankai Wu, Lijun Sun", "title": "Towards Efficient Connected and Automated Driving System via Multi-agent\n  Graph Reinforcement Learning", "comments": "the paper is not even ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connected and automated vehicles (CAVs) have attracted more and more\nattention recently. The fast actuation time allows them having the potential to\npromote the efficiency and safety of the whole transportation system. Due to\ntechnical challenges, there will be a proportion of vehicles that can be\nequipped with automation while other vehicles are without automation. Instead\nof learning a reliable behavior for ego automated vehicle, we focus on how to\nimprove the outcomes of the total transportation system by allowing each\nautomated vehicle to learn cooperation with each other and regulate\nhuman-driven traffic flow. One of state of the art method is using\nreinforcement learning to learn intelligent decision making policy. However,\ndirect reinforcement learning framework cannot improve the performance of the\nwhole system. In this article, we demonstrate that considering the problem in\nmulti-agent setting with shared policy can help achieve better system\nperformance than non-shared policy in single-agent setting. Furthermore, we\nfind that utilization of attention mechanism on interaction features can\ncapture the interplay between each agent in order to boost cooperation. To the\nbest of our knowledge, while previous automated driving studies mainly focus on\nenhancing individual's driving performance, this work serves as a starting\npoint for research on system-level multi-agent cooperation performance using\ngraph information sharing. We conduct extensive experiments in car-following\nand unsignalized intersection settings. The results demonstrate that CAVs\ncontrolled by our method can achieve the best performance against several state\nof the art baselines.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 14:55:48 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 13:58:02 GMT"}, {"version": "v3", "created": "Sat, 1 Aug 2020 14:15:47 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Shi", "Tianyu", ""], ["Wang", "Jiawei", ""], ["Wu", "Yuankai", ""], ["Sun", "Lijun", ""]]}, {"id": "2007.02801", "submitter": "Stephen Pasteris", "authors": "Stephen Pasteris, Ting He, Fabio Vitale, Shiqiang Wang, Mark Herbster", "title": "Online Learning of Facility Locations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide a rigorous theoretical investigation of an online\nlearning version of the Facility Location problem which is motivated by\nemerging problems in real-world applications. In our formulation, we are given\na set of sites and an online sequence of user requests. At each trial, the\nlearner selects a subset of sites and then incurs a cost for each selected site\nand an additional cost which is the price of the user's connection to the\nnearest site in the selected subset. The problem may be solved by an\napplication of the well-known Hedge algorithm. This would, however, require\ntime and space exponential in the number of the given sites, which motivates\nour design of a novel quasi-linear time algorithm for this problem, with good\ntheoretical guarantees on its performance.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 15:04:53 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Pasteris", "Stephen", ""], ["He", "Ting", ""], ["Vitale", "Fabio", ""], ["Wang", "Shiqiang", ""], ["Herbster", "Mark", ""]]}, {"id": "2007.02809", "submitter": "Jean-Francois Ton", "authors": "Jean-Francois Ton, Dino Sejdinovic, Kenji Fukumizu", "title": "Meta Learning for Causal Direction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inaccessibility of controlled randomized trials due to inherent\nconstraints in many fields of science has been a fundamental issue in causal\ninference. In this paper, we focus on distinguishing the cause from effect in\nthe bivariate setting under limited observational data. Based on recent\ndevelopments in meta learning as well as in causal inference, we introduce a\nnovel generative model that allows distinguishing cause and effect in the small\ndata setting. Using a learnt task variable that contains distributional\ninformation of each dataset, we propose an end-to-end algorithm that makes use\nof similar training datasets at test time. We demonstrate our method on various\nsynthetic as well as real-world data and show that it is able to maintain high\naccuracy in detecting directions across varying dataset sizes.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 15:12:05 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 01:39:02 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Ton", "Jean-Francois", ""], ["Sejdinovic", "Dino", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "2007.02811", "submitter": "Mahdi Rezaei", "authors": "Fatemeh Serpush, Mahdi Rezaei", "title": "Complex Human Action Recognition in Live Videos Using Hybrid FR-DL\n  Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated human action recognition is one of the most attractive and\npractical research fields in computer vision, in spite of its high\ncomputational costs. In such systems, the human action labelling is based on\nthe appearance and patterns of the motions in the video sequences; however, the\nconventional methodologies and classic neural networks cannot use temporal\ninformation for action recognition prediction in the upcoming frames in a video\nsequence. On the other hand, the computational cost of the preprocessing stage\nis high. In this paper, we address challenges of the preprocessing phase, by an\nautomated selection of representative frames among the input sequences.\nFurthermore, we extract the key features of the representative frame rather\nthan the entire features. We propose a hybrid technique using background\nsubtraction and HOG, followed by application of a deep neural network and\nskeletal modelling method. The combination of a CNN and the LSTM recursive\nnetwork is considered for feature selection and maintaining the previous\ninformation, and finally, a Softmax-KNN classifier is used for labelling human\nactivities. We name our model as Feature Reduction & Deep Learning based action\nrecognition method, or FR-DL in short. To evaluate the proposed method, we use\nthe UCF dataset for the benchmarking which is widely-used among researchers in\naction recognition research. The dataset includes 101 complicated activities in\nthe wild. Experimental results show a significant improvement in terms of\naccuracy and speed in comparison with six state-of-the-art articles.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 15:12:50 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Serpush", "Fatemeh", ""], ["Rezaei", "Mahdi", ""]]}, {"id": "2007.02816", "submitter": "Alexander Tornede", "authors": "Alexander Tornede, Marcel Wever, Stefan Werner, Felix Mohr, Eyke\n  H\\\"ullermeier", "title": "Run2Survive: A Decision-theoretic Approach to Algorithm Selection based\n  on Survival Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Algorithm selection (AS) deals with the automatic selection of an algorithm\nfrom a fixed set of candidate algorithms most suitable for a specific instance\nof an algorithmic problem class, where \"suitability\" often refers to an\nalgorithm's runtime. Due to possibly extremely long runtimes of candidate\nalgorithms, training data for algorithm selection models is usually generated\nunder time constraints in the sense that not all algorithms are run to\ncompletion on all instances. Thus, training data usually comprises censored\ninformation, as the true runtime of algorithms timed out remains unknown.\nHowever, many standard AS approaches are not able to handle such information in\na proper way. On the other side, survival analysis (SA) naturally supports\ncensored data and offers appropriate ways to use such data for learning\ndistributional models of algorithm runtime, as we demonstrate in this work. We\nleverage such models as a basis of a sophisticated decision-theoretic approach\nto algorithm selection, which we dub Run2Survive. Moreover, taking advantage of\na framework of this kind, we advocate a risk-averse approach to algorithm\nselection, in which the avoidance of a timeout is given high priority. In an\nextensive experimental study with the standard benchmark ASlib, our approach is\nshown to be highly competitive and in many cases even superior to\nstate-of-the-art AS approaches.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 15:20:17 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 08:07:47 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Tornede", "Alexander", ""], ["Wever", "Marcel", ""], ["Werner", "Stefan", ""], ["Mohr", "Felix", ""], ["H\u00fcllermeier", "Eyke", ""]]}, {"id": "2007.02817", "submitter": "Matthew Fahrbach", "authors": "Matthew Fahrbach, Gramoz Goranci, Richard Peng, Sushant Sachdeva, Chi\n  Wang", "title": "Faster Graph Embeddings via Coarsening", "comments": "18 pages, 2 figures, to appear in the Proceedings of the 37th\n  International Conference on Machine Learning (ICML 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph embeddings are a ubiquitous tool for machine learning tasks, such as\nnode classification and link prediction, on graph-structured data. However,\ncomputing the embeddings for large-scale graphs is prohibitively inefficient\neven if we are interested only in a small subset of relevant vertices. To\naddress this, we present an efficient graph coarsening approach, based on Schur\ncomplements, for computing the embedding of the relevant vertices. We prove\nthat these embeddings are preserved exactly by the Schur complement graph that\nis obtained via Gaussian elimination on the non-relevant vertices. As computing\nSchur complements is expensive, we give a nearly-linear time algorithm that\ngenerates a coarsened graph on the relevant vertices that provably matches the\nSchur complement in expectation in each iteration. Our experiments involving\nprediction tasks on graphs demonstrate that computing embeddings on the\ncoarsened graph, rather than the entire graph, leads to significant time\nsavings without sacrificing accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 15:22:25 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 03:42:19 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 13:49:46 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Fahrbach", "Matthew", ""], ["Goranci", "Gramoz", ""], ["Peng", "Richard", ""], ["Sachdeva", "Sushant", ""], ["Wang", "Chi", ""]]}, {"id": "2007.02820", "submitter": "Krzysztof M. Graczyk", "authors": "Krzysztof M. Graczyk and Maciej Matyka", "title": "Predicting Porosity, Permeability, and Tortuosity of Porous Media from\n  Images by Deep Learning", "comments": "12 pages, 9 figures", "journal-ref": "Sci Rep 10, 21488 (2020)", "doi": "10.1038/s41598-020-78415-x", "report-no": null, "categories": "physics.comp-ph cond-mat.dis-nn physics.flu-dyn stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) are utilized to encode the relation\nbetween initial configurations of obstacles and three fundamental quantities in\nporous media: porosity ($\\varphi$), permeability $k$, and tortuosity ($T$). The\ntwo-dimensional systems with obstacles are considered. The fluid flow through a\nporous medium is simulated with the lattice Boltzmann method. It is\ndemonstrated that the CNNs are able to predict the porosity, permeability, and\ntortuosity with good accuracy. With the usage of the CNN models, the relation\nbetween $T$ and $\\varphi$ has been reproduced and compared with the empirical\nestimate. The analysis has been performed for the systems with $\\varphi \\in\n(0.37,0.99)$ which covers five orders of magnitude span for permeability $k \\in\n(0.78, 2.1\\times 10^5)$ and tortuosity $T \\in (1.03,2.74)$.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 15:27:14 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Graczyk", "Krzysztof M.", ""], ["Matyka", "Maciej", ""]]}, {"id": "2007.02821", "submitter": "Yue Liu", "authors": "Yue Liu, Adam Ghandar, Georgios Theodoropoulos", "title": "Online NEAT for Credit Evaluation -- a Dynamic Problem with Sequential\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe application of Neuroevolution to a P2P lending\nproblem in which a credit evaluation model is updated based on streaming data.\nWe apply the algorithm Neuroevolution of Augmenting Topologies (NEAT) which has\nnot been widely applied generally in the credit evaluation domain. In addition\nto comparing the methodology with other widely applied machine learning\ntechniques, we develop and evaluate several enhancements to the algorithm which\nmake it suitable for the particular aspects of online learning that are\nrelevant in the problem. These include handling unbalanced streaming data, high\ncomputation costs, and maintaining model similarity over time, that is training\nthe stochastic learning algorithm with new data but minimizing model change\nexcept where there is a clear benefit for model performance\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 15:27:14 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Liu", "Yue", ""], ["Ghandar", "Adam", ""], ["Theodoropoulos", "Georgios", ""]]}, {"id": "2007.02829", "submitter": "Ronald Seoh", "authors": "Ronald Seoh", "title": "Solving Bayesian Network Structure Learning Problem with Integer Linear\n  Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This dissertation investigates integer linear programming (ILP) formulation\nof Bayesian Network structure learning problem. We review the definition and\nkey properties of Bayesian network and explain score metrics used to measure\nhow well certain Bayesian network structure fits the dataset. We outline the\ninteger linear programming formulation based on the decomposability of score\nmetrics. In order to ensure acyclicity of the structure, we add ``cluster\nconstraints'' developed specifically for Bayesian network, in addition to cycle\nconstraints applicable to directed acyclic graphs in general. Since there would\nbe exponential number of these constraints if we specify them fully, we explain\nthe methods to add them as cutting planes without declaring them all in the\ninitial model. Also, we develop a heuristic algorithm that finds a feasible\nsolution based on the idea of sink node on directed acyclic graphs. We\nimplemented the ILP formulation and cutting planes as a \\textsf{Python}\npackage, and present the results of experiments with different settings on\nreference datasets.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 15:34:03 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Seoh", "Ronald", ""]]}, {"id": "2007.02832", "submitter": "Silviu Pitis", "authors": "Silviu Pitis, Harris Chan, Stephen Zhao, Bradly Stadie, Jimmy Ba", "title": "Maximum Entropy Gain Exploration for Long Horizon Multi-goal\n  Reinforcement Learning", "comments": "12 pages (+12 appendix). Published as a conference paper at ICML\n  2020. Code available at https://github.com/spitis/mrl", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What goals should a multi-goal reinforcement learning agent pursue during\ntraining in long-horizon tasks? When the desired (test time) goal distribution\nis too distant to offer a useful learning signal, we argue that the agent\nshould not pursue unobtainable goals. Instead, it should set its own intrinsic\ngoals that maximize the entropy of the historical achieved goal distribution.\nWe propose to optimize this objective by having the agent pursue past achieved\ngoals in sparsely explored areas of the goal space, which focuses exploration\non the frontier of the achievable goal set. We show that our strategy achieves\nan order of magnitude better sample efficiency than the prior state of the art\non long-horizon multi-goal tasks including maze navigation and block stacking.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 15:36:05 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Pitis", "Silviu", ""], ["Chan", "Harris", ""], ["Zhao", "Stephen", ""], ["Stadie", "Bradly", ""], ["Ba", "Jimmy", ""]]}, {"id": "2007.02837", "submitter": "Katarzyna Wo\\'znica", "authors": "Katarzyna Wo\\'znica and Przemys{\\l}aw Biecek", "title": "Does imputation matter? Benchmark for predictive models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incomplete data are common in practical applications. Most predictive machine\nlearning models do not handle missing values so they require some\npreprocessing. Although many algorithms are used for data imputation, we do not\nunderstand the impact of the different methods on the predictive models'\nperformance. This paper is first that systematically evaluates the empirical\neffectiveness of data imputation algorithms for predictive models. The main\ncontributions are (1) the recommendation of a general method for empirical\nbenchmarking based on real-life classification tasks and the (2) comparative\nanalysis of different imputation methods for a collection of data sets and a\ncollection of ML algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 15:47:36 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Wo\u017anica", "Katarzyna", ""], ["Biecek", "Przemys\u0142aw", ""]]}, {"id": "2007.02842", "submitter": "Lei Bai", "authors": "Lei Bai and Lina Yao and Can Li and Xianzhi Wang and Can Wang", "title": "Adaptive Graph Convolutional Recurrent Network for Traffic Forecasting", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling complex spatial and temporal correlations in the correlated time\nseries data is indispensable for understanding the traffic dynamics and\npredicting the future status of an evolving traffic system. Recent works focus\non designing complicated graph neural network architectures to capture shared\npatterns with the help of pre-defined graphs. In this paper, we argue that\nlearning node-specific patterns is essential for traffic forecasting while the\npre-defined graph is avoidable. To this end, we propose two adaptive modules\nfor enhancing Graph Convolutional Network (GCN) with new capabilities: 1) a\nNode Adaptive Parameter Learning (NAPL) module to capture node-specific\npatterns; 2) a Data Adaptive Graph Generation (DAGG) module to infer the\ninter-dependencies among different traffic series automatically. We further\npropose an Adaptive Graph Convolutional Recurrent Network (AGCRN) to capture\nfine-grained spatial and temporal correlations in traffic series automatically\nbased on the two modules and recurrent networks. Our experiments on two\nreal-world traffic datasets show AGCRN outperforms state-of-the-art by a\nsignificant margin without pre-defined graphs about spatial connections.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 15:51:10 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 00:08:46 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Bai", "Lei", ""], ["Yao", "Lina", ""], ["Li", "Can", ""], ["Wang", "Xianzhi", ""], ["Wang", "Can", ""]]}, {"id": "2007.02845", "submitter": "Francisco Ibarrola", "authors": "Francisco J. Ibarrola, Nishant Ravikumar and Alejandro F. Frangi", "title": "Partially Conditioned Generative Adversarial Networks", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models are undoubtedly a hot topic in Artificial Intelligence,\namong which the most common type is Generative Adversarial Networks (GANs).\nThese architectures let one synthesise artificial datasets by implicitly\nmodelling the underlying probability distribution of a real-world training\ndataset. With the introduction of Conditional GANs and their variants, these\nmethods were extended to generating samples conditioned on ancillary\ninformation available for each sample within the dataset. From a practical\nstandpoint, however, one might desire to generate data conditioned on partial\ninformation. That is, only a subset of the ancillary conditioning variables\nmight be of interest when synthesising data. In this work, we argue that\nstandard Conditional GANs are not suitable for such a task and propose a new\nAdversarial Network architecture and training strategy to deal with the ensuing\nproblems. Experiments illustrating the value of the proposed approach in digit\nand face image synthesis under partial conditioning information are presented,\nshowing that the proposed method can effectively outperform the standard\napproach under these circumstances.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 15:59:28 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Ibarrola", "Francisco J.", ""], ["Ravikumar", "Nishant", ""], ["Frangi", "Alejandro F.", ""]]}, {"id": "2007.02848", "submitter": "Daniel Messenger", "authors": "Daniel A. Messenger and David M. Bortz", "title": "Weak SINDy For Partial Differential Equations", "comments": "Code used in this manuscript is publicly available on GitHub at\n  https://github.com/dm973/WSINDy PDE", "journal-ref": null, "doi": "10.1016/j.jcp.2021.110525", "report-no": null, "categories": "math.NA cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse Identification of Nonlinear Dynamics (SINDy) is a method of system\ndiscovery that has been shown to successfully recover governing dynamical\nsystems from data (Brunton et al., PNAS, '16; Rudy et al., Sci. Adv. '17).\nRecently, several groups have independently discovered that the weak\nformulation provides orders of magnitude better robustness to noise. Here we\nextend our Weak SINDy (WSINDy) framework introduced in (arXiv:2005.04339) to\nthe setting of partial differential equations (PDEs). The elimination of\npointwise derivative approximations via the weak form enables effective\nmachine-precision recovery of model coefficients from noise-free data (i.e.\nbelow the tolerance of the simulation scheme) as well as robust identification\nof PDEs in the large noise regime (with signal-to-noise ratio approaching one\nin many well-known cases). This is accomplished by discretizing a convolutional\nweak form of the PDE and exploiting separability of test functions for\nefficient model identification using the Fast Fourier Transform. The resulting\nWSINDy algorithm for PDEs has a worst-case computational complexity of\n$\\mathcal{O}(N^{D+1}\\log(N))$ for datasets with $N$ points in each of $D+1$\ndimensions (i.e. $\\mathcal{O}(\\log(N))$ operations per datapoint). Furthermore,\nour Fourier-based implementation reveals a connection between robustness to\nnoise and the spectra of test functions, which we utilize in an \\textit{a\npriori} selection algorithm for test functions. Finally, we introduce a\nlearning algorithm for the threshold in sequential-thresholding least-squares\n(STLS) that enables model identification from large libraries, and we utilize\nscale-invariance at the continuum level to identify PDEs from poorly-scaled\ndatasets. We demonstrate WSINDy's robustness, speed and accuracy on several\nchallenging PDEs.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 16:03:51 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 21:34:59 GMT"}, {"version": "v3", "created": "Mon, 21 Dec 2020 17:26:50 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Messenger", "Daniel A.", ""], ["Bortz", "David M.", ""]]}, {"id": "2007.02857", "submitter": "Jack Gorham", "authors": "Jackson Gorham, Anant Raj, Lester Mackey", "title": "Stochastic Stein Discrepancies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stein discrepancies (SDs) monitor convergence and non-convergence in\napproximate inference when exact integration and sampling are intractable.\nHowever, the computation of a Stein discrepancy can be prohibitive if the Stein\noperator - often a sum over likelihood terms or potentials - is expensive to\nevaluate. To address this deficiency, we show that stochastic Stein\ndiscrepancies (SSDs) based on subsampled approximations of the Stein operator\ninherit the convergence control properties of standard SDs with probability 1.\nAlong the way, we establish the convergence of Stein variational gradient\ndescent (SVGD) on unbounded domains, resolving an open question of Liu (2017).\nIn our experiments with biased Markov chain Monte Carlo (MCMC) hyperparameter\ntuning, approximate MCMC sampler selection, and stochastic SVGD, SSDs deliver\ncomparable inferences to standard SDs with orders of magnitude fewer likelihood\nevaluations.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 16:15:33 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 18:48:24 GMT"}, {"version": "v3", "created": "Tue, 20 Oct 2020 01:49:55 GMT"}, {"version": "v4", "created": "Thu, 22 Oct 2020 18:56:24 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Gorham", "Jackson", ""], ["Raj", "Anant", ""], ["Mackey", "Lester", ""]]}, {"id": "2007.02861", "submitter": "Luka Petrovi\\'c", "authors": "Luka V. Petrovi\\'c and Ingo Scholtes", "title": "Learning the Markov order of paths in a network", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning the Markov order in categorical sequences\nthat represent paths in a network, i.e. sequences of variable lengths where\ntransitions between states are constrained to a known graph. Such data pose\nchallenges for standard Markov order detection methods and demand modelling\ntechniques that explicitly account for the graph constraint. Adopting a\nmulti-order modelling framework for paths, we develop a Bayesian learning\ntechnique that (i) more reliably detects the correct Markov order compared to a\ncompeting method based on the likelihood ratio test, (ii) requires considerably\nless data compared to methods using AIC or BIC, and (iii) is robust against\npartial knowledge of the underlying constraints. We further show that a\nrecently published method that uses a likelihood ratio test has a tendency to\noverfit the true Markov order of paths, which is not the case for our Bayesian\ntechnique. Our method is important for data scientists analyzing patterns in\ncategorical sequence data that are subject to (partially) known constraints,\ne.g. sequences with forbidden words, mobility trajectories and click stream\ndata, or sequence data in bioinformatics. Addressing the key challenge of model\nselection, our work is further relevant for the growing body of research that\nemphasizes the need for higher-order models in network analysis.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 16:27:02 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Petrovi\u0107", "Luka V.", ""], ["Scholtes", "Ingo", ""]]}, {"id": "2007.02863", "submitter": "Silviu Pitis", "authors": "Silviu Pitis, Elliot Creager, Animesh Garg", "title": "Counterfactual Data Augmentation using Locally Factored Dynamics", "comments": "In Proceedings of NeurIPS 2020. 10 pages (+5 references, +12\n  appendix). Code available at \\url{https://github.com/spitis/mrl}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many dynamic processes, including common scenarios in robotic control and\nreinforcement learning (RL), involve a set of interacting subprocesses. Though\nthe subprocesses are not independent, their interactions are often sparse, and\nthe dynamics at any given time step can often be decomposed into locally\nindependent causal mechanisms. Such local causal structures can be leveraged to\nimprove the sample efficiency of sequence prediction and off-policy\nreinforcement learning. We formalize this by introducing local causal models\n(LCMs), which are induced from a global causal model by conditioning on a\nsubset of the state space. We propose an approach to inferring these structures\ngiven an object-oriented state representation, as well as a novel algorithm for\nCounterfactual Data Augmentation (CoDA). CoDA uses local structures and an\nexperience replay to generate counterfactual experiences that are causally\nvalid in the global model. We find that CoDA significantly improves the\nperformance of RL agents in locally factored tasks, including the\nbatch-constrained and goal-conditioned settings.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 16:29:00 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 23:50:13 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Pitis", "Silviu", ""], ["Creager", "Elliot", ""], ["Garg", "Animesh", ""]]}, {"id": "2007.02876", "submitter": "James Vuckovic", "authors": "James Vuckovic, Aristide Baratin, Remi Tachet des Combes", "title": "A Mathematical Theory of Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Attention is a powerful component of modern neural networks across a wide\nvariety of domains. However, despite its ubiquity in machine learning, there is\na gap in our understanding of attention from a theoretical point of view. We\npropose a framework to fill this gap by building a mathematically equivalent\nmodel of attention using measure theory. With this model, we are able to\ninterpret self-attention as a system of self-interacting particles, we shed\nlight on self-attention from a maximum entropy perspective, and we show that\nattention is actually Lipschitz-continuous (with an appropriate metric) under\nsuitable assumptions. We then apply these insights to the problem of\nmis-specified input data; infinitely-deep, weight-sharing self-attention\nnetworks; and more general Lipschitz estimates for a specific type of attention\nstudied in concurrent work.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 16:42:24 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 13:57:49 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Vuckovic", "James", ""], ["Baratin", "Aristide", ""], ["Combes", "Remi Tachet des", ""]]}, {"id": "2007.02896", "submitter": "Xinliang Zhang", "authors": "Xinliang Zhang, Mojtaba Vaezi", "title": "Multi-Objective DNN-based Precoder for MIMO Communications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a unified deep neural network (DNN)-based precoder for\ntwo-user multiple-input multiple-output (MIMO) networks with five objectives:\ndata transmission, energy harvesting, simultaneous wireless information and\npower transfer, physical layer (PHY) security, and multicasting. First, a\nrotation-based precoding is developed to solve the above problems\nindependently. Rotation-based precoding is new precoding and power allocation\nthat beats existing solutions in PHY security and multicasting and is reliable\nin different antenna settings. Next, a DNN-based precoder is designed to unify\nthe solution for all objectives. The proposed DNN concurrently learns the\nsolutions given by conventional methods, i.e., analytical or rotation-based\nsolutions. A binary vector is designed as an input feature to distinguish the\nobjectives. Numerical results demonstrate that, compared to the conventional\nsolutions, the proposed DNN-based precoder reduces on-the-fly computational\ncomplexity more than an order of magnitude while reaching near-optimal\nperformance (99.45\\% of the averaged optimal solutions). The new precoder is\nalso more robust to the variations of the numbers of antennas at the receivers.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 17:20:46 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Zhang", "Xinliang", ""], ["Vaezi", "Mojtaba", ""]]}, {"id": "2007.02901", "submitter": "P\\'eter Mernyei", "authors": "P\\'eter Mernyei, C\\u{a}t\\u{a}lina Cangea", "title": "Wiki-CS: A Wikipedia-Based Benchmark for Graph Neural Networks", "comments": "Graph Representation Learning and Beyond workshop (ICML 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Wiki-CS, a novel dataset derived from Wikipedia for benchmarking\nGraph Neural Networks. The dataset consists of nodes corresponding to Computer\nScience articles, with edges based on hyperlinks and 10 classes representing\ndifferent branches of the field. We use the dataset to evaluate semi-supervised\nnode classification and single-relation link prediction models. Our experiments\nshow that these methods perform well on a new domain, with structural\nproperties different from earlier benchmarks. The dataset is publicly\navailable, along with the implementation of the data pipeline and the benchmark\nexperiments, at https://github.com/pmernyei/wiki-cs-dataset .\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 17:25:47 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Mernyei", "P\u00e9ter", ""], ["Cangea", "C\u0103t\u0103lina", ""]]}, {"id": "2007.02912", "submitter": "Ruqi Zhang", "authors": "Ruqi Zhang, Yingzhen Li, Christopher De Sa, Sam Devlin, Cheng Zhang", "title": "Meta-Learning Divergences of Variational Inference", "comments": "Published at AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference (VI) plays an essential role in approximate Bayesian\ninference due to its computational efficiency and broad applicability. Crucial\nto the performance of VI is the selection of the associated divergence measure,\nas VI approximates the intractable distribution by minimizing this divergence.\nIn this paper we propose a meta-learning algorithm to learn the divergence\nmetric suited for the task of interest, automating the design of VI methods. In\naddition, we learn the initialization of the variational parameters without\nadditional cost when our method is deployed in the few-shot learning scenarios.\nWe demonstrate our approach outperforms standard VI on Gaussian mixture\ndistribution approximation, Bayesian neural network regression, image\ngeneration with variational autoencoders and recommender systems with a partial\nvariational autoencoder.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 17:43:01 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 20:28:21 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Zhang", "Ruqi", ""], ["Li", "Yingzhen", ""], ["De Sa", "Christopher", ""], ["Devlin", "Sam", ""], ["Zhang", "Cheng", ""]]}, {"id": "2007.02914", "submitter": "Lin Lan", "authors": "Lin Lan, Pinghui Wang, Xuefeng Du, Kaikai Song, Jing Tao, Xiaohong\n  Guan", "title": "Node Classification on Graphs with Few-Shot Novel Labels via Meta\n  Transformed Network Embedding", "comments": "Accepted to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of node classification on graphs with few-shot novel\nlabels, which has two distinctive properties: (1) There are novel labels to\nemerge in the graph; (2) The novel labels have only a few representative nodes\nfor training a classifier. The study of this problem is instructive and\ncorresponds to many applications such as recommendations for newly formed\ngroups with only a few users in online social networks. To cope with this\nproblem, we propose a novel Meta Transformed Network Embedding framework\n(MetaTNE), which consists of three modules: (1) A \\emph{structural module}\nprovides each node a latent representation according to the graph structure.\n(2) A \\emph{meta-learning module} captures the relationships between the graph\nstructure and the node labels as prior knowledge in a meta-learning manner.\nAdditionally, we introduce an \\emph{embedding transformation function} that\nremedies the deficiency of the straightforward use of meta-learning.\nInherently, the meta-learned prior knowledge can be used to facilitate the\nlearning of few-shot novel labels. (3) An \\emph{optimization module} employs a\nsimple yet effective scheduling strategy to train the above two modules with a\nbalance between graph structure learning and meta-learning. Experiments on four\nreal-world datasets show that MetaTNE brings a huge improvement over the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 17:45:11 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 09:41:44 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Lan", "Lin", ""], ["Wang", "Pinghui", ""], ["Du", "Xuefeng", ""], ["Song", "Kaikai", ""], ["Tao", "Jing", ""], ["Guan", "Xiaohong", ""]]}, {"id": "2007.02923", "submitter": "Saeed Sharifi-Malvajerdi", "authors": "Seth Neel, Aaron Roth, Saeed Sharifi-Malvajerdi", "title": "Descent-to-Delete: Gradient-Based Methods for Machine Unlearning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the data deletion problem for convex models. By leveraging\ntechniques from convex optimization and reservoir sampling, we give the first\ndata deletion algorithms that are able to handle an arbitrarily long sequence\nof adversarial updates while promising both per-deletion run-time and\nsteady-state error that do not grow with the length of the update sequence. We\nalso introduce several new conceptual distinctions: for example, we can ask\nthat after a deletion, the entire state maintained by the optimization\nalgorithm is statistically indistinguishable from the state that would have\nresulted had we retrained, or we can ask for the weaker condition that only the\nobservable output is statistically indistinguishable from the observable output\nthat would have resulted from retraining. We are able to give more efficient\ndeletion algorithms under this weaker deletion criterion.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 17:54:35 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Neel", "Seth", ""], ["Roth", "Aaron", ""], ["Sharifi-Malvajerdi", "Saeed", ""]]}, {"id": "2007.02924", "submitter": "Albert Qiaochu Jiang", "authors": "Yuhuai Wu, Albert Qiaochu Jiang, Jimmy Ba, Roger Grosse", "title": "INT: An Inequality Benchmark for Evaluating Generalization in Theorem\n  Proving", "comments": "Published as a conference paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.LO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In learning-assisted theorem proving, one of the most critical challenges is\nto generalize to theorems unlike those seen at training time. In this paper, we\nintroduce INT, an INequality Theorem proving benchmark, specifically designed\nto test agents' generalization ability. INT is based on a procedure for\ngenerating theorems and proofs; this procedure's knobs allow us to measure 6\ndifferent types of generalization, each reflecting a distinct challenge\ncharacteristic to automated theorem proving. In addition, unlike prior\nbenchmarks for learning-assisted theorem proving, INT provides a lightweight\nand user-friendly theorem proving environment with fast simulations, conducive\nto performing learning-based and search-based research. We introduce\nlearning-based baselines and evaluate them across 6 dimensions of\ngeneralization with the benchmark. We then evaluate the same agents augmented\nwith Monte Carlo Tree Search (MCTS) at test time, and show that MCTS can help\nto prove new theorems.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 17:55:33 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 16:21:59 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wu", "Yuhuai", ""], ["Jiang", "Albert Qiaochu", ""], ["Ba", "Jimmy", ""], ["Grosse", "Roger", ""]]}, {"id": "2007.02929", "submitter": "Rooholla Khorrambakht", "authors": "R. Khorrambakht, H. Damirchi, and H. D. Taghirad", "title": "Preintegrated IMU Features For Efficient Deep Inertial Odometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MEMS Inertial Measurement Units (IMUs) are inexpensive and effective sensors\nthat provide proprioceptive motion measurements for many robots and consumer\ndevices. However, their noise characteristics and manufacturing imperfections\nlead to complex ramifications in classical fusion pipelines. While deep\nlearning models provide the required flexibility to model these complexities\nfrom data, they have higher computation and memory requirements, making them\nimpractical choices for low-power and embedded applications. This paper\nattempts to address the mentioned conflict by proposing a computationally,\nefficient inertial representation for deep inertial odometry. Replacing the raw\nIMU data in deep Inertial models, preintegrated features improves the model's\nefficiency. The effectiveness of this method has been demonstrated for the task\nof pedestrian inertial odometry, and its efficiency has been shown through its\nembedded implementation on a microcontroller with restricted resources.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 17:58:35 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Khorrambakht", "R.", ""], ["Damirchi", "H.", ""], ["Taghirad", "H. D.", ""]]}, {"id": "2007.02931", "submitter": "Marvin Zhang", "authors": "Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey\n  Levine, Chelsea Finn", "title": "Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group\n  Distribution Shift", "comments": "Project website:\n  https://sites.google.com/view/adaptive-risk-minimization ; Code:\n  https://github.com/henrikmarklund/arm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental assumption of most machine learning algorithms is that the\ntraining and test data are drawn from the same underlying distribution.\nHowever, this assumption is violated in almost all practical applications:\nmachine learning systems are regularly tested under distribution shift, due to\nchanging temporal correlations, atypical end users, or other factors. In this\nwork, we consider the setting where the training data are structured into\ngroups and there may be multiple test time shifts, corresponding to new groups\nor group distributions. Most prior methods aim to learn a single robust model\nor invariant feature space to tackle this group shift. In contrast, we aim to\nlearn models that adapt at test time to shift using unlabeled test points. Our\nprimary contribution is to introduce the framework of adaptive risk\nminimization (ARM), in which models are optimized for post adaptation\nperformance on training batches sampled from different groups, which simulate\ngroup shifts that may occur at test time. We use meta-learning to solve the ARM\nproblem, and compared to prior methods for robustness, invariance, and\nadaptation, ARM methods provide consistent gains of 1-4% test accuracy on image\nclassification problems exhibiting group shift.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 17:59:30 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 00:48:51 GMT"}, {"version": "v3", "created": "Mon, 8 Feb 2021 19:58:18 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Zhang", "Marvin", ""], ["Marklund", "Henrik", ""], ["Dhawan", "Nikita", ""], ["Gupta", "Abhishek", ""], ["Levine", "Sergey", ""], ["Finn", "Chelsea", ""]]}, {"id": "2007.02933", "submitter": "Allan Zhou", "authors": "Allan Zhou, Tom Knowles, Chelsea Finn", "title": "Meta-Learning Symmetries by Reparameterization", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many successful deep learning architectures are equivariant to certain\ntransformations in order to conserve parameters and improve generalization:\nmost famously, convolution layers are equivariant to shifts of the input. This\napproach only works when practitioners know the symmetries of the task and can\nmanually construct an architecture with the corresponding equivariances. Our\ngoal is an approach for learning equivariances from data, without needing to\ndesign custom task-specific architectures. We present a method for learning and\nencoding equivariances into networks by learning corresponding parameter\nsharing patterns from data. Our method can provably represent\nequivariance-inducing parameter sharing for any finite group of symmetry\ntransformations. Our experiments suggest that it can automatically learn to\nencode equivariances to common transformations used in image processing tasks.\nWe provide our experiment code at\nhttps://github.com/AllanYangZhou/metalearning-symmetries.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 17:59:54 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 17:59:54 GMT"}, {"version": "v3", "created": "Tue, 30 Mar 2021 06:44:43 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Zhou", "Allan", ""], ["Knowles", "Tom", ""], ["Finn", "Chelsea", ""]]}, {"id": "2007.02938", "submitter": "Anant Raj", "authors": "Anant Raj, Stefan Bauer, Ashkan Soleymani, Michel Besserve and\n  Bernhard Sch\\\"olkopf", "title": "Causal Feature Selection via Orthogonal Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of inferring the direct causal parents of a response variable\namong a large set of explanatory variables is of high practical importance in\nmany disciplines. Recent work in the field of causal discovery exploits\ninvariance properties of models across different experimental conditions for\ndetecting direct causal links. However, these approaches generally do not scale\nwell with the number of explanatory variables, are difficult to extend to\nnonlinear relationships, and require data across different experiments.\nInspired by {\\em Debiased} machine learning methods, we study a\none-vs.-the-rest feature selection approach to discover the direct causal\nparent of the response. We propose an algorithm that works for purely\nobservational data, while also offering theoretical guarantees, including the\ncase of partially nonlinear relationships. Requiring only one estimation for\neach variable, we can apply our approach even to large graphs, demonstrating\nsignificant improvements compared to established approaches.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 12:56:43 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Raj", "Anant", ""], ["Bauer", "Stefan", ""], ["Soleymani", "Ashkan", ""], ["Besserve", "Michel", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "2007.02977", "submitter": "Hanlin Lu", "authors": "Hanlin Lu, Changchang Liu, Ting He, Shiqiang Wang and Kevin S. Chan", "title": "Sharing Models or Coresets: A Study based on Membership Inference Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed machine learning generally aims at training a global model based\non distributed data without collecting all the data to a centralized location,\nwhere two different approaches have been proposed: collecting and aggregating\nlocal models (federated learning) and collecting and training over\nrepresentative data summaries (coreset). While each approach preserves data\nprivacy to some extent thanks to not sharing the raw data, the exact extent of\nprotection is unclear under sophisticated attacks that try to infer the raw\ndata from the shared information. We present the first comparison between the\ntwo approaches in terms of target model accuracy, communication cost, and data\nprivacy, where the last is measured by the accuracy of a state-of-the-art\nattack strategy called the membership inference attack. Our experiments\nquantify the accuracy-privacy-cost tradeoff of each approach, and reveal a\nnontrivial comparison that can be used to guide the design of model training\nprocesses.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 18:06:53 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Lu", "Hanlin", ""], ["Liu", "Changchang", ""], ["He", "Ting", ""], ["Wang", "Shiqiang", ""], ["Chan", "Kevin S.", ""]]}, {"id": "2007.03020", "submitter": "Lakshya Kumar", "authors": "Shreyas Mangalgi, Lakshya Kumar and Ravindra Babu Tallamraju", "title": "Deep Contextual Embeddings for Address Classification in E-commerce", "comments": "9 Pages, 8 Figures, AI for fashion supply chain, KDD2020 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  E-commerce customers in developing nations like India tend to follow no fixed\nformat while entering shipping addresses. Parsing such addresses is challenging\nbecause of a lack of inherent structure or hierarchy. It is imperative to\nunderstand the language of addresses, so that shipments can be routed without\ndelays. In this paper, we propose a novel approach towards understanding\ncustomer addresses by deriving motivation from recent advances in Natural\nLanguage Processing (NLP). We also formulate different pre-processing steps for\naddresses using a combination of edit distance and phonetic algorithms. Then we\napproach the task of creating vector representations for addresses using\nWord2Vec with TF-IDF, Bi-LSTM and BERT based approaches. We compare these\napproaches with respect to sub-region classification task for North and South\nIndian cities. Through experiments, we demonstrate the effectiveness of\ngeneralized RoBERTa model, pre-trained over a large address corpus for language\nmodelling task. Our proposed RoBERTa model achieves a classification accuracy\nof around 90% with minimal text preprocessing for sub-region classification\ntask outperforming all other approaches. Once pre-trained, the RoBERTa model\ncan be fine-tuned for various downstream tasks in supply chain like pincode\nsuggestion and geo-coding. The model generalizes well for such tasks even with\nlimited labelled data. To the best of our knowledge, this is the first of its\nkind research proposing a novel approach of understanding customer addresses in\ne-commerce domain by pre-training language models and fine-tuning them for\ndifferent purposes.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 19:06:34 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Mangalgi", "Shreyas", ""], ["Kumar", "Lakshya", ""], ["Tallamraju", "Ravindra Babu", ""]]}, {"id": "2007.03046", "submitter": "Eyke H\\\"ullermeier", "authors": "Sadegh Abbaszadeh and Eyke H\\\"ullermeier", "title": "Machine Learning with the Sugeno Integral: The Case of Binary\n  Classification", "comments": "20 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we elaborate on the use of the Sugeno integral in the context\nof machine learning. More specifically, we propose a method for binary\nclassification, in which the Sugeno integral is used as an aggregation function\nthat combines several local evaluations of an instance, pertaining to different\nfeatures or measurements, into a single global evaluation. Due to the specific\nnature of the Sugeno integral, this approach is especially suitable for\nlearning from ordinal data, that is, when measurements are taken from ordinal\nscales. This is a topic that has not received much attention in machine\nlearning so far. The core of the learning problem itself consists of\nidentifying the capacity underlying the Sugeno integral. To tackle this\nproblem, we develop an algorithm based on linear programming. The algorithm\nalso includes a suitable technique for transforming the original feature values\ninto local evaluations (local utility scores), as well as a method for tuning a\nthreshold on the global evaluation. To control the flexibility of the\nclassifier and mitigate the problem of overfitting the training data, we\ngeneralize our approach toward $k$-maxitive capacities, where $k$ plays the\nrole of a hyper-parameter of the learner. We present experimental studies, in\nwhich we compare our method with competing approaches on several benchmark data\nsets.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 20:22:01 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Abbaszadeh", "Sadegh", ""], ["H\u00fcllermeier", "Eyke", ""]]}, {"id": "2007.03047", "submitter": "Vivien Sainte Fare Garnot", "authors": "Vivien Sainte Fare Garnot and Loic Landrieu", "title": "Leveraging Class Hierarchies with Metric-Guided Prototype Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Not all errors are created equal. This is especially true for many key\nmachine learning applications. In the case of classification tasks, the\nseverity of errors can be summarized under the form of a cost matrix, which\nassesses the gravity of confusing each pair of classes. When the target classes\nare organized into a hierarchical structure, this matrix defines a metric. We\npropose to integrate this metric in a new and versatile classification layer in\norder to model the disparity of errors. Our method relies on jointly learning a\nfeature-extracting network and a set of class representations, or prototypes,\nwhich incorporate the error metric into their relative arrangement in the\nembedding space. Our approach allows for consistent improvement of the severity\nof the network's errors with regard to the cost matrix. Furthermore, when the\ninduced metric contains insight on the data structure, our approach improves\nthe overall precision as well. Experiments on four different public datasets --\nfrom agricultural time series classification to depth image semantic\nsegmentation -- validate our approach.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 20:22:08 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 15:08:46 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Garnot", "Vivien Sainte Fare", ""], ["Landrieu", "Loic", ""]]}, {"id": "2007.03051", "submitter": "Raghavendra Selvan", "authors": "Lasse F. Wolff Anthony, Benjamin Kanding, Raghavendra Selvan", "title": "Carbontracker: Tracking and Predicting the Carbon Footprint of Training\n  Deep Learning Models", "comments": "Accepted to be presented at the ICML Workshop on \"Challenges in\n  Deploying and monitoring Machine Learning Systems\", 2020. Source code at this\n  link https://github.com/lfwa/carbontracker/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) can achieve impressive results across a wide variety of\ntasks, but this often comes at the cost of training models for extensive\nperiods on specialized hardware accelerators. This energy-intensive workload\nhas seen immense growth in recent years. Machine learning (ML) may become a\nsignificant contributor to climate change if this exponential trend continues.\nIf practitioners are aware of their energy and carbon footprint, then they may\nactively take steps to reduce it whenever possible. In this work, we present\nCarbontracker, a tool for tracking and predicting the energy and carbon\nfootprint of training DL models. We propose that energy and carbon footprint of\nmodel development and training is reported alongside performance metrics using\ntools like Carbontracker. We hope this will promote responsible computing in ML\nand encourage research into energy-efficient deep neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 20:24:31 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Anthony", "Lasse F. Wolff", ""], ["Kanding", "Benjamin", ""], ["Selvan", "Raghavendra", ""]]}, {"id": "2007.03071", "submitter": "Zhongnan Qu", "authors": "Zhongnan Qu, Cong Liu, Junfeng Guo, Lothar Thiele", "title": "Deep Partial Updating", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging edge intelligence applications require the server to continuously\nretrain and update deep neural networks deployed on remote edge nodes in order\nto leverage newly collected data samples. Unfortunately, it may be impossible\nin practice to continuously send fully updated weights to these edge nodes due\nto the highly constrained communication resource. In this paper, we propose the\nweight-wise deep partial updating paradigm, which smartly selects only a subset\nof weights to update at each server-to-edge communication round, while\nachieving a similar performance compared to full updating. Our method is\nestablished through analytically upper-bounding the loss difference between\npartial updating and full updating, and only updates the weights which make the\nlargest contributions to the upper bound. Extensive experimental results\ndemonstrate the efficacy of our partial updating methodology which achieves a\nhigh inference accuracy while updating a rather small number of weights.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 21:22:20 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 13:46:22 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Qu", "Zhongnan", ""], ["Liu", "Cong", ""], ["Guo", "Junfeng", ""], ["Thiele", "Lothar", ""]]}, {"id": "2007.03074", "submitter": "Wei-Cheng Chang", "authors": "Wei-Cheng Chang, Chun-Liang Li, Youssef Mroueh, Yiming Yang", "title": "Kernel Stein Generative Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in gradient-based Explicit Generative Modeling where\nsamples can be derived from iterative gradient updates based on an estimate of\nthe score function of the data distribution. Recent advances in Stochastic\nGradient Langevin Dynamics (SGLD) demonstrates impressive results with\nenergy-based models on high-dimensional and complex data distributions. Stein\nVariational Gradient Descent (SVGD) is a deterministic sampling algorithm that\niteratively transports a set of particles to approximate a given distribution,\nbased on functional gradient descent that decreases the KL divergence. SVGD has\npromising results on several Bayesian inference applications. However, applying\nSVGD on high dimensional problems is still under-explored. The goal of this\nwork is to study high dimensional inference with SVGD. We first identify key\nchallenges in practical kernel SVGD inference in high-dimension. We propose\nnoise conditional kernel SVGD (NCK-SVGD), that works in tandem with the\nrecently introduced Noise Conditional Score Network estimator. NCK is crucial\nfor successful inference with SVGD in high dimension, as it adapts the kernel\nto the noise level of the score estimate. As we anneal the noise, NCK-SVGD\ntargets the real data distribution. We then extend the annealed SVGD with an\nentropic regularization. We show that this offers a flexible control between\nsample quality and diversity, and verify it empirically by precision and recall\nevaluations. The NCK-SVGD produces samples comparable to GANs and annealed SGLD\non computer vision benchmarks, including MNIST and CIFAR-10.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 21:26:04 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Chang", "Wei-Cheng", ""], ["Li", "Chun-Liang", ""], ["Mroueh", "Youssef", ""], ["Yang", "Yiming", ""]]}, {"id": "2007.03092", "submitter": "Rex Ying", "authors": "Rex (Zhitao) Ying, Zhaoyu Lou, Jiaxuan You, Chengtao Wen, Arquimedes\n  Canedo, Jure Leskovec", "title": "Neural Subgraph Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subgraph matching is the problem of determining the presence and location(s)\nof a given query graph in a large target graph. Despite being an NP-complete\nproblem, the subgraph matching problem is crucial in domains ranging from\nnetwork science and database systems to biochemistry and cognitive science.\nHowever, existing techniques based on combinatorial matching and integer\nprogramming cannot handle matching problems with both large target and query\ngraphs. Here we propose NeuroMatch, an accurate, efficient, and robust neural\napproach to subgraph matching. NeuroMatch decomposes query and target graphs\ninto small subgraphs and embeds them using graph neural networks. Trained to\ncapture geometric constraints corresponding to subgraph relations, NeuroMatch\nthen efficiently performs subgraph matching directly in the embedding space.\nExperiments demonstrate NeuroMatch is 100x faster than existing combinatorial\napproaches and 18% more accurate than existing approximate subgraph matching\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 22:06:38 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 17:48:05 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Rex", "", "", "Zhitao"], ["Ying", "", ""], ["Lou", "Zhaoyu", ""], ["You", "Jiaxuan", ""], ["Wen", "Chengtao", ""], ["Canedo", "Arquimedes", ""], ["Leskovec", "Jure", ""]]}, {"id": "2007.03100", "submitter": "Emiliano Valdez", "authors": "Banghee So and Jean-Philippe Boucher and Emiliano A. Valdez", "title": "Cost-sensitive Multi-class AdaBoost for Understanding Driving Behavior\n  with Telematics", "comments": "27 pages, 9 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Powered with telematics technology, insurers can now capture a wide range of\ndata, such as distance traveled, how drivers brake, accelerate or make turns,\nand travel frequency each day of the week, to better decode driver's behavior.\nSuch additional information helps insurers improve risk assessments for\nusage-based insurance (UBI), an increasingly popular industry innovation. In\nthis article, we explore how to integrate telematics information to better\npredict claims frequency. For motor insurance during a policy year, we\ntypically observe a large proportion of drivers with zero claims, a less\nproportion with exactly one claim, and far lesser with two or more claims. We\nintroduce the use of a cost-sensitive multi-class adaptive boosting (AdaBoost)\nalgorithm, which we call SAMME.C2, to handle such imbalances. To calibrate\nSAMME.C2 algorithm, we use empirical data collected from a telematics program\nin Canada and we find improved assessment of driving behavior with telematics\nrelative to traditional risk variables. We demonstrate our algorithm can\noutperform other models that can handle class imbalances: SAMME, SAMME with\nSMOTE, RUSBoost, and SMOTEBoost. The sampled data on telematics were\nobservations during 2013-2016 for which 50,301 are used for training and\nanother 21,574 for testing. Broadly speaking, the additional information\nderived from vehicle telematics helps refine risk classification of drivers of\nUBI.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 22:26:56 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["So", "Banghee", ""], ["Boucher", "Jean-Philippe", ""], ["Valdez", "Emiliano A.", ""]]}, {"id": "2007.03109", "submitter": "Yuan-Sen Ting Dr.", "authors": "Teaghan O'Briain, Yuan-Sen Ting, S\\'ebastien Fabbro, Kwang M. Yi, Kim\n  Venn, Spencer Bialek", "title": "Cycle-StarNet: Bridging the gap between theory and data by leveraging\n  large datasets", "comments": "23 pages, 15 figures, 2 tables, accepted for publication on Nov 12,\n  2020, Nov 12. A companion 4-page preview is accepted to the ICML 2020 Machine\n  Learning Interpretability for Scientific Discovery workshop. The code used in\n  this study is made publicly available on github:\n  https://github.com/teaghan/Cycle_SN", "journal-ref": "2021, ApJ, 906, 130", "doi": "10.3847/1538-4357/abca96", "report-no": null, "categories": "astro-ph.SR astro-ph.GA astro-ph.IM physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advancements in stellar spectroscopy data acquisition have made it\nnecessary to accomplish similar improvements in efficient data analysis\ntechniques. Current automated methods for analyzing spectra are either (a)\ndata-driven, which requires prior knowledge of stellar parameters and elemental\nabundances, or (b) based on theoretical synthetic models that are susceptible\nto the gap between theory and practice. In this study, we present a hybrid\ngenerative domain adaptation method that turns simulated stellar spectra into\nrealistic spectra by applying unsupervised learning to large spectroscopic\nsurveys. We apply our technique to the APOGEE H-band spectra at R=22,500 and\nthe Kurucz synthetic models. As a proof of concept, two case studies are\npresented. The first of which is the calibration of synthetic data to become\nconsistent with observations. To accomplish this, synthetic models are morphed\ninto spectra that resemble observations, thereby reducing the gap between\ntheory and observations. Fitting the observed spectra shows an improved average\nreduced $\\chi_R^2$ from 1.97 to 1.22, along with a reduced mean residual from\n0.16 to -0.01 in normalized flux. The second case study is the identification\nof the elemental source of missing spectral lines in the synthetic modelling. A\nmock dataset is used to show that absorption lines can be recovered when they\nare absent in one of the domains. This method can be applied to other fields,\nwhich use large data sets and are currently limited by modelling accuracy. The\ncode used in this study is made publicly available on github.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 23:06:58 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 22:56:32 GMT"}, {"version": "v3", "created": "Sat, 14 Nov 2020 04:00:37 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["O'Briain", "Teaghan", ""], ["Ting", "Yuan-Sen", ""], ["Fabbro", "S\u00e9bastien", ""], ["Yi", "Kwang M.", ""], ["Venn", "Kim", ""], ["Bialek", "Spencer", ""]]}, {"id": "2007.03112", "submitter": "Yuan-Sen Ting Dr.", "authors": "Teaghan O'Briain, Yuan-Sen Ting, S\\'ebastien Fabbro, Kwang M. Yi, Kim\n  Venn, Spencer Bialek", "title": "Interpreting Stellar Spectra with Unsupervised Domain Adaptation", "comments": "4 pages, 4 figure, accepted to the ICML 2020 Machine Learning\n  Interpretability for Scientific Discovery workshop. A full 20-page version is\n  submitted to ApJ. The code used in this study is made publicly available on\n  github: https://github.com/teaghan/Cycle_SN", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.SR astro-ph.GA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss how to achieve mapping from large sets of imperfect simulations\nand observational data with unsupervised domain adaptation. Under the\nhypothesis that simulated and observed data distributions share a common\nunderlying representation, we show how it is possible to transfer between\nsimulated and observed domains. Driven by an application to interpret stellar\nspectroscopic sky surveys, we construct the domain transfer pipeline from two\nadversarial autoencoders on each domains with a disentangling latent space, and\na cycle-consistency constraint. We then construct a differentiable pipeline\nfrom physical stellar parameters to realistic observed spectra, aided by a\nsupplementary generative surrogate physics emulator network. We further\nexemplify the potential of the method on the reconstructed spectra quality and\nto discover new spectral features associated to elemental abundances.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 23:09:13 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["O'Briain", "Teaghan", ""], ["Ting", "Yuan-Sen", ""], ["Fabbro", "S\u00e9bastien", ""], ["Yi", "Kwang M.", ""], ["Venn", "Kim", ""], ["Bialek", "Spencer", ""]]}, {"id": "2007.03114", "submitter": "Adam Fisch", "authors": "Adam Fisch, Tal Schuster, Tommi Jaakkola, Regina Barzilay", "title": "Efficient Conformal Prediction via Cascaded Inference with Expanded\n  Admission", "comments": "ICLR 2021. Revision of \"Relaxed Conformal Prediction Cascades for\n  Efficient Inference Over Many Labels\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel approach for conformal prediction (CP), in\nwhich we aim to identify a set of promising prediction candidates -- in place\nof a single prediction. This set is guaranteed to contain a correct answer with\nhigh probability, and is well-suited for many open-ended classification tasks.\nIn the standard CP paradigm, the predicted set can often be unusably large and\nalso costly to obtain. This is particularly pervasive in settings where the\ncorrect answer is not unique, and the number of total possible answers is high.\nWe first expand the CP correctness criterion to allow for additional, inferred\n\"admissible\" answers, which can substantially reduce the size of the predicted\nset while still providing valid performance guarantees. Second, we amortize\ncosts by conformalizing prediction cascades, in which we aggressively prune\nimplausible labels early on by using progressively stronger classifiers --\nagain, while still providing valid performance guarantees. We demonstrate the\nempirical effectiveness of our approach for multiple applications in natural\nlanguage processing and computational chemistry for drug discovery.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 23:13:07 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 19:56:04 GMT"}, {"version": "v3", "created": "Tue, 2 Feb 2021 06:29:04 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Fisch", "Adam", ""], ["Schuster", "Tal", ""], ["Jaakkola", "Tommi", ""], ["Barzilay", "Regina", ""]]}, {"id": "2007.03117", "submitter": "Shibo Li", "authors": "Shibo Li, Wei Xing, Mike Kirby and Shandian Zhe", "title": "Multi-Fidelity Bayesian Optimization via Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization (BO) is a popular framework to optimize black-box\nfunctions. In many applications, the objective function can be evaluated at\nmultiple fidelities to enable a trade-off between the cost and accuracy. To\nreduce the optimization cost, many multi-fidelity BO methods have been\nproposed. Despite their success, these methods either ignore or over-simplify\nthe strong, complex correlations across the fidelities, and hence can be\ninefficient in estimating the objective function. To address this issue, we\npropose Deep Neural Network Multi-Fidelity Bayesian Optimization (DNN-MFBO)\nthat can flexibly capture all kinds of complicated relationships between the\nfidelities to improve the objective function estimation and hence the\noptimization performance. We use sequential, fidelity-wise Gauss-Hermite\nquadrature and moment-matching to fulfill a mutual information-based\nacquisition function, which is computationally tractable and efficient. We show\nthe advantages of our method in both synthetic benchmark datasets and\nreal-world applications in engineering design.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 23:28:40 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 13:43:38 GMT"}, {"version": "v3", "created": "Wed, 21 Oct 2020 17:12:50 GMT"}, {"version": "v4", "created": "Thu, 10 Dec 2020 05:29:15 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Li", "Shibo", ""], ["Xing", "Wei", ""], ["Kirby", "Mike", ""], ["Zhe", "Shandian", ""]]}, {"id": "2007.03121", "submitter": "Wenbo Ren", "authors": "Wenbo Ren, Xingyu Zhou, Jia Liu, Ness B. Shroff", "title": "Multi-Armed Bandits with Local Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the problem of regret minimization for multi-armed\nbandit (MAB) problems with local differential privacy (LDP) guarantee. In\nstochastic bandit systems, the rewards may refer to the users' activities,\nwhich may involve private information and the users may not want the agent to\nknow. However, in many cases, the agent needs to know these activities to\nprovide better services such as recommendations and news feeds. To handle this\ndilemma, we adopt differential privacy and study the regret upper and lower\nbounds for MAB algorithms with a given LDP guarantee. In this paper, we prove a\nlower bound and propose algorithms whose regret upper bounds match the lower\nbound up to constant factors. Numerical experiments also confirm our\nconclusions.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 23:36:20 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Ren", "Wenbo", ""], ["Zhou", "Xingyu", ""], ["Liu", "Jia", ""], ["Shroff", "Ness B.", ""]]}, {"id": "2007.03133", "submitter": "Wenbo Ren", "authors": "Wenbo Ren, Jia Liu, Ness B. Shroff", "title": "The Sample Complexity of Best-$k$ Items Selection from Pairwise\n  Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the sample complexity (aka number of comparisons) bounds\nfor the active best-$k$ items selection from pairwise comparisons. From a given\nset of items, the learner can make pairwise comparisons on every pair of items,\nand each comparison returns an independent noisy result about the preferred\nitem. At any time, the learner can adaptively choose a pair of items to compare\naccording to past observations (i.e., active learning). The learner's goal is\nto find the (approximately) best-$k$ items with a given confidence, while\ntrying to use as few comparisons as possible. In this paper, we study two\nproblems: (i) finding the probably approximately correct (PAC) best-$k$ items\nand (ii) finding the exact best-$k$ items, both under strong stochastic\ntransitivity and stochastic triangle inequality. For PAC best-$k$ items\nselection, we first show a lower bound and then propose an algorithm whose\nsample complexity upper bound matches the lower bound up to a constant factor.\nFor the exact best-$k$ items selection, we first prove a worst-instance lower\nbound. We then propose two algorithms based on our PAC best items selection\nalgorithms: one works for $k=1$ and is sample complexity optimal up to a loglog\nfactor, and the other works for all values of $k$ and is sample complexity\noptimal up to a log factor.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 23:53:09 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Ren", "Wenbo", ""], ["Liu", "Jia", ""], ["Shroff", "Ness B.", ""]]}, {"id": "2007.03141", "submitter": "Yuan Wu", "authors": "Yuan Wu, Diana Inkpen and Ahmed El-Roby", "title": "Dual Mixup Regularized Learning for Adversarial Domain Adaptation", "comments": "This paper has been accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances on unsupervised domain adaptation (UDA) rely on adversarial\nlearning to disentangle the explanatory and transferable features for domain\nadaptation. However, there are two issues with the existing methods. First, the\ndiscriminability of the latent space cannot be fully guaranteed without\nconsidering the class-aware information in the target domain. Second, samples\nfrom the source and target domains alone are not sufficient for\ndomain-invariant feature extracting in the latent space. In order to alleviate\nthe above issues, we propose a dual mixup regularized learning (DMRL) method\nfor UDA, which not only guides the classifier in enhancing consistent\npredictions in-between samples, but also enriches the intrinsic structures of\nthe latent space. The DMRL jointly conducts category and domain mixup\nregularizations on pixel level to improve the effectiveness of models. A series\nof empirical studies on four domain adaptation benchmarks demonstrate that our\napproach can achieve the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 00:24:14 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 22:01:18 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Wu", "Yuan", ""], ["Inkpen", "Diana", ""], ["El-Roby", "Ahmed", ""]]}, {"id": "2007.03151", "submitter": "Adel Nabli", "authors": "Adel Nabli, Margarida Carvalho", "title": "Curriculum learning for multilevel budgeted combinatorial problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning heuristics for combinatorial optimization problems through graph\nneural networks have recently shown promising results on some classic NP-hard\nproblems. These are single-level optimization problems with only one player.\nMultilevel combinatorial optimization problems are their generalization,\nencompassing situations with multiple players taking decisions sequentially. By\nframing them in a multi-agent reinforcement learning setting, we devise a\nvalue-based method to learn to solve multilevel budgeted combinatorial problems\ninvolving two players in a zero-sum game over a graph. Our framework is based\non a simple curriculum: if an agent knows how to estimate the value of\ninstances with budgets up to $B$, then solving instances with budget $B+1$ can\nbe done in polynomial time regardless of the direction of the optimization by\nchecking the value of every possible afterstate. Thus, in a bottom-up approach,\nwe generate datasets of heuristically solved instances with increasingly larger\nbudgets to train our agent. We report results close to optimality on graphs up\nto $100$ nodes and a $185 \\times$ speedup on average compared to the quickest\nexact solver known for the Multilevel Critical Node problem, a max-min-max\ntrilevel problem that has been shown to be at least $\\Sigma_2^p$-hard.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 01:09:37 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 12:23:53 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Nabli", "Adel", ""], ["Carvalho", "Margarida", ""]]}, {"id": "2007.03155", "submitter": "Keisuke Fujii", "authors": "Keisuke Fujii, Naoya Takeishi, Yoshinobu Kawahara, Kazuya Takeda", "title": "Policy learning with partial observation and mechanical constraints for\n  multi-person modeling", "comments": "17 pages with 7 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Extracting the rules of real-world biological multi-agent behaviors is a\ncurrent challenge in various scientific and engineering fields. Biological\nagents generally have limited observation and mechanical constraints; however,\nmost of the conventional data-driven models ignore such assumptions, resulting\nin lack of biological plausibility and model interpretability for behavioral\nanalyses in biological and cognitive science. Here we propose sequential\ngenerative models with partial observation and mechanical constraints, which\ncan visualize whose information the agents utilize and can generate\nbiologically plausible actions. We formulate this as a decentralized\nmulti-agent imitation learning problem, leveraging binary partial observation\nmodels with a Gumbel-Softmax reparameterization and policy models based on\nhierarchical variational recurrent neural networks with physical and\nbiomechanical constraints. We investigate the empirical performances using\nreal-world multi-person motion datasets from basketball and soccer games.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 01:24:22 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Fujii", "Keisuke", ""], ["Takeishi", "Naoya", ""], ["Kawahara", "Yoshinobu", ""], ["Takeda", "Kazuya", ""]]}, {"id": "2007.03158", "submitter": "Harm van Seijen", "authors": "Harm van Seijen and Hadi Nekoei and Evan Racah and Sarath Chandar", "title": "The LoCA Regret: A Consistent Metric to Evaluate Model-Based Behavior in\n  Reinforcement Learning", "comments": "NeurIPS 2020, code: https://github.com/chandar-lab/LoCA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep model-based Reinforcement Learning (RL) has the potential to\nsubstantially improve the sample-efficiency of deep RL. While various\nchallenges have long held it back, a number of papers have recently come out\nreporting success with deep model-based methods. This is a great development,\nbut the lack of a consistent metric to evaluate such methods makes it difficult\nto compare various approaches. For example, the common single-task\nsample-efficiency metric conflates improvements due to model-based learning\nwith various other aspects, such as representation learning, making it\ndifficult to assess true progress on model-based RL. To address this, we\nintroduce an experimental setup to evaluate model-based behavior of RL methods,\ninspired by work from neuroscience on detecting model-based behavior in humans\nand animals. Our metric based on this setup, the Local Change Adaptation (LoCA)\nregret, measures how quickly an RL method adapts to a local change in the\nenvironment. Our metric can identify model-based behavior, even if the method\nuses a poor representation and provides insight in how close a method's\nbehavior is from optimal model-based behavior. We use our setup to evaluate the\nmodel-based behavior of MuZero on a variation of the classic Mountain Car task.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 01:34:55 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 12:18:07 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["van Seijen", "Harm", ""], ["Nekoei", "Hadi", ""], ["Racah", "Evan", ""], ["Chandar", "Sarath", ""]]}, {"id": "2007.03165", "submitter": "Kevin Ong", "authors": "Kevin Shen Hoong Ong, Yang Zhang, Dusit Niyato", "title": "Cognitive Radio Network Throughput Maximization with Deep Reinforcement\n  Learning", "comments": "5 pages, 2 figures, accepted in IEEE VTC-Fall 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radio Frequency powered Cognitive Radio Networks (RF-CRN) are likely to be\nthe eyes and ears of upcoming modern networks such as Internet of Things (IoT),\nrequiring increased decentralization and autonomous operation. To be considered\nautonomous, the RF-powered network entities need to make decisions locally to\nmaximize the network throughput under the uncertainty of any network\nenvironment. However, in complex and large-scale networks, the state and action\nspaces are usually large, and existing Tabular Reinforcement Learning technique\nis unable to find the optimal state-action policy quickly. In this paper, deep\nreinforcement learning is proposed to overcome the mentioned shortcomings and\nallow a wireless gateway to derive an optimal policy to maximize network\nthroughput. When benchmarked against advanced DQN techniques, our proposed DQN\nconfiguration offers performance speedup of up to 1.8x with good overall\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 01:49:07 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Ong", "Kevin Shen Hoong", ""], ["Zhang", "Yang", ""], ["Niyato", "Dusit", ""]]}, {"id": "2007.03167", "submitter": "Yingshui Tan", "authors": "Baihong Jin, Yingshui Tan, Yuxin Chen, Kameshwar Poolla, Alberto\n  Sangiovanni Vincentelli", "title": "Are Ensemble Classifiers Powerful Enough for the Detection and Diagnosis\n  of Intermediate-Severity Faults?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intermediate-Severity (IS) faults present milder symptoms compared to severe\nfaults, and are more difficult to detect and diagnose due to their close\nresemblance to normal operating conditions. The lack of IS fault examples in\nthe training data can pose severe risks to Fault Detection and Diagnosis (FDD)\nmethods that are built upon Machine Learning (ML) techniques, because these\nfaults can be easily mistaken as normal operating conditions. Ensemble models\nare widely applied in ML and are considered promising methods for detecting\nout-of-distribution (OOD) data. We identify common pitfalls in these models\nthrough extensive experiments with several popular ensemble models on two\nreal-world datasets. Then, we discuss how to design more effective ensemble\nmodels for detecting and diagnosing IS faults.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 02:05:04 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 23:48:06 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Jin", "Baihong", ""], ["Tan", "Yingshui", ""], ["Chen", "Yuxin", ""], ["Poolla", "Kameshwar", ""], ["Vincentelli", "Alberto Sangiovanni", ""]]}, {"id": "2007.03181", "submitter": "Xinyuan Liu", "authors": "Xinyuan Liu, Jihua Zhu, Qinghai Zheng, Zhongyu Li, Ruixin Liu and Jun\n  Wang", "title": "Bidirectional Loss Function for Label Enhancement and Distribution\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Label distribution learning (LDL) is an interpretable and general learning\nparadigm that has been applied in many real-world applications. In contrast to\nthe simple logical vector in single-label learning (SLL) and multi-label\nlearning (MLL), LDL assigns labels with a description degree to each instance.\nIn practice, two challenges exist in LDL, namely, how to address the\ndimensional gap problem during the learning process of LDL and how to exactly\nrecover label distributions from existing logical labels, i.e., Label\nEnhancement (LE). For most existing LDL and LE algorithms, the fact that the\ndimension of the input matrix is much higher than that of the output one is\nalway ignored and it typically leads to the dimensional reduction owing to the\nunidirectional projection. The valuable information hidden in the feature space\nis lost during the mapping process. To this end, this study considers\nbidirectional projections function which can be applied in LE and LDL problems\nsimultaneously. More specifically, this novel loss function not only considers\nthe mapping errors generated from the projection of the input space into the\noutput one but also accounts for the reconstruction errors generated from the\nprojection of the output space back to the input one. This loss function aims\nto potentially reconstruct the input data from the output data. Therefore, it\nis expected to obtain more accurate results. Finally, experiments on several\nreal-world datasets are carried out to demonstrate the superiority of the\nproposed method for both LE and LDL.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 03:02:54 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Liu", "Xinyuan", ""], ["Zhu", "Jihua", ""], ["Zheng", "Qinghai", ""], ["Li", "Zhongyu", ""], ["Liu", "Ruixin", ""], ["Wang", "Jun", ""]]}, {"id": "2007.03183", "submitter": "Manqing Dong", "authors": "Manqing Dong and Feng Yuan and Lina Yao and Xiwei Xu and Liming Zhu", "title": "MAMO: Memory-Augmented Meta-Optimization for Cold-start Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A common challenge for most current recommender systems is the cold-start\nproblem. Due to the lack of user-item interactions, the fine-tuned recommender\nsystems are unable to handle situations with new users or new items. Recently,\nsome works introduce the meta-optimization idea into the recommendation\nscenarios, i.e. predicting the user preference by only a few of past interacted\nitems. The core idea is learning a global sharing initialization parameter for\nall users and then learning the local parameters for each user separately.\nHowever, most meta-learning based recommendation approaches adopt\nmodel-agnostic meta-learning for parameter initialization, where the global\nsharing parameter may lead the model into local optima for some users. In this\npaper, we design two memory matrices that can store task-specific memories and\nfeature-specific memories. Specifically, the feature-specific memories are used\nto guide the model with personalized parameter initialization, while the\ntask-specific memories are used to guide the model fast predicting the user\npreference. And we adopt a meta-optimization approach for optimizing the\nproposed method. We test the model on two widely used recommendation datasets\nand consider four cold-start situations. The experimental results show the\neffectiveness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 03:25:15 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Dong", "Manqing", ""], ["Yuan", "Feng", ""], ["Yao", "Lina", ""], ["Xu", "Xiwei", ""], ["Zhu", "Liming", ""]]}, {"id": "2007.03196", "submitter": "Zhongkai Hao", "authors": "Zhongkai Hao, Chengqiang Lu, Zheyuan Hu, Hao Wang, Zhenya Huang, Qi\n  Liu, Enhong Chen, Cheekong Lee", "title": "ASGN: An Active Semi-supervised Graph Neural Network for Molecular\n  Property Prediction", "comments": "9 pages", "journal-ref": null, "doi": "10.1145/3394486.3403117", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Molecular property prediction (e.g., energy) is an essential problem in\nchemistry and biology. Unfortunately, many supervised learning methods usually\nsuffer from the problem of scarce labeled molecules in the chemical space,\nwhere such property labels are generally obtained by Density Functional Theory\n(DFT) calculation which is extremely computational costly. An effective\nsolution is to incorporate the unlabeled molecules in a semi-supervised\nfashion. However, learning semi-supervised representation for large amounts of\nmolecules is challenging, including the joint representation issue of both\nmolecular essence and structure, the conflict between representation and\nproperty leaning. Here we propose a novel framework called Active\nSemi-supervised Graph Neural Network (ASGN) by incorporating both labeled and\nunlabeled molecules. Specifically, ASGN adopts a teacher-student framework. In\nthe teacher model, we propose a novel semi-supervised learning method to learn\ngeneral representation that jointly exploits information from molecular\nstructure and molecular distribution. Then in the student model, we target at\nproperty prediction task to deal with the learning loss conflict. At last, we\nproposed a novel active learning strategy in terms of molecular diversities to\nselect informative data during the whole framework learning. We conduct\nextensive experiments on several public datasets. Experimental results show the\nremarkable performance of our ASGN framework.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 04:22:39 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Hao", "Zhongkai", ""], ["Lu", "Chengqiang", ""], ["Hu", "Zheyuan", ""], ["Wang", "Hao", ""], ["Huang", "Zhenya", ""], ["Liu", "Qi", ""], ["Chen", "Enhong", ""], ["Lee", "Cheekong", ""]]}, {"id": "2007.03198", "submitter": "Utku Ozbulak", "authors": "Utku Ozbulak, Jonathan Peck, Wesley De Neve, Bart Goossens, Yvan Saeys\n  and Arnout Van Messem", "title": "Regional Image Perturbation Reduces $L_p$ Norms of Adversarial Examples\n  While Maintaining Model-to-model Transferability", "comments": "Accepted for the ICML 2020, Workshop on Uncertainty and Robustness in\n  Deep Learning (UDL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regional adversarial attacks often rely on complicated methods for generating\nadversarial perturbations, making it hard to compare their efficacy against\nwell-known attacks. In this study, we show that effective regional\nperturbations can be generated without resorting to complex methods. We develop\na very simple regional adversarial perturbation attack method using\ncross-entropy sign, one of the most commonly used losses in adversarial machine\nlearning. Our experiments on ImageNet with multiple models reveal that, on\naverage, $76\\%$ of the generated adversarial examples maintain model-to-model\ntransferability when the perturbation is applied to local image regions.\nDepending on the selected region, these localized adversarial examples require\nsignificantly less $L_p$ norm distortion (for $p \\in \\{0, 2, \\infty\\}$)\ncompared to their non-local counterparts. These localized attacks therefore\nhave the potential to undermine defenses that claim robustness under the\naforementioned norms.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 04:33:16 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 08:23:59 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Ozbulak", "Utku", ""], ["Peck", "Jonathan", ""], ["De Neve", "Wesley", ""], ["Goossens", "Bart", ""], ["Saeys", "Yvan", ""], ["Van Messem", "Arnout", ""]]}, {"id": "2007.03203", "submitter": "Yuwen Yang", "authors": "Yuwen Yang, Jayant Rajgopal", "title": "Learning Combined Set Covering and Traveling Salesman Problem", "comments": "38 pages, 1 figure, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Traveling Salesman Problem is one of the most intensively studied\ncombinatorial optimization problems due both to its range of real-world\napplications and its computational complexity. When combined with the Set\nCovering Problem, it raises even more issues related to tractability and\nscalability. We study a combined Set Covering and Traveling Salesman problem\nand provide a mixed integer programming formulation to solve the problem.\nMotivated by applications where the optimal policy needs to be updated on a\nregular basis and repetitively solving this via MIP can be computationally\nexpensive, we propose a machine learning approach to effectively deal with this\nproblem by providing an opportunity to learn from historical optimal solutions\nthat are derived from the MIP formulation. We also present a case study using\nthe vaccine distribution chain of the World Health Organization, and provide\nnumerical results with data derived from four countries in sub-Saharan Africa.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 05:11:28 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Yang", "Yuwen", ""], ["Rajgopal", "Jayant", ""]]}, {"id": "2007.03204", "submitter": "Pashootan Vaezipoor", "authors": "Pashootan Vaezipoor, Gil Lederman, Yuhuai Wu, Chris J. Maddison, Roger\n  Grosse, Edward Lee, Sanjit A. Seshia, Fahiem Bacchus", "title": "Learning Branching Heuristics for Propositional Model Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.LO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Propositional model counting or #SAT is the problem of computing the number\nof satisfying assignments of a Boolean formula and many discrete probabilistic\ninference problems can be translated into a model counting problem to be solved\nby #SAT solvers. Generic ``exact'' #SAT solvers, however, are often not\nscalable to industrial-level instances. In this paper, we present Neuro#, an\napproach for learning branching heuristics for exact #SAT solvers via evolution\nstrategies (ES) to reduce the number of branching steps the solver takes to\nsolve an instance. We experimentally show that our approach not only reduces\nthe step count on similarly distributed held-out instances but it also\ngeneralizes to much larger instances from the same problem family. The gap\nbetween the learned and the vanilla solver on larger instances is sometimes so\nwide that the learned solver can even overcome the run time overhead of\nquerying the model and beat the vanilla in wall-clock time by orders of\nmagnitude.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 05:20:29 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Vaezipoor", "Pashootan", ""], ["Lederman", "Gil", ""], ["Wu", "Yuhuai", ""], ["Maddison", "Chris J.", ""], ["Grosse", "Roger", ""], ["Lee", "Edward", ""], ["Seshia", "Sanjit A.", ""], ["Bacchus", "Fahiem", ""]]}, {"id": "2007.03208", "submitter": "Min-Chun Wu", "authors": "Min-Chun Wu, Vladimir Itskov", "title": "A Topological Approach to Inferring the Intrinsic Dimension of Convex\n  Sensing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT math.CO q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a common measurement paradigm, where an unknown subset of an\naffine space is measured by unknown continuous quasi-convex functions. Given\nthe measurement data, can one determine the dimension of this space? In this\npaper, we develop a method for inferring the intrinsic dimension of the data\nfrom measurements by quasi-convex functions, under natural generic assumptions.\n  The dimension inference problem depends only on discrete data of the ordering\nof the measured points of space, induced by the sensor functions. We introduce\na construction of a filtration of Dowker complexes, associated to measurements\nby quasi-convex functions. Topological features of these complexes are then\nused to infer the intrinsic dimension. We prove convergence theorems that\nguarantee obtaining the correct intrinsic dimension in the limit of large data,\nunder natural generic assumptions. We also illustrate the usability of this\nmethod in simulations.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 05:35:23 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Wu", "Min-Chun", ""], ["Itskov", "Vladimir", ""]]}, {"id": "2007.03212", "submitter": "Doyup Lee", "authors": "Doyup Lee and Yeongjae Cheon", "title": "Soft Labeling Affects Out-of-Distribution Detection of Deep Neural\n  Networks", "comments": "ICML'20 Workshop on Uncertainty and Robustness in Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Soft labeling becomes a common output regularization for generalization and\nmodel compression of deep neural networks. However, the effect of soft labeling\non out-of-distribution (OOD) detection, which is an important topic of machine\nlearning safety, is not explored. In this study, we show that soft labeling can\ndetermine OOD detection performance. Specifically, how to regularize outputs of\nincorrect classes by soft labeling can deteriorate or improve OOD detection.\nBased on the empirical results, we postulate a future work for OOD-robust DNNs:\na proper output regularization by soft labeling can construct OOD-robust DNNs\nwithout additional training of OOD samples or modifying the models, while\nimproving classification accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 05:50:52 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Lee", "Doyup", ""], ["Cheon", "Yeongjae", ""]]}, {"id": "2007.03213", "submitter": "Yawen Wu", "authors": "Yawen Wu, Zhepeng Wang, Yiyu Shi, Jingtong Hu", "title": "Enabling On-Device CNN Training by Self-Supervised Instance Filtering\n  and Error Map Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work aims to enable on-device training of convolutional neural networks\n(CNNs) by reducing the computation cost at training time. CNN models are\nusually trained on high-performance computers and only the trained models are\ndeployed to edge devices. But the statically trained model cannot adapt\ndynamically in a real environment and may result in low accuracy for new\ninputs. On-device training by learning from the real-world data after\ndeployment can greatly improve accuracy. However, the high computation cost\nmakes training prohibitive for resource-constrained devices. To tackle this\nproblem, we explore the computational redundancies in training and reduce the\ncomputation cost by two complementary approaches: self-supervised early\ninstance filtering on data level and error map pruning on the algorithm level.\nThe early instance filter selects important instances from the input stream to\ntrain the network and drops trivial ones. The error map pruning further prunes\nout insignificant computations when training with the selected instances.\nExtensive experiments show that the computation cost is substantially reduced\nwithout any or with marginal accuracy loss. For example, when training\nResNet-110 on CIFAR-10, we achieve 68% computation saving while preserving full\naccuracy and 75% computation saving with a marginal accuracy loss of 1.3%.\nAggressive computation saving of 96% is achieved with less than 0.1% accuracy\nloss when quantization is integrated into the proposed approaches. Besides,\nwhen training LeNet on MNIST, we save 79% computation while boosting accuracy\nby 0.2%.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 05:52:37 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Wu", "Yawen", ""], ["Wang", "Zhepeng", ""], ["Shi", "Yiyu", ""], ["Hu", "Jingtong", ""]]}, {"id": "2007.03219", "submitter": "Hongduan Tian", "authors": "Hongduan Tian, Bo Liu, Xiao-Tong Yuan, Qingshan Liu", "title": "Meta-Learning with Network Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning is a powerful paradigm for few-shot learning. Although with\nremarkable success witnessed in many applications, the existing optimization\nbased meta-learning models with over-parameterized neural networks have been\nevidenced to ovetfit on training tasks. To remedy this deficiency, we propose a\nnetwork pruning based meta-learning approach for overfitting reduction via\nexplicitly controlling the capacity of network. A uniform concentration\nanalysis reveals the benefit of network capacity constraint for reducing\ngeneralization gap of the proposed meta-learner. We have implemented our\napproach on top of Reptile assembled with two network pruning routines:\nDense-Sparse-Dense (DSD) and Iterative Hard Thresholding (IHT). Extensive\nexperimental results on benchmark datasets with different over-parameterized\ndeep networks demonstrate that our method not only effectively alleviates\nmeta-overfitting but also in many cases improves the overall generalization\nperformance when applied to few-shot classification tasks.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 06:13:11 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 14:15:19 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Tian", "Hongduan", ""], ["Liu", "Bo", ""], ["Yuan", "Xiao-Tong", ""], ["Liu", "Qingshan", ""]]}, {"id": "2007.03224", "submitter": "Hassan Saber", "authors": "Hassan Saber (SEQUEL), Pierre M\\'enard (SEQUEL), Odalric-Ambrym\n  Maillard (SEQUEL)", "title": "Optimal Strategies for Graph-Structured Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a structured variant of the multi-armed bandit problem specified by\na set of Bernoulli distributions $ \\nu \\!= \\!(\\nu\\_{a,b})\\_{a \\in \\mathcal{A},\nb \\in \\mathcal{B}}$ with means $(\\mu\\_{a,b})\\_{a \\in \\mathcal{A}, b \\in\n\\mathcal{B}}\\!\\in\\![0,1]^{\\mathcal{A}\\times\\mathcal{B}}$ and by a given weight\nmatrix $\\omega\\!=\\! (\\omega\\_{b,b'})\\_{b,b' \\in \\mathcal{B}}$, where $\n\\mathcal{A}$ is a finite set of arms and $ \\mathcal{B} $ is a finite set of\nusers. The weight matrix $\\omega$ is such that for any two users\n$b,b'\\!\\in\\!\\mathcal{B}, \\text{max}\\_{a\\in\\mathcal{A}}|\\mu\\_{a,b} \\!-\\!\n\\mu\\_{a,b'}| \\!\\leq\\! \\omega\\_{b,b'} $. This formulation is flexible enough to\ncapture various situations, from highly-structured scenarios\n($\\omega\\!\\in\\!\\{0,1\\}^{\\mathcal{B}\\times\\mathcal{B}}$) to fully unstructured\nsetups ($\\omega\\!\\equiv\\! 1$).We consider two scenarios depending on whether\nthe learner chooses only the actions to sample rewards from or both users and\nactions. We first derive problem-dependent lower bounds on the regret for this\ngeneric graph-structure that involves a structure dependent linear programming\nproblem. Second, we adapt to this setting the Indexed Minimum Empirical\nDivergence (IMED) algorithm introduced by Honda and Takemura (2015), and\nintroduce the IMED-GS$^\\star$ algorithm. Interestingly, IMED-GS$^\\star$ does\nnot require computing the solution of the linear programming problem more than\nabout $\\log(T)$ times after $T$ steps, while being provably asymptotically\noptimal. Also, unlike existing bandit strategies designed for other popular\nstructures, IMED-GS$^\\star$ does not resort to an explicit forced exploration\nscheme and only makes use of local counts of empirical events. We finally\nprovide numerical illustration of our results that confirm the performance of\nIMED-GS$^\\star$.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 06:27:51 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 09:17:09 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Saber", "Hassan", "", "SEQUEL"], ["M\u00e9nard", "Pierre", "", "SEQUEL"], ["Maillard", "Odalric-Ambrym", "", "SEQUEL"]]}, {"id": "2007.03238", "submitter": "Alexander Fisch", "authors": "Alexander T. M. Fisch, Idris A. Eckley, P. Fearnhead", "title": "Innovative And Additive Outlier Robust Kalman Filtering With A Robust\n  Particle Filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose CE-BASS, a particle mixture Kalman filter which is\nrobust to both innovative and additive outliers, and able to fully capture\nmulti-modality in the distribution of the hidden state. Furthermore, the\nparticle sampling approach re-samples past states, which enables CE-BASS to\nhandle innovative outliers which are not immediately visible in the\nobservations, such as trend changes. The filter is computationally efficient as\nwe derive new, accurate approximations to the optimal proposal distributions\nfor the particles. The proposed algorithm is shown to compare well with\nexisting approaches and is applied to both machine temperature and server data.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 07:11:09 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Fisch", "Alexander T. M.", ""], ["Eckley", "Idris A.", ""], ["Fearnhead", "P.", ""]]}, {"id": "2007.03244", "submitter": "Weiyu Guo", "authors": "Weiyu Guo, Yidong Ouyang", "title": "Robust Learning with Frequency Domain Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution neural networks have achieved remarkable performance in many\ntasks of computing vision. However, CNN tends to bias to low frequency\ncomponents. They prioritize capturing low frequency patterns which lead them\nfail when suffering from application scenario transformation. While adversarial\nexample implies the model is very sensitive to high frequency perturbations. In\nthis paper, we introduce a new regularization method by constraining the\nfrequency spectra of the filter of the model. Different from band-limit\ntraining, our method considers the valid frequency range probably entangles in\ndifferent layers rather than continuous and trains the valid frequency range\nend-to-end by backpropagation. We demonstrate the effectiveness of our\nregularization by (1) defensing to adversarial perturbations; (2) reducing the\ngeneralization gap in different architecture; (3) improving the generalization\nability in transfer learning scenario without fine-tune.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 07:29:20 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Guo", "Weiyu", ""], ["Ouyang", "Yidong", ""]]}, {"id": "2007.03248", "submitter": "Alessandro Bregoli", "authors": "Alessandro Bregoli, Marco Scutari, Fabio Stella", "title": "A Constraint-Based Algorithm for the Structural Learning of\n  Continuous-Time Bayesian Networks", "comments": null, "journal-ref": "Proceedings of Machine Learning Research (138, PGM 2020), 41-52", "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic Bayesian networks have been well explored in the literature as\ndiscrete-time models: however, their continuous-time extensions have seen\ncomparatively little attention. In this paper, we propose the first\nconstraint-based algorithm for learning the structure of continuous-time\nBayesian networks. We discuss the different statistical tests and the\nunderlying hypotheses used by our proposal to establish conditional\nindependence. Furthermore, we analyze and discuss the computational complexity\nof the best and worst cases for the proposed algorithm. Finally, we validate\nits performance using synthetic data, and we discuss its strengths and\nlimitations comparing it with the score-based structure learning algorithm from\nNodelman et al. (2003). We find the latter to be more accurate in learning\nnetworks with binary variables, while our constraint-based approach is more\naccurate with variables assuming more than two values. Numerical experiments\nconfirm that score-based and constraint-based algorithms are comparable in\nterms of computation time.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 07:34:09 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 08:40:00 GMT"}, {"version": "v3", "created": "Wed, 2 Jun 2021 19:27:44 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Bregoli", "Alessandro", ""], ["Scutari", "Marco", ""], ["Stella", "Fabio", ""]]}, {"id": "2007.03253", "submitter": "Stefano Favaro", "authors": "Stefano Peluchetti and Stefano Favaro", "title": "Doubly infinite residual networks: a diffusion process approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When neural network's parameters are initialized as i.i.d., neural networks\nexhibit undesirable forward and backward properties as the number of layers\nincreases, e.g., vanishing dependency on the input, and perfectly correlated\noutputs for any two inputs. To overcome these drawbacks Peluchetti and Favaro\n(2020) considered fully connected residual networks (ResNets) with parameters'\ndistributions that shrink as the number of layers increases. In particular,\nthey established an interplay between infinitely deep ResNets and solutions to\nstochastic differential equations, i.e. diffusion processes, showing that\ninfinitely deep ResNets does not suffer from undesirable forward properties. In\nthis paper, we review the forward-propagation results of Peluchetti and Favaro\n(2020), extending them to the setting of convolutional ResNets. Then, we study\nanalogous backward-propagation results, which directly relate to the problem of\ntraining deep ResNets. Finally, we extend our study to the doubly infinite\nregime where both network's width and depth grow unboundedly. Within this novel\nregime the dynamics of quantities of interest converge, at initialization, to\ndeterministic limits. This allow us to provide analytical expressions for\ninference, both in the case of weakly trained and fully trained networks. These\nresults point to a limited expressive power of doubly infinite ResNets when the\nunscaled parameters are i.i.d, and residual blocks are shallow.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 07:45:34 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Peluchetti", "Stefano", ""], ["Favaro", "Stefano", ""]]}, {"id": "2007.03254", "submitter": "Tianyu Mu", "authors": "Tianyu Mu, Hongzhi Wang, Chunnan Wang, Zheng Liang", "title": "Auto-CASH: Autonomous Classification Algorithm Selection with Deep\n  Q-Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The great amount of datasets generated by various data sources have posed the\nchallenge to machine learning algorithm selection and hyperparameter\nconfiguration. For a specific machine learning task, it usually takes domain\nexperts plenty of time to select an appropriate algorithm and configure its\nhyperparameters. If the problem of algorithm selection and hyperparameter\noptimization can be solved automatically, the task will be executed more\nefficiently with performance guarantee. Such problem is also known as the CASH\nproblem. Early work either requires a large amount of human labor, or suffers\nfrom high time or space complexity. In our work, we present Auto-CASH, a\npre-trained model based on meta-learning, to solve the CASH problem more\nefficiently. Auto-CASH is the first approach that utilizes Deep Q-Network to\nautomatically select the meta-features for each dataset, thus reducing the time\ncost tremendously without introducing too much human labor. To demonstrate the\neffectiveness of our model, we conduct extensive experiments on 120 real-world\nclassification datasets. Compared with classical and the state-of-art CASH\napproaches, experimental results show that Auto-CASH achieves better\nperformance within shorter time.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 07:47:47 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Mu", "Tianyu", ""], ["Wang", "Hongzhi", ""], ["Wang", "Chunnan", ""], ["Liang", "Zheng", ""]]}, {"id": "2007.03260", "submitter": "Xiaohan Ding", "authors": "Xiaohan Ding, Tianxiang Hao, Jianchao Tan, Ji Liu, Jungong Han, Yuchen\n  Guo, Guiguang Ding", "title": "Lossless CNN Channel Pruning via Decoupling Remembering and Forgetting", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose ResRep, a novel method for lossless channel pruning (a.k.a. filter\npruning), which aims to slim down a convolutional neural network (CNN) by\nreducing the width (number of output channels) of convolutional layers.\nInspired by the neurobiology research about the independence of remembering and\nforgetting, we propose to re-parameterize a CNN into the remembering parts and\nforgetting parts, where the former learn to maintain the performance and the\nlatter learn for efficiency. By training the re-parameterized model using\nregular SGD on the former but a novel update rule with penalty gradients on the\nlatter, we realize structured sparsity, enabling us to equivalently convert the\nre-parameterized model into the original architecture with narrower layers.\nSuch a methodology distinguishes ResRep from the traditional learning-based\npruning paradigm that applies a penalty on parameters to produce structured\nsparsity, which may suppress the parameters essential for the remembering. Our\nmethod slims down a standard ResNet-50 with 76.15% accuracy on ImageNet to a\nnarrower one with only 45% FLOPs and no accuracy drop, which is the first to\nachieve lossless pruning with such a high compression ratio, to the best of our\nknowledge.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 07:56:45 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 14:37:57 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2020 14:37:31 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Ding", "Xiaohan", ""], ["Hao", "Tianxiang", ""], ["Tan", "Jianchao", ""], ["Liu", "Ji", ""], ["Han", "Jungong", ""], ["Guo", "Yuchen", ""], ["Ding", "Guiguang", ""]]}, {"id": "2007.03273", "submitter": "Saurav Prakash", "authors": "Saurav Prakash, Sagar Dhakal, Mustafa Akdeniz, A. Salman Avestimehr,\n  Nageen Himayat", "title": "Coded Computing for Federated Learning at the Edge", "comments": "Work accepted for presentation at the International Workshop on\n  Federated Learning for User Privacy and Data Confidentiality, in Conjunction\n  with ICML 2020 (FL-ICML'20). This work was part of Saurav Prakash's\n  internship projects at Intel. arXiv admin note: text overlap with\n  arXiv:2011.06223", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) is an exciting new paradigm that enables training a\nglobal model from data generated locally at the client nodes, without moving\nclient data to a centralized server. Performance of FL in a multi-access edge\ncomputing (MEC) network suffers from slow convergence due to heterogeneity and\nstochastic fluctuations in compute power and communication link qualities\nacross clients. A recent work, Coded Federated Learning (CFL), proposes to\nmitigate stragglers and speed up training for linear regression tasks by\nassigning redundant computations at the MEC server. Coding redundancy in CFL is\ncomputed by exploiting statistical properties of compute and communication\ndelays. We develop CodedFedL that addresses the difficult task of extending CFL\nto distributed non-linear regression and classification problems with\nmultioutput labels. The key innovation of our work is to exploit distributed\nkernel embedding using random Fourier features that transforms the training\ntask into distributed linear regression. We provide an analytical solution for\nload allocation, and demonstrate significant performance gains for CodedFedL\nthrough experiments over benchmark datasets using practical network parameters.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 08:20:47 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 10:36:58 GMT"}, {"version": "v3", "created": "Sun, 9 May 2021 20:09:36 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Prakash", "Saurav", ""], ["Dhakal", "Sagar", ""], ["Akdeniz", "Mustafa", ""], ["Avestimehr", "A. Salman", ""], ["Himayat", "Nageen", ""]]}, {"id": "2007.03278", "submitter": "Minh N. H. Nguyen Mr.", "authors": "Minh N. H. Nguyen, Shashi Raj Pandey, Tri Nguyen Dang, Eui-Nam Huh,\n  Choong Seon Hong, Nguyen H. Tran, Walid Saad", "title": "Self-organizing Democratized Learning: Towards Large-scale Distributed\n  Learning Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging cross-device artificial intelligence (AI) applications require a\ntransition from conventional centralized learning systems towards large-scale\ndistributed AI systems that can collaboratively perform complex learning tasks.\nIn this regard, democratized learning (Dem-AI) (Minh et al. 2020) lays out a\nholistic philosophy with underlying principles for building large-scale\ndistributed and democratized machine learning systems. The outlined principles\nare meant to provide a generalization of distributed learning that goes beyond\nexisting mechanisms such as federated learning. Inspired from this philosophy,\na novel distributed learning approach is proposed in this paper. The approach\nconsists of a self-organizing hierarchical structuring mechanism based on\nagglomerative clustering, hierarchical generalization, and corresponding\nlearning mechanism. Subsequently, a hierarchical generalized learning problem\nin a recursive form is formulated and shown to be approximately solved using\nthe solutions of distributed personalized learning problems and hierarchical\ngeneralized averaging mechanism. To that end, a distributed learning algorithm,\nnamely DemLearn and its variant, DemLearn-P is proposed. Extensive experiments\non benchmark MNIST and Fashion-MNIST datasets show that proposed algorithms\ndemonstrate better results in the generalization performance of learning model\nat agents compared to the conventional FL algorithms. Detailed analysis\nprovides useful configurations to further tune up both the generalization and\nspecialization performance of the learning models in Dem-AI systems.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 08:34:48 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Nguyen", "Minh N. H.", ""], ["Pandey", "Shashi Raj", ""], ["Dang", "Tri Nguyen", ""], ["Huh", "Eui-Nam", ""], ["Hong", "Choong Seon", ""], ["Tran", "Nguyen H.", ""], ["Saad", "Walid", ""]]}, {"id": "2007.03285", "submitter": "Ilija Bogunovic", "authors": "Ilija Bogunovic, Arpan Losalka, Andreas Krause, Jonathan Scarlett", "title": "Stochastic Linear Bandits Robust to Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a stochastic linear bandit problem in which the rewards are not\nonly subject to random noise, but also adversarial attacks subject to a\nsuitable budget $C$ (i.e., an upper bound on the sum of corruption magnitudes\nacross the time horizon). We provide two variants of a Robust Phased\nElimination algorithm, one that knows $C$ and one that does not. Both variants\nare shown to attain near-optimal regret in the non-corrupted case $C = 0$,\nwhile incurring additional additive terms respectively having a linear and\nquadratic dependency on $C$ in general. We present algorithm independent lower\nbounds showing that these additive terms are near-optimal. In addition, in a\ncontextual setting, we revisit a setup of diverse contexts, and show that a\nsimple greedy algorithm is provably robust with a near-optimal additive regret\nterm, despite performing no explicit exploration and not knowing $C$.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 09:00:57 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 20:18:09 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Bogunovic", "Ilija", ""], ["Losalka", "Arpan", ""], ["Krause", "Andreas", ""], ["Scarlett", "Jonathan", ""]]}, {"id": "2007.03293", "submitter": "Kai Brach", "authors": "Kai Brach, Beate Sick, Oliver D\\\"urr", "title": "Single Shot MC Dropout Approximation", "comments": "Presented at the ICML 2020 Workshop on Uncertainty and Robustness in\n  Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are known for their high prediction performance,\nespecially in perceptual tasks such as object recognition or autonomous\ndriving. Still, DNNs are prone to yield unreliable predictions when\nencountering completely new situations without indicating their uncertainty.\nBayesian variants of DNNs (BDNNs), such as MC dropout BDNNs, do provide\nuncertainty measures. However, BDNNs are slow during test time because they\nrely on a sampling approach. Here we present a single shot MC dropout\napproximation that preserves the advantages of BDNNs without being slower than\na DNN. Our approach is to analytically approximate for each layer in a fully\nconnected network the expected value and the variance of the MC dropout signal.\nWe evaluate our approach on different benchmark datasets and a simulated toy\nexample. We demonstrate that our single shot MC dropout approximation resembles\nthe point estimate and the uncertainty estimate of the predictive distribution\nthat is achieved with an MC approach, while being fast enough for real-time\ndeployments of BDNNs.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 09:17:17 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Brach", "Kai", ""], ["Sick", "Beate", ""], ["D\u00fcrr", "Oliver", ""]]}, {"id": "2007.03298", "submitter": "Weiyan Wang", "authors": "Weiyan Wang, Cengguang Zhang, Liu Yang, Jiacheng Xia, Kai Chen, Kun\n  Tan", "title": "Divide-and-Shuffle Synchronization for Distributed Machine Learning", "comments": "15 pages, 5 figures, NeurIPS 2020 under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed Machine Learning suffers from the bottleneck of synchronization\nto all-reduce workers' updates. Previous works mainly consider better network\ntopology, gradient compression, or stale updates to speed up communication and\nrelieve the bottleneck. However, all these works ignore the importance of\nreducing the scale of synchronized elements and inevitable serial executed\noperators. To address the problem, our work proposes the Divide-and-Shuffle\nSynchronization(DS-Sync), which divides workers into several parallel groups\nand shuffles group members. DS-Sync only synchronizes the workers in the same\ngroup so that the scale of a group is much smaller. The shuffle of workers\nmaintains the algorithm's convergence speed, which is interpreted in theory.\nComprehensive experiments also show the significant improvements in the latest\nand popular models like Bert, WideResnet, and DeepFM on challenging datasets.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 09:29:01 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Wang", "Weiyan", ""], ["Zhang", "Cengguang", ""], ["Yang", "Liu", ""], ["Xia", "Jiacheng", ""], ["Chen", "Kai", ""], ["Tan", "Kun", ""]]}, {"id": "2007.03313", "submitter": "Kevin Ong", "authors": "Kevin Shen Hoong Ong, Dusit Niyato, Chau Yuen", "title": "Predictive Maintenance for Edge-Based Sensor Networks: A Deep\n  Reinforcement Learning Approach", "comments": "6 pages, 5 figures, accepted in IEEE WF-IoT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Failure of mission-critical equipment interrupts production and results in\nmonetary loss. The risk of unplanned equipment downtime can be minimized\nthrough Predictive Maintenance of revenue generating assets to ensure optimal\nperformance and safe operation of equipment. However, the increased\nsensorization of the equipment generates a data deluge, and existing\nmachine-learning based predictive model alone becomes inadequate for timely\nequipment condition predictions. In this paper, a model-free Deep Reinforcement\nLearning algorithm is proposed for predictive equipment maintenance from an\nequipment-based sensor network context. Within each equipment, a sensor device\naggregates raw sensor data, and the equipment health status is analyzed for\nanomalous events. Unlike traditional black-box regression models, the proposed\nalgorithm self-learns an optimal maintenance policy and provides actionable\nrecommendation for each equipment. Our experimental results demonstrate the\npotential for broader range of equipment maintenance applications as an\nautomatic learning framework.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 10:00:32 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Ong", "Kevin Shen Hoong", ""], ["Niyato", "Dusit", ""], ["Yuen", "Chau", ""]]}, {"id": "2007.03315", "submitter": "Daniel Ting", "authors": "Daniel Ting and Michael I. Jordan", "title": "Manifold Learning via Manifold Deflation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear dimensionality reduction methods provide a valuable means to\nvisualize and interpret high-dimensional data. However, many popular methods\ncan fail dramatically, even on simple two-dimensional manifolds, due to\nproblems such as vulnerability to noise, repeated eigendirections, holes in\nconvex bodies, and boundary bias. We derive an embedding method for Riemannian\nmanifolds that iteratively uses single-coordinate estimates to eliminate\ndimensions from an underlying differential operator, thus \"deflating\" it. These\ndifferential operators have been shown to characterize any local, spectral\ndimensionality reduction method. The key to our method is a novel, incremental\ntangent space estimator that incorporates global structure as coordinates are\nadded. We prove its consistency when the coordinates converge to true\ncoordinates. Empirically, we show our algorithm recovers novel and interesting\nembeddings on real-world and synthetic datasets.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 10:04:28 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Ting", "Daniel", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2007.03317", "submitter": "Tianyu Pang", "authors": "Tianyu Pang, Kun Xu, Chongxuan Li, Yang Song, Stefano Ermon, Jun Zhu", "title": "Efficient Learning of Generative Models via Finite-Difference Score\n  Matching", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several machine learning applications involve the optimization of\nhigher-order derivatives (e.g., gradients of gradients) during training, which\ncan be expensive in respect to memory and computation even with automatic\ndifferentiation. As a typical example in generative modeling, score matching\n(SM) involves the optimization of the trace of a Hessian. To improve computing\nefficiency, we rewrite the SM objective and its variants in terms of\ndirectional derivatives, and present a generic strategy to efficiently\napproximate any-order directional derivative with finite difference (FD). Our\napproximation only involves function evaluations, which can be executed in\nparallel, and no gradient computations. Thus, it reduces the total\ncomputational cost while also improving numerical stability. We provide two\ninstantiations by reformulating variants of SM objectives into the FD forms.\nEmpirically, we demonstrate that our methods produce results comparable to the\ngradient-based counterparts while being much more computationally efficient.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 10:05:01 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 16:10:07 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Pang", "Tianyu", ""], ["Xu", "Kun", ""], ["Li", "Chongxuan", ""], ["Song", "Yang", ""], ["Ermon", "Stefano", ""], ["Zhu", "Jun", ""]]}, {"id": "2007.03325", "submitter": "Graham Spinks", "authors": "Graham Spinks, Marie-Francine Moens", "title": "Structured (De)composable Representations Trained with Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes a novel technique for representing templates and instances\nof concept classes. A template representation refers to the generic\nrepresentation that captures the characteristics of an entire class. The\nproposed technique uses end-to-end deep learning to learn structured and\ncomposable representations from input images and discrete labels. The obtained\nrepresentations are based on distance estimates between the distributions given\nby the class label and those given by contextual information, which are modeled\nas environments. We prove that the representations have a clear structure\nallowing to decompose the representation into factors that represent classes\nand environments. We evaluate our novel technique on classification and\nretrieval tasks involving different modalities (visual and language data).\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 10:20:31 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Spinks", "Graham", ""], ["Moens", "Marie-Francine", ""]]}, {"id": "2007.03349", "submitter": "Xingjian Li", "authors": "Xingjian Li, Haoyi Xiong, Haozhe An, Chengzhong Xu, Dejing Dou", "title": "RIFLE: Backpropagation in Depth for Deep Transfer Learning through\n  Re-Initializing the Fully-connected LayEr", "comments": "Accepted by ICML'2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-tuning the deep convolution neural network(CNN) using a pre-trained\nmodel helps transfer knowledge learned from larger datasets to the target task.\nWhile the accuracy could be largely improved even when the training dataset is\nsmall, the transfer learning outcome is usually constrained by the pre-trained\nmodel with close CNN weights (Liu et al., 2019), as the backpropagation here\nbrings smaller updates to deeper CNN layers. In this work, we propose RIFLE - a\nsimple yet effective strategy that deepens backpropagation in transfer learning\nsettings, through periodically Re-Initializing the Fully-connected LayEr with\nrandom scratch during the fine-tuning procedure. RIFLE brings meaningful\nupdates to the weights of deep CNN layers and improves low-level feature\nlearning, while the effects of randomization can be easily converged throughout\nthe overall learning procedure. The experiments show that the use of RIFLE\nsignificantly improves deep transfer learning accuracy on a wide range of\ndatasets, out-performing known tricks for the similar purpose, such as Dropout,\nDropConnect, StochasticDepth, Disturb Label and Cyclic Learning Rate, under the\nsame settings with 0.5% -2% higher testing accuracy. Empirical cases and\nablation studies further indicate RIFLE brings meaningful updates to deep CNN\nlayers with accuracy improved.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 11:27:43 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Li", "Xingjian", ""], ["Xiong", "Haoyi", ""], ["An", "Haozhe", ""], ["Xu", "Chengzhong", ""], ["Dou", "Dejing", ""]]}, {"id": "2007.03356", "submitter": "Jack Rae", "authors": "Jack W. Rae and Ali Razavi", "title": "Do Transformers Need Deep Long-Range Memory", "comments": "published at 58th Annual Meeting of the Association for Computational\n  Linguistics. 6 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep attention models have advanced the modelling of sequential data across\nmany domains. For language modelling in particular, the Transformer-XL -- a\nTransformer augmented with a long-range memory of past activations -- has been\nshown to be state-of-the-art across a variety of well-studied benchmarks. The\nTransformer-XL incorporates a long-range memory at every layer of the network,\nwhich renders its state to be thousands of times larger than RNN predecessors.\nHowever it is unclear whether this is necessary. We perform a set of\ninterventions to show that comparable performance can be obtained with 6X fewer\nlong range memories and better performance can be obtained by limiting the\nrange of attention in lower layers of the network.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 11:48:49 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Rae", "Jack W.", ""], ["Razavi", "Ali", ""]]}, {"id": "2007.03373", "submitter": "Louis B\\'ethune", "authors": "Louis B\\'ethune, Yacouba Kaloga, Pierre Borgnat, Aur\\'elien Garivier,\n  Amaury Habrard", "title": "Hierarchical and Unsupervised Graph Representation Learning with\n  Loukas's Coarsening", "comments": "19 pages, 15 figures, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel algorithm for unsupervised graph representation learning\nwith attributed graphs. It combines three advantages addressing some current\nlimitations of the literature: i) The model is inductive: it can embed new\ngraphs without re-training in the presence of new data; ii) The method takes\ninto account both micro-structures and macro-structures by looking at the\nattributed graphs at different scales; iii) The model is end-to-end\ndifferentiable: it is a building block that can be plugged into deep learning\npipelines and allows for back-propagation. We show that combining a coarsening\nmethod having strong theoretical guarantees with mutual information\nmaximization suffices to produce high quality embeddings. We evaluate them on\nclassification tasks with common benchmarks of the literature. We show that our\nalgorithm is competitive with state of the art among unsupervised graph\nrepresentation learning methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 12:04:38 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 15:15:18 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["B\u00e9thune", "Louis", ""], ["Kaloga", "Yacouba", ""], ["Borgnat", "Pierre", ""], ["Garivier", "Aur\u00e9lien", ""], ["Habrard", "Amaury", ""]]}, {"id": "2007.03378", "submitter": "Pablo L\\'opez-Garc\\'ia", "authors": "Laurin Herbsthofer, Barbara Prietl, Martina Tomberger, Thomas Pieber,\n  Pablo L\\'opez-Garc\\'ia", "title": "C2G-Net: Exploiting Morphological Properties for Image Classification", "comments": "10 pages, 5 figures (Figure 3 with 4 sub-figures), Appendix A and\n  Appendix B after the references. Originally submitted to ICML2020 but\n  rejected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose C2G-Net, a pipeline for image classification that\nexploits the morphological properties of images containing a large number of\nsimilar objects like biological cells. C2G-Net consists of two components: (1)\nCell2Grid, an image compression algorithm that identifies objects using\nsegmentation and arranges them on a grid, and (2) DeepLNiNo, a CNN architecture\nwith less than 10,000 trainable parameters aimed at facilitating model\ninterpretability. To test the performance of C2G-Net we used multiplex\nimmunohistochemistry images for predicting relapse risk in colon cancer.\nCompared to conventional CNN architectures trained on raw images, C2G-Net\nachieved similar prediction accuracy while training time was reduced by 85% and\nits model was is easier to interpret.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 12:16:17 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Herbsthofer", "Laurin", ""], ["Prietl", "Barbara", ""], ["Tomberger", "Martina", ""], ["Pieber", "Thomas", ""], ["L\u00f3pez-Garc\u00eda", "Pablo", ""]]}, {"id": "2007.03383", "submitter": "Kang Liu", "authors": "Kang Liu, Feng Xue, and Richang Hong", "title": "RGCF: Refined Graph Convolution Collaborative Filtering with concise and\n  expressive embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Convolution Network (GCN) has attracted significant attention and\nbecome the most popular method for learning graph representations. In recent\nyears, many efforts have been focused on integrating GCN into the recommender\ntasks and have made remarkable progress. At its core is to explicitly capture\nhigh-order connectivities between the nodes in user-item bipartite graph.\nHowever, we theoretically and empirically find an inherent drawback existed in\nthese GCN-based recommendation methods, where GCN is directly applied to\naggregate neighboring nodes will introduce noise and information redundancy.\nConsequently, the these models' capability of capturing high-order\nconnectivities among different nodes is limited, leading to suboptimal\nperformance of the recommender tasks. The main reason is that the the nonlinear\nnetwork layer inside GCN structure is not suitable for extracting non-sematic\nfeatures(such as one-hot ID feature) in the collaborative filtering scenarios.\nIn this work, we develop a new GCN-based Collaborative Filtering model, named\nRefined Graph convolution Collaborative Filtering(RGCF), where the construction\nof the embeddings of users (items) are delicately redesigned from several\naspects during the aggregation on the graph. Compared to the state-of-the-art\nGCN-based recommendation, RGCF is more capable for capturing the implicit\nhigh-order connectivities inside the graph and the resultant vector\nrepresentations are more expressive. We conduct extensive experiments on three\npublic million-size datasets, demonstrating that our RGCF significantly\noutperforms state-of-the-art models. We release our code at\nhttps://github.com/hfutmars/RGCF.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 12:26:10 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2020 04:32:32 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Liu", "Kang", ""], ["Xue", "Feng", ""], ["Hong", "Richang", ""]]}, {"id": "2007.03408", "submitter": "Nicolas Papadakis", "authors": "Antoine Houdard and Arthur Leclaire and Nicolas Papadakis and Julien\n  Rabin", "title": "Wasserstein Generative Models for Patch-based Texture Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a framework to train a generative model for texture\nimage synthesis from a single example. To do so, we exploit the local\nrepresentation of images via the space of patches, that is, square sub-images\nof fixed size (e.g. $4\\times 4$). Our main contribution is to consider optimal\ntransport to enforce the multiscale patch distribution of generated images,\nwhich leads to two different formulations. First, a pixel-based optimization\nmethod is proposed, relying on discrete optimal transport. We show that it is\nrelated to a well-known texture optimization framework based on iterated patch\nnearest-neighbor projections, while avoiding some of its shortcomings. Second,\nin a semi-discrete setting, we exploit the differential properties of\nWasserstein distances to learn a fully convolutional network for texture\ngeneration. Once estimated, this network produces realistic and arbitrarily\nlarge texture samples in real time. The two formulations result in non-convex\nconcave problems that can be optimized efficiently with convergence properties\nand improved stability compared to adversarial approaches, without relying on\nany regularization. By directly dealing with the patch distribution of\nsynthesized images, we also overcome limitations of state-of-the art\ntechniques, such as patch aggregation issues that usually lead to low frequency\nartifacts (e.g. blurring) in traditional patch-based approaches, or statistical\ninconsistencies (e.g. color or patterns) in learning approaches.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 13:32:55 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Houdard", "Antoine", ""], ["Leclaire", "Arthur", ""], ["Papadakis", "Nicolas", ""], ["Rabin", "Julien", ""]]}, {"id": "2007.03424", "submitter": "Sen Na", "authors": "Mingyuan Ma, Sen Na, Hongyu Wang", "title": "AEGCN: An Autoencoder-Constrained Graph Convolutional Network", "comments": "23 pages, 4 figures", "journal-ref": "Neurocomputing 2021", "doi": "10.1016/j.neucom.2020.12.061", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel neural network architecture, called\nautoencoder-constrained graph convolutional network, to solve node\nclassification task on graph domains. As suggested by its name, the core of\nthis model is a convolutional network operating directly on graphs, whose\nhidden layers are constrained by an autoencoder. Comparing with vanilla graph\nconvolutional networks, the autoencoder step is added to reduce the information\nloss brought by Laplacian smoothing. We consider applying our model on both\nhomogeneous graphs and heterogeneous graphs. For homogeneous graphs, the\nautoencoder approximates to the adjacency matrix of the input graph by taking\nhidden layer representations as encoder and another one-layer graph\nconvolutional network as decoder. For heterogeneous graphs, since there are\nmultiple adjacency matrices corresponding to different types of edges, the\nautoencoder approximates to the feature matrix of the input graph instead, and\nchanges the encoder to a particularly designed multi-channel pre-processing\nnetwork with two layers. In both cases, the error occurred in the autoencoder\napproximation goes to the penalty term in the loss function. In extensive\nexperiments on citation networks and other heterogeneous graphs, we demonstrate\nthat adding autoencoder constraints significantly improves the performance of\ngraph convolutional networks. Further, we notice that our technique can be\napplied on graph attention network to improve the performance as well. This\nreveals the wide applicability of the proposed autoencoder technique.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 16:42:55 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2020 04:29:50 GMT"}, {"version": "v3", "created": "Wed, 10 Feb 2021 09:40:28 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Ma", "Mingyuan", ""], ["Na", "Sen", ""], ["Wang", "Hongyu", ""]]}, {"id": "2007.03437", "submitter": "Arnab Kumar Mondal", "authors": "Arnab Kumar Mondal, Pratheeksha Nair, Kaleem Siddiqi", "title": "Group Equivariant Deep Reinforcement Learning", "comments": "Presented at the ICML 2020 Workshop on Inductive Biases, Invariances\n  and Generalization in RL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Reinforcement Learning (RL), Convolutional Neural Networks(CNNs) have been\nsuccessfully applied as function approximators in Deep Q-Learning algorithms,\nwhich seek to learn action-value functions and policies in various\nenvironments. However, to date, there has been little work on the learning of\nsymmetry-transformation equivariant representations of the input environment\nstate. In this paper, we propose the use of Equivariant CNNs to train RL agents\nand study their inductive bias for transformation equivariant Q-value\napproximation. We demonstrate that equivariant architectures can dramatically\nenhance the performance and sample efficiency of RL agents in a highly\nsymmetric environment while requiring fewer parameters. Additionally, we show\nthat they are robust to changes in the environment caused by affine\ntransformations.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 02:38:48 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Mondal", "Arnab Kumar", ""], ["Nair", "Pratheeksha", ""], ["Siddiqi", "Kaleem", ""]]}, {"id": "2007.03438", "submitter": "Mengjiao Yang", "authors": "Mengjiao Yang, Ofir Nachum, Bo Dai, Lihong Li, Dale Schuurmans", "title": "Off-Policy Evaluation via the Regularized Lagrangian", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed distribution correction estimation (DICE) family of\nestimators has advanced the state of the art in off-policy evaluation from\nbehavior-agnostic data. While these estimators all perform some form of\nstationary distribution correction, they arise from different derivations and\nobjective functions. In this paper, we unify these estimators as regularized\nLagrangians of the same linear program. The unification allows us to expand the\nspace of DICE estimators to new alternatives that demonstrate improved\nperformance. More importantly, by analyzing the expanded space of estimators\nboth mathematically and empirically we find that dual solutions offer greater\nflexibility in navigating the tradeoff between optimization stability and\nestimation bias, and generally provide superior estimates in practice.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 13:45:56 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 21:32:33 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Yang", "Mengjiao", ""], ["Nachum", "Ofir", ""], ["Dai", "Bo", ""], ["Li", "Lihong", ""], ["Schuurmans", "Dale", ""]]}, {"id": "2007.03441", "submitter": "Sho Sonoda Dr", "authors": "Sho Sonoda, Isao Ishikawa, Masahiro Ikeda", "title": "Ridge Regression with Over-Parametrized Two-Layer Networks Converge to\n  Ridgelet Spectrum", "comments": "published at AISTATS2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterization of local minima draws much attention in theoretical studies\nof deep learning. In this study, we investigate the distribution of parameters\nin an over-parametrized finite neural network trained by ridge regularized\nempirical square risk minimization (RERM). We develop a new theory of ridgelet\ntransform, a wavelet-like integral transform that provides a powerful and\ngeneral framework for the theoretical study of neural networks involving not\nonly the ReLU but general activation functions. We show that the distribution\nof the parameters converges to a spectrum of the ridgelet transform. This\nresult provides a new insight into the characterization of the local minima of\nneural networks, and the theoretical background of an inductive bias theory\nbased on lazy regimes. We confirm the visual resemblance between the parameter\ndistribution trained by SGD, and the ridgelet spectrum calculated by numerical\nintegration through numerical experiments with finite models.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 13:47:57 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 07:28:27 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Sonoda", "Sho", ""], ["Ishikawa", "Isao", ""], ["Ikeda", "Masahiro", ""]]}, {"id": "2007.03479", "submitter": "Peng Zhao", "authors": "Peng Zhao, Yu-Jie Zhang, Lijun Zhang, Zhi-Hua Zhou", "title": "Dynamic Regret of Convex and Smooth Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate online convex optimization in non-stationary environments and\nchoose the dynamic regret as the performance measure, defined as the difference\nbetween cumulative loss incurred by the online algorithm and that of any\nfeasible comparator sequence. Let $T$ be the time horizon and $P_T$ be the\npath-length that essentially reflects the non-stationarity of environments, the\nstate-of-the-art dynamic regret is $\\mathcal{O}(\\sqrt{T(1+P_T)})$. Although\nthis bound is proved to be minimax optimal for convex functions, in this paper,\nwe demonstrate that it is possible to further enhance the dynamic regret by\nexploiting the smoothness condition. Specifically, we propose novel online\nalgorithms that are capable of leveraging smoothness and replace the dependence\non $T$ in the dynamic regret by problem-dependent quantities: the variation in\ngradients of loss functions, the cumulative loss of the comparator sequence,\nand the minimum of the previous two terms. These quantities are at most\n$\\mathcal{O}(T)$ while could be much smaller in benign environments. Therefore,\nour results are adaptive to the intrinsic difficulty of the problem, since the\nbounds are tighter than existing results for easy problems and meanwhile\nguarantee the same rate in the worst case.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 14:10:57 GMT"}, {"version": "v2", "created": "Sat, 28 Nov 2020 08:52:02 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Zhao", "Peng", ""], ["Zhang", "Yu-Jie", ""], ["Zhang", "Lijun", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "2007.03480", "submitter": "Jong Chul Ye", "authors": "Junghyun Lee, Jawook Gu, and Jong Chul Ye", "title": "Unsupervised CT Metal Artifact Learning using Attention-guided\n  beta-CycleGAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metal artifact reduction (MAR) is one of the most important research topics\nin computed tomography (CT). With the advance of deep learning technology for\nimage reconstruction,various deep learning methods have been also suggested for\nmetal artifact removal, among which supervised learning methods are most\npopular. However, matched non-metal and metal image pairs are difficult to\nobtain in real CT acquisition. Recently, a promising unsupervised learning for\nMAR was proposed using feature disentanglement, but the resulting network\narchitecture is complication and difficult to handle large size clinical\nimages. To address this, here we propose a much simpler and much effective\nunsupervised MAR method for CT. The proposed method is based on a novel\nbeta-cycleGAN architecture derived from the optimal transport theory for\nappropriate feature space disentanglement. Another important contribution is to\nshow that attention mechanism is the key element to effectively remove the\nmetal artifacts. Specifically, by adding the convolutional block attention\nmodule (CBAM) layers with a proper disentanglement parameter, experimental\nresults confirm that we can get more improved MAR that preserves the detailed\ntexture of the original image.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 14:11:47 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Lee", "Junghyun", ""], ["Gu", "Jawook", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2007.03481", "submitter": "Vikram Krishnamurthy", "authors": "Vikram Krishnamurthy and Kunal Pattanayak", "title": "Necessary and Sufficient Conditions for Inverse Reinforcement Learning\n  of Bayesian Stopping Time Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY econ.TH eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an inverse reinforcement learning (IRL) framework for\nBayesian stopping time problems. By observing the actions of a Bayesian\ndecision maker, we provide a necessary and sufficient condition to identify if\nthese actions are consistent with optimizing a cost function; then we construct\nset valued estimates of the cost function. To achieve this IRL objective, we\nuse novel ideas from Bayesian revealed preferences stemming from\nmicroeconomics. To illustrate our IRL scheme,we consider two important examples\nof stopping time problems, namely, sequential hypothesis testing and Bayesian\nsearch. Finally, for finite datasets, we propose an IRL detection algorithm and\ngive finite sample bounds on its error probabilities. Also we discuss how to\nidentify $\\epsilon$-optimal Bayesian decision makers and perform IRL.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 14:14:12 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 00:40:40 GMT"}, {"version": "v3", "created": "Thu, 22 Jul 2021 12:03:29 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Krishnamurthy", "Vikram", ""], ["Pattanayak", "Kunal", ""]]}, {"id": "2007.03502", "submitter": "Anh Tran", "authors": "Anh Tran, Mike Eldred, Scott McCann, Yan Wang", "title": "srMO-BO-3GP: A sequential regularized multi-objective constrained\n  Bayesian optimization for design applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization (BO) is an efficient and flexible global optimization\nframework that is applicable to a very wide range of engineering applications.\nTo leverage the capability of the classical BO, many extensions, including\nmulti-objective, multi-fidelity, parallelization, latent-variable model, have\nbeen proposed to improve the limitation of the classical BO framework. In this\nwork, we propose a novel multi-objective (MO) extension, called srMO-BO-3GP, to\nsolve the MO optimization problems in a sequential setting. Three different\nGaussian processes (GPs) are stacked together, where each of the GP is assigned\nwith a different task: the first GP is used to approximate the single-objective\nfunction, the second GP is used to learn the unknown constraints, and the third\nGP is used to learn the uncertain Pareto frontier. At each iteration, a MO\naugmented Tchebycheff function converting MO to single-objective is adopted and\nextended with a regularized ridge term, where the regularization is introduced\nto smoothen the single-objective function. Finally, we couple the third GP\nalong with the classical BO framework to promote the richness and diversity of\nthe Pareto frontier by the exploitation and exploration acquisition function.\nThe proposed framework is demonstrated using several numerical benchmark\nfunctions, as well as a thermomechanical finite element model for flip-chip\npackage design optimization.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 14:40:00 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 02:14:48 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Tran", "Anh", ""], ["Eldred", "Mike", ""], ["McCann", "Scott", ""], ["Wang", "Yan", ""]]}, {"id": "2007.03506", "submitter": "Diego Doimo", "authors": "Diego Doimo, Aldo Glielmo, Alessio Ansuini, Alessandro Laio", "title": "Hierarchical nucleation in deep neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional networks (DCNs) learn meaningful representations where\ndata that share the same abstract characteristics are positioned closer and\ncloser. Understanding these representations and how they are generated is of\nunquestioned practical and theoretical interest. In this work we study the\nevolution of the probability density of the ImageNet dataset across the hidden\nlayers in some state-of-the-art DCNs. We find that the initial layers generate\na unimodal probability density getting rid of any structure irrelevant for\nclassification. In subsequent layers density peaks arise in a hierarchical\nfashion that mirrors the semantic hierarchy of the concepts. Density peaks\ncorresponding to single categories appear only close to the output and via a\nvery sharp transition which resembles the nucleation process of a heterogeneous\nliquid. This process leaves a footprint in the probability density of the\noutput layer where the topography of the peaks allows reconstructing the\nsemantic relationships of the categories.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 14:42:18 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 15:14:19 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Doimo", "Diego", ""], ["Glielmo", "Aldo", ""], ["Ansuini", "Alessio", ""], ["Laio", "Alessandro", ""]]}, {"id": "2007.03511", "submitter": "Ching-Yao Chuang", "authors": "Ching-Yao Chuang, Antonio Torralba, Stefanie Jegelka", "title": "Estimating Generalization under Distribution Shifts via Domain-Invariant\n  Representations", "comments": "arXiv admin note: text overlap with arXiv:1910.05804", "journal-ref": "International Conference on Machine Learning, 2020", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When machine learning models are deployed on a test distribution different\nfrom the training distribution, they can perform poorly, but overestimate their\nperformance. In this work, we aim to better estimate a model's performance\nunder distribution shift, without supervision. To do so, we use a set of\ndomain-invariant predictors as a proxy for the unknown, true target labels.\nSince the error of the resulting risk estimate depends on the target risk of\nthe proxy model, we study generalization of domain-invariant representations\nand show that the complexity of the latent representation has a significant\ninfluence on the target risk. Empirically, our approach (1) enables self-tuning\nof domain adaptation models, and (2) accurately estimates the target error of\ngiven models under distribution shift. Other applications include model\nselection, deciding early stopping and error detection.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 17:21:24 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Chuang", "Ching-Yao", ""], ["Torralba", "Antonio", ""], ["Jegelka", "Stefanie", ""]]}, {"id": "2007.03514", "submitter": "Aleksei Shpilman", "authors": "Mikita Sazanovich, Konstantin Chaika, Kirill Krinkin, Aleksei Shpilman", "title": "Imitation Learning Approach for AI Driving Olympics Trained on\n  Real-world and Simulation Data Simultaneously", "comments": "Accepted to the Workshop on AI for Autonomous Driving (AIAD), the\n  37th International Conference on Machine Learning (ICML2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe our winning approach to solving the Lane Following\nChallenge at the AI Driving Olympics Competition through imitation learning on\na mixed set of simulation and real-world data. AI Driving Olympics is a\ntwo-stage competition: at stage one, algorithms compete in a simulated\nenvironment with the best ones advancing to a real-world final. One of the main\nproblems that participants encounter during the competition is that algorithms\ntrained for the best performance in simulated environments do not hold up in a\nreal-world environment and vice versa. Classic control algorithms also do not\ntranslate well between tasks since most of them have to be tuned to specific\ndriving conditions such as lighting, road type, camera position, etc. To\novercome this problem, we employed the imitation learning algorithm and trained\nit on a dataset collected from sources both from simulation and real-world,\nforcing our model to perform equally well in all environments.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 14:48:11 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Sazanovich", "Mikita", ""], ["Chaika", "Konstantin", ""], ["Krinkin", "Kirill", ""], ["Shpilman", "Aleksei", ""]]}, {"id": "2007.03533", "submitter": "Jing Ji", "authors": "Kun Li, Fanglan Zheng, Jiang Tian and Xiaojia Xiang", "title": "A Federated F-score Based Ensemble Model for Automatic Rule Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript, we propose a federated F-score based ensemble tree model\nfor automatic rule extraction, namely Fed-FEARE. Under the premise of data\nprivacy protection, Fed-FEARE enables multiple agencies to jointly extract set\nof rules both vertically and horizontally. Compared with that without federated\nlearning, measures in evaluating model performance are highly improved. At\npresent, Fed-FEARE has already been applied to multiple business, including\nanti-fraud and precision marketing, in a China nation-wide financial holdings\ngroup.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 15:05:06 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 00:24:08 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 15:59:01 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Li", "Kun", ""], ["Zheng", "Fanglan", ""], ["Tian", "Jiang", ""], ["Xiang", "Xiaojia", ""]]}, {"id": "2007.03545", "submitter": "Zheng Wang", "authors": "Zheng Wang (1), Xiaojun Ye (2), Chaokun Wang (2), Jian Cui (1), Philip\n  S. Yu (3)((1) Department of Computer Science, University of Science and\n  Technology Beijing (2) School of Software, Tsinghua University,(3) Department\n  of Computer Science, University of Illinois at Chicago)", "title": "Network Embedding with Completely-imbalanced Labels", "comments": "A preliminary version of this work was accepted in AAAI 2018. This\n  version has been accepted in IEEE Transactions on Knowledge and Data\n  Engineering (TKDE) 2020. Project page:\n  https://zhengwang100.github.io/project/zero_shot_graph_embedding.html", "journal-ref": null, "doi": "10.1109/TKDE.2020.2971490", "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Network embedding, aiming to project a network into a low-dimensional space,\nis increasingly becoming a focus of network research. Semi-supervised network\nembedding takes advantage of labeled data, and has shown promising performance.\nHowever, existing semi-supervised methods would get unappealing results in the\ncompletely-imbalanced label setting where some classes have no labeled nodes at\nall. To alleviate this, we propose two novel semi-supervised network embedding\nmethods. The first one is a shallow method named RSDNE. Specifically, to\nbenefit from the completely-imbalanced labels, RSDNE guarantees both\nintra-class similarity and inter-class dissimilarity in an approximate way. The\nother method is RECT which is a new class of graph neural networks. Different\nfrom RSDNE, to benefit from the completely-imbalanced labels, RECT explores the\nclass-semantic knowledge. This enables RECT to handle networks with node\nfeatures and multi-label setting. Experimental results on several real-world\ndatasets demonstrate the superiority of the proposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 15:22:54 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Wang", "Zheng", ""], ["Ye", "Xiaojun", ""], ["Wang", "Chaokun", ""], ["Cui", "Jian", ""], ["Yu", "Philip S.", ""]]}, {"id": "2007.03562", "submitter": "Cesar A. Uribe", "authors": "C\\'esar A. Uribe and Ali Jadbabaie", "title": "A Distributed Cubic-Regularized Newton Method for Smooth Convex\n  Optimization over Networks", "comments": "22 pages, 2 figures. Preprint, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a distributed, cubic-regularized Newton method for large-scale\nconvex optimization over networks. The proposed method requires only local\ncomputations and communications and is suitable for federated learning\napplications over arbitrary network topologies. We show a $O(k^{{-}3})$\nconvergence rate when the cost function is convex with Lipschitz gradient and\nHessian, with $k$ being the number of iterations. We further provide\nnetwork-dependent bounds for the communication required in each step of the\nalgorithm. We provide numerical experiments that validate our theoretical\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 15:38:47 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Uribe", "C\u00e9sar A.", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "2007.03568", "submitter": "Alexander Acker", "authors": "Alexander Acker, Thorsten Wittkopp, Sasho Nedelkoski, Jasmin\n  Bogatinovski, Odej Kao", "title": "Superiority of Simplicity: A Lightweight Model for Network Device\n  Workload Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid growth and distribution of IT systems increases their complexity\nand aggravates operation and maintenance. To sustain control over large sets of\nhosts and the connecting networks, monitoring solutions are employed and\nconstantly enhanced. They collect diverse key performance indicators (KPIs)\n(e.g. CPU utilization, allocated memory, etc.) and provide detailed information\nabout the system state. Storing such metrics over a period of time naturally\nraises the motivation of predicting future KPI progress based on past\nobservations. Although, a variety of time series forecasting methods exist,\nforecasting the progress of IT system KPIs is very hard. First, KPI types like\nCPU utilization or allocated memory are very different and hard to be expressed\nby the same model. Second, system components are interconnected and constantly\nchanging due to soft- or firmware updates and hardware modernization. Thus a\nfrequent model retraining or fine-tuning must be expected. Therefore, we\npropose a lightweight solution for KPI series prediction based on historic\nobservations. It consists of a weighted heterogeneous ensemble method composed\nof two models - a neural network and a mean predictor. As ensemble method a\nweighted summation is used, whereby a heuristic is employed to set the weights.\nThe modelling approach is evaluated on the available FedCSIS 2020 challenge\ndataset and achieves an overall $R^2$ score of 0.10 on the preliminary 10% test\ndata and 0.15 on the complete test data. We publish our code on the following\ngithub repository: https://github.com/citlab/fed_challenge\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 15:44:16 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Acker", "Alexander", ""], ["Wittkopp", "Thorsten", ""], ["Nedelkoski", "Sasho", ""], ["Bogatinovski", "Jasmin", ""], ["Kao", "Odej", ""]]}, {"id": "2007.03572", "submitter": "Jiacheng Zhuo", "authors": "Jiacheng Zhuo, Liu Liu, Constantine Caramanis", "title": "Robust Structured Statistical Estimation via Conditional Gradient Type\n  Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured statistical estimation problems are often solved by Conditional\nGradient (CG) type methods to avoid the computationally expensive projection\noperation. However, the existing CG type methods are not robust to data\ncorruption. To address this, we propose to robustify CG type methods against\nHuber's corruption model and heavy-tailed data. First, we show that the two\nPairwise CG methods are stable, i.e., do not accumulate error. Combined with\nrobust mean gradient estimation techniques, we can therefore guarantee\nrobustness to a wide class of problems, but now in a projection-free\nalgorithmic framework. Next, we consider high dimensional problems. Robust mean\nestimation based approaches may have an unacceptably high sample complexity.\nWhen the constraint set is a $\\ell_0$ norm ball,\nIterative-Hard-Thresholding-based methods have been developed recently. Yet\nextension is non-trivial even for general sets with $O(d)$ extreme points. For\nsetting where the feasible set has $O(\\text{poly}(d))$ extreme points, we\ndevelop a novel robustness method, based on a new condition we call the Robust\nAtom Selection Condition (RASC). When RASC is satisfied, our method converges\nlinearly with a corresponding statistical error, with sample complexity that\nscales correctly in the sparsity of the problem, rather than the ambient\ndimension as would be required by any approach based on robust mean estimation.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 15:49:31 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Zhuo", "Jiacheng", ""], ["Liu", "Liu", ""], ["Caramanis", "Constantine", ""]]}, {"id": "2007.03574", "submitter": "Melrose Roderick", "authors": "Melrose Roderick, Vaishnavh Nagarajan, J. Zico Kolter", "title": "Provably Safe PAC-MDP Exploration Using Analogies", "comments": "10 pages, 3 figures, In proceedings of the 24th International\n  Conference on Artificial Intelligence and Statistics (AISTATS) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge in applying reinforcement learning to safety-critical domains\nis understanding how to balance exploration (needed to attain good performance\non the task) with safety (needed to avoid catastrophic failure). Although a\ngrowing line of work in reinforcement learning has investigated this area of\n\"safe exploration,\" most existing techniques either 1) do not guarantee safety\nduring the actual exploration process; and/or 2) limit the problem to a priori\nknown and/or deterministic transition dynamics with strong smoothness\nassumptions. Addressing this gap, we propose Analogous Safe-state Exploration\n(ASE), an algorithm for provably safe exploration in MDPs with unknown,\nstochastic dynamics. Our method exploits analogies between state-action pairs\nto safely learn a near-optimal policy in a PAC-MDP sense. Additionally, ASE\nalso guides exploration towards the most task-relevant states, which\nempirically results in significant improvements in terms of sample efficiency,\nwhen compared to existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 15:50:50 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 14:28:54 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Roderick", "Melrose", ""], ["Nagarajan", "Vaishnavh", ""], ["Kolter", "J. Zico", ""]]}, {"id": "2007.03592", "submitter": "Shaojie Tang", "authors": "Shaojie Tang and Jing Yuan", "title": "Adaptive Cascade Submodular Maximization", "comments": "Accepted at AAMAS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose and study the cascade submodular maximization\nproblem under the adaptive setting. The input of our problem is a set of items,\neach item is in a particular state (i.e., the marginal contribution of an item)\nwhich is drawn from a known probability distribution. However, we can not know\nits actual state before selecting it. As compared with existing studies on\nstochastic submodular maximization, one unique setting of our problem is that\neach item is associated with a continuation probability which represents the\nprobability that one is allowed to continue to select the next item after\nselecting the current one. Intuitively, this term captures the externality of\nselecting one item to all its subsequent items in terms of the opportunity of\nbeing selected. Therefore, the actual set of items that can be selected by a\npolicy depends on the specific ordering it adopts to select items, this makes\nour problem fundamentally different from classical submodular set optimization\nproblems. Our objective is to identify the best sequence of selecting items so\nas to maximize the expected utility of the selected items. We propose a class\nof stochastic utility functions, \\emph{adaptive cascade submodular functions},\nand show that the objective functions in many practical application domains\nsatisfy adaptive cascade submodularity. Then we develop a $0.12$ approximation\nalgorithm to the adaptive cascade submodular maximization problem.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 16:21:56 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 21:57:43 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Tang", "Shaojie", ""], ["Yuan", "Jing", ""]]}, {"id": "2007.03608", "submitter": "Zhihao Yi", "authors": "Yang Liu, Zhihao Yi, Tianjian Chen", "title": "Backdoor attacks and defenses in feature-partitioned collaborative\n  learning", "comments": "to be published in FL-ICML 2020 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since there are multiple parties in collaborative learning, malicious parties\nmight manipulate the learning process for their own purposes through backdoor\nattacks. However, most of existing works only consider the federated learning\nscenario where data are partitioned by samples. The feature-partitioned\nlearning can be another important scenario since in many real world\napplications, features are often distributed across different parties. Attacks\nand defenses in such scenario are especially challenging when the attackers\nhave no labels and the defenders are not able to access the data and model\nparameters of other participants. In this paper, we show that even parties with\nno access to labels can successfully inject backdoor attacks, achieving high\naccuracy on both main and backdoor tasks. Next, we introduce several defense\ntechniques, demonstrating that the backdoor can be successfully blocked by a\ncombination of these techniques without hurting main task accuracy. To the best\nof our knowledge, this is the first systematical study to deal with backdoor\nattacks in the feature-partitioned collaborative learning framework.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 16:45:20 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Liu", "Yang", ""], ["Yi", "Zhihao", ""], ["Chen", "Tianjian", ""]]}, {"id": "2007.03615", "submitter": "Weisong Yang", "authors": "Rafael Poyiadzi and Weisong Yang and Yoav Ben-Shlomo and Ian Craddock\n  and Liz Coulthard and Raul Santos-Rodriguez and James Selwood and Niall\n  Twomey", "title": "Detecting Signatures of Early-stage Dementia with Behavioural Models\n  Derived from Sensor Data", "comments": "Accepted by the 1st edition of HELPLINE: Artificial Intelligence for\n  Health, Personalized Medicine and Wellbeing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a pressing need to automatically understand the state and\nprogression of chronic neurological diseases such as dementia. The emergence of\nstate-of-the-art sensing platforms offers unprecedented opportunities for\nindirect and automatic evaluation of disease state through the lens of\nbehavioural monitoring. This paper specifically seeks to characterise\nbehavioural signatures of mild cognitive impairment (MCI) and Alzheimer's\ndisease (AD) in the \\textit{early} stages of the disease. We introduce bespoke\nbehavioural models and analyses of key symptoms and deploy these on a novel\ndataset of longitudinal sensor data from persons with MCI and AD. We present\npreliminary findings that show the relationship between levels of sleep quality\nand wandering can be subtly different between patients in the early stages of\ndementia and healthy cohabiting controls.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 18:46:49 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Poyiadzi", "Rafael", ""], ["Yang", "Weisong", ""], ["Ben-Shlomo", "Yoav", ""], ["Craddock", "Ian", ""], ["Coulthard", "Liz", ""], ["Santos-Rodriguez", "Raul", ""], ["Selwood", "James", ""], ["Twomey", "Niall", ""]]}, {"id": "2007.03619", "submitter": "Rakshit Trivedi", "authors": "Rakshit Trivedi, Jiachen Yang, Hongyuan Zha", "title": "GraphOpt: Learning Optimization Models of Graph Formation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formation mechanisms are fundamental to the study of complex networks, but\nlearning them from observations is challenging. In real-world domains, one\noften has access only to the final constructed graph, instead of the full\nconstruction process, and observed graphs exhibit complex structural\nproperties. In this work, we propose GraphOpt, an end-to-end framework that\njointly learns an implicit model of graph structure formation and discovers an\nunderlying optimization mechanism in the form of a latent objective function.\nThe learned objective can serve as an explanation for the observed graph\nproperties, thereby lending itself to transfer across different graphs within a\ndomain. GraphOpt poses link formation in graphs as a sequential decision-making\nprocess and solves it using maximum entropy inverse reinforcement learning\nalgorithm. Further, it employs a novel continuous latent action space that aids\nscalability. Empirically, we demonstrate that GraphOpt discovers a latent\nobjective transferable across graphs with different characteristics. GraphOpt\nalso learns a robust stochastic policy that achieves competitive link\nprediction performance without being explicitly trained on this task and\nfurther enables construction of graphs with properties similar to those of the\nobserved graph.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 16:51:39 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Trivedi", "Rakshit", ""], ["Yang", "Jiachen", ""], ["Zha", "Hongyuan", ""]]}, {"id": "2007.03626", "submitter": "Jianing Yang", "authors": "Jianing Yang, Yuying Zhu, Yongxin Wang, Ruitao Yi, Amir Zadeh,\n  Louis-Philippe Morency", "title": "What Gives the Answer Away? Question Answering Bias Analysis on Video QA\n  Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question answering biases in video QA datasets can mislead multimodal model\nto overfit to QA artifacts and jeopardize the model's ability to generalize.\nUnderstanding how strong these QA biases are and where they come from helps the\ncommunity measure progress more accurately and provide researchers insights to\ndebug their models. In this paper, we analyze QA biases in popular video\nquestion answering datasets and discover pretrained language models can answer\n37-48% questions correctly without using any multimodal context information,\nfar exceeding the 20% random guess baseline for 5-choose-1 multiple-choice\nquestions. Our ablation study shows biases can come from annotators and type of\nquestions. Specifically, annotators that have been seen during training are\nbetter predicted by the model and reasoning, abstract questions incur more\nbiases than factual, direct questions. We also show empirically that using\nannotator-non-overlapping train-test splits can reduce QA biases for video QA\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 17:00:11 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Yang", "Jianing", ""], ["Zhu", "Yuying", ""], ["Wang", "Yongxin", ""], ["Yi", "Ruitao", ""], ["Zadeh", "Amir", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "2007.03629", "submitter": "Yujia Li", "authors": "Yujia Li, Felix Gimeno, Pushmeet Kohli, Oriol Vinyals", "title": "Strong Generalization and Efficiency in Neural Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning efficient algorithms that strongly\ngeneralize in the framework of neural program induction. By carefully designing\nthe input / output interfaces of the neural model and through imitation, we are\nable to learn models that produce correct results for arbitrary input sizes,\nachieving strong generalization. Moreover, by using reinforcement learning, we\noptimize for program efficiency metrics, and discover new algorithms that\nsurpass the teacher used in imitation. With this, our approach can learn to\noutperform custom-written solutions for a variety of problems, as we tested it\non sorting, searching in ordered lists and the NP-complete 0/1 knapsack\nproblem, which sets a notable milestone in the field of Neural Program\nInduction. As highlights, our learned model can perform sorting perfectly on\nany input data size we tested on, with $O(n log n)$ complexity, whilst\noutperforming hand-coded algorithms, including quick sort, in number of\noperations even for list sizes far beyond those seen during training.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 17:03:02 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 09:19:58 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Li", "Yujia", ""], ["Gimeno", "Felix", ""], ["Kohli", "Pushmeet", ""], ["Vinyals", "Oriol", ""]]}, {"id": "2007.03634", "submitter": "Aditya Pal", "authors": "Aditya Pal, Chantat Eksombatchai, Yitong Zhou, Bo Zhao, Charles\n  Rosenberg, Jure Leskovec", "title": "PinnerSage: Multi-Modal User Embedding Framework for Recommendations at\n  Pinterest", "comments": "10 pages, 7 figures", "journal-ref": "KDD 2020", "doi": "10.1145/3394486.3403280", "report-no": null, "categories": "cs.LG cs.IR cs.SI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Latent user representations are widely adopted in the tech industry for\npowering personalized recommender systems. Most prior work infers a single high\ndimensional embedding to represent a user, which is a good starting point but\nfalls short in delivering a full understanding of the user's interests. In this\nwork, we introduce PinnerSage, an end-to-end recommender system that represents\neach user via multi-modal embeddings and leverages this rich representation of\nusers to provides high quality personalized recommendations. PinnerSage\nachieves this by clustering users' actions into conceptually coherent clusters\nwith the help of a hierarchical clustering method (Ward) and summarizes the\nclusters via representative pins (Medoids) for efficiency and interpretability.\nPinnerSage is deployed in production at Pinterest and we outline the several\ndesign decisions that makes it run seamlessly at a very large scale. We conduct\nseveral offline and online A/B experiments to show that our method\nsignificantly outperforms single embedding methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 17:13:20 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Pal", "Aditya", ""], ["Eksombatchai", "Chantat", ""], ["Zhou", "Yitong", ""], ["Zhao", "Bo", ""], ["Rosenberg", "Charles", ""], ["Leskovec", "Jure", ""]]}, {"id": "2007.03640", "submitter": "Rogan Morrow", "authors": "Rogan Morrow, Wei-Chen Chiu", "title": "Benefiting Deep Latent Variable Models via Learning the Prior and\n  Removing Latent Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exist many forms of deep latent variable models, such as the\nvariational autoencoder and adversarial autoencoder. Regardless of the specific\nclass of model, there exists an implicit consensus that the latent distribution\nshould be regularized towards the prior, even in the case where the prior\ndistribution is learned. Upon investigating the effect of latent regularization\non image generation our results indicate that in the case where a sufficiently\nexpressive prior is learned, latent regularization is not necessary and may in\nfact be harmful insofar as image quality is concerned. We additionally\ninvestigate the benefit of learned priors on two common problems in computer\nvision: latent variable disentanglement, and diversity in image-to-image\ntranslation.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 17:25:37 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 13:05:49 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Morrow", "Rogan", ""], ["Chiu", "Wei-Chen", ""]]}, {"id": "2007.03641", "submitter": "Jie Shen", "authors": "Jie Shen", "title": "One-Bit Compressed Sensing via One-Shot Hard Thresholding", "comments": "Accepted to The Conference on Uncertainty in Artificial Intelligence\n  (UAI) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper concerns the problem of 1-bit compressed sensing, where the goal\nis to estimate a sparse signal from a few of its binary measurements. We study\na non-convex sparsity-constrained program and present a novel and concise\nanalysis that moves away from the widely used notion of Gaussian width. We show\nthat with high probability a simple algorithm is guaranteed to produce an\naccurate approximation to the normalized signal of interest under the\n$\\ell_2$-metric. On top of that, we establish an ensemble of new results that\naddress norm estimation, support recovery, and model misspecification. On the\ncomputational side, it is shown that the non-convex program can be solved via\none-step hard thresholding which is dramatically efficient in terms of time\ncomplexity and memory footprint. On the statistical side, it is shown that our\nestimator enjoys a near-optimal error rate under standard conditions. The\ntheoretical results are substantiated by numerical experiments.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 17:28:03 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 11:58:57 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Shen", "Jie", ""]]}, {"id": "2007.03668", "submitter": "Badih Ghazi", "authors": "Badih Ghazi, Noah Golowich, Ravi Kumar, Pasin Manurangsi", "title": "Near-tight closure bounds for Littlestone and threshold dimensions", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study closure properties for the Littlestone and threshold dimensions of\nbinary hypothesis classes. Given classes $\\mathcal{H}_1, \\ldots, \\mathcal{H}_k$\nof Boolean functions with bounded Littlestone (respectively, threshold)\ndimension, we establish an upper bound on the Littlestone (respectively,\nthreshold) dimension of the class defined by applying an arbitrary binary\naggregation rule to $\\mathcal{H}_1, \\ldots, \\mathcal{H}_k$. We also show that\nour upper bounds are nearly tight. Our upper bounds give an exponential (in\n$k$) improvement upon analogous bounds shown by Alon et al. (COLT 2020), thus\nanswering a question posed by their work.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 17:56:06 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Ghazi", "Badih", ""], ["Golowich", "Noah", ""], ["Kumar", "Ravi", ""], ["Manurangsi", "Pasin", ""]]}, {"id": "2007.03669", "submitter": "Victoria Dean", "authors": "Victoria Dean, Shubham Tulsiani, Abhinav Gupta", "title": "See, Hear, Explore: Curiosity via Audio-Visual Association", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploration is one of the core challenges in reinforcement learning. A common\nformulation of curiosity-driven exploration uses the difference between the\nreal future and the future predicted by a learned model. However, predicting\nthe future is an inherently difficult task which can be ill-posed in the face\nof stochasticity. In this paper, we introduce an alternative form of curiosity\nthat rewards novel associations between different senses. Our approach exploits\nmultiple modalities to provide a stronger signal for more efficient\nexploration. Our method is inspired by the fact that, for humans, both sight\nand sound play a critical role in exploration. We present results on several\nAtari environments and Habitat (a photorealistic navigation simulator), showing\nthe benefits of using an audio-visual association model for intrinsically\nguiding learning agents in the absence of external rewards. For videos and\ncode, see https://vdean.github.io/audio-curiosity.html.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 17:56:35 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 16:26:54 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Dean", "Victoria", ""], ["Tulsiani", "Shubham", ""], ["Gupta", "Abhinav", ""]]}, {"id": "2007.03681", "submitter": "Prateek Bansal", "authors": "Prateek Bansal, Rico Krueger, Daniel J. Graham", "title": "Fast Bayesian Estimation of Spatial Count Data Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial count data models are used to explain and predict the frequency of\nphenomena such as traffic accidents in geographically distinct entities such as\ncensus tracts or road segments. These models are typically estimated using\nBayesian Markov chain Monte Carlo (MCMC) simulation methods, which, however,\nare computationally expensive and do not scale well to large datasets.\nVariational Bayes (VB), a method from machine learning, addresses the\nshortcomings of MCMC by casting Bayesian estimation as an optimisation problem\ninstead of a simulation problem. Considering all these advantages of VB, a VB\nmethod is derived for posterior inference in negative binomial models with\nunobserved parameter heterogeneity and spatial dependence. P\\'olya-Gamma\naugmentation is used to deal with the non-conjugacy of the negative binomial\nlikelihood and an integrated non-factorised specification of the variational\ndistribution is adopted to capture posterior dependencies. The benefits of the\nproposed approach are demonstrated in a Monte Carlo study and an empirical\napplication on estimating youth pedestrian injury counts in census tracts of\nNew York City. The VB approach is around 45 to 50 times faster than MCMC on a\nregular eight-core processor in a simulation and an empirical study, while\noffering similar estimation and predictive accuracy. Conditional on the\navailability of computational resources, the embarrassingly parallel\narchitecture of the proposed VB method can be exploited to further accelerate\nits estimation by up to 20 times.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 10:24:45 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 13:19:03 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Bansal", "Prateek", ""], ["Krueger", "Rico", ""], ["Graham", "Daniel J.", ""]]}, {"id": "2007.03714", "submitter": "Yuqing Li", "authors": "Yuqing Li, Tao Luo, Nung Kwan Yip", "title": "Towards an Understanding of Residual Networks Using Neural Tangent\n  Hierarchy (NTH)", "comments": "72 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient descent yields zero training loss in polynomial time for deep neural\nnetworks despite non-convex nature of the objective function. The behavior of\nnetwork in the infinite width limit trained by gradient descent can be\ndescribed by the Neural Tangent Kernel (NTK) introduced in\n\\cite{Jacot2018Neural}. In this paper, we study dynamics of the NTK for finite\nwidth Deep Residual Network (ResNet) using the neural tangent hierarchy (NTH)\nproposed in \\cite{Huang2019Dynamics}. For a ResNet with smooth and Lipschitz\nactivation function, we reduce the requirement on the layer width $m$ with\nrespect to the number of training samples $n$ from quartic to cubic. Our\nanalysis suggests strongly that the particular skip-connection structure of\nResNet is the main reason for its triumph over fully-connected network.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 18:08:16 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Li", "Yuqing", ""], ["Luo", "Tao", ""], ["Yip", "Nung Kwan", ""]]}, {"id": "2007.03722", "submitter": "David Ginsbourger", "authors": "Trygve Olav Fossum, C\\'edric Travelletti, Jo Eidsvik, David\n  Ginsbourger, Kanna Rajan", "title": "Learning excursion sets of vector-valued Gaussian random fields for\n  autonomous ocean sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving and optimizing oceanographic sampling is a crucial task for marine\nscience and maritime resource management. Faced with limited resources in\nunderstanding processes in the water-column, the combination of statistics and\nautonomous systems provide new opportunities for experimental design. In this\nwork we develop efficient spatial sampling methods for characterizing regions\ndefined by simultaneous exceedances above prescribed thresholds of several\nresponses, with an application focus on mapping coastal ocean phenomena based\non temperature and salinity measurements. Specifically, we define a design\ncriterion based on uncertainty in the excursions of vector-valued Gaussian\nrandom fields, and derive tractable expressions for the expected integrated\nBernoulli variance reduction in such a framework. We demonstrate how this\ncriterion can be used to prioritize sampling efforts at locations that are\nambiguous, making exploration more effective. We use simulations to study and\ncompare properties of the considered approaches, followed by results from field\ndeployments with an autonomous underwater vehicle as part of a study mapping\nthe boundary of a river plume. The results demonstrate the potential of\ncombining statistical methods and robotic platforms to effectively inform and\nexecute data-driven environmental sampling.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 18:23:46 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 13:32:27 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Fossum", "Trygve Olav", ""], ["Travelletti", "C\u00e9dric", ""], ["Eidsvik", "Jo", ""], ["Ginsbourger", "David", ""], ["Rajan", "Kanna", ""]]}, {"id": "2007.03742", "submitter": "Mariah Schrum", "authors": "Mariah L. Schrum, Mark Connolly, Eric Cole, Mihir Ghetiya, Robert\n  Gross, Matthew C. Gombolay", "title": "Meta-active Learning in Probabilistically-Safe Optimization", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to control a safety-critical system with latent dynamics (e.g. for\ndeep brain stimulation) requires taking calculated risks to gain information as\nefficiently as possible. To address this problem, we present a\nprobabilistically-safe, meta-active learning approach to efficiently learn\nsystem dynamics and optimal configurations. We cast this problem as\nmeta-learning an acquisition function, which is represented by a Long-Short\nTerm Memory Network (LSTM) encoding sampling history. This acquisition function\nis meta-learned offline to learn high quality sampling strategies. We employ a\nmixed-integer linear program as our policy with the final, linearized layers of\nour LSTM acquisition function directly encoded into the objective to trade off\nexpected information gain (e.g., improvement in the accuracy of the model of\nsystem dynamics) with the likelihood of safe control. We set a new\nstate-of-the-art in active learning for control of a high-dimensional system\nwith altered dynamics (i.e., a damaged aircraft), achieving a 46% increase in\ninformation gain and a 20% speedup in computation time over baselines.\nFurthermore, we demonstrate our system's ability to learn the optimal parameter\nsettings for deep brain stimulation in a rat's brain while avoiding unwanted\nside effects (i.e., triggering seizures), outperforming prior state-of-the-art\napproaches with a 58% increase in information gain. Additionally, our algorithm\nachieves a 97% likelihood of terminating in a safe state while losing only 15%\nof information gain.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 19:14:06 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Schrum", "Mariah L.", ""], ["Connolly", "Mark", ""], ["Cole", "Eric", ""], ["Ghetiya", "Mihir", ""], ["Gross", "Robert", ""], ["Gombolay", "Matthew C.", ""]]}, {"id": "2007.03744", "submitter": "Maryam Rahbaralam Ph.D.", "authors": "Maryam Rahbaralam, David Modesto, Jaume Card\\'us, Amir Abdollahi, and\n  Fernando M Cucchietti", "title": "Predictive Analytics for Water Asset Management: Machine Learning and\n  Survival Analysis", "comments": "19 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding performance and prioritizing resources for the maintenance of\nthe drinking-water pipe network throughout its life-cycle is a key part of\nwater asset management. Renovation of this vital network is generally hindered\nby the difficulty or impossibility to gain physical access to the pipes. We\nstudy a statistical and machine learning framework for the prediction of water\npipe failures. We employ classical and modern classifiers for a short-term\nprediction and survival analysis to provide a broader perspective and long-term\nforecast, usually needed for the economic analysis of the renovation. To enrich\nthese models, we introduce new predictors based on water distribution domain\nknowledge and employ a modern oversampling technique to remedy the high\nimbalance coming from the few failures observed each year. For our case study,\nwe use a dataset containing the failure records of all pipes within the water\ndistribution network in Barcelona, Spain. The results shed light on the effect\nof important risk factors, such as pipe geometry, age, material, and soil\ncover, among others, and can help utility managers conduct more informed\npredictive maintenance tasks.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 19:08:36 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Rahbaralam", "Maryam", ""], ["Modesto", "David", ""], ["Card\u00fas", "Jaume", ""], ["Abdollahi", "Amir", ""], ["Cucchietti", "Fernando M", ""]]}, {"id": "2007.03746", "submitter": "Dongrui Wu", "authors": "Dongrui Wu and Xue Jiang and Ruimin Peng and Wanzeng Kong and Jian\n  Huang and Zhigang Zeng", "title": "Transfer Learning for Motor Imagery Based Brain-Computer Interfaces: A\n  Complete Pipeline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning (TL) has been widely used in motor imagery (MI) based\nbrain-computer interfaces (BCIs) to reduce the calibration effort for a new\nsubject, and demonstrated promising performance. While a closed-loop MI-based\nBCI system, after electroencephalogram (EEG) signal acquisition and temporal\nfiltering, includes spatial filtering, feature engineering, and classification\nblocks before sending out the control signal to an external device, previous\napproaches only considered TL in one or two such components. This paper\nproposes that TL could be considered in all three components (spatial\nfiltering, feature engineering, and classification) of MI-based BCIs.\nFurthermore, it is also very important to specifically add a data alignment\ncomponent before spatial filtering to make the data from different subjects\nmore consistent, and hence to facilitate subsequential TL. Offline calibration\nexperiments on two MI datasets verified our proposal. Especially, integrating\ndata alignment and sophisticated TL approaches can significantly improve the\nclassification performance, and hence greatly reduces the calibration effort.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 23:44:21 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 20:56:41 GMT"}, {"version": "v3", "created": "Fri, 22 Jan 2021 20:37:14 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Wu", "Dongrui", ""], ["Jiang", "Xue", ""], ["Peng", "Ruimin", ""], ["Kong", "Wanzeng", ""], ["Huang", "Jian", ""], ["Zeng", "Zhigang", ""]]}, {"id": "2007.03747", "submitter": "Christoph Muehlmann", "authors": "Christoph Muehlmann, Klaus Nordhausen, Mengxi Yi", "title": "On Cokriging, Neural Networks, and Spatial Blind Source Separation for\n  Multivariate Spatial Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate measurements taken at irregularly sampled locations are a common\nform of data, for example in geochemical analysis of soil. In practical\nconsiderations predictions of these measurements at unobserved locations are of\ngreat interest. For standard multivariate spatial prediction methods it is\nmandatory to not only model spatial dependencies but also cross-dependencies\nwhich makes it a demanding task. Recently, a blind source separation approach\nfor spatial data was suggested. When using this spatial blind source separation\nmethod prior the actual spatial prediction, modelling of spatial\ncross-dependencies is avoided, which in turn simplifies the spatial prediction\ntask significantly. In this paper we investigate the use of spatial blind\nsource separation as a pre-processing tool for spatial prediction and compare\nit with predictions from Cokriging and neural networks in an extensive\nsimulation study as well as a geochemical dataset.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 10:59:45 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Muehlmann", "Christoph", ""], ["Nordhausen", "Klaus", ""], ["Yi", "Mengxi", ""]]}, {"id": "2007.03749", "submitter": "Ahmed Touati", "authors": "Ahmed Touati and Pascal Vincent", "title": "Sharp Analysis of Smoothed Bellman Error Embedding", "comments": "Accepted at the ICML 2020 Workshop on Theoretical Foundations of\n  Reinforcement Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \\textit{Smoothed Bellman Error Embedding} algorithm~\\citep{dai2018sbeed},\nknown as SBEED, was proposed as a provably convergent reinforcement learning\nalgorithm with general nonlinear function approximation. It has been\nsuccessfully implemented with neural networks and achieved strong empirical\nresults. In this work, we study the theoretical behavior of SBEED in batch-mode\nreinforcement learning. We prove a near-optimal performance guarantee that\ndepends on the representation power of the used function classes and a tight\nnotion of the distribution shift. Our results improve upon prior guarantees for\nSBEED in ~\\citet{dai2018sbeed} in terms of the dependence on the planning\nhorizon and on the sample size. Our analysis builds on the recent work of\n~\\citet{Xie2020} which studies a related algorithm MSBO, that could be\ninterpreted as a \\textit{non-smooth} counterpart of SBEED.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 19:27:09 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Touati", "Ahmed", ""], ["Vincent", "Pascal", ""]]}, {"id": "2007.03758", "submitter": "Quercus Hern\\'andez Lain", "authors": "Quercus Hernandez, Alberto Badias, David Gonzalez, Francisco Chinesta,\n  Elias Cueto", "title": "Deep learning of thermodynamics-aware reduced-order models from data", "comments": "17 pages, 7 figures", "journal-ref": null, "doi": "10.1016/j.cma.2021.113763", "report-no": null, "categories": "cs.CE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm to learn the relevant latent variables of a\nlarge-scale discretized physical system and predict its time evolution using\nthermodynamically-consistent deep neural networks. Our method relies on sparse\nautoencoders, which reduce the dimensionality of the full order model to a set\nof sparse latent variables with no prior knowledge of the coded space\ndimensionality. Then, a second neural network is trained to learn the\nmetriplectic structure of those reduced physical variables and predict its time\nevolution with a so-called structure-preserving neural network. This data-based\nintegrator is guaranteed to conserve the total energy of the system and the\nentropy inequality, and can be applied to both conservative and dissipative\nsystems. The integrated paths can then be decoded to the original\nfull-dimensional manifold and be compared to the ground truth solution. This\nmethod is tested with two examples applied to fluid and solid mechanics.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 08:49:01 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 17:51:00 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Hernandez", "Quercus", ""], ["Badias", "Alberto", ""], ["Gonzalez", "David", ""], ["Chinesta", "Francisco", ""], ["Cueto", "Elias", ""]]}, {"id": "2007.03760", "submitter": "Ming Yin", "authors": "Ming Yin, Yu Bai and Yu-Xiang Wang", "title": "Near-Optimal Provable Uniform Convergence in Offline Policy Evaluation\n  for Reinforcement Learning", "comments": "Short version presented at Offline RL workshop at Neurips, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of Offline Policy Evaluation (OPE) in Reinforcement Learning (RL)\nis a critical step towards applying RL in real-life applications. Existing work\non OPE mostly focus on evaluating a fixed target policy $\\pi$, which does not\nprovide useful bounds for offline policy learning as $\\pi$ will then be\ndata-dependent. We address this problem by simultaneously evaluating all\npolicies in a policy class $\\Pi$ -- uniform convergence in OPE -- and obtain\nnearly optimal error bounds for a number of global / local policy classes. Our\nresults imply that the model-based planning achieves an optimal episode\ncomplexity of $\\widetilde{O}(H^3/d_m\\epsilon^2)$ in identifying an\n$\\epsilon$-optimal policy under the time-inhomogeneous episodic MDP model ($H$\nis the planning horizon, $d_m$ is a quantity that reflects the exploration of\nthe logging policy $\\mu$). To the best of our knowledge, this is the first time\nthe optimal rate is shown to be possible for the offline RL setting and the\npaper is the first that systematically investigates the uniform convergence in\nOPE.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 19:44:14 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 09:14:25 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Yin", "Ming", ""], ["Bai", "Yu", ""], ["Wang", "Yu-Xiang", ""]]}, {"id": "2007.03762", "submitter": "Ilkay Oksuz", "authors": "Salih Gunduz, Umut Ugurlu, and Ilkay Oksuz", "title": "Transfer Learning for Electricity Price Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electricity price forecasting is an essential task for all the deregulated\nmarkets of the world. The accurate prediction of the day-ahead electricity\nprices is an active research field and available data from various markets can\nbe used as an input for forecasting. A collection of models have been proposed\nfor this task, but the fundamental question on how to use the available big\ndata is often neglected. In this paper, we propose to use transfer learning as\na tool for utilizing information from other electricity price markets for\nforecasting. We pre-train a bidirectional Gated Recurrent Units (BGRU) network\non source markets and finally do a fine-tuning for the target market. Moreover,\nwe test different ways to use the input data from various markets in the\nmodels. Our experiments on five different day-ahead markets indicate that\ntransfer learning improves the performance of electricity price forecasting in\na statistically significant manner.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 17:24:36 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 09:14:06 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Gunduz", "Salih", ""], ["Ugurlu", "Umut", ""], ["Oksuz", "Ilkay", ""]]}, {"id": "2007.03767", "submitter": "Mustafa Ozdayi", "authors": "Mustafa Safa Ozdayi, Murat Kantarcioglu, Yulia R. Gel", "title": "Defending against Backdoors in Federated Learning with Robust Learning\n  Rate", "comments": "Published at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) allows a set of agents to collaboratively train a\nmodel without sharing their potentially sensitive data. This makes FL suitable\nfor privacy-preserving applications. At the same time, FL is susceptible to\nadversarial attacks due to decentralized and unvetted data. One important line\nof attacks against FL is the backdoor attacks. In a backdoor attack, an\nadversary tries to embed a backdoor functionality to the model during training\nthat can later be activated to cause a desired misclassification. To prevent\nbackdoor attacks, we propose a lightweight defense that requires minimal change\nto the FL protocol. At a high level, our defense is based on carefully\nadjusting the aggregation server's learning rate, per dimension and per round,\nbased on the sign information of agents' updates. We first conjecture the\nnecessary steps to carry a successful backdoor attack in FL setting, and then,\nexplicitly formulate the defense based on our conjecture. Through experiments,\nwe provide empirical evidence that supports our conjecture, and we test our\ndefense against backdoor attacks under different settings. We observe that\neither backdoor is completely eliminated, or its accuracy is significantly\nreduced. Overall, our experiments suggest that our defense significantly\noutperforms some of the recently proposed defenses in the literature. We\nachieve this by having minimal influence over the accuracy of the trained\nmodels. In addition, we also provide convergence rate analysis for our proposed\nscheme.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 23:38:35 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2020 03:07:54 GMT"}, {"version": "v3", "created": "Sun, 13 Jun 2021 05:48:42 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Ozdayi", "Mustafa Safa", ""], ["Kantarcioglu", "Murat", ""], ["Gel", "Yulia R.", ""]]}, {"id": "2007.03774", "submitter": "Xin Wang", "authors": "Xin Wang", "title": "The curious case of developmental BERTology: On sparsity, transfer\n  learning, generalization and the brain", "comments": "9 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this essay, we explore a point of intersection between deep learning and\nneuroscience, through the lens of large language models, transfer learning and\nnetwork compression. Just like perceptual and cognitive neurophysiology has\ninspired effective deep neural network architectures which in turn make a\nuseful model for understanding the brain, here we explore how biological neural\ndevelopment might inspire efficient and robust optimization procedures which in\nturn serve as a useful model for the maturation and aging of the brain.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 20:16:30 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Wang", "Xin", ""]]}, {"id": "2007.03775", "submitter": "Sungho Park", "authors": "Sungho Park, Dohyung Kim, Sunhee Hwang, Hyeran Byun", "title": "README: REpresentation learning by fairness-Aware Disentangling MEthod", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fair representation learning aims to encode invariant representation with\nrespect to the protected attribute, such as gender or age. In this paper, we\ndesign Fairness-aware Disentangling Variational AutoEncoder (FD-VAE) for fair\nrepresentation learning. This network disentangles latent space into three\nsubspaces with a decorrelation loss that encourages each subspace to contain\nindependent information: 1) target attribute information, 2) protected\nattribute information, 3) mutual attribute information. After the\nrepresentation learning, this disentangled representation is leveraged for\nfairer downstream classification by excluding the subspace with the protected\nattribute information. We demonstrate the effectiveness of our model through\nextensive experiments on CelebA and UTK Face datasets. Our method outperforms\nthe previous state-of-the-art method by large margins in terms of equal\nopportunity and equalized odds.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 20:16:49 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Park", "Sungho", ""], ["Kim", "Dohyung", ""], ["Hwang", "Sunhee", ""], ["Byun", "Hyeran", ""]]}, {"id": "2007.03795", "submitter": "Maria-Luiza Vladarean", "authors": "Maria-Luiza Vladarean, Ahmet Alacaoglu, Ya-Ping Hsieh, Volkan Cevher", "title": "Conditional gradient methods for stochastically constrained convex\n  minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two novel conditional gradient-based methods for solving\nstructured stochastic convex optimization problems with a large number of\nlinear constraints. Instances of this template naturally arise from\nSDP-relaxations of combinatorial problems, which involve a number of\nconstraints that is polynomial in the problem dimension. The most important\nfeature of our framework is that only a subset of the constraints is processed\nat each iteration, thus gaining a computational advantage over prior works that\nrequire full passes. Our algorithms rely on variance reduction and smoothing\nused in conjunction with conditional gradient steps, and are accompanied by\nrigorous convergence guarantees. Preliminary numerical experiments are provided\nfor illustrating the practical performance of the methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 21:26:35 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Vladarean", "Maria-Luiza", ""], ["Alacaoglu", "Ahmet", ""], ["Hsieh", "Ya-Ping", ""], ["Cevher", "Volkan", ""]]}, {"id": "2007.03797", "submitter": "Lingyang Chu", "authors": "Yutao Huang, Lingyang Chu, Zirui Zhou, Lanjun Wang, Jiangchuan Liu,\n  Jian Pei, Yong Zhang", "title": "Personalized Cross-Silo Federated Learning on Non-IID Data", "comments": "Accepted by AAAI 2021. The API of this work is available at Huawei\n  Cloud (https://t.ly/nGN9), free registration is required before use", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-IID data present a tough challenge for federated learning. In this paper,\nwe explore a novel idea of facilitating pairwise collaborations between clients\nwith similar data. We propose FedAMP, a new method employing federated\nattentive message passing to facilitate similar clients to collaborate more. We\nestablish the convergence of FedAMP for both convex and non-convex models, and\npropose a heuristic method to further improve the performance of FedAMP when\nclients adopt deep neural networks as personalized models. Our extensive\nexperiments on benchmark data sets demonstrate the superior performance of the\nproposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 21:38:36 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 22:32:43 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2021 17:25:31 GMT"}, {"version": "v4", "created": "Fri, 15 Jan 2021 20:45:13 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Huang", "Yutao", ""], ["Chu", "Lingyang", ""], ["Zhou", "Zirui", ""], ["Wang", "Lanjun", ""], ["Liu", "Jiangchuan", ""], ["Pei", "Jian", ""], ["Zhang", "Yong", ""]]}, {"id": "2007.03800", "submitter": "Paul Irofti", "authors": "Cristian Rusu and Paul Irofti", "title": "Efficient and Parallel Separable Dictionary Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA eess.IV math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Separable, or Kronecker product, dictionaries provide natural decompositions\nfor 2D signals, such as images. In this paper, we describe a highly\nparallelizable algorithm to learn such dictionaries which reaches sparse\nrepresentations competitive with the previous state of the art dictionary\nlearning algorithms from the literature but at a lower computational cost. We\nhighlight the performance of the proposed method to sparsely represent image\nand hyperspectral data, and for image denoising.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 21:46:32 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 20:15:43 GMT"}, {"version": "v3", "created": "Thu, 17 Dec 2020 14:44:06 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Rusu", "Cristian", ""], ["Irofti", "Paul", ""]]}, {"id": "2007.03807", "submitter": "Vincent Liu", "authors": "Vincent Liu, Adam White, Hengshuai Yao, Martha White", "title": "Towards a practical measure of interference for reinforcement learning", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Catastrophic interference is common in many network-based learning systems,\nand many proposals exist for mitigating it. But, before we overcome\ninterference we must understand it better. In this work, we provide a\ndefinition of interference for control in reinforcement learning. We\nsystematically evaluate our new measures, by assessing correlation with several\nmeasures of learning performance, including stability, sample efficiency, and\nonline and offline control performance across a variety of learning\narchitectures. Our new interference measure allows us to ask novel scientific\nquestions about commonly used deep learning architectures. In particular we\nshow that target network frequency is a dominating factor for interference, and\nthat updates on the last layer result in significantly higher interference than\nupdates internal to the network. This new measure can be expensive to compute;\nwe conclude with motivation for an efficient proxy measure and empirically\ndemonstrate it is correlated with our definition of interference.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 22:02:00 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Liu", "Vincent", ""], ["White", "Adam", ""], ["Yao", "Hengshuai", ""], ["White", "Martha", ""]]}, {"id": "2007.03812", "submitter": "Daniel Vial", "authors": "Daniel Vial, Sanjay Shakkottai, R. Srikant", "title": "Robust Multi-Agent Multi-Armed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have shown that agents facing independent instances of a\nstochastic $K$-armed bandit can collaborate to decrease regret. However, these\nworks assume that each agent always recommends their individual best-arm\nestimates to other agents, which is unrealistic in envisioned applications\n(machine faults in distributed computing or spam in social recommendation\nsystems). Hence, we generalize the setting to include $n$ honest and $m$\nmalicious agents who recommend best-arm estimates and arbitrary arms,\nrespectively. We first show that even with a single malicious agent, existing\ncollaboration-based algorithms fail to improve regret guarantees over a\nsingle-agent baseline. We propose a scheme where honest agents learn who is\nmalicious and dynamically reduce communication with (i.e., \"block\") them. We\nshow that collaboration indeed decreases regret for this algorithm, assuming\n$m$ is small compared to $K$ but without assumptions on malicious agents'\nbehavior, thus ensuring that our algorithm is robust against any malicious\nrecommendation strategy.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 22:27:30 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 22:31:09 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Vial", "Daniel", ""], ["Shakkottai", "Sanjay", ""], ["Srikant", "R.", ""]]}, {"id": "2007.03813", "submitter": "Yingxue Zhou", "authors": "Yingxue Zhou, Zhiwei Steven Wu, Arindam Banerjee", "title": "Bypassing the Ambient Dimension: Private SGD with Gradient Subspace\n  Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentially private SGD (DP-SGD) is one of the most popular methods for\nsolving differentially private empirical risk minimization (ERM). Due to its\nnoisy perturbation on each gradient update, the error rate of DP-SGD scales\nwith the ambient dimension $p$, the number of parameters in the model. Such\ndependence can be problematic for over-parameterized models where $p \\gg n$,\nthe number of training samples. Existing lower bounds on private ERM show that\nsuch dependence on $p$ is inevitable in the worst case. In this paper, we\ncircumvent the dependence on the ambient dimension by leveraging a\nlow-dimensional structure of gradient space in deep networks -- that is, the\nstochastic gradients for deep nets usually stay in a low dimensional subspace\nin the training process. We propose Projected DP-SGD that performs noise\nreduction by projecting the noisy gradients to a low-dimensional subspace,\nwhich is given by the top gradient eigenspace on a small public dataset. We\nprovide a general sample complexity analysis on the public dataset for the\ngradient subspace identification problem and demonstrate that under certain\nlow-dimensional assumptions the public sample complexity only grows\nlogarithmically in $p$. Finally, we provide a theoretical analysis and\nempirical evaluations to show that our method can substantially improve the\naccuracy of DP-SGD in the high privacy regime (corresponding to low privacy\nloss $\\epsilon$).\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 22:31:01 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 23:07:28 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Zhou", "Yingxue", ""], ["Wu", "Zhiwei Steven", ""], ["Banerjee", "Arindam", ""]]}, {"id": "2007.03814", "submitter": "Jeremiah Birrell", "authors": "Jeremiah Birrell, Paul Dupuis, Markos A. Katsoulakis, Luc Rey-Bellet,\n  Jie Wang", "title": "Variational Representations and Neural Network Estimation of R\\'enyi\n  Divergences", "comments": "24 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a new variational formula for the R\\'enyi family of divergences,\n$R_\\alpha(Q\\|P)$, between probability measures $Q$ and $P$. Our result\ngeneralizes the classical Donsker-Varadhan variational formula for the\nKullback-Leibler divergence. We further show that this R\\'enyi variational\nformula holds over a range of function spaces; this leads to a formula for the\noptimizer under very weak assumptions and is also key in our development of a\nconsistency theory for R\\'enyi divergence estimators. By applying this theory\nto neural-network estimators, we show that if a neural network family satisfies\none of several strengthened versions of the universal approximation property\nthen the corresponding R\\'enyi divergence estimator is consistent. In contrast\nto density-estimator based methods, our estimators involve only expectations\nunder $Q$ and $P$ and hence are more effective in high dimensional systems. We\nillustrate this via several numerical examples of neural network estimation in\nsystems of up to 5000 dimensions.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 22:34:30 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 15:51:29 GMT"}, {"version": "v3", "created": "Tue, 27 Apr 2021 13:07:27 GMT"}, {"version": "v4", "created": "Tue, 20 Jul 2021 16:12:35 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Birrell", "Jeremiah", ""], ["Dupuis", "Paul", ""], ["Katsoulakis", "Markos A.", ""], ["Rey-Bellet", "Luc", ""], ["Wang", "Jie", ""]]}, {"id": "2007.03828", "submitter": "Abel Peirson V", "authors": "A.L.Peirson, R.W.Romani, H.L.Marshall, J.F.Steiner, L.Baldini", "title": "Deep Ensemble Analysis for Imaging X-ray Polarimetry", "comments": "18 pages, 9 figures. Accepted to Nuclear Instruments and Methods in\n  Physics Research Section A, Sep 2020", "journal-ref": null, "doi": "10.1016/j.nima.2020.164740", "report-no": null, "categories": "astro-ph.IM astro-ph.HE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for enhancing the sensitivity of X-ray telescopic\nobservations with imaging polarimeters, with a focus on the gas pixel detectors\n(GPDs) to be flown on the Imaging X-ray Polarimetry Explorer (IXPE). Our\nanalysis determines photoelectron directions, X-ray absorption points and X-ray\nenergies for 1-9 keV event tracks, with estimates for both the statistical and\nmodel (reconstruction) uncertainties. We use a weighted maximum likelihood\ncombination of predictions from a deep ensemble of ResNet convolutional neural\nnetworks, trained on Monte Carlo event simulations. We define a figure of merit\nto compare the polarization bias-variance trade-off in track reconstruction\nalgorithms. For power-law source spectra, our method improves on the current\nplanned IXPE analysis (and previous deep learning approaches), providing ~45%\nincrease in effective exposure times. For individual energies, our method\nproduces 20-30% absolute improvements in modulation factor for simulated 100%\npolarized events, while keeping residual systematic modulation within 1 sigma\nof the finite sample minimum. Absorption point location and photon energy\nestimates are also significantly improved. We have validated our method with\nsample data from real GPD detectors.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 00:10:15 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 20:46:26 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Peirson", "A. L.", ""], ["Romani", "R. W.", ""], ["Marshall", "H. L.", ""], ["Steiner", "J. F.", ""], ["Baldini", "L.", ""]]}, {"id": "2007.03832", "submitter": "Olivia Brown", "authors": "Justin Goodwin, Olivia Brown, Victoria Helus", "title": "Fast Training of Deep Neural Networks Robust to Adversarial\n  Perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are capable of training fast and generalizing well\nwithin many domains. Despite their promising performance, deep networks have\nshown sensitivities to perturbations of their inputs (e.g., adversarial\nexamples) and their learned feature representations are often difficult to\ninterpret, raising concerns about their true capability and trustworthiness.\nRecent work in adversarial training, a form of robust optimization in which the\nmodel is optimized against adversarial examples, demonstrates the ability to\nimprove performance sensitivities to perturbations and yield feature\nrepresentations that are more interpretable. Adversarial training, however,\ncomes with an increased computational cost over that of standard (i.e.,\nnonrobust) training, rendering it impractical for use in large-scale problems.\nRecent work suggests that a fast approximation to adversarial training shows\npromise for reducing training time and maintaining robustness in the presence\nof perturbations bounded by the infinity norm. In this work, we demonstrate\nthat this approach extends to the Euclidean norm and preserves the\nhuman-aligned feature representations that are common for robust models.\nAdditionally, we show that using a distributed training scheme can further\nreduce the time to train robust deep networks. Fast adversarial training is a\npromising approach that will provide increased security and explainability in\nmachine learning applications for which robust optimization was previously\nthought to be impractical.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 00:35:39 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Goodwin", "Justin", ""], ["Brown", "Olivia", ""], ["Helus", "Victoria", ""]]}, {"id": "2007.03844", "submitter": "Zexi Chen", "authors": "Zexi Chen, Bharathkumar Ramachandra, Ranga Raju Vatsavai", "title": "Consistency Regularization with Generative Adversarial Networks for\n  Semi-Supervised Learning", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) based semi-supervised learning (SSL)\napproaches are shown to improve classification performance by utilizing a large\nnumber of unlabeled samples in conjunction with limited labeled samples.\nHowever, their performance still lags behind the state-of-the-art non-GAN based\nSSL approaches. We identify that the main reason for this is the lack of\nconsistency in class probability predictions on the same image under local\nperturbations. Following the general literature, we address this issue via\nlabel consistency regularization, which enforces the class probability\npredictions for an input image to be unchanged under various\nsemantic-preserving perturbations. In this work, we introduce consistency\nregularization into the vanilla semi-GAN to address this critical limitation.\nIn particular, we present a new composite consistency regularization method\nwhich, in spirit, leverages both local consistency and interpolation\nconsistency. We demonstrate the efficacy of our approach on two SSL image\nclassification benchmark datasets, SVHN and CIFAR-10. Our experiments show that\nthis new composite consistency regularization based semi-GAN significantly\nimproves its performance and achieves new state-of-the-art performance among\nGAN-based SSL approaches.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 01:47:10 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 08:23:06 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Chen", "Zexi", ""], ["Ramachandra", "Bharathkumar", ""], ["Vatsavai", "Ranga Raju", ""]]}, {"id": "2007.03856", "submitter": "Vaikkunth Mugunthan", "authors": "Vaikkunth Mugunthan, Ravi Rahman and Lalana Kagal", "title": "BlockFLow: An Accountable and Privacy-Preserving Solution for Federated\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning enables the development of a machine learning model among\ncollaborating agents without requiring them to share their underlying data.\nHowever, malicious agents who train on random data, or worse, on datasets with\nthe result classes inverted, can weaken the combined model. BlockFLow is an\naccountable federated learning system that is fully decentralized and\nprivacy-preserving. Its primary goal is to reward agents proportional to the\nquality of their contribution while protecting the privacy of the underlying\ndatasets and being resilient to malicious adversaries. Specifically, BlockFLow\nincorporates differential privacy, introduces a novel auditing mechanism for\nmodel contribution, and uses Ethereum smart contracts to incentivize good\nbehavior. Unlike existing auditing and accountability methods for federated\nlearning systems, our system does not require a centralized test dataset,\nsharing of datasets between the agents, or one or more trusted auditors; it is\nfully decentralized and resilient up to a 50% collusion attack in a malicious\ntrust model. When run on the public Ethereum blockchain, BlockFLow uses the\nresults from the audit to reward parties with cryptocurrency based on the\nquality of their contribution. We evaluated BlockFLow on two datasets that\noffer classification tasks solvable via logistic regression models. Our results\nshow that the resultant auditing scores reflect the quality of the honest\nagents' datasets. Moreover, the scores from dishonest agents are statistically\nlower than those from the honest agents. These results, along with the\nreasonable blockchain costs, demonstrate the effectiveness of BlockFLow as an\naccountable federated learning system.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 02:24:26 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Mugunthan", "Vaikkunth", ""], ["Rahman", "Ravi", ""], ["Kagal", "Lalana", ""]]}, {"id": "2007.03898", "submitter": "Arash Vahdat", "authors": "Arash Vahdat, Jan Kautz", "title": "NVAE: A Deep Hierarchical Variational Autoencoder", "comments": "Neural Information Processing Systems (NeurIPS) 2020 (spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalizing flows, autoregressive models, variational autoencoders (VAEs),\nand deep energy-based models are among competing likelihood-based frameworks\nfor deep generative learning. Among them, VAEs have the advantage of fast and\ntractable sampling and easy-to-access encoding networks. However, they are\ncurrently outperformed by other models such as normalizing flows and\nautoregressive models. While the majority of the research in VAEs is focused on\nthe statistical challenges, we explore the orthogonal direction of carefully\ndesigning neural architectures for hierarchical VAEs. We propose Nouveau VAE\n(NVAE), a deep hierarchical VAE built for image generation using depth-wise\nseparable convolutions and batch normalization. NVAE is equipped with a\nresidual parameterization of Normal distributions and its training is\nstabilized by spectral regularization. We show that NVAE achieves\nstate-of-the-art results among non-autoregressive likelihood-based models on\nthe MNIST, CIFAR-10, CelebA 64, and CelebA HQ datasets and it provides a strong\nbaseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art\nfrom 2.98 to 2.91 bits per dimension, and it produces high-quality images on\nCelebA HQ. To the best of our knowledge, NVAE is the first successful VAE\napplied to natural images as large as 256$\\times$256 pixels. The source code is\navailable at https://github.com/NVlabs/NVAE .\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 04:56:56 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 17:27:38 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2021 03:08:58 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Vahdat", "Arash", ""], ["Kautz", "Jan", ""]]}, {"id": "2007.03899", "submitter": "Masanari Kimura", "authors": "Masanari Kimura and Ryohei Izawa", "title": "Density Fixing: Simple yet Effective Regularization Method based on the\n  Class Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models suffer from overfitting, which is caused by a lack of\nlabeled data. To tackle this problem, we proposed a framework of regularization\nmethods, called density-fixing, that can be used commonly for supervised and\nsemi-supervised learning. Our proposed regularization method improves the\ngeneralization performance by forcing the model to approximate the class's\nprior distribution or the frequency of occurrence. This regularization term is\nnaturally derived from the formula of maximum likelihood estimation and is\ntheoretically justified. We further provide the several theoretical analyses of\nthe proposed method including asymptotic behavior. Our experimental results on\nmultiple benchmark datasets are sufficient to support our argument, and we\nsuggest that this simple and effective regularization method is useful in\nreal-world machine learning problems.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 04:58:22 GMT"}, {"version": "v2", "created": "Sun, 6 Sep 2020 05:07:23 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Kimura", "Masanari", ""], ["Izawa", "Ryohei", ""]]}, {"id": "2007.03912", "submitter": "Koji Maruhashi Dr.", "authors": "Koji Maruhashi, Heewon Park, Rui Yamaguchi, Satoru Miyano", "title": "Linear Tensor Projection Revealing Nonlinearity", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality reduction is an effective method for learning high-dimensional\ndata, which can provide better understanding of decision boundaries in\nhuman-readable low-dimensional subspace. Linear methods, such as principal\ncomponent analysis and linear discriminant analysis, make it possible to\ncapture the correlation between many variables; however, there is no guarantee\nthat the correlations that are important in predicting data can be captured.\nMoreover, if the decision boundary has strong nonlinearity, the guarantee\nbecomes increasingly difficult. This problem is exacerbated when the data are\nmatrices or tensors that represent relationships between variables. We propose\na learning method that searches for a subspace that maximizes the prediction\naccuracy while retaining as much of the original data information as possible,\neven if the prediction model in the subspace has strong nonlinearity. This\nmakes it easier to interpret the mechanism of the group of variables behind the\nprediction problem that the user wants to know. We show the effectiveness of\nour method by applying it to various types of data including matrices and\ntensors.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 06:10:39 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Maruhashi", "Koji", ""], ["Park", "Heewon", ""], ["Yamaguchi", "Rui", ""], ["Miyano", "Satoru", ""]]}, {"id": "2007.03920", "submitter": "Andrii Trelin", "authors": "Andrii Trelin and Ale\\v{s} Proch\\'azka", "title": "Binary Stochastic Filtering: feature selection and beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is one of the most decisive tools in understanding data and\nmachine learning models. Among other methods, sparsity induced by $L^{1}$\npenalty is one of the simplest and best studied approaches to this problem.\nAlthough such regularization is frequently used in neural networks to achieve\nsparsity of weights or unit activations, it is unclear how it can be employed\nin the feature selection problem. This work aims at extending the neural\nnetwork with ability to automatically select features by rethinking how the\nsparsity regularization can be used, namely, by stochastically penalizing\nfeature involvement instead of the layer weights. The proposed method has\ndemonstrated superior efficiency when compared to a few classical methods,\nachieved with minimal or no computational overhead, and can be directly applied\nto any existing architecture. Furthermore, the method is easily generalizable\nfor neuron pruning and selection of regions of importance for spectral data.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 06:57:10 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Trelin", "Andrii", ""], ["Proch\u00e1zka", "Ale\u0161", ""]]}, {"id": "2007.03937", "submitter": "Tommaso Cesari", "authors": "Tommaso Cesari (TSE), Roberto Colomboni (IIT)", "title": "A Nearest Neighbor Characterization of Lebesgue Points in Metric Measure\n  Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The property of almost every point being a Lebesgue point has proven to be\ncrucial for the consistency of several classification algorithms based on\nnearest neighbors. We characterize Lebesgue points in terms of a 1-Nearest\nNeighbor regression algorithm for pointwise estimation, fleshing out the role\nplayed by tie-breaking rules in the corresponding convergence problem. We then\ngive an application of our results, proving the convergence of the risk of a\nlarge class of 1-Nearest Neighbor classification algorithms in general metric\nspaces where almost every point is a Lebesgue point.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 07:42:31 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 10:13:43 GMT"}, {"version": "v3", "created": "Fri, 11 Dec 2020 10:37:56 GMT"}, {"version": "v4", "created": "Tue, 12 Jan 2021 15:15:27 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Cesari", "Tommaso", "", "TSE"], ["Colomboni", "Roberto", "", "IIT"]]}, {"id": "2007.03938", "submitter": "Minsoo Kang", "authors": "Minsoo Kang and Bohyung Han", "title": "Operation-Aware Soft Channel Pruning using Differentiable Masks", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple but effective data-driven channel pruning algorithm,\nwhich compresses deep neural networks in a differentiable way by exploiting the\ncharacteristics of operations. The proposed approach makes a joint\nconsideration of batch normalization (BN) and rectified linear unit (ReLU) for\nchannel pruning; it estimates how likely the two successive operations\ndeactivate each feature map and prunes the channels with high probabilities. To\nthis end, we learn differentiable masks for individual channels and make soft\ndecisions throughout the optimization procedure, which facilitates to explore\nlarger search space and train more stable networks. The proposed framework\nenables us to identify compressed models via a joint learning of model\nparameters and channel pruning without an extra procedure of fine-tuning. We\nperform extensive experiments and achieve outstanding performance in terms of\nthe accuracy of output networks given the same amount of resources when\ncompared with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 07:44:00 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 02:33:59 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Kang", "Minsoo", ""], ["Han", "Bohyung", ""]]}, {"id": "2007.03961", "submitter": "Fanchen Bu", "authors": "Fanchen Bu, Dong Eui Chang", "title": "Double Prioritized State Recycled Experience Replay", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Experience replay enables online reinforcement learning agents to store and\nreuse the previous experiences of interacting with the environment. In the\noriginal method, the experiences are sampled and replayed uniformly at random.\nA prior work called prioritized experience replay was developed where\nexperiences are prioritized, so as to replay experiences seeming to be more\nimportant more frequently. In this paper, we develop a method called\ndouble-prioritized state-recycled (DPSR) experience replay, prioritizing the\nexperiences in both training stage and storing stage, as well as replacing the\nexperiences in the memory with state recycling to make the best of experiences\nthat seem to have low priorities temporarily. We used this method in Deep\nQ-Networks (DQN), and achieved a state-of-the-art result, outperforming the\noriginal method and prioritized experience replay on many Atari games.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 08:36:41 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 02:03:06 GMT"}, {"version": "v3", "created": "Mon, 21 Sep 2020 12:15:24 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Bu", "Fanchen", ""], ["Chang", "Dong Eui", ""]]}, {"id": "2007.03966", "submitter": "Xin-Yu Zhang", "authors": "Xin-Yu Zhang, Taihong Xiao, Haolin Jia, Ming-Ming Cheng, Ming-Hsuan\n  Yang", "title": "Semi-Supervised Learning with Meta-Gradient", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a simple yet effective meta-learning algorithm in\nsemi-supervised learning. We notice that most existing consistency-based\napproaches suffer from overfitting and limited model generalization ability,\nespecially when training with only a small number of labeled data. To alleviate\nthis issue, we propose a learn-to-generalize regularization term by utilizing\nthe label information and optimize the problem in a meta-learning fashion.\nSpecifically, we seek the pseudo labels of the unlabeled data so that the model\ncan generalize well on the labeled data, which is formulated as a nested\noptimization problem. We address this problem using the meta-gradient that\nbridges between the pseudo label and the regularization term. In addition, we\nintroduce a simple first-order approximation to avoid computing higher-order\nderivatives and provide theoretic convergence analysis. Extensive evaluations\non the SVHN, CIFAR, and ImageNet datasets demonstrate that the proposed\nalgorithm performs favorably against state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 08:48:56 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 07:04:26 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Zhang", "Xin-Yu", ""], ["Xiao", "Taihong", ""], ["Jia", "Haolin", ""], ["Cheng", "Ming-Ming", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2007.03995", "submitter": "Nabeel Seedat", "authors": "Nabeel Seedat", "title": "MCU-Net: A framework towards uncertainty representations for decision\n  support system patient referrals in healthcare contexts", "comments": "4 pages, 4 figures,Spotlight Talk at KDD 2020 - Applied Data Science\n  for Healthcare Workshop & presented at ICML 2020: Uncertainty and Robustness\n  in Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating a human-in-the-loop system when deploying automated decision\nsupport is critical in healthcare contexts to create trust, as well as provide\nreliable performance on a patient-to-patient basis. Deep learning methods while\nhaving high performance, do not allow for this patient-centered approach due to\nthe lack of uncertainty representation. Thus, we present a framework of\nuncertainty representation evaluated for medical image segmentation, using\nMCU-Net which combines a U-Net with Monte Carlo Dropout, evaluated with four\ndifferent uncertainty metrics. The framework augments this by adding a\nhuman-in-the-loop aspect based on an uncertainty threshold for automated\nreferral of uncertain cases to a medical professional. We demonstrate that\nMCU-Net combined with epistemic uncertainty and an uncertainty threshold tuned\nfor this application maximizes automated performance on an individual patient\nlevel, yet refers truly uncertain cases. This is a step towards uncertainty\nrepresentations when deploying machine learning based decision support in\nhealthcare settings.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 09:54:56 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 12:52:00 GMT"}, {"version": "v3", "created": "Tue, 25 Aug 2020 11:12:16 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Seedat", "Nabeel", ""]]}, {"id": "2007.04001", "submitter": "Pim Verschuuren", "authors": "Pim Verschuuren, Serena Palazzo, Tom Powell, Steve Sutton, Alfred\n  Pilgrim, Michele Faucci Giannelli", "title": "Supervised machine learning techniques for data matching based on\n  similarity metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Businesses, governmental bodies and NGO's have an ever-increasing amount of\ndata at their disposal from which they try to extract valuable information.\nOften, this needs to be done not only accurately but also within a short time\nframe. Clean and consistent data is therefore crucial. Data matching is the\nfield that tries to identify instances in data that refer to the same\nreal-world entity. In this study, machine learning techniques are combined with\nstring similarity functions to the field of data matching. A dataset of\ninvoices from a variety of businesses and organizations was preprocessed with a\ngrouping scheme to reduce pair dimensionality and a set of similarity functions\nwas used to quantify similarity between invoice pairs. The resulting invoice\npair dataset was then used to train and validate a neural network and a boosted\ndecision tree. The performance was compared with a solution from FISCAL\nTechnologies as a benchmark against currently available deduplication\nsolutions. Both the neural network and boosted decision tree showed equal to\nbetter performance.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 10:04:35 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Verschuuren", "Pim", ""], ["Palazzo", "Serena", ""], ["Powell", "Tom", ""], ["Sutton", "Steve", ""], ["Pilgrim", "Alfred", ""], ["Giannelli", "Michele Faucci", ""]]}, {"id": "2007.04002", "submitter": "Daisuke Moriwaki", "authors": "Daisuke Moriwaki and Yuta Hayakawa and Isshu Munemasa and Yuta Saito\n  and Akira Matsui", "title": "Unbiased Lift-based Bidding System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional bidding strategies for online display ad auction heavily relies\non observed performance indicators such as clicks or conversions. A bidding\nstrategy naively pursuing these easily observable metrics, however, fails to\noptimize the profitability of the advertisers. Rather, the bidding strategy\nthat leads to the maximum revenue is a strategy pursuing the performance lift\nof showing ads to a specific user. Therefore, it is essential to predict the\nlift-effect of showing ads to each user on their target variables from observed\nlog data. However, there is a difficulty in predicting the lift-effect, as the\ntraining data gathered by a past bidding strategy may have a strong bias\ntowards the winning impressions. In this study, we develop Unbiased Lift-based\nBidding System, which maximizes the advertisers' profit by accurately\npredicting the lift-effect from biased log data. Our system is the first to\nenable high-performing lift-based bidding strategy by theoretically alleviating\nthe inherent bias in the log. Real-world, large-scale A/B testing successfully\ndemonstrates the superiority and practicability of the proposed system.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 10:05:53 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 02:09:17 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Moriwaki", "Daisuke", ""], ["Hayakawa", "Yuta", ""], ["Munemasa", "Isshu", ""], ["Saito", "Yuta", ""], ["Matsui", "Akira", ""]]}, {"id": "2007.04005", "submitter": "Kirien Whan", "authors": "Simon Veldkamp, Kirien Whan, Sjoerd Dirksen and Maurice Schmeits", "title": "Statistical post-processing of wind speed forecasts using convolutional\n  neural networks", "comments": "44 pages, 5 figures", "journal-ref": null, "doi": "10.1175/MWR-D-20-0219.1", "report-no": null, "categories": "stat.ML cs.LG physics.ao-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current statistical post-processing methods for probabilistic weather\nforecasting are not capable of using full spatial patterns from the numerical\nweather prediction (NWP) model. In this paper we incorporate spatial wind speed\ninformation by using convolutional neural networks (CNNs) and obtain\nprobabilistic wind speed forecasts in the Netherlands for 48 hours ahead, based\non KNMI's deterministic Harmonie-Arome NWP model. The probabilistic forecasts\nfrom the CNNs are shown to have higher Brier skill scores for medium to higher\nwind speeds, as well as a better continuous ranked probability score (CRPS) and\nlogarithmic score, than the forecasts from fully connected neural networks and\nquantile regression forests. As a secondary result, we have compared the CNNs\nusing 3 different density estimation methods (quantized softmax (QS), kernel\nmixture networks, and fitting a truncated normal distribution), and found the\nprobabilistic forecasts based on the QS method to be best.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 10:18:07 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 11:49:59 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Veldkamp", "Simon", ""], ["Whan", "Kirien", ""], ["Dirksen", "Sjoerd", ""], ["Schmeits", "Maurice", ""]]}, {"id": "2007.04006", "submitter": "Yiping Jiang MPhil", "authors": "Yiping Jiang, Tianshi Chen", "title": "Accelerated Sparse Bayesian Learning via Screening Test and Its\n  Applications", "comments": "15 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high-dimensional settings, sparse structures are critical for efficiency\nin term of memory and computation complexity. For a linear system, to find the\nsparsest solution provided with an over-complete dictionary of features\ndirectly is typically NP-hard, and thus alternative approximate methods should\nbe considered. In this paper, our choice for alternative method is sparse\nBayesian learning, which, as empirical Bayesian approaches, uses a\nparameterized prior to encourage sparsity in solution, rather than the other\nmethods with fixed priors such as LASSO. Screening test, however, aims at\nquickly identifying a subset of features whose coefficients are guaranteed to\nbe zero in the optimal solution, and then can be safely removed from the\ncomplete dictionary to obtain a smaller, more easily solved problem. Next, we\nsolve the smaller problem, after which the solution of the original problem can\nbe recovered by padding the smaller solution with zeros. The performance of the\nproposed method will be examined on various data sets and applications.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 10:21:56 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Jiang", "Yiping", ""], ["Chen", "Tianshi", ""]]}, {"id": "2007.04028", "submitter": "Amartya Sanyal", "authors": "Amartya Sanyal, Puneet K Dokania, Varun Kanade, Philip H.S. Torr", "title": "How benign is benign overfitting?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate two causes for adversarial vulnerability in deep neural\nnetworks: bad data and (poorly) trained models. When trained with SGD, deep\nneural networks essentially achieve zero training error, even in the presence\nof label noise, while also exhibiting good generalization on natural test data,\nsomething referred to as benign overfitting [2, 10]. However, these models are\nvulnerable to adversarial attacks. We identify label noise as one of the causes\nfor adversarial vulnerability, and provide theoretical and empirical evidence\nin support of this. Surprisingly, we find several instances of label noise in\ndatasets such as MNIST and CIFAR, and that robustly trained models incur\ntraining error on some of these, i.e. they don't fit the noise. However,\nremoving noisy labels alone does not suffice to achieve adversarial robustness.\nStandard training procedures bias neural networks towards learning \"simple\"\nclassification boundaries, which may be less robust than more complex ones. We\nobserve that adversarial training does produce more complex decision\nboundaries. We conjecture that in part the need for complex decision boundaries\narises from sub-optimal representation learning. By means of simple toy\nexamples, we show theoretically how the choice of representation can\ndrastically affect adversarial robustness.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 11:07:10 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Sanyal", "Amartya", ""], ["Dokania", "Puneet K", ""], ["Kanade", "Varun", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "2007.04030", "submitter": "Deepak Maurya Mr", "authors": "Deepak Maurya, Sivadurgaprasad Chinta, Abhishek Sivaram and\n  Raghunathan Rengaswamy", "title": "Incorporating prior knowledge about structural constraints in model\n  identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model identification is a crucial problem in chemical industries. In recent\nyears, there has been increasing interest in learning data-driven models\nutilizing partial knowledge about the system of interest. Most techniques for\nmodel identification do not provide the freedom to incorporate any partial\ninformation such as the structure of the model. In this article, we propose\nmodel identification techniques that could leverage such partial information to\nproduce better estimates. Specifically, we propose Structural Principal\nComponent Analysis (SPCA) which improvises over existing methods like PCA by\nutilizing the essential structural information about the model. Most of the\nexisting methods or closely related methods use sparsity constraints which\ncould be computationally expensive. Our proposed method is a wise modification\nof PCA to utilize structural information. The efficacy of the proposed approach\nis demonstrated using synthetic and industrial case-studies.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 11:09:59 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Maurya", "Deepak", ""], ["Chinta", "Sivadurgaprasad", ""], ["Sivaram", "Abhishek", ""], ["Rengaswamy", "Raghunathan", ""]]}, {"id": "2007.04043", "submitter": "Tianyi Zhang", "authors": "Tianyi Zhang, Ikko Yamane, Nan Lu, Masashi Sugiyama", "title": "A One-step Approach to Covariate Shift Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A default assumption in many machine learning scenarios is that the training\nand test samples are drawn from the same probability distribution. However,\nsuch an assumption is often violated in the real world due to non-stationarity\nof the environment or bias in sample selection. In this work, we consider a\nprevalent setting called covariate shift, where the input distribution differs\nbetween the training and test stages while the conditional distribution of the\noutput given the input remains unchanged. Most of the existing methods for\ncovariate shift adaptation are two-step approaches, which first calculate the\nimportance weights and then conduct importance-weighted empirical risk\nminimization. In this paper, we propose a novel one-step approach that jointly\nlearns the predictive model and the associated weights in one optimization by\nminimizing an upper bound of the test risk. We theoretically analyze the\nproposed method and provide a generalization error bound. We also empirically\ndemonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 11:35:47 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 03:37:00 GMT"}, {"version": "v3", "created": "Mon, 3 May 2021 04:59:06 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Zhang", "Tianyi", ""], ["Yamane", "Ikko", ""], ["Lu", "Nan", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "2007.04068", "submitter": "Shakir Mohamed", "authors": "Shakir Mohamed, Marie-Therese Png, William Isaac", "title": "Decolonial AI: Decolonial Theory as Sociotechnical Foresight in\n  Artificial Intelligence", "comments": "28 Pages. Accepted, to appear in: Philosophy and Technology (405),\n  Springer. Submitted 16 January, Accepted 26 May 2020", "journal-ref": null, "doi": "10.1007/s13347-020-00405-8", "report-no": null, "categories": "cs.CY cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper explores the important role of critical science, and in particular\nof post-colonial and decolonial theories, in understanding and shaping the\nongoing advances in artificial intelligence. Artificial Intelligence (AI) is\nviewed as amongst the technological advances that will reshape modern societies\nand their relations. Whilst the design and deployment of systems that\ncontinually adapt holds the promise of far-reaching positive change, they\nsimultaneously pose significant risks, especially to already vulnerable\npeoples. Values and power are central to this discussion. Decolonial theories\nuse historical hindsight to explain patterns of power that shape our\nintellectual, political, economic, and social world. By embedding a decolonial\ncritical approach within its technical practice, AI communities can develop\nforesight and tactics that can better align research and technology development\nwith established ethical principles, centring vulnerable peoples who continue\nto bear the brunt of negative impacts of innovation and scientific progress. We\nhighlight problematic applications that are instances of coloniality, and using\na decolonial lens, submit three tactics that can form a decolonial field of\nartificial intelligence: creating a critical technical practice of AI, seeking\nreverse tutelage and reverse pedagogies, and the renewal of affective and\npolitical communities. The years ahead will usher in a wave of new scientific\nbreakthroughs and technologies driven by AI research, making it incumbent upon\nAI communities to strengthen the social contract through ethical foresight and\nthe multiplicity of intellectual perspectives available to us; ultimately\nsupporting future technologies that enable greater well-being, with the goal of\nbeneficence and justice for all.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 12:36:21 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Mohamed", "Shakir", ""], ["Png", "Marie-Therese", ""], ["Isaac", "William", ""]]}, {"id": "2007.04074", "submitter": "Matthias Feurer", "authors": "Matthias Feurer, Katharina Eggensperger, Stefan Falkner, Marius\n  Lindauer and Frank Hutter", "title": "Auto-Sklearn 2.0: The Next Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated Machine Learning, which supports practitioners and researchers with\nthe tedious task of manually designing machine learning pipelines, has recently\nachieved substantial success. In this paper we introduce new Automated Machine\nLearning (AutoML) techniques motivated by our winning submission to the second\nChaLearn AutoML challenge, PoSH Auto-sklearn. For this, we extend Auto-sklearn\nwith a new, simpler meta-learning technique, improve its way of handling\niterative algorithms and enhance it with a successful bandit strategy for\nbudget allocation. Furthermore, we go one step further and study the design\nspace of AutoML itself and propose a solution towards truly hand-free AutoML.\nTogether, these changes give rise to the next generation of our AutoML system,\nAuto-sklearn (2.0). We verify the improvement by these additions in a large\nexperimental study on 39 AutoML benchmark datasets and conclude the paper by\ncomparing to Auto-sklearn (1.0), reducing the regret by up to a factor of five.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 12:41:03 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Feurer", "Matthias", ""], ["Eggensperger", "Katharina", ""], ["Falkner", "Stefan", ""], ["Lindauer", "Marius", ""], ["Hutter", "Frank", ""]]}, {"id": "2007.04087", "submitter": "Minsu Cho", "authors": "Minsu Cho, Mohammadreza Soltani, and Chinmay Hegde", "title": "Hyperparameter Optimization in Neural Networks via Structured Sparse\n  Recovery", "comments": "arXiv admin note: text overlap with arXiv:1906.02869", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study two important problems in the automated design of\nneural networks -- Hyper-parameter Optimization (HPO), and Neural Architecture\nSearch (NAS) -- through the lens of sparse recovery methods. In the first part\nof this paper, we establish a novel connection between HPO and structured\nsparse recovery. In particular, we show that a special encoding of the\nhyperparameter space enables a natural group-sparse recovery formulation, which\nwhen coupled with HyperBand (a multi-armed bandit strategy), leads to\nimprovement over existing hyperparameter optimization methods. Experimental\nresults on image datasets such as CIFAR-10 confirm the benefits of our\napproach. In the second part of this paper, we establish a connection between\nNAS and structured sparse recovery. Building upon ``one-shot'' approaches in\nNAS, we propose a novel algorithm that we call CoNAS by merging ideas from\none-shot approaches with a techniques for learning low-degree sparse Boolean\npolynomials. We provide theoretical analysis on the number of validation error\nmeasurements. Finally, we validate our approach on several datasets and\ndiscover novel architectures hitherto unreported, achieving competitive (or\nbetter) results in both performance and search time compared to the existing\nNAS approaches.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 00:57:09 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Cho", "Minsu", ""], ["Soltani", "Mohammadreza", ""], ["Hegde", "Chinmay", ""]]}, {"id": "2007.04091", "submitter": "Michela Paganini", "authors": "Michela Paganini, Jessica Zosa Forde", "title": "Bespoke vs. Pr\\^et-\\`a-Porter Lottery Tickets: Exploiting Mask\n  Similarity for Trainable Sub-Network Finding", "comments": "arXiv admin note: text overlap with arXiv:2001.05050", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The observation of sparse trainable sub-networks within over-parametrized\nnetworks - also known as Lottery Tickets (LTs) - has prompted inquiries around\ntheir trainability, scaling, uniqueness, and generalization properties. Across\n28 combinations of image classification tasks and architectures, we discover\ndifferences in the connectivity structure of LTs found through different\niterative pruning techniques, thus disproving their uniqueness and connecting\nemergent mask structure to the choice of pruning. In addition, we propose a\nconsensus-based method for generating refined lottery tickets. This lottery\nticket denoising procedure, based on the principle that parameters that always\ngo unpruned across different tasks more reliably identify important\nsub-networks, is capable of selecting a meaningful portion of the architecture\nin an embarrassingly parallel way, while quickly discarding extra parameters\nwithout the need for further pruning iterations. We successfully train these\nsub-networks to performance comparable to that of ordinary lottery tickets.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 22:48:35 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Paganini", "Michela", ""], ["Forde", "Jessica Zosa", ""]]}, {"id": "2007.04131", "submitter": "Christoph Molnar", "authors": "Christoph Molnar, Gunnar K\\\"onig, Julia Herbinger, Timo Freiesleben,\n  Susanne Dandl, Christian A. Scholbeck, Giuseppe Casalicchio, Moritz\n  Grosse-Wentrup, Bernd Bischl", "title": "Pitfalls to Avoid when Interpreting Machine Learning Models", "comments": "This article was accepted at the ICML 2020 workshop XXAI: Extending\n  Explainable AI Beyond Deep Models and Classifiers (see\n  http://interpretable-ml.org/icml2020workshop/ )", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern requirements for machine learning (ML) models include both high\npredictive performance and model interpretability. A growing number of\ntechniques provide model interpretations, but can lead to wrong conclusions if\napplied incorrectly. We illustrate pitfalls of ML model interpretation such as\nbad model generalization, dependent features, feature interactions or\nunjustified causal interpretations. Our paper addresses ML practitioners by\nraising awareness of pitfalls and pointing out solutions for correct model\ninterpretation, as well as ML researchers by discussing open issues for further\nresearch.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 14:02:56 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Molnar", "Christoph", ""], ["K\u00f6nig", "Gunnar", ""], ["Herbinger", "Julia", ""], ["Freiesleben", "Timo", ""], ["Dandl", "Susanne", ""], ["Scholbeck", "Christian A.", ""], ["Casalicchio", "Giuseppe", ""], ["Grosse-Wentrup", "Moritz", ""], ["Bischl", "Bernd", ""]]}, {"id": "2007.04146", "submitter": "Ahmed Frikha", "authors": "Ahmed Frikha, Denis Krompa{\\ss}, Hans-Georg K\\\"opken and Volker Tresp", "title": "Few-Shot One-Class Classification via Meta-Learning", "comments": "Accepted at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although few-shot learning and one-class classification (OCC), i.e., learning\na binary classifier with data from only one class, have been separately well\nstudied, their intersection remains rather unexplored. Our work addresses the\nfew-shot OCC problem and presents a method to modify the episodic data sampling\nstrategy of the model-agnostic meta-learning (MAML) algorithm to learn a model\ninitialization particularly suited for learning few-shot OCC tasks. This is\ndone by explicitly optimizing for an initialization which only requires few\ngradient steps with one-class minibatches to yield a performance increase on\nclass-balanced test data. We provide a theoretical analysis that explains why\nour approach works in the few-shot OCC scenario, while other meta-learning\nalgorithms fail, including the unmodified MAML. Our experiments on eight\ndatasets from the image and time-series domains show that our method leads to\nbetter results than classical OCC and few-shot classification approaches, and\ndemonstrate the ability to learn unseen tasks from only few normal class\nsamples. Moreover, we successfully train anomaly detectors for a real-world\napplication on sensor readings recorded during industrial manufacturing of\nworkpieces with a CNC milling machine, by using few normal examples. Finally,\nwe empirically demonstrate that the proposed data sampling technique increases\nthe performance of more recent meta-learning algorithms in few-shot OCC and\nyields state-of-the-art results in this problem setting.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 14:19:29 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 18:52:13 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Frikha", "Ahmed", ""], ["Krompa\u00df", "Denis", ""], ["K\u00f6pken", "Hans-Georg", ""], ["Tresp", "Volker", ""]]}, {"id": "2007.04154", "submitter": "David \\v{S}i\\v{s}ka", "authors": "Patryk Gierjatowicz and Marc Sabate-Vidales and David \\v{S}i\\v{s}ka\n  and Lukasz Szpruch and \\v{Z}an \\v{Z}uri\\v{c}", "title": "Robust pricing and hedging via neural SDEs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.MF cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical modelling is ubiquitous in the financial industry and drives key\ndecision processes. Any given model provides only a crude approximation to\nreality and the risk of using an inadequate model is hard to detect and\nquantify. By contrast, modern data science techniques are opening the door to\nmore robust and data-driven model selection mechanisms. However, most machine\nlearning models are \"black-boxes\" as individual parameters do not have\nmeaningful interpretation. The aim of this paper is to combine the above\napproaches achieving the best of both worlds. Combining neural networks with\nrisk models based on classical stochastic differential equations (SDEs), we\nfind robust bounds for prices of derivatives and the corresponding hedging\nstrategies while incorporating relevant market data. The resulting model called\nneural SDE is an instantiation of generative models and is closely linked with\nthe theory of causal optimal transport. Neural SDEs allow consistent\ncalibration under both the risk-neutral and the real-world measures. Thus the\nmodel can be used to simulate market scenarios needed for assessing risk\nprofiles and hedging strategies. We develop and analyse novel algorithms needed\nfor efficient use of neural SDEs. We validate our approach with numerical\nexperiments using both local and stochastic volatility models.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 14:33:17 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Gierjatowicz", "Patryk", ""], ["Sabate-Vidales", "Marc", ""], ["\u0160i\u0161ka", "David", ""], ["Szpruch", "Lukasz", ""], ["\u017duri\u010d", "\u017dan", ""]]}, {"id": "2007.04169", "submitter": "Jay Budzik", "authors": "Geoff Ward, Sean Kamkar, Jay Budzik", "title": "An exploration of the influence of path choice in game-theoretic\n  attribution algorithms", "comments": "21 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare machine learning explainability methods based on the theory of\natomic (Shapley, 1953) and infinitesimal (Aumann and Shapley, 1974) games, in a\ntheoretical and experimental investigation into how the model and choice of\nintegration path can influence the resulting feature attributions. To gain\ninsight into differences in attributions resulting from interventional Shapley\nvalues (Sundararajan and Najmi, 2019; Janzing et al., 2019; Chen et al., 2019)\nand Generalized Integrated Gradients (GIG) (Merrill et al., 2019) we note\ninterventional Shapley is equivalent to a multi-path integration along $n!$\npaths where $n$ is the number of model input features. Applying Stoke's theorem\nwe show that the path symmetry of these two methods results in the same\nattributions when the model is composed of a sum of separable functions of\nindividual features and a sum of two-feature products. We then perform a series\nof experiments with varying degrees of data missingness to demonstrate how\ninterventional Shapley's multi-path approach can yield less consistent\nattributions than the single straight-line path of Aumann-Shapley. We argue\nthis is because the multiple paths employed by interventional Shapley extend\naway from the training data manifold and are therefore more likely to pass\nthrough regions where the model has little support. In the absence of a more\nmeaningful path choice, we therefore advocate the straight-line path since it\nwill almost always pass closer to the data manifold. Among straight-line path\nattribution algorithms, GIG is uniquely robust since it will still yield\nShapley values for atomic games modeled by decision trees.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 14:57:28 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 19:33:47 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Ward", "Geoff", ""], ["Kamkar", "Sean", ""], ["Budzik", "Jay", ""]]}, {"id": "2007.04176", "submitter": "Jiun-Huei Proty Wu", "authors": "Yu-Chiung Lin, Jiun-Huei Proty Wu", "title": "Detection of Gravitational Waves Using Bayesian Neural Networks", "comments": "17 pages, 13 figures", "journal-ref": "Phys. Rev. D 103, 063034 (2021)", "doi": "10.1103/PhysRevD.103.063034", "report-no": null, "categories": "astro-ph.IM astro-ph.CO cs.LG physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new model of Bayesian Neural Networks to not only detect the\nevents of compact binary coalescence in the observational data of gravitational\nwaves (GW) but also identify the full length of the event duration including\nthe inspiral stage. This is achieved by incorporating the Bayesian approach\ninto the CLDNN classifier, which integrates together the Convolutional Neural\nNetwork (CNN) and the Long Short-Term Memory Recurrent Neural Network (LSTM).\nOur model successfully detect all seven BBH events in the LIGO Livingston O2\ndata, with the periods of their GW waveforms correctly labeled. The ability of\na Bayesian approach for uncertainty estimation enables a newly defined\n`awareness' state for recognizing the possible presence of signals of unknown\ntypes, which is otherwise rejected in a non-Bayesian model. Such data chunks\nlabeled with the awareness state can then be further investigated rather than\noverlooked. Performance tests with 40,960 training samples against 512 chunks\nof 8-second real noise mixed with mock signals of various optimal\nsignal-to-noise ratio $0 \\leq \\rho_\\text{opt} \\leq 18$ show that our model\nrecognizes 90% of the events when $\\rho_\\text{opt} >7$ (100% when\n$\\rho_\\text{opt} >8.5$) and successfully labels more than 95% of the waveform\nperiods when $\\rho_\\text{opt} >8$. The latency between the arrival of peak\nsignal and generating an alert with the associated waveform period labeled is\nonly about 20 seconds for an unoptimized code on a moderate GPU-equipped\npersonal computer. This makes our model possible for nearly real-time detection\nand for forecasting the coalescence events when assisted with deeper training\non a larger dataset using the state-of-art HPCs.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 15:07:08 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 01:09:15 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Lin", "Yu-Chiung", ""], ["Wu", "Jiun-Huei Proty", ""]]}, {"id": "2007.04202", "submitter": "Nicolas Loizou", "authors": "Nicolas Loizou, Hugo Berard, Alexia Jolicoeur-Martineau, Pascal\n  Vincent, Simon Lacoste-Julien, Ioannis Mitliagkas", "title": "Stochastic Hamiltonian Gradient Methods for Smooth Games", "comments": "ICML 2020 - Proceedings of the 37th International Conference on\n  Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of adversarial formulations in machine learning has brought\nrenewed motivation for smooth games. In this work, we focus on the class of\nstochastic Hamiltonian methods and provide the first convergence guarantees for\ncertain classes of stochastic smooth games. We propose a novel unbiased\nestimator for the stochastic Hamiltonian gradient descent (SHGD) and highlight\nits benefits. Using tools from the optimization literature we show that SHGD\nconverges linearly to the neighbourhood of a stationary point. To guarantee\nconvergence to the exact solution, we analyze SHGD with a decreasing step-size\nand we also present the first stochastic variance reduced Hamiltonian method.\nOur results provide the first global non-asymptotic last-iterate convergence\nguarantees for the class of stochastic unconstrained bilinear games and for the\nmore general class of stochastic games that satisfy a \"sufficiently bilinear\"\ncondition, notably including some non-convex non-concave problems. We\nsupplement our analysis with experiments on stochastic bilinear and\nsufficiently bilinear games, where our theory is shown to be tight, and on\nsimple adversarial machine learning formulations.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 15:42:13 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Loizou", "Nicolas", ""], ["Berard", "Hugo", ""], ["Jolicoeur-Martineau", "Alexia", ""], ["Vincent", "Pascal", ""], ["Lacoste-Julien", "Simon", ""], ["Mitliagkas", "Ioannis", ""]]}, {"id": "2007.04203", "submitter": "Thomas Spooner", "authors": "Thomas Spooner and Rahul Savani", "title": "A Natural Actor-Critic Algorithm with Downside Risk Constraints", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI q-fin.CP q-fin.PM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing work on risk-sensitive reinforcement learning - both for symmetric\nand downside risk measures - has typically used direct Monte-Carlo estimation\nof policy gradients. While this approach yields unbiased gradient estimates, it\nalso suffers from high variance and decreased sample efficiency compared to\ntemporal-difference methods. In this paper, we study prediction and control\nwith aversion to downside risk which we gauge by the lower partial moment of\nthe return. We introduce a new Bellman equation that upper bounds the lower\npartial moment, circumventing its non-linearity. We prove that this proxy for\nthe lower partial moment is a contraction, and provide intuition into the\nstability of the algorithm by variance decomposition. This allows\nsample-efficient, on-line estimation of partial moments. For risk-sensitive\ncontrol, we instantiate Reward Constrained Policy Optimization, a recent\nactor-critic method for finding constrained policies, with our proxy for the\nlower partial moment. We extend the method to use natural policy gradients and\ndemonstrate the effectiveness of our approach on three benchmark problems for\nrisk-sensitive reinforcement learning.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 15:44:33 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Spooner", "Thomas", ""], ["Savani", "Rahul", ""]]}, {"id": "2007.04205", "submitter": "Mar\\'ia Andrea Cruz Bland\\'on", "authors": "Mar\\'ia Andrea Cruz Bland\\'on and Okko R\\\"as\\\"anen", "title": "Analysis of Predictive Coding Models for Phonemic Representation\n  Learning in Small Datasets", "comments": "7 pages, 5 figures, 5 tables. Accepted paper at the workshop on\n  Self-supervision in Audio and Speech at ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural network models using predictive coding are interesting from the\nviewpoint of computational modelling of human language acquisition, where the\nobjective is to understand how linguistic units could be learned from speech\nwithout any labels. Even though several promising predictive coding -based\nlearning algorithms have been proposed in the literature, it is currently\nunclear how well they generalise to different languages and training dataset\nsizes. In addition, despite that such models have shown to be effective\nphonemic feature learners, it is unclear whether minimisation of the predictive\nloss functions of these models also leads to optimal phoneme-like\nrepresentations. The present study investigates the behaviour of two predictive\ncoding models, Autoregressive Predictive Coding and Contrastive Predictive\nCoding, in a phoneme discrimination task (ABX task) for two languages with\ndifferent dataset sizes. Our experiments show a strong correlation between the\nautoregressive loss and the phoneme discrimination scores with the two\ndatasets. However, to our surprise, the CPC model shows rapid convergence\nalready after one pass over the training data, and, on average, its\nrepresentations outperform those of APC on both languages.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 15:46:13 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Bland\u00f3n", "Mar\u00eda Andrea Cruz", ""], ["R\u00e4s\u00e4nen", "Okko", ""]]}, {"id": "2007.04206", "submitter": "Asa Cooper Stickland", "authors": "Asa Cooper Stickland and Iain Murray", "title": "Diverse Ensembles Improve Calibration", "comments": "Presented at the ICML 2020 Workshop on Uncertainty and Robustness in\n  Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern deep neural networks can produce badly calibrated predictions,\nespecially when train and test distributions are mismatched. Training an\nensemble of models and averaging their predictions can help alleviate these\nissues. We propose a simple technique to improve calibration, using a different\ndata augmentation for each ensemble member. We additionally use the idea of\n`mixing' un-augmented and augmented inputs to improve calibration when test and\ntraining distributions are the same. These simple techniques improve\ncalibration and accuracy over strong baselines on the CIFAR10 and CIFAR100\nbenchmarks, and out-of-domain data from their corrupted versions.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 15:48:12 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Stickland", "Asa Cooper", ""], ["Murray", "Iain", ""]]}, {"id": "2007.04212", "submitter": "Yuhuai(Tony) Wu", "authors": "Yuhuai Wu, Honghua Dong, Roger Grosse, Jimmy Ba", "title": "The Scattering Compositional Learner: Discovering Objects, Attributes,\n  Relationships in Analogical Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.LO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we focus on an analogical reasoning task that contains rich\ncompositional structures, Raven's Progressive Matrices (RPM). To discover\ncompositional structures of the data, we propose the Scattering Compositional\nLearner (SCL), an architecture that composes neural networks in a sequence. Our\nSCL achieves state-of-the-art performance on two RPM datasets, with a 48.7%\nrelative improvement on Balanced-RAVEN and 26.4% on PGM over the previous\nstate-of-the-art. We additionally show that our model discovers compositional\nrepresentations of objects' attributes (e.g., shape color, size), and their\nrelationships (e.g., progression, union). We also find that the compositional\nrepresentation makes the SCL significantly more robust to test-time domain\nshifts and greatly improves zero-shot generalization to previously unseen\nanalogies.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 15:53:06 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Wu", "Yuhuai", ""], ["Dong", "Honghua", ""], ["Grosse", "Roger", ""], ["Ba", "Jimmy", ""]]}, {"id": "2007.04214", "submitter": "Shaojie Tang", "authors": "Shaojie Tang", "title": "Linear-Time Algorithms for Adaptive Submodular Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop fast algorithms for two stochastic submodular\nmaximization problems. We start with the well-studied adaptive submodular\nmaximization problem subject to a cardinality constraint. We develop the first\nlinear-time algorithm which achieves a $(1-1/e-\\epsilon)$ approximation ratio.\nNotably, the time complexity of our algorithm is $O(n\\log\\frac{1}{\\epsilon})$\n(number of function evaluations) which is independent of the cardinality\nconstraint, where $n$ is the size of the ground set. Then we introduce the\nconcept of fully adaptive submodularity, and develop a linear-time algorithm\nfor maximizing a fully adaptive submoudular function subject to a partition\nmatroid constraint. We show that our algorithm achieves a\n$\\frac{1-1/e-\\epsilon}{4-2/e-2\\epsilon}$ approximation ratio using only\n$O(n\\log\\frac{1}{\\epsilon})$ number of function evaluations.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 15:54:28 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Tang", "Shaojie", ""]]}, {"id": "2007.04216", "submitter": "Samuel Glass", "authors": "Samuel Glass, Simeon Spasov, Pietro Li\\`o", "title": "RicciNets: Curvature-guided Pruning of High-performance Neural Networks\n  Using Ricci Flow", "comments": "To appear at ICML 2020, AutoML Workshop. Contains 11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel method to identify salient computational paths within randomly wired\nneural networks before training is proposed. The computational graph is pruned\nbased on a node mass probability function defined by local graph measures and\nweighted by hyperparameters produced by a reinforcement learning-based\ncontroller neural network. We use the definition of Ricci curvature to remove\nedges of low importance before mapping the computational graph to a neural\nnetwork. We show a reduction of almost $35\\%$ in the number of floating-point\noperations (FLOPs) per pass, with no degradation in performance. Further, our\nmethod can successfully regularize randomly wired neural networks based on\npurely structural properties, and also find that the favourable characteristics\nidentified in one network generalise to other networks. The method produces\nnetworks with better performance under similar compression to those pruned by\nlowest-magnitude weights. To our best knowledge, this is the first work on\npruning randomly wired neural networks, as well as the first to utilize the\ntopological measure of Ricci curvature in the pruning mechanism.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 15:56:02 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Glass", "Samuel", ""], ["Spasov", "Simeon", ""], ["Li\u00f2", "Pietro", ""]]}, {"id": "2007.04234", "submitter": "Xingyi Yang", "authors": "Xingyi Yang, Xuehai He, Yuxiao Liang, Yue Yang, Shanghang Zhang,\n  Pengtao Xie", "title": "Transfer Learning or Self-supervised Learning? A Tale of Two Pretraining\n  Paradigms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretraining has become a standard technique in computer vision and natural\nlanguage processing, which usually helps to improve performance substantially.\nPreviously, the most dominant pretraining method is transfer learning (TL),\nwhich uses labeled data to learn a good representation network. Recently, a new\npretraining approach -- self-supervised learning (SSL) -- has demonstrated\npromising results on a wide range of applications. SSL does not require\nannotated labels. It is purely conducted on input data by solving auxiliary\ntasks defined on the input data examples. The current reported results show\nthat in certain applications, SSL outperforms TL and the other way around in\nother applications. There has not been a clear understanding on what properties\nof data and tasks render one approach outperforms the other. Without an\ninformed guideline, ML researchers have to try both methods to find out which\none is better empirically. It is usually time-consuming to do so. In this work,\nwe aim to address this problem. We perform a comprehensive comparative study\nbetween SSL and TL regarding which one works better under different properties\nof data and tasks, including domain difference between source and target tasks,\nthe amount of pretraining data, class imbalance in source data, and usage of\ntarget data for additional pretraining, etc. The insights distilled from our\ncomparative studies can help ML researchers decide which method to use based on\nthe properties of their applications.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 05:21:00 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Yang", "Xingyi", ""], ["He", "Xuehai", ""], ["Liang", "Yuxiao", ""], ["Yang", "Yue", ""], ["Zhang", "Shanghang", ""], ["Xie", "Pengtao", ""]]}, {"id": "2007.04238", "submitter": "Myriam Bontonou", "authors": "Myriam Bontonou, Louis B\\'ethune, Vincent Gripon", "title": "Predicting the Accuracy of a Few-Shot Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of few-shot learning, one cannot measure the generalization\nability of a trained classifier using validation sets, due to the small number\nof labeled samples. In this paper, we are interested in finding alternatives to\nanswer the question: is my classifier generalizing well to previously unseen\ndata? We first analyze the reasons for the variability of generalization\nperformances. We then investigate the case of using transfer-based solutions,\nand consider three settings: i) supervised where we only have access to a few\nlabeled samples, ii) semi-supervised where we have access to both a few labeled\nsamples and a set of unlabeled samples and iii) unsupervised where we only have\naccess to unlabeled samples. For each setting, we propose reasonable measures\nthat we empirically demonstrate to be correlated with the generalization\nability of considered classifiers. We also show that these simple measures can\nbe used to predict generalization up to a certain confidence. We conduct our\nexperiments on standard few-shot vision datasets.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 16:31:28 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Bontonou", "Myriam", ""], ["B\u00e9thune", "Louis", ""], ["Gripon", "Vincent", ""]]}, {"id": "2007.04239", "submitter": "Zaid Alyafeai Mr", "authors": "Zaid Alyafeai, Maged Saeed AlShaibani, Irfan Ahmad", "title": "A Survey on Transfer Learning in Natural Language Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models usually require a huge amount of data. However, these\nlarge datasets are not always attainable. This is common in many challenging\nNLP tasks. Consider Neural Machine Translation, for instance, where curating\nsuch large datasets may not be possible specially for low resource languages.\nAnother limitation of deep learning models is the demand for huge computing\nresources. These obstacles motivate research to question the possibility of\nknowledge transfer using large trained models. The demand for transfer learning\nis increasing as many large models are emerging. In this survey, we feature the\nrecent transfer learning advances in the field of NLP. We also provide a\ntaxonomy for categorizing different transfer learning approaches from the\nliterature.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 21:52:31 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Alyafeai", "Zaid", ""], ["AlShaibani", "Maged Saeed", ""], ["Ahmad", "Irfan", ""]]}, {"id": "2007.04249", "submitter": "Subramaniam Kazhuparambil Mr.", "authors": "Subramaniam Kazhuparambil (1) and Abhishek Kaushik (1 and 2) ((1)\n  Dublin Business School, (2) Dublin City University)", "title": "Cooking Is All About People: Comment Classification On Cookery Channels\n  Using BERT and Classification Models (Malayalam-English Mix-Code)", "comments": "Rectified typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The scope of a lucrative career promoted by Google through its video\ndistribution platform YouTube has attracted a large number of users to become\ncontent creators. An important aspect of this line of work is the feedback\nreceived in the form of comments which show how well the content is being\nreceived by the audience. However, volume of comments coupled with spam and\nlimited tools for comment classification makes it virtually impossible for a\ncreator to go through each and every comment and gather constructive feedback.\nAutomatic classification of comments is a challenge even for established\nclassification models, since comments are often of variable lengths riddled\nwith slang, symbols and abbreviations. This is a greater challenge where\ncomments are multilingual as the messages are often rife with the respective\nvernacular. In this work, we have evaluated top-performing classification\nmodels for classifying comments which are a mix of different combinations of\nEnglish and Malayalam (only English, only Malayalam and Mix of English and\nMalayalam). The statistical analysis of results indicates that Multinomial\nNaive Bayes, K-Nearest Neighbors (KNN), Support Vector Machine (SVM), Random\nForest and Decision Trees offer similar level of accuracy in comment\nclassification. Further, we have also evaluated 3 multilingual transformer\nbased language models (BERT, DISTILBERT and XLM) and compared their performance\nto the traditional machine learning classification techniques. XLM was the\ntop-performing BERT model with an accuracy of 67.31. Random Forest with Term\nFrequency Vectorizer was the best performing model out of all the traditional\nclassification models with an accuracy of 63.59.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 19:07:06 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 12:57:24 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 08:40:09 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Kazhuparambil", "Subramaniam", "", "1 and 2"], ["Kaushik", "Abhishek", "", "1 and 2"]]}, {"id": "2007.04250", "submitter": "Tianshi Cao", "authors": "Tianshi Cao, Chin-Wei Huang, David Yu-Tung Hui, Joseph Paul Cohen", "title": "A Benchmark of Medical Out of Distribution Detection", "comments": "Submitted to Machine Learning for Biomedical Imaging Journal (MELBA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Deep learning models deployed for use on medical tasks can be\nequipped with Out-of-Distribution Detection (OoDD) methods in order to avoid\nerroneous predictions. However it is unclear which OoDD method should be used\nin practice. Specific Problem: Systems trained for one particular domain of\nimages cannot be expected to perform accurately on images of a different\ndomain. These images should be flagged by an OoDD method prior to diagnosis.\nOur approach: This paper defines 3 categories of OoD examples and benchmarks\npopular OoDD methods in three domains of medical imaging: chest X-ray, fundus\nimaging, and histology slides. Results: Our experiments show that despite\nmethods yielding good results on some categories of out-of-distribution\nsamples, they fail to recognize images close to the training distribution.\nConclusion: We find a simple binary classifier on the feature representation\nhas the best accuracy and AUPRC on average. Users of diagnostic tools which\nemploy these OoDD methods should still remain vigilant that images very close\nto the training distribution yet not in it could yield unexpected results.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 16:39:34 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 02:05:43 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Cao", "Tianshi", ""], ["Huang", "Chin-Wei", ""], ["Hui", "David Yu-Tung", ""], ["Cohen", "Joseph Paul", ""]]}, {"id": "2007.04275", "submitter": "Michael Maser", "authors": "Serim Ryou, Michael R. Maser, Alexander Y. Cui, Travis J. DeLano,\n  Yisong Yue, Sarah E. Reisman", "title": "Graph Neural Networks for the Prediction of Substrate-Specific Organic\n  Reaction Conditions", "comments": "23 pages, 10 tables, 13 figures, to appear in the ICML 2020 Workshop\n  on Graph Representation Learning and Beyond (GRLB)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a systematic investigation using graph neural networks (GNNs) to\nmodel organic chemical reactions. To do so, we prepared a dataset collection of\nfour ubiquitous reactions from the organic chemistry literature. We evaluate\nseven different GNN architectures for classification tasks pertaining to the\nidentification of experimental reagents and conditions. We find that models are\nable to identify specific graph features that affect reaction conditions and\nlead to accurate predictions. The results herein show great promise in\nadvancing molecular machine learning.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 17:21:00 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 13:03:34 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Ryou", "Serim", ""], ["Maser", "Michael R.", ""], ["Cui", "Alexander Y.", ""], ["DeLano", "Travis J.", ""], ["Yue", "Yisong", ""], ["Reisman", "Sarah E.", ""]]}, {"id": "2007.04285", "submitter": "Gang Li", "authors": "Gang Li, Jan Hannig", "title": "Deep Fiducial Inference", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the mid-2000s, there has been a resurrection of interest in modern\nmodifications of fiducial inference. To date, the main computational tool to\nextract a generalized fiducial distribution is Markov chain Monte Carlo (MCMC).\nWe propose an alternative way of computing a generalized fiducial distribution\nthat could be used in complex situations. In particular, to overcome the\ndifficulty when the unnormalized fiducial density (needed for MCMC), we design\na fiducial autoencoder (FAE). The fitted autoencoder is used to generate\ngeneralized fiducial samples of the unknown parameters. To increase accuracy,\nwe then apply an approximate fiducial computation (AFC) algorithm, by rejecting\nsamples that when plugged into a decoder do not replicate the observed data\nwell enough. Our numerical experiments show the effectiveness of our FAE-based\ninverse solution and the excellent coverage performance of the AFC corrected\nFAE solution.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 17:33:14 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Li", "Gang", ""], ["Hannig", "Jan", ""]]}, {"id": "2007.04287", "submitter": "R\\'emi Bardenet", "authors": "R\\'emi Bardenet and Subhroshekhar Ghosh", "title": "Learning from DPPs via Sampling: Beyond HKPV and symmetry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal point processes (DPPs) have become a significant tool for\nrecommendation systems, feature selection, or summary extraction, harnessing\nthe intrinsic ability of these probabilistic models to facilitate sample\ndiversity. The ability to sample from DPPs is paramount to the empirical\ninvestigation of these models. Most exact samplers are variants of a spectral\nmeta-algorithm due to Hough, Krishnapur, Peres and Vir\\'ag (henceforth HKPV),\nwhich is in general time and resource intensive. For DPPs with symmetric\nkernels, scalable HKPV samplers have been proposed that either first downsample\nthe ground set of items, or force the kernel to be low-rank, using e.g.\nNystr\\\"om-type decompositions.\n  In the present work, we contribute a radically different approach than HKPV.\nExploiting the fact that many statistical and learning objectives can be\neffectively accomplished by only sampling certain key observables of a DPP\n(so-called linear statistics), we invoke an expression for the Laplace\ntransform of such an observable as a single determinant, which holds in\ncomplete generality. Combining traditional low-rank approximation techniques\nwith Laplace inversion algorithms from numerical analysis, we show how to\ndirectly approximate the distribution function of a linear statistic of a DPP.\nThis distribution function can then be used in hypothesis testing or to\nactually sample the linear statistic, as per requirement. Our approach is\nscalable and applies to very general DPPs, beyond traditional symmetric\nkernels.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 17:33:45 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Bardenet", "R\u00e9mi", ""], ["Ghosh", "Subhroshekhar", ""]]}, {"id": "2007.04309", "submitter": "Nicklas Hansen", "authors": "Nicklas Hansen, Rishabh Jangir, Yu Sun, Guillem Aleny\\`a, Pieter\n  Abbeel, Alexei A. Efros, Lerrel Pinto, Xiaolong Wang", "title": "Self-Supervised Policy Adaptation during Deployment", "comments": "Website: https://nicklashansen.github.io/PAD/ Code:\n  https://github.com/nicklashansen/policy-adaptation-during-deployment ICLR\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most real world scenarios, a policy trained by reinforcement learning in\none environment needs to be deployed in another, potentially quite different\nenvironment. However, generalization across different environments is known to\nbe hard. A natural solution would be to keep training after deployment in the\nnew environment, but this cannot be done if the new environment offers no\nreward signal. Our work explores the use of self-supervision to allow the\npolicy to continue training after deployment without using any rewards. While\nprevious methods explicitly anticipate changes in the new environment, we\nassume no prior knowledge of those changes yet still obtain significant\nimprovements. Empirical evaluations are performed on diverse simulation\nenvironments from DeepMind Control suite and ViZDoom, as well as real robotic\nmanipulation tasks in continuously changing environments, taking observations\nfrom an uncalibrated camera. Our method improves generalization in 31 out of 36\nenvironments across various tasks and outperforms domain randomization on a\nmajority of environments.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 17:56:27 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 19:01:06 GMT"}, {"version": "v3", "created": "Fri, 9 Apr 2021 02:47:39 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Hansen", "Nicklas", ""], ["Jangir", "Rishabh", ""], ["Sun", "Yu", ""], ["Aleny\u00e0", "Guillem", ""], ["Abbeel", "Pieter", ""], ["Efros", "Alexei A.", ""], ["Pinto", "Lerrel", ""], ["Wang", "Xiaolong", ""]]}, {"id": "2007.04393", "submitter": "Edgar Minasyan", "authors": "Paula Gradu, Elad Hazan, Edgar Minasyan", "title": "Adaptive Regret for Control of Time-Varying Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider regret minimization for online control with time-varying linear\ndynamical systems. The metric of performance we study is adaptive policy\nregret, or regret compared to the best policy on {\\it any interval in time}. We\ngive an efficient algorithm that attains first-order adaptive regret guarantees\nfor the setting of online convex optimization with memory. We also show that\nthese first-order bounds are nearly tight. This algorithm is then used to\nderive a controller with adaptive regret guarantees that provably competes with\nthe best linear dynamical controller on any interval in time. We validate these\ntheoretical findings experimentally on (1) simulations of time-varying linear\ndynamics and disturbances, and (2) the non-linear inverted pendulum benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 19:40:34 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 11:51:36 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Gradu", "Paula", ""], ["Hazan", "Elad", ""], ["Minasyan", "Edgar", ""]]}, {"id": "2007.04395", "submitter": "Xiang Ling", "authors": "Xiang Ling, Lingfei Wu, Saizhuo Wang, Tengfei Ma, Fangli Xu, Alex X.\n  Liu, Chunming Wu, Shouling Ji", "title": "Multi-Level Graph Matching Networks for Deep Graph Similarity Learning", "comments": "14 pages, rename names & fix typos, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the celebrated graph neural networks yield effective representations\nfor individual nodes of a graph, there has been relatively less success in\nextending to the task of graph similarity learning. Recent work on graph\nsimilarity learning has considered either global-level graph-graph interactions\nor low-level node-node interactions, ignoring the rich cross-level interactions\n(e.g., between nodes of a graph and the other whole graph). In this paper, we\npropose a Multi-Level Graph Matching Network (MGMN) framework for computing the\ngraph similarity between any pair of graph-structured objects in an end-to-end\nfashion. The proposed model MGMN consists of a node-graph matching network for\neffectively learning cross-level interactions between nodes of a graph and the\nother whole graph, and a siamese graph neural network to learn global-level\ninteractions between two input graphs. Furthermore, to bridge the gap of the\nlack of standard graph similarity learning benchmarks, we have created and\ncollected a set of datasets for both the graph-graph classification and\ngraph-graph regression tasks with different sizes in order to evaluate the\neffectiveness and robustness of our models. Comprehensive experiments\ndemonstrate that the proposed model MGMN consistently outperforms\nstate-of-the-art baseline models one both the graph-graph classification and\ngraph-graph regression tasks. Compared with previous work, MGMN also exhibits\nstronger robustness as the sizes of the two input graphs increase.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 19:48:19 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 06:46:11 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Ling", "Xiang", ""], ["Wu", "Lingfei", ""], ["Wang", "Saizhuo", ""], ["Ma", "Tengfei", ""], ["Xu", "Fangli", ""], ["Liu", "Alex X.", ""], ["Wu", "Chunming", ""], ["Ji", "Shouling", ""]]}, {"id": "2007.04410", "submitter": "Aditi Shenvi", "authors": "F.O.Bunnin, A.Shenvi, J.Q.Smith", "title": "Network Modelling of Criminal Collaborations with Dynamic Bayesian\n  Steady Evolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The threat status and criminal collaborations of potential terrorists are\nhidden but give rise to observable behaviours and communications. Terrorists,\nwhen acting in concert, need to communicate to organise their plots. The\nauthorities utilise such observable behaviour and communication data to inform\ntheir investigations and policing. We present a dynamic latent network model\nthat integrates real-time communications data with prior knowledge on\nindividuals. This model estimates and predicts the latent strength of criminal\ncollaboration between individuals to assist in the identification of potential\ncells and the measurement of their threat levels. We demonstrate how, by\nassuming certain plausible conditional independences across the measurements\nassociated with this population, the network model can be combined with models\nof individual suspects to provide fast transparent algorithms to predict group\nattacks. The methods are illustrated using a simulated example involving the\nthreat posed by a cell suspected of plotting an attack.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 20:23:25 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Bunnin", "F. O.", ""], ["Shenvi", "A.", ""], ["Smith", "J. Q.", ""]]}, {"id": "2007.04432", "submitter": "Aditya Mate", "authors": "Aditya Mate, Jackson A. Killian, Haifeng Xu, Andrew Perrault, Milind\n  Tambe", "title": "Collapsing Bandits and Their Application to Public Health Interventions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and study Collpasing Bandits, a new restless multi-armed bandit\n(RMAB) setting in which each arm follows a binary-state Markovian process with\na special structure: when an arm is played, the state is fully observed, thus\n\"collapsing\" any uncertainty, but when an arm is passive, no observation is\nmade, thus allowing uncertainty to evolve. The goal is to keep as many arms in\nthe \"good\" state as possible by planning a limited budget of actions per round.\nSuch Collapsing Bandits are natural models for many healthcare domains in which\nworkers must simultaneously monitor patients and deliver interventions in a way\nthat maximizes the health of their patient cohort. Our main contributions are\nas follows: (i) Building on the Whittle index technique for RMABs, we derive\nconditions under which the Collapsing Bandits problem is indexable. Our\nderivation hinges on novel conditions that characterize when the optimal\npolicies may take the form of either \"forward\" or \"reverse\" threshold policies.\n(ii) We exploit the optimality of threshold policies to build fast algorithms\nfor computing the Whittle index, including a closed-form. (iii) We evaluate our\nalgorithm on several data distributions including data from a real-world\nhealthcare task in which a worker must monitor and deliver interventions to\nmaximize their patients' adherence to tuberculosis medication. Our algorithm\nachieves a 3-order-of-magnitude speedup compared to state-of-the-art RMAB\ntechniques while achieving similar performance.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 00:33:30 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Mate", "Aditya", ""], ["Killian", "Jackson A.", ""], ["Xu", "Haifeng", ""], ["Perrault", "Andrew", ""], ["Tambe", "Milind", ""]]}, {"id": "2007.04439", "submitter": "Filipe de Avila Belbute-Peres", "authors": "Filipe de Avila Belbute-Peres, Thomas D. Economon, J. Zico Kolter", "title": "Combining Differentiable PDE Solvers and Graph Neural Networks for Fluid\n  Flow Prediction", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving large complex partial differential equations (PDEs), such as those\nthat arise in computational fluid dynamics (CFD), is a computationally\nexpensive process. This has motivated the use of deep learning approaches to\napproximate the PDE solutions, yet the simulation results predicted from these\napproaches typically do not generalize well to truly novel scenarios. In this\nwork, we develop a hybrid (graph) neural network that combines a traditional\ngraph convolutional network with an embedded differentiable fluid dynamics\nsimulator inside the network itself. By combining an actual CFD simulator (run\non a much coarser resolution representation of the problem) with the graph\nnetwork, we show that we can both generalize well to new situations and benefit\nfrom the substantial speedup of neural network CFD predictions, while also\nsubstantially outperforming the coarse CFD simulation alone.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 21:23:19 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2020 19:28:05 GMT"}, {"version": "v3", "created": "Sun, 16 Aug 2020 16:32:02 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Belbute-Peres", "Filipe de Avila", ""], ["Economon", "Thomas D.", ""], ["Kolter", "J. Zico", ""]]}, {"id": "2007.04440", "submitter": "Matthew Leavitt", "authors": "Matthew L. Leavitt, Ari S. Morcos", "title": "On the relationship between class selectivity, dimensionality, and\n  robustness", "comments": "Presented at the ICML 2020 Workshop on Uncertainty and Robustness in\n  Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the relative trade-offs between sparse and distributed representations\nin deep neural networks (DNNs) are well-studied, less is known about how these\ntrade-offs apply to representations of semantically-meaningful information.\nClass selectivity, the variability of a unit's responses across data classes or\ndimensions, is one way of quantifying the sparsity of semantic representations.\nGiven recent evidence showing that class selectivity can impair generalization,\nwe sought to investigate whether it also confers robustness (or vulnerability)\nto perturbations of input data. We found that mean class selectivity predicts\nvulnerability to naturalistic corruptions; networks regularized to have lower\nlevels of class selectivity are more robust to corruption, while networks with\nhigher class selectivity are more vulnerable to corruption, as measured using\nTiny ImageNetC and CIFAR10C. In contrast, we found that class selectivity\nincreases robustness to multiple types of gradient-based adversarial attacks.\nTo examine this difference, we studied the dimensionality of the change in the\nrepresentation due to perturbation, finding that decreasing class selectivity\nincreases the dimensionality of this change for both corruption types, but with\na notably larger increase for adversarial attacks. These results demonstrate\nthe causal relationship between selectivity and robustness and provide new\ninsights into the mechanisms of this relationship.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 21:24:45 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 22:35:42 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Leavitt", "Matthew L.", ""], ["Morcos", "Ari S.", ""]]}, {"id": "2007.04446", "submitter": "Brian Lucena", "authors": "Brian Lucena", "title": "StructureBoost: Efficient Gradient Boosting for Structured Categorical\n  Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient boosting methods based on Structured Categorical Decision Trees\n(SCDT) have been demonstrated to outperform numerical and one-hot-encodings on\nproblems where the categorical variable has a known underlying structure.\nHowever, the enumeration procedure in the SCDT is infeasible except for\ncategorical variables with low or moderate cardinality. We propose and\nimplement two methods to overcome the computational obstacles and efficiently\nperform Gradient Boosting on complex structured categorical variables. The\nresulting package, called StructureBoost, is shown to outperform established\npackages such as CatBoost and LightGBM on problems with categorical predictors\nthat contain sophisticated structure. Moreover, we demonstrate that\nStructureBoost can make accurate predictions on unseen categorical values due\nto its knowledge of the underlying structure.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 21:37:15 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Lucena", "Brian", ""]]}, {"id": "2007.04451", "submitter": "Marek Wydmuch", "authors": "Kalina Jasinska-Kobus, Marek Wydmuch, Devanathan Thiruvenkatachari,\n  Krzysztof Dembczy\\'nski", "title": "Online probabilistic label trees", "comments": "Accepted at AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce online probabilistic label trees (OPLTs), an algorithm that\ntrains a label tree classifier in a fully online manner without any prior\nknowledge about the number of training instances, their features and labels.\nOPLTs are characterized by low time and space complexity as well as strong\ntheoretical guarantees. They can be used for online multi-label and multi-class\nclassification, including the very challenging scenarios of one- or few-shot\nlearning. We demonstrate the attractiveness of OPLTs in a wide empirical study\non several instances of the tasks mentioned above.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 21:45:00 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 14:50:58 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Jasinska-Kobus", "Kalina", ""], ["Wydmuch", "Marek", ""], ["Thiruvenkatachari", "Devanathan", ""], ["Dembczy\u0144ski", "Krzysztof", ""]]}, {"id": "2007.04458", "submitter": "Viet Anh Nguyen", "authors": "Viet Anh Nguyen and Nian Si and Jose Blanchet", "title": "Robust Bayesian Classification Using an Optimistic Score Ratio", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We build a Bayesian contextual classification model using an optimistic score\nratio for robust binary classification when there is limited information on the\nclass-conditional, or contextual, distribution. The optimistic score searches\nfor the distribution that is most plausible to explain the observed outcomes in\nthe testing sample among all distributions belonging to the contextual\nambiguity set which is prescribed using a limited structural constraint on the\nmean vector and the covariance matrix of the underlying contextual\ndistribution. We show that the Bayesian classifier using the optimistic score\nratio is conceptually attractive, delivers solid statistical guarantees and is\ncomputationally tractable. We showcase the power of the proposed optimistic\nscore ratio classifier on both synthetic and empirical data.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 22:25:29 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Nguyen", "Viet Anh", ""], ["Si", "Nian", ""], ["Blanchet", "Jose", ""]]}, {"id": "2007.04459", "submitter": "Gabriella Contardo", "authors": "Ademola Oladosu, Tony Xu, Philip Ekfeldt, Brian A. Kelly, Miles\n  Cranmer, Shirley Ho, Adrian M. Price-Whelan, Gabriella Contardo", "title": "Meta-Learning for One-Class Classification with Few Examples using\n  Order-Equivariant Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG astro-ph.GA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a meta-learning framework for few-shots One-Class\nClassification (OCC) at test-time, a setting where labeled examples are only\navailable for the positive class, and no supervision is given for the negative\nexample. We consider that we have a set of `one-class classification'\nobjective-tasks with only a small set of positive examples available for each\ntask, and a set of training tasks with full supervision (i.e. highly imbalanced\nclassification). We propose an approach using order-equivariant networks to\nlearn a 'meta' binary-classifier. The model will take as input an example to\nclassify from a given task, as well as the corresponding supervised set of\npositive examples for this OCC task. Thus, the output of the model will be\n'conditioned' on the available positive example of a given task, allowing to\npredict on new tasks and new examples without labeled negative examples. In\nthis paper, we are motivated by an astronomy application. Our goal is to\nidentify if stars belong to a specific stellar group (the 'one-class' for a\ngiven task), called \\textit{stellar streams}, where each stellar stream is a\ndifferent OCC-task. We show that our method transfers well on unseen (test)\nsynthetic streams, and outperforms the baselines even though it is not\nretrained and accesses a much smaller part of the data per task to predict\n(only positive supervision). We see however that it doesn't transfer as well on\nthe real stream GD-1. This could come from intrinsic differences from the\nsynthetic and real stream, highlighting the need for consistency in the\n'nature' of the task for this method. However, light fine-tuning improve\nperformances and outperform our baselines. Our experiments show encouraging\nresults to further explore meta-learning methods for OCC tasks.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 22:33:09 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 18:58:18 GMT"}, {"version": "v3", "created": "Fri, 21 May 2021 20:05:24 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Oladosu", "Ademola", ""], ["Xu", "Tony", ""], ["Ekfeldt", "Philip", ""], ["Kelly", "Brian A.", ""], ["Cranmer", "Miles", ""], ["Ho", "Shirley", ""], ["Price-Whelan", "Adrian M.", ""], ["Contardo", "Gabriella", ""]]}, {"id": "2007.04462", "submitter": "Yongxin Chen", "authors": "Jiaojiao Fan, Amirhossein Taghvaei, Yongxin Chen", "title": "Scalable Computations of Wasserstein Barycenter via Input Convex Neural\n  Networks", "comments": "20 pages,19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wasserstein Barycenter is a principled approach to represent the weighted\nmean of a given set of probability distributions, utilizing the geometry\ninduced by optimal transport. In this work, we present a novel scalable\nalgorithm to approximate the Wasserstein Barycenters aiming at high-dimensional\napplications in machine learning. Our proposed algorithm is based on the\nKantorovich dual formulation of the Wasserstein-2 distance as well as a recent\nneural network architecture, input convex neural network, that is known to\nparametrize convex functions. The distinguishing features of our method are: i)\nit only requires samples from the marginal distributions; ii) unlike the\nexisting approaches, it represents the Barycenter with a generative model and\ncan thus generate infinite samples from the barycenter without querying the\nmarginal distributions; iii) it works similar to Generative Adversarial Model\nin one marginal case. We demonstrate the efficacy of our algorithm by comparing\nit with the state-of-art methods in multiple experiments.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 22:41:18 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 18:21:47 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Fan", "Jiaojiao", ""], ["Taghvaei", "Amirhossein", ""], ["Chen", "Yongxin", ""]]}, {"id": "2007.04466", "submitter": "Meet Vadera", "authors": "Meet P. Vadera, Adam D. Cobb, Brian Jalaian, Benjamin M. Marlin", "title": "URSABench: Comprehensive Benchmarking of Approximate Bayesian Inference\n  Methods for Deep Neural Networks", "comments": "Presented at the ICML 2020 Workshop on Uncertainty and Robustness in\n  Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning methods continue to improve in predictive accuracy on a\nwide range of application domains, significant issues remain with other aspects\nof their performance including their ability to quantify uncertainty and their\nrobustness. Recent advances in approximate Bayesian inference hold significant\npromise for addressing these concerns, but the computational scalability of\nthese methods can be problematic when applied to large-scale models. In this\npaper, we describe initial work on the development ofURSABench(the Uncertainty,\nRobustness, Scalability, and Accu-racy Benchmark), an open-source suite of\nbench-marking tools for comprehensive assessment of approximate Bayesian\ninference methods with a focus on deep learning-based classification tasks\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 22:51:28 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Vadera", "Meet P.", ""], ["Cobb", "Adam D.", ""], ["Jalaian", "Brian", ""], ["Marlin", "Benjamin M.", ""]]}, {"id": "2007.04470", "submitter": "Diana Cai", "authors": "Diana Cai, Trevor Campbell, Tamara Broderick", "title": "Finite mixture models do not reliably learn the number of components", "comments": "Proceedings of the 38th International Conference on Machine Learning\n  (ICML), to appear. 25 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientists and engineers are often interested in learning the number of\nsubpopulations (or components) present in a data set. A common suggestion is to\nuse a finite mixture model (FMM) with a prior on the number of components. Past\nwork has shown the resulting FMM component-count posterior is consistent; that\nis, the posterior concentrates on the true, generating number of components.\nBut consistency requires the assumption that the component likelihoods are\nperfectly specified, which is unrealistic in practice. In this paper, we add\nrigor to data-analysis folk wisdom by proving that under even the slightest\nmodel misspecification, the FMM component-count posterior diverges: the\nposterior probability of any particular finite number of components converges\nto 0 in the limit of infinite data. Contrary to intuition, posterior-density\nconsistency is not sufficient to establish this result. We develop novel\nsufficient conditions that are more realistic and easily checkable than those\ncommon in the asymptotics literature. We illustrate practical consequences of\nour theory on simulated and real data.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 23:05:18 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 16:12:03 GMT"}, {"version": "v3", "created": "Wed, 7 Jul 2021 15:38:13 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Cai", "Diana", ""], ["Campbell", "Trevor", ""], ["Broderick", "Tamara", ""]]}, {"id": "2007.04472", "submitter": "Rana Abou Khamis", "authors": "Rana Abou Khamis and Ashraf Matrawy", "title": "Evaluation of Adversarial Training on Different Types of Neural Networks\n  in Deep Learning-based IDSs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network security applications, including intrusion detection systems of deep\nneural networks, are increasing rapidly to make detection task of anomaly\nactivities more accurate and robust. With the rapid increase of using DNN and\nthe volume of data traveling through systems, different growing types of\nadversarial attacks to defeat them create a severe challenge. In this paper, we\nfocus on investigating the effectiveness of different evasion attacks and how\nto train a resilience deep learning-based IDS using different Neural networks,\ne.g., convolutional neural networks (CNN) and recurrent neural networks (RNN).\nWe use the min-max approach to formulate the problem of training robust IDS\nagainst adversarial examples using two benchmark datasets. Our experiments on\ndifferent deep learning algorithms and different benchmark datasets demonstrate\nthat defense using an adversarial training-based min-max approach improves the\nrobustness against the five well-known adversarial attack methods.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 23:33:30 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Khamis", "Rana Abou", ""], ["Matrawy", "Ashraf", ""]]}, {"id": "2007.04480", "submitter": "Richard Droste", "authors": "Richard Droste, Lior Drukker, Aris T. Papageorghiou, J. Alison Noble", "title": "Automatic Probe Movement Guidance for Freehand Obstetric Ultrasound", "comments": "Accepted at the 23rd International Conference on Medical Image\n  Computing and Computer Assisted Intervention (MICCAI 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first system that provides real-time probe movement guidance\nfor acquiring standard planes in routine freehand obstetric ultrasound\nscanning. Such a system can contribute to the worldwide deployment of obstetric\nultrasound scanning by lowering the required level of operator expertise. The\nsystem employs an artificial neural network that receives the ultrasound video\nsignal and the motion signal of an inertial measurement unit (IMU) that is\nattached to the probe, and predicts a guidance signal. The network termed\nUS-GuideNet predicts either the movement towards the standard plane position\n(goal prediction), or the next movement that an expert sonographer would\nperform (action prediction). While existing models for other ultrasound\napplications are trained with simulations or phantoms, we train our model with\nreal-world ultrasound video and probe motion data from 464 routine clinical\nscans by 17 accredited sonographers. Evaluations for 3 standard plane types\nshow that the model provides a useful guidance signal with an accuracy of 88.8%\nfor goal prediction and 90.9% for action prediction.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 23:58:41 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Droste", "Richard", ""], ["Drukker", "Lior", ""], ["Papageorghiou", "Aris T.", ""], ["Noble", "J. Alison", ""]]}, {"id": "2007.04484", "submitter": "Aria Shahverdi", "authors": "Mingliang Chen, Aria Shahverdi, Sarah Anderson, Se Yong Park, Justin\n  Zhang, Dana Dachman-Soled, Kristin Lauter, Min Wu", "title": "Transparency Tools for Fairness in AI (Luskin)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose new tools for policy-makers to use when assessing and correcting\nfairness and bias in AI algorithms. The three tools are:\n  - A new definition of fairness called \"controlled fairness\" with respect to\nchoices of protected features and filters. The definition provides a simple\ntest of fairness of an algorithm with respect to a dataset. This notion of\nfairness is suitable in cases where fairness is prioritized over accuracy, such\nas in cases where there is no \"ground truth\" data, only data labeled with past\ndecisions (which may have been biased).\n  - Algorithms for retraining a given classifier to achieve \"controlled\nfairness\" with respect to a choice of features and filters. Two algorithms are\npresented, implemented and tested. These algorithms require training two\ndifferent models in two stages. We experiment with combinations of various\ntypes of models for the first and second stage and report on which combinations\nperform best in terms of fairness and accuracy.\n  - Algorithms for adjusting model parameters to achieve a notion of fairness\ncalled \"classification parity\". This notion of fairness is suitable in cases\nwhere accuracy is prioritized. Two algorithms are presented, one which assumes\nthat protected features are accessible to the model during testing, and one\nwhich assumes protected features are not accessible during testing.\n  We evaluate our tools on three different publicly available datasets. We find\nthat the tools are useful for understanding various dimensions of bias, and\nthat in practice the algorithms are effective in starkly reducing a given\nobserved bias when tested on new data.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 00:21:54 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Chen", "Mingliang", ""], ["Shahverdi", "Aria", ""], ["Anderson", "Sarah", ""], ["Park", "Se Yong", ""], ["Zhang", "Justin", ""], ["Dachman-Soled", "Dana", ""], ["Lauter", "Kristin", ""], ["Wu", "Min", ""]]}, {"id": "2007.04486", "submitter": "Matthew J. Holland", "authors": "Matthew J. Holland", "title": "Making learning more transparent using conformalized performance\n  prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study some novel applications of conformal inference\ntechniques to the problem of providing machine learning procedures with more\ntransparent, accurate, and practical performance guarantees. We provide a\nnatural extension of the traditional conformal prediction framework, done in\nsuch a way that we can make valid and well-calibrated predictive statements\nabout the future performance of arbitrary learning algorithms, when passed an\nas-yet unseen training set. In addition, we include some nascent empirical\nexamples to illustrate potential applications.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 00:29:55 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Holland", "Matthew J.", ""]]}, {"id": "2007.04504", "submitter": "Jacob Kelly", "authors": "Jacob Kelly, Jesse Bettencourt, Matthew James Johnson, David Duvenaud", "title": "Learning Differential Equations that are Easy to Solve", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential equations parameterized by neural networks become expensive to\nsolve numerically as training progresses. We propose a remedy that encourages\nlearned dynamics to be easier to solve. Specifically, we introduce a\ndifferentiable surrogate for the time cost of standard numerical solvers, using\nhigher-order derivatives of solution trajectories. These derivatives are\nefficient to compute with Taylor-mode automatic differentiation. Optimizing\nthis additional objective trades model performance against the time cost of\nsolving the learned dynamics. We demonstrate our approach by training\nsubstantially faster, while nearly as accurate, models in supervised\nclassification, density estimation, and time-series modelling tasks.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 01:39:34 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 18:56:41 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Kelly", "Jacob", ""], ["Bettencourt", "Jesse", ""], ["Johnson", "Matthew James", ""], ["Duvenaud", "David", ""]]}, {"id": "2007.04518", "submitter": "Hee-Seok Oh", "authors": "Ha-Young Shin and Hee-Seok Oh", "title": "Robust Geodesic Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies robust regression for data on Riemannian manifolds.\nGeodesic regression is the generalization of linear regression to a setting\nwith a manifold-valued dependent variable and one or more real-valued\nindependent variables. The existing work on geodesic regression uses the\nsum-of-squared errors to find the solution, but as in the classical Euclidean\ncase, the least-squares method is highly sensitive to outliers. In this paper,\nwe use M-type estimators, including the $L_1$, Huber and Tukey biweight\nestimators, to perform robust geodesic regression, and describe how to\ncalculate the tuning parameters for the latter two. We also show that, on\ncompact symmetric spaces, all M-type estimators are maximum likelihood\nestimators, and argue for the overall superiority of the $L_1$ estimator over\nthe $L_2$ and Huber estimators on high-dimensional manifolds and over the Tukey\nbiweight estimator on compact high-dimensional manifolds. Results from\nnumerical examples, including analysis of real neuroimaging data, demonstrate\nthe promising empirical properties of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 02:41:32 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 08:16:43 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Shin", "Ha-Young", ""], ["Oh", "Hee-Seok", ""]]}, {"id": "2007.04528", "submitter": "Brian Bullins", "authors": "Brian Bullins and Kevin A. Lai", "title": "Higher-order methods for convex-concave min-max optimization and\n  monotone variational inequalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide improved convergence rates for constrained convex-concave min-max\nproblems and monotone variational inequalities with higher-order smoothness. In\nmin-max settings where the $p^{th}$-order derivatives are Lipschitz continuous,\nwe give an algorithm HigherOrderMirrorProx that achieves an iteration\ncomplexity of $O(1/T^{\\frac{p+1}{2}})$ when given access to an oracle for\nfinding a fixed point of a $p^{th}$-order equation. We give analogous rates for\nthe weak monotone variational inequality problem. For $p>2$, our results\nimprove upon the iteration complexity of the first-order Mirror Prox method of\nNemirovski [2004] and the second-order method of Monteiro and Svaiter [2012].\nWe further instantiate our entire algorithm in the unconstrained $p=2$ case.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 03:12:33 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Bullins", "Brian", ""], ["Lai", "Kevin A.", ""]]}, {"id": "2007.04532", "submitter": "Fartash Faghri", "authors": "Fartash Faghri, David Duvenaud, David J. Fleet, Jimmy Ba", "title": "A Study of Gradient Variance in Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The impact of gradient noise on training deep models is widely acknowledged\nbut not well understood. In this context, we study the distribution of\ngradients during training. We introduce a method, Gradient Clustering, to\nminimize the variance of average mini-batch gradient with stratified sampling.\nWe prove that the variance of average mini-batch gradient is minimized if the\nelements are sampled from a weighted clustering in the gradient space. We\nmeasure the gradient variance on common deep learning benchmarks and observe\nthat, contrary to common assumptions, gradient variance increases during\ntraining, and smaller learning rates coincide with higher variance. In\naddition, we introduce normalized gradient variance as a statistic that better\ncorrelates with the speed of convergence compared to gradient variance.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 03:23:10 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Faghri", "Fartash", ""], ["Duvenaud", "David", ""], ["Fleet", "David J.", ""], ["Ba", "Jimmy", ""]]}, {"id": "2007.04540", "submitter": "Takanori Fujiwara", "authors": "Takanori Fujiwara, Tzu-Ping Liu", "title": "Contrastive Multiple Correspondence Analysis (cMCA): Using Contrastive\n  Learning to Identify Latent Subgroups in Political Parties", "comments": "Both authors contributed equally to the paper and listed\n  alphabetically. This manuscript is currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scaling methods have long been utilized to simplify and cluster\nhigh-dimensional data. However, the latent spaces derived from these methods\nare sometimes uninformative or unable to identify significant differences in\nthe data. To tackle this common issue, we adopt an emerging analysis approach\ncalled contrastive learning. We contribute to this emerging field by extending\nits ideas to multiple correspondence analysis (MCA) in order to enable an\nanalysis of data often encountered by social scientists -- namely binary,\nordinal, and nominal variables. We demonstrate the utility of contrastive MCA\n(cMCA) by analyzing three different surveys of voters in Europe, Japan, and the\nUnited States. Our results suggest that, first, cMCA can identify substantively\nimportant dimensions and divisions among (sub)groups that are overlooked by\ntraditional methods; second, for certain cases, cMCA can still derive latent\ntraits that generalize across and apply to multiple groups in the dataset;\nfinally, when data is high-dimensional and unstructured, cMCA provides\nobjective heuristics, above and beyond the standard results, enabling more\ncomplex subgroup analysis.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 03:45:13 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 00:43:59 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Fujiwara", "Takanori", ""], ["Liu", "Tzu-Ping", ""]]}, {"id": "2007.04546", "submitter": "Mengye Ren", "authors": "Mengye Ren, Michael L. Iuzzolino, Michael C. Mozer, Richard S. Zemel", "title": "Wandering Within a World: Online Contextualized Few-Shot Learning", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to bridge the gap between typical human and machine-learning\nenvironments by extending the standard framework of few-shot learning to an\nonline, continual setting. In this setting, episodes do not have separate\ntraining and testing phases, and instead models are evaluated online while\nlearning novel classes. As in the real world, where the presence of\nspatiotemporal context helps us retrieve learned skills in the past, our online\nfew-shot learning setting also features an underlying context that changes\nthroughout time. Object classes are correlated within a context and inferring\nthe correct context can lead to better performance. Building upon this setting,\nwe propose a new few-shot learning dataset based on large scale indoor imagery\nthat mimics the visual experience of an agent wandering within a world.\nFurthermore, we convert popular few-shot learning approaches into online\nversions and we also propose a new contextual prototypical memory model that\ncan make use of spatiotemporal contextual information from the recent past.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 04:05:04 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 02:34:57 GMT"}, {"version": "v3", "created": "Thu, 22 Apr 2021 20:15:19 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Ren", "Mengye", ""], ["Iuzzolino", "Michael L.", ""], ["Mozer", "Michael C.", ""], ["Zemel", "Richard S.", ""]]}, {"id": "2007.04568", "submitter": "Yanjun Han", "authors": "Yanjun Han, Zhengyuan Zhou, Aaron Flores, Erik Ordentlich, Tsachy\n  Weissman", "title": "Learning to Bid Optimally and Efficiently in Adversarial First-price\n  Auctions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First-price auctions have very recently swept the online advertising\nindustry, replacing second-price auctions as the predominant auction mechanism\non many platforms. This shift has brought forth important challenges for a\nbidder: how should one bid in a first-price auction, where unlike in\nsecond-price auctions, it is no longer optimal to bid one's private value\ntruthfully and hard to know the others' bidding behaviors? In this paper, we\ntake an online learning angle and address the fundamental problem of learning\nto bid in repeated first-price auctions, where both the bidder's private\nvaluations and other bidders' bids can be arbitrary. We develop the first\nminimax optimal online bidding algorithm that achieves an\n$\\widetilde{O}(\\sqrt{T})$ regret when competing with the set of all Lipschitz\nbidding policies, a strong oracle that contains a rich set of bidding\nstrategies. This novel algorithm is built on the insight that the presence of a\ngood expert can be leveraged to improve performance, as well as an original\nhierarchical expert-chaining structure, both of which could be of independent\ninterest in online learning. Further, by exploiting the product structure that\nexists in the problem, we modify this algorithm--in its vanilla form\nstatistically optimal but computationally infeasible--to a computationally\nefficient and space efficient algorithm that also retains the same\n$\\widetilde{O}(\\sqrt{T})$ minimax optimal regret guarantee. Additionally,\nthrough an impossibility result, we highlight that one is unlikely to compete\nthis favorably with a stronger oracle (than the considered Lipschitz bidding\npolicies). Finally, we test our algorithm on three real-world first-price\nauction datasets obtained from Verizon Media and demonstrate our algorithm's\nsuperior performance compared to several existing bidding algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 05:40:39 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Han", "Yanjun", ""], ["Zhou", "Zhengyuan", ""], ["Flores", "Aaron", ""], ["Ordentlich", "Erik", ""], ["Weissman", "Tsachy", ""]]}, {"id": "2007.04583", "submitter": "Xin Liu Dr.", "authors": "Hibiki Taguchi, Xin Liu, Tsuyoshi Murata", "title": "Graph Convolutional Networks for Graphs Containing Missing Features", "comments": null, "journal-ref": "Future Generation Computer Systems, Volume 117, Pages 155-168,\n  2021", "doi": "10.1016/j.future.2020.11.016", "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Convolutional Network (GCN) has experienced great success in graph\nanalysis tasks. It works by smoothing the node features across the graph. The\ncurrent GCN models overwhelmingly assume that the node feature information is\ncomplete. However, real-world graph data are often incomplete and containing\nmissing features. Traditionally, people have to estimate and fill in the\nunknown features based on imputation techniques and then apply GCN. However,\nthe process of feature filling and graph learning are separated, resulting in\ndegraded and unstable performance. This problem becomes more serious when a\nlarge number of features are missing. We propose an approach that adapts GCN to\ngraphs containing missing features. In contrast to traditional strategy, our\napproach integrates the processing of missing features and graph learning\nwithin the same neural network architecture. Our idea is to represent the\nmissing data by Gaussian Mixture Model (GMM) and calculate the expected\nactivation of neurons in the first hidden layer of GCN, while keeping the other\nlayers of the network unchanged. This enables us to learn the GMM parameters\nand network weight parameters in an end-to-end manner. Notably, our approach\ndoes not increase the computational complexity of GCN and it is consistent with\nGCN when the features are complete. We demonstrate through extensive\nexperiments that our approach significantly outperforms the imputation-based\nmethods in node classification and link prediction tasks. We show that the\nperformance of our approach for the case with a low level of missing features\nis even superior to GCN for the case with complete features.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 06:47:21 GMT"}, {"version": "v2", "created": "Sun, 6 Dec 2020 13:07:35 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Taguchi", "Hibiki", ""], ["Liu", "Xin", ""], ["Murata", "Tsuyoshi", ""]]}, {"id": "2007.04589", "submitter": "Kwot Sin Lee", "authors": "Kwot Sin Lee, Ngoc-Trung Tran, Ngai-Man Cheung", "title": "InfoMax-GAN: Improved Adversarial Image Generation via Information\n  Maximization and Contrastive Learning", "comments": "Accepted to WACV 2021. An initial version was accepted to NeurIPS\n  2019 Workshop on Information Theory and Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Generative Adversarial Networks (GANs) are fundamental to many\ngenerative modelling applications, they suffer from numerous issues. In this\nwork, we propose a principled framework to simultaneously mitigate two\nfundamental issues in GANs: catastrophic forgetting of the discriminator and\nmode collapse of the generator. We achieve this by employing for GANs a\ncontrastive learning and mutual information maximization approach, and perform\nextensive analyses to understand sources of improvements. Our approach\nsignificantly stabilizes GAN training and improves GAN performance for image\nsynthesis across five datasets under the same training and evaluation\nconditions against state-of-the-art works. In particular, compared to the\nstate-of-the-art SSGAN, our approach does not suffer from poorer performance on\nimage domains such as faces, and instead improves performance significantly.\nOur approach is simple to implement and practical: it involves only one\nauxiliary objective, has a low computational cost, and performs robustly across\na wide range of training settings and datasets without any hyperparameter\ntuning. For reproducibility, our code is available in Mimicry:\nhttps://github.com/kwotsin/mimicry.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 06:56:11 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 06:16:53 GMT"}, {"version": "v3", "created": "Wed, 26 Aug 2020 08:15:42 GMT"}, {"version": "v4", "created": "Mon, 2 Nov 2020 20:19:00 GMT"}, {"version": "v5", "created": "Tue, 10 Nov 2020 05:00:07 GMT"}, {"version": "v6", "created": "Sun, 22 Nov 2020 18:40:18 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Lee", "Kwot Sin", ""], ["Tran", "Ngoc-Trung", ""], ["Cheung", "Ngai-Man", ""]]}, {"id": "2007.04596", "submitter": "Hongyang Zhang", "authors": "Yuanzhi Li, Tengyu Ma, Hongyang R. Zhang", "title": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK", "comments": "Conference on Learning Theory (COLT) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the dynamic of gradient descent for learning a two-layer neural\nnetwork. We assume the input $x\\in\\mathbb{R}^d$ is drawn from a Gaussian\ndistribution and the label of $x$ satisfies $f^{\\star}(x) =\na^{\\top}|W^{\\star}x|$, where $a\\in\\mathbb{R}^d$ is a nonnegative vector and\n$W^{\\star} \\in\\mathbb{R}^{d\\times d}$ is an orthonormal matrix. We show that an\nover-parametrized two-layer neural network with ReLU activation, trained by\ngradient descent from random initialization, can provably learn the ground\ntruth network with population loss at most $o(1/d)$ in polynomial time with\npolynomial samples. On the other hand, we prove that any kernel method,\nincluding Neural Tangent Kernel, with a polynomial number of samples in $d$,\nhas population loss at least $\\Omega(1 / d)$.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 07:09:28 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Li", "Yuanzhi", ""], ["Ma", "Tengyu", ""], ["Zhang", "Hongyang R.", ""]]}, {"id": "2007.04604", "submitter": "Sao Mai Nguyen", "authors": "Linda Nanan Vall\\'ee (ESATIC), Christophe Lohr, Sao Mai Nguyen (IMT\n  Atlantique), Ioannis Kanellos (IMT Atlantique - INFO), O. Asseu (ESATIC)", "title": "Building an Automated Gesture Imitation Game for Teenagers with ASD", "comments": null, "journal-ref": "Far East Journal of Electronics and Communications, 2019, 22,\n  pp.19 - 28", "doi": "10.17654/EC023010001", "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autism spectrum disorder is a neurodevelopmental condition that includes\nissues with communication and social interactions. People with ASD also often\nhave restricted interests and repetitive behaviors. In this paper we build\npreliminary bricks of an automated gesture imitation game that will aim at\nimproving social interactions with teenagers with ASD. The structure of the\ngame is presented, as well as support tools and methods for skeleton detection\nand imitation learning. The game shall later be implemented using an\ninteractive robot.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 07:27:43 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Vall\u00e9e", "Linda Nanan", "", "ESATIC"], ["Lohr", "Christophe", "", "IMT\n  Atlantique"], ["Nguyen", "Sao Mai", "", "IMT\n  Atlantique"], ["Kanellos", "Ioannis", "", "IMT Atlantique - INFO"], ["Asseu", "O.", "", "ESATIC"]]}, {"id": "2007.04612", "submitter": "Pang Wei Koh", "authors": "Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma\n  Pierson, Been Kim, Percy Liang", "title": "Concept Bottleneck Models", "comments": "Edited for clarity from the ICML 2020 version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We seek to learn models that we can interact with using high-level concepts:\nif the model did not think there was a bone spur in the x-ray, would it still\npredict severe arthritis? State-of-the-art models today do not typically\nsupport the manipulation of concepts like \"the existence of bone spurs\", as\nthey are trained end-to-end to go directly from raw input (e.g., pixels) to\noutput (e.g., arthritis severity). We revisit the classic idea of first\npredicting concepts that are provided at training time, and then using these\nconcepts to predict the label. By construction, we can intervene on these\nconcept bottleneck models by editing their predicted concept values and\npropagating these changes to the final prediction. On x-ray grading and bird\nidentification, concept bottleneck models achieve competitive accuracy with\nstandard end-to-end models, while enabling interpretation in terms of\nhigh-level clinical concepts (\"bone spurs\") or bird attributes (\"wing color\").\nThese models also allow for richer human-model interaction: accuracy improves\nsignificantly if we can correct model mistakes on concepts at test time.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 07:47:28 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 21:22:49 GMT"}, {"version": "v3", "created": "Tue, 29 Dec 2020 00:32:21 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Koh", "Pang Wei", ""], ["Nguyen", "Thao", ""], ["Tang", "Yew Siang", ""], ["Mussmann", "Stephen", ""], ["Pierson", "Emma", ""], ["Kim", "Been", ""], ["Liang", "Percy", ""]]}, {"id": "2007.04618", "submitter": "Hossein Hosseini", "authors": "Hossein Hosseini, Sungrack Yun, Hyunsin Park, Christos Louizos, Joseph\n  Soriaga and Max Welling", "title": "Federated Learning of User Authentication Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning-based User Authentication (UA) models have been widely\ndeployed in smart devices. UA models are trained to map input data of different\nusers to highly separable embedding vectors, which are then used to accept or\nreject new inputs at test time. Training UA models requires having direct\naccess to the raw inputs and embedding vectors of users, both of which are\nprivacy-sensitive information. In this paper, we propose Federated User\nAuthentication (FedUA), a framework for privacy-preserving training of UA\nmodels. FedUA adopts federated learning framework to enable a group of users to\njointly train a model without sharing the raw inputs. It also allows users to\ngenerate their embeddings as random binary vectors, so that, unlike the\nexisting approach of constructing the spread out embeddings by the server, the\nembedding vectors are kept private as well. We show our method is\nprivacy-preserving, scalable with number of users, and allows new users to be\nadded to training without changing the output layer. Our experimental results\non the VoxCeleb dataset for speaker verification shows our method reliably\nrejects data of unseen users at very high true positive rates.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 08:04:38 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Hosseini", "Hossein", ""], ["Yun", "Sungrack", ""], ["Park", "Hyunsin", ""], ["Louizos", "Christos", ""], ["Soriaga", "Joseph", ""], ["Welling", "Max", ""]]}, {"id": "2007.04630", "submitter": "Jianlong Wu", "authors": "Xingyu Xie, Hao Kong, Jianlong Wu, Wayne Zhang, Guangcan Liu, Zhouchen\n  Lin", "title": "Maximum-and-Concatenation Networks", "comments": "Accepted by ICML2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While successful in many fields, deep neural networks (DNNs) still suffer\nfrom some open problems such as bad local minima and unsatisfactory\ngeneralization performance. In this work, we propose a novel architecture\ncalled Maximum-and-Concatenation Networks (MCN) to try eliminating bad local\nminima and improving generalization ability as well. Remarkably, we prove that\nMCN has a very nice property; that is, \\emph{every local minimum of an\n$(l+1)$-layer MCN can be better than, at least as good as, the global minima of\nthe network consisting of its first $l$ layers}. In other words, by increasing\nthe network depth, MCN can autonomously improve its local minima's goodness,\nwhat is more, \\emph{it is easy to plug MCN into an existing deep model to make\nit also have this property}. Finally, under mild conditions, we show that MCN\ncan approximate certain continuous functions arbitrarily well with \\emph{high\nefficiency}; that is, the covering number of MCN is much smaller than most\nexisting DNNs such as deep ReLU. Based on this, we further provide a tight\ngeneralization bound to guarantee the inference ability of MCN when dealing\nwith testing samples.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 08:32:02 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Xie", "Xingyu", ""], ["Kong", "Hao", ""], ["Wu", "Jianlong", ""], ["Zhang", "Wayne", ""], ["Liu", "Guangcan", ""], ["Lin", "Zhouchen", ""]]}, {"id": "2007.04637", "submitter": "Christopher Mutschler", "authors": "Christoffer L\\\"offler and Christopher Mutschler", "title": "IALE: Imitating Active Learner Ensembles", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning (AL) prioritizes the labeling of the most informative data\nsamples. However, the performance of AL heuristics depends on the structure of\nthe underlying classifier model and the data. We propose an imitation learning\nscheme that imitates the selection of the best expert heuristic at each stage\nof the AL cycle in a batch-mode pool-based setting. We use DAGGER to train the\npolicy on a dataset and later apply it to datasets from similar domains. With\nmultiple AL heuristics as experts, the policy is able to reflect the choices of\nthe best AL heuristics given the current state of the AL process. Our\nexperiment on well-known datasets show that we both outperform state of the art\nimitation learners and heuristics.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 08:38:10 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 10:29:11 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2020 07:03:54 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["L\u00f6ffler", "Christoffer", ""], ["Mutschler", "Christopher", ""]]}, {"id": "2007.04640", "submitter": "Mirco Mutti", "authors": "Mirco Mutti, Lorenzo Pratissoli, Marcello Restelli", "title": "Task-Agnostic Exploration via Policy Gradient of a Non-Parametric State\n  Entropy Estimate", "comments": "In 35th AAAI Conference on Artificial Intelligence (AAAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a reward-free environment, what is a suitable intrinsic objective for an\nagent to pursue so that it can learn an optimal task-agnostic exploration\npolicy? In this paper, we argue that the entropy of the state distribution\ninduced by finite-horizon trajectories is a sensible target. Especially, we\npresent a novel and practical policy-search algorithm, Maximum Entropy POLicy\noptimization (MEPOL), to learn a policy that maximizes a non-parametric,\n$k$-nearest neighbors estimate of the state distribution entropy. In contrast\nto known methods, MEPOL is completely model-free as it requires neither to\nestimate the state distribution of any policy nor to model transition dynamics.\nThen, we empirically show that MEPOL allows learning a maximum-entropy\nexploration policy in high-dimensional, continuous-control domains, and how\nthis policy facilitates learning a variety of meaningful reward-based tasks\ndownstream.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 08:44:39 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 20:48:49 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Mutti", "Mirco", ""], ["Pratissoli", "Lorenzo", ""], ["Restelli", "Marcello", ""]]}, {"id": "2007.04641", "submitter": "Gunarto Sindoro Njoo", "authors": "Gunarto Sindoro Njoo, Baihua Zheng, Kuo-Wei Hsu, and Wen-Chih Peng", "title": "Probabilistic Value Selection for Space Efficient Model", "comments": "Accepted in the 21st IEEE International Conference on Mobile Data\n  Management (July 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An alternative to current mainstream preprocessing methods is proposed: Value\nSelection (VS). Unlike the existing methods such as feature selection that\nremoves features and instance selection that eliminates instances, value\nselection eliminates the values (with respect to each feature) in the dataset\nwith two purposes: reducing the model size and preserving its accuracy. Two\nprobabilistic methods based on information theory's metric are proposed: PVS\nand P + VS. Extensive experiments on the benchmark datasets with various sizes\nare elaborated. Those results are compared with the existing preprocessing\nmethods such as feature selection, feature transformation, and instance\nselection methods. Experiment results show that value selection can achieve the\nbalance between accuracy and model size reduction.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 08:45:13 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Njoo", "Gunarto Sindoro", ""], ["Zheng", "Baihua", ""], ["Hsu", "Kuo-Wei", ""], ["Peng", "Wen-Chih", ""]]}, {"id": "2007.04649", "submitter": "Yingce Xia", "authors": "Yang Fan, Yingce Xia, Lijun Wu, Shufang Xie, Weiqing Liu, Jiang Bian,\n  Tao Qin, Xiang-Yang Li", "title": "Learning to Reweight with Deep Interactions", "comments": "Accepted to AAAI-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the concept of teaching has been introduced into machine learning,\nin which a teacher model is used to guide the training of a student model\n(which will be used in real tasks) through data selection, loss function\ndesign, etc. Learning to reweight, which is a specific kind of teaching that\nreweights training data using a teacher model, receives much attention due to\nits simplicity and effectiveness. In existing learning to reweight works, the\nteacher model only utilizes shallow/surface information such as training\niteration number and loss/accuracy of the student model from\ntraining/validation sets, but ignores the internal states of the student model,\nwhich limits the potential of learning to reweight. In this work, we propose an\nimproved data reweighting algorithm, in which the student model provides its\ninternal states to the teacher model, and the teacher model returns adaptive\nweights of training samples to enhance the training of the student model. The\nteacher model is jointly trained with the student model using meta gradients\npropagated from a validation set. Experiments on image classification with\nclean/noisy labels and neural machine translation empirically demonstrate that\nour algorithm makes significant improvement over previous methods.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 09:06:31 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 08:07:46 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Fan", "Yang", ""], ["Xia", "Yingce", ""], ["Wu", "Lijun", ""], ["Xie", "Shufang", ""], ["Liu", "Weiqing", ""], ["Bian", "Jiang", ""], ["Qin", "Tao", ""], ["Li", "Xiang-Yang", ""]]}, {"id": "2007.04662", "submitter": "Vihari Piratla Mr.", "authors": "Vihari Piratla, Shiv Shankar", "title": "Untapped Potential of Data Augmentation: A Domain Generalization\n  Viewpoint", "comments": "6 pages, ICML 2020 Workshop on Uncertainty and Ro-bustness in Deep\n  Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is a popular pre-processing trick to improve generalization\naccuracy. It is believed that by processing augmented inputs in tandem with the\noriginal ones, the model learns a more robust set of features which are shared\nbetween the original and augmented counterparts. However, we show that is not\nthe case even for the best augmentation technique. In this work, we take a\nDomain Generalization viewpoint of augmentation based methods. This new\nperspective allowed for probing overfitting and delineating avenues for\nimprovement. Our exploration with the state-of-art augmentation method provides\nevidence that the learned representations are not as robust even towards\ndistortions used during training. This suggests evidence for the untapped\npotential of augmented examples.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 09:40:54 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Piratla", "Vihari", ""], ["Shankar", "Shiv", ""]]}, {"id": "2007.04674", "submitter": "Francesco Grassi", "authors": "Francesco Grassi, Giorgio Manganini, Michele Garraffa, Laura Mainini", "title": "Resource Aware Multifidelity Active Learning for Efficient Optimization", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional methods for black box optimization require a considerable number\nof evaluations which can be time consuming, unpractical, and often unfeasible\nfor many engineering applications that rely on accurate representations and\nexpensive models to evaluate. Bayesian Optimization (BO) methods search for the\nglobal optimum by progressively (actively) learning a surrogate model of the\nobjective function along the search path. Bayesian optimization can be\naccelerated through multifidelity approaches which leverage multiple black-box\napproximations of the objective functions that can be computationally cheaper\nto evaluate, but still provide relevant information to the search task. Further\ncomputational benefits are offered by the availability of parallel and\ndistributed computing architectures whose optimal usage is an open opportunity\nwithin the context of active learning. This paper introduces the Resource Aware\nActive Learning (RAAL) strategy, a multifidelity Bayesian scheme to accelerate\nthe optimization of black box functions. At each optimization step, the RAAL\nprocedure computes the set of best sample locations and the associated fidelity\nsources that maximize the information gain to acquire during the\nparallel/distributed evaluation of the objective function, while accounting for\nthe limited computational budget. The scheme is demonstrated for a variety of\nbenchmark problems and results are discussed for both single fidelity and\nmultifidelity settings. In particular we observe that the RAAL strategy\noptimally seeds multiple points at each iteration allowing for a major speed up\nof the optimization task.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 10:01:32 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Grassi", "Francesco", ""], ["Manganini", "Giorgio", ""], ["Garraffa", "Michele", ""], ["Mainini", "Laura", ""]]}, {"id": "2007.04676", "submitter": "Xiangming Meng", "authors": "Xiangming Meng", "title": "Training Restricted Boltzmann Machines with Binary Synapses using the\n  Bayesian Learning Rule", "comments": "Technical note. Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzmann machines (RBMs) with low-precision synapses are much\nappealing with high energy efficiency. However, training RBMs with binary\nsynapses is challenging due to the discrete nature of synapses. Recently Huang\nproposed one efficient method to train RBMs with binary synapses by using a\ncombination of gradient ascent and the message passing algorithm under the\nvariational inference framework. However, additional heuristic clipping\noperation is needed. In this technical note, inspired from Huang's work , we\npropose one alternative optimization method using the Bayesian learning rule,\nwhich is one natural gradient variational inference method. As opposed to\nHuang's method, we update the natural parameters of the variational symmetric\nBernoulli distribution rather than the expectation parameters. Since the\nnatural parameters take values in the entire real domain, no additional\nclipping is needed. Interestingly, the algorithm in \\cite{huang2019data} could\nbe viewed as one first-order approximation of the proposed algorithm, which\njustifies its efficacy with heuristic clipping.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 10:05:46 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Meng", "Xiangming", ""]]}, {"id": "2007.04725", "submitter": "Ahmed Hallawa Mr.", "authors": "Ahmed Hallawa, Thorsten Born, Anke Schmeink, Guido Dartmann, Arne\n  Peine, Lukas Martin, Giovanni Iacca, A. E. Eiben, Gerd Ascheid", "title": "EVO-RL: Evolutionary-Driven Reinforcement Learning", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel approach for reinforcement learning driven\nby evolutionary computation. Our algorithm, dubbed as Evolutionary-Driven\nReinforcement Learning (evo-RL), embeds the reinforcement learning algorithm in\nan evolutionary cycle, where we distinctly differentiate between purely\nevolvable (instinctive) behaviour versus purely learnable behaviour.\nFurthermore, we propose that this distinction is decided by the evolutionary\nprocess, thus allowing evo-RL to be adaptive to different environments. In\naddition, evo-RL facilitates learning on environments with rewardless states,\nwhich makes it more suited for real-world problems with incomplete information.\nTo show that evo-RL leads to state-of-the-art performance, we present the\nperformance of different state-of-the-art reinforcement learning algorithms\nwhen operating within evo-RL and compare it with the case when these same\nalgorithms are executed independently. Results show that reinforcement learning\nalgorithms embedded within our evo-RL approach significantly outperform the\nstand-alone versions of the same RL algorithms on OpenAI Gym control problems\nwith rewardless states constrained by the same computational budget.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 11:52:19 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 16:14:58 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Hallawa", "Ahmed", ""], ["Born", "Thorsten", ""], ["Schmeink", "Anke", ""], ["Dartmann", "Guido", ""], ["Peine", "Arne", ""], ["Martin", "Lukas", ""], ["Iacca", "Giovanni", ""], ["Eiben", "A. E.", ""], ["Ascheid", "Gerd", ""]]}, {"id": "2007.04728", "submitter": "Ofir Lindenbaum", "authors": "Ofir Lindenbaum, Uri Shaham, Jonathan Svirsky, Erez Peterfreund, Yuval\n  Kluger", "title": "Differentiable Unsupervised Feature Selection based on a Gated Laplacian", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific observations may consist of a large number of variables\n(features). Identifying a subset of meaningful features is often ignored in\nunsupervised learning, despite its potential for unraveling clear patterns\nhidden in the ambient space. In this paper, we present a method for\nunsupervised feature selection, and we demonstrate its use for the task of\nclustering. We propose a differentiable loss function that combines the\nLaplacian score, which favors low-frequency features, with a gating mechanism\nfor feature selection. We improve the Laplacian score, by replacing it with a\ngated variant computed on a subset of features. This subset is obtained using a\ncontinuous approximation of Bernoulli variables whose parameters are trained to\ngate the full feature space. We mathematically motivate the proposed approach\nand demonstrate that in the high noise regime, it is crucial to compute the\nLaplacian on the gated inputs, rather than on the full feature set.\nExperimental demonstration of the efficacy of the proposed approach and its\nadvantage over current baselines is provided using several real-world examples.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 11:58:16 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2020 18:28:35 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2020 11:23:01 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Lindenbaum", "Ofir", ""], ["Shaham", "Uri", ""], ["Svirsky", "Jonathan", ""], ["Peterfreund", "Erez", ""], ["Kluger", "Yuval", ""]]}, {"id": "2007.04731", "submitter": "Paul Chang", "authors": "Paul E. Chang, William J. Wilkinson, Mohammad Emtiyaz Khan, Arno Solin", "title": "Fast Variational Learning in State-Space Gaussian Process Models", "comments": "To appear in MLSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process (GP) regression with 1D inputs can often be performed in\nlinear time via a stochastic differential equation formulation. However, for\nnon-Gaussian likelihoods, this requires application of approximate inference\nmethods which can make the implementation difficult, e.g., expectation\npropagation can be numerically unstable and variational inference can be\ncomputationally inefficient. In this paper, we propose a new method that\nremoves such difficulties. Building upon an existing method called\nconjugate-computation variational inference, our approach enables linear-time\ninference via Kalman recursions while avoiding numerical instabilities and\nconvergence issues. We provide an efficient JAX implementation which exploits\njust-in-time compilation and allows for fast automatic differentiation through\nlarge for-loops. Overall, our approach leads to fast and stable variational\ninference in state-space GP models that can be scaled to time series with\nmillions of data points.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 12:06:34 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 10:46:44 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Chang", "Paul E.", ""], ["Wilkinson", "William J.", ""], ["Khan", "Mohammad Emtiyaz", ""], ["Solin", "Arno", ""]]}, {"id": "2007.04750", "submitter": "Paulo Rauber", "authors": "Aditya Ramesh, Paulo Rauber, J\\\"urgen Schmidhuber", "title": "Recurrent Neural-Linear Posterior Sampling for Non-Stationary Contextual\n  Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An agent in a non-stationary contextual bandit problem should balance between\nexploration and the exploitation of (periodic or structured) patterns present\nin its previous experiences. Handcrafting an appropriate historical context is\nan attractive alternative to transform a non-stationary problem into a\nstationary problem that can be solved efficiently. However, even a carefully\ndesigned historical context may introduce spurious relationships or lack a\nconvenient representation of crucial information. In order to address these\nissues, we propose an approach that learns to represent the relevant context\nfor a decision based solely on the raw history of interactions between the\nagent and the environment. This approach relies on a combination of features\nextracted by recurrent neural networks with a contextual linear bandit\nalgorithm based on posterior sampling. Our experiments on a diverse selection\nof contextual and non-contextual non-stationary problems show that our\nrecurrent approach consistently outperforms its feedforward counterpart, which\nrequires handcrafted historical contexts, while being more widely applicable\nthan conventional non-stationary bandit algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 12:46:51 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Ramesh", "Aditya", ""], ["Rauber", "Paulo", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "2007.04759", "submitter": "Mones Raslan", "authors": "Ingo G\\\"uhring, Mones Raslan, Gitta Kutyniok", "title": "Expressivity of Deep Neural Networks", "comments": "This review paper will appear as a book chapter in the book \"Theory\n  of Deep Learning\" by Cambridge University Press", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.FA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this review paper, we give a comprehensive overview of the large variety\nof approximation results for neural networks. Approximation rates for classical\nfunction spaces as well as benefits of deep neural networks over shallow ones\nfor specifically structured function classes are discussed. While the mainbody\nof existing results is for general feedforward architectures, we also depict\napproximation results for convolutional, residual and recurrent neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 13:08:01 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["G\u00fchring", "Ingo", ""], ["Raslan", "Mones", ""], ["Kutyniok", "Gitta", ""]]}, {"id": "2007.04777", "submitter": "Arijit Sehanobish", "authors": "Arijit Sehanobish, Neal G. Ravindra, David van Dijk", "title": "Self-supervised edge features for improved Graph Neural Network training", "comments": "Comments welcome. arXiv admin note: substantial text overlap with\n  arXiv:2006.12971", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNN) have been extensively used to extract meaningful\nrepresentations from graph structured data and to perform predictive tasks such\nas node classification and link prediction. In recent years, there has been a\nlot of work incorporating edge features along with node features for prediction\ntasks. One of the main difficulties in using edge features is that they are\noften handcrafted, hard to get, specific to a particular domain, and may\ncontain redundant information. In this work, we present a framework for\ncreating new edge features, applicable to any domain, via a combination of\nself-supervised and unsupervised learning. In addition to this, we use\nForman-Ricci curvature as an additional edge feature to encapsulate the local\ngeometry of the graph. We then encode our edge features via a Set Transformer\nand combine them with node features extracted from popular GNN architectures\nfor node classification in an end-to-end training scheme. We validate our work\non three biological datasets comprising of single-cell RNA sequencing data of\nneurological disease, \\textit{in vitro} SARS-CoV-2 infection, and human\nCOVID-19 patients. We demonstrate that our method achieves better performance\non node classification tasks over baseline Graph Attention Network (GAT) and\nGraph Convolutional Network (GCN) models. Furthermore, given the attention\nmechanism on edge and node features, we are able to interpret the cell types\nand genes that determine the course and severity of COVID-19, contributing to a\ngrowing list of potential disease biomarkers and therapeutic targets.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 20:18:22 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Sehanobish", "Arijit", ""], ["Ravindra", "Neal G.", ""], ["van Dijk", "David", ""]]}, {"id": "2007.04785", "submitter": "Renqian Luo", "authors": "Renqian Luo, Xu Tan, Rui Wang, Tao Qin, Enhong Chen, Tie-Yan Liu", "title": "Accuracy Prediction with Non-neural Model for Neural Architecture Search", "comments": "Code is available at https://github.com/renqianluo/GBDT-NAS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search (NAS) with an accuracy predictor that predicts the\naccuracy of candidate architectures has drawn increasing attention due to its\nsimplicity and effectiveness. Previous works usually employ neural\nnetwork-based predictors which require more delicate design and are easy to\noverfit. Considering that most architectures are represented as sequences of\ndiscrete symbols which are more like tabular data and preferred by non-neural\npredictors, in this paper, we study an alternative approach which uses\nnon-neural model for accuracy prediction. Specifically, as decision tree based\nmodels can better handle tabular data, we leverage gradient boosting decision\ntree (GBDT) as the predictor for NAS. We demonstrate that the GBDT predictor\ncan achieve comparable (if not better) prediction accuracy than neural network\nbased predictors. Moreover, considering that a compact search space can ease\nthe search process, we propose to prune the search space gradually according to\nimportant features derived from GBDT. In this way, NAS can be performed by\nfirst pruning the search space and then searching a neural architecture, which\nis more efficient and effective. Experiments on NASBench-101 and ImageNet\ndemonstrate the effectiveness of using GBDT as predictor for NAS: (1) On\nNASBench-101, it is 22x, 8x, and 6x more sample efficient than random search,\nregularized evolution, and Monte Carlo Tree Search (MCTS) in finding the global\noptimum; (2) It achieves 24.2% top-1 error rate on ImageNet, and further\nachieves 23.4% top-1 error rate on ImageNet when enhanced with search space\npruning. Code is provided at https://github.com/renqianluo/GBDT-NAS.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 13:28:49 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 11:43:02 GMT"}, {"version": "v3", "created": "Mon, 19 Jul 2021 07:31:57 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Luo", "Renqian", ""], ["Tan", "Xu", ""], ["Wang", "Rui", ""], ["Qin", "Tao", ""], ["Chen", "Enhong", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2007.04790", "submitter": "Wei Chen", "authors": "Wei Chen and Faez Ahmed", "title": "MO-PaDGAN: Generating Diverse Designs with Multivariate Performance\n  Enhancement", "comments": "arXiv admin note: text overlap with arXiv:2002.11304", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models have proven useful for automatic design synthesis and\ndesign space exploration. However, they face three challenges when applied to\nengineering design: 1) generated designs lack diversity, 2) it is difficult to\nexplicitly improve all the performance measures of generated designs, and 3)\nexisting models generally do not generate high-performance novel designs,\noutside the domain of the training data. To address these challenges, we\npropose MO-PaDGAN, which contains a new Determinantal Point Processes based\nloss function for probabilistic modeling of diversity and performances. Through\na real-world airfoil design example, we demonstrate that MO-PaDGAN expands the\nexisting boundary of the design space towards high-performance regions and\ngenerates new designs with high diversity and performances exceeding training\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 21:57:29 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Chen", "Wei", ""], ["Ahmed", "Faez", ""]]}, {"id": "2007.04800", "submitter": "Sebastian Bordt", "authors": "Sebastian Bordt, Ulrike von Luxburg", "title": "When Humans and Machines Make Joint Decisions: A Non-Symmetric Bandit\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can humans and machines learn to make joint decisions? This has become an\nimportant question in domains such as medicine, law and finance. We approach\nthe question from a theoretical perspective and formalize our intuitions about\nhuman-machine decision making in a non-symmetric bandit model. In doing so, we\nfollow the example of a doctor who is assisted by a computer program. We show\nthat in our model, exploration is generally hard. In particular, unless one is\nwilling to make assumptions about how human and machine interact, the machine\ncannot explore efficiently. We highlight one such assumption, policy space\nindependence, which resolves the coordination problem and allows both players\nto explore independently. Our results shed light on the fundamental\ndifficulties faced by the interaction of humans and machines. We also discuss\npractical implications for the design of algorithmic decision systems.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 13:43:08 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Bordt", "Sebastian", ""], ["von Luxburg", "Ulrike", ""]]}, {"id": "2007.04803", "submitter": "Mathieu Gerber", "authors": "Mathieu Gerber and Randal Douc", "title": "A Global Stochastic Optimization Particle Filter Algorithm", "comments": "67 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new algorithm to learn on the fly the parameter value\n$\\theta_\\star:=\\mathrm{argmax}_{\\theta\\in\\Theta}\\mathbb{E}[\\log f_\\theta(Y_0)]$\nfrom a sequence $(Y_t)_{t\\geq 1}$ of independent copies of $Y_0$, with\n$\\{f_\\theta,\\,\\theta\\in\\Theta\\subseteq\\mathbb{R}^d\\}$ a parametric model. The\nmain idea of the proposed approach is to define a sequence\n$(\\tilde{\\pi}_t)_{t\\geq 1}$ of probability distributions on $\\Theta$ which (i)\nis shown to concentrate on $\\theta_\\star$ as $t\\rightarrow\\infty$ and (ii) can\nbe estimated in an online fashion by means of a standard particle filter (PF)\nalgorithm. The sequence $(\\tilde{\\pi}_t)_{t\\geq 1}$ depends on a learning rate\n$h_t\\rightarrow 0$, with the slower $h_t$ converges to zero the greater is the\nability of the PF approximation $\\tilde{\\pi}_t^N$ of $\\tilde{\\pi}_t$ to escape\nfrom a local optimum of the objective function, but the slower is the rate at\nwhich $\\tilde{\\pi}_t$ concentrates on $\\theta_\\star$. To conciliate ability to\nescape from a local optimum and fast convergence towards $\\theta_\\star$ we\nexploit the acceleration property of averaging, well-known in the stochastic\ngradient descent literature, by letting $\\bar{\\theta}_t^N:=t^{-1}\\sum_{s=1}^t\n\\int_{\\Theta}\\theta\\ \\tilde{\\pi}_s^N(\\mathrm{d} \\theta)$ be the proposed\nestimator of $\\theta_\\star$. Our numerical experiments suggest that\n$\\bar{\\theta}_t^N$ converges to $\\theta_\\star$ at the optimal $t^{-1/2}$ rate\nin challenging models and in situations where $\\tilde{\\pi}_t^N$ concentrates on\nthis parameter value at a slower rate. We illustrate the practical usefulness\nof the proposed optimization algorithm for online parameter learning and for\ncomputing the maximum likelihood estimator.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 14:17:43 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 13:33:23 GMT"}, {"version": "v3", "created": "Mon, 3 May 2021 14:49:53 GMT"}, {"version": "v4", "created": "Fri, 14 May 2021 11:14:49 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Gerber", "Mathieu", ""], ["Douc", "Randal", ""]]}, {"id": "2007.04806", "submitter": "Rasmus Malik Thaarup H{\\o}egh", "authors": "Laura Rieger, Rasmus M. Th. H{\\o}egh, and Lars K. Hansen", "title": "Client Adaptation improves Federated Learning with Simulated Non-IID\n  Clients", "comments": "11 pages, 11 figures. To appear at International Workshop on\n  Federated Learning for User Privacy and Data Confidentiality in Conjunction\n  with ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a federated learning approach for learning a client adaptable,\nrobust model when data is non-identically and non-independently distributed\n(non-IID) across clients. By simulating heterogeneous clients, we show that\nadding learned client-specific conditioning improves model performance, and the\napproach is shown to work on balanced and imbalanced data set from both audio\nand image domains. The client adaptation is implemented by a conditional gated\nactivation unit and is particularly beneficial when there are large differences\nbetween the data distribution for each client, a common scenario in federated\nlearning.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 13:48:39 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Rieger", "Laura", ""], ["H\u00f8egh", "Rasmus M. Th.", ""], ["Hansen", "Lars K.", ""]]}, {"id": "2007.04813", "submitter": "Binh Tang", "authors": "Binh Tang, David S. Matteson", "title": "Graph-Based Continual Learning", "comments": "Published as a conference paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant advances, continual learning models still suffer from\ncatastrophic forgetting when exposed to incrementally available data from\nnon-stationary distributions. Rehearsal approaches alleviate the problem by\nmaintaining and replaying a small episodic memory of previous samples, often\nimplemented as an array of independent memory slots. In this work, we propose\nto augment such an array with a learnable random graph that captures pairwise\nsimilarities between its samples, and use it not only to learn new tasks but\nalso to guard against forgetting. Empirical results on several benchmark\ndatasets show that our model consistently outperforms recently proposed\nbaselines for task-free continual learning.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 14:03:31 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 01:54:00 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Tang", "Binh", ""], ["Matteson", "David S.", ""]]}, {"id": "2007.04825", "submitter": "Apoorv Vyas", "authors": "Apoorv Vyas, Angelos Katharopoulos, Fran\\c{c}ois Fleuret", "title": "Fast Transformers with Clustered Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers have been proven a successful model for a variety of tasks in\nsequence modeling. However, computing the attention matrix, which is their key\ncomponent, has quadratic complexity with respect to the sequence length, thus\nmaking them prohibitively expensive for large sequences. To address this, we\npropose clustered attention, which instead of computing the attention for every\nquery, groups queries into clusters and computes attention just for the\ncentroids. To further improve this approximation, we use the computed clusters\nto identify the keys with the highest attention per query and compute the exact\nkey/query dot products. This results in a model with linear complexity with\nrespect to the sequence length for a fixed number of clusters. We evaluate our\napproach on two automatic speech recognition datasets and show that our model\nconsistently outperforms vanilla transformers for a given computational budget.\nFinally, we demonstrate that our model can approximate arbitrarily complex\nattention distributions with a minimal number of clusters by approximating a\npretrained BERT model on GLUE and SQuAD benchmarks with only 25 clusters and no\nloss in performance.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 14:17:50 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 20:18:43 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Vyas", "Apoorv", ""], ["Katharopoulos", "Angelos", ""], ["Fleuret", "Fran\u00e7ois", ""]]}, {"id": "2007.04838", "submitter": "Thierry Roncalli", "authors": "Edmond Lezmi, Jules Roche, Thierry Roncalli, Jiali Xu", "title": "Improving the Robustness of Trading Strategy Backtesting with Boltzmann\n  Machines and Generative Adversarial Networks", "comments": "72 pages, 30 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-fin.PM q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article explores the use of machine learning models to build a market\ngenerator. The underlying idea is to simulate artificial multi-dimensional\nfinancial time series, whose statistical properties are the same as those\nobserved in the financial markets. In particular, these synthetic data must\npreserve the probability distribution of asset returns, the stochastic\ndependence between the different assets and the autocorrelation across time.\nThe article proposes then a new approach for estimating the probability\ndistribution of backtest statistics. The final objective is to develop a\nframework for improving the risk management of quantitative investment\nstrategies, in particular in the space of smart beta, factor investing and\nalternative risk premia.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 14:37:45 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Lezmi", "Edmond", ""], ["Roche", "Jules", ""], ["Roncalli", "Thierry", ""], ["Xu", "Jiali", ""]]}, {"id": "2007.04871", "submitter": "Joseph Cheng", "authors": "Joseph Y. Cheng, Hanlin Goh, Kaan Dogrusoz, Oncel Tuzel, Erdrin Azemi", "title": "Subject-Aware Contrastive Learning for Biosignals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datasets for biosignals, such as electroencephalogram (EEG) and\nelectrocardiogram (ECG), often have noisy labels and have limited number of\nsubjects (<100). To handle these challenges, we propose a self-supervised\napproach based on contrastive learning to model biosignals with a reduced\nreliance on labeled data and with fewer subjects. In this regime of limited\nlabels and subjects, intersubject variability negatively impacts model\nperformance. Thus, we introduce subject-aware learning through (1) a\nsubject-specific contrastive loss, and (2) an adversarial training to promote\nsubject-invariance during the self-supervised learning. We also develop a\nnumber of time-series data augmentation techniques to be used with the\ncontrastive loss for biosignals. Our method is evaluated on publicly available\ndatasets of two different biosignals with different tasks: EEG decoding and ECG\nanomaly detection. The embeddings learned using self-supervision yield\ncompetitive classification results compared to entirely supervised methods. We\nshow that subject-invariance improves representation quality for these tasks,\nand observe that subject-specific loss increases performance when fine-tuning\nwith supervised labels.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 20:32:37 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Cheng", "Joseph Y.", ""], ["Goh", "Hanlin", ""], ["Dogrusoz", "Kaan", ""], ["Tuzel", "Oncel", ""], ["Azemi", "Erdrin", ""]]}, {"id": "2007.04873", "submitter": "Yuming Shen", "authors": "Yuming Shen, Jie Qin, Lei Huang", "title": "Invertible Zero-Shot Recognition Flows", "comments": "ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models have been successfully applied to Zero-Shot Learning\n(ZSL) recently. However, the underlying drawbacks of GANs and VAEs (e.g., the\nhardness of training with ZSL-oriented regularizers and the limited generation\nquality) hinder the existing generative ZSL models from fully bypassing the\nseen-unseen bias. To tackle the above limitations, for the first time, this\nwork incorporates a new family of generative models (i.e., flow-based models)\ninto ZSL. The proposed Invertible Zero-shot Flow (IZF) learns factorized data\nembeddings (i.e., the semantic factors and the non-semantic ones) with the\nforward pass of an invertible flow network, while the reverse pass generates\ndata samples. This procedure theoretically extends conventional generative\nflows to a factorized conditional scheme. To explicitly solve the bias problem,\nour model enlarges the seen-unseen distributional discrepancy based on negative\nsample-based distance measurement. Notably, IZF works flexibly with either a\nnaive Bayesian classifier or a held-out trainable one for zero-shot\nrecognition. Experiments on widely-adopted ZSL benchmarks demonstrate the\nsignificant performance gain of IZF over existing methods, in both classic and\ngeneralized settings.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 15:21:28 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Shen", "Yuming", ""], ["Qin", "Jie", ""], ["Huang", "Lei", ""]]}, {"id": "2007.04876", "submitter": "Yuan Zhou", "authors": "Kefan Dong, Yingkai Li, Qin Zhang, Yuan Zhou", "title": "Multinomial Logit Bandit with Low Switching Cost", "comments": "Accepted for presentation at the International Conference on Machine\n  Learning (ICML) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study multinomial logit bandit with limited adaptivity, where the\nalgorithms change their exploration actions as infrequently as possible when\nachieving almost optimal minimax regret. We propose two measures of adaptivity:\nthe assortment switching cost and the more fine-grained item switching cost. We\npresent an anytime algorithm (AT-DUCB) with $O(N \\log T)$ assortment switches,\nalmost matching the lower bound $\\Omega(\\frac{N \\log T}{ \\log \\log T})$. In the\nfixed-horizon setting, our algorithm FH-DUCB incurs $O(N \\log \\log T)$\nassortment switches, matching the asymptotic lower bound. We also present the\nESUCB algorithm with item switching cost $O(N \\log^2 T)$.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 15:26:54 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Dong", "Kefan", ""], ["Li", "Yingkai", ""], ["Zhang", "Qin", ""], ["Zhou", "Yuan", ""]]}, {"id": "2007.04897", "submitter": "Sungsoo Ahn", "authors": "Sungsoo Ahn, Junsu Kim, Hankook Lee, Jinwoo Shin", "title": "Guiding Deep Molecular Optimization with Genetic Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  De novo molecular design attempts to search over the chemical space for\nmolecules with the desired property. Recently, deep learning has gained\nconsiderable attention as a promising approach to solve the problem. In this\npaper, we propose genetic expert-guided learning (GEGL), a simple yet novel\nframework for training a deep neural network (DNN) to generate highly-rewarding\nmolecules. Our main idea is to design a \"genetic expert improvement\" procedure,\nwhich generates high-quality targets for imitation learning of the DNN.\nExtensive experiments show that GEGL significantly improves over\nstate-of-the-art methods. For example, GEGL manages to solve the penalized\noctanol-water partition coefficient optimization with a score of 31.40, while\nthe best-known score in the literature is 27.22. Besides, for the GuacaMol\nbenchmark with 20 tasks, our method achieves the highest score for 19 tasks, in\ncomparison with state-of-the-art methods, and newly obtains the perfect score\nfor three tasks.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 05:01:26 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 11:23:07 GMT"}, {"version": "v3", "created": "Tue, 27 Oct 2020 10:49:47 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Ahn", "Sungsoo", ""], ["Kim", "Junsu", ""], ["Lee", "Hankook", ""], ["Shin", "Jinwoo", ""]]}, {"id": "2007.04911", "submitter": "Pieter Gijsbers", "authors": "Pieter Gijsbers and Joaquin Vanschoren", "title": "GAMA: a General Automated Machine learning Assistant", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The General Automated Machine learning Assistant (GAMA) is a modular AutoML\nsystem developed to empower users to track and control how AutoML algorithms\nsearch for optimal machine learning pipelines, and facilitate AutoML research\nitself. In contrast to current, often black-box systems, GAMA allows users to\nplug in different AutoML and post-processing techniques, logs and visualizes\nthe search process, and supports easy benchmarking. It currently features three\nAutoML search algorithms, two model post-processing steps, and is designed to\nallow for more components to be added.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 16:16:25 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Gijsbers", "Pieter", ""], ["Vanschoren", "Joaquin", ""]]}, {"id": "2007.04915", "submitter": "Tong Yu", "authors": "Tong Yu, Branislav Kveton, Zheng Wen, Ruiyi Zhang, Ole J. Mengshoel", "title": "Influence Diagram Bandits: Variational Thompson Sampling for Structured\n  Bandit Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel framework for structured bandits, which we call an\ninfluence diagram bandit. Our framework captures complex statistical\ndependencies between actions, latent variables, and observations; and thus\nunifies and extends many existing models, such as combinatorial semi-bandits,\ncascading bandits, and low-rank bandits. We develop novel online learning\nalgorithms that learn to act efficiently in our models. The key idea is to\ntrack a structured posterior distribution of model parameters, either exactly\nor approximately. To act, we sample model parameters from their posterior and\nthen use the structure of the influence diagram to find the most optimistic\naction under the sampled parameters. We empirically evaluate our algorithms in\nthree structured bandit problems, and show that they perform as well as or\nbetter than problem-specific state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 16:25:40 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Yu", "Tong", ""], ["Kveton", "Branislav", ""], ["Wen", "Zheng", ""], ["Zhang", "Ruiyi", ""], ["Mengshoel", "Ole J.", ""]]}, {"id": "2007.04921", "submitter": "Andrew White", "authors": "Zhiheng Li, Geemi P. Wellawatte, Maghesree Chakraborty, Heta A.\n  Gandhi, Chenliang Xu, Andrew D. White", "title": "Graph Neural Network Based Coarse-Grained Mapping Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The selection of coarse-grained (CG) mapping operators is a critical step for\nCG molecular dynamics (MD) simulation. It is still an open question about what\nis optimal for this choice and there is a need for theory. The current\nstate-of-the art method is mapping operators manually selected by experts. In\nthis work, we demonstrate an automated approach by viewing this problem as\nsupervised learning where we seek to reproduce the mapping operators produced\nby experts. We present a graph neural network based CG mapping predictor called\nDEEP SUPERVISED GRAPH PARTITIONING MODEL(DSGPM) that treats mapping operators\nas a graph segmentation problem. DSGPM is trained on a novel dataset,\nHuman-annotated Mappings (HAM), consisting of 1,206 molecules with expert\nannotated mapping operators. HAM can be used to facilitate further research in\nthis area. Our model uses a novel metric learning objective to produce\nhigh-quality atomic features that are used in spectral clustering. The results\nshow that the DSGPM outperforms state-of-the-art methods in the field of graph\nsegmentation. Finally, we find that predicted CG mapping operators indeed\nresult in good CG MD models when used in simulation.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 15:05:39 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 20:05:13 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Li", "Zhiheng", ""], ["Wellawatte", "Geemi P.", ""], ["Chakraborty", "Maghesree", ""], ["Gandhi", "Heta A.", ""], ["Xu", "Chenliang", ""], ["White", "Andrew D.", ""]]}, {"id": "2007.04929", "submitter": "Daniel D. Johnson", "authors": "Daniel D. Johnson, Hugo Larochelle, Daniel Tarlow", "title": "Learning Graph Structure With A Finite-State Automaton Layer", "comments": "Accepted at NeurIPS 2020 (spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based neural network models are producing strong results in a number of\ndomains, in part because graphs provide flexibility to encode domain knowledge\nin the form of relational structure (edges) between nodes in the graph. In\npractice, edges are used both to represent intrinsic structure (e.g., abstract\nsyntax trees of programs) and more abstract relations that aid reasoning for a\ndownstream task (e.g., results of relevant program analyses). In this work, we\nstudy the problem of learning to derive abstract relations from the intrinsic\ngraph structure. Motivated by their power in program analyses, we consider\nrelations defined by paths on the base graph accepted by a finite-state\nautomaton. We show how to learn these relations end-to-end by relaxing the\nproblem into learning finite-state automata policies on a graph-based POMDP and\nthen training these policies using implicit differentiation. The result is a\ndifferentiable Graph Finite-State Automaton (GFSA) layer that adds a new edge\ntype (expressed as a weighted adjacency matrix) to a base graph. We demonstrate\nthat this layer can find shortcuts in grid-world graphs and reproduce simple\nstatic analyses on Python programs. Additionally, we combine the GFSA layer\nwith a larger graph-based model trained end-to-end on the variable misuse\nprogram understanding task, and find that using the GFSA layer leads to better\nperformance than using hand-engineered semantic edges or other baseline methods\nfor adding learned edge types.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 17:01:34 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 18:26:49 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Johnson", "Daniel D.", ""], ["Larochelle", "Hugo", ""], ["Tarlow", "Daniel", ""]]}, {"id": "2007.04938", "submitter": "Kimin Lee", "authors": "Kimin Lee, Michael Laskin, Aravind Srinivas, Pieter Abbeel", "title": "SUNRISE: A Simple Unified Framework for Ensemble Learning in Deep\n  Reinforcement Learning", "comments": "ICML 2021 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Off-policy deep reinforcement learning (RL) has been successful in a range of\nchallenging domains. However, standard off-policy RL algorithms can suffer from\nseveral issues, such as instability in Q-learning and balancing exploration and\nexploitation. To mitigate these issues, we present SUNRISE, a simple unified\nensemble method, which is compatible with various off-policy RL algorithms.\nSUNRISE integrates two key ingredients: (a) ensemble-based weighted Bellman\nbackups, which re-weight target Q-values based on uncertainty estimates from a\nQ-ensemble, and (b) an inference method that selects actions using the highest\nupper-confidence bounds for efficient exploration. By enforcing the diversity\nbetween agents using Bootstrap with random initialization, we show that these\ndifferent ideas are largely orthogonal and can be fruitfully integrated,\ntogether further improving the performance of existing off-policy RL\nalgorithms, such as Soft Actor-Critic and Rainbow DQN, for both continuous and\ndiscrete control tasks on both low-dimensional and high-dimensional\nenvironments. Our training code is available at\nhttps://github.com/pokaxpoka/sunrise.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 17:08:44 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 20:10:34 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 22:27:09 GMT"}, {"version": "v4", "created": "Fri, 11 Jun 2021 21:00:13 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Lee", "Kimin", ""], ["Laskin", "Michael", ""], ["Srinivas", "Aravind", ""], ["Abbeel", "Pieter", ""]]}, {"id": "2007.04965", "submitter": "Colin White", "authors": "Colin White, Willie Neiswanger, Sam Nolen, Yash Savani", "title": "A Study on Encodings for Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search (NAS) has been extensively studied in the past few\nyears. A popular approach is to represent each neural architecture in the\nsearch space as a directed acyclic graph (DAG), and then search over all DAGs\nby encoding the adjacency matrix and list of operations as a set of\nhyperparameters. Recent work has demonstrated that even small changes to the\nway each architecture is encoded can have a significant effect on the\nperformance of NAS algorithms.\n  In this work, we present the first formal study on the effect of architecture\nencodings for NAS, including a theoretical grounding and an empirical study.\nFirst we formally define architecture encodings and give a theoretical\ncharacterization on the scalability of the encodings we study Then we identify\nthe main encoding-dependent subroutines which NAS algorithms employ, running\nexperiments to show which encodings work best with each subroutine for many\npopular algorithms. The experiments act as an ablation study for prior work,\ndisentangling the algorithmic and encoding-based contributions, as well as a\nguideline for future work. Our results demonstrate that NAS encodings are an\nimportant design decision which can have a significant impact on overall\nperformance. Our code is available at\nhttps://github.com/naszilla/nas-encodings.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 17:52:11 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 23:04:02 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["White", "Colin", ""], ["Neiswanger", "Willie", ""], ["Nolen", "Sam", ""], ["Savani", "Yash", ""]]}, {"id": "2007.04972", "submitter": "Shaheer Ullah Saeed", "authors": "Shaheer U. Saeed, Zeike A. Taylor, Mark A. Pinnock, Mark Emberton,\n  Dean C. Barratt, Yipeng Hu", "title": "Prostate motion modelling using biomechanically-trained deep neural\n  networks on unstructured nodes", "comments": "Accepted to MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to train deep neural networks with biomechanical\nsimulations, to predict the prostate motion encountered during\nultrasound-guided interventions. In this application, unstructured points are\nsampled from segmented pre-operative MR images to represent the anatomical\nregions of interest. The point sets are then assigned with point-specific\nmaterial properties and displacement loads, forming the un-ordered input\nfeature vectors. An adapted PointNet can be trained to predict the nodal\ndisplacements, using finite element (FE) simulations as ground-truth data.\nFurthermore, a versatile bootstrap aggregating mechanism is validated to\naccommodate the variable number of feature vectors due to different patient\ngeometries, comprised of a training-time bootstrap sampling and a model\naveraging inference. This results in a fast and accurate approximation to the\nFE solutions without requiring subject-specific solid meshing. Based on 160,000\nnonlinear FE simulations on clinical imaging data from 320 patients, we\ndemonstrate that the trained networks generalise to unstructured point sets\nsampled directly from holdout patient segmentation, yielding a near real-time\ninference and an expected error of 0.017 mm in predicted nodal displacement.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 17:58:41 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Saeed", "Shaheer U.", ""], ["Taylor", "Zeike A.", ""], ["Pinnock", "Mark A.", ""], ["Emberton", "Mark", ""], ["Barratt", "Dean C.", ""], ["Hu", "Yipeng", ""]]}, {"id": "2007.04973", "submitter": "Paras Jain", "authors": "Paras Jain, Ajay Jain, Tianjun Zhang, Pieter Abbeel, Joseph E.\n  Gonzalez, Ion Stoica", "title": "Contrastive Code Representation Learning", "comments": "Code available at https://github.com/parasj/contracode", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL cs.SE stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recent work learns contextual representations of source code by\nreconstructing tokens from their context. For downstream semantic understanding\ntasks like summarizing code in English, these representations should ideally\ncapture program functionality. However, we show that the popular\nreconstruction-based BERT model is sensitive to source code edits, even when\nthe edits preserve semantics. We propose ContraCode: a contrastive pre-training\ntask that learns code functionality, not form. ContraCode pre-trains a neural\nnetwork to identify functionally similar variants of a program among many\nnon-equivalent distractors. We scalably generate these variants using an\nautomated source-to-source compiler as a form of data augmentation. Contrastive\npre-training improves JavaScript summarization and TypeScript type inference\naccuracy by 2% to 13%. We also propose a new zero-shot JavaScript code clone\ndetection dataset, showing that ContraCode is both more robust and semantically\nmeaningful. On it, we outperform RoBERTa by 39% AUROC in an adversarial setting\nand up to 5% on natural code.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 17:59:06 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 05:30:35 GMT"}, {"version": "v3", "created": "Thu, 15 Apr 2021 17:58:44 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Jain", "Paras", ""], ["Jain", "Ajay", ""], ["Zhang", "Tianjun", ""], ["Abbeel", "Pieter", ""], ["Gonzalez", "Joseph E.", ""], ["Stoica", "Ion", ""]]}, {"id": "2007.04976", "submitter": "Deepak Pathak", "authors": "Wenlong Huang, Igor Mordatch, Deepak Pathak", "title": "One Policy to Control Them All: Shared Modular Policies for\n  Agent-Agnostic Control", "comments": "Accepted at ICML 2020. Videos and code at\n  https://huangwl18.github.io/modular-rl/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning is typically concerned with learning control policies\ntailored to a particular agent. We investigate whether there exists a single\nglobal policy that can generalize to control a wide variety of agent\nmorphologies -- ones in which even dimensionality of state and action spaces\nchanges. We propose to express this global policy as a collection of identical\nmodular neural networks, dubbed as Shared Modular Policies (SMP), that\ncorrespond to each of the agent's actuators. Every module is only responsible\nfor controlling its corresponding actuator and receives information from only\nits local sensors. In addition, messages are passed between modules,\npropagating information between distant modules. We show that a single modular\npolicy can successfully generate locomotion behaviors for several planar agents\nwith different skeletal structures such as monopod hoppers, quadrupeds, bipeds,\nand generalize to variants not seen during training -- a process that would\nnormally require training and manual hyperparameter tuning for each morphology.\nWe observe that a wide variety of drastically diverse locomotion styles across\nmorphologies as well as centralized coordination emerges via message passing\nbetween decentralized modules purely from the reinforcement learning objective.\nVideos and code at https://huangwl18.github.io/modular-rl/\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 17:59:35 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Huang", "Wenlong", ""], ["Mordatch", "Igor", ""], ["Pathak", "Deepak", ""]]}, {"id": "2007.05003", "submitter": "Florence Regol", "authors": "Florence Regol and Soumyasundar Pal and Yingxue Zhang and Mark Coates", "title": "Active Learning on Attributed Graphs via Graph Cognizant Logistic\n  Regression and Preemptive Query Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Node classification in attributed graphs is an important task in multiple\npractical settings, but it can often be difficult or expensive to obtain\nlabels. Active learning can improve the achieved classification performance for\na given budget on the number of queried labels. The best existing methods are\nbased on graph neural networks, but they often perform poorly unless a sizeable\nvalidation set of labelled nodes is available in order to choose good\nhyperparameters. We propose a novel graph-based active learning algorithm for\nthe task of node classification in attributed graphs; our algorithm uses graph\ncognizant logistic regression, equivalent to a linearized graph convolutional\nneural network (GCN), for the prediction phase and maximizes the expected error\nreduction in the query phase. To reduce the delay experienced by a labeller\ninteracting with the system, we derive a preemptive querying system that\ncalculates a new query during the labelling process, and to address the setting\nwhere learning starts with almost no labelled data, we also develop a hybrid\nalgorithm that performs adaptive model averaging of label propagation and\nlinearized GCN inference. We conduct experiments on five public benchmark\ndatasets, demonstrating a significant improvement over state-of-the-art\napproaches and illustrate the practical value of the method by applying it to a\nprivate microwave link network dataset.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 18:00:53 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Regol", "Florence", ""], ["Pal", "Soumyasundar", ""], ["Zhang", "Yingxue", ""], ["Coates", "Mark", ""]]}, {"id": "2007.05028", "submitter": "Li Wang", "authors": "Li Wang and Ren-Cang Li and Wen-Wei", "title": "Multi-view Orthonormalized Partial Least Squares: Regularizations and\n  Deep Extensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish a family of subspace-based learning method for multi-view\nlearning using the least squares as the fundamental basis. Specifically, we\ninvestigate orthonormalized partial least squares (OPLS) and study its\nimportant properties for both multivariate regression and classification.\nBuilding on the least squares reformulation of OPLS, we propose a unified\nmulti-view learning framework to learn a classifier over a common latent space\nshared by all views. The regularization technique is further leveraged to\nunleash the power of the proposed framework by providing three generic types of\nregularizers on its inherent ingredients including model parameters, decision\nvalues and latent projected points. We instantiate a set of regularizers in\nterms of various priors. The proposed framework with proper choices of\nregularizers not only can recast existing methods, but also inspire new models.\nTo further improve the performance of the proposed framework on complex real\nproblems, we propose to learn nonlinear transformations parameterized by deep\nnetworks. Extensive experiments are conducted to compare various methods on\nnine data sets with different numbers of views in terms of both feature\nextraction and cross-modal retrieval.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 19:00:39 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Wang", "Li", ""], ["Li", "Ren-Cang", ""], ["Wen-Wei", "", ""]]}, {"id": "2007.05033", "submitter": "Adarsh K. Jeewajee", "authors": "Adarsh K. Jeewajee, Leslie P. Kaelbling", "title": "Adversarially-learned Inference via an Ensemble of Discrete Undirected\n  Graphical Models", "comments": "17 pages, 5 figures, 5 tables. NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Undirected graphical models are compact representations of joint probability\ndistributions over random variables. To solve inference tasks of interest,\ngraphical models of arbitrary topology can be trained using empirical risk\nminimization. However, to solve inference tasks that were not seen during\ntraining, these models (EGMs) often need to be re-trained. Instead, we propose\nan inference-agnostic adversarial training framework which produces an\ninfinitely-large ensemble of graphical models (AGMs). The ensemble is optimized\nto generate data within the GAN framework, and inference is performed using a\nfinite subset of these models. AGMs perform comparably with EGMs on inference\ntasks that the latter were specifically optimized for. Most importantly, AGMs\nshow significantly better generalization to unseen inference tasks compared to\nEGMs, as well as deep neural architectures like GibbsNet and VAEAC which allow\narbitrary conditioning. Finally, AGMs allow fast data sampling, competitive\nwith Gibbs sampling from EGMs.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 19:13:36 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 22:51:27 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 05:05:01 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Jeewajee", "Adarsh K.", ""], ["Kaelbling", "Leslie P.", ""]]}, {"id": "2007.05034", "submitter": "Wentao Weng", "authors": "Wentao Weng, Harsh Gupta, Niao He, Lei Ying, R. Srikant", "title": "Provably-Efficient Double Q-Learning", "comments": "15 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we establish a theoretical comparison between the asymptotic\nmean-squared error of Double Q-learning and Q-learning. Our result builds upon\nan analysis for linear stochastic approximation based on Lyapunov equations and\napplies to both tabular setting and with linear function approximation,\nprovided that the optimal policy is unique and the algorithms converge. We show\nthat the asymptotic mean-squared error of Double Q-learning is exactly equal to\nthat of Q-learning if Double Q-learning uses twice the learning rate of\nQ-learning and outputs the average of its two estimators. We also present some\npractical implications of this theoretical observation using simulations.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 19:15:17 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Weng", "Wentao", ""], ["Gupta", "Harsh", ""], ["He", "Niao", ""], ["Ying", "Lei", ""], ["Srikant", "R.", ""]]}, {"id": "2007.05042", "submitter": "Alaa Othman", "authors": "Alaa Tharwat", "title": "Behavioral analysis of support vector machine classifier with Gaussian\n  kernel and imbalanced data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The parameters of support vector machines (SVMs) such as the penalty\nparameter and the kernel parameters have a great impact on the classification\naccuracy and the complexity of the SVM model. Therefore, the model selection in\nSVM involves the tuning of these parameters. However, these parameters are\nusually tuned and used as a black box, without understanding the mathematical\nbackground or internal details. In this paper, the behavior of the SVM\nclassification model is analyzed when these parameters take different values\nwith balanced and imbalanced data. This analysis including visualization,\nmathematical and geometrical interpretations and illustrative numerical\nexamples with the aim of providing the basics of the Gaussian and linear kernel\nfunctions with SVM. From this analysis, we proposed a novel search algorithm.\nIn this algorithm, we search for the optimal SVM parameters into two\none-dimensional spaces instead of searching into one two-dimensional space.\nThis reduces the computational time significantly. Moreover, in our algorithm,\nfrom the analysis of the data, the range of kernel function can be expected.\nThis also reduces the search space and hence reduces the required computational\ntime. Different experiments were conducted to evaluate our search algorithm\nusing different balanced and imbalanced datasets. The results demonstrated how\nthe proposed strategy is fast and effective than other searching strategies.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 19:28:25 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Tharwat", "Alaa", ""]]}, {"id": "2007.05052", "submitter": "Zachary Fisher", "authors": "Zachary F. Fisher and Younghoon Kim and Barbara Fredrickson and Vladas\n  Pipiras", "title": "Penalized Estimation and Forecasting of Multiple Subject Intensive\n  Longitudinal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Intensive Longitudinal Data (ILD) is an increasingly common data type in the\nsocial and behavioral sciences. Despite the many benefits these data provide,\nlittle work has been dedicated to realizing the potential such data hold for\nforecasting dynamic processes at the individual level. To address this gap in\nthe literature we present the multi-VAR framework, a novel methodological\napproach for penalized estimation and forecasting of ILD collected from\nmultiple individuals. Importantly, our approach estimates models for all\nindividuals simultaneously and is capable of adaptively adjusting to the amount\nof heterogeneity exhibited across individual dynamic processes. To accomplish\nthis we propose proximal gradient descent algorithm for solving the multi-VAR\nproblem and prove the consistency of the recovered transition matrices. We\nevaluate the forecasting performance of our method in comparison with a number\nof benchmark forecasting methods and provide an illustrative example involving\nthe day-to-day emotional experiences of 16 individuals over an 11-week period.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 20:34:23 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Fisher", "Zachary F.", ""], ["Kim", "Younghoon", ""], ["Fredrickson", "Barbara", ""], ["Pipiras", "Vladas", ""]]}, {"id": "2007.05057", "submitter": "Tom Lovett", "authors": "Tom Lovett, Mark Briers, Marcos Charalambides, Radka Jersakova, James\n  Lomax and Chris Holmes", "title": "Inferring proximity from Bluetooth Low Energy RSSI with Unscented Kalman\n  Smoothers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Covid-19 pandemic has resulted in a variety of approaches for managing\ninfection outbreaks in international populations. One example is mobile phone\napplications, which attempt to alert infected individuals and their contacts by\nautomatically inferring two key components of infection risk: the proximity to\nan individual who may be infected, and the duration of proximity. The former\ncomponent, proximity, relies on Bluetooth Low Energy (BLE) Received Signal\nStrength Indicator(RSSI) as a distance sensor, and this has been shown to be\nproblematic; not least because of unpredictable variations caused by different\ndevice types, device location on-body, device orientation, the local\nenvironment and the general noise associated with radio frequency propagation.\nIn this paper, we present an approach that infers posterior probabilities over\ndistance given sequences of RSSI values. Using a single-dimensional Unscented\nKalman Smoother (UKS) for non-linear state space modelling, we outline several\nGaussian process observation transforms, including: a generative model that\ndirectly captures sources of variation; and a discriminative model that learns\na suitable observation function from training data using both distance and\ninfection risk as optimisation objective functions. Our results show that good\nrisk prediction can be achieved in $\\mathcal{O}(n)$ time on real-world data\nsets, with the UKS outperforming more traditional classification methods\nlearned from the same training data.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 20:47:02 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Lovett", "Tom", ""], ["Briers", "Mark", ""], ["Charalambides", "Marcos", ""], ["Jersakova", "Radka", ""], ["Lomax", "James", ""], ["Holmes", "Chris", ""]]}, {"id": "2007.05073", "submitter": "Keshav Vemuri", "authors": "Keshav Vemuri, Nathan Srebro", "title": "Predictive Value Generalization Bounds", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a bi-criterion framework for assessing scoring\nfunctions in the context of binary classification. The positive and negative\npredictive values (ppv and npv, respectively) are conditional probabilities of\nthe true label matching a classifier's predicted label. The usual\nclassification error rate is a linear combination of these probabilities, and\ntherefore, concentration inequalities for the error rate do not yield\nconfidence intervals for the two separate predictive values. We study\ngeneralization properties of scoring functions with respect to predictive\nvalues by deriving new distribution-free large deviation and uniform\nconvergence bounds. The latter bound is stated in terms of a measure of\nfunction class complexity that we call the order coefficient; we relate this\ncombinatorial quantity to the VC-subgraph dimension.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 21:23:28 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Vemuri", "Keshav", ""], ["Srebro", "Nathan", ""]]}, {"id": "2007.05074", "submitter": "Boumediene Hamzi", "authors": "Boumediene Hamzi and Houman Owhadi", "title": "Learning dynamical systems from data: a simple cross-validation\n  perspective", "comments": "File uploaded on arxiv on Sunday, July 5th, 2020. Got delayed due to\n  tex problems on ArXiv. Original version at\n  https://www.researchgate.net/publication/342693818_Learning_dynamical_systems_from_data_a_simple_cross-validation_perspective", "journal-ref": null, "doi": "10.1016/j.physd.2020.132817", "report-no": null, "categories": "cs.LG math.DS nlin.CD stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regressing the vector field of a dynamical system from a finite number of\nobserved states is a natural way to learn surrogate models for such systems. We\npresent variants of cross-validation (Kernel Flows \\cite{Owhadi19} and its\nvariants based on Maximum Mean Discrepancy and Lyapunov exponents) as simple\napproaches for learning the kernel used in these emulators.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 21:26:09 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Hamzi", "Boumediene", ""], ["Owhadi", "Houman", ""]]}, {"id": "2007.05078", "submitter": "Omar Darwiche Domingues", "authors": "Omar Darwiche Domingues, Pierre M\\'enard, Matteo Pirotta, Emilie\n  Kaufmann, Michal Valko", "title": "A Kernel-Based Approach to Non-Stationary Reinforcement Learning in\n  Metric Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose KeRNS: an algorithm for episodic reinforcement\nlearning in non-stationary Markov Decision Processes (MDPs) whose state-action\nset is endowed with a metric. Using a non-parametric model of the MDP built\nwith time-dependent kernels, we prove a regret bound that scales with the\ncovering dimension of the state-action space and the total variation of the MDP\nwith time, which quantifies its level of non-stationarity. Our method\ngeneralizes previous approaches based on sliding windows and exponential\ndiscounting used to handle changing environments. We further propose a\npractical implementation of KeRNS, we analyze its regret and validate it\nexperimentally.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 21:37:13 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Domingues", "Omar Darwiche", ""], ["M\u00e9nard", "Pierre", ""], ["Pirotta", "Matteo", ""], ["Kaufmann", "Emilie", ""], ["Valko", "Michal", ""]]}, {"id": "2007.05084", "submitter": "Kartik Sreenivasan", "authors": "Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma,\n  Saurabh Agarwal, Jy-yong Sohn, Kangwook Lee, Dimitris Papailiopoulos", "title": "Attack of the Tails: Yes, You Really Can Backdoor Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its decentralized nature, Federated Learning (FL) lends itself to\nadversarial attacks in the form of backdoors during training. The goal of a\nbackdoor is to corrupt the performance of the trained model on specific\nsub-tasks (e.g., by classifying green cars as frogs). A range of FL backdoor\nattacks have been introduced in the literature, but also methods to defend\nagainst them, and it is currently an open question whether FL systems can be\ntailored to be robust against backdoors. In this work, we provide evidence to\nthe contrary. We first establish that, in the general case, robustness to\nbackdoors implies model robustness to adversarial examples, a major open\nproblem in itself. Furthermore, detecting the presence of a backdoor in a FL\nmodel is unlikely assuming first order oracles or polynomial time. We couple\nour theoretical results with a new family of backdoor attacks, which we refer\nto as edge-case backdoors. An edge-case backdoor forces a model to misclassify\non seemingly easy inputs that are however unlikely to be part of the training,\nor test data, i.e., they live on the tail of the input distribution. We explain\nhow these edge-case backdoors can lead to unsavory failures and may have\nserious repercussions on fairness, and exhibit that with careful tuning at the\nside of the adversary, one can insert them across a range of machine learning\ntasks (e.g., image classification, OCR, text prediction, sentiment analysis).\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 21:50:54 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Wang", "Hongyi", ""], ["Sreenivasan", "Kartik", ""], ["Rajput", "Shashank", ""], ["Vishwakarma", "Harit", ""], ["Agarwal", "Saurabh", ""], ["Sohn", "Jy-yong", ""], ["Lee", "Kangwook", ""], ["Papailiopoulos", "Dimitris", ""]]}, {"id": "2007.05086", "submitter": "Yaoqing Yang", "authors": "Yaoqing Yang, Rajiv Khanna, Yaodong Yu, Amir Gholami, Kurt Keutzer,\n  Joseph E. Gonzalez, Kannan Ramchandran, Michael W. Mahoney", "title": "Boundary thickness and robustness in learning models", "comments": null, "journal-ref": "NeurIPS 2020", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robustness of machine learning models to various adversarial and\nnon-adversarial corruptions continues to be of interest. In this paper, we\nintroduce the notion of the boundary thickness of a classifier, and we describe\nits connection with and usefulness for model robustness. Thick decision\nboundaries lead to improved performance, while thin decision boundaries lead to\noverfitting (e.g., measured by the robust generalization gap between training\nand testing) and lower robustness. We show that a thicker boundary helps\nimprove robustness against adversarial examples (e.g., improving the robust\ntest accuracy of adversarial training) as well as so-called out-of-distribution\n(OOD) transforms, and we show that many commonly-used regularization and data\naugmentation procedures can increase boundary thickness. On the theoretical\nside, we establish that maximizing boundary thickness during training is akin\nto the so-called mixup training. Using these observations, we show that\nnoise-augmentation on mixup training further increases boundary thickness,\nthereby combating vulnerability to various forms of adversarial attacks and OOD\ntransforms. We can also show that the performance improvement in several lines\nof recent work happens in conjunction with a thicker boundary.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 21:56:35 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 19:54:22 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Yang", "Yaoqing", ""], ["Khanna", "Rajiv", ""], ["Yu", "Yaodong", ""], ["Gholami", "Amir", ""], ["Keutzer", "Kurt", ""], ["Gonzalez", "Joseph E.", ""], ["Ramchandran", "Kannan", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "2007.05089", "submitter": "Laurens van der Maaten", "authors": "Laurens van der Maaten and Awni Hannun", "title": "The Trade-Offs of Private Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning models leak information about their training data every time\nthey reveal a prediction. This is problematic when the training data needs to\nremain private. Private prediction methods limit how much information about the\ntraining data is leaked by each prediction. Private prediction can also be\nachieved using models that are trained by private training methods. In private\nprediction, both private training and private prediction methods exhibit\ntrade-offs between privacy, privacy failure probability, amount of training\ndata, and inference budget. Although these trade-offs are theoretically\nwell-understood, they have hardly been studied empirically. This paper presents\nthe first empirical study into the trade-offs of private prediction. Our study\nsheds light on which methods are best suited for which learning setting.\nPerhaps surprisingly, we find private training methods outperform private\nprediction methods in a wide range of private prediction settings.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 22:02:37 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["van der Maaten", "Laurens", ""], ["Hannun", "Awni", ""]]}, {"id": "2007.05096", "submitter": "Quinlan Sykora", "authors": "Quinlan Sykora, Mengye Ren, Raquel Urtasun", "title": "Multi-Agent Routing Value Iteration Network", "comments": "Published at ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we tackle the problem of routing multiple agents in a\ncoordinated manner. This is a complex problem that has a wide range of\napplications in fleet management to achieve a common goal, such as mapping from\na swarm of robots and ride sharing. Traditional methods are typically not\ndesigned for realistic environments hich contain sparsely connected graphs and\nunknown traffic, and are often too slow in runtime to be practical. In\ncontrast, we propose a graph neural network based model that is able to perform\nmulti-agent routing based on learned value iteration in a sparsely connected\ngraph with dynamically changing traffic conditions. Moreover, our learned\ncommunication module enables the agents to coordinate online and adapt to\nchanges more effectively. We created a simulated environment to mimic realistic\nmapping performed by autonomous vehicles with unknown minimum edge coverage and\ntraffic conditions; our approach significantly outperforms traditional solvers\nboth in terms of total cost and runtime. We also show that our model trained\nwith only two agents on graphs with a maximum of 25 nodes can easily generalize\nto situations with more agents and/or nodes.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 22:16:45 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 18:08:25 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Sykora", "Quinlan", ""], ["Ren", "Mengye", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2007.05100", "submitter": "Yuke Wang", "authors": "Boyuan Feng, Yuke Wang, Xu Li, Shu Yang, Xueqiao Peng, and Yufei Ding", "title": "SGQuant: Squeezing the Last Bit on Graph Neural Networks with\n  Specialized Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing popularity of graph-based learning, Graph Neural Networks\n(GNNs) win lots of attention from the research and industry field because of\ntheir high accuracy. However, existing GNNs suffer from high memory footprints\n(e.g., node embedding features). This high memory footprint hurdles the\npotential applications towards memory-constrained devices, such as the\nwidely-deployed IoT devices. To this end, we propose a specialized GNN\nquantization scheme, SGQuant, to systematically reduce the GNN memory\nconsumption. Specifically, we first propose a GNN-tailored quantization\nalgorithm design and a GNN quantization fine-tuning scheme to reduce memory\nconsumption while maintaining accuracy. Then, we investigate the\nmulti-granularity quantization strategy that operates at different levels\n(components, graph topology, and layers) of GNN computation. Moreover, we offer\nan automatic bit-selecting (ABS) to pinpoint the most appropriate quantization\nbits for the above multi-granularity quantizations. Intensive experiments show\nthat SGQuant can effectively reduce the memory footprint from 4.25x to 31.9x\ncompared with the original full-precision GNNs while limiting the accuracy drop\nto 0.4% on average.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 22:42:34 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 07:13:58 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Feng", "Boyuan", ""], ["Wang", "Yuke", ""], ["Li", "Xu", ""], ["Yang", "Shu", ""], ["Peng", "Xueqiao", ""], ["Ding", "Yufei", ""]]}, {"id": "2007.05105", "submitter": "Tyler Johnson", "authors": "Tyler B. Johnson, Pulkit Agrawal, Haijie Gu, Carlos Guestrin", "title": "AdaScale SGD: A User-Friendly Algorithm for Distributed Training", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When using large-batch training to speed up stochastic gradient descent,\nlearning rates must adapt to new batch sizes in order to maximize speed-ups and\npreserve model quality. Re-tuning learning rates is resource intensive, while\nfixed scaling rules often degrade model quality. We propose AdaScale SGD, an\nalgorithm that reliably adapts learning rates to large-batch training. By\ncontinually adapting to the gradient's variance, AdaScale automatically\nachieves speed-ups for a wide range of batch sizes. We formally describe this\nquality with AdaScale's convergence bound, which maintains final objective\nvalues, even as batch sizes grow large and the number of iterations decreases.\nIn empirical comparisons, AdaScale trains well beyond the batch size limits of\npopular \"linear learning rate scaling\" rules. This includes large-batch\ntraining with no model degradation for machine translation, image\nclassification, object detection, and speech recognition tasks. AdaScale's\nqualitative behavior is similar to that of \"warm-up\" heuristics, but unlike\nwarm-up, this behavior emerges naturally from a principled mechanism. The\nalgorithm introduces negligible computational overhead and no new\nhyperparameters, making AdaScale an attractive choice for large-scale training\nin practice.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 23:26:13 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Johnson", "Tyler B.", ""], ["Agrawal", "Pulkit", ""], ["Gu", "Haijie", ""], ["Guestrin", "Carlos", ""]]}, {"id": "2007.05120", "submitter": "Joshua Bridge", "authors": "Joshua Bridge, Simon P. Harding, Yalin Zheng", "title": "Development and Validation of a Novel Prognostic Model for Predicting\n  AMD Progression Using Longitudinal Fundus Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prognostic models aim to predict the future course of a disease or condition\nand are a vital component of personalized medicine. Statistical models make use\nof longitudinal data to capture the temporal aspect of disease progression;\nhowever, these models require prior feature extraction. Deep learning avoids\nexplicit feature extraction, meaning we can develop models for images where\nfeatures are either unknown or impossible to quantify accurately. Previous\nprognostic models using deep learning with imaging data require annotation\nduring training or only utilize a single time point. We propose a novel deep\nlearning method to predict the progression of diseases using longitudinal\nimaging data with uneven time intervals, which requires no prior feature\nextraction. Given previous images from a patient, our method aims to predict\nwhether the patient will progress onto the next stage of the disease. The\nproposed method uses InceptionV3 to produce feature vectors for each image. In\norder to account for uneven intervals, a novel interval scaling is proposed.\nFinally, a Recurrent Neural Network is used to prognosticate the disease. We\ndemonstrate our method on a longitudinal dataset of color fundus images from\n4903 eyes with age-related macular degeneration (AMD), taken from the\nAge-Related Eye Disease Study, to predict progression to late AMD. Our method\nattains a testing sensitivity of 0.878, a specificity of 0.887, and an area\nunder the receiver operating characteristic of 0.950. We compare our method to\nprevious methods, displaying superior performance in our model. Class\nactivation maps display how the network reaches the final decision.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 00:33:19 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Bridge", "Joshua", ""], ["Harding", "Simon P.", ""], ["Zheng", "Yalin", ""]]}, {"id": "2007.05123", "submitter": "Tuan Anh Bui", "authors": "Anh Bui, Trung Le, He Zhao, Paul Montague, Olivier deVel, Tamas\n  Abraham, Dinh Phung", "title": "Improving Adversarial Robustness by Enforcing Local and Global\n  Compactness", "comments": "Proceeding of the European Conference on Computer Vision (ECCV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fact that deep neural networks are susceptible to crafted perturbations\nseverely impacts the use of deep learning in certain domains of application.\nAmong many developed defense models against such attacks, adversarial training\nemerges as the most successful method that consistently resists a wide range of\nattacks. In this work, based on an observation from a previous study that the\nrepresentations of a clean data example and its adversarial examples become\nmore divergent in higher layers of a deep neural net, we propose the Adversary\nDivergence Reduction Network which enforces local/global compactness and the\nclustering assumption over an intermediate layer of a deep neural network. We\nconduct comprehensive experiments to understand the isolating behavior of each\ncomponent (i.e., local/global compactness and the clustering assumption) and\ncompare our proposed model with state-of-the-art adversarial training methods.\nThe experimental results demonstrate that augmenting adversarial training with\nour proposed components can further improve the robustness of the network,\nleading to higher unperturbed and adversarial predictive performances.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 00:43:06 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Bui", "Anh", ""], ["Le", "Trung", ""], ["Zhao", "He", ""], ["Montague", "Paul", ""], ["deVel", "Olivier", ""], ["Abraham", "Tamas", ""], ["Phung", "Dinh", ""]]}, {"id": "2007.05134", "submitter": "Shreyas Padhy", "authors": "Shreyas Padhy, Zachary Nado, Jie Ren, Jeremiah Liu, Jasper Snoek,\n  Balaji Lakshminarayanan", "title": "Revisiting One-vs-All Classifiers for Predictive Uncertainty and\n  Out-of-Distribution Detection in Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate estimation of predictive uncertainty in modern neural networks is\ncritical to achieve well calibrated predictions and detect out-of-distribution\n(OOD) inputs. The most promising approaches have been predominantly focused on\nimproving model uncertainty (e.g. deep ensembles and Bayesian neural networks)\nand post-processing techniques for OOD detection (e.g. ODIN and Mahalanobis\ndistance). However, there has been relatively little investigation into how the\nparametrization of the probabilities in discriminative classifiers affects the\nuncertainty estimates, and the dominant method, softmax cross-entropy, results\nin misleadingly high confidences on OOD data and under covariate shift. We\ninvestigate alternative ways of formulating probabilities using (1) a\none-vs-all formulation to capture the notion of \"none of the above\", and (2) a\ndistance-based logit representation to encode uncertainty as a function of\ndistance to the training manifold. We show that one-vs-all formulations can\nimprove calibration on image classification tasks, while matching the\npredictive performance of softmax without incurring any additional training or\ntest-time complexity.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 01:55:02 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Padhy", "Shreyas", ""], ["Nado", "Zachary", ""], ["Ren", "Jie", ""], ["Liu", "Jeremiah", ""], ["Snoek", "Jasper", ""], ["Lakshminarayanan", "Balaji", ""]]}, {"id": "2007.05145", "submitter": "Omar Montasser", "authors": "Shafi Goldwasser, Adam Tauman Kalai, Yael Tauman Kalai, and Omar\n  Montasser", "title": "Beyond Perturbations: Learning Guarantees with Arbitrary Adversarial\n  Test Examples", "comments": "To appear in NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a transductive learning algorithm that takes as input training\nexamples from a distribution $P$ and arbitrary (unlabeled) test examples,\npossibly chosen by an adversary. This is unlike prior work that assumes that\ntest examples are small perturbations of $P$. Our algorithm outputs a selective\nclassifier, which abstains from predicting on some examples. By considering\nselective transductive learning, we give the first nontrivial guarantees for\nlearning classes of bounded VC dimension with arbitrary train and test\ndistributions---no prior guarantees were known even for simple classes of\nfunctions such as intervals on the line. In particular, for any function in a\nclass $C$ of bounded VC dimension, we guarantee a low test error rate and a low\nrejection rate with respect to $P$. Our algorithm is efficient given an\nEmpirical Risk Minimizer (ERM) for $C$. Our guarantees hold even for test\nexamples chosen by an unbounded white-box adversary. We also give guarantees\nfor generalization, agnostic, and unsupervised settings.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 03:00:12 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 20:11:20 GMT"}, {"version": "v3", "created": "Wed, 30 Sep 2020 04:26:27 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Goldwasser", "Shafi", ""], ["Kalai", "Adam Tauman", ""], ["Kalai", "Yael Tauman", ""], ["Montasser", "Omar", ""]]}, {"id": "2007.05157", "submitter": "Audra McMillan", "authors": "Daniel Alabi, Audra McMillan, Jayshree Sarathy, Adam Smith and Salil\n  Vadhan", "title": "Differentially Private Simple Linear Regression", "comments": "20 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Economics and social science research often require analyzing datasets of\nsensitive personal information at fine granularity, with models fit to small\nsubsets of the data. Unfortunately, such fine-grained analysis can easily\nreveal sensitive individual information. We study algorithms for simple linear\nregression that satisfy differential privacy, a constraint which guarantees\nthat an algorithm's output reveals little about any individual input data\nrecord, even to an attacker with arbitrary side information about the dataset.\nWe consider the design of differentially private algorithms for simple linear\nregression for small datasets, with tens to hundreds of datapoints, which is a\nparticularly challenging regime for differential privacy. Focusing on a\nparticular application to small-area analysis in economics research, we study\nthe performance of a spectrum of algorithms we adapt to the setting. We\nidentify key factors that affect their performance, showing through a range of\nexperiments that algorithms based on robust estimators (in particular, the\nTheil-Sen estimator) perform well on the smallest datasets, but that other more\nstandard algorithms do better as the dataset size increases.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 04:28:43 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Alabi", "Daniel", ""], ["McMillan", "Audra", ""], ["Sarathy", "Jayshree", ""], ["Smith", "Adam", ""], ["Vadhan", "Salil", ""]]}, {"id": "2007.05166", "submitter": "Ifigeneia Apostolopoulou Ms", "authors": "Ifigeneia Apostolopoulou, Elan Rosenfeld, Artur Dubrawski", "title": "Self-Reflective Variational Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Variational Autoencoder (VAE) is a powerful framework for learning\nprobabilistic latent variable generative models. However, typical assumptions\non the approximate posterior distribution of the encoder and/or the prior,\nseriously restrict its capacity for inference and generative modeling.\nVariational inference based on neural autoregressive models respects the\nconditional dependencies of the exact posterior, but this flexibility comes at\na cost: such models are expensive to train in high-dimensional regimes and can\nbe slow to produce samples. In this work, we introduce an orthogonal solution,\nwhich we call self-reflective inference. By redesigning the hierarchical\nstructure of existing VAE architectures, self-reflection ensures that the\nstochastic flow preserves the factorization of the exact posterior,\nsequentially updating the latent codes in a recurrent manner consistent with\nthe generative model. We empirically demonstrate the clear advantages of\nmatching the variational posterior to the exact posterior - on binarized MNIST,\nself-reflective inference achieves state-of-the art performance without\nresorting to complex, computationally expensive components such as\nautoregressive layers. Moreover, we design a variational normalizing flow that\nemploys the proposed architecture, yielding predictive benefits compared to its\npurely generative counterpart. Our proposed modification is quite general and\ncomplements the existing literature; self-reflective inference can naturally\nleverage advances in distribution estimation and generative modeling to improve\nthe capacity of each layer in the hierarchy.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 05:05:26 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Apostolopoulou", "Ifigeneia", ""], ["Rosenfeld", "Elan", ""], ["Dubrawski", "Artur", ""]]}, {"id": "2007.05169", "submitter": "Rachit Agarwal", "authors": "Rachit Agarwal, Shikhar Barve, Sandeep K. Shukla", "title": "Detecting Malicious Accounts in Permissionless Blockchains using\n  Temporal Graph Properties", "comments": "Submitted to Springer Applied Network Science Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The temporal nature of modeling accounts as nodes and transactions as\ndirected edges in a directed graph -- for a blockchain, enables us to\nunderstand the behavior (malicious or benign) of the accounts. Predictive\nclassification of accounts as malicious or benign could help users of the\npermissionless blockchain platforms to operate in a secure manner. Motivated by\nthis, we introduce temporal features such as burst and attractiveness on top of\nseveral already used graph properties such as the node degree and clustering\ncoefficient. Using identified features, we train various Machine Learning (ML)\nalgorithms and identify the algorithm that performs the best in detecting which\naccounts are malicious. We then study the behavior of the accounts over\ndifferent temporal granularities of the dataset before assigning them malicious\ntags. For Ethereum blockchain, we identify that for the entire dataset - the\nExtraTreesClassifier performs the best among supervised ML algorithms. On the\nother hand, using cosine similarity on top of the results provided by\nunsupervised ML algorithms such as K-Means on the entire dataset, we were able\nto detect 554 more suspicious accounts. Further, using behavior change analysis\nfor accounts, we identify 814 unique suspicious accounts across different\ntemporal granularities.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 05:15:26 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 10:58:41 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Agarwal", "Rachit", ""], ["Barve", "Shikhar", ""], ["Shukla", "Sandeep K.", ""]]}, {"id": "2007.05181", "submitter": "Yongseok Choi", "authors": "Yunho Jeon, Yongseok Choi, Jaesun Park, Subin Yi, Dongyeon Cho, Jiwon\n  Kim", "title": "Sample-based Regularization: A Transfer Learning Strategy Toward Better\n  Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a deep neural network with a small amount of data is a challenging\nproblem as it is vulnerable to overfitting. However, one of the practical\ndifficulties that we often face is to collect many samples. Transfer learning\nis a cost-effective solution to this problem. By using the source model trained\nwith a large-scale dataset, the target model can alleviate the overfitting\noriginated from the lack of training data. Resorting to the ability of\ngeneralization of the source model, several methods proposed to use the source\nknowledge during the whole training procedure. However, this is likely to\nrestrict the potential of the target model and some transferred knowledge from\nthe source can interfere with the training procedure. For improving the\ngeneralization performance of the target model with a few training samples, we\nproposed a regularization method called sample-based regularization (SBR),\nwhich does not rely on the source's knowledge during training. With SBR, we\nsuggested a new training framework for transfer learning. Experimental results\nshowed that our framework outperformed existing methods in various\nconfigurations.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 06:02:05 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Jeon", "Yunho", ""], ["Choi", "Yongseok", ""], ["Park", "Jaesun", ""], ["Yi", "Subin", ""], ["Cho", "Dongyeon", ""], ["Kim", "Jiwon", ""]]}, {"id": "2007.05188", "submitter": "Hang Miao", "authors": "Hang Miao, Kui Zhao, Zhun Wang, Linbo Jiang, Quanhui Jia, Yanming\n  Fang, Quan Yu", "title": "Intelligent Credit Limit Management in Consumer Loans Based on Causal\n  Inference", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE econ.EM stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Nowadays consumer loan plays an important role in promoting the economic\ngrowth, and credit cards are the most popular consumer loan. One of the most\nessential parts in credit cards is the credit limit management. Traditionally,\ncredit limits are adjusted based on limited heuristic strategies, which are\ndeveloped by experienced professionals. In this paper, we present a data-driven\napproach to manage the credit limit intelligently. Firstly, a conditional\nindependence testing is conducted to acquire the data for building models.\nBased on these testing data, a response model is then built to measure the\nheterogeneous treatment effect of increasing credit limits (i.e. treatments)\nfor different customers, who are depicted by several control variables (i.e.\nfeatures). In order to incorporate the diminishing marginal effect, a carefully\nselected log transformation is introduced to the treatment variable. Moreover,\nthe model's capability can be further enhanced by applying a non-linear\ntransformation on features via GBDT encoding. Finally, a well-designed metric\nis proposed to properly measure the performances of compared methods. The\nexperimental results demonstrate the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 06:22:44 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Miao", "Hang", ""], ["Zhao", "Kui", ""], ["Wang", "Zhun", ""], ["Jiang", "Linbo", ""], ["Jia", "Quanhui", ""], ["Fang", "Yanming", ""], ["Yu", "Quan", ""]]}, {"id": "2007.05189", "submitter": "Kamil Nar", "authors": "Kamil Nar, Yuan Xue, Andrew M. Dai", "title": "Learning Unstable Dynamical Systems with Time-Weighted Logarithmic Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When training the parameters of a linear dynamical model, the gradient\ndescent algorithm is likely to fail to converge if the squared-error loss is\nused as the training loss function. Restricting the parameter space to a\nsmaller subset and running the gradient descent algorithm within this subset\ncan allow learning stable dynamical systems, but this strategy does not work\nfor unstable systems. In this work, we look into the dynamics of the gradient\ndescent algorithm and pinpoint what causes the difficulty of learning unstable\nsystems. We show that observations taken at different times from the system to\nbe learned influence the dynamics of the gradient descent algorithm in\nsubstantially different degrees. We introduce a time-weighted logarithmic loss\nfunction to fix this imbalance and demonstrate its effectiveness in learning\nunstable systems.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 06:28:05 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Nar", "Kamil", ""], ["Xue", "Yuan", ""], ["Dai", "Andrew M.", ""]]}, {"id": "2007.05196", "submitter": "Matthias Hutsebaut-Buysse", "authors": "Matthias Hutsebaut-Buysse, Kevin Mets, Steven Latr\\'e", "title": "Pre-trained Word Embeddings for Goal-conditional Transfer Learning in\n  Reinforcement Learning", "comments": "Paper accepted to the ICML 2020 Language in Reinforcement Learning\n  (LaReL) Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) algorithms typically start tabula rasa, without\nany prior knowledge of the environment, and without any prior skills. This\nhowever often leads to low sample efficiency, requiring a large amount of\ninteraction with the environment. This is especially true in a lifelong\nlearning setting, in which the agent needs to continually extend its\ncapabilities. In this paper, we examine how a pre-trained task-independent\nlanguage model can make a goal-conditional RL agent more sample efficient. We\ndo this by facilitating transfer learning between different related tasks. We\nexperimentally demonstrate our approach on a set of object navigation tasks.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 06:42:00 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Hutsebaut-Buysse", "Matthias", ""], ["Mets", "Kevin", ""], ["Latr\u00e9", "Steven", ""]]}, {"id": "2007.05205", "submitter": "Jong Chul Ye", "authors": "Jaeyoung Huh, Shujaat Khan, and Jong Chul Ye", "title": "OT-driven Multi-Domain Unsupervised Ultrasound Image Artifact Removal\n  using a Single CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound imaging (US) often suffers from distinct image artifacts from\nvarious sources. Classic approaches for solving these problems are usually\nmodel-based iterative approaches that have been developed specifically for each\ntype of artifact, which are often computationally intensive. Recently, deep\nlearning approaches have been proposed as computationally efficient and high\nperformance alternatives. Unfortunately, in the current deep learning\napproaches, a dedicated neural network should be trained with matched training\ndata for each specific artifact type. This poses a fundamental limitation in\nthe practical use of deep learning for US, since large number of models should\nbe stored to deal with various US image artifacts. Inspired by the recent\nsuccess of multi-domain image transfer, here we propose a novel, unsupervised,\ndeep learning approach in which a single neural network can be used to deal\nwith different types of US artifacts simply by changing a mask vector that\nswitches between different target domains. Our algorithm is rigorously derived\nusing an optimal transport (OT) theory for cascaded probability measures.\nExperimental results using phantom and in vivo data demonstrate that the\nproposed method can generate high quality image by removing distinct artifacts,\nwhich are comparable to those obtained by separately trained multiple neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 07:11:04 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Huh", "Jaeyoung", ""], ["Khan", "Shujaat", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2007.05216", "submitter": "Sajan Kedia", "authors": "Sajan Kedia, Samyak Jain, Abhishek Sharma", "title": "Price Optimization in Fashion E-commerce", "comments": "8 pages, 6 figures, AI for fashion supply chain Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth in the fashion e-commerce industry, it is becoming\nextremely challenging for the E-tailers to set an optimal price point for all\nthe products on the platform. By establishing an optimal price point, they can\nmaximize overall revenue and profit for the platform. In this paper, we propose\na novel machine learning and optimization technique to find the optimal price\npoint at an individual product level. It comprises three major components.\nFirstly, we use a demand prediction model to predict the next day demand for\neach product at a certain discount percentage. Next step, we use the concept of\nprice elasticity of demand to get the multiple demand values by varying the\ndiscount percentage. Thus we obtain multiple price demand pairs for each\nproduct and we have to choose one of them for the live platform. Typically\nfashion e-commerce has millions of products, so there can be many permutations.\nEach permutation will assign a unique price point for all the products, which\nwill sum up to a unique revenue number. To choose the best permutation which\ngives maximum revenue, a linear programming optimization technique is used. We\nhave deployed the above methods in the live production environment and\nconducted several AB tests. According to the AB test result, our model is\nimproving the revenue by 1 percent and gross margin by 0.81 percent.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 07:40:28 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 10:18:53 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Kedia", "Sajan", ""], ["Jain", "Samyak", ""], ["Sharma", "Abhishek", ""]]}, {"id": "2007.05239", "submitter": "Kai Bergermann", "authors": "Kai Bergermann, Martin Stoll, Toni Volkmer", "title": "Semi-supervised Learning for Aggregated Multilayer Graphs Using Diffuse\n  Interface Methods and Fast Matrix Vector Products", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize a graph-based multiclass semi-supervised classification\ntechnique based on diffuse interface methods to multilayer graphs. Besides the\ntreatment of various applications with an inherent multilayer structure, we\npresent a very flexible approach that interprets high-dimensional data in a\nlow-dimensional multilayer graph representation. Highly efficient numerical\nmethods involving the spectral decomposition of the corresponding differential\ngraph operators as well as fast matrix-vector products based on the\nnonequispaced fast Fourier transform (NFFT) enable the rapid treatment of large\nand high-dimensional data sets. We perform various numerical tests putting a\nspecial focus on image segmentation. In particular, we test the performance of\nour method on data sets with up to 10 million nodes per layer as well as up to\n104 dimensions resulting in graphs with up to 52 layers. While all presented\nnumerical experiments can be run on an average laptop computer, the linear\ndependence per iteration step of the runtime on the network size in all stages\nof our algorithm makes it scalable to even larger and higher-dimensional\nproblems.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 08:29:11 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 13:43:49 GMT"}, {"version": "v3", "created": "Fri, 12 Mar 2021 13:15:50 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Bergermann", "Kai", ""], ["Stoll", "Martin", ""], ["Volkmer", "Toni", ""]]}, {"id": "2007.05270", "submitter": "Edward Beeching", "authors": "Edward Beeching and Jilles Dibangoye and Olivier Simonin and Christian\n  Wolf", "title": "Learning to plan with uncertain topological maps", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We train an agent to navigate in 3D environments using a hierarchical\nstrategy including a high-level graph based planner and a local policy. Our\nmain contribution is a data driven learning based approach for planning under\nuncertainty in topological maps, requiring an estimate of shortest paths in\nvalued graphs with a probabilistic structure. Whereas classical symbolic\nalgorithms achieve optimal results on noise-less topologies, or optimal results\nin a probabilistic sense on graphs with probabilistic structure, we aim to show\nthat machine learning can overcome missing information in the graph by taking\ninto account rich high-dimensional node features, for instance visual\ninformation available at each location of the map. Compared to purely learned\nneural white box algorithms, we structure our neural model with an inductive\nbias for dynamic programming based shortest path algorithms, and we show that a\nparticular parameterization of our neural model corresponds to the Bellman-Ford\nalgorithm. By performing an empirical analysis of our method in simulated\nphoto-realistic 3D environments, we demonstrate that the inclusion of visual\nfeatures in the learned neural planner outperforms classical symbolic solutions\nfor graph based planning.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 09:28:30 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Beeching", "Edward", ""], ["Dibangoye", "Jilles", ""], ["Simonin", "Olivier", ""], ["Wolf", "Christian", ""]]}, {"id": "2007.05271", "submitter": "Pier Giuseppe Sessa", "authors": "Pier Giuseppe Sessa, Ilija Bogunovic, Maryam Kamgarpour, Andreas\n  Krause", "title": "Learning to Play Sequential Games versus Unknown Opponents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a repeated sequential game between a learner, who plays first,\nand an opponent who responds to the chosen action. We seek to design strategies\nfor the learner to successfully interact with the opponent. While most previous\napproaches consider known opponent models, we focus on the setting in which the\nopponent's model is unknown. To this end, we use kernel-based regularity\nassumptions to capture and exploit the structure in the opponent's response. We\npropose a novel algorithm for the learner when playing against an adversarial\nsequence of opponents. The algorithm combines ideas from bilevel optimization\nand online learning to effectively balance between exploration (learning about\nthe opponent's model) and exploitation (selecting highly rewarding actions for\nthe learner). Our results include algorithm's regret guarantees that depend on\nthe regularity of the opponent's response and scale sublinearly with the number\nof game rounds. Moreover, we specialize our approach to repeated Stackelberg\ngames, and empirically demonstrate its effectiveness in a traffic routing and\nwildlife conservation task\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 09:33:05 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Sessa", "Pier Giuseppe", ""], ["Bogunovic", "Ilija", ""], ["Kamgarpour", "Maryam", ""], ["Krause", "Andreas", ""]]}, {"id": "2007.05278", "submitter": "Vibhati Burman", "authors": "Rajesh Kumar Vashishtha, Vibhati Burman, Rajan Kumar, Srividhya\n  Sethuraman, Abhinaya R Sekar, Sharadha Ramanan", "title": "Product age based demand forecast model for fashion retail", "comments": "Accepted in KDD 2020 workshop , AI for fashion supply chain.\n  https://kddfashion2020.mybluemix.net/#accepted-list", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion retailers require accurate demand forecasts for the next season,\nalmost a year in advance, for demand management and supply chain planning\npurposes. Accurate forecasts are important to ensure retailers' profitability\nand to reduce environmental damage caused by disposal of unsold inventory. It\nis challenging because most products are new in a season and have short life\ncycles, huge sales variations and long lead-times. In this paper, we present a\nnovel product age based forecast model, where product age refers to the number\nof weeks since its launch, and show that it outperforms existing models. We\ndemonstrate the robust performance of the approach through real world use case\nof a multinational fashion retailer having over 300 stores, 35k items and\naround 40 categories. The main contributions of this work include unique and\nsignificant feature engineering for product attribute values, accurate demand\nforecast 6-12 months in advance and extending our approach to recommend product\nlaunch time for the next season. We use our fashion assortment optimization\nmodel to produce list and quantity of items to be listed in a store for the\nnext season that maximizes total revenue and satisfies business constraints. We\nfound a revenue uplift of 41% from our framework in comparison to the\nretailer's plan. We also compare our forecast results with the current methods\nand show that it outperforms existing models. Our framework leads to better\nordering, inventory planning, assortment planning and overall increase in\nprofit for the retailer's supply chain.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 09:44:59 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Vashishtha", "Rajesh Kumar", ""], ["Burman", "Vibhati", ""], ["Kumar", "Rajan", ""], ["Sethuraman", "Srividhya", ""], ["Sekar", "Abhinaya R", ""], ["Ramanan", "Sharadha", ""]]}, {"id": "2007.05292", "submitter": "Yushan Liu", "authors": "Yushan Liu, Marcel Hildebrandt, Mitchell Joblin, Martin Ringsquandl,\n  Volker Tresp", "title": "Integrating Logical Rules Into Neural Multi-Hop Reasoning for Drug\n  Repurposing", "comments": "Accepted at the ICML 2020 Workshop Graph Representation Learning and\n  Beyond (GRL+)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The graph structure of biomedical data differs from those in typical\nknowledge graph benchmark tasks. A particular property of biomedical data is\nthe presence of long-range dependencies, which can be captured by patterns\ndescribed as logical rules. We propose a novel method that combines these rules\nwith a neural multi-hop reasoning approach that uses reinforcement learning. We\nconduct an empirical study based on the real-world task of drug repurposing by\nformulating this task as a link prediction problem. We apply our method to the\nbiomedical knowledge graph Hetionet and show that our approach outperforms\nseveral baseline methods.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 10:32:08 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Liu", "Yushan", ""], ["Hildebrandt", "Marcel", ""], ["Joblin", "Mitchell", ""], ["Ringsquandl", "Martin", ""], ["Tresp", "Volker", ""]]}, {"id": "2007.05303", "submitter": "Chin-Chia Michael Yeh", "authors": "Chin-Chia Michael Yeh, Zhongfang Zhuang, Wei Zhang, Liang Wang", "title": "Multi-future Merchant Transaction Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multivariate time series generated from merchant transaction history can\nprovide critical insights for payment processing companies. The capability of\npredicting merchants' future is crucial for fraud detection and recommendation\nsystems. Conventionally, this problem is formulated to predict one multivariate\ntime series under the multi-horizon setting. However, real-world applications\noften require more than one future trend prediction considering the\nuncertainties, where more than one multivariate time series needs to be\npredicted. This problem is called multi-future prediction. In this work, we\ncombine the two research directions and propose to study this new problem:\nmulti-future, multi-horizon and multivariate time series prediction. This\nproblem is crucial as it has broad use cases in the financial industry to\nreduce the risk while improving user experience by providing alternative\nfutures. This problem is also challenging as now we not only need to capture\nthe patterns and insights from the past but also train a model that has a\nstrong inference capability to project multiple possible outcomes. To solve\nthis problem, we propose a new model using convolutional neural networks and a\nsimple yet effective encoder-decoder structure to learn the time series pattern\nfrom multiple perspectives. We use experiments on real-world merchant\ntransaction data to demonstrate the effectiveness of our proposed model. We\nalso provide extensive discussions on different model design choices in our\nexperimental section.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 11:07:32 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Yeh", "Chin-Chia Michael", ""], ["Zhuang", "Zhongfang", ""], ["Zhang", "Wei", ""], ["Wang", "Liang", ""]]}, {"id": "2007.05304", "submitter": "Kristian Miok", "authors": "Kristian Miok, Blaz Skrlj, Daniela Zaharie and Marko Robnik-Sikonja", "title": "To BAN or not to BAN: Bayesian Attention Networks for Reliable Hate\n  Speech Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hate speech is an important problem in the management of user-generated\ncontent. To remove offensive content or ban misbehaving users, content\nmoderators need reliable hate speech detectors. Recently, deep neural networks\nbased on the transformer architecture, such as the (multilingual) BERT model,\nachieve superior performance in many natural language classification tasks,\nincluding hate speech detection. So far, these methods have not been able to\nquantify their output in terms of reliability. We propose a Bayesian method\nusing Monte Carlo dropout within the attention layers of the transformer models\nto provide well-calibrated reliability estimates. We evaluate and visualize the\nresults of the proposed approach on hate speech detection problems in several\nlanguages. Additionally, we test if affective dimensions can enhance the\ninformation extracted by the BERT model in hate speech classification. Our\nexperiments show that Monte Carlo dropout provides a viable mechanism for\nreliability estimation in transformer networks. Used within the BERT model, it\nofers state-of-the-art classification performance and can detect less trusted\npredictions. Also, it was observed that affective dimensions extracted using\nsentic computing methods can provide insights toward interpretation of emotions\ninvolved in hate speech. Our approach not only improves the classification\nperformance of the state-of-the-art multilingual BERT model but the computed\nreliability scores also significantly reduce the workload in an inspection of\nofending cases and reannotation campaigns. The provided visualization helps to\nunderstand the borderline outcomes.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 11:09:00 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 10:02:23 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 10:19:23 GMT"}, {"version": "v4", "created": "Mon, 20 Jul 2020 10:28:12 GMT"}, {"version": "v5", "created": "Fri, 25 Sep 2020 07:30:47 GMT"}, {"version": "v6", "created": "Mon, 7 Dec 2020 08:01:25 GMT"}, {"version": "v7", "created": "Thu, 17 Dec 2020 09:43:00 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Miok", "Kristian", ""], ["Skrlj", "Blaz", ""], ["Zaharie", "Daniela", ""], ["Robnik-Sikonja", "Marko", ""]]}, {"id": "2007.05305", "submitter": "Amirmasoud Ghiassi", "authors": "Amirmasoud Ghiassi, Robert Birke, Rui Han, Lydia Y.Chen", "title": "ExpertNet: Adversarial Learning and Recovery Against Noisy Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's available datasets in the wild, e.g., from social media and open\nplatforms, present tremendous opportunities and challenges for deep learning,\nas there is a significant portion of tagged images, but often with noisy, i.e.\nerroneous, labels. Recent studies improve the robustness of deep models against\nnoisy labels without the knowledge of true labels. In this paper, we advocate\nto derive a stronger classifier which proactively makes use of the noisy labels\nin addition to the original images - turning noisy labels into learning\nfeatures. To such an end, we propose a novel framework, ExpertNet, composed of\nAmateur and Expert, which iteratively learn from each other. Amateur is a\nregular image classifier trained by the feedback of Expert, which imitates how\nhuman experts would correct the predicted labels from Amateur using the noise\npattern learnt from the knowledge of both the noisy and ground truth labels.\nThe trained Amateur and Expert proactively leverage the images and their noisy\nlabels to infer image classes. Our empirical evaluations on noisy versions of\nCIFAR-10, CIFAR-100 and real-world data of Clothing1M show that the proposed\nmodel can achieve robust classification against a wide range of noise ratios\nand with as little as 20-50% training data, compared to state-of-the-art deep\nmodels that solely focus on distilling the impact of noisy labels.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 11:12:16 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 09:58:33 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Ghiassi", "Amirmasoud", ""], ["Birke", "Robert", ""], ["Han", "Rui", ""], ["Chen", "Lydia Y.", ""]]}, {"id": "2007.05307", "submitter": "Yushan Liu", "authors": "Yushan Liu, Markus M. Geipel, Christoph Tietz, Florian Buettner", "title": "TIMELY: Improving Labeling Consistency in Medical Imaging for Cell Type\n  Classification", "comments": "Accepted at ECAI 2020 (24th European Conference on Artificial\n  Intelligence)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagnosing diseases such as leukemia or anemia requires reliable counts of\nblood cells. Hematologists usually label and count microscopy images of blood\ncells manually. In many cases, however, cells in different maturity states are\ndifficult to distinguish, and in combination with image noise and subjectivity,\nhumans are prone to make labeling mistakes. This results in labels that are\noften not reproducible, which can directly affect the diagnoses. We introduce\nTIMELY, a probabilistic model that combines pseudotime inference methods with\ninhomogeneous hidden Markov trees, which addresses this challenge of label\ninconsistency. We show first on simulation data that TIMELY is able to identify\nand correct wrong labels with higher precision and recall than baseline methods\nfor labeling correction. We then apply our method to two real-world datasets of\nblood cell data and show that TIMELY successfully finds inconsistent labels,\nthereby improving the quality of human-generated labels.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 11:13:13 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Liu", "Yushan", ""], ["Geipel", "Markus M.", ""], ["Tietz", "Christoph", ""], ["Buettner", "Florian", ""]]}, {"id": "2007.05315", "submitter": "Jo\\~ao Batista Matos J\\'unior", "authors": "Jo\\~ao Batista Pereira Matos Ju\\'unior, Lucas Carvalho Cordeiro,\n  Marcelo d'Amorim, Xiaowei Huang", "title": "Generating Adversarial Inputs Using A Black-box Differential Technique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Networks (NNs) are known to be vulnerable to adversarial attacks. A\nmalicious agent initiates these attacks by perturbing an input into another one\nsuch that the two inputs are classified differently by the NN. In this paper,\nwe consider a special class of adversarial examples, which can exhibit not only\nthe weakness of NN models - as do for the typical adversarial examples - but\nalso the different behavior between two NN models. We call them\ndifference-inducing adversarial examples or DIAEs. Specifically, we propose\nDAEGEN, the first black-box differential technique for adversarial input\ngeneration. DAEGEN takes as input two NN models of the same classification\nproblem and reports on output an adversarial example. The obtained adversarial\nexample is a DIAE, so that it represents a point-wise difference in the input\nspace between the two NN models. Algorithmically, DAEGEN uses a local\nsearch-based optimization algorithm to find DIAEs by iteratively perturbing an\ninput to maximize the difference of two models on predicting the input. We\nconduct experiments on a spectrum of benchmark datasets (e.g., MNIST, ImageNet,\nand Driving) and NN models (e.g., LeNet, ResNet, Dave, and VGG). Experimental\nresults are promising. First, we compare DAEGEN with two existing white-box\ndifferential techniques (DeepXplore and DLFuzz) and find that under the same\nsetting, DAEGEN is 1) effective, i.e., it is the only technique that succeeds\nin generating attacks in all cases, 2) precise, i.e., the adversarial attacks\nare very likely to fool machines and humans, and 3) efficient, i.e, it requires\na reasonable number of classification queries. Second, we compare DAEGEN with\nstate-of-the-art black-box adversarial attack methods (simba and tremba), by\nadapting them to work on a differential setting. The experimental results show\nthat DAEGEN performs better than both of them.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 11:25:31 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Ju\u00fanior", "Jo\u00e3o Batista Pereira Matos", ""], ["Cordeiro", "Lucas Carvalho", ""], ["d'Amorim", "Marcelo", ""], ["Huang", "Xiaowei", ""]]}, {"id": "2007.05320", "submitter": "Philipp Marquetand", "authors": "Julia Westermayr, Philipp Marquetand", "title": "Machine learning for electronically excited states of molecules", "comments": null, "journal-ref": null, "doi": "10.1021/acs.chemrev.0c00749", "report-no": null, "categories": "physics.chem-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronically excited states of molecules are at the heart of\nphotochemistry, photophysics, as well as photobiology and also play a role in\nmaterial science. Their theoretical description requires highly accurate\nquantum chemical calculations, which are computationally expensive. In this\nreview, we focus on how machine learning is employed not only to speed up such\nexcited-state simulations but also how this branch of artificial intelligence\ncan be used to advance this exciting research field in all its aspects.\nDiscussed applications of machine learning for excited states include\nexcited-state dynamics simulations, static calculations of absorption spectra,\nas well as many others. In order to put these studies into context, we discuss\nthe promises and pitfalls of the involved machine learning techniques. Since\nthe latter are mostly based on quantum chemistry calculations, we also provide\na short introduction into excited-state electronic structure methods,\napproaches for nonadiabatic dynamics simulations and describe tricks and\nproblems when using them in machine learning for excited states of molecules.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 11:42:29 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Westermayr", "Julia", ""], ["Marquetand", "Philipp", ""]]}, {"id": "2007.05335", "submitter": "Hrant Khachatrian", "authors": "Tigran Galstyan, Hrant Khachatrian, Greg Ver Steeg, Aram Galstyan", "title": "Robust Classification under Class-Dependent Domain Shift", "comments": "Accepted at ICML 2020 workshop on Uncertainty and Robustness in Deep\n  Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investigation of machine learning algorithms robust to changes between the\ntraining and test distributions is an active area of research. In this paper we\nexplore a special type of dataset shift which we call class-dependent domain\nshift. It is characterized by the following features: the input data causally\ndepends on the label, the shift in the data is fully explained by a known\nvariable, the variable which controls the shift can depend on the label, there\nis no shift in the label distribution. We define a simple optimization problem\nwith an information theoretic constraint and attempt to solve it with neural\nnetworks. Experiments on a toy dataset demonstrate the proposed method is able\nto learn robust classifiers which generalize well to unseen domains.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 12:26:57 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Galstyan", "Tigran", ""], ["Khachatrian", "Hrant", ""], ["Steeg", "Greg Ver", ""], ["Galstyan", "Aram", ""]]}, {"id": "2007.05384", "submitter": "Lukas Herrmann", "authors": "Philipp Grohs and Lukas Herrmann", "title": "Deep neural network approximation for high-dimensional elliptic PDEs\n  with boundary conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent work it has been established that deep neural networks are capable\nof approximating solutions to a large class of parabolic partial differential\nequations without incurring the curse of dimension. However, all this work has\nbeen restricted to problems formulated on the whole Euclidean domain. On the\nother hand, most problems in engineering and the sciences are formulated on\nfinite domains and subjected to boundary conditions. The present paper\nconsiders an important such model problem, namely the Poisson equation on a\ndomain $D\\subset \\mathbb{R}^d$ subject to Dirichlet boundary conditions. It is\nshown that deep neural networks are capable of representing solutions of that\nproblem without incurring the curse of dimension. The proofs are based on a\nprobabilistic representation of the solution to the Poisson equation as well as\na suitable sampling method.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 13:40:27 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 11:53:24 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Grohs", "Philipp", ""], ["Herrmann", "Lukas", ""]]}, {"id": "2007.05385", "submitter": "Owen Ward", "authors": "Owen G. Ward, Zhen Huang, Andrew Davison, Tian Zheng", "title": "Next Waves in Veridical Network Embedding", "comments": null, "journal-ref": null, "doi": "10.1002/sam.11486", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding nodes of a large network into a metric (e.g., Euclidean) space has\nbecome an area of active research in statistical machine learning, which has\nfound applications in natural and social sciences. Generally, a representation\nof a network object is learned in a Euclidean geometry and is then used for\nsubsequent tasks regarding the nodes and/or edges of the network, such as\ncommunity detection, node classification and link prediction. Network embedding\nalgorithms have been proposed in multiple disciplines, often with\ndomain-specific notations and details. In addition, different measures and\ntools have been adopted to evaluate and compare the methods proposed under\ndifferent settings, often dependent of the downstream tasks. As a result, it is\nchallenging to study these algorithms in the literature systematically.\nMotivated by the recently proposed Veridical Data Science (VDS) framework, we\npropose a framework for network embedding algorithms and discuss how the\nprinciples of predictability, computability and stability apply in this\ncontext. The utilization of this framework in network embedding holds the\npotential to motivate and point to new directions for future research.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 13:41:48 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Ward", "Owen G.", ""], ["Huang", "Zhen", ""], ["Davison", "Andrew", ""], ["Zheng", "Tian", ""]]}, {"id": "2007.05394", "submitter": "Sao Mai Nguyen", "authors": "Linda Nanan Vall\\'ee (ESATIC), Sao Mai Nguyen (IMT Atlantique, IMT\n  Atlantique - INFO, Lab-STICC, Flowers), Christophe Lohr (Lab-STICC, IMT\n  Atlantique - INFO, IMT Atlantique), Ioannis Kanellos (Lab-STICC, IMT\n  Atlantique - INFO, IMT Atlantique), Olivier Asseu (ESATIC)", "title": "How An Automated Gesture Imitation Game Can Improve Social Interactions\n  With Teenagers With ASD", "comments": null, "journal-ref": "IEEE ICRA Workshop on Social Robotics for Neurodevelopmental\n  Disorders, Jun 2020, Paris, France", "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the outlook of improving communication and social abilities of people\nwith ASD, we propose to extend the paradigm of robot-based imitation games to\nASD teenagers. In this paper, we present an interaction scenario adapted to ASD\nteenagers, propose a computational architecture using the latest machine\nlearning algorithm Openpose for human pose detection, and present the results\nof our basic testing of the scenario with human caregivers. These results are\npreliminary due to the number of session (1) and participants (4). They include\na technical assessment of the performance of Openpose, as well as a preliminary\nuser study to confirm our game scenario could elicit the expected response from\nsubjects.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 14:01:24 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Vall\u00e9e", "Linda Nanan", "", "ESATIC"], ["Nguyen", "Sao Mai", "", "IMT Atlantique, IMT\n  Atlantique - INFO, Lab-STICC, Flowers"], ["Lohr", "Christophe", "", "Lab-STICC, IMT\n  Atlantique - INFO, IMT Atlantique"], ["Kanellos", "Ioannis", "", "Lab-STICC, IMT\n  Atlantique - INFO, IMT Atlantique"], ["Asseu", "Olivier", "", "ESATIC"]]}, {"id": "2007.05426", "submitter": "Anthony Caterini", "authors": "Anthony Caterini and Rob Cornish and Dino Sejdinovic and Arnaud Doucet", "title": "Variational Inference with Continuously-Indexed Normalizing Flows", "comments": "Accepted for publication at UAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuously-indexed flows (CIFs) have recently achieved improvements over\nbaseline normalizing flows on a variety of density estimation tasks. CIFs do\nnot possess a closed-form marginal density, and so, unlike standard flows,\ncannot be plugged in directly to a variational inference (VI) scheme in order\nto produce a more expressive family of approximate posteriors. However, we show\nhere how CIFs can be used as part of an auxiliary VI scheme to formulate and\ntrain expressive posterior approximations in a natural way. We exploit the\nconditional independence structure of multi-layer CIFs to build the required\nauxiliary inference models, which we show empirically yield low-variance\nestimators of the model evidence. We then demonstrate the advantages of CIFs\nover baseline flows in VI problems when the posterior distribution of interest\npossesses a complicated topology, obtaining improved results in both the\nBayesian inference and surrogate maximum likelihood settings.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 15:00:04 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 18:20:21 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Caterini", "Anthony", ""], ["Cornish", "Rob", ""], ["Sejdinovic", "Dino", ""], ["Doucet", "Arnaud", ""]]}, {"id": "2007.05432", "submitter": "Christoph Raab", "authors": "Christoph Raab, Moritz Heusinger, Frank-Michael Schleif", "title": "Reactive Soft Prototype Computing for Concept Drift Streams", "comments": null, "journal-ref": "Neurocomputing, 2020, ISSN 0925-2312", "doi": "10.1016/j.neucom.2019.11.111", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The amount of real-time communication between agents in an information system\nhas increased rapidly since the beginning of the decade. This is because the\nuse of these systems, e. g. social media, has become commonplace in today's\nsociety. This requires analytical algorithms to learn and predict this stream\nof information in real-time. The nature of these systems is non-static and can\nbe explained, among other things, by the fast pace of trends. This creates an\nenvironment in which algorithms must recognize changes and adapt. Recent work\nshows vital research in the field, but mainly lack stable performance during\nmodel adaptation. In this work, a concept drift detection strategy followed by\na prototype-based adaptation strategy is proposed. Validated through\nexperimental results on a variety of typical non-static data, our solution\nprovides stable and quick adjustments in times of change.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 15:07:46 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Raab", "Christoph", ""], ["Heusinger", "Moritz", ""], ["Schleif", "Frank-Michael", ""]]}, {"id": "2007.05434", "submitter": "Maram Akila", "authors": "Joachim Sicking, Maram Akila, Tim Wirtz, Sebastian Houben, Asja\n  Fischer", "title": "Characteristics of Monte Carlo Dropout in Wide Neural Networks", "comments": "Accepted at the ICML 2020 workshop for Uncertainty and Robustness in\n  Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo (MC) dropout is one of the state-of-the-art approaches for\nuncertainty estimation in neural networks (NNs). It has been interpreted as\napproximately performing Bayesian inference. Based on previous work on the\napproximation of Gaussian processes by wide and deep neural networks with\nrandom weights, we study the limiting distribution of wide untrained NNs under\ndropout more rigorously and prove that they as well converge to Gaussian\nprocesses for fixed sets of weights and biases. We sketch an argument that this\nproperty might also hold for infinitely wide feed-forward networks that are\ntrained with (full-batch) gradient descent. The theory is contrasted by an\nempirical analysis in which we find correlations and non-Gaussian behaviour for\nthe pre-activations of finite width NNs. We therefore investigate how\n(strongly) correlated pre-activations can induce non-Gaussian behavior in NNs\nwith strongly correlated weights.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 15:14:43 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Sicking", "Joachim", ""], ["Akila", "Maram", ""], ["Wirtz", "Tim", ""], ["Houben", "Sebastian", ""], ["Fischer", "Asja", ""]]}, {"id": "2007.05447", "submitter": "Santiago Mazuelas", "authors": "Santiago Mazuelas, Yuan Shen, and Aritz P\\'erez", "title": "Generalized Maximum Entropy for Supervised Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximum entropy principle advocates to evaluate events' probabilities\nusing a distribution that maximizes entropy among those that satisfy certain\nexpectations' constraints. Such principle can be generalized for arbitrary\ndecision problems where it corresponds to minimax approaches. This paper\nestablishes a framework for supervised classification based on the generalized\nmaximum entropy principle that leads to minimax risk classifiers (MRCs). We\ndevelop learning techniques that determine MRCs for general entropy functions\nand provide performance guarantees by means of convex optimization. In\naddition, we describe the relationship of the presented techniques with\nexisting classification methods, and quantify MRCs performance in comparison\nwith the proposed bounds and conventional methods.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 15:41:17 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Mazuelas", "Santiago", ""], ["Shen", "Yuan", ""], ["P\u00e9rez", "Aritz", ""]]}, {"id": "2007.05453", "submitter": "Giuseppe Vietri", "authors": "Giuseppe Vietri, Grace Tian, Mark Bun, Thomas Steinke, Zhiwei Steven\n  Wu", "title": "New Oracle-Efficient Algorithms for Private Synthetic Data Release", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present three new algorithms for constructing differentially private\nsynthetic data---a sanitized version of a sensitive dataset that approximately\npreserves the answers to a large collection of statistical queries. All three\nalgorithms are \\emph{oracle-efficient} in the sense that they are\ncomputationally efficient when given access to an optimization oracle. Such an\noracle can be implemented using many existing (non-private) optimization tools\nsuch as sophisticated integer program solvers. While the accuracy of the\nsynthetic data is contingent on the oracle's optimization performance, the\nalgorithms satisfy differential privacy even in the worst case. For all three\nalgorithms, we provide theoretical guarantees for both accuracy and privacy.\nThrough empirical evaluation, we demonstrate that our methods scale well with\nboth the dimensionality of the data and the number of queries. Compared to the\nstate-of-the-art method High-Dimensional Matrix Mechanism \\cite{McKennaMHM18},\nour algorithms provide better accuracy in the large workload and high privacy\nregime (corresponding to low privacy loss $\\varepsilon$).\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 15:46:05 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Vietri", "Giuseppe", ""], ["Tian", "Grace", ""], ["Bun", "Mark", ""], ["Steinke", "Thomas", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "2007.05456", "submitter": "Matteo Pirotta", "authors": "Ronan Fruit, Matteo Pirotta, Alessandro Lazaric", "title": "Improved Analysis of UCRL2 with Empirical Bernstein Inequality", "comments": "Document in support of the tutorial at ALT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of exploration-exploitation in communicating Markov\nDecision Processes. We provide an analysis of UCRL2 with Empirical Bernstein\ninequalities (UCRL2B). For any MDP with $S$ states, $A$ actions, $\\Gamma \\leq\nS$ next states and diameter $D$, the regret of UCRL2B is bounded as\n$\\widetilde{O}(\\sqrt{D\\Gamma S A T})$.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 15:52:21 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Fruit", "Ronan", ""], ["Pirotta", "Matteo", ""], ["Lazaric", "Alessandro", ""]]}, {"id": "2007.05470", "submitter": "A. John Woodill", "authors": "A. John Woodill, Maria Kavanaugh, Michael Harte, and James R. Watson", "title": "Predicting Illegal Fishing on the Patagonia Shelf from Oceanographic\n  Seascapes", "comments": "27 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many of the world's most important fisheries are experiencing increases in\nillegal fishing, undermining efforts to sustainably conserve and manage fish\nstocks. A major challenge to ending illegal, unreported, and unregulated (IUU)\nfishing is improving our ability to identify whether a vessel is fishing\nillegally and where illegal fishing is likely to occur in the ocean. However,\nmonitoring the oceans is costly, time-consuming, and logistically challenging\nfor maritime authorities to patrol. To address this problem, we use vessel\ntracking data and machine learning to predict illegal fishing on the Patagonian\nShelf, one of the world's most productive regions for fisheries. Specifically,\nwe focus on Chinese fishing vessels, which have consistently fished illegally\nin this region. We combine vessel location data with oceanographic seascapes --\nclasses of oceanic areas based on oceanographic variables -- as well as other\nremotely sensed oceanographic variables to train a series of machine learning\nmodels of varying levels of complexity. These models are able to predict\nwhether a Chinese vessel is operating illegally with 69-96% confidence,\ndepending on the year and predictor variables used. These results offer a\npromising step towards preempting illegal activities, rather than reacting to\nthem forensically.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 16:31:25 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Woodill", "A. John", ""], ["Kavanaugh", "Maria", ""], ["Harte", "Michael", ""], ["Watson", "James R.", ""]]}, {"id": "2007.05477", "submitter": "Amit Kadan", "authors": "Amit Kadan and Hu Fu", "title": "Exponential Convergence of Gradient Methods in Concave Network Zero-sum\n  Games", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by Generative Adversarial Networks, we study the computation of\nNash equilibrium in concave network zero-sum games (NZSGs), a multiplayer\ngeneralization of two-player zero-sum games first proposed with linear payoffs.\nExtending previous results, we show that various game theoretic properties of\nconvex-concave two-player zero-sum games are preserved in this generalization.\nWe then generalize last iterate convergence results obtained previously in\ntwo-player zero-sum games. We analyze convergence rates when players update\ntheir strategies using Gradient Ascent, and its variant, Optimistic Gradient\nAscent, showing last iterate convergence in three settings -- when the payoffs\nof players are linear, strongly concave and Lipschitz, and strongly concave and\nsmooth. We provide experimental results that support these theoretical\nfindings.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 16:56:56 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Kadan", "Amit", ""], ["Fu", "Hu", ""]]}, {"id": "2007.05493", "submitter": "Alexander Kocian", "authors": "Alexander Kocian and Luca Incrocci", "title": "Learning from Data to Optimize Control in Precision Farming", "comments": "Editorial of \"Statistical Tools in Precision Farming\", MDPI/Stats", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precision farming is one way of many to meet a 70 percent increase in global\ndemand for agricultural products on current agricultural land by 2050 at\nreduced need of fertilizers and efficient use of water resources. The catalyst\nfor the emergence of precision farming has been satellite positioning and\nnavigation followed by Internet-of-Things, generating vast information that can\nbe used to optimize farming processes in real-time. Statistical tools from data\nmining, predictive modeling, and machine learning analyze pattern in historical\ndata, to make predictions about future events as well as intelligent actions.\nThis special issue presents the latest development in statistical inference,\nmachine learning and optimum control for precision farming.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 12:44:17 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Kocian", "Alexander", ""], ["Incrocci", "Luca", ""]]}, {"id": "2007.05499", "submitter": "Begum Taskazan", "authors": "Begum Taskazan, Jiri Navratil, Matthew Arnold, Anupama Murthi, Ganesh\n  Venkataraman, Benjamin Elder", "title": "Not Your Grandfathers Test Set: Reducing Labeling Effort for Testing", "comments": "International Workshop on Challenges in Deploying and Monitoring\n  Machine Learning Systems in Conjunction with ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building and maintaining high-quality test sets remains a laborious and\nexpensive task. As a result, test sets in the real world are often not properly\nkept up to date and drift from the production traffic they are supposed to\nrepresent. The frequency and severity of this drift raises serious concerns\nover the value of manually labeled test sets in the QA process. This paper\nproposes a simple but effective technique that drastically reduces the effort\nneeded to construct and maintain a high-quality test set (reducing labeling\neffort by 80-100% across a range of practical scenarios). This result\nencourages a fundamental rethinking of the testing process by both\npractitioners, who can use these techniques immediately to improve their\ntesting, and researchers who can help address many of the open questions raised\nby this new approach.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 17:25:11 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Taskazan", "Begum", ""], ["Navratil", "Jiri", ""], ["Arnold", "Matthew", ""], ["Murthi", "Anupama", ""], ["Venkataraman", "Ganesh", ""], ["Elder", "Benjamin", ""]]}, {"id": "2007.05520", "submitter": "Dibya Ghosh", "authors": "Dibya Ghosh, Marc G. Bellemare", "title": "Representations for Stable Off-Policy Reinforcement Learning", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning with function approximation can be unstable and even\ndivergent, especially when combined with off-policy learning and Bellman\nupdates. In deep reinforcement learning, these issues have been dealt with\nempirically by adapting and regularizing the representation, in particular with\nauxiliary tasks. This suggests that representation learning may provide a means\nto guarantee stability. In this paper, we formally show that there are indeed\nnontrivial state representations under which the canonical TD algorithm is\nstable, even when learning off-policy. We analyze representation learning\nschemes that are based on the transition matrix of a policy, such as\nproto-value functions, along three axes: approximation error, stability, and\nease of estimation. In the most general case, we show that a Schur basis\nprovides convergence guarantees, but is difficult to estimate from samples. For\na fixed reward function, we find that an orthogonal basis of the corresponding\nKrylov subspace is an even better choice. We conclude by empirically\ndemonstrating that these stable representations can be learned using stochastic\ngradient descent, opening the door to improved techniques for representation\nlearning with deep networks.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 17:55:54 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 20:58:51 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Ghosh", "Dibya", ""], ["Bellemare", "Marc G.", ""]]}, {"id": "2007.05535", "submitter": "Ana Diaz Rivero", "authors": "Ana Diaz Rivero and Cora Dvorkin", "title": "Flow-Based Likelihoods for Non-Gaussian Inference", "comments": "14 pages, 6 figures + appendices. v2 matches published version", "journal-ref": "Phys. Rev. D 102, 103507 (2020)", "doi": "10.1103/PhysRevD.102.103507", "report-no": null, "categories": "astro-ph.CO physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the use of data-driven likelihoods to bypass a key assumption\nmade in many scientific analyses, which is that the true likelihood of the data\nis Gaussian. In particular, we suggest using the optimization targets of\nflow-based generative models, a class of models that can capture complex\ndistributions by transforming a simple base distribution through layers of\nnonlinearities. We call these flow-based likelihoods (FBL). We analyze the\naccuracy and precision of the reconstructed likelihoods on mock Gaussian data,\nand show that simply gauging the quality of samples drawn from the trained\nmodel is not a sufficient indicator that the true likelihood has been learned.\nWe nevertheless demonstrate that the likelihood can be reconstructed to a\nprecision equal to that of sampling error due to a finite sample size. We then\napply FBLs to mock weak lensing convergence power spectra, a cosmological\nobservable that is significantly non-Gaussian (NG). We find that the FBL\ncaptures the NG signatures in the data extremely well, while other commonly\nused data-driven likelihoods, such as Gaussian mixture models and independent\ncomponent analysis, fail to do so. This suggests that works that have found\nsmall posterior shifts in NG data with data-driven likelihoods such as these\ncould be underestimating the impact of non-Gaussianity in parameter\nconstraints. By introducing a suite of tests that can capture different levels\nof NG in the data, we show that the success or failure of traditional\ndata-driven likelihoods can be tied back to the structure of the NG in the\ndata. Unlike other methods, the flexibility of the FBL makes it successful at\ntackling different types of NG simultaneously. Because of this, and\nconsequently their likely applicability across datasets and domains, we\nencourage their use for inference when sufficient mock data are available for\ntraining.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 18:00:00 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 17:33:17 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Rivero", "Ana Diaz", ""], ["Dvorkin", "Cora", ""]]}, {"id": "2007.05549", "submitter": "Alexander Irpan", "authors": "Janarthanan Rajendran, Alex Irpan, Eric Jang", "title": "Meta-Learning Requires Meta-Augmentation", "comments": "14 pages, 8 figures. NeurIPS 2020 camera ready. Code at\n  https://github.com/google-research/google-research/tree/master/meta_augmentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning algorithms aim to learn two components: a model that predicts\ntargets for a task, and a base learner that quickly updates that model when\ngiven examples from a new task. This additional level of learning can be\npowerful, but it also creates another potential source for overfitting, since\nwe can now overfit in either the model or the base learner. We describe both of\nthese forms of metalearning overfitting, and demonstrate that they appear\nexperimentally in common meta-learning benchmarks. We then use an\ninformation-theoretic framework to discuss meta-augmentation, a way to add\nrandomness that discourages the base learner and model from learning trivial\nsolutions that do not generalize to new tasks. We demonstrate that\nmeta-augmentation produces large complementary benefits to recently proposed\nmeta-regularization techniques.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 18:04:04 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 00:03:33 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Rajendran", "Janarthanan", ""], ["Irpan", "Alex", ""], ["Jang", "Eric", ""]]}, {"id": "2007.05553", "submitter": "Mikko Heikkil\\\"a", "authors": "Mikko A. Heikkil\\\"a, Antti Koskela, Kana Shimizu, Samuel Kaski, Antti\n  Honkela", "title": "Differentially private cross-silo federated learning", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strict privacy is of paramount importance in distributed machine learning.\nFederated learning, with the main idea of communicating only what is needed for\nlearning, has been recently introduced as a general approach for distributed\nlearning to enhance learning and improve security. However, federated learning\nby itself does not guarantee any privacy for data subjects. To quantify and\ncontrol how much privacy is compromised in the worst-case, we can use\ndifferential privacy.\n  In this paper we combine additively homomorphic secure summation protocols\nwith differential privacy in the so-called cross-silo federated learning\nsetting. The goal is to learn complex models like neural networks while\nguaranteeing strict privacy for the individual data subjects. We demonstrate\nthat our proposed solutions give prediction accuracy that is comparable to the\nnon-distributed setting, and are fast enough to enable learning models with\nmillions of parameters in a reasonable time.\n  To enable learning under strict privacy guarantees that need privacy\namplification by subsampling, we present a general algorithm for oblivious\ndistributed subsampling. However, we also argue that when malicious parties are\npresent, a simple approach using distributed Poisson subsampling gives better\nprivacy.\n  Finally, we show that by leveraging random projections we can further\nscale-up our approach to larger models while suffering only a modest\nperformance loss.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 18:15:10 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Heikkil\u00e4", "Mikko A.", ""], ["Koskela", "Antti", ""], ["Shimizu", "Kana", ""], ["Kaski", "Samuel", ""], ["Honkela", "Antti", ""]]}, {"id": "2007.05554", "submitter": "Sait Cakmak", "authors": "Sait Cakmak, Raul Astudillo, Peter Frazier and Enlu Zhou", "title": "Bayesian Optimization of Risk Measures", "comments": "Main paper: 15 pages with 2 figures. Supplement: 14 pages with 3\n  figures. To appear in NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Bayesian optimization of objective functions of the form $\\rho[\nF(x, W) ]$, where $F$ is a black-box expensive-to-evaluate function and $\\rho$\ndenotes either the VaR or CVaR risk measure, computed with respect to the\nrandomness induced by the environmental random variable $W$. Such problems\narise in decision making under uncertainty, such as in portfolio optimization\nand robust systems design. We propose a family of novel Bayesian optimization\nalgorithms that exploit the structure of the objective function to\nsubstantially improve sampling efficiency. Instead of modeling the objective\nfunction directly as is typical in Bayesian optimization, these algorithms\nmodel $F$ as a Gaussian process, and use the implied posterior on the objective\nfunction to decide which points to evaluate. We demonstrate the effectiveness\nof our approach in a variety of numerical experiments.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 18:20:46 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 02:47:29 GMT"}, {"version": "v3", "created": "Wed, 4 Nov 2020 01:17:35 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Cakmak", "Sait", ""], ["Astudillo", "Raul", ""], ["Frazier", "Peter", ""], ["Zhou", "Enlu", ""]]}, {"id": "2007.05557", "submitter": "Yingyu Liang", "authors": "Yingyu Liang and Hui Yuan", "title": "Learning Entangled Single-Sample Gaussians in the Subset-of-Signals\n  Model", "comments": "Appear in COLT'2020. Updates: corrected comments on existing works;\n  added comparison to median estimator", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the setting of entangled single-sample distributions, the goal is to\nestimate some common parameter shared by a family of $n$ distributions, given\none single sample from each distribution. This paper studies mean estimation\nfor entangled single-sample Gaussians that have a common mean but different\nunknown variances. We propose the subset-of-signals model where an unknown\nsubset of $m$ variances are bounded by 1 while there are no assumptions on the\nother variances. In this model, we analyze a simple and natural method based on\niteratively averaging the truncated samples, and show that the method achieves\nerror $O \\left(\\frac{\\sqrt{n\\ln n}}{m}\\right)$ with high probability when\n$m=\\Omega(\\sqrt{n\\ln n})$, matching existing bounds for this range of $m$. We\nfurther prove lower bounds, showing that the error is\n$\\Omega\\left(\\left(\\frac{n}{m^4}\\right)^{1/2}\\right)$ when $m$ is between\n$\\Omega(\\ln n)$ and $O(n^{1/4})$, and the error is\n$\\Omega\\left(\\left(\\frac{n}{m^4}\\right)^{1/6}\\right)$ when $m$ is between\n$\\Omega(n^{1/4})$ and $O(n^{1 - \\epsilon})$ for an arbitrarily small\n$\\epsilon>0$, improving existing lower bounds and extending to a wider range of\n$m$.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 18:25:38 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Liang", "Yingyu", ""], ["Yuan", "Hui", ""]]}, {"id": "2007.05558", "submitter": "Neil Thompson", "authors": "Neil C. Thompson, Kristjan Greenewald, Keeheon Lee, Gabriel F. Manso", "title": "The Computational Limits of Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning's recent history has been one of achievement: from triumphing\nover humans in the game of Go to world-leading performance in image\nrecognition, voice recognition, translation, and other tasks. But this progress\nhas come with a voracious appetite for computing power. This article reports on\nthe computational demands of Deep Learning applications in five prominent\napplication areas and shows that progress in all five is strongly reliant on\nincreases in computing power. Extrapolating forward this reliance reveals that\nprogress along current lines is rapidly becoming economically, technically, and\nenvironmentally unsustainable. Thus, continued progress in these applications\nwill require dramatically more computationally-efficient methods, which will\neither have to come from changes to deep learning or from moving to other\nmachine learning methods.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 18:26:17 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Thompson", "Neil C.", ""], ["Greenewald", "Kristjan", ""], ["Lee", "Keeheon", ""], ["Manso", "Gabriel F.", ""]]}, {"id": "2007.05565", "submitter": "John Golden", "authors": "John Golden, Daniel O'Malley", "title": "Reverse Annealing for Nonnegative/Binary Matrix Factorization", "comments": "9 pages, 5 figures", "journal-ref": "PLOS ONE 2021 16(1): e0244026", "doi": "10.1371/journal.pone.0244026", "report-no": null, "categories": "cs.LG cs.ET quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It was recently shown that quantum annealing can be used as an effective,\nfast subroutine in certain types of matrix factorization algorithms. The\nquantum annealing algorithm performed best for quick, approximate answers, but\nperformance rapidly plateaued. In this paper, we utilize reverse annealing\ninstead of forward annealing in the quantum annealing subroutine for\nnonnegative/binary matrix factorization problems. After an initial global\nsearch with forward annealing, reverse annealing performs a series of local\nsearches that refine existing solutions. The combination of forward and reverse\nannealing significantly improves performance compared to forward annealing\nalone for all but the shortest run times.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 18:40:02 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 16:42:15 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Golden", "John", ""], ["O'Malley", "Daniel", ""]]}, {"id": "2007.05566", "submitter": "Jim Winkens", "authors": "Jim Winkens, Rudy Bunel, Abhijit Guha Roy, Robert Stanforth, Vivek\n  Natarajan, Joseph R. Ledsam, Patricia MacWilliams, Pushmeet Kohli, Alan\n  Karthikesalingam, Simon Kohl, Taylan Cemgil, S. M. Ali Eslami and Olaf\n  Ronneberger", "title": "Contrastive Training for Improved Out-of-Distribution Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Reliable detection of out-of-distribution (OOD) inputs is increasingly\nunderstood to be a precondition for deployment of machine learning systems.\nThis paper proposes and investigates the use of contrastive training to boost\nOOD detection performance. Unlike leading methods for OOD detection, our\napproach does not require access to examples labeled explicitly as OOD, which\ncan be difficult to collect in practice. We show in extensive experiments that\ncontrastive training significantly helps OOD detection performance on a number\nof common benchmarks. By introducing and employing the Confusion Log\nProbability (CLP) score, which quantifies the difficulty of the OOD detection\ntask by capturing the similarity of inlier and outlier datasets, we show that\nour method especially improves performance in the `near OOD' classes -- a\nparticularly challenging setting for previous methods.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 18:40:37 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Winkens", "Jim", ""], ["Bunel", "Rudy", ""], ["Roy", "Abhijit Guha", ""], ["Stanforth", "Robert", ""], ["Natarajan", "Vivek", ""], ["Ledsam", "Joseph R.", ""], ["MacWilliams", "Patricia", ""], ["Kohli", "Pushmeet", ""], ["Karthikesalingam", "Alan", ""], ["Kohl", "Simon", ""], ["Cemgil", "Taylan", ""], ["Eslami", "S. M. Ali", ""], ["Ronneberger", "Olaf", ""]]}, {"id": "2007.05572", "submitter": "Zongheng Yang", "authors": "Eric Liang, Zongheng Yang, Ion Stoica, Pieter Abbeel, Yan Duan, Xi\n  Chen", "title": "Variable Skipping for Autoregressive Range Density Estimation", "comments": "ICML 2020. Code released at: https://var-skip.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep autoregressive models compute point likelihood estimates of individual\ndata points. However, many applications (i.e., database cardinality estimation)\nrequire estimating range densities, a capability that is under-explored by\ncurrent neural density estimation literature. In these applications, fast and\naccurate range density estimates over high-dimensional data directly impact\nuser-perceived performance. In this paper, we explore a technique, variable\nskipping, for accelerating range density estimation over deep autoregressive\nmodels. This technique exploits the sparse structure of range density queries\nto avoid sampling unnecessary variables during approximate inference. We show\nthat variable skipping provides 10-100$\\times$ efficiency improvements when\ntargeting challenging high-quantile error metrics, enables complex applications\nsuch as text pattern matching, and can be realized via a simple data\naugmentation procedure without changing the usual maximum likelihood objective.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 19:01:40 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Liang", "Eric", ""], ["Yang", "Zongheng", ""], ["Stoica", "Ion", ""], ["Abbeel", "Pieter", ""], ["Duan", "Yan", ""], ["Chen", "Xi", ""]]}, {"id": "2007.05577", "submitter": "Shubhankar Deshpande", "authors": "Shuby Deshpande, Jeff Schneider", "title": "Vizarel: A System to Help Better Understand RL Agents", "comments": "Accepted to ICML 2020 Workshop on Human Interpretability in Machine\n  Learning (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualization tools for supervised learning have allowed users to interpret,\nintrospect, and gain intuition for the successes and failures of their models.\nWhile reinforcement learning practitioners ask many of the same questions,\nexisting tools are not applicable to the RL setting. In this work, we describe\nour initial attempt at constructing a prototype of these ideas, through\nidentifying possible features that such a system should encapsulate. Our design\nis motivated by envisioning the system to be a platform on which to experiment\nwith interpretable reinforcement learning.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 19:19:22 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Deshpande", "Shuby", ""], ["Schneider", "Jeff", ""]]}, {"id": "2007.05610", "submitter": "Mark Crowley", "authors": "Milad Sikaroudi, Benyamin Ghojogh, Fakhri Karray, Mark Crowley, H.R.\n  Tizhoosh", "title": "Batch-Incremental Triplet Sampling for Training Triplet Networks Using\n  Bayesian Updating Theorem", "comments": "Accepted for presentation at the 25th International Conference on\n  Pattern Recognition (ICPR), IEEE, 2020. The first two authors contributed\n  equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variants of Triplet networks are robust entities for learning a\ndiscriminative embedding subspace. There exist different triplet mining\napproaches for selecting the most suitable training triplets. Some of these\nmining methods rely on the extreme distances between instances, and some others\nmake use of sampling. However, sampling from stochastic distributions of data\nrather than sampling merely from the existing embedding instances can provide\nmore discriminative information. In this work, we sample triplets from\ndistributions of data rather than from existing instances. We consider a\nmultivariate normal distribution for the embedding of each class. Using\nBayesian updating and conjugate priors, we update the distributions of classes\ndynamically by receiving the new mini-batches of training data. The proposed\ntriplet mining with Bayesian updating can be used with any triplet-based loss\nfunction, e.g., triplet-loss or Neighborhood Component Analysis (NCA) loss.\nAccordingly, Our triplet mining approaches are called Bayesian Updating Triplet\n(BUT) and Bayesian Updating NCA (BUNCA), depending on which loss function is\nbeing used. Experimental results on two public datasets, namely MNIST and\nhistopathology colorectal cancer (CRC), substantiate the effectiveness of the\nproposed triplet mining method.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 21:07:51 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 14:13:35 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Sikaroudi", "Milad", ""], ["Ghojogh", "Benyamin", ""], ["Karray", "Fakhri", ""], ["Crowley", "Mark", ""], ["Tizhoosh", "H. R.", ""]]}, {"id": "2007.05611", "submitter": "Rohan Kodialam", "authors": "Rohan S. Kodialam, Rebecca Boiarsky, Justin Lim, Neil Dixit, Aditya\n  Sai, David Sontag", "title": "Deep Contextual Clinical Prediction with Reverse Distillation", "comments": "To appear in AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Healthcare providers are increasingly using machine learning to predict\npatient outcomes to make meaningful interventions. However, despite innovations\nin this area, deep learning models often struggle to match performance of\nshallow linear models in predicting these outcomes, making it difficult to\nleverage such techniques in practice. In this work, motivated by the task of\nclinical prediction from insurance claims, we present a new technique called\nReverse Distillation which pretrains deep models by using high-performing\nlinear models for initialization. We make use of the longitudinal structure of\ninsurance claims datasets to develop Self Attention with Reverse Distillation,\nor SARD, an architecture that utilizes a combination of contextual embedding,\ntemporal embedding and self-attention mechanisms and most critically is trained\nvia reverse distillation. SARD outperforms state-of-the-art methods on multiple\nclinical prediction outcomes, with ablation studies revealing that reverse\ndistillation is a primary driver of these improvements. Code is available at\nhttps://github.com/clinicalml/omop-learn.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 21:10:34 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 01:56:08 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Kodialam", "Rohan S.", ""], ["Boiarsky", "Rebecca", ""], ["Lim", "Justin", ""], ["Dixit", "Neil", ""], ["Sai", "Aditya", ""], ["Sontag", "David", ""]]}, {"id": "2007.05627", "submitter": "Shaofeng Deng", "authors": "March Boedihardjo, Shaofeng Deng, Thomas Strohmer", "title": "A Performance Guarantee for Spectral Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two-step spectral clustering method, which consists of the Laplacian\neigenmap and a rounding step, is a widely used method for graph partitioning.\nIt can be seen as a natural relaxation to the NP-hard minimum ratio cut\nproblem. In this paper we study the central question: when is spectral\nclustering able to find the global solution to the minimum ratio cut problem?\nFirst we provide a condition that naturally depends on the intra- and\ninter-cluster connectivities of a given partition under which we may certify\nthat this partition is the solution to the minimum ratio cut problem. Then we\ndevelop a deterministic two-to-infinity norm perturbation bound for the the\ninvariant subspace of the graph Laplacian that corresponds to the $k$ smallest\neigenvalues. Finally by combining these two results we give a condition under\nwhich spectral clustering is guaranteed to output the global solution to the\nminimum ratio cut problem, which serves as a performance guarantee for spectral\nclustering.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 22:03:43 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Boedihardjo", "March", ""], ["Deng", "Shaofeng", ""], ["Strohmer", "Thomas", ""]]}, {"id": "2007.05646", "submitter": "Tom Bertalan", "authors": "Tom Bertalan and Felix Dietrich and Ioannis G. Kevrekidis", "title": "Transformations between deep neural networks", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to test, and when possible establish, an equivalence between two\ndifferent artificial neural networks by attempting to construct a data-driven\ntransformation between them, using manifold-learning techniques. In particular,\nwe employ diffusion maps with a Mahalanobis-like metric. If the construction\nsucceeds, the two networks can be thought of as belonging to the same\nequivalence class.\n  We first discuss transformation functions between only the outputs of the two\nnetworks; we then also consider transformations that take into account outputs\n(activations) of a number of internal neurons from each network. In general,\nWhitney's theorem dictates the number of measurements from one of the networks\nrequired to reconstruct each and every feature of the second network. The\nconstruction of the transformation function relies on a consistent, intrinsic\nrepresentation of the network input space.\n  We illustrate our algorithm by matching neural network pairs trained to learn\n(a) observations of scalar functions; (b) observations of two-dimensional\nvector fields; and (c) representations of images of a moving three-dimensional\nobject (a rotating horse). The construction of such equivalence classes across\ndifferent network instantiations clearly relates to transfer learning. We also\nexpect that it will be valuable in establishing equivalence between different\nMachine Learning-based models of the same phenomenon observed through different\ninstruments and by different research groups.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 23:32:12 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 04:35:51 GMT"}, {"version": "v3", "created": "Thu, 14 Jan 2021 16:56:27 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Bertalan", "Tom", ""], ["Dietrich", "Felix", ""], ["Kevrekidis", "Ioannis G.", ""]]}, {"id": "2007.05665", "submitter": "Mark Bun", "authors": "Mark Bun", "title": "A Computational Separation between Private Learning and Online Learning", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent line of work has shown a qualitative equivalence between\ndifferentially private PAC learning and online learning: A concept class is\nprivately learnable if and only if it is online learnable with a finite mistake\nbound. However, both directions of this equivalence incur significant losses in\nboth sample and computational efficiency. Studying a special case of this\nconnection, Gonen, Hazan, and Moran (NeurIPS 2019) showed that uniform or\nhighly sample-efficient pure-private learners can be time-efficiently compiled\ninto online learners. We show that, assuming the existence of one-way\nfunctions, such an efficient conversion is impossible even for general\npure-private learners with polynomial sample complexity. This resolves a\nquestion of Neel, Roth, and Wu (FOCS 2019).\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 02:41:54 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Bun", "Mark", ""]]}, {"id": "2007.05670", "submitter": "Yimin Huang", "authors": "Yimin Huang, Yujun Li, Hanrong Ye, Zhenguo Li, Zhihua Zhang", "title": "An Asymptotically Optimal Multi-Armed Bandit Algorithm and\n  Hyperparameter Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evaluation of hyperparameters, neural architectures, or data augmentation\npolicies becomes a critical model selection problem in advanced deep learning\nwith a large hyperparameter search space. In this paper, we propose an\nefficient and robust bandit-based algorithm called Sub-Sampling (SS) in the\nscenario of hyperparameter search evaluation. It evaluates the potential of\nhyperparameters by the sub-samples of observations and is theoretically proved\nto be optimal under the criterion of cumulative regret. We further combine SS\nwith Bayesian Optimization and develop a novel hyperparameter optimization\nalgorithm called BOSS. Empirical studies validate our theoretical arguments of\nSS and demonstrate the superior performance of BOSS on a number of\napplications, including Neural Architecture Search (NAS), Data Augmentation\n(DA), Object Detection (OD), and Reinforcement Learning (RL).\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 03:15:21 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 10:28:43 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Huang", "Yimin", ""], ["Li", "Yujun", ""], ["Ye", "Hanrong", ""], ["Li", "Zhenguo", ""], ["Zhang", "Zhihua", ""]]}, {"id": "2007.05675", "submitter": "Jinhai Yang", "authors": "Jinhai Yang, Hua Yang, Lin Chen", "title": "Towards Cross-Granularity Few-Shot Learning: Coarse-to-Fine\n  Pseudo-Labeling with Visual-Semantic Meta-Embedding", "comments": "Accepted by ACM MM 2021", "journal-ref": null, "doi": "10.1145/3474085.3475200", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning aims at rapidly adapting to novel categories with only a\nhandful of samples at test time, which has been predominantly tackled with the\nidea of meta-learning. However, meta-learning approaches essentially learn\nacross a variety of few-shot tasks and thus still require large-scale training\ndata with fine-grained supervision to derive a generalized model, thereby\ninvolving prohibitive annotation cost. In this paper, we advance the few-shot\nclassification paradigm towards a more challenging scenario, i.e.,\ncross-granularity few-shot classification, where the model observes only coarse\nlabels during training while is expected to perform fine-grained classification\nduring testing. This task largely relieves the annotation cost since\nfine-grained labeling usually requires strong domain-specific expertise. To\nbridge the cross-granularity gap, we approximate the fine-grained data\ndistribution by greedy clustering of each coarse-class into pseudo-fine-classes\naccording to the similarity of image embeddings. We then propose a\nmeta-embedder that jointly optimizes the visual- and semantic-discrimination,\nin both instance-wise and coarse class-wise, to obtain a good feature space for\nthis coarse-to-fine pseudo-labeling process. Extensive experiments and ablation\nstudies are conducted to demonstrate the effectiveness and robustness of our\napproach on three representative datasets.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 03:44:21 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 02:46:54 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2021 12:39:41 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Yang", "Jinhai", ""], ["Yang", "Hua", ""], ["Chen", "Lin", ""]]}, {"id": "2007.05683", "submitter": "Zheda Mai", "authors": "Zheda Mai, Hyunwoo Kim, Jihwan Jeong, Scott Sanner", "title": "Batch-level Experience Replay with Review for Continual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning is a branch of deep learning that seeks to strike a\nbalance between learning stability and plasticity. The CVPR 2020 CLVision\nContinual Learning for Computer Vision challenge is dedicated to evaluating and\nadvancing the current state-of-the-art continual learning methods using the\nCORe50 dataset with three different continual learning scenarios. This paper\npresents our approach, called Batch-level Experience Replay with Review, to\nthis challenge. Our team achieved the 1'st place in all three scenarios out of\n79 participated teams. The codebase of our implementation is publicly available\nat https://github.com/RaptorMai/CVPR20_CLVision_challenge\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 05:20:09 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Mai", "Zheda", ""], ["Kim", "Hyunwoo", ""], ["Jeong", "Jihwan", ""], ["Sanner", "Scott", ""]]}, {"id": "2007.05690", "submitter": "Kaixiang Lin", "authors": "Zhaonan Qu, Kaixiang Lin, Jayant Kalagnanam, Zhaojian Li, Jiayu Zhou,\n  Zhengyuan Zhou", "title": "Federated Learning's Blessing: FedAvg has Linear Speedup", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) learns a model jointly from a set of participating\ndevices without sharing each other's privately held data. The characteristics\nof non-iid data across the network, low device participation, and the mandate\nthat data remain private bring challenges in understanding the convergence of\nFL algorithms, particularly in regards to how convergence scales with the\nnumber of participating devices. In this paper, we focus on Federated Averaging\n(FedAvg)--the most widely used and effective FL algorithm in use today--and\nprovide a comprehensive study of its convergence rate. Although FedAvg has\nrecently been studied by an emerging line of literature, it remains open as to\nhow FedAvg's convergence scales with the number of participating devices in the\nFL setting--a crucial question whose answer would shed light on the performance\nof FedAvg in large FL systems. We fill this gap by establishing convergence\nguarantees for FedAvg under three classes of problems: strongly convex smooth,\nconvex smooth, and overparameterized strongly convex smooth problems. We show\nthat FedAvg enjoys linear speedup in each case, although with different\nconvergence rates. For each class, we also characterize the corresponding\nconvergence rates for the Nesterov accelerated FedAvg algorithm in the FL\nsetting: to the best of our knowledge, these are the first linear speedup\nguarantees for FedAvg when Nesterov acceleration is used. To accelerate FedAvg,\nwe also design a new momentum-based FL algorithm that further improves the\nconvergence rate in overparameterized linear regression problems. Empirical\nstudies of the algorithms in various settings have supported our theoretical\nresults.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 05:59:08 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Qu", "Zhaonan", ""], ["Lin", "Kaixiang", ""], ["Kalagnanam", "Jayant", ""], ["Li", "Zhaojian", ""], ["Zhou", "Jiayu", ""], ["Zhou", "Zhengyuan", ""]]}, {"id": "2007.05692", "submitter": "Xueshuang Xiang", "authors": "Xuejiao Liu and Xueshuang Xiang", "title": "How Does GAN-based Semi-supervised Learning Work?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have been widely used and have\nachieved competitive results in semi-supervised learning. This paper\ntheoretically analyzes how GAN-based semi-supervised learning (GAN-SSL) works.\nWe first prove that, given a fixed generator, optimizing the discriminator of\nGAN-SSL is equivalent to optimizing that of supervised learning. Thus, the\noptimal discriminator in GAN-SSL is expected to be perfect on labeled data.\nThen, if the perfect discriminator can further cause the optimization objective\nto reach its theoretical maximum, the optimal generator will match the true\ndata distribution. Since it is impossible to reach the theoretical maximum in\npractice, one cannot expect to obtain a perfect generator for generating data,\nwhich is apparently different from the objective of GANs. Furthermore, if the\nlabeled data can traverse all connected subdomains of the data manifold, which\nis reasonable in semi-supervised classification, we additionally expect the\noptimal discriminator in GAN-SSL to also be perfect on unlabeled data. In\nconclusion, the minimax optimization in GAN-SSL will theoretically output a\nperfect discriminator on both labeled and unlabeled data by unexpectedly\nlearning an imperfect generator, i.e., GAN-SSL can effectively improve the\ngeneralization ability of the discriminator by leveraging unlabeled\ninformation.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 06:16:43 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Liu", "Xuejiao", ""], ["Xiang", "Xueshuang", ""]]}, {"id": "2007.05694", "submitter": "Ugurkan Ates", "authors": "Ugurkan Ates", "title": "Long-Term Planning with Deep Reinforcement Learning on Autonomous Drones", "comments": "Submitted to Association for the Advancement of Artificial\n  Intelligence(AAAI) 2020 Fall Symposium Series", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study a long-term planning scenario that is based on drone\nracing competitions held in real life. We conducted this experiment on a\nframework created for \"Game of Drones: Drone Racing Competition\" at NeurIPS\n2019. The racing environment was created using Microsoft's AirSim Drone Racing\nLab. A reinforcement learning agent, a simulated quadrotor in our case, has\ntrained with the Policy Proximal Optimization(PPO) algorithm was able to\nsuccessfully compete against another simulated quadrotor that was running a\nclassical path planning algorithm. Agent observations consist of data from IMU\nsensors, GPS coordinates of drone obtained through simulation and opponent\ndrone GPS information. Using opponent drone GPS information during training\nhelps dealing with complex state spaces, serving as expert guidance allows for\nefficient and stable training process. All experiments performed in this paper\ncan be found and reproduced with code at our GitHub repository\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 06:16:50 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Ates", "Ugurkan", ""]]}, {"id": "2007.05700", "submitter": "Jiajun Zhou", "authors": "Jiajun Zhou, Jie Shen, Shanqing Yu, Guanrong Chen, Qi Xuan", "title": "M-Evolve: Structural-Mapping-Based Data Augmentation for Graph\n  Classification", "comments": "11 pages, 9 figures. arXiv admin note: text overlap with\n  arXiv:2009.09863", "journal-ref": null, "doi": "10.1109/TNSE.2020.3032950", "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph classification, which aims to identify the category labels of graphs,\nplays a significant role in drug classification, toxicity detection, protein\nanalysis etc. However, the limitation of scale in the benchmark datasets makes\nit easy for graph classification models to fall into over-fitting and\nundergeneralization. To improve this, we introduce data augmentation on graphs\n(i.e. graph augmentation) and present four methods:random mapping,\nvertex-similarity mapping, motif-random mapping and motif-similarity mapping,\nto generate more weakly labeled data for small-scale benchmark datasets via\nheuristic transformation of graph structures. Furthermore, we propose a generic\nmodel evolution framework, named M-Evolve, which combines graph augmentation,\ndata filtration and model retraining to optimize pre-trained graph classifiers.\nExperiments on six benchmark datasets demonstrate that the proposed framework\nhelps existing graph classification models alleviate over-fitting and\nundergeneralization in the training on small-scale benchmark datasets, which\nsuccessfully yields an average improvement of 3 - 13% accuracy on graph\nclassification tasks.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 06:58:07 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 05:09:42 GMT"}, {"version": "v3", "created": "Sat, 20 Mar 2021 14:25:41 GMT"}, {"version": "v4", "created": "Sat, 3 Apr 2021 12:48:26 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Zhou", "Jiajun", ""], ["Shen", "Jie", ""], ["Yu", "Shanqing", ""], ["Chen", "Guanrong", ""], ["Xuan", "Qi", ""]]}, {"id": "2007.05721", "submitter": "Alvaro H. C. Correia", "authors": "Alvaro H. C. Correia, Robert Peharz, Cassio de Campos", "title": "Towards Robust Classification with Deep Generative Forests", "comments": "Presented at the ICML 2020 Workshop on Uncertainty and Robustness in\n  Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Decision Trees and Random Forests are among the most widely used machine\nlearning models, and often achieve state-of-the-art performance in tabular,\ndomain-agnostic datasets. Nonetheless, being primarily discriminative models\nthey lack principled methods to manipulate the uncertainty of predictions. In\nthis paper, we exploit Generative Forests (GeFs), a recent class of deep\nprobabilistic models that addresses these issues by extending Random Forests to\ngenerative models representing the full joint distribution over the feature\nspace. We demonstrate that GeFs are uncertainty-aware classifiers, capable of\nmeasuring the robustness of each prediction as well as detecting\nout-of-distribution samples.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 08:57:52 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Correia", "Alvaro H. C.", ""], ["Peharz", "Robert", ""], ["de Campos", "Cassio", ""]]}, {"id": "2007.05724", "submitter": "Hedda Cohen Indelman", "authors": "Hedda Cohen Indelman, Tamir Hazan", "title": "Learning Randomly Perturbed Structured Predictors for Direct Loss\n  Minimization", "comments": "Proceedings of the 38th International Conference on Machine Learning,\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct loss minimization is a popular approach for learning predictors over\nstructured label spaces. This approach is computationally appealing as it\nreplaces integration with optimization and allows to propagate gradients in a\ndeep net using loss-perturbed prediction. Recently, this technique was extended\nto generative models, while introducing a randomized predictor that samples a\nstructure from a randomly perturbed score function. In this work, we learn the\nvariance of these randomized structured predictors and show that it balances\nbetter between the learned score function and the randomized noise in\nstructured prediction. We demonstrate empirically the effectiveness of learning\nthe balance between the signal and the random noise in structured discrete\nspaces.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 08:59:11 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 08:55:45 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Indelman", "Hedda Cohen", ""], ["Hazan", "Tamir", ""]]}, {"id": "2007.05732", "submitter": "Massimiliano Pontil", "authors": "Giulia Denevi, Dimitris Stamos, Massimiliano Pontil", "title": "Online Parameter-Free Learning of Multiple Low Variance Tasks", "comments": null, "journal-ref": "Conference on Uncertainty in Artificial Intelligence (UAI) 2020", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to learn a common bias vector for a growing sequence of\nlow-variance tasks. Unlike state-of-the-art approaches, our method does not\nrequire tuning any hyper-parameter. Our approach is presented in the\nnon-statistical setting and can be of two variants. The \"aggressive\" one\nupdates the bias after each datapoint, the \"lazy\" one updates the bias only at\nthe end of each task. We derive an across-tasks regret bound for the method.\nWhen compared to state-of-the-art approaches, the aggressive variant returns\nfaster rates, the lazy one recovers standard rates, but with no need of tuning\nhyper-parameters. We then adapt the methods to the statistical setting: the\naggressive variant becomes a multi-task learning method, the lazy one a\nmeta-learning method. Experiments confirm the effectiveness of our methods in\npractice.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 09:52:53 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Denevi", "Giulia", ""], ["Stamos", "Dimitris", ""], ["Pontil", "Massimiliano", ""]]}, {"id": "2007.05742", "submitter": "Zhao Kang", "authors": "Zhao Kang and Xiao Lu and Jian Liang and Kun Bai and Zenglin Xu", "title": "Relation-Guided Representation Learning", "comments": "Appear in Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep auto-encoders (DAEs) have achieved great success in learning data\nrepresentations via the powerful representability of neural networks. But most\nDAEs only focus on the most dominant structures which are able to reconstruct\nthe data from a latent space and neglect rich latent structural information. In\nthis work, we propose a new representation learning method that explicitly\nmodels and leverages sample relations, which in turn is used as supervision to\nguide the representation learning. Different from previous work, our framework\nwell preserves the relations between samples. Since the prediction of pairwise\nrelations themselves is a fundamental problem, our model adaptively learns them\nfrom data. This provides much flexibility to encode real data manifold. The\nimportant role of relation and representation learning is evaluated on the\nclustering task. Extensive experiments on benchmark data sets demonstrate the\nsuperiority of our approach. By seeking to embed samples into subspace, we\nfurther show that our method can address the large-scale and out-of-sample\nproblem.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 10:57:45 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Kang", "Zhao", ""], ["Lu", "Xiao", ""], ["Liang", "Jian", ""], ["Bai", "Kun", ""], ["Xu", "Zenglin", ""]]}, {"id": "2007.05756", "submitter": "Boris Knyazev", "authors": "Boris Knyazev, Harm de Vries, C\\u{a}t\\u{a}lina Cangea, Graham W.\n  Taylor, Aaron Courville, Eugene Belilovsky", "title": "Generative Compositional Augmentations for Scene Graph Prediction", "comments": "extended version with major updates; 19 pages; the code is available\n  at https://github.com/bknyaz/sgg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring objects and their relationships from an image in the form of a\nscene graph is useful in many applications at the intersection of vision and\nlanguage. In this work, we consider a challenging problem of compositional\ngeneralization that emerges in this task due to a long tail data distribution.\nCurrent scene graph generation models are trained on a tiny fraction of the\ndistribution corresponding to the most frequent compositions, e.g. <cup, on,\ntable>. However, test images might contain zero- and few-shot compositions of\nobjects and relationships, e.g. <cup, on, surfboard>. Despite each of the\nobject categories and the predicate (e.g. 'on') being frequent in the training\ndata, the models often fail to properly understand such unseen or rare\ncompositions. To improve generalization, it is natural to attempt increasing\nthe diversity of the training distribution. However, in the graph domain this\nis non-trivial. To that end, we propose a method to synthesize rare yet\nplausible scene graphs by perturbing real ones. We then propose and empirically\nstudy a model based on conditional generative adversarial networks (GANs) that\nallows us to generate visual features of perturbed scene graphs and learn from\nthem in a joint fashion. When evaluated on the Visual Genome dataset, our\napproach yields marginal, but consistent improvements in zero- and few-shot\nmetrics. We analyze the limitations of our approach indicating promising\ndirections for future research.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 12:11:53 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 17:42:25 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Knyazev", "Boris", ""], ["de Vries", "Harm", ""], ["Cangea", "C\u0103t\u0103lina", ""], ["Taylor", "Graham W.", ""], ["Courville", "Aaron", ""], ["Belilovsky", "Eugene", ""]]}, {"id": "2007.05758", "submitter": "Kshitij Goyal", "authors": "Kshitij Goyal, Sebastijan Dumancic, Hendrik Blockeel", "title": "Feature Interactions in XGBoost", "comments": "7 pages, 2 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate how feature interactions can be identified to\nbe used as constraints in the gradient boosting tree models using XGBoost's\nimplementation. Our results show that accurate identification of these\nconstraints can help improve the performance of baseline XGBoost model\nsignificantly. Further, the improvement in the model structure can also lead to\nbetter interpretability.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 12:17:40 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Goyal", "Kshitij", ""], ["Dumancic", "Sebastijan", ""], ["Blockeel", "Hendrik", ""]]}, {"id": "2007.05783", "submitter": "Xiao Huang", "authors": "Dong Xu, Xiao Huang, Joseph Mango, Xiang Li, Zhenlong Li", "title": "Simulating multi-exit evacuation using deep reinforcement learning", "comments": "25 pages, 5 figures, submitted to Transactions in GIS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conventional simulations on multi-exit indoor evacuation focus primarily on\nhow to determine a reasonable exit based on numerous factors in a changing\nenvironment. Results commonly include some congested and other under-utilized\nexits, especially with massive pedestrians. We propose a multi-exit evacuation\nsimulation based on Deep Reinforcement Learning (DRL), referred to as the\nMultiExit-DRL, which involves in a Deep Neural Network (DNN) framework to\nfacilitate state-to-action mapping. The DNN framework applies Rainbow Deep\nQ-Network (DQN), a DRL algorithm that integrates several advanced DQN methods,\nto improve data utilization and algorithm stability, and further divides the\naction space into eight isometric directions for possible pedestrian choices.\nWe compare MultiExit-DRL with two conventional multi-exit evacuation simulation\nmodels in three separate scenarios: 1) varying pedestrian distribution ratios,\n2) varying exit width ratios, and 3) varying open schedules for an exit. The\nresults show that MultiExit-DRL presents great learning efficiency while\nreducing the total number of evacuation frames in all designed experiments. In\naddition, the integration of DRL allows pedestrians to explore other potential\nexits and helps determine optimal directions, leading to the high efficiency of\nexit utilization.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 14:27:02 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Xu", "Dong", ""], ["Huang", "Xiao", ""], ["Mango", "Joseph", ""], ["Li", "Xiang", ""], ["Li", "Zhenlong", ""]]}, {"id": "2007.05817", "submitter": "Guanxiong Liu", "authors": "Guanxiong Liu, Issa Khalil, Abdallah Khreishah, Abdulelah Algosaibi,\n  Adel Aldalbahi, Mohammed Alaneem, Abdulaziz Alhumam, Mohammed Anan", "title": "ManiGen: A Manifold Aided Black-box Generator of Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models, especially neural network (NN) classifiers, have\nacceptable performance and accuracy that leads to their wide adoption in\ndifferent aspects of our daily lives. The underlying assumption is that these\nmodels are generated and used in attack free scenarios. However, it has been\nshown that neural network based classifiers are vulnerable to adversarial\nexamples. Adversarial examples are inputs with special perturbations that are\nignored by human eyes while can mislead NN classifiers. Most of the existing\nmethods for generating such perturbations require a certain level of knowledge\nabout the target classifier, which makes them not very practical. For example,\nsome generators require knowledge of pre-softmax logits while others utilize\nprediction scores.\n  In this paper, we design a practical black-box adversarial example generator,\ndubbed ManiGen. ManiGen does not require any knowledge of the inner state of\nthe target classifier. It generates adversarial examples by searching along the\nmanifold, which is a concise representation of input data. Through extensive\nset of experiments on different datasets, we show that (1) adversarial examples\ngenerated by ManiGen can mislead standalone classifiers by being as successful\nas the state-of-the-art white-box generator, Carlini, and (2) adversarial\nexamples generated by ManiGen can more effectively attack classifiers with\nstate-of-the-art defenses.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 17:34:17 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Liu", "Guanxiong", ""], ["Khalil", "Issa", ""], ["Khreishah", "Abdallah", ""], ["Algosaibi", "Abdulelah", ""], ["Aldalbahi", "Adel", ""], ["Alaneem", "Mohammed", ""], ["Alhumam", "Abdulaziz", ""], ["Anan", "Mohammed", ""]]}, {"id": "2007.05824", "submitter": "Taiji Suzuki", "authors": "Taiji Suzuki", "title": "Generalization bound of globally optimal non-convex neural network\n  training: Transportation map estimation by infinite dimensional Langevin\n  dynamics", "comments": "Accepted in NeurIPS2020 (Thirty-fourth Conference on Neural\n  Information Processing Systems)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new theoretical framework to analyze deep learning\noptimization with connection to its generalization error. Existing frameworks\nsuch as mean field theory and neural tangent kernel theory for neural network\noptimization analysis typically require taking limit of infinite width of the\nnetwork to show its global convergence. This potentially makes it difficult to\ndirectly deal with finite width network; especially in the neural tangent\nkernel regime, we cannot reveal favorable properties of neural networks beyond\nkernel methods. To realize more natural analysis, we consider a completely\ndifferent approach in which we formulate the parameter training as a\ntransportation map estimation and show its global convergence via the theory of\nthe infinite dimensional Langevin dynamics. This enables us to analyze narrow\nand wide networks in a unifying manner. Moreover, we give generalization gap\nand excess risk bounds for the solution obtained by the dynamics. The excess\nrisk bound achieves the so-called fast learning rate. In particular, we show an\nexponential convergence for a classification problem and a minimax optimal rate\nfor a regression problem.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 18:19:50 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 18:10:22 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Suzuki", "Taiji", ""]]}, {"id": "2007.05830", "submitter": "M. F. Mridha", "authors": "Abu Quwsar Ohi, M. F. Mridha, Farisa Benta Safir, Md. Abdul Hamid,\n  Muhammad Mostafa Monowar", "title": "AutoEmbedder: A semi-supervised DNN embedding system for clustering", "comments": "The manuscript is accepted and published in Knowledge-Based System", "journal-ref": "Knowledge-Based Systems, p.106190 (2020)", "doi": "10.1016/j.knosys.2020.106190", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is widely used in unsupervised learning method that deals with\nunlabeled data. Deep clustering has become a popular study area that relates\nclustering with Deep Neural Network (DNN) architecture. Deep clustering method\ndownsamples high dimensional data, which may also relate clustering loss. Deep\nclustering is also introduced in semi-supervised learning (SSL). Most SSL\nmethods depend on pairwise constraint information, which is a matrix containing\nknowledge if data pairs can be in the same cluster or not. This paper\nintroduces a novel embedding system named AutoEmbedder, that downsamples higher\ndimensional data to clusterable embedding points. To the best of our knowledge,\nthis is the first research endeavor that relates to traditional classifier DNN\narchitecture with a pairwise loss reduction technique. The training process is\nsemi-supervised and uses Siamese network architecture to compute pairwise\nconstraint loss in the feature learning phase. The AutoEmbedder outperforms\nmost of the existing DNN based semi-supervised methods tested on famous\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 19:00:45 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Ohi", "Abu Quwsar", ""], ["Mridha", "M. F.", ""], ["Safir", "Farisa Benta", ""], ["Hamid", "Md. Abdul", ""], ["Monowar", "Muhammad Mostafa", ""]]}, {"id": "2007.05836", "submitter": "G\\\"orkem Algan", "authors": "G\\\"orkem Algan, Ilkay Ulusoy", "title": "Meta Soft Label Generation for Noisy Labels", "comments": "Accepted by ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existence of noisy labels in the dataset causes significant performance\ndegradation for deep neural networks (DNNs). To address this problem, we\npropose a Meta Soft Label Generation algorithm called MSLG, which can jointly\ngenerate soft labels using meta-learning techniques and learn DNN parameters in\nan end-to-end fashion. Our approach adapts the meta-learning paradigm to\nestimate optimal label distribution by checking gradient directions on both\nnoisy training data and noise-free meta-data. In order to iteratively update\nsoft labels, meta-gradient descent step is performed on estimated labels, which\nwould minimize the loss of noise-free meta samples. In each iteration, the base\nclassifier is trained on estimated meta labels. MSLG is model-agnostic and can\nbe added on top of any existing model at hand with ease. We performed extensive\nexperiments on CIFAR10, Clothing1M and Food101N datasets. Results show that our\napproach outperforms other state-of-the-art methods by a large margin.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 19:37:44 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 09:59:38 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Algan", "G\u00f6rkem", ""], ["Ulusoy", "Ilkay", ""]]}, {"id": "2007.05838", "submitter": "Alexander Tschantz", "authors": "Alexander Tschantz, Beren Millidge, Anil K. Seth, Christopher L.\n  Buckley", "title": "Control as Hybrid Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of reinforcement learning can be split into model-based and\nmodel-free methods. Here, we unify these approaches by casting model-free\npolicy optimisation as amortised variational inference, and model-based\nplanning as iterative variational inference, within a `control as hybrid\ninference' (CHI) framework. We present an implementation of CHI which naturally\nmediates the balance between iterative and amortised inference. Using a\ndidactic experiment, we demonstrate that the proposed algorithm operates in a\nmodel-based manner at the onset of learning, before converging to a model-free\nalgorithm once sufficient data have been collected. We verify the scalability\nof our algorithm on a continuous control benchmark, demonstrating that it\noutperforms strong model-free and model-based baselines. CHI thus provides a\nprincipled framework for harnessing the sample efficiency of model-based\nplanning while retaining the asymptotic performance of model-free policy\noptimisation.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 19:44:09 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Tschantz", "Alexander", ""], ["Millidge", "Beren", ""], ["Seth", "Anil K.", ""], ["Buckley", "Christopher L.", ""]]}, {"id": "2007.05840", "submitter": "Anoop Cherian", "authors": "Anoop Cherian, Shuchin Aeron", "title": "Representation Learning via Adversarially-Contrastive Optimal Transport", "comments": "Accepted at ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of learning compact (low-dimensional)\nrepresentations for sequential data that captures its implicit spatio-temporal\ncues. To maximize extraction of such informative cues from the data, we set the\nproblem within the context of contrastive representation learning and to that\nend propose a novel objective via optimal transport. Specifically, our\nformulation seeks a low-dimensional subspace representation of the data that\njointly (i) maximizes the distance of the data (embedded in this subspace) from\nan adversarial data distribution under the optimal transport, a.k.a. the\nWasserstein distance, (ii) captures the temporal order, and (iii) minimizes the\ndata distortion. To generate the adversarial distribution, we propose a novel\nframework connecting Wasserstein GANs with a classifier, allowing a principled\nmechanism for producing good negative distributions for contrastive learning,\nwhich is currently a challenging problem. Our full objective is cast as a\nsubspace learning problem on the Grassmann manifold and solved via Riemannian\noptimization. To empirically study our formulation, we provide experiments on\nthe task of human action recognition in video sequences. Our results\ndemonstrate competitive performance against challenging baselines.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 19:46:18 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Cherian", "Anoop", ""], ["Aeron", "Shuchin", ""]]}, {"id": "2007.05852", "submitter": "Arman Adibi", "authors": "Arman Adibi, Aryan Mokhtari, Hamed Hassani", "title": "Submodular Meta-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a discrete variant of the meta-learning\nframework. Meta-learning aims at exploiting prior experience and data to\nimprove performance on future tasks. By now, there exist numerous formulations\nfor meta-learning in the continuous domain. Notably, the Model-Agnostic\nMeta-Learning (MAML) formulation views each task as a continuous optimization\nproblem and based on prior data learns a suitable initialization that can be\nadapted to new, unseen tasks after a few simple gradient updates. Motivated by\nthis terminology, we propose a novel meta-learning framework in the discrete\ndomain where each task is equivalent to maximizing a set function under a\ncardinality constraint. Our approach aims at using prior data, i.e., previously\nvisited tasks, to train a proper initial solution set that can be quickly\nadapted to a new task at a relatively low computational cost. This approach\nleads to (i) a personalized solution for each individual task, and (ii)\nsignificantly reduced computational cost at test time compared to the case\nwhere the solution is fully optimized once the new task is revealed. The\ntraining procedure is performed by solving a challenging discrete optimization\nproblem for which we present deterministic and randomized algorithms. In the\ncase where the tasks are monotone and submodular, we show strong theoretical\nguarantees for our proposed methods even though the training objective may not\nbe submodular. We also demonstrate the effectiveness of our framework on two\nreal-world problem instances where we observe that our methods lead to a\nsignificant reduction in computational complexity in solving the new tasks\nwhile incurring a small performance loss compared to when the tasks are fully\noptimized.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 21:02:48 GMT"}, {"version": "v2", "created": "Sun, 10 Jan 2021 00:39:23 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Adibi", "Arman", ""], ["Mokhtari", "Aryan", ""], ["Hassani", "Hamed", ""]]}, {"id": "2007.05864", "submitter": "Bobby He", "authors": "Bobby He, Balaji Lakshminarayanan and Yee Whye Teh", "title": "Bayesian Deep Ensembles via the Neural Tangent Kernel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the link between deep ensembles and Gaussian processes (GPs)\nthrough the lens of the Neural Tangent Kernel (NTK): a recent development in\nunderstanding the training dynamics of wide neural networks (NNs). Previous\nwork has shown that even in the infinite width limit, when NNs become GPs,\nthere is no GP posterior interpretation to a deep ensemble trained with squared\nerror loss. We introduce a simple modification to standard deep ensembles\ntraining, through addition of a computationally-tractable, randomised and\nuntrainable function to each ensemble member, that enables a posterior\ninterpretation in the infinite width limit. When ensembled together, our\ntrained NNs give an approximation to a posterior predictive distribution, and\nwe prove that our Bayesian deep ensembles make more conservative predictions\nthan standard deep ensembles in the infinite width limit. Finally, using finite\nwidth NNs we demonstrate that our Bayesian deep ensembles faithfully emulate\nthe analytic posterior predictive when available, and can outperform standard\ndeep ensembles in various out-of-distribution settings, for both regression and\nclassification tasks.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 22:10:52 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 16:51:14 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["He", "Bobby", ""], ["Lakshminarayanan", "Balaji", ""], ["Teh", "Yee Whye", ""]]}, {"id": "2007.05869", "submitter": "N. Benjamin Erichson", "authors": "Francisco Utrera, Evan Kravitz, N. Benjamin Erichson, Rajiv Khanna and\n  Michael W. Mahoney", "title": "Adversarially-Trained Deep Nets Transfer Better: Illustration on Image\n  Classification", "comments": "Published as a conference paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning has emerged as a powerful methodology for adapting\npre-trained deep neural networks on image recognition tasks to new domains.\nThis process consists of taking a neural network pre-trained on a large\nfeature-rich source dataset, freezing the early layers that encode essential\ngeneric image properties, and then fine-tuning the last few layers in order to\ncapture specific information related to the target situation. This approach is\nparticularly useful when only limited or weakly labeled data are available for\nthe new task. In this work, we demonstrate that adversarially-trained models\ntransfer better than non-adversarially-trained models, especially if only\nlimited data are available for the new domain task. Further, we observe that\nadversarial training biases the learnt representations to retaining shapes, as\nopposed to textures, which impacts the transferability of the source models.\nFinally, through the lens of influence functions, we discover that transferred\nadversarially-trained models contain more human-identifiable semantic\ninformation, which explains -- at least partly -- why adversarially-trained\nmodels transfer better.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 22:48:42 GMT"}, {"version": "v2", "created": "Sat, 24 Apr 2021 03:21:05 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Utrera", "Francisco", ""], ["Kravitz", "Evan", ""], ["Erichson", "N. Benjamin", ""], ["Khanna", "Rajiv", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "2007.05879", "submitter": "Gaurav Rajavendra Reddy", "authors": "Gaurav Rajavendra Reddy, Constantinos Xanthopoulos and Yiorgos Makris", "title": "On Improving Hotspot Detection Through Synthetic Pattern-Based Database\n  Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous technology scaling and the introduction of advanced technology\nnodes in Integrated Circuit (IC) fabrication is constantly exposing new\nmanufacturability issues. One such issue, stemming from complex interaction\nbetween design and process, is the problem of design hotspots. Such hotspots\nare known to vary from design to design and, ideally, should be predicted early\nand corrected in the design stage itself, as opposed to relying on the foundry\nto develop process fixes for every hotspot, which would be intractable. In the\npast, various efforts have been made to address this issue by using a known\ndatabase of hotspots as the source of information. The majority of these\nefforts use either Machine Learning (ML) or Pattern Matching (PM) techniques to\nidentify and predict hotspots in new incoming designs. However, almost all of\nthem suffer from high false-alarm rates, mainly because they are oblivious to\nthe root causes of hotspots. In this work, we seek to address this limitation\nby using a novel database enhancement approach through synthetic pattern\ngeneration based on carefully crafted Design of Experiments (DOEs).\nEffectiveness of the proposed method against the state-of-the-art is evaluated\non a 45nm process using industry-standard tools and designs.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 00:38:51 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Reddy", "Gaurav Rajavendra", ""], ["Xanthopoulos", "Constantinos", ""], ["Makris", "Yiorgos", ""]]}, {"id": "2007.05881", "submitter": "Marko Smilevski", "authors": "Marko Smilevski", "title": "Applying recent advances in Visual Question Answering to Record Linkage", "comments": "48 pages, 15 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DB stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Multi-modal Record Linkage is the process of matching multi-modal records\nfrom multiple sources that represent the same entity. This field has not been\nexplored in research and we propose two solutions based on Deep Learning\narchitectures that are inspired by recent work in Visual Question Answering.\nThe neural networks we propose use two different fusion modules, the Recurrent\nNeural Network + Convolutional Neural Network fusion module and the Stacked\nAttention Network fusion module, that jointly combine the visual and the\ntextual data of the records. The output of these fusion models is the input of\na Siamese Neural Network that computes the similarity of the records. Using\ndata from the Avito Duplicate Advertisements Detection dataset, we train these\nsolutions and from the experiments, we concluded that the Recurrent Neural\nNetwork + Convolutional Neural Network fusion module outperforms a simple model\nthat uses hand-crafted features. We also find that the Recurrent Neural Network\n+ Convolutional Neural Network fusion module classifies dissimilar\nadvertisements as similar more frequently if their average description is\nbigger than 40 words. We conclude that the reason for this is that the longer\nadvertisements have a different distribution then the shorter advertisements\nwho are more prevalent in the dataset. In the end, we also conclude that\nfurther research needs to be done with the Stacked Attention Network, to\nfurther explore the effects of the visual data on the performance of the fusion\nmodules.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 01:24:47 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Smilevski", "Marko", ""]]}, {"id": "2007.05896", "submitter": "Evan Liu", "authors": "Evan Zheran Liu, Ramtin Keramati, Sudarshan Seshadri, Kelvin Guu,\n  Panupong Pasupat, Emma Brunskill, Percy Liang", "title": "Learning Abstract Models for Strategic Exploration and Fast Reward\n  Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Model-based reinforcement learning (RL) is appealing because (i) it enables\nplanning and thus more strategic exploration, and (ii) by decoupling dynamics\nfrom rewards, it enables fast transfer to new reward functions. However,\nlearning an accurate Markov Decision Process (MDP) over high-dimensional states\n(e.g., raw pixels) is extremely challenging because it requires function\napproximation, which leads to compounding errors. Instead, to avoid compounding\nerrors, we propose learning an abstract MDP over abstract states:\nlow-dimensional coarse representations of the state (e.g., capturing agent\nposition, ignoring other objects). We assume access to an abstraction function\nthat maps the concrete states to abstract states. In our approach, we construct\nan abstract MDP, which grows through strategic exploration via planning.\nSimilar to hierarchical RL approaches, the abstract actions of the abstract MDP\nare backed by learned subpolicies that navigate between abstract states. Our\napproach achieves strong results on three of the hardest Arcade Learning\nEnvironment games (Montezuma's Revenge, Pitfall!, and Private Eye), including\nsuperhuman performance on Pitfall! without demonstrations. After training on\none task, we can reuse the learned abstract MDP for new reward functions,\nachieving higher reward in 1000x fewer samples than model-free methods trained\nfrom scratch.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 03:33:50 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Liu", "Evan Zheran", ""], ["Keramati", "Ramtin", ""], ["Seshadri", "Sudarshan", ""], ["Guu", "Kelvin", ""], ["Pasupat", "Panupong", ""], ["Brunskill", "Emma", ""], ["Liang", "Percy", ""]]}, {"id": "2007.05929", "submitter": "Ankesh Anand", "authors": "Max Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron\n  Courville, Philip Bachman", "title": "Data-Efficient Reinforcement Learning with Self-Predictive\n  Representations", "comments": "The first two authors contributed equally to this work. v4 includes\n  new ablations and reformatting for ICLR camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep reinforcement learning excels at solving tasks where large amounts\nof data can be collected through virtually unlimited interaction with the\nenvironment, learning from limited interaction remains a key challenge. We\nposit that an agent can learn more efficiently if we augment reward\nmaximization with self-supervised objectives based on structure in its visual\ninput and sequential interaction with the environment. Our method,\nSelf-Predictive Representations(SPR), trains an agent to predict its own latent\nstate representations multiple steps into the future. We compute target\nrepresentations for future states using an encoder which is an exponential\nmoving average of the agent's parameters and we make predictions using a\nlearned transition model. On its own, this future prediction objective\noutperforms prior methods for sample-efficient deep RL from pixels. We further\nimprove performance by adding data augmentation to the future prediction loss,\nwhich forces the agent's representations to be consistent across multiple views\nof an observation. Our full self-supervised objective, which combines future\nprediction and data augmentation, achieves a median human-normalized score of\n0.415 on Atari in a setting limited to 100k steps of environment interaction,\nwhich represents a 55% relative improvement over the previous state-of-the-art.\nNotably, even in this limited data regime, SPR exceeds expert human scores on 7\nout of 26 games. The code associated with this work is available at\nhttps://github.com/mila-iqia/spr\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 07:38:15 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 03:09:35 GMT"}, {"version": "v3", "created": "Sun, 4 Oct 2020 02:15:07 GMT"}, {"version": "v4", "created": "Thu, 20 May 2021 09:15:57 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Schwarzer", "Max", ""], ["Anand", "Ankesh", ""], ["Goel", "Rishab", ""], ["Hjelm", "R Devon", ""], ["Courville", "Aaron", ""], ["Bachman", "Philip", ""]]}, {"id": "2007.05943", "submitter": "Sandor Szedmak", "authors": "Sandor Szedmak (1) Eric Bach (1) ((1) Department of Computer Science,\n  Aalto University)", "title": "On the generalization of Tanimoto-type kernels to real valued functions", "comments": "Pages 12, 3 PDF figures, uses arxiv.sty", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Tanimoto kernel (Jaccard index) is a well known tool to describe the\nsimilarity between sets of binary attributes. It has been extended to the case\nwhen the attributes are nonnegative real values. This paper introduces a more\ngeneral Tanimoto kernel formulation which allows to measure the similarity of\narbitrary real-valued functions. This extension is constructed by unifying the\nrepresentation of the attributes via properly chosen sets. After deriving the\ngeneral form of the kernel, explicit feature representation is extracted from\nthe kernel function, and a simply way of including general kernels into the\nTanimoto kernel is shown. Finally, the kernel is also expressed as a quotient\nof piecewise linear functions, and a smooth approximation is provided.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 09:02:27 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Szedmak", "Sandor", ""], ["Bach", "Eric", ""]]}, {"id": "2007.05970", "submitter": "Tian Bian", "authors": "Tian Bian, Xi Xiao, Tingyang Xu, Yu Rong, Wenbing Huang, Peilin Zhao,\n  Junzhou Huang", "title": "Inverse Graph Identification: Can We Identify Node Labels Given Graph\n  Labels?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Identification (GI) has long been researched in graph learning and is\nessential in certain applications (e.g. social community detection).\nSpecifically, GI requires to predict the label/score of a target graph given\nits collection of node features and edge connections. While this task is\ncommon, more complex cases arise in practice---we are supposed to do the\ninverse thing by, for example, grouping similar users in a social network given\nthe labels of different communities. This triggers an interesting thought: can\nwe identify nodes given the labels of the graphs they belong to? Therefore,\nthis paper defines a novel problem dubbed Inverse Graph Identification (IGI),\nas opposed to GI. Upon a formal discussion of the variants of IGI, we choose a\nparticular case study of node clustering by making use of the graph labels and\nnode features, with an assistance of a hierarchical graph that further\ncharacterizes the connections between different graphs. To address this task,\nwe propose Gaussian Mixture Graph Convolutional Network (GMGCN), a simple yet\neffective method that makes the node-level message passing process using Graph\nAttention Network (GAT) under the protocol of GI and then infers the category\nof each node via a Gaussian Mixture Layer (GML). The training of GMGCN is\nfurther boosted by a proposed consensus loss to take advantage of the structure\nof the hierarchical graph. Extensive experiments are conducted to test the\nrationality of the formulation of IGI. We verify the superiority of the\nproposed method compared to other baselines on several benchmarks we have built\nup. We will release our codes along with the benchmark data to facilitate more\nresearch attention to the IGI problem.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 12:06:17 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Bian", "Tian", ""], ["Xiao", "Xi", ""], ["Xu", "Tingyang", ""], ["Rong", "Yu", ""], ["Huang", "Wenbing", ""], ["Zhao", "Peilin", ""], ["Huang", "Junzhou", ""]]}, {"id": "2007.05975", "submitter": "Benjamin Rubinstein", "authors": "Tobias Edwards, Benjamin I. P. Rubinstein, Zuhe Zhang, Sanming Zhou", "title": "A Graph Symmetrisation Bound on Channel Information Leakage under\n  Blowfish Privacy", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blowfish privacy is a recent generalisation of differential privacy that\nenables improved utility while maintaining privacy policies with semantic\nguarantees, a factor that has driven the popularity of differential privacy in\ncomputer science. This paper relates Blowfish privacy to an important measure\nof privacy loss of information channels from the communications theory\ncommunity: min-entropy leakage. Symmetry in an input data neighbouring relation\nis central to known connections between differential privacy and min-entropy\nleakage. But while differential privacy exhibits strong symmetry, Blowfish\nneighbouring relations correspond to arbitrary simple graphs owing to the\nframework's flexible privacy policies. To bound the min-entropy leakage of\nBlowfish-private mechanisms we organise our analysis over symmetrical\npartitions corresponding to orbits of graph automorphism groups. A construction\nmeeting our bound with asymptotic equality demonstrates tightness.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 12:38:32 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 06:43:47 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Edwards", "Tobias", ""], ["Rubinstein", "Benjamin I. P.", ""], ["Zhang", "Zuhe", ""], ["Zhou", "Sanming", ""]]}, {"id": "2007.05994", "submitter": "William Wilkinson", "authors": "William J. Wilkinson, Paul E. Chang, Michael Riis Andersen, Arno Solin", "title": "State Space Expectation Propagation: Efficient Inference Schemes for\n  Temporal Gaussian Processes", "comments": "Accepted to International Conference on Machine Learning (ICML) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate approximate Bayesian inference in non-conjugate temporal and\nspatio-temporal Gaussian process models as a simple parameter update rule\napplied during Kalman smoothing. This viewpoint encompasses most inference\nschemes, including expectation propagation (EP), the classical (Extended,\nUnscented, etc.) Kalman smoothers, and variational inference. We provide a\nunifying perspective on these algorithms, showing how replacing the power EP\nmoment matching step with linearisation recovers the classical smoothers. EP\nprovides some benefits over the traditional methods via introduction of the\nso-called cavity distribution, and we combine these benefits with the\ncomputational efficiency of linearisation, providing extensive empirical\nanalysis demonstrating the efficacy of various algorithms under this unifying\nframework. We provide a fast implementation of all methods in JAX.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 13:59:25 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Wilkinson", "William J.", ""], ["Chang", "Paul E.", ""], ["Andersen", "Michael Riis", ""], ["Solin", "Arno", ""]]}, {"id": "2007.06007", "submitter": "Paulo Tabuada", "authors": "Paulo Tabuada and Bahman Gharesifard", "title": "Universal Approximation Power of Deep Residual Neural Networks via\n  Nonlinear Control Theory", "comments": "Main result was strengthened by reducing, from $2n$ to $n+1$, the\n  number of neurons that are sufficient for approximating arbitrarily well any\n  continuous function $f:E\\to\\mathbb{R}^n$ defined on a compact set $E\\subset\n  \\mathbb{R}^n$", "journal-ref": "ICLR 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explain the universal approximation capabilities of deep\nresidual neural networks through geometric nonlinear control. Inspired by\nrecent work establishing links between residual networks and control systems,\nwe provide a general sufficient condition for a residual network to have the\npower of universal approximation by asking the activation function, or one of\nits derivatives, to satisfy a quadratic differential equation. Many activation\nfunctions used in practice satisfy this assumption, exactly or approximately,\nand we show this property to be sufficient for an adequately deep neural\nnetwork with $n+1$ neurons per layer to approximate arbitrarily well, on a\ncompact set and with respect to the supremum norm, any continuous function from\n$\\mathbb{R}^n$ to $\\mathbb{R}^n$. We further show this result to hold for very\nsimple architectures for which the weights only need to assume two values. The\nfirst key technical contribution consists of relating the universal\napproximation problem to controllability of an ensemble of control systems\ncorresponding to a residual network and to leverage classical Lie algebraic\ntechniques to characterize controllability. The second technical contribution\nis to identify monotonicity as the bridge between controllability of finite\nensembles and uniform approximability on compact sets.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 14:53:30 GMT"}, {"version": "v2", "created": "Sat, 3 Oct 2020 15:32:56 GMT"}, {"version": "v3", "created": "Wed, 16 Dec 2020 19:44:38 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Tabuada", "Paulo", ""], ["Gharesifard", "Bahman", ""]]}, {"id": "2007.06011", "submitter": "Inga Str\\\"umke", "authors": "Daniel Vidali Fryer, Inga Str\\\"umke, Hien Nguyen", "title": "Explaining the data or explaining a model? Shapley values that uncover\n  non-linear dependencies", "comments": "26 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shapley values have become increasingly popular in the machine learning\nliterature thanks to their attractive axiomatisation, flexibility, and\nuniqueness in satisfying certain notions of `fairness'. The flexibility arises\nfrom the myriad potential forms of the Shapley value \\textit{game formulation}.\nAmongst the consequences of this flexibility is that there are now many types\nof Shapley values being discussed, with such variety being a source of\npotential misunderstanding. To the best of our knowledge, all existing game\nformulations in the machine learning and statistics literature fall into a\ncategory which we name the model-dependent category of game formulations. In\nthis work, we consider an alternative and novel formulation which leads to the\nfirst instance of what we call model-independent Shapley values. These Shapley\nvalues use a (non-parametric) measure of non-linear dependence as the\ncharacteristic function. The strength of these Shapley values is in their\nability to uncover and attribute non-linear dependencies amongst features. We\nintroduce and demonstrate the use of the energy distance correlations,\naffine-invariant distance correlation, and Hilbert-Shmidt independence\ncriterion as Shapley value characteristic functions. In particular, we\ndemonstrate their potential value for exploratory data analysis and model\ndiagnostics. We conclude with an interesting expository application to a\nclassical medical survey data set.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 15:04:59 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 08:31:29 GMT"}, {"version": "v3", "created": "Tue, 28 Jul 2020 09:11:04 GMT"}, {"version": "v4", "created": "Sat, 6 Mar 2021 05:46:11 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Fryer", "Daniel Vidali", ""], ["Str\u00fcmke", "Inga", ""], ["Nguyen", "Hien", ""]]}, {"id": "2007.06018", "submitter": "Lei Li", "authors": "Yuxuan Song, Ning Miao, Hao Zhou, Lantao Yu, Mingxuan Wang, Lei Li", "title": "Improving Maximum Likelihood Training for Text Generation with Density\n  Ratio Estimation", "comments": "Accepted to International Conference on Artificial Intelligence and\n  Statistics 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Auto-regressive sequence generative models trained by Maximum Likelihood\nEstimation suffer the exposure bias problem in practical finite sample\nscenarios. The crux is that the number of training samples for Maximum\nLikelihood Estimation is usually limited and the input data distributions are\ndifferent at training and inference stages. Many method shave been proposed to\nsolve the above problem (Yu et al., 2017; Lu et al., 2018), which relies on\nsampling from the non-stationary model distribution and suffers from high\nvariance or biased estimations. In this paper, we propose{\\psi}-MLE, a new\ntraining scheme for auto-regressive sequence generative models, which is\neffective and stable when operating at large sample space encountered in text\ngeneration. We derive our algorithm from a new perspective of self-augmentation\nand introduce bias correction with density ratio estimation. Extensive\nexperimental results on synthetic data and real-world text generation tasks\ndemonstrate that our method stably outperforms Maximum Likelihood Estimation\nand other state-of-the-art sequence generative models in terms of both quality\nand diversity.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 15:31:24 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Song", "Yuxuan", ""], ["Miao", "Ning", ""], ["Zhou", "Hao", ""], ["Yu", "Lantao", ""], ["Wang", "Mingxuan", ""], ["Li", "Lei", ""]]}, {"id": "2007.06024", "submitter": "Kailash Karthik Saravanakumar", "authors": "Kailash Karthik Saravanakumar", "title": "The Impossibility Theorem of Machine Fairness -- A Causal Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing pervasive use of machine learning in social and economic\nsettings, there has been an interest in the notion of machine bias in the AI\ncommunity. Models trained on historic data reflect biases that exist in society\nand propagated them to the future through their decisions. There are three\nprominent metrics of machine fairness used in the community, and it has been\nshown statistically that it is impossible to satisfy them all at the same time.\nThis has led to an ambiguity with regards to the definition of fairness. In\nthis report, a causal perspective to the impossibility theorem of fairness is\npresented along with a causal goal for machine fairness.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 15:56:15 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 22:45:07 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Saravanakumar", "Kailash Karthik", ""]]}, {"id": "2007.06029", "submitter": "Debmalya Mandal", "authors": "Debmalya Mandal, Samuel Deng, Suman Jana, Jeannette M. Wing, and\n  Daniel Hsu", "title": "Ensuring Fairness Beyond the Training Data", "comments": "18 pages, 3 figures, To appear at NeurIPS-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of fair classifiers that are robust to perturbations in\nthe training distribution. Despite recent progress, the literature on fairness\nhas largely ignored the design of fair and robust classifiers. In this work, we\ndevelop classifiers that are fair not only with respect to the training\ndistribution, but also for a class of distributions that are weighted\nperturbations of the training samples. We formulate a min-max objective\nfunction whose goal is to minimize a distributionally robust training loss, and\nat the same time, find a classifier that is fair with respect to a class of\ndistributions. We first reduce this problem to finding a fair classifier that\nis robust with respect to the class of distributions. Based on online learning\nalgorithm, we develop an iterative algorithm that provably converges to such a\nfair and robust solution. Experiments on standard machine learning fairness\ndatasets suggest that, compared to the state-of-the-art fair classifiers, our\nclassifier retains fairness guarantees and test accuracy for a large class of\nperturbations on the test set. Furthermore, our experiments show that there is\nan inherent trade-off between fairness robustness and accuracy of such\nclassifiers.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 16:20:28 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 15:52:43 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Mandal", "Debmalya", ""], ["Deng", "Samuel", ""], ["Jana", "Suman", ""], ["Wing", "Jeannette M.", ""], ["Hsu", "Daniel", ""]]}, {"id": "2007.06037", "submitter": "Ruixin Wang", "authors": "Ruixin Wang, Prateek Jaiwal and Harsha Honnappa", "title": "Estimating Stochastic Poisson Intensities Using Deep Latent Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present methodology for estimating the stochastic intensity of a doubly\nstochastic Poisson process. Statistical and theoretical analyses of traffic\ntraces show that these processes are appropriate models of high intensity\ntraffic arriving at an array of service systems. The statistical estimation of\nthe underlying latent stochastic intensity process driving the traffic model\ninvolves a rather complicated nonlinear filtering problem. We develop a novel\nsimulation methodology, using deep neural networks to approximate the path\nmeasures induced by the stochastic intensity process, for solving this\nnonlinear filtering problem. Our simulation studies demonstrate that the method\nis quite accurate on both in-sample estimation and on an out-of-sample\nperformance prediction task for an infinite server queue.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 16:57:53 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 16:44:41 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 17:53:31 GMT"}, {"version": "v4", "created": "Thu, 23 Jul 2020 02:07:59 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Wang", "Ruixin", ""], ["Jaiwal", "Prateek", ""], ["Honnappa", "Harsha", ""]]}, {"id": "2007.06049", "submitter": "Scott Fujimoto", "authors": "Scott Fujimoto, David Meger, Doina Precup", "title": "An Equivalence between Loss Functions and Non-Uniform Sampling in\n  Experience Replay", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prioritized Experience Replay (PER) is a deep reinforcement learning\ntechnique in which agents learn from transitions sampled with non-uniform\nprobability proportionate to their temporal-difference error. We show that any\nloss function evaluated with non-uniformly sampled data can be transformed into\nanother uniformly sampled loss function with the same expected gradient.\nSurprisingly, we find in some environments PER can be replaced entirely by this\nnew loss function without impact to empirical performance. Furthermore, this\nrelationship suggests a new branch of improvements to PER by correcting its\nuniformly sampled loss function equivalent. We demonstrate the effectiveness of\nour proposed modifications to PER and the equivalent loss function in several\nMuJoCo and Atari environments.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 17:45:24 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 16:36:44 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Fujimoto", "Scott", ""], ["Meger", "David", ""], ["Precup", "Doina", ""]]}, {"id": "2007.06059", "submitter": "Mark Hamilton", "authors": "Mark Hamilton, Evan Shelhamer, William T. Freeman", "title": "It Is Likely That Your Loss Should be a Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many common loss functions such as mean-squared-error, cross-entropy, and\nreconstruction loss are unnecessarily rigid. Under a probabilistic\ninterpretation, these common losses correspond to distributions with fixed\nshapes and scales. We instead argue for optimizing full likelihoods that\ninclude parameters like the normal variance and softmax temperature. Joint\noptimization of these \"likelihood parameters\" with model parameters can\nadaptively tune the scales and shapes of losses in addition to the strength of\nregularization. We explore and systematically evaluate how to parameterize and\napply likelihood parameters for robust modeling, outlier-detection, and\nre-calibration. Additionally, we propose adaptively tuning $L_2$ and $L_1$\nweights by fitting the scale parameters of normal and Laplace priors and\nintroduce more flexible element-wise regularizers.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 18:25:17 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 14:39:37 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Hamilton", "Mark", ""], ["Shelhamer", "Evan", ""], ["Freeman", "William T.", ""]]}, {"id": "2007.06062", "submitter": "Yuchao Ma", "authors": "Yuchao Ma, Andrew T. Campbell, Diane J. Cook, John Lach, Shwetak N.\n  Patel, Thomas Ploetz, Majid Sarrafzadeh, Donna Spruijt-Metz, Hassan\n  Ghasemzadeh", "title": "Transfer Learning for Activity Recognition in Mobile Health", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While activity recognition from inertial sensors holds potential for mobile\nhealth, differences in sensing platforms and user movement patterns cause\nperformance degradation. Aiming to address these challenges, we propose a\ntransfer learning framework, TransFall, for sensor-based activity recognition.\nTransFall's design contains a two-tier data transformation, a label estimation\nlayer, and a model generation layer to recognize activities for the new\nscenario. We validate TransFall analytically and empirically.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 18:32:46 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Ma", "Yuchao", ""], ["Campbell", "Andrew T.", ""], ["Cook", "Diane J.", ""], ["Lach", "John", ""], ["Patel", "Shwetak N.", ""], ["Ploetz", "Thomas", ""], ["Sarrafzadeh", "Majid", ""], ["Spruijt-Metz", "Donna", ""], ["Ghasemzadeh", "Hassan", ""]]}, {"id": "2007.06063", "submitter": "Baihong Jin", "authors": "Yingshui Tan, Baihong Jin, Xiangyu Yue, Yuxin Chen, Alberto\n  Sangiovanni Vincentelli", "title": "Exploiting Uncertainties from Ensemble Learners to Improve\n  Decision-Making in Healthcare AI", "comments": "Preprint of submission to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble learning is widely applied in Machine Learning (ML) to improve model\nperformance and to mitigate decision risks. In this approach, predictions from\na diverse set of learners are combined to obtain a joint decision. Recently,\nvarious methods have been explored in literature for estimating decision\nuncertainties using ensemble learning; however, determining which metrics are a\nbetter fit for certain decision-making applications remains a challenging task.\nIn this paper, we study the following key research question in the selection of\nuncertainty metrics: when does an uncertainty metric outperforms another? We\nanswer this question via a rigorous analysis of two commonly used uncertainty\nmetrics in ensemble learning, namely ensemble mean and ensemble variance. We\nshow that, under mild assumptions on the ensemble learners, ensemble mean is\npreferable with respect to ensemble variance as an uncertainty metric for\ndecision making. We empirically validate our assumptions and theoretical\nresults via an extensive case study: the diagnosis of referable diabetic\nretinopathy.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 18:33:09 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Tan", "Yingshui", ""], ["Jin", "Baihong", ""], ["Yue", "Xiangyu", ""], ["Chen", "Yuxin", ""], ["Vincentelli", "Alberto Sangiovanni", ""]]}, {"id": "2007.06068", "submitter": "Bilal Alsallakh", "authors": "Bilal Alsallakh and Zhixin Yan and Shabnam Ghaffarzadegan and Zeng Dai\n  and Liu Ren", "title": "Visualizing Classification Structure of Large-Scale Classifiers", "comments": "2020 ICML Workshop on Human Interpretability in Machine Learning (WHI\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a measure to compute class similarity in large-scale\nclassification based on prediction scores. Such measure has not been formally\npro-posed in the literature. We show how visualizing the class similarity\nmatrix can reveal hierarchical structures and relationships that govern the\nclasses. Through examples with various classifiers, we demonstrate how such\nstructures can help in analyzing the classification behavior and in inferring\npotential corner cases. The source code for one example is available as a\nnotebook at https://github.com/bilalsal/blocks\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 18:55:31 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 01:58:03 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Alsallakh", "Bilal", ""], ["Yan", "Zhixin", ""], ["Ghaffarzadegan", "Shabnam", ""], ["Dai", "Zeng", ""], ["Ren", "Liu", ""]]}, {"id": "2007.06072", "submitter": "Jules Depersin", "authors": "Jules Depersin", "title": "A spectral algorithm for robust regression with subgaussian rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a new linear up to quadratic time algorithm for linear regression in\nthe absence of strong assumptions on the underlying distributions of samples,\nand in the presence of outliers. The goal is to design a procedure which comes\nwith actual working code that attains the optimal sub-gaussian error bound even\nthough the data have only finite moments (up to $L_4$) and in the presence of\npossibly adversarial outliers. A polynomial-time solution to this problem has\nbeen recently discovered but has high runtime due to its use of Sum-of-Square\nhierarchy programming. At the core of our algorithm is an adaptation of the\nspectral method introduced for the mean estimation problem to the linear\nregression problem. As a by-product we established a connection between the\nlinear regression problem and the furthest hyperplane problem. From a\nstochastic point of view, in addition to the study of the classical quadratic\nand multiplier processes we introduce a third empirical process that comes\nnaturally in the study of the statistical properties of the algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 19:33:50 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Depersin", "Jules", ""]]}, {"id": "2007.06075", "submitter": "Ali Hasan", "authors": "Ali Hasan, Jo\\~ao M. Pereira, Sina Farsiu, Vahid Tarokh", "title": "Identifying Latent Stochastic Differential Equations with Variational\n  Auto-Encoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a method for learning latent stochastic differential equations\n(SDEs) from high dimensional time series data. Given a time series generated\nfrom a lower dimensional It\\^{o} process, the proposed method uncovers the\nrelevant parameters of the SDE through a self-supervised learning approach.\nUsing the framework of variational autoencoders (VAEs), we consider a\nconditional generative model for the data based on the Euler-Maruyama\napproximation of SDE solutions. Furthermore, we use recent results on\nidentifiability of semi-supervised learning to show that our model can recover\nnot only the underlying SDE parameters, but also the original latent space, up\nto an isometry, in the limit of infinite data. We validate the model through a\nseries of different simulated video processing tasks where the underlying SDE\nis known. Our results suggest that the proposed method effectively learns the\nunderlying SDE, as predicted by the theory.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 19:46:31 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 14:00:31 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 16:28:17 GMT"}, {"version": "v4", "created": "Thu, 6 May 2021 14:55:48 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Hasan", "Ali", ""], ["Pereira", "Jo\u00e3o M.", ""], ["Farsiu", "Sina", ""], ["Tarokh", "Vahid", ""]]}, {"id": "2007.06081", "submitter": "Yuejiao Sun", "authors": "Tianyi Chen, Xiao Jin, Yuejiao Sun, and Wotao Yin", "title": "VAFL: a Method of Vertical Asynchronous Federated Learning", "comments": "FL-ICML'20: Proc. of ICML Workshop on Federated Learning for User\n  Privacy and Data Confidentiality, July 2020", "journal-ref": "Proc. of ICML Workshop on Federated Learning for User Privacy and\n  Data Confidentiality, July 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Horizontal Federated learning (FL) handles multi-client data that share the\nsame set of features, and vertical FL trains a better predictor that combine\nall the features from different clients. This paper targets solving vertical FL\nin an asynchronous fashion, and develops a simple FL method. The new method\nallows each client to run stochastic gradient algorithms without coordination\nwith other clients, so it is suitable for intermittent connectivity of clients.\nThis method further uses a new technique of perturbed local embedding to ensure\ndata privacy and improve communication efficiency. Theoretically, we present\nthe convergence rate and privacy level of our method for strongly convex,\nnonconvex and even nonsmooth objectives separately. Empirically, we apply our\nmethod to FL on various image and healthcare datasets. The results compare\nfavorably to centralized and synchronous FL methods.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 20:09:25 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Chen", "Tianyi", ""], ["Jin", "Xiao", ""], ["Sun", "Yuejiao", ""], ["Yin", "Wotao", ""]]}, {"id": "2007.06082", "submitter": "John Martyn", "authors": "John Martyn, Guifre Vidal, Chase Roberts, Stefan Leichenauer", "title": "Entanglement and Tensor Networks for Supervised Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor networks, originally designed to address computational problems in\nquantum many-body physics, have recently been applied to machine learning\ntasks. However, compared to quantum physics, where the reasons for the success\nof tensor network approaches over the last 30 years is well understood, very\nlittle is yet known about why these techniques work for machine learning. The\ngoal of this paper is to investigate entanglement properties of tensor network\nmodels in a current machine learning application, in order to uncover general\nprinciples that may guide future developments. We revisit the use of tensor\nnetworks for supervised image classification using the MNIST data set of\nhandwritten digits, as pioneered by Stoudenmire and Schwab [Adv. in Neur.\nInform. Proc. Sys. 29, 4799 (2016)]. Firstly we hypothesize about which state\nthe tensor network might be learning during training. For that purpose, we\npropose a plausible candidate state $|\\Sigma_{\\ell}\\rangle$ (built as a\nsuperposition of product states corresponding to images in the training set)\nand investigate its entanglement properties. We conclude that\n$|\\Sigma_{\\ell}\\rangle$ is so robustly entangled that it cannot be approximated\nby the tensor network used in that work, which must therefore be representing a\nvery different state. Secondly, we use tensor networks with a block product\nstructure, in which entanglement is restricted within small blocks of $n \\times\nn$ pixels/qubits. We find that these states are extremely expressive (e.g.\ntraining accuracy of $99.97 \\%$ already for $n=2$), suggesting that long-range\nentanglement may not be essential for image classification. However, in our\ncurrent implementation, optimization leads to over-fitting, resulting in test\naccuracies that are not competitive with other current approaches.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 20:09:26 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Martyn", "John", ""], ["Vidal", "Guifre", ""], ["Roberts", "Chase", ""], ["Leichenauer", "Stefan", ""]]}, {"id": "2007.06093", "submitter": "Zi Wang", "authors": "Zi Wang, Aws Albarghouthi, Gautam Prakriya, Somesh Jha", "title": "Interval Universal Approximation for Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To verify safety and robustness of neural networks, researchers have\nsuccessfully applied abstract interpretation, primarily using the interval\nabstract domain. In this paper, we study the theoretical power and limits of\nthe interval domain for neural-network verification.\n  First, we introduce the interval universal approximation (IUA) theorem. IUA\nshows that neural networks not only can approximate any continuous function $f$\n(universal approximation) as we have known for decades, but we can find a\nneural network, using any well-behaved activation function, whose interval\nbounds are an arbitrarily close approximation of the set semantics of $f$ (the\nresult of applying $f$ to a set of inputs). We call this notion of\napproximation interval approximation. Our theorem generalizes the recent result\nof Baader et al. (2020) from ReLUs to a rich class of activation functions that\nwe call squashable functions. Additionally, the IUA theorem implies that we can\nalways construct provably robust neural networks under $\\ell_\\infty$-norm using\nalmost any practical activation function.\n  Second, we study the computational complexity of constructing neural networks\nthat are amenable to precise interval analysis. This is a crucial question, as\nour constructive proof of IUA is exponential in the size of the approximation\ndomain. We boil this question down to the problem of approximating the range of\na neural network with squashable activation functions. We show that the range\napproximation problem (RA) is a $\\Delta_2$-intermediate problem, which is\nstrictly harder than $\\mathsf{NP}$-complete problems, assuming\n$\\mathsf{coNP}\\not\\subset \\mathsf{NP}$. As a result, IUA is an inherently hard\nproblem: No matter what abstract domain or computational tools we consider to\nachieve interval approximation, there is no efficient construction of such a\nuniversal approximator.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 20:43:56 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 16:12:48 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 19:21:37 GMT"}, {"version": "v4", "created": "Mon, 8 Feb 2021 07:24:08 GMT"}, {"version": "v5", "created": "Wed, 14 Jul 2021 05:51:30 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Wang", "Zi", ""], ["Albarghouthi", "Aws", ""], ["Prakriya", "Gautam", ""], ["Jha", "Somesh", ""]]}, {"id": "2007.06096", "submitter": "Theo Guenais", "authors": "Th\\'eo Gu\\'enais, Dimitris Vamvourellis, Yaniv Yacoby, Finale\n  Doshi-Velez, Weiwei Pan", "title": "BaCOUn: Bayesian Classifers with Out-of-Distribution Uncertainty", "comments": "ICML 2020 Workshop on Uncertainty and Robustness in Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional training of deep classifiers yields overconfident models that are\nnot reliable under dataset shift. We propose a Bayesian framework to obtain\nreliable uncertainty estimates for deep classifiers. Our approach consists of a\nplug-in \"generator\" used to augment the data with an additional class of points\nthat lie on the boundary of the training data, followed by Bayesian inference\non top of features that are trained to distinguish these \"out-of-distribution\"\npoints.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 20:52:55 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Gu\u00e9nais", "Th\u00e9o", ""], ["Vamvourellis", "Dimitris", ""], ["Yacoby", "Yaniv", ""], ["Doshi-Velez", "Finale", ""], ["Pan", "Weiwei", ""]]}, {"id": "2007.06103", "submitter": "Martin Ferianc", "authors": "Martin Ferianc, Hongxiang Fan and Miguel Rodrigues", "title": "VINNAS: Variational Inference-based Neural Network Architecture Search", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, neural architecture search (NAS) has received intensive\nscientific and industrial interest due to its capability of finding a neural\narchitecture with high accuracy for various artificial intelligence tasks such\nas image classification or object detection. In particular, gradient-based NAS\napproaches have become one of the more popular approaches thanks to their\ncomputational efficiency during the search. However, these methods often\nexperience a mode collapse, where the quality of the found architectures is\npoor due to the algorithm resorting to choosing a single operation type for the\nentire network, or stagnating at a local minima for various datasets or search\nspaces.\n  To address these defects, we present a differentiable variational\ninference-based NAS method for searching sparse convolutional neural networks.\nOur approach finds the optimal neural architecture by dropping out candidate\noperations in an over-parameterised supergraph using variational dropout with\nautomatic relevance determination prior, which makes the algorithm gradually\nremove unnecessary operations and connections without risking mode collapse.\nThe evaluation is conducted through searching two types of convolutional cells\nthat shape the neural network for classifying different image datasets. Our\nmethod finds diverse network cells, while showing state-of-the-art accuracy\nwith up to almost 2 times fewer non-zero parameters.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 21:47:35 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 20:09:14 GMT"}, {"version": "v3", "created": "Tue, 20 Oct 2020 10:58:20 GMT"}, {"version": "v4", "created": "Sat, 24 Oct 2020 12:09:06 GMT"}, {"version": "v5", "created": "Thu, 14 Jan 2021 21:26:57 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Ferianc", "Martin", ""], ["Fan", "Hongxiang", ""], ["Rodrigues", "Miguel", ""]]}, {"id": "2007.06106", "submitter": "Mart\\'in Palazzo", "authors": "Martin Palazzo, Pierre Beauseroy, Patricio Yankilevich", "title": "Unsupervised Feature Selection for Tumor Profiles using Autoencoders and\n  Kernel Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.GN q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Molecular data from tumor profiles is high dimensional. Tumor profiles can be\ncharacterized by tens of thousands of gene expression features. Due to the size\nof the gene expression feature set machine learning methods are exposed to\nnoisy variables and complexity. Tumor types present heterogeneity and can be\nsubdivided in tumor subtypes. In many cases tumor data does not include tumor\nsubtype labeling thus unsupervised learning methods are necessary for tumor\nsubtype discovery. This work aims to learn meaningful and low dimensional\nrepresentations of tumor samples and find tumor subtype clusters while keeping\nbiological signatures without using tumor labels. The proposed method named\nLatent Kernel Feature Selection (LKFS) is an unsupervised approach for gene\nselection in tumor gene expression profiles. By using Autoencoders a low\ndimensional and denoised latent space is learned as a target representation to\nguide a Multiple Kernel Learning model that selects a subset of genes. By using\nthe selected genes a clustering method is used to group samples. In order to\nevaluate the performance of the proposed unsupervised feature selection method\nthe obtained features and clusters are analyzed by clinical significance. The\nproposed method has been applied on three tumor datasets which are Brain, Renal\nand Lung, each one composed by two tumor subtypes. When compared with benchmark\nunsupervised feature selection methods the results obtained by the proposed\nmethod reveal lower redundancy in the selected features and a better clustering\nperformance.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 21:59:05 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Palazzo", "Martin", ""], ["Beauseroy", "Pierre", ""], ["Yankilevich", "Patricio", ""]]}, {"id": "2007.06120", "submitter": "Khalil Elkhalil", "authors": "Khalil Elkhalil, Ali Hasan, Jie Ding, Sina Farsiu, Vahid Tarokh", "title": "Fisher Auto-Encoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been conjectured that the Fisher divergence is more robust to model\nuncertainty than the conventional Kullback-Leibler (KL) divergence. This\nmotivates the design of a new class of robust generative auto-encoders (AE)\nreferred to as Fisher auto-encoders. Our approach is to design Fisher AEs by\nminimizing the Fisher divergence between the intractable joint distribution of\nobserved data and latent variables, with that of the postulated/modeled joint\ndistribution. In contrast to KL-based variational AEs (VAEs), the Fisher AE can\nexactly quantify the distance between the true and the model-based posterior\ndistributions. Qualitative and quantitative results are provided on both MNIST\nand celebA datasets demonstrating the competitive performance of Fisher AEs in\nterms of robustness compared to other AEs such as VAEs and Wasserstein AEs.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 22:43:20 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 12:45:31 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Elkhalil", "Khalil", ""], ["Hasan", "Ali", ""], ["Ding", "Jie", ""], ["Farsiu", "Sina", ""], ["Tarokh", "Vahid", ""]]}, {"id": "2007.06123", "submitter": "Omkar Ranadive", "authors": "Omkar Ranadive, Grant Gasser, David Terpay, Prem Seetharaman", "title": "OtoWorld: Towards Learning to Separate by Learning to Move", "comments": "Published in Self Supervision in Audio and Speech Workshop, 37th\n  International Conference on Machine Learning, Vienna, Austria (ICML 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present OtoWorld, an interactive environment in which agents must learn to\nlisten in order to solve navigational tasks. The purpose of OtoWorld is to\nfacilitate reinforcement learning research in computer audition, where agents\nmust learn to listen to the world around them to navigate. OtoWorld is built on\nthree open source libraries: OpenAI Gym for environment and agent interaction,\nPyRoomAcoustics for ray-tracing and acoustics simulation, and nussl for\ntraining deep computer audition models. OtoWorld is the audio analogue of\nGridWorld, a simple navigation game. OtoWorld can be easily extended to more\ncomplex environments and games. To solve one episode of OtoWorld, an agent must\nmove towards each sounding source in the auditory scene and \"turn it off\". The\nagent receives no other input than the current sound of the room. The sources\nare placed randomly within the room and can vary in number. The agent receives\na reward for turning off a source. We present preliminary results on the\nability of agents to win at OtoWorld. OtoWorld is open-source and available.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 22:53:24 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Ranadive", "Omkar", ""], ["Gasser", "Grant", ""], ["Terpay", "David", ""], ["Seetharaman", "Prem", ""]]}, {"id": "2007.06126", "submitter": "Junwen Bai", "authors": "Junwen Bai, Shufeng Kong, Carla Gomes", "title": "Disentangled Variational Autoencoder based Multi-Label Classification\n  with Covariance-Aware Multivariate Probit Model", "comments": null, "journal-ref": null, "doi": "10.24963/ijcai.2020/595", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label classification is the challenging task of predicting the presence\nand absence of multiple targets, involving representation learning and label\ncorrelation modeling. We propose a novel framework for multi-label\nclassification, Multivariate Probit Variational AutoEncoder (MPVAE), that\neffectively learns latent embedding spaces as well as label correlations. MPVAE\nlearns and aligns two probabilistic embedding spaces for labels and features\nrespectively. The decoder of MPVAE takes in the samples from the embedding\nspaces and models the joint distribution of output targets under a Multivariate\nProbit model by learning a shared covariance matrix. We show that MPVAE\noutperforms the existing state-of-the-art methods on a variety of application\ndomains, using public real-world datasets. MPVAE is further shown to remain\nrobust under noisy settings. Lastly, we demonstrate the interpretability of the\nlearned covariance by a case study on a bird observation dataset.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 23:08:07 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Bai", "Junwen", ""], ["Kong", "Shufeng", ""], ["Gomes", "Carla", ""]]}, {"id": "2007.06133", "submitter": "Deng Pan", "authors": "Deng Pan, Xiangrui Li, Xin Li and Dongxiao Zhu", "title": "Explainable Recommendation via Interpretable Feature Mapping and\n  Evaluation of Explainability", "comments": "Proceedings of the Twenty-Ninth International Joint Conference on\n  Artificial Intelligence (IJCAI)", "journal-ref": "IJCAI 2020, pages 2690-2696", "doi": "10.24963/ijcai.2020/373", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent factor collaborative filtering (CF) has been a widely used technique\nfor recommender system by learning the semantic representations of users and\nitems. Recently, explainable recommendation has attracted much attention from\nresearch community. However, trade-off exists between explainability and\nperformance of the recommendation where metadata is often needed to alleviate\nthe dilemma. We present a novel feature mapping approach that maps the\nuninterpretable general features onto the interpretable aspect features,\nachieving both satisfactory accuracy and explainability in the recommendations\nby simultaneous minimization of rating prediction loss and interpretation loss.\nTo evaluate the explainability, we propose two new evaluation metrics\nspecifically designed for aspect-level explanation using surrogate ground\ntruth. Experimental results demonstrate a strong performance in both\nrecommendation and explaining explanation, eliminating the need for metadata.\nCode is available from https://github.com/pd90506/AMCF.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 23:49:12 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Pan", "Deng", ""], ["Li", "Xiangrui", ""], ["Li", "Xin", ""], ["Zhu", "Dongxiao", ""]]}, {"id": "2007.06134", "submitter": "Peng Jiang", "authors": "Peng Jiang, Gagan Agrawal", "title": "Adaptive Periodic Averaging: A Practical Approach to Reducing\n  Communication in Distributed Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stochastic Gradient Descent (SGD) is the key learning algorithm for many\nmachine learning tasks. Because of its computational costs, there is a growing\ninterest in accelerating SGD on HPC resources like GPU clusters. However, the\nperformance of parallel SGD is still bottlenecked by the high communication\ncosts even with a fast connection among the machines. A simple approach to\nalleviating this problem, used in many existing efforts, is to perform\ncommunication every few iterations, using a constant averaging period. In this\npaper, we show that the optimal averaging period in terms of convergence and\ncommunication cost is not a constant, but instead varies over the course of the\nexecution. Specifically, we observe that reducing the variance of model\nparameters among the computing nodes is critical to the convergence of periodic\nparameter averaging SGD. Given a fixed communication budget, we show that it is\nmore beneficial to synchronize more frequently in early iterations to reduce\nthe initial large variance and synchronize less frequently in the later phase\nof the training process. We propose a practical algorithm, named ADaptive\nPeriodic parameter averaging SGD (ADPSGD), to achieve a smaller overall\nvariance of model parameters, and thus better convergence compared with the\nConstant Periodic parameter averaging SGD (CPSGD). We evaluate our method with\nseveral image classification benchmarks and show that our ADPSGD indeed\nachieves smaller training losses and higher test accuracies with smaller\ncommunication compared with CPSGD. Compared with gradient-quantization SGD, we\nshow that our algorithm achieves faster convergence with only half of the\ncommunication. Compared with full-communication SGD, our ADPSGD achieves 1:14x\nto 1:27x speedups with a 100Gbps connection among computing nodes, and the\nspeedups increase to 1:46x ~ 1:95x with a 10Gbps connection.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 00:04:55 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 15:45:04 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Jiang", "Peng", ""], ["Agrawal", "Gagan", ""]]}, {"id": "2007.06140", "submitter": "Chris Cannella", "authors": "Chris Cannella, Mohammadreza Soltani, Vahid Tarokh", "title": "Projected Latent Markov Chain Monte Carlo: Conditional Sampling of\n  Normalizing Flows", "comments": "27 pages, 22 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Projected Latent Markov Chain Monte Carlo (PL-MCMC), a technique\nfor sampling from the high-dimensional conditional distributions learned by a\nnormalizing flow. We prove that a Metropolis-Hastings implementation of PL-MCMC\nasymptotically samples from the exact conditional distributions associated with\na normalizing flow. As a conditional sampling method, PL-MCMC enables Monte\nCarlo Expectation Maximization (MC-EM) training of normalizing flows from\nincomplete data. Through experimental tests applying normalizing flows to\nmissing data tasks for a variety of data sets, we demonstrate the efficacy of\nPL-MCMC for conditional sampling from normalizing flows.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 00:47:39 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 17:47:53 GMT"}, {"version": "v3", "created": "Wed, 30 Sep 2020 16:39:06 GMT"}, {"version": "v4", "created": "Fri, 26 Feb 2021 16:52:28 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Cannella", "Chris", ""], ["Soltani", "Mohammadreza", ""], ["Tarokh", "Vahid", ""]]}, {"id": "2007.06159", "submitter": "Mingyuan Zhou", "authors": "Yuguang Yue, Zhendong Wang, Mingyuan Zhou", "title": "Implicit Distributional Reinforcement Learning", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve the sample efficiency of policy-gradient based reinforcement\nlearning algorithms, we propose implicit distributional actor-critic (IDAC)\nthat consists of a distributional critic, built on two deep generator networks\n(DGNs), and a semi-implicit actor (SIA), powered by a flexible policy\ndistribution. We adopt a distributional perspective on the discounted\ncumulative return and model it with a state-action-dependent implicit\ndistribution, which is approximated by the DGNs that take state-action pairs\nand random noises as their input. Moreover, we use the SIA to provide a\nsemi-implicit policy distribution, which mixes the policy parameters with a\nreparameterizable distribution that is not constrained by an analytic density\nfunction. In this way, the policy's marginal distribution is implicit,\nproviding the potential to model complex properties such as covariance\nstructure and skewness, but its parameter and entropy can still be estimated.\nWe incorporate these features with an off-policy algorithm framework to solve\nproblems with continuous action space and compare IDAC with state-of-the-art\nalgorithms on representative OpenAI Gym environments. We observe that IDAC\noutperforms these baselines in most tasks. Python code is provided.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 02:52:18 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 20:23:32 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Yue", "Yuguang", ""], ["Wang", "Zhendong", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "2007.06168", "submitter": "Mikhail Yurochkin", "authors": "Sebastian Claici, Mikhail Yurochkin, Soumya Ghosh and Justin Solomon", "title": "Model Fusion with Kullback--Leibler Divergence", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to fuse posterior distributions learned from\nheterogeneous datasets. Our algorithm relies on a mean field assumption for\nboth the fused model and the individual dataset posteriors and proceeds using a\nsimple assign-and-average approach. The components of the dataset posteriors\nare assigned to the proposed global model components by solving a regularized\nvariant of the assignment problem. The global components are then updated based\non these assignments by their mean under a KL divergence. For exponential\nfamily variational distributions, our formulation leads to an efficient\nnon-parametric algorithm for computing the fused model. Our algorithm is easy\nto describe and implement, efficient, and competitive with state-of-the-art on\nmotion capture analysis, topic modeling, and federated learning of Bayesian\nneural networks.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 03:27:45 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Claici", "Sebastian", ""], ["Yurochkin", "Mikhail", ""], ["Ghosh", "Soumya", ""], ["Solomon", "Justin", ""]]}, {"id": "2007.06169", "submitter": "Tetsuya Kaji", "authors": "Tetsuya Kaji, Elena Manresa, Guillaume Pouliot", "title": "An Adversarial Approach to Structural Estimation", "comments": "58 pages, 3 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new simulation-based estimation method, adversarial estimation,\nfor structural models. The estimator is formulated as the solution to a minimax\nproblem between a generator (which generates synthetic observations using the\nstructural model) and a discriminator (which classifies if an observation is\nsynthetic). The discriminator maximizes the accuracy of its classification\nwhile the generator minimizes it. We show that, with a sufficiently rich\ndiscriminator, the adversarial estimator attains parametric efficiency under\ncorrect specification and the parametric rate under misspecification. We\nadvocate the use of a neural network as a discriminator that can exploit\nadaptivity properties and attain fast rates of convergence. We apply our method\nto the elderly's saving decision model and show that including gender and\nhealth profiles in the discriminator uncovers the bequest motive as an\nimportant source of saving across the wealth distribution, not only for the\nrich.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 03:31:02 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Kaji", "Tetsuya", ""], ["Manresa", "Elena", ""], ["Pouliot", "Guillaume", ""]]}, {"id": "2007.06178", "submitter": "Miaoyun Zhao", "authors": "Miaoyun Zhao, Yulai Cong, Shuyang Dai, Lawrence Carin", "title": "Bridging Maximum Likelihood and Adversarial Learning via\n  $\\alpha$-Divergence", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum likelihood (ML) and adversarial learning are two popular approaches\nfor training generative models, and from many perspectives these techniques are\ncomplementary. ML learning encourages the capture of all data modes, and it is\ntypically characterized by stable training. However, ML learning tends to\ndistribute probability mass diffusely over the data space, $e.g.$, yielding\nblurry synthetic images. Adversarial learning is well known to synthesize\nhighly realistic natural images, despite practical challenges like mode\ndropping and delicate training. We propose an $\\alpha$-Bridge to unify the\nadvantages of ML and adversarial learning, enabling the smooth transfer from\none to the other via the $\\alpha$-divergence. We reveal that generalizations of\nthe $\\alpha$-Bridge are closely related to approaches developed recently to\nregularize adversarial learning, providing insights into that prior work, and\nfurther understanding of why the $\\alpha$-Bridge performs well in practice.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 04:06:43 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Zhao", "Miaoyun", ""], ["Cong", "Yulai", ""], ["Dai", "Shuyang", ""], ["Carin", "Lawrence", ""]]}, {"id": "2007.06184", "submitter": "Roshan Shariff", "authors": "Roshan Shariff and Csaba Szepesv\\'ari", "title": "Efficient Planning in Large MDPs with Weak Linear Function Approximation", "comments": "12 pages and appendix (10 pages). Submitted to the 34th Conference on\n  Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale Markov decision processes (MDPs) require planning algorithms with\nruntime independent of the number of states of the MDP. We consider the\nplanning problem in MDPs using linear value function approximation with only\nweak requirements: low approximation error for the optimal value function, and\na small set of \"core\" states whose features span those of other states. In\nparticular, we make no assumptions about the representability of policies or\nvalue functions of non-optimal policies. Our algorithm produces almost-optimal\nactions for any state using a generative oracle (simulator) for the MDP, while\nits computation time scales polynomially with the number of features, core\nstates, and actions and the effective horizon.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 04:40:41 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Shariff", "Roshan", ""], ["Szepesv\u00e1ri", "Csaba", ""]]}, {"id": "2007.06192", "submitter": "Blaine Rister", "authors": "Blaine Rister and Daniel L. Rubin", "title": "Probabilistic bounds on neuron death in deep rectifier networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuron death is a complex phenomenon with implications for model\ntrainability: the deeper the network, the lower the probability of finding a\nvalid initialization. In this work, we derive both upper and lower bounds on\nthe probability that a ReLU network is initialized to a trainable point, as a\nfunction of model hyperparameters. We show that it is possible to increase the\ndepth of a network indefinitely, so long as the width increases as well.\nFurthermore, our bounds are asymptotically tight under reasonable assumptions:\nfirst, the upper bound coincides with the true probability for a single-layer\nnetwork with the largest possible input set. Second, the true probability\nconverges to our lower bound as the input set shrinks to a single point, or as\nthe network complexity grows under an assumption about the output variance. We\nconfirm these results by numerical simulation, showing rapid convergence to the\nlower bound with increasing network depth. Then, motivated by the theory, we\npropose a practical sign flipping scheme which guarantees that the ratio of\nliving data points in a $k$-layer network is at least $2^{-k}$. Finally, we\nshow how these issues are mitigated by network design features currently seen\nin practice, such as batch normalization, residual connections, dense networks\nand skip connections. This suggests that neuron death may provide insight into\nthe efficacy of various model architectures.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 05:15:04 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 20:54:09 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Rister", "Blaine", ""], ["Rubin", "Daniel L.", ""]]}, {"id": "2007.06207", "submitter": "Siwei Chen", "authors": "Siwei Chen, Xiao Ma, David Hsu", "title": "DinerDash Gym: A Benchmark for Policy Learning in High-Dimensional\n  Action Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been arduous to assess the progress of a policy learning algorithm in\nthe domain of hierarchical task with high dimensional action space due to the\nlack of a commonly accepted benchmark. In this work, we propose a new\nlight-weight benchmark task called Diner Dash for evaluating the performance in\na complicated task with high dimensional action space. In contrast to the\ntraditional Atari games that only have a flat structure of goals and very few\nactions, the proposed benchmark task has a hierarchical task structure and size\nof 57 for the action space and hence can facilitate the development of policy\nlearning in complicated tasks. On top of that, we introduce Decomposed Policy\nGraph Modelling (DPGM), an algorithm that combines both graph modelling and\ndeep learning to allow explicit domain knowledge embedding and achieves\nsignificant improvement comparing to the baseline. In the experiments, we have\nshown the effectiveness of the domain knowledge injection via a specially\ndesigned imitation algorithm as well as results of other popular algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 06:22:55 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Chen", "Siwei", ""], ["Ma", "Xiao", ""], ["Hsu", "David", ""]]}, {"id": "2007.06225", "submitter": "Ahmed Elnaggar", "authors": "Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rihawi,\n  Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin\n  Steinegger, Debsindhu Bhowmik, Burkhard Rost", "title": "ProtTrans: Towards Cracking the Language of Life's Code Through\n  Self-Supervised Deep Learning and High Performance Computing", "comments": "17 pages, 9 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational biology and bioinformatics provide vast data gold-mines from\nprotein sequences, ideal for Language Models taken from NLP. These LMs reach\nfor new prediction frontiers at low inference costs. Here, we trained two\nauto-regressive models (Transformer-XL, XLNet) and four auto-encoder models\n(BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393\nbillion amino acids. The LMs were trained on the Summit supercomputer using\n5616 GPUs and TPU Pod up-to 1024 cores. Dimensionality reduction revealed that\nthe raw protein LM-embeddings from unlabeled data captured some biophysical\nfeatures of protein sequences. We validated the advantage of using the\nembeddings as exclusive input for several subsequent tasks. The first was a\nper-residue prediction of protein secondary structure (3-state accuracy\nQ3=81%-87%); the second were per-protein predictions of protein sub-cellular\nlocalization (ten-state accuracy: Q10=81%) and membrane vs. water-soluble\n(2-state accuracy Q2=91%). For the per-residue predictions the transfer of the\nmost informative embeddings (ProtT5) for the first time outperformed the\nstate-of-the-art without using evolutionary information thereby bypassing\nexpensive database searches. Taken together, the results implied that protein\nLMs learned some of the grammar of the language of life. To facilitate future\nwork, we released our models at https://github.com/agemagician/ProtTrans.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 07:54:20 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 21:18:05 GMT"}, {"version": "v3", "created": "Tue, 4 May 2021 20:18:22 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Elnaggar", "Ahmed", ""], ["Heinzinger", "Michael", ""], ["Dallago", "Christian", ""], ["Rihawi", "Ghalia", ""], ["Wang", "Yu", ""], ["Jones", "Llion", ""], ["Gibbs", "Tom", ""], ["Feher", "Tamas", ""], ["Angerer", "Christoph", ""], ["Steinegger", "Martin", ""], ["Bhowmik", "Debsindhu", ""], ["Rost", "Burkhard", ""]]}, {"id": "2007.06226", "submitter": "Mauro Sanchirico", "authors": "Mauro J. Sanchirico III, Xun Jiao and C. Nataraj", "title": "AMITE: A Novel Polynomial Expansion for Analyzing Neural Network\n  Nonlinearities", "comments": "12 pages, 2 tables, 8 figures, LaTeX; full revision posted with new\n  results and additional proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polynomial expansions are an important technique in the analysis and study of\nneural network nonlinearities. Recently, expansions have been applied to neural\nnetworks addressing well known difficulties in the verifiable, explainable and\nsecure deployment thereof. Existing approaches span classical Taylor and\nChebyshev methods, asymptotics, and many numerical and algorithmic approaches.\nWe find that while existing approaches individually have useful properties such\nas exact error formulas, monic form, adjustable domain, and robustness to\nundefined derivatives, there are no approaches that provide a consistent method\nyielding an expansion with all these properties. To address this gap, we\ndevelop an analytically modified integral transform expansion referred to as\nAMITE, which is a novel expansion via integral transforms modified using\nderived criteria for convergence. We apply AMITE to the nonlinear activation\nfunctions of neural networks including hyperbolic tangent and rectified linear\nunits. Compared with existing state-of-the-art expansion techniques such as\nChebyshev, Taylor series, and numerical approximations, AMITE is the first\npolynomial expansion that can provide six previously mutually exclusive desired\nexpansion properties such as exact formulas for the coefficients and exact\nexpansion errors (Table II). Using an MLP as a case study, we demonstrate the\neffectiveness of AMITE in the equivalence testing problem of MLP where a\nblack-box network under test is stimulated, and a replicated multivariate\npolynomial form is efficiently extracted from a noisy response to enable\ncomparison against an original network. AMITE presents a new dimension of\nexpansion methods that are suitable for analysis/approximation of\nnonlinearities in neural networks, which opens up new directions and\nopportunities for the theoretical analysis and systematic testing of neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 07:58:47 GMT"}, {"version": "v2", "created": "Sat, 25 Jul 2020 16:45:14 GMT"}, {"version": "v3", "created": "Fri, 12 Feb 2021 20:37:16 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Sanchirico", "Mauro J.", "III"], ["Jiao", "Xun", ""], ["Nataraj", "C.", ""]]}, {"id": "2007.06229", "submitter": "Byung-Hak Kim", "authors": "Byung-Hak Kim, Seshadri Sridharan, Andy Atwal and Varun Ganapathi", "title": "Deep Claim: Payer Response Prediction from Claims Data with Deep\n  Learning", "comments": "To be presented at the Healthcare Systems, Population Health, and the\n  Role of Health-Tech (HSYS) Workshop at the 37th International Conference on\n  Machine Learning, Vienna, Austria, July 13-18, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Each year, almost 10% of claims are denied by payers (i.e., health insurance\nplans). With the cost to recover these denials and underpayments, predicting\npayer response (likelihood of payment) from claims data with a high degree of\naccuracy and precision is anticipated to improve healthcare staffs' performance\nproductivity and drive better patient financial experience and satisfaction in\nthe revenue cycle (Barkholz, 2017). However, constructing advanced predictive\nanalytics models has been considered challenging in the last twenty years. That\nsaid, we propose a (low-level) context-dependent compact representation of\npatients' historical claim records by effectively learning complicated\ndependencies in the (high-level) claim inputs. Built on this new latent\nrepresentation, we demonstrate that a deep learning-based framework, Deep\nClaim, can accurately predict various responses from multiple payers using\n2,905,026 de-identified claims data from two US health systems. Deep Claim's\nimprovements over carefully chosen baselines in predicting claim denials are\nmost pronounced as 22.21% relative recall gain (at 95% precision) on Health\nSystem A, which implies Deep Claim can find 22.21% more denials than the best\nbaseline system.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 08:05:17 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Kim", "Byung-Hak", ""], ["Sridharan", "Seshadri", ""], ["Atwal", "Andy", ""], ["Ganapathi", "Varun", ""]]}, {"id": "2007.06230", "submitter": "Aman Agarwal", "authors": "Aman Agarwal, Aditya Mishra, Priyanka Sharma, Swati Jain, Sutapa\n  Ranjan, Ranjana Manchanda", "title": "Using LSTM for the Prediction of Disruption in ADITYA Tokamak", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Major disruptions in tokamak pose a serious threat to the vessel and its\nsurrounding pieces of equipment. The ability of the systems to detect any\nbehavior that can lead to disruption can help in alerting the system beforehand\nand prevent its harmful effects. Many machine learning techniques have already\nbeen in use at large tokamaks like JET and ASDEX, but are not suitable for\nADITYA, which is comparatively small. Through this work, we discuss a new\nreal-time approach to predict the time of disruption in ADITYA tokamak and\nvalidate the results on an experimental dataset. The system uses selected\ndiagnostics from the tokamak and after some pre-processing steps, sends them to\na time-sequence Long Short-Term Memory (LSTM) network. The model can make the\npredictions 12 ms in advance at less computation cost that is quick enough to\nbe deployed in real-time applications.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 08:06:43 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Agarwal", "Aman", ""], ["Mishra", "Aditya", ""], ["Sharma", "Priyanka", ""], ["Jain", "Swati", ""], ["Ranjan", "Sutapa", ""], ["Manchanda", "Ranjana", ""]]}, {"id": "2007.06236", "submitter": "Bal\\'azs Pej\\'o", "authors": "Bal\\'azs Pej\\'o and Gergely Bicz\\'ok", "title": "Quality Inference in Federated Learning with Secure Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Collaborative machine learning algorithms are developed both for efficiency\nreasons, and to ensure the privacy protection of sensitive data used for\nprocessing. Federated learning is the most popular of these methods, where 1)\nlearning is done locally, and 2) only a subset of the participants contribute\nin each training round. Despite individual data is not shared explicitly,\nrecent studies showed that federated learning models could still leak\ninformation. In this paper we focus on the quality of individual training\ndatasets, and show that such information could be inferred and connected to\nspecific participants even when secure aggregation is applied. Specifically, we\nuse three simple scoring rules for evaluating per round aggregated updates in\nthe federated learning process, and mount a novel differential quality\ninference attack (i.e., relative quality ordering reconstruction). Through a\nseries of image recognition experiments we show that the attack is able to\ninfer the relative quality ordering of participants. Whilst an attack in the\ntraditional sense, quality inference could also improve the federated learning\nprocess: we demonstrate how it can be used to (i) boost training efficiency and\n(ii) detect misbehavior. Finally, as a system designer might want to alleviate\nquality inference in certain use-cases, we discuss mitigation approaches.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 08:36:04 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 08:18:28 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Pej\u00f3", "Bal\u00e1zs", ""], ["Bicz\u00f3k", "Gergely", ""]]}, {"id": "2007.06240", "submitter": "Yucan Zhou", "authors": "Yucan Zhou, Yu Wang, Jianfei Cai, Yu Zhou, Qinghua Hu, Weiping Wang", "title": "Expert Training: Task Hardness Aware Meta-Learning for Few-Shot\n  Classification", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are highly effective when a large number of labeled\nsamples are available but fail with few-shot classification tasks. Recently,\nmeta-learning methods have received much attention, which train a meta-learner\non massive additional tasks to gain the knowledge to instruct the few-shot\nclassification. Usually, the training tasks are randomly sampled and performed\nindiscriminately, often making the meta-learner stuck into a bad local optimum.\nSome works in the optimization of deep neural networks have shown that a better\narrangement of training data can make the classifier converge faster and\nperform better. Inspired by this idea, we propose an easy-to-hard expert\nmeta-training strategy to arrange the training tasks properly, where easy tasks\nare preferred in the first phase, then, hard tasks are emphasized in the second\nphase. A task hardness aware module is designed and integrated into the\ntraining procedure to estimate the hardness of a task based on the\ndistinguishability of its categories. In addition, we explore multiple hardness\nmeasurements including the semantic relation, the pairwise Euclidean distance,\nthe Hausdorff distance, and the Hilbert-Schmidt independence criterion.\nExperimental results on the miniImageNet and tieredImageNetSketch datasets show\nthat the meta-learners can obtain better results with our expert training\nstrategy.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 08:49:00 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Zhou", "Yucan", ""], ["Wang", "Yu", ""], ["Cai", "Jianfei", ""], ["Zhou", "Yu", ""], ["Hu", "Qinghua", ""], ["Wang", "Weiping", ""]]}, {"id": "2007.06245", "submitter": "Martin Engelcke", "authors": "Martin Engelcke, Oiwi Parker Jones, Ingmar Posner", "title": "Reconstruction Bottlenecks in Object-Centric Generative Models", "comments": "10 pages, 7 Figures, Workshop on Object-Oriented Learning at ICML\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A range of methods with suitable inductive biases exist to learn\ninterpretable object-centric representations of images without supervision.\nHowever, these are largely restricted to visually simple images; robust object\ndiscovery in real-world sensory datasets remains elusive. To increase the\nunderstanding of such inductive biases, we empirically investigate the role of\n\"reconstruction bottlenecks\" for scene decomposition in GENESIS, a recent\nVAE-based model. We show such bottlenecks determine reconstruction and\nsegmentation quality and critically influence model behaviour.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 08:52:48 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 13:52:23 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Engelcke", "Martin", ""], ["Jones", "Oiwi Parker", ""], ["Posner", "Ingmar", ""]]}, {"id": "2007.06252", "submitter": "Pedro Hermosilla Casajus", "authors": "Pedro Hermosilla, Marco Sch\\\"afer, Mat\\v{e}j Lang, Gloria Fackelmann,\n  Pere Pau V\\'azquez, Barbora Kozl\\'ikov\\'a, Michael Krone, Tobias Ritschel,\n  Timo Ropinski", "title": "Intrinsic-Extrinsic Convolution and Pooling for Learning on 3D Protein\n  Structures", "comments": "International Conference on Learning Representations (ICLR) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.BM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proteins perform a large variety of functions in living organisms, thus\nplaying a key role in biology. As of now, available learning algorithms to\nprocess protein data do not consider several particularities of such data\nand/or do not scale well for large protein conformations. To fill this gap, we\npropose two new learning operations enabling deep 3D analysis of large-scale\nprotein data. First, we introduce a novel convolution operator which considers\nboth, the intrinsic (invariant under protein folding) as well as extrinsic\n(invariant under bonding) structure, by using $n$-D convolutions defined on\nboth the Euclidean distance, as well as multiple geodesic distances between\natoms in a multi-graph. Second, we enable a multi-scale protein analysis by\nintroducing hierarchical pooling operators, exploiting the fact that proteins\nare a recombination of a finite set of amino acids, which can be pooled using\nshared pooling matrices. Lastly, we evaluate the accuracy of our algorithms on\nseveral large-scale data sets for common protein analysis tasks, where we\noutperform state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 09:02:40 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 17:27:56 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Hermosilla", "Pedro", ""], ["Sch\u00e4fer", "Marco", ""], ["Lang", "Mat\u011bj", ""], ["Fackelmann", "Gloria", ""], ["V\u00e1zquez", "Pere Pau", ""], ["Kozl\u00edkov\u00e1", "Barbora", ""], ["Krone", "Michael", ""], ["Ritschel", "Tobias", ""], ["Ropinski", "Timo", ""]]}, {"id": "2007.06281", "submitter": "Simone Scardapane", "authors": "Simone Scardapane, Indro Spinelli, Paolo Di Lorenzo", "title": "Distributed Training of Graph Convolutional Networks", "comments": "Published on IEEE Transactions on Signal and Information Processing\n  over Networks", "journal-ref": null, "doi": "10.1109/TSIPN.2020.3046237", "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this work is to develop a fully-distributed algorithmic framework\nfor training graph convolutional networks (GCNs). The proposed method is able\nto exploit the meaningful relational structure of the input data, which are\ncollected by a set of agents that communicate over a sparse network topology.\nAfter formulating the centralized GCN training problem, we first show how to\nmake inference in a distributed scenario where the underlying data graph is\nsplit among different agents. Then, we propose a distributed gradient descent\nprocedure to solve the GCN training problem. The resulting model distributes\ncomputation along three lines: during inference, during back-propagation, and\nduring optimization. Convergence to stationary solutions of the GCN training\nproblem is also established under mild conditions. Finally, we propose an\noptimization criterion to design the communication topology between agents in\norder to match with the graph describing data relationships. A wide set of\nnumerical results validate our proposal. To the best of our knowledge, this is\nthe first work combining graph convolutional neural networks with distributed\noptimization.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 10:04:20 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 10:00:24 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Scardapane", "Simone", ""], ["Spinelli", "Indro", ""], ["Di Lorenzo", "Paolo", ""]]}, {"id": "2007.06299", "submitter": "Janis Klaise", "authors": "Janis Klaise, Arnaud Van Looveren, Clive Cox, Giovanni Vacanti,\n  Alexandru Coca", "title": "Monitoring and explainability of models in production", "comments": "Workshop on Challenges in Deploying and Monitoring Machine Learning\n  Systems (ICML 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The machine learning lifecycle extends beyond the deployment stage.\nMonitoring deployed models is crucial for continued provision of high quality\nmachine learning enabled services. Key areas include model performance and data\nmonitoring, detecting outliers and data drift using statistical techniques, and\nproviding explanations of historic predictions. We discuss the challenges to\nsuccessful implementation of solutions in each of these areas with some recent\nexamples of production ready solutions using open source tools.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 10:37:05 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Klaise", "Janis", ""], ["Van Looveren", "Arnaud", ""], ["Cox", "Clive", ""], ["Vacanti", "Giovanni", ""], ["Coca", "Alexandru", ""]]}, {"id": "2007.06324", "submitter": "Amirmasoud Ghiassi", "authors": "Amirmasoud Ghiassi, Taraneh Younesian, Robert Birke, Lydia Y.Chen", "title": "TrustNet: Learning from Trusted Data Against (A)symmetric Label Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robustness to label noise is a critical property for weakly-supervised\nclassifiers trained on massive datasets. Robustness to label noise is a\ncritical property for weakly-supervised classifiers trained on massive\ndatasets. In this paper, we first derive analytical bound for any given noise\npatterns. Based on the insights, we design TrustNet that first adversely learns\nthe pattern of noise corruption, being it both symmetric or asymmetric, from a\nsmall set of trusted data. Then, TrustNet is trained via a robust loss\nfunction, which weights the given labels against the inferred labels from the\nlearned noise pattern. The weight is adjusted based on model uncertainty across\ntraining epochs. We evaluate TrustNet on synthetic label noise for CIFAR-10 and\nCIFAR-100, and real-world data with label noise, i.e., Clothing1M. We compare\nagainst state-of-the-art methods demonstrating the strong robustness of\nTrustNet under a diverse set of noise patterns.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 11:41:52 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Ghiassi", "Amirmasoud", ""], ["Younesian", "Taraneh", ""], ["Birke", "Robert", ""], ["Chen", "Lydia Y.", ""]]}, {"id": "2007.06346", "submitter": "Aleksandr Ermolov", "authors": "Aleksandr Ermolov, Aliaksandr Siarohin, Enver Sangineto, Nicu Sebe", "title": "Whitening for Self-Supervised Representation Learning", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the current self-supervised representation learning (SSL) methods are\nbased on the contrastive loss and the instance-discrimination task, where\naugmented versions of the same image instance (\"positives\") are contrasted with\ninstances extracted from other images (\"negatives\"). For the learning to be\neffective, many negatives should be compared with a positive pair, which is\ncomputationally demanding. In this paper, we propose a different direction and\na new loss function for SSL, which is based on the whitening of the\nlatent-space features. The whitening operation has a \"scattering\" effect on the\nbatch samples, avoiding degenerate solutions where all the sample\nrepresentations collapse to a single point. Our solution does not require\nasymmetric networks and it is conceptually simple. Moreover, since negatives\nare not needed, we can extract multiple positive pairs from the same image\ninstance. The source code of the method and of all the experiments is available\nat: https://github.com/htdt/self-supervised.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 12:33:25 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 16:24:04 GMT"}, {"version": "v3", "created": "Tue, 1 Dec 2020 14:01:00 GMT"}, {"version": "v4", "created": "Mon, 8 Feb 2021 11:05:58 GMT"}, {"version": "v5", "created": "Fri, 14 May 2021 15:10:06 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Ermolov", "Aleksandr", ""], ["Siarohin", "Aliaksandr", ""], ["Sangineto", "Enver", ""], ["Sebe", "Nicu", ""]]}, {"id": "2007.06352", "submitter": "Valentin De Bortoli", "authors": "Valentin De Bortoli, Alain Durmus, Xavier Fontaine, Umut Simsekli", "title": "Quantitative Propagation of Chaos for SGD in Wide Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the limiting behavior of a continuous-time\ncounterpart of the Stochastic Gradient Descent (SGD) algorithm applied to\ntwo-layer overparameterized neural networks, as the number or neurons (ie, the\nsize of the hidden layer) $N \\to +\\infty$. Following a probabilistic approach,\nwe show 'propagation of chaos' for the particle system defined by this\ncontinuous-time dynamics under different scenarios, indicating that the\nstatistical interaction between the particles asymptotically vanishes. In\nparticular, we establish quantitative convergence with respect to $N$ of any\nparticle to a solution of a mean-field McKean-Vlasov equation in the metric\nspace endowed with the Wasserstein distance. In comparison to previous works on\nthe subject, we consider settings in which the sequence of stepsizes in SGD can\npotentially depend on the number of neurons and the iterations. We then\nidentify two regimes under which different mean-field limits are obtained, one\nof them corresponding to an implicitly regularized version of the minimization\nproblem at hand. We perform various experiments on real datasets to validate\nour theoretical results, assessing the existence of these two regimes on\nclassification problems and illustrating our convergence results.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 12:55:21 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 06:19:18 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["De Bortoli", "Valentin", ""], ["Durmus", "Alain", ""], ["Fontaine", "Xavier", ""], ["Simsekli", "Umut", ""]]}, {"id": "2007.06363", "submitter": "Dario Azzimonti", "authors": "Dario Azzimonti, Manuel Sch\\\"urch, Alessio Benavoli, Marco Zaffalon", "title": "Orthogonally Decoupled Variational Fourier Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse inducing points have long been a standard method to fit Gaussian\nprocesses to big data. In the last few years, spectral methods that exploit\napproximations of the covariance kernel have shown to be competitive. In this\nwork we exploit a recently introduced orthogonally decoupled variational basis\nto combine spectral methods and sparse inducing points methods. We show that\nthe method is competitive with the state-of-the-art on synthetic and on\nreal-world data.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 13:18:00 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Azzimonti", "Dario", ""], ["Sch\u00fcrch", "Manuel", ""], ["Benavoli", "Alessio", ""], ["Zaffalon", "Marco", ""]]}, {"id": "2007.06368", "submitter": "Djallel Bouneffouf", "authors": "Djallel Bouneffouf, Sohini Upadhyay and Yasaman Khazaeni", "title": "Contextual Bandit with Missing Rewards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a novel variant of the contextual bandit problem (i.e., the\nmulti-armed bandit with side-information, or context, available to a\ndecision-maker) where the reward associated with each context-based decision\nmay not always be observed(\"missing rewards\"). This new problem is motivated by\ncertain online settings including clinical trial and ad recommendation\napplications. In order to address the missing rewards setting, we propose to\ncombine the standard contextual bandit approach with an unsupervised learning\nmechanism such as clustering. Unlike standard contextual bandit methods, by\nleveraging clustering to estimate missing reward, we are able to learn from\neach incoming event, even those with missing rewards. Promising empirical\nresults are obtained on several real-life datasets.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 13:29:51 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 00:16:49 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Bouneffouf", "Djallel", ""], ["Upadhyay", "Sohini", ""], ["Khazaeni", "Yasaman", ""]]}, {"id": "2007.06379", "submitter": "Ilker Birbil", "authors": "S. Ilker Birbil, Mert Edali, Birol Yuceoglu", "title": "Rule Covering for Interpretation and Boosting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose two algorithms for interpretation and boosting of tree-based\nensemble methods. Both algorithms make use of mathematical programming models\nthat are constructed with a set of rules extracted from an ensemble of decision\ntrees. The objective is to obtain the minimum total impurity with the least\nnumber of rules that cover all the samples. The first algorithm uses the\ncollection of decision trees obtained from a trained random forest model. Our\nnumerical results show that the proposed rule covering approach selects only a\nfew rules that could be used for interpreting the random forest model.\nMoreover, the resulting set of rules closely matches the accuracy level of the\nrandom forest model. Inspired by the column generation algorithm in linear\nprogramming, our second algorithm uses a rule generation scheme for boosting\ndecision trees. We use the dual optimal solutions of the linear programming\nmodels as sample weights to obtain only those rules that would improve the\naccuracy. With a computational study, we observe that our second algorithm\nperforms competitively with the other well-known boosting methods. Our\nimplementations also demonstrate that both algorithms can be trivially coupled\nwith the existing random forest and decision tree packages.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 13:42:57 GMT"}, {"version": "v2", "created": "Sat, 19 Sep 2020 13:56:58 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Birbil", "S. Ilker", ""], ["Edali", "Mert", ""], ["Yuceoglu", "Birol", ""]]}, {"id": "2007.06381", "submitter": "Laura Rieger", "authors": "Laura Rieger, Lars Kai Hansen", "title": "A simple defense against adversarial attacks on heatmap explanations", "comments": "Accepted at 2020 Workshop on Human Interpretability in Machine\n  Learning (WHI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With machine learning models being used for more sensitive applications, we\nrely on interpretability methods to prove that no discriminating attributes\nwere used for classification. A potential concern is the so-called\n\"fair-washing\" - manipulating a model such that the features used in reality\nare hidden and more innocuous features are shown to be important instead.\n  In our work we present an effective defence against such adversarial attacks\non neural networks. By a simple aggregation of multiple explanation methods,\nthe network becomes robust against manipulation. This holds even when the\nattacker has exact knowledge of the model weights and the explanation methods\nused.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 13:44:13 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Rieger", "Laura", ""], ["Hansen", "Lars Kai", ""]]}, {"id": "2007.06402", "submitter": "Raphael Achddou", "authors": "Rapha\\\"el Achddou, J.Matias di Martino, Guillermo Sapiro", "title": "Nested Learning For Multi-Granular Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard deep neural networks (DNNs) are commonly trained in an end-to-end\nfashion for specific tasks such as object recognition, face identification, or\ncharacter recognition, among many examples. This specificity often leads to\noverconfident models that generalize poorly to samples that are not from the\noriginal training distribution. Moreover, such standard DNNs do not allow to\nleverage information from heterogeneously annotated training data, where for\nexample, labels may be provided with different levels of granularity.\nFurthermore, DNNs do not produce results with simultaneous different levels of\nconfidence for different levels of detail, they are most commonly an all or\nnothing approach. To address these challenges, we introduce the concept of\nnested learning: how to obtain a hierarchical representation of the input such\nthat a coarse label can be extracted first, and sequentially refine this\nrepresentation, if the sample permits, to obtain successively refined\npredictions, all of them with the corresponding confidence. We explicitly\nenforce this behavior by creating a sequence of nested information bottlenecks.\nLooking at the problem of nested learning from an information theory\nperspective, we design a network topology with two important properties. First,\na sequence of low dimensional (nested) feature embeddings are enforced. Then we\nshow how the explicit combination of nested outputs can improve both the\nrobustness and the accuracy of finer predictions. Experimental results on\nCifar-10, Cifar-100, MNIST, Fashion-MNIST, Dbpedia, and Plantvillage\ndemonstrate that nested learning outperforms the same network trained in the\nstandard end-to-end fashion.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 14:27:14 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Achddou", "Rapha\u00ebl", ""], ["di Martino", "J. Matias", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "2007.06408", "submitter": "Nan Wu", "authors": "Hau-Tieng Wu and Nan Wu", "title": "Strong Uniform Consistency with Rates for Kernel Density Estimators with\n  General Kernels on Manifolds", "comments": "50 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When analyzing modern machine learning algorithms, we may need to handle\nkernel density estimation (KDE) with intricate kernels that are not designed by\nthe user and might even be irregular and asymmetric. To handle this emerging\nchallenge, we provide a strong uniform consistency result with the $L^\\infty$\nconvergence rate for KDE on Riemannian manifolds with Riemann integrable\nkernels (in the ambient Euclidean space). We also provide an $L^1$ consistency\nresult for kernel density estimation on Riemannian manifolds with Lebesgue\nintegrable kernels. The isotropic kernels considered in this paper are\ndifferent from the kernels in the Vapnik-Chervonenkis class that are frequently\nconsidered in statistics society. We illustrate the difference when we apply\nthem to estimate the probability density function. Moreover, we elaborate the\ndelicate difference when the kernel is designed on the intrinsic manifold and\non the ambient Euclidian space, both might be encountered in practice. At last,\nwe prove the necessary and sufficient condition for an isotropic kernel to be\nRiemann integrable on a submanifold in the Euclidean space.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 14:36:06 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 16:45:55 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Wu", "Hau-Tieng", ""], ["Wu", "Nan", ""]]}, {"id": "2007.06418", "submitter": "Shichang Tang", "authors": "Shichang Tang", "title": "Lessons Learned from the Training of GANs on Artificial Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have made great progress in\nsynthesizing realistic images in recent years. However, they are often trained\non image datasets with either too few samples or too many classes belonging to\ndifferent data distributions. Consequently, GANs are prone to underfitting or\noverfitting, making the analysis of them difficult and constrained. Therefore,\nin order to conduct a thorough study on GANs while obviating unnecessary\ninterferences introduced by the datasets, we train them on artificial datasets\nwhere there are infinitely many samples and the real data distributions are\nsimple, high-dimensional and have structured manifolds. Moreover, the\ngenerators are designed such that optimal sets of parameters exist.\nEmpirically, we find that under various distance measures, the generator fails\nto learn such parameters with the GAN training procedure. We also find that\ntraining mixtures of GANs leads to more performance gain compared to increasing\nthe network depth or width when the model complexity is high enough. Our\nexperimental results demonstrate that a mixture of generators can discover\ndifferent modes or different classes automatically in an unsupervised setting,\nwhich we attribute to the distribution of the generation and discrimination\ntasks across multiple generators and discriminators. As an example of the\ngeneralizability of our conclusions to realistic datasets, we train a mixture\nof GANs on the CIFAR-10 dataset and our method significantly outperforms the\nstate-of-the-art in terms of popular metrics, i.e., Inception Score (IS) and\nFr\\'echet Inception Distance (FID).\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 14:51:02 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 15:48:16 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Tang", "Shichang", ""]]}, {"id": "2007.06437", "submitter": "Jean Tarbouriech", "authors": "Jean Tarbouriech, Matteo Pirotta, Michal Valko, Alessandro Lazaric", "title": "A Provably Efficient Sample Collection Strategy for Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common assumption in reinforcement learning (RL) is to have access to a\ngenerative model (i.e., a simulator of the environment), which allows to\ngenerate samples from any desired state-action pair. Nonetheless, in many\nsettings a generative model may not be available and an adaptive exploration\nstrategy is needed to efficiently collect samples from an unknown environment\nby direct interaction. In this paper, we study the scenario where an algorithm\nbased on the generative model assumption defines the (possibly time-varying)\namount of samples $b(s,a)$ required at each state-action pair $(s,a)$ and an\nexploration strategy has to learn how to generate $b(s,a)$ samples as fast as\npossible. Building on recent results for regret minimization in the stochastic\nshortest path (SSP) setting (Cohen et al., 2020; Tarbouriech et al., 2020), we\nderive an algorithm that requires $\\tilde{O}( B D + D^{3/2} S^2 A)$ time steps\nto collect the $B = \\sum_{s,a} b(s,a)$ desired samples, in any unknown and\ncommunicating MDP with $S$ states, $A$ actions and diameter $D$. Leveraging the\ngenerality of our strategy, we readily apply it to a variety of existing\nsettings (e.g., model estimation, pure exploration in MDPs) for which we obtain\nimproved sample-complexity guarantees, and to a set of new problems such as\nbest-state identification and sparse reward discovery.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 15:17:35 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Tarbouriech", "Jean", ""], ["Pirotta", "Matteo", ""], ["Valko", "Michal", ""], ["Lazaric", "Alessandro", ""]]}, {"id": "2007.06461", "submitter": "Marcello Colasante", "authors": "Marcello Colasante, Attilio Meucci", "title": "Minimum Relative Entropy Inference for Normal and Monte Carlo\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We represent affine sub-manifolds of exponential family distributions as\nminimum relative entropy sub-manifolds. With such representation we derive\nanalytical formulas for the inference from partial information on expectations\nand covariances of multivariate normal distributions; and we improve the\nnumerical implementation via Monte Carlo simulations for the inference from\npartial information of generalized expectation type.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 15:53:39 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Colasante", "Marcello", ""], ["Meucci", "Attilio", ""]]}, {"id": "2007.06482", "submitter": "Marc Abeille", "authors": "Marc Abeille and Alessandro Lazaric", "title": "Efficient Optimistic Exploration in Linear-Quadratic Regulators via\n  Lagrangian Relaxation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the exploration-exploitation dilemma in the linear quadratic\nregulator (LQR) setting. Inspired by the extended value iteration algorithm\nused in optimistic algorithms for finite MDPs, we propose to relax the\noptimistic optimization of \\ofulq and cast it into a constrained\n\\textit{extended} LQR problem, where an additional control variable implicitly\nselects the system dynamics within a confidence interval. We then move to the\ncorresponding Lagrangian formulation for which we prove strong duality. As a\nresult, we show that an $\\epsilon$-optimistic controller can be computed\nefficiently by solving at most $O\\big(\\log(1/\\epsilon)\\big)$ Riccati equations.\nFinally, we prove that relaxing the original \\ofu problem does not impact the\nlearning performance, thus recovering the $\\tilde{O}(\\sqrt{T})$ regret of\n\\ofulq. To the best of our knowledge, this is the first computationally\nefficient confidence-based algorithm for LQR with worst-case optimal regret\nguarantees.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 16:30:47 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Abeille", "Marc", ""], ["Lazaric", "Alessandro", ""]]}, {"id": "2007.06503", "submitter": "Yanjun Li", "authors": "Yanjun Li, Shujian Yu, Jose C. Principe, Xiaolin Li, and Dapeng Wu", "title": "PRI-VAE: Principle-of-Relevant-Information Variational Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although substantial efforts have been made to learn disentangled\nrepresentations under the variational autoencoder (VAE) framework, the\nfundamental properties to the dynamics of learning of most VAE models still\nremain unknown and under-investigated. In this work, we first propose a novel\nlearning objective, termed the principle-of-relevant-information variational\nautoencoder (PRI-VAE), to learn disentangled representations. We then present\nan information-theoretic perspective to analyze existing VAE models by\ninspecting the evolution of some critical information-theoretic quantities\nacross training epochs. Our observations unveil some fundamental properties\nassociated with VAEs. Empirical results also demonstrate the effectiveness of\nPRI-VAE on four benchmark data sets.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 16:56:00 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Li", "Yanjun", ""], ["Yu", "Shujian", ""], ["Principe", "Jose C.", ""], ["Li", "Xiaolin", ""], ["Wu", "Dapeng", ""]]}, {"id": "2007.06528", "submitter": "Ahmet Alacaoglu", "authors": "Ahmet Alacaoglu, Olivier Fercoq, Volkan Cevher", "title": "Random extrapolation for primal-dual coordinate descent", "comments": "To appear in ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a randomly extrapolated primal-dual coordinate descent method\nthat adapts to sparsity of the data matrix and the favorable structures of the\nobjective function. Our method updates only a subset of primal and dual\nvariables with sparse data, and it uses large step sizes with dense data,\nretaining the benefits of the specific methods designed for each case. In\naddition to adapting to sparsity, our method attains fast convergence\nguarantees in favorable cases \\textit{without any modifications}. In\nparticular, we prove linear convergence under metric subregularity, which\napplies to strongly convex-strongly concave problems and piecewise linear\nquadratic functions. We show almost sure convergence of the sequence and\noptimal sublinear convergence rates for the primal-dual gap and objective\nvalues, in the general convex-concave case. Numerical evidence demonstrates the\nstate-of-the-art empirical performance of our method in sparse and dense\nsettings, matching and improving the existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 17:39:35 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Alacaoglu", "Ahmet", ""], ["Fercoq", "Olivier", ""], ["Cevher", "Volkan", ""]]}, {"id": "2007.06533", "submitter": "Nasim Rahaman", "authors": "Nasim Rahaman, Anirudh Goyal, Muhammad Waleed Gondal, Manuel Wuthrich,\n  Stefan Bauer, Yash Sharma, Yoshua Bengio, Bernhard Sch\\\"olkopf", "title": "S2RMs: Spatially Structured Recurrent Modules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing the structure of a data-generating process by means of appropriate\ninductive biases can help in learning models that generalize well and are\nrobust to changes in the input distribution. While methods that harness spatial\nand temporal structures find broad application, recent work has demonstrated\nthe potential of models that leverage sparse and modular structure using an\nensemble of sparingly interacting modules. In this work, we take a step towards\ndynamic models that are capable of simultaneously exploiting both modular and\nspatiotemporal structures. We accomplish this by abstracting the modeled\ndynamical system as a collection of autonomous but sparsely interacting\nsub-systems. The sub-systems interact according to a topology that is learned,\nbut also informed by the spatial structure of the underlying real-world system.\nThis results in a class of models that are well suited for modeling the\ndynamics of systems that only offer local views into their state, along with\ncorresponding spatial locations of those views. On the tasks of video\nprediction from cropped frames and multi-agent world modeling from partial\nobservations in the challenging Starcraft2 domain, we find our models to be\nmore robust to the number of available views and better capable of\ngeneralization to novel tasks without additional training, even when compared\nagainst strong baselines that perform equally well or better on the training\ndistribution.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 17:44:30 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Rahaman", "Nasim", ""], ["Goyal", "Anirudh", ""], ["Gondal", "Muhammad Waleed", ""], ["Wuthrich", "Manuel", ""], ["Bauer", "Stefan", ""], ["Sharma", "Yash", ""], ["Bengio", "Yoshua", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "2007.06552", "submitter": "Blair Bilodeau", "authors": "Blair Bilodeau, Jeffrey Negrea, Daniel M. Roy", "title": "Relaxing the I.I.D. Assumption: Adaptively Minimax Optimal Regret via\n  Root-Entropic Regularization", "comments": "71 pages, 2 figures. Blair Bilodeau and Jeffrey Negrea are\n  equal-contribution authors; order was determined randomly", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider sequential prediction with expert advice when data are generated\nfrom distributions varying arbitrarily within an unknown constraint set. We\nquantify relaxations of the classical i.i.d. assumption in terms of these\nconstraint sets, with i.i.d. sequences at one extreme and adversarial\nmechanisms at the other. The Hedge algorithm, long known to be minimax optimal\nin the adversarial regime, was recently shown to be minimax optimal for i.i.d.\ndata. We show that Hedge with deterministic learning rates is suboptimal\nbetween these extremes, and present a new algorithm that adaptively achieves\nthe minimax optimal rate of regret with respect to our relaxations of the\ni.i.d. assumption, and does so without knowledge of the underlying constraint\nset. We analyze our algorithm using the follow-the-regularized-leader\nframework, and prove it corresponds to Hedge with an adaptive learning rate\nthat implicitly scales as the square root of the entropy of the current\npredictive distribution, rather than the entropy of the initial predictive\ndistribution.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 17:54:34 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 17:03:49 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Bilodeau", "Blair", ""], ["Negrea", "Jeffrey", ""], ["Roy", "Daniel M.", ""]]}, {"id": "2007.06555", "submitter": "Pranjal Awasthi", "authors": "Pranjal Awasthi, Himanshu Jain, Ankit Singh Rawat, Aravindan\n  Vijayaraghavan", "title": "Adversarial robustness via robust low rank representations", "comments": "fixed a bug in the proof of Proposition B.2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial robustness measures the susceptibility of a classifier to\nimperceptible perturbations made to the inputs at test time. In this work we\nhighlight the benefits of natural low rank representations that often exist for\nreal data such as images, for training neural networks with certified\nrobustness guarantees.\n  Our first contribution is for certified robustness to perturbations measured\nin $\\ell_2$ norm. We exploit low rank data representations to provide improved\nguarantees over state-of-the-art randomized smoothing-based approaches on\nstandard benchmark datasets such as CIFAR-10 and CIFAR-100.\n  Our second contribution is for the more challenging setting of certified\nrobustness to perturbations measured in $\\ell_\\infty$ norm. We demonstrate\nempirically that natural low rank representations have inherent robustness\nproperties, that can be leveraged to provide significantly better guarantees\nfor certified robustness to $\\ell_\\infty$ perturbations in those\nrepresentations. Our certificate of $\\ell_\\infty$ robustness relies on a\nnatural quantity involving the $\\infty \\to 2$ matrix operator norm associated\nwith the representation, to translate robustness guarantees from $\\ell_2$ to\n$\\ell_\\infty$ perturbations.\n  A key technical ingredient for our certification guarantees is a fast\nalgorithm with provable guarantees based on the multiplicative weights update\nmethod to provide upper bounds on the above matrix norm. Our algorithmic\nguarantees improve upon the state of the art for this problem, and may be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 17:57:00 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 04:25:25 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Awasthi", "Pranjal", ""], ["Jain", "Himanshu", ""], ["Rawat", "Ankit Singh", ""], ["Vijayaraghavan", "Aravindan", ""]]}, {"id": "2007.06557", "submitter": "Mateusz Wilinski", "authors": "Mateusz Wilinski, Andrey Y. Lokhov", "title": "Prediction-Centric Learning of Independent Cascade Dynamics from Partial\n  Observations", "comments": "International Conference on Machine Learning 2021, p. 11182-11192", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spreading processes play an increasingly important role in modeling for\ndiffusion networks, information propagation, marketing and opinion setting. We\naddress the problem of learning of a spreading model such that the predictions\ngenerated from this model are accurate and could be subsequently used for the\noptimization, and control of diffusion dynamics. We focus on a challenging\nsetting where full observations of the dynamics are not available, and standard\napproaches such as maximum likelihood quickly become intractable for large\nnetwork instances. We introduce a computationally efficient algorithm, based on\na scalable dynamic message-passing approach, which is able to learn parameters\nof the effective spreading model given only limited information on the\nactivation times of nodes in the network. The popular Independent Cascade model\nis used to illustrate our approach. We show that tractable inference from the\nlearned model generates a better prediction of marginal probabilities compared\nto the original model. We develop a systematic procedure for learning a mixture\nof models which further improves the prediction quality.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 17:58:21 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 06:15:20 GMT"}, {"version": "v3", "created": "Sat, 24 Jul 2021 23:04:00 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Wilinski", "Mateusz", ""], ["Lokhov", "Andrey Y.", ""]]}, {"id": "2007.06558", "submitter": "Shicong Cen", "authors": "Shicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, Yuejie Chi", "title": "Fast Global Convergence of Natural Policy Gradient Methods with Entropy\n  Regularization", "comments": "v2 adds new proofs and improved results; accepted to Operations\n  Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural policy gradient (NPG) methods are among the most widely used policy\noptimization algorithms in contemporary reinforcement learning. This class of\nmethods is often applied in conjunction with entropy regularization -- an\nalgorithmic scheme that encourages exploration -- and is closely related to\nsoft policy iteration and trust region policy optimization. Despite the\nempirical success, the theoretical underpinnings for NPG methods remain limited\neven for the tabular setting. This paper develops $\\textit{non-asymptotic}$\nconvergence guarantees for entropy-regularized NPG methods under softmax\nparameterization, focusing on discounted Markov decision processes (MDPs).\nAssuming access to exact policy evaluation, we demonstrate that the algorithm\nconverges linearly -- or even quadratically once it enters a local region\naround the optimal policy -- when computing optimal value functions of the\nregularized MDP. Moreover, the algorithm is provably stable vis-\\`a-vis\ninexactness of policy evaluation. Our convergence results accommodate a wide\nrange of learning rates, and shed light upon the role of entropy regularization\nin enabling fast convergence.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 17:58:41 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 15:19:12 GMT"}, {"version": "v3", "created": "Mon, 10 Aug 2020 16:02:18 GMT"}, {"version": "v4", "created": "Thu, 24 Sep 2020 19:16:41 GMT"}, {"version": "v5", "created": "Thu, 8 Apr 2021 19:47:39 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Cen", "Shicong", ""], ["Cheng", "Chen", ""], ["Chen", "Yuxin", ""], ["Wei", "Yuting", ""], ["Chi", "Yuejie", ""]]}, {"id": "2007.06559", "submitter": "Jiaxuan You", "authors": "Jiaxuan You, Jure Leskovec, Kaiming He, Saining Xie", "title": "Graph Structure of Neural Networks", "comments": "ICML 2020, with open-source code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are often represented as graphs of connections between\nneurons. However, despite their wide use, there is currently little\nunderstanding of the relationship between the graph structure of the neural\nnetwork and its predictive performance. Here we systematically investigate how\ndoes the graph structure of neural networks affect their predictive\nperformance. To this end, we develop a novel graph-based representation of\nneural networks called relational graph, where layers of neural network\ncomputation correspond to rounds of message exchange along the graph structure.\nUsing this representation we show that: (1) a \"sweet spot\" of relational graphs\nleads to neural networks with significantly improved predictive performance;\n(2) neural network's performance is approximately a smooth function of the\nclustering coefficient and average path length of its relational graph; (3) our\nfindings are consistent across many different tasks and datasets; (4) the sweet\nspot can be identified efficiently; (5) top-performing neural networks have\ngraph structure surprisingly similar to those of real biological neural\nnetworks. Our work opens new directions for the design of neural architectures\nand the understanding on neural networks in general.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 17:59:31 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 17:58:07 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["You", "Jiaxuan", ""], ["Leskovec", "Jure", ""], ["He", "Kaiming", ""], ["Xie", "Saining", ""]]}, {"id": "2007.06566", "submitter": "Seth Flaxman", "authors": "Michaela A. C. Vollmer, Ben Glampson, Thomas A. Mellan, Swapnil\n  Mishra, Luca Mercuri, Ceire Costello, Robert Klaber, Graham Cooke, Seth\n  Flaxman, Samir Bhatt", "title": "A unified machine learning approach to time series forecasting applied\n  to demand at emergency departments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There were 25.6 million attendances at Emergency Departments (EDs) in England\nin 2019 corresponding to an increase of 12 million attendances over the past\nten years. The steadily rising demand at EDs creates a constant challenge to\nprovide adequate quality of care while maintaining standards and productivity.\nManaging hospital demand effectively requires an adequate knowledge of the\nfuture rate of admission. Using 8 years of electronic admissions data from two\nmajor acute care hospitals in London, we develop a novel ensemble methodology\nthat combines the outcomes of the best performing time series and machine\nlearning approaches in order to make highly accurate forecasts of demand, 1, 3\nand 7 days in the future. Both hospitals face an average daily demand of 208\nand 106 attendances respectively and experience considerable volatility around\nthis mean. However, our approach is able to predict attendances at these\nemergency departments one day in advance up to a mean absolute error of +/- 14\nand +/- 10 patients corresponding to a mean absolute percentage error of 6.8%\nand 8.6% respectively. Our analysis compares machine learning algorithms to\nmore traditional linear models. We find that linear models often outperform\nmachine learning methods and that the quality of our predictions for any of the\nforecasting horizons of 1, 3 or 7 days are comparable as measured in MAE. In\naddition to comparing and combining state-of-the-art forecasting methods to\npredict hospital demand, we consider two different hyperparameter tuning\nmethods, enabling a faster deployment of our models without compromising\nperformance. We believe our framework can readily be used to forecast a wide\nrange of policy relevant indicators.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 07:59:24 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Vollmer", "Michaela A. C.", ""], ["Glampson", "Ben", ""], ["Mellan", "Thomas A.", ""], ["Mishra", "Swapnil", ""], ["Mercuri", "Luca", ""], ["Costello", "Ceire", ""], ["Klaber", "Robert", ""], ["Cooke", "Graham", ""], ["Flaxman", "Seth", ""], ["Bhatt", "Samir", ""]]}, {"id": "2007.06605", "submitter": "Om Thakkar", "authors": "Borja Balle, Peter Kairouz, H. Brendan McMahan, Om Thakkar, Abhradeep\n  Thakurta", "title": "Privacy Amplification via Random Check-Ins", "comments": "Updated proof for $(\\epsilon_0, \\delta_0)$-DP local randomizers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Differentially Private Stochastic Gradient Descent (DP-SGD) forms a\nfundamental building block in many applications for learning over sensitive\ndata. Two standard approaches, privacy amplification by subsampling, and\nprivacy amplification by shuffling, permit adding lower noise in DP-SGD than\nvia na\\\"{\\i}ve schemes. A key assumption in both these approaches is that the\nelements in the data set can be uniformly sampled, or be uniformly permuted --\nconstraints that may become prohibitive when the data is processed in a\ndecentralized or distributed fashion. In this paper, we focus on conducting\niterative methods like DP-SGD in the setting of federated learning (FL) wherein\nthe data is distributed among many devices (clients). Our main contribution is\nthe \\emph{random check-in} distributed protocol, which crucially relies only on\nrandomized participation decisions made locally and independently by each\nclient. It has privacy/accuracy trade-offs similar to privacy amplification by\nsubsampling/shuffling. However, our method does not require server-initiated\ncommunication, or even knowledge of the population size. To our knowledge, this\nis the first privacy amplification tailored for a distributed learning\nframework, and it may have broader applicability beyond FL. Along the way, we\nextend privacy amplification by shuffling to incorporate $(\\epsilon,\\delta)$-DP\nlocal randomizers, and exponentially improve its guarantees. In practical\nregimes, this improvement allows for similar privacy and utility using data\nfrom an order of magnitude fewer users.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 18:14:09 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 16:26:38 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Balle", "Borja", ""], ["Kairouz", "Peter", ""], ["McMahan", "H. Brendan", ""], ["Thakkar", "Om", ""], ["Thakurta", "Abhradeep", ""]]}, {"id": "2007.06631", "submitter": "Anton Obukhov", "authors": "Anton Obukhov, Maxim Rakhuba, Stamatios Georgoulis, Menelaos Kanakis,\n  Dengxin Dai, Luc Van Gool", "title": "T-Basis: a Compact Representation for Neural Networks", "comments": "Accepted at ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce T-Basis, a novel concept for a compact representation of a set\nof tensors, each of an arbitrary shape, which is often seen in Neural Networks.\nEach of the tensors in the set is modeled using Tensor Rings, though the\nconcept applies to other Tensor Networks. Owing its name to the T-shape of\nnodes in diagram notation of Tensor Rings, T-Basis is simply a list of equally\nshaped three-dimensional tensors, used to represent Tensor Ring nodes. Such\nrepresentation allows us to parameterize the tensor set with a small number of\nparameters (coefficients of the T-Basis tensors), scaling logarithmically with\neach tensor's size in the set and linearly with the dimensionality of T-Basis.\nWe evaluate the proposed approach on the task of neural network compression and\ndemonstrate that it reaches high compression rates at acceptable performance\ndrops. Finally, we analyze memory and operation requirements of the compressed\nnetworks and conclude that T-Basis networks are equally well suited for\ntraining and inference in resource-constrained environments and usage on the\nedge devices.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 19:03:22 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 17:34:09 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Obukhov", "Anton", ""], ["Rakhuba", "Maxim", ""], ["Georgoulis", "Stamatios", ""], ["Kanakis", "Menelaos", ""], ["Dai", "Dengxin", ""], ["Van Gool", "Luc", ""]]}, {"id": "2007.06633", "submitter": "Darrick Lee", "authors": "Darrick Lee, Robert Ghrist", "title": "Path Signatures on Lie Groups", "comments": "64 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG math.DG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Path signatures are powerful nonparametric tools for time series analysis,\nshown to form a universal and characteristic feature map for Euclidean valued\ntime series data. We lift the theory of path signatures to the setting of Lie\ngroup valued time series, adapting these tools for time series with underlying\ngeometric constraints. We prove that this generalized path signature is\nuniversal and characteristic. To demonstrate universality, we analyze the human\naction recognition problem in computer vision, using $SO(3)$ representations\nfor the time series, providing comparable performance to other shallow learning\napproaches, while offering an easily interpretable feature set. We also provide\na two-sample hypothesis test for Lie group-valued random walks to illustrate\nits characteristic property. Finally we provide algorithms and a Julia\nimplementation of these methods.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 18:38:49 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 17:29:03 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Lee", "Darrick", ""], ["Ghrist", "Robert", ""]]}, {"id": "2007.06634", "submitter": "Xiangmin Han", "authors": "Han Xiangmin, Wang Jun, Zhou Weijun, Chang Cai, Ying Shihui and Shi\n  Jun", "title": "Deep Doubly Supervised Transfer Network for Diagnosis of Breast Cancer\n  with Imbalanced Ultrasound Imaging Modalities", "comments": "Accepted by MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Elastography ultrasound (EUS) provides additional bio-mechanical in-formation\nabout lesion for B-mode ultrasound (BUS) in the diagnosis of breast cancers.\nHowever, joint utilization of both BUS and EUS is not popular due to the lack\nof EUS devices in rural hospitals, which arouses a novel modality im-balance\nproblem in computer-aided diagnosis (CAD) for breast cancers. Current transfer\nlearning (TL) pay little attention to this special issue of clinical modality\nimbalance, that is, the source domain (EUS modality) has fewer labeled samples\nthan those in the target domain (BUS modality). Moreover, these TL methods\ncannot fully use the label information to explore the intrinsic relation\nbetween two modalities and then guide the promoted knowledge transfer. To this\nend, we propose a novel doubly supervised TL network (DDSTN) that integrates\nthe Learning Using Privileged Information (LUPI) paradigm and the Maximum Mean\nDiscrepancy (MMD) criterion into a unified deep TL framework. The proposed\nalgorithm can not only make full use of the shared labels to effectively guide\nknowledge transfer by LUPI paradigm, but also perform additional super-vised\ntransfer between unpaired data. We further introduce the MMD criterion to\nenhance the knowledge transfer. The experimental results on the breast\nultra-sound dataset indicate that the proposed DDSTN outperforms all the\ncompared state-of-the-art algorithms for the BUS-based CAD.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 07:32:07 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Xiangmin", "Han", ""], ["Jun", "Wang", ""], ["Weijun", "Zhou", ""], ["Cai", "Chang", ""], ["Shihui", "Ying", ""], ["Jun", "Shi", ""]]}, {"id": "2007.06637", "submitter": "Ali Ayub", "authors": "Ali Ayub, Alan R. Wagner", "title": "Storing Encoded Episodes as Concepts for Continual Learning", "comments": "Accepted at ICML2020 (Workshop on Lifelong Learning)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two main challenges faced by continual learning approaches are\ncatastrophic forgetting and memory limitations on the storage of data. To cope\nwith these challenges, we propose a novel, cognitively-inspired approach which\ntrains autoencoders with Neural Style Transfer to encode and store images.\nReconstructed images from encoded episodes are replayed when training the\nclassifier model on a new task to avoid catastrophic forgetting. The loss\nfunction for the reconstructed images is weighted to reduce its effect during\nclassifier training to cope with image degradation. When the system runs out of\nmemory the encoded episodes are converted into centroids and covariance\nmatrices, which are used to generate pseudo-images during classifier training,\nkeeping classifier performance stable with less memory. Our approach increases\nclassification accuracy by 13-17% over state-of-the-art methods on benchmark\ndatasets, while requiring 78% less storage space.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 04:15:56 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Ayub", "Ali", ""], ["Wagner", "Alan R.", ""]]}, {"id": "2007.06650", "submitter": "Xinyi Chen", "authors": "Xinyi Chen, Elad Hazan", "title": "Black-Box Control for Linear Dynamical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of controlling an unknown linear time-invariant\ndynamical system from a single chain of black-box interactions, with no access\nto resets or offline simulation. Under the assumption that the system is\ncontrollable, we give the first efficient algorithm that is capable of\nattaining sublinear regret in a single trajectory under the setting of online\nnonstochastic control. This resolves an open problem on the stochastic LQR\nproblem, and in a more challenging setting that allows for adversarial\nperturbations and adversarially chosen and changing convex loss functions.\n  We give finite-time regret bounds for our algorithm on the order of\n$2^{\\tilde{O}(\\mathcal{L})} + \\tilde{O}(\\text{poly}(\\mathcal{L}) T^{2/3})$ for\ngeneral nonstochastic control, and $2^{\\tilde{O}(\\mathcal{L})} +\n\\tilde{O}(\\text{poly}(\\mathcal{L}) \\sqrt{T})$ for black-box LQR, where\n$\\mathcal{L}$ is the system size which is an upper bound on the dimension. The\ncrucial step is a new system identification method that is robust to\nadversarial noise, but incurs exponential cost.\n  To complete the picture, we investigate the complexity of the online\nblack-box control problem, and give a matching lower bound of\n$2^{\\Omega(\\mathcal{L})}$ on the regret, showing that the additional\nexponential cost is inevitable. This lower bound holds even in the noiseless\nsetting, and applies to any, randomized or deterministic, black-box control\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 19:43:19 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 20:27:13 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2021 22:10:12 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Chen", "Xinyi", ""], ["Hazan", "Elad", ""]]}, {"id": "2007.06655", "submitter": "Siamak Mehrkanoon", "authors": "Siamak Mehrkanoon", "title": "Deep Neural-Kernel Machines", "comments": "16 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this chapter we review the main literature related to the recent\nadvancement of deep neural-kernel architecture, an approach that seek the\nsynergy between two powerful class of models, i.e. kernel-based models and\nartificial neural networks. The introduced deep neural-kernel framework is\ncomposed of a hybridization of the neural networks architecture and a kernel\nmachine. More precisely, for the kernel counterpart the model is based on Least\nSquares Support Vector Machines with explicit feature mapping. Here we discuss\nthe use of one form of an explicit feature map obtained by random Fourier\nfeatures. Thanks to this explicit feature map, in one hand bridging the two\narchitectures has become more straightforward and on the other hand one can\nfind the solution of the associated optimization problem in the primal,\ntherefore making the model scalable to large scale datasets. We begin by\nintroducing a neural-kernel architecture that serves as the core module for\ndeeper models equipped with different pooling layers. In particular, we review\nthree neural-kernel machines with average, maxout and convolutional pooling\nlayers. In average pooling layer the outputs of the previous representation\nlayers are averaged. The maxout layer triggers competition among different\ninput representations and allows the formation of multiple sub-networks within\nthe same model. The convolutional pooling layer reduces the dimensionality of\nthe multi-scale output representations. Comparison with neural-kernel model,\nkernel based models and the classical neural networks architecture have been\nmade and the numerical experiments illustrate the effectiveness of the\nintroduced models on several benchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 19:46:29 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 11:03:51 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Mehrkanoon", "Siamak", ""]]}, {"id": "2007.06661", "submitter": "Megha Srivastava", "authors": "Megha Srivastava, Tatsunori Hashimoto, Percy Liang", "title": "Robustness to Spurious Correlations via Human Annotations", "comments": "ICML 2020 final version, 16 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reliability of machine learning systems critically assumes that the\nassociations between features and labels remain similar between training and\ntest distributions. However, unmeasured variables, such as confounders, break\nthis assumption---useful correlations between features and labels at training\ntime can become useless or even harmful at test time. For example, high obesity\nis generally predictive for heart disease, but this relation may not hold for\nsmokers who generally have lower rates of obesity and higher rates of heart\ndisease. We present a framework for making models robust to spurious\ncorrelations by leveraging humans' common sense knowledge of causality.\nSpecifically, we use human annotation to augment each training example with a\npotential unmeasured variable (i.e. an underweight patient with heart disease\nmay be a smoker), reducing the problem to a covariate shift problem. We then\nintroduce a new distributionally robust optimization objective over unmeasured\nvariables (UV-DRO) to control the worst-case loss over possible test-time\nshifts. Empirically, we show improvements of 5-10% on a digit recognition task\nconfounded by rotation, and 1.5-5% on the task of analyzing NYPD Police Stops\nconfounded by location.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 20:05:19 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 22:48:37 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Srivastava", "Megha", ""], ["Hashimoto", "Tatsunori", ""], ["Liang", "Percy", ""]]}, {"id": "2007.06662", "submitter": "Christoph Gote", "authors": "Christoph Gote, Giona Casiraghi, Frank Schweitzer, and Ingo Scholtes", "title": "Predicting Sequences of Traversed Nodes in Graphs using Network Models\n  with Multiple Higher Orders", "comments": "18 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT cs.SI math.IT physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel sequence prediction method for sequential data capturing\nnode traversals in graphs. Our method builds on a statistical modelling\nframework that combines multiple higher-order network models into a single\nmulti-order model. We develop a technique to fit such multi-order models in\nempirical sequential data and to select the optimal maximum order. Our\nframework facilitates both next-element and full sequence prediction given a\nsequence-prefix of any length. We evaluate our model based on six empirical\ndata sets containing sequences from website navigation as well as public\ntransport systems. The results show that our method out-performs\nstate-of-the-art algorithms for next-element prediction. We further demonstrate\nthe accuracy of our method during out-of-sample sequence prediction and\nvalidate that our method can scale to data sets with millions of sequences.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 20:08:14 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Gote", "Christoph", ""], ["Casiraghi", "Giona", ""], ["Schweitzer", "Frank", ""], ["Scholtes", "Ingo", ""]]}, {"id": "2007.06667", "submitter": "Anirudh Som", "authors": "Anirudh Som, Sujeong Kim, Bladimir Lopez-Prado, Svati Dhamija, Nonye\n  Alozie, Amir Tamrakar", "title": "A Machine Learning Approach to Assess Student Group Collaboration Using\n  Individual Level Behavioral Cues", "comments": "Accepted in the ECCV 2020 workshop on Imbalance Problems in Computer\n  Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  K-12 classrooms consistently integrate collaboration as part of their\nlearning experiences. However, owing to large classroom sizes, teachers do not\nhave the time to properly assess each student and give them feedback. In this\npaper we propose using simple deep-learning-based machine learning models to\nautomatically determine the overall collaboration quality of a group based on\nannotations of individual roles and individual level behavior of all the\nstudents in the group. We come across the following challenges when building\nthese models: 1) Limited training data, 2) Severe class label imbalance. We\naddress these challenges by using a controlled variant of Mixup data\naugmentation, a method for generating additional data samples by linearly\ncombining different pairs of data samples and their corresponding class labels.\nAdditionally, the label space for our problem exhibits an ordered structure. We\ntake advantage of this fact and also explore using an ordinal-cross-entropy\nloss function and study its effects with and without Mixup.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 20:15:29 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 18:07:59 GMT"}, {"version": "v3", "created": "Fri, 21 Aug 2020 00:50:39 GMT"}, {"version": "v4", "created": "Wed, 2 Sep 2020 22:09:32 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Som", "Anirudh", ""], ["Kim", "Sujeong", ""], ["Lopez-Prado", "Bladimir", ""], ["Dhamija", "Svati", ""], ["Alozie", "Nonye", ""], ["Tamrakar", "Amir", ""]]}, {"id": "2007.06679", "submitter": "Nicolas Garcia Trillos", "authors": "Jeff Calder and Nicolas Garcia Trillos and Marta Lewicka", "title": "Lipschitz regularity of graph Laplacians on random data clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AP cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study Lipschitz regularity of elliptic PDEs on geometric\ngraphs, constructed from random data points. The data points are sampled from a\ndistribution supported on a smooth manifold. The family of equations that we\nstudy arises in data analysis in the context of graph-based learning and\ncontains, as important examples, the equations satisfied by graph Laplacian\neigenvectors. In particular, we prove high probability interior and global\nLipschitz estimates for solutions of graph Poisson equations. Our results can\nbe used to show that graph Laplacian eigenvectors are, with high probability,\nessentially Lipschitz regular with constants depending explicitly on their\ncorresponding eigenvalues. Our analysis relies on a probabilistic coupling\nargument of suitable random walks at the continuum level, and an interpolation\nmethod for extending functions on random point clouds to the continuum\nmanifold. As a byproduct of our general regularity results, we obtain high\nprobability $L^\\infty$ and approximate $\\mathcal{C}^{0,1}$ convergence rates\nfor the convergence of graph Laplacian eigenvectors towards eigenfunctions of\nthe corresponding weighted Laplace-Beltrami operators. The convergence rates we\nobtain scale like the $L^2$-convergence rates established by two of the authors\nin previous work.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 20:43:19 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Calder", "Jeff", ""], ["Trillos", "Nicolas Garcia", ""], ["Lewicka", "Marta", ""]]}, {"id": "2007.06680", "submitter": "Feihu Huang", "authors": "Feihu Huang, Shangqian Gao, Jian Pei, Heng Huang", "title": "Momentum-Based Policy Gradient Methods", "comments": "ICML 2020, 24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO cs.SY eess.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper, we propose a class of efficient momentum-based policy gradient\nmethods for the model-free reinforcement learning, which use adaptive learning\nrates and do not require any large batches. Specifically, we propose a fast\nimportant-sampling momentum-based policy gradient (IS-MBPG) method based on a\nnew momentum-based variance reduced technique and the importance sampling\ntechnique. We also propose a fast Hessian-aided momentum-based policy gradient\n(HA-MBPG) method based on the momentum-based variance reduced technique and the\nHessian-aided technique. Moreover, we prove that both the IS-MBPG and HA-MBPG\nmethods reach the best known sample complexity of $O(\\epsilon^{-3})$ for\nfinding an $\\epsilon$-stationary point of the non-concave performance function,\nwhich only require one trajectory at each iteration. In particular, we present\na non-adaptive version of IS-MBPG method, i.e., IS-MBPG*, which also reaches\nthe best known sample complexity of $O(\\epsilon^{-3})$ without any large\nbatches. In the experiments, we apply four benchmark tasks to demonstrate the\neffectiveness of our algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 20:44:15 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 13:34:33 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Huang", "Feihu", ""], ["Gao", "Shangqian", ""], ["Pei", "Jian", ""], ["Huang", "Heng", ""]]}, {"id": "2007.06682", "submitter": "Robert Ravier", "authors": "Robert J. Ravier, Mohammadreza Soltani, Miguel Sim\\~oes, Denis\n  Garagic, Vahid Tarokh", "title": "GeoStat Representations of Time Series for Fast Classification", "comments": "28 pages, 8 tables, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in time series classification have largely focused on methods\nthat either employ deep learning or utilize other machine learning models for\nfeature extraction. Though successful, their power often comes at the\nrequirement of computational complexity. In this paper, we introduce GeoStat\nrepresentations for time series. GeoStat representations are based off of a\ngeneralization of recent methods for trajectory classification, and summarize\nthe information of a time series in terms of comprehensive statistics of\n(possibly windowed) distributions of easy to compute differential geometric\nquantities, requiring no dynamic time warping. The features used are intuitive\nand require minimal parameter tuning. We perform an exhaustive evaluation of\nGeoStat on a number of real datasets, showing that simple KNN and SVM\nclassifiers trained on these representations exhibit surprising performance\nrelative to modern single model methods requiring significant computational\npower, achieving state of the art results in many cases. In particular, we show\nthat this methodology achieves good performance on a challenging dataset\ninvolving the classification of fishing vessels, where our methods achieve good\nperformance relative to the state of the art despite only having access to\napproximately two percent of the dataset used in training and evaluating this\nstate of the art.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 20:48:03 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 18:44:10 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2021 22:03:10 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Ravier", "Robert J.", ""], ["Soltani", "Mohammadreza", ""], ["Sim\u00f5es", "Miguel", ""], ["Garagic", "Denis", ""], ["Tarokh", "Vahid", ""]]}, {"id": "2007.06686", "submitter": "Xiaojie Guo", "authors": "Xiaojie Guo, Liang Zhao", "title": "A Systematic Survey on Deep Generative Models for Graph Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs are important data representations for describing objects and their\nrelationships, which appear in a wide diversity of real-world scenarios. As one\nof a critical problem in this area, graph generation considers learning the\ndistributions of given graphs and generating more novel graphs. Owing to its\nwide range of applications, generative models for graphs have a rich history,\nwhich, however, are traditionally hand-crafted and only capable of modeling a\nfew statistical properties of graphs. Recent advances in deep generative models\nfor graph generation is an important step towards improving the fidelity of\ngenerated graphs and paves the way for new kinds of applications. This article\nprovides an extensive overview of the literature in the field of deep\ngenerative models for the graph generation. Firstly, the formal definition of\ndeep generative models for the graph generation as well as preliminary\nknowledge is provided. Secondly, two taxonomies of deep generative models for\nunconditional, and conditional graph generation respectively are proposed; the\nexisting works of each are compared and analyzed. After that, an overview of\nthe evaluation metrics in this specific domain is provided. Finally, the\napplications that deep graph generation enables are summarized and five\npromising future research directions are highlighted.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 20:56:27 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 02:42:16 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Guo", "Xiaojie", ""], ["Zhao", "Liang", ""]]}, {"id": "2007.06700", "submitter": "William Fedus", "authors": "William Fedus, Prajit Ramachandran, Rishabh Agarwal, Yoshua Bengio,\n  Hugo Larochelle, Mark Rowland, Will Dabney", "title": "Revisiting Fundamentals of Experience Replay", "comments": "Published at ICML 2020. First two authors contributed equally and\n  code available at\n  https://github.com/google-research/google-research/tree/master/experience_replay", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experience replay is central to off-policy algorithms in deep reinforcement\nlearning (RL), but there remain significant gaps in our understanding. We\ntherefore present a systematic and extensive analysis of experience replay in\nQ-learning methods, focusing on two fundamental properties: the replay capacity\nand the ratio of learning updates to experience collected (replay ratio). Our\nadditive and ablative studies upend conventional wisdom around experience\nreplay -- greater capacity is found to substantially increase the performance\nof certain algorithms, while leaving others unaffected. Counterintuitively we\nshow that theoretically ungrounded, uncorrected n-step returns are uniquely\nbeneficial while other techniques confer limited benefit for sifting through\nlarger memory. Separately, by directly controlling the replay ratio we\ncontextualize previous observations in the literature and empirically measure\nits importance across a variety of deep RL algorithms. Finally, we conclude by\ntesting a set of hypotheses on the nature of these performance benefits.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 21:22:17 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Fedus", "William", ""], ["Ramachandran", "Prajit", ""], ["Agarwal", "Rishabh", ""], ["Bengio", "Yoshua", ""], ["Larochelle", "Hugo", ""], ["Rowland", "Mark", ""], ["Dabney", "Will", ""]]}, {"id": "2007.06705", "submitter": "Paul Henderson", "authors": "Paul Henderson and Christoph H. Lampert", "title": "Unsupervised object-centric video generation and decomposition in 3D", "comments": "Appeared at NeurIPS 2020. Project page: http://pmh47.net/o3v/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A natural approach to generative modeling of videos is to represent them as a\ncomposition of moving objects. Recent works model a set of 2D sprites over a\nslowly-varying background, but without considering the underlying 3D scene that\ngives rise to them. We instead propose to model a video as the view seen while\nmoving through a scene with multiple 3D objects and a 3D background. Our model\nis trained from monocular videos without any supervision, yet learns to\ngenerate coherent 3D scenes containing several moving objects. We conduct\ndetailed experiments on two datasets, going beyond the visual complexity\nsupported by state-of-the-art generative approaches. We evaluate our method on\ndepth-prediction and 3D object detection -- tasks which cannot be addressed by\nthose earlier works -- and show it out-performs them even on 2D instance\nsegmentation and tracking.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 18:01:29 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 19:11:43 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Henderson", "Paul", ""], ["Lampert", "Christoph H.", ""]]}, {"id": "2007.06711", "submitter": "Derek Prijatelj", "authors": "Derek S. Prijatelj (1), Mel McCurrie (2), Walter J. Scheirer (1) ((1)\n  University of Notre Dame, Notre Dame, USA, (2) Perceptive Automata, Boston,\n  USA)", "title": "A Bayesian Evaluation Framework for Ground Truth-Free Visual Recognition\n  Tasks", "comments": "21 pages. 11 figures. 2 tables. Submitted to NeurIPS2020. Code to be\n  included after publication at\n  https://github.com/prijatelj/bayesian_eval_ground_truth-free", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An interesting development in automatic visual recognition has been the\nemergence of tasks where it is not possible to assign ground truth labels to\nimages, yet still feasible to collect annotations that reflect human judgements\nabout them. Such tasks include subjective visual attribute assignment and the\nlabeling of ambiguous scenes. Machine learning-based predictors for these tasks\nrely on supervised training that models the behavior of the annotators, e.g.,\nwhat would the average person's judgement be for an image? A key open question\nfor this type of work, especially for applications where inconsistency with\nhuman behavior can lead to ethical lapses, is how to evaluate the uncertainty\nof trained predictors. Given that the real answer is unknowable, we are left\nwith often noisy judgements from human annotators to work with. In order to\naccount for the uncertainty that is present, we propose a relative Bayesian\nframework for evaluating predictors trained on such data. The framework\nspecifies how to estimate a predictor's uncertainty due to the human labels by\napproximating a conditional distribution and producing a credible interval for\nthe predictions and their measures of performance. The framework is\nsuccessfully applied to four image classification tasks that use subjective\nhuman judgements: facial beauty assessment using the SCUT-FBP5500 dataset,\nsocial attribute assignment using data from TestMyBrain.org, apparent age\nestimation using data from the ChaLearn series of challenges, and ambiguous\nscene labeling using the LabelMe dataset.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 18:35:33 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Prijatelj", "Derek S.", ""], ["McCurrie", "Mel", ""], ["Scheirer", "Walter J.", ""]]}, {"id": "2007.06731", "submitter": "Xuchan Bao", "authors": "Xuchan Bao, James Lucas, Sushant Sachdeva, Roger Grosse", "title": "Regularized linear autoencoders recover the principal components,\n  eventually", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our understanding of learning input-output relationships with neural nets has\nimproved rapidly in recent years, but little is known about the convergence of\nthe underlying representations, even in the simple case of linear autoencoders\n(LAEs). We show that when trained with proper regularization, LAEs can directly\nlearn the optimal representation -- ordered, axis-aligned principal components.\nWe analyze two such regularization schemes: non-uniform $\\ell_2$ regularization\nand a deterministic variant of nested dropout [Rippel et al, ICML' 2014].\nThough both regularization schemes converge to the optimal representation, we\nshow that this convergence is slow due to ill-conditioning that worsens with\nincreasing latent dimension. We show that the inefficiency of learning the\noptimal representation is not inevitable -- we present a simple modification to\nthe gradient descent update that greatly speeds up convergence empirically.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 23:08:25 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Bao", "Xuchan", ""], ["Lucas", "James", ""], ["Sachdeva", "Sushant", ""], ["Grosse", "Roger", ""]]}, {"id": "2007.06737", "submitter": "Xuhong Li", "authors": "Xuhong Li, Yves Grandvalet, R\\'emi Flamary, Nicolas Courty, Dejing Dou", "title": "Representation Transfer by Optimal Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning generic representations with deep networks requires massive training\nsamples and significant computer resources. To learn a new specific task, an\nimportant issue is to transfer the generic teacher's representation to a\nstudent network. In this paper, we propose to use a metric between\nrepresentations that is based on a functional view of neurons. We use optimal\ntransport to quantify the match between two representations, yielding a\ndistance that embeds some invariances inherent to the representation of deep\nnetworks. This distance defines a regularizer promoting the similarity of the\nstudent's representation with that of the teacher. Our approach can be used in\nany learning context where representation transfer is applicable. We experiment\nhere on two standard settings: inductive transfer learning, where the teacher's\nrepresentation is transferred to a student network of same architecture for a\nnew related task, and knowledge distillation, where the teacher's\nrepresentation is transferred to a student of simpler architecture for the same\ntask (model compression). Our approach also lends itself to solving new\nlearning problems; we demonstrate this by showing how to directly transfer the\nteacher's representation to a simpler architecture student for a new related\ntask.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 23:42:06 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 06:34:09 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Li", "Xuhong", ""], ["Grandvalet", "Yves", ""], ["Flamary", "R\u00e9mi", ""], ["Courty", "Nicolas", ""], ["Dou", "Dejing", ""]]}, {"id": "2007.06738", "submitter": "Edward Moroshko", "authors": "Edward Moroshko, Suriya Gunasekar, Blake Woodworth, Jason D. Lee,\n  Nathan Srebro, Daniel Soudry", "title": "Implicit Bias in Deep Linear Classification: Initialization Scale vs\n  Training Accuracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a detailed asymptotic study of gradient flow trajectories and\ntheir implicit optimization bias when minimizing the exponential loss over\n\"diagonal linear networks\". This is the simplest model displaying a transition\nbetween \"kernel\" and non-kernel (\"rich\" or \"active\") regimes. We show how the\ntransition is controlled by the relationship between the initialization scale\nand how accurately we minimize the training loss. Our results indicate that\nsome limit behaviors of gradient descent only kick in at ridiculous training\naccuracies (well beyond $10^{-100}$). Moreover, the implicit bias at reasonable\ninitialization scales and training accuracies is more complex and not captured\nby these limits.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 23:49:53 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Moroshko", "Edward", ""], ["Gunasekar", "Suriya", ""], ["Woodworth", "Blake", ""], ["Lee", "Jason D.", ""], ["Srebro", "Nathan", ""], ["Soudry", "Daniel", ""]]}, {"id": "2007.06741", "submitter": "Jo\\~ao Pedro Ara\\'ujo", "authors": "Jo\\~ao Pedro Ara\\'ujo, M\\'ario Figueiredo, Miguel Ayala Botto", "title": "Single-partition adaptive Q-learning", "comments": "34 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces single-partition adaptive Q-learning (SPAQL), an\nalgorithm for model-free episodic reinforcement learning (RL), which adaptively\npartitions the state-action space of a Markov decision process (MDP), while\nsimultaneously learning a time-invariant policy (i. e., the mapping from states\nto actions does not depend explicitly on the episode time step) for maximizing\nthe cumulative reward. The trade-off between exploration and exploitation is\nhandled by using a mixture of upper confidence bounds (UCB) and Boltzmann\nexploration during training, with a temperature parameter that is automatically\ntuned as training progresses. The algorithm is an improvement over adaptive\nQ-learning (AQL). It converges faster to the optimal solution, while also using\nfewer arms. Tests on episodes with a large number of time steps show that SPAQL\nhas no problems scaling, unlike AQL. Based on this empirical evidence, we claim\nthat SPAQL may have a higher sample efficiency than AQL, thus being a relevant\ncontribution to the field of efficient model-free RL methods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 00:03:25 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Ara\u00fajo", "Jo\u00e3o Pedro", ""], ["Figueiredo", "M\u00e1rio", ""], ["Botto", "Miguel Ayala", ""]]}, {"id": "2007.06744", "submitter": "David Woodruff", "authors": "Edith Cohen, Rasmus Pagh, David P. Woodruff", "title": "WOR and $p$'s: Sketches for $\\ell_p$-Sampling Without Replacement", "comments": "21 pages. Added references in related work in the introduction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weighted sampling is a fundamental tool in data analysis and machine learning\npipelines. Samples are used for efficient estimation of statistics or as sparse\nrepresentations of the data. When weight distributions are skewed, as is often\nthe case in practice, without-replacement (WOR) sampling is much more effective\nthan with-replacement (WR) sampling: it provides a broader representation and\nhigher accuracy for the same number of samples. We design novel composable\nsketches for WOR $\\ell_p$ sampling, weighted sampling of keys according to a\npower $p\\in[0,2]$ of their frequency (or for signed data, sum of updates). Our\nsketches have size that grows only linearly with the sample size. Our design is\nsimple and practical, despite intricate analysis, and based on off-the-shelf\nuse of widely implemented heavy hitters sketches such as CountSketch. Our\nmethod is the first to provide WOR sampling in the important regime of $p>1$\nand the first to handle signed updates for $p>0$.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 00:19:27 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 15:06:53 GMT"}, {"version": "v3", "created": "Sat, 15 Aug 2020 06:12:25 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Cohen", "Edith", ""], ["Pagh", "Rasmus", ""], ["Woodruff", "David P.", ""]]}, {"id": "2007.06753", "submitter": "Qing Qu", "authors": "Yuqian Zhang, Qing Qu, and John Wright", "title": "From Symmetry to Geometry: Tractable Nonconvex Problems", "comments": "review paper submitted to SIAM Review, 34 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As science and engineering have become increasingly data-driven, the role of\noptimization has expanded to touch almost every stage of the data analysis\npipeline, from the signal and data acquisition to modeling and prediction. The\noptimization problems encountered in practice are often nonconvex. While\nchallenges vary from problem to problem, one common source of nonconvexity is\nnonlinearity in the data or measurement model. Nonlinear models often exhibit\nsymmetries, creating complicated, nonconvex objective landscapes, with multiple\nequivalent solutions. Nevertheless, simple methods (e.g., gradient descent)\noften perform surprisingly well in practice.\n  The goal of this survey is to highlight a class of tractable nonconvex\nproblems, which can be understood through the lens of symmetries. These\nproblems exhibit a characteristic geometric structure: local minimizers are\nsymmetric copies of a single \"ground truth\" solution, while other critical\npoints occur at balanced superpositions of symmetric copies of the ground\ntruth, and exhibit negative curvature in directions that break the symmetry.\nThis structure enables efficient methods to obtain global minimizers. We\ndiscuss examples of this phenomenon arising from a wide range of problems in\nimaging, signal processing, and data analysis. We highlight the key role of\nsymmetry in shaping the objective landscape and discuss the different roles of\nrotational and discrete symmetries. This area is rich with observed phenomena\nand open problems; we close by highlighting directions for future research.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 01:19:15 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 03:11:17 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 04:03:32 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zhang", "Yuqian", ""], ["Qu", "Qing", ""], ["Wright", "John", ""]]}, {"id": "2007.06758", "submitter": "Chaoran Huang", "authors": "May Altulyan, Lina Yao, Xianzhi Wang, Chaoran Huang, Salil S Kanhere,\n  Quan Z Sheng", "title": "Recommender Systems for the Internet of Things: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation represents a vital stage in developing and promoting the\nbenefits of the Internet of Things (IoT). Traditional recommender systems fail\nto exploit ever-growing, dynamic, and heterogeneous IoT data. This paper\npresents a comprehensive review of the state-of-the-art recommender systems, as\nwell as related techniques and application in the vibrant field of IoT. We\ndiscuss several limitations of applying recommendation systems to IoT and\npropose a reference framework for comparing existing studies to guide future\nresearch and practices.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 01:24:44 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Altulyan", "May", ""], ["Yao", "Lina", ""], ["Wang", "Xianzhi", ""], ["Huang", "Chaoran", ""], ["Kanhere", "Salil S", ""], ["Sheng", "Quan Z", ""]]}, {"id": "2007.06776", "submitter": "Tristan Jean-Baptiste", "authors": "Jean-Baptiste Tristan, Joseph Tassarotti, Koundinya Vajjha, Michael L.\n  Wick, Anindya Banerjee", "title": "Verification of ML Systems via Reparameterization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machine learning is increasingly used in essential systems, it is\nimportant to reduce or eliminate the incidence of serious bugs. A growing body\nof research has developed machine learning algorithms with formal guarantees\nabout performance, robustness, or fairness. Yet, the analysis of these\nalgorithms is often complex, and implementing such systems in practice\nintroduces room for error. Proof assistants can be used to formally verify\nmachine learning systems by constructing machine checked proofs of correctness\nthat rule out such bugs. However, reasoning about probabilistic claims inside\nof a proof assistant remains challenging. We show how a probabilistic program\ncan be automatically represented in a theorem prover using the concept of\n\\emph{reparameterization}, and how some of the tedious proofs of measurability\ncan be generated automatically from the probabilistic program. To demonstrate\nthat this approach is broad enough to handle rather different types of machine\nlearning systems, we verify both a classic result from statistical learning\ntheory (PAC-learnability of decision stumps) and prove that the null model used\nin a Bayesian hypothesis test satisfies a fairness criterion called demographic\nparity.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 02:19:25 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Tristan", "Jean-Baptiste", ""], ["Tassarotti", "Joseph", ""], ["Vajjha", "Koundinya", ""], ["Wick", "Michael L.", ""], ["Banerjee", "Anindya", ""]]}, {"id": "2007.06799", "submitter": "Anjaly Parayil", "authors": "Anjaly Parayil, He Bai, Jemin George, and Prudhvi Gurram", "title": "A Decentralized Approach to Bayesian Learning", "comments": "42 pages, 37 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by decentralized approaches to machine learning, we propose a\ncollaborative Bayesian learning algorithm taking the form of decentralized\nLangevin dynamics in a non-convex setting. Our analysis show that the initial\nKL-divergence between the Markov Chain and the target posterior distribution is\nexponentially decreasing while the error contributions to the overall\nKL-divergence from the additive noise is decreasing in polynomial time. We\nfurther show that the polynomial-term experiences speed-up with number of\nagents and provide sufficient conditions on the time-varying step-sizes to\nguarantee convergence to the desired distribution. The performance of the\nproposed algorithm is evaluated on a wide variety of machine learning tasks.\nThe empirical results show that the performance of individual agents with\nlocally available data is on par with the centralized setting with considerable\nimprovement in the convergence rate.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 03:59:17 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 16:33:20 GMT"}, {"version": "v3", "created": "Mon, 2 Nov 2020 18:08:40 GMT"}, {"version": "v4", "created": "Sat, 9 Jan 2021 17:01:44 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Parayil", "Anjaly", ""], ["Bai", "He", ""], ["George", "Jemin", ""], ["Gurram", "Prudhvi", ""]]}, {"id": "2007.06803", "submitter": "Bo Lin", "authors": "Rui Zhu, Bo Lin, Haixu Tang", "title": "Bounding The Number of Linear Regions in Local Area for Neural Networks\n  with ReLU Activations", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of linear regions is one of the distinct properties of the neural\nnetworks using piecewise linear activation functions such as ReLU, comparing\nwith those conventional ones using other activation functions. Previous studies\nshowed this property reflected the expressivity of a neural network family\n([14]); as a result, it can be used to characterize how the structural\ncomplexity of a neural network model affects the function it aims to compute.\nNonetheless, it is challenging to directly compute the number of linear\nregions; therefore, many researchers focus on estimating the bounds (in\nparticular the upper bound) of the number of linear regions for deep neural\nnetworks using ReLU. These methods, however, attempted to estimate the upper\nbound in the entire input space. The theoretical methods are still lacking to\nestimate the number of linear regions within a specific area of the input\nspace, e.g., a sphere centered at a training data point such as an adversarial\nexample or a backdoor trigger. In this paper, we present the first method to\nestimate the upper bound of the number of linear regions in any sphere in the\ninput space of a given ReLU neural network. We implemented the method, and\ncomputed the bounds in deep neural networks using the piece-wise linear active\nfunction. Our experiments showed that, while training a neural network, the\nboundaries of the linear regions tend to move away from the training data\npoints. In addition, we observe that the spheres centered at the training data\npoints tend to contain more linear regions than any arbitrary points in the\ninput space. To the best of our knowledge, this is the first study of bounding\nlinear regions around a specific data point. We consider our work as a first\nstep toward the investigation of the structural complexity of deep neural\nnetworks in a specific input area.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 04:06:00 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Zhu", "Rui", ""], ["Lin", "Bo", ""], ["Tang", "Haixu", ""]]}, {"id": "2007.06823", "submitter": "Laurent Valentin Jospin", "authors": "Laurent Valentin Jospin and Wray Buntine and Farid Boussaid and Hamid\n  Laga and Mohammed Bennamoun", "title": "Hands-on Bayesian Neural Networks -- a Tutorial for Deep Learning Users", "comments": "35 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep learning methods have equipped researchers and engineers with\nincredibly powerful tools to tackle problems that previously seemed impossible.\nHowever, since deep learning methods operate as black boxes, the uncertainty\nassociated with their predictions is often challenging to quantify. Bayesian\nstatistics offer a formalism to understand and quantify the uncertainty\nassociated with deep neural networks predictions. This paper provides a\ntutorial for researchers and scientists who are using machine learning,\nespecially deep learning, with an overview of the relevant literature and a\ncomplete toolset to design, implement, train, use and evaluate Bayesian neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 05:21:27 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Jospin", "Laurent Valentin", ""], ["Buntine", "Wray", ""], ["Boussaid", "Farid", ""], ["Laga", "Hamid", ""], ["Bennamoun", "Mohammed", ""]]}, {"id": "2007.06827", "submitter": "Yaroslav Averyanov", "authors": "Yaroslav Averyanov and Alain Celisse", "title": "Early stopping and polynomial smoothing in regression with reproducing\n  kernels", "comments": "typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of early stopping for iterative learning\nalgorithms in a reproducing kernel Hilbert space (RKHS) in the nonparametric\nregression framework. In particular, we work with the gradient descent and\n(iterative) kernel ridge regression algorithms. We present a data-driven rule\nto perform early stopping without a validation set that is based on the\nso-called minimum discrepancy principle. This method enjoys only one assumption\non the regression function: it belongs to a reproducing kernel Hilbert space\n(RKHS). The proposed rule is proved to be minimax-optimal over different types\nof kernel spaces, including finite-rank and Sobolev smoothness classes. The\nproof is derived from the fixed-point analysis of the localized Rademacher\ncomplexities, which is a standard technique for obtaining optimal rates in the\nnonparametric regression literature. In addition to that, we present simulation\nresults on artificial datasets that show the comparable performance of the\ndesigned rule with respect to other stopping rules such as the one determined\nby V-fold cross-validation.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 05:27:18 GMT"}, {"version": "v2", "created": "Sat, 28 Nov 2020 21:26:11 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Averyanov", "Yaroslav", ""], ["Celisse", "Alain", ""]]}, {"id": "2007.06831", "submitter": "Zhe Liu", "authors": "Zhe Liu, Lina Yao, Lei Bai, Xianzhi Wang, Can Wang", "title": "Spectrum-Guided Adversarial Disparity Learning", "comments": null, "journal-ref": null, "doi": "10.1145/3394486.3403054", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been a significant challenge to portray intraclass disparity precisely\nin the area of activity recognition, as it requires a robust representation of\nthe correlation between subject-specific variation for each activity class. In\nthis work, we propose a novel end-to-end knowledge directed adversarial\nlearning framework, which portrays the class-conditioned intraclass disparity\nusing two competitive encoding distributions and learns the purified latent\ncodes by denoising learned disparity. Furthermore, the domain knowledge is\nincorporated in an unsupervised manner to guide the optimization and further\nboosts the performance. The experiments on four HAR benchmark datasets\ndemonstrate the robustness and generalization of our proposed methods over a\nset of state-of-the-art. We further prove the effectiveness of automatic domain\nknowledge incorporation in performance enhancement.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 05:46:27 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Liu", "Zhe", ""], ["Yao", "Lina", ""], ["Bai", "Lei", ""], ["Wang", "Xianzhi", ""], ["Wang", "Can", ""]]}, {"id": "2007.06833", "submitter": "Efthymios Tzinis", "authors": "Efthymios Tzinis, Zhepei Wang and Paris Smaragdis", "title": "Sudo rm -rf: Efficient Networks for Universal Audio Source Separation", "comments": "accepted to MLSP 2020", "journal-ref": "Published in 2020 IEEE 30th International Workshop on Machine\n  Learning for Signal Processing (MLSP)", "doi": "10.1109/MLSP49062.2020.9231900", "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an efficient neural network for end-to-end general\npurpose audio source separation. Specifically, the backbone structure of this\nconvolutional network is the SUccessive DOwnsampling and Resampling of\nMulti-Resolution Features (SuDoRMRF) as well as their aggregation which is\nperformed through simple one-dimensional convolutions. In this way, we are able\nto obtain high quality audio source separation with limited number of floating\npoint operations, memory requirements, number of parameters and latency. Our\nexperiments on both speech and environmental sound separation datasets show\nthat SuDoRMRF performs comparably and even surpasses various state-of-the-art\napproaches with significantly higher computational resource requirements.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 05:46:38 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Tzinis", "Efthymios", ""], ["Wang", "Zhepei", ""], ["Smaragdis", "Paris", ""]]}, {"id": "2007.06835", "submitter": "Nagarajan Natarajan", "authors": "Nagarajan Natarajan, Ajaykrishna Karthikeyan, Prateek Jain, Ivan\n  Radicek, Sriram Rajamani, Sumit Gulwani, Johannes Gehrke", "title": "Programming by Rewards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formalize and study ``programming by rewards'' (PBR), a new approach for\nspecifying and synthesizing subroutines for optimizing some quantitative metric\nsuch as performance, resource utilization, or correctness over a benchmark. A\nPBR specification consists of (1) input features $x$, and (2) a reward function\n$r$, modeled as a black-box component (which we can only run), that assigns a\nreward for each execution. The goal of the synthesizer is to synthesize a\n\"decision function\" $f$ which transforms the features to a decision value for\nthe black-box component so as to maximize the expected reward $E[r \\circ f\n(x)]$ for executing decisions $f(x)$ for various values of $x$. We consider a\nspace of decision functions in a DSL of loop-free if-then-else programs, which\ncan branch on linear functions of the input features in a tree-structure and\ncompute a linear function of the inputs in the leaves of the tree. We find that\nthis DSL captures decision functions that are manually written in practice by\nprogrammers. Our technical contribution is the use of continuous-optimization\ntechniques to perform synthesis of such decision functions as if-then-else\nprograms. We also show that the framework is theoretically-founded ---in cases\nwhen the rewards satisfy nice properties, the synthesized code is optimal in a\nprecise sense.\n  We have leveraged PBR to synthesize non-trivial decision functions related to\nsearch and ranking heuristics in the PROSE codebase (an industrial strength\nprogram synthesis framework) and achieve competitive results to manually\nwritten procedures over multiple man years of tuning. We present empirical\nevaluation against other baseline techniques over real-world case studies\n(including PROSE) as well on simple synthetic benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 05:49:14 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Natarajan", "Nagarajan", ""], ["Karthikeyan", "Ajaykrishna", ""], ["Jain", "Prateek", ""], ["Radicek", "Ivan", ""], ["Rajamani", "Sriram", ""], ["Gulwani", "Sumit", ""], ["Gehrke", "Johannes", ""]]}, {"id": "2007.06847", "submitter": "Po-Nan Li", "authors": "Po-Nan Li and Saulo H. P. de Oliveira and Soichi Wakatsuki and Henry\n  van den Bedem", "title": "Sequence-guided protein structure determination using graph\n  convolutional and recurrent networks", "comments": "6 pages, 5 figures; accepted to IEEE BIBE 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.CE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single particle, cryogenic electron microscopy (cryo-EM) experiments now\nroutinely produce high-resolution data for large proteins and their complexes.\nBuilding an atomic model into a cryo-EM density map is challenging,\nparticularly when no structure for the target protein is known a priori.\nExisting protocols for this type of task often rely on significant human\nintervention and can take hours to many days to produce an output. Here, we\npresent a fully automated, template-free model building approach that is based\nentirely on neural networks. We use a graph convolutional network (GCN) to\ngenerate an embedding from a set of rotamer-based amino acid identities and\ncandidate 3-dimensional C$\\alpha$ locations. Starting from this embedding, we\nuse a bidirectional long short-term memory (LSTM) module to order and label the\ncandidate identities and atomic locations consistent with the input protein\nsequence to obtain a structural model. Our approach paves the way for\ndetermining protein structures from cryo-EM densities at a fraction of the time\nof existing approaches and without the need for human intervention.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 06:24:07 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 13:59:01 GMT"}, {"version": "v3", "created": "Thu, 3 Sep 2020 02:25:28 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Li", "Po-Nan", ""], ["de Oliveira", "Saulo H. P.", ""], ["Wakatsuki", "Soichi", ""], ["Bedem", "Henry van den", ""]]}, {"id": "2007.06849", "submitter": "Hao Zhu", "authors": "Yifei Zhang and Hao Zhu", "title": "Additively Homomorphical Encryption based Deep Neural Network for\n  Asymmetrically Collaborative Machine Learning", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The financial sector presents many opportunities to apply various machine\nlearning techniques. Centralized machine learning creates a constraint which\nlimits further applications in finance sectors. Data privacy is a fundamental\nchallenge for a variety of finance and insurance applications that account on\nlearning a model across different sections. In this paper, we define a new\npractical scheme of collaborative machine learning that one party owns data,\nbut another party owns labels only, and term this \\textbf{Asymmetrically\nCollaborative Machine Learning}. For this scheme, we propose a novel\nprivacy-preserving architecture where two parties can collaboratively train a\ndeep learning model efficiently while preserving the privacy of each party's\ndata. More specifically, we decompose the forward propagation and\nbackpropagation of the neural network into four different steps and propose a\nnovel protocol to handle information leakage in these steps. Our extensive\nexperiments on different datasets demonstrate not only stable training without\naccuracy loss, but also more than 100 times speedup compared with the\nstate-of-the-art system.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 06:43:25 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Zhang", "Yifei", ""], ["Zhu", "Hao", ""]]}, {"id": "2007.06869", "submitter": "Karthik Abinav Sankararaman", "authors": "Karthik Abinav Sankararaman, Anand Louis, Navin Goyal", "title": "Robust Identifiability in Linear Structural Equation Models of Causal\n  Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the problem of robust parameter estimation from\nobservational data in the context of linear structural equation models (LSEMs).\nLSEMs are a popular and well-studied class of models for inferring causality in\nthe natural and social sciences. One of the main problems related to LSEMs is\nto recover the model parameters from the observational data. Under various\nconditions on LSEMs and the model parameters the prior work provides efficient\nalgorithms to recover the parameters. However, these results are often about\ngeneric identifiability. In practice, generic identifiability is not sufficient\nand we need robust identifiability: small changes in the observational data\nshould not affect the parameters by a huge amount. Robust identifiability has\nreceived far less attention and remains poorly understood. Sankararaman et al.\n(2019) recently provided a set of sufficient conditions on parameters under\nwhich robust identifiability is feasible. However, a limitation of their work\nis that their results only apply to a small sub-class of LSEMs, called\n``bow-free paths.'' In this work, we significantly extend their work along\nmultiple dimensions. First, for a large and well-studied class of LSEMs, namely\n``bow free'' models, we provide a sufficient condition on model parameters\nunder which robust identifiability holds, thereby removing the restriction of\npaths required by prior work. We then show that this sufficient condition holds\nwith high probability which implies that for a large set of parameters robust\nidentifiability holds and that for such parameters, existing algorithms already\nachieve robust identifiability. Finally, we validate our results on both\nsimulated and real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 07:32:36 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Sankararaman", "Karthik Abinav", ""], ["Louis", "Anand", ""], ["Goyal", "Navin", ""]]}, {"id": "2007.06878", "submitter": "Hao Cheng", "authors": "Hao Cheng, Joey Tianyi Zhou, Wee Peng Tay and Bihan Wen", "title": "Attentive Graph Neural Networks for Few-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNN) has demonstrated the superior performance in many\nchallenging applications, including the few-shot learning tasks. Despite its\npowerful capacity to learn and generalize the model from few samples, GNN\nusually suffers from severe over-fitting and over-smoothing as the model\nbecomes deep, which limit the scalability. In this work, we propose a novel\nAttentive GNN to tackle these challenges, by incorporating a triple-attention\nmechanism, i.e. node self-attention, neighborhood attention, and layer memory\nattention. We explain why the proposed attentive modules can improve GNN for\nfew-shot learning with theoretical analysis and illustrations. Extensive\nexperiments show that the proposed Attentive GNN model achieves the promising\nresults, comparing to the state-of-the-art GNN- and CNN-based methods for\nfew-shot learning tasks, over the mini-ImageNet and tiered-ImageNet benchmarks,\nunder ConvNet-4 and ResNet-based backbone with both inductive and transductive\nsettings. The codes will be made publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 07:43:09 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 13:08:56 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Cheng", "Hao", ""], ["Zhou", "Joey Tianyi", ""], ["Tay", "Wee Peng", ""], ["Wen", "Bihan", ""]]}, {"id": "2007.06894", "submitter": "Roel Henckaerts", "authors": "Roel Henckaerts and Katrien Antonio and Marie-Pier C\\^ot\\'e", "title": "When stakes are high: balancing accuracy and transparency with\n  Model-Agnostic Interpretable Data-driven suRRogates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highly regulated industries, like banking and insurance, ask for transparent\ndecision-making algorithms. At the same time, competitive markets are pushing\nfor the use of complex black box models. We therefore present a procedure to\ndevelop a Model-Agnostic Interpretable Data-driven suRRogate (maidrr) suited\nfor structured tabular data. Knowledge is extracted from a black box via\npartial dependence effects. These are used to perform smart feature engineering\nby grouping variable values. This results in a segmentation of the feature\nspace with automatic variable selection. A transparent generalized linear model\n(GLM) is fit to the features in categorical format and their relevant\ninteractions. We demonstrate our R package maidrr with a case study on general\ninsurance claim frequency modeling for six publicly available datasets. Our\nmaidrr GLM closely approximates a gradient boosting machine (GBM) black box and\noutperforms both a linear and tree surrogate as benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 08:10:05 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 17:44:03 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Henckaerts", "Roel", ""], ["Antonio", "Katrien", ""], ["C\u00f4t\u00e9", "Marie-Pier", ""]]}, {"id": "2007.06918", "submitter": "Aswin Raghavan", "authors": "Aswin Raghavan, Jesse Hostetler, Indranil Sur, Abrar Rahman, Ajay\n  Divakaran", "title": "Lifelong Learning using Eigentasks: Task Separation, Skill Acquisition,\n  and Selective Transfer", "comments": "Accepted at the 4th Lifelong Machine Learning Workshop at the\n  Thirty-seventh International Conference on Machine Learning (ICML) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the eigentask framework for lifelong learning. An eigentask is a\npairing of a skill that solves a set of related tasks, paired with a generative\nmodel that can sample from the skill's input space. The framework extends\ngenerative replay approaches, which have mainly been used to avoid catastrophic\nforgetting, to also address other lifelong learning goals such as forward\nknowledge transfer. We propose a wake-sleep cycle of alternating task learning\nand knowledge consolidation for learning in our framework, and instantiate it\nfor lifelong supervised learning and lifelong RL. We achieve improved\nperformance over the state-of-the-art in supervised continual learning, and\nshow evidence of forward knowledge transfer in a lifelong RL application in the\ngame Starcraft2.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 09:06:13 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Raghavan", "Aswin", ""], ["Hostetler", "Jesse", ""], ["Sur", "Indranil", ""], ["Rahman", "Abrar", ""], ["Divakaran", "Ajay", ""]]}, {"id": "2007.06927", "submitter": "Karlson Pfannschmidt", "authors": "Karlson Pfannschmidt, Eyke H\\\"ullermeier", "title": "Learning Choice Functions via Pareto-Embeddings", "comments": "5 pages, 4 figures, presented at KI 2020, 43. German Conference on\n  Artificial Intelligence, Bamberg, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning to choose from a given set of objects,\nwhere each object is represented by a feature vector. Traditional approaches in\nchoice modelling are mainly based on learning a latent, real-valued utility\nfunction, thereby inducing a linear order on choice alternatives. While this\napproach is suitable for discrete (top-1) choices, it is not straightforward\nhow to use it for subset choices. Instead of mapping choice alternatives to the\nreal number line, we propose to embed them into a higher-dimensional utility\nspace, in which we identify choice sets with Pareto-optimal points. To this\nend, we propose a learning algorithm that minimizes a differentiable loss\nfunction suitable for this task. We demonstrate the feasibility of learning a\nPareto-embedding on a suite of benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 09:34:44 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Pfannschmidt", "Karlson", ""], ["H\u00fcllermeier", "Eyke", ""]]}, {"id": "2007.06944", "submitter": "Augusto Fasano", "authors": "Augusto Fasano and Daniele Durante", "title": "A Class of Conjugate Priors for Multinomial Probit Models which Includes\n  the Multivariate Normal One", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multinomial probit models are widely-implemented representations which allow\nboth classification and inference by learning changes in vectors of class\nprobabilities with a set of p observed predictors. Although various frequentist\nmethods have been developed for estimation, inference and classification within\nsuch a class of models, Bayesian inference is still lagging behind. This is due\nto the apparent absence of a tractable class of conjugate priors, that may\nfacilitate posterior inference on the multinomial probit coefficients. Such an\nissue has motivated increasing efforts toward the development of effective\nMarkov chain Monte Carlo methods, but state-of-the-art solutions still face\nsevere computational bottlenecks, especially in large p settings. In this\narticle, we prove that the entire class of unified skew-normal (SUN)\ndistributions is conjugate to a wide variety of multinomial probit models, and\nwe exploit the SUN properties to improve upon state-of-art-solutions for\nposterior inference and classification both in terms of closed-form results for\nkey functionals of interest, and also by developing novel computational methods\nrelying either on independent and identically distributed samples from the\nexact posterior or on scalable and accurate variational approximations based on\nblocked partially-factorized representations. As illustrated in a\ngastrointestinal lesions application, the magnitude of the improvements\nrelative to current methods is particularly evident, in practice, when the\nfocus is on large p applications.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 10:08:23 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Fasano", "Augusto", ""], ["Durante", "Daniele", ""]]}, {"id": "2007.06965", "submitter": "Zhiding Yu", "authors": "Wuyang Chen, Zhiding Yu, Zhangyang Wang, Anima Anandkumar", "title": "Automated Synthetic-to-Real Generalization", "comments": "Accepted to ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models trained on synthetic images often face degraded generalization to real\ndata. As a convention, these models are often initialized with ImageNet\npre-trained representation. Yet the role of ImageNet knowledge is seldom\ndiscussed despite common practices that leverage this knowledge to maintain the\ngeneralization ability. An example is the careful hand-tuning of early stopping\nand layer-wise learning rates, which is shown to improve synthetic-to-real\ngeneralization but is also laborious and heuristic. In this work, we explicitly\nencourage the synthetically trained model to maintain similar representations\nwith the ImageNet pre-trained model, and propose a \\textit{learning-to-optimize\n(L2O)} strategy to automate the selection of layer-wise learning rates. We\ndemonstrate that the proposed framework can significantly improve the\nsynthetic-to-real generalization performance without seeing and training on\nreal data, while also benefiting downstream tasks such as domain adaptation.\nCode is available at: https://github.com/NVlabs/ASG.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 10:57:34 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Chen", "Wuyang", ""], ["Yu", "Zhiding", ""], ["Wang", "Zhangyang", ""], ["Anandkumar", "Anima", ""]]}, {"id": "2007.06968", "submitter": "Tiangang Cui", "authors": "Tiangang Cui and Sergey Dolgov", "title": "Deep Composition of Tensor Trains using Squared Inverse Rosenblatt\n  Transports", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterising intractable high-dimensional random variables is one of the\nfundamental challenges in stochastic computation. The recent surge of transport\nmaps offers a mathematical foundation and new insights for tackling this\nchallenge by coupling intractable random variables with tractable reference\nrandom variables. This paper generalises a recently developed functional\ntensor-train (FTT) approximation of the inverse Rosenblatt transport [14] to a\nwide class of high-dimensional nonnegative functions, such as unnormalised\nprobability density functions. First, we extend the inverse Rosenblatt\ntransform to enable the transport to general reference measures other than the\nuniform measure. We develop an efficient procedure to compute this transport\nfrom a squared FTT decomposition which preserves the monotonicity. More\ncrucially, we integrate the proposed monotonicity-preserving FTT transport into\na nested variable transformation framework inspired by deep neural networks.\nThe resulting deep inverse Rosenblatt transport significantly expands the\ncapability of tensor approximations and transport maps to random variables with\ncomplicated nonlinear interactions and concentrated density functions. We\ndemonstrate the efficacy of the proposed approach on a range of applications in\nstatistical learning and uncertainty quantification, including parameter\nestimation for dynamical systems and inverse problems constrained by partial\ndifferential equations.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 11:04:18 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Cui", "Tiangang", ""], ["Dolgov", "Sergey", ""]]}, {"id": "2007.06985", "submitter": "Mathieu Garchery", "authors": "Mathieu Garchery and Michael Granitzer", "title": "ADSAGE: Anomaly Detection in Sequences of Attributed Graph Edges applied\n  to insider threat detection at fine-grained level", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous works on the CERT insider threat detection case have neglected graph\nand text features despite their relevance to describe user behavior.\nAdditionally, existing systems heavily rely on feature engineering and audit\ndata aggregation to detect malicious activities. This is time consuming,\nrequires expert knowledge and prevents tracing back alerts to precise user\nactions. To address these issues we introduce ADSAGE to detect anomalies in\naudit log events modeled as graph edges. Our general method is the first to\nperform anomaly detection at edge level while supporting both edge sequences\nand attributes, which can be numeric, categorical or even text. We describe how\nADSAGE can be used for fine-grained, event level insider threat detection in\ndifferent audit logs from the CERT use case. Remarking that there is no\nstandard benchmark for the CERT problem, we use a previously proposed\nevaluation setting based on realistic recall-based metrics. We evaluate ADSAGE\non authentication, email traffic and web browsing logs from the CERT insider\nthreat datasets, as well as on real-world authentication events. ADSAGE is\neffective to detect anomalies in authentications, modeled as user to computer\ninteractions, and in email communications. Simple baselines give surprisingly\nstrong results as well. We also report performance split by malicious scenarios\npresent in the CERT datasets: interestingly, several detectors are\ncomplementary and could be combined to improve detection. Overall, our results\nshow that graph features are informative to characterize malicious insider\nactivities, and that detection at fine-grained level is possible.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 12:05:05 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Garchery", "Mathieu", ""], ["Granitzer", "Michael", ""]]}, {"id": "2007.07001", "submitter": "Yigit Alparslan", "authors": "Ken Alparslan, Yigit Alparslan, Matthew Burlick", "title": "Adversarial Attacks against Neural Networks in Audio Domain: Exploiting\n  Principal Components", "comments": "8 pages, 14 figures, fixed typos, enumerated equations, fixed\n  equation (4) latex error, clarified author contributions via footnote", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Adversarial attacks are inputs that are similar to original inputs but\naltered on purpose. Speech-to-text neural networks that are widely used today\nare prone to misclassify adversarial attacks. In this study, first, we\ninvestigate the presence of targeted adversarial attacks by altering wave forms\nfrom Common Voice data set. We craft adversarial wave forms via Connectionist\nTemporal Classification Loss Function, and attack DeepSpeech, a speech-to-text\nneural network implemented by Mozilla. We achieve 100% adversarial success rate\n(zero successful classification by DeepSpeech) on all 25 adversarial wave forms\nthat we crafted. Second, we investigate the use of PCA as a defense mechanism\nagainst adversarial attacks. We reduce dimensionality by applying PCA to these\n25 attacks that we created and test them against DeepSpeech. We observe zero\nsuccessful classification by DeepSpeech, which suggests PCA is not a good\ndefense mechanism in audio domain. Finally, instead of using PCA as a defense\nmechanism, we use PCA this time to craft adversarial inputs under a black-box\nsetting with minimal adversarial knowledge. With no knowledge regarding the\nmodel, parameters, or weights, we craft adversarial attacks by applying PCA to\nsamples from Common Voice data set and achieve 100% adversarial success under\nblack-box setting again when tested against DeepSpeech. We also experiment with\ndifferent percentage of components necessary to result in a classification\nduring attacking process. In all cases, adversary becomes successful.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 12:35:03 GMT"}, {"version": "v2", "created": "Sun, 20 Sep 2020 21:43:06 GMT"}, {"version": "v3", "created": "Wed, 13 Jan 2021 16:28:22 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Alparslan", "Ken", ""], ["Alparslan", "Yigit", ""], ["Burlick", "Matthew", ""]]}, {"id": "2007.07011", "submitter": "Jorge A Mendez", "authors": "Jorge A. Mendez and Boyu Wang and Eric Eaton", "title": "Lifelong Policy Gradient Learning of Factored Policies for Faster\n  Training Without Forgetting", "comments": "To appear in Advances in Neural Information Processing Systems 33\n  (NeurIPS-20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy gradient methods have shown success in learning control policies for\nhigh-dimensional dynamical systems. Their biggest downside is the amount of\nexploration they require before yielding high-performing policies. In a\nlifelong learning setting, in which an agent is faced with multiple consecutive\ntasks over its lifetime, reusing information from previously seen tasks can\nsubstantially accelerate the learning of new tasks. We provide a novel method\nfor lifelong policy gradient learning that trains lifelong function\napproximators directly via policy gradients, allowing the agent to benefit from\naccumulated knowledge throughout the entire training process. We show\nempirically that our algorithm learns faster and converges to better policies\nthan single-task and lifelong learning baselines, and completely avoids\ncatastrophic forgetting on a variety of challenging domains.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 13:05:42 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 20:36:14 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Mendez", "Jorge A.", ""], ["Wang", "Boyu", ""], ["Eaton", "Eric", ""]]}, {"id": "2007.07029", "submitter": "Alexander Becker", "authors": "Vladimir Golkov, Alexander Becker, Daniel T. Plop, Daniel\n  \\v{C}uturilo, Neda Davoudi, Jeffrey Mendenhall, Rocco Moretti, Jens Meiler,\n  Daniel Cremers", "title": "Deep Learning for Virtual Screening: Five Reasons to Use ROC Cost\n  Functions", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-aided drug discovery is an essential component of modern drug\ndevelopment. Therein, deep learning has become an important tool for rapid\nscreening of billions of molecules in silico for potential hits containing\ndesired chemical features. Despite its importance, substantial challenges\npersist in training these models, such as severe class imbalance, high decision\nthresholds, and lack of ground truth labels in some datasets. In this work we\nargue in favor of directly optimizing the receiver operating characteristic\n(ROC) in such cases, due to its robustness to class imbalance, its ability to\ncompromise over different decision thresholds, certain freedom to influence the\nrelative weights in this compromise, fidelity to typical benchmarking measures,\nand equivalence to positive/unlabeled learning. We also propose new training\nschemes (coherent mini-batch arrangement, and usage of out-of-batch samples)\nfor cost functions based on the ROC, as well as a cost function based on the\nlogAUC metric that facilitates early enrichment (i.e. improves performance at\nhigh decision thresholds, as often desired when synthesizing predicted hit\ncompounds). We demonstrate that these approaches outperform standard deep\nlearning approaches on a series of PubChem high-throughput screening datasets\nthat represent realistic and diverse drug discovery campaigns on major drug\ntarget families.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 08:46:37 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Golkov", "Vladimir", ""], ["Becker", "Alexander", ""], ["Plop", "Daniel T.", ""], ["\u010cuturilo", "Daniel", ""], ["Davoudi", "Neda", ""], ["Mendenhall", "Jeffrey", ""], ["Moretti", "Rocco", ""], ["Meiler", "Jens", ""], ["Cremers", "Daniel", ""]]}, {"id": "2007.07056", "submitter": "Yichen Jia", "authors": "Yichen Jia and Jong-Hyeon Jeong", "title": "Deep Learning for Quantile Regression under Right Censoring:\n  DeepQuantreg", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational prediction algorithm of neural network, or deep learning,\nhas drawn much attention recently in statistics as well as in image recognition\nand natural language processing. Particularly in statistical application for\ncensored survival data, the loss function used for optimization has been mainly\nbased on the partial likelihood from Cox's model and its variations to utilize\nexisting neural network library such as Keras, which was built upon the open\nsource library of TensorFlow. This paper presents a novel application of the\nneural network to the quantile regression for survival data with right\ncensoring, which is adjusted by the inverse of the estimated censoring\ndistribution in the check function. The main purpose of this work is to show\nthat the deep learning method could be flexible enough to predict nonlinear\npatterns more accurately compared to existing quantile regression methods such\nas traditional linear quantile regression and nonparametric quantile regression\nwith total variation regularization, emphasizing practicality of the method for\ncensored survival data. Simulation studies were performed to generate nonlinear\ncensored survival data and compare the deep learning method with existing\nquantile regression methods in terms of prediction accuracy. The proposed\nmethod is illustrated with two publicly available breast cancer data sets with\ngene signatures. The method has been built into a package and is freely\navailable at \\url{https://github.com/yicjia/DeepQuantreg}.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 14:31:11 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 03:35:52 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Jia", "Yichen", ""], ["Jeong", "Jong-Hyeon", ""]]}, {"id": "2007.07081", "submitter": "Ilia Kravets", "authors": "Ilia Kravets, Tal Heletz, Hayit Greenspan", "title": "Nodule2vec: a 3D Deep Learning System for Pulmonary Nodule Retrieval\n  Using Semantic Representation", "comments": "to appear at MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-based retrieval supports a radiologist decision making process by\npresenting the doctor the most similar cases from the database containing both\nhistorical diagnosis and further disease development history. We present a deep\nlearning system that transforms a 3D image of a pulmonary nodule from a CT scan\ninto a low-dimensional embedding vector. We demonstrate that such a vector\nrepresentation preserves semantic information about the nodule and offers a\nviable approach for content-based image retrieval (CBIR). We discuss the\ntheoretical limitations of the available datasets and overcome them by applying\ntransfer learning of the state-of-the-art lung nodule detection model. We\nevaluate the system using the LIDC-IDRI dataset of thoracic CT scans. We devise\na similarity score and show that it can be utilized to measure similarity 1)\nbetween annotations of the same nodule by different radiologists and 2) between\nthe query nodule and the top four CBIR results. A comparison between doctors\nand algorithm scores suggests that the benefit provided by the system to the\nradiologist end-user is comparable to obtaining a second radiologist's opinion.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 16:26:16 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Kravets", "Ilia", ""], ["Heletz", "Tal", ""], ["Greenspan", "Hayit", ""]]}, {"id": "2007.07084", "submitter": "Shihao Li", "authors": "Shihao Li (1), Dekun Yang (1), Bufeng Zhang (1) ((1) Alibaba Inc)", "title": "MRIF: Multi-resolution Interest Fusion for Recommendation", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main task of personalized recommendation is capturing users' interests\nbased on their historical behaviors. Most of recent advances in recommender\nsystems mainly focus on modeling users' preferences accurately using deep\nlearning based approaches. There are two important properties of users'\ninterests, one is that users' interests are dynamic and evolve over time, the\nother is that users' interests have different resolutions, or temporal-ranges\nto be precise, such as long-term and short-term preferences. Existing\napproaches either use Recurrent Neural Networks (RNNs) to address the drifts in\nusers' interests without considering different temporal-ranges, or design two\ndifferent networks to model long-term and short-term preferences separately.\nThis paper presents a multi-resolution interest fusion model (MRIF) that takes\nboth properties of users' interests into consideration. The proposed model is\ncapable to capture the dynamic changes in users' interests at different\ntemporal-ranges, and provides an effective way to combine a group of\nmulti-resolution user interests to make predictions. Experiments show that our\nmethod outperforms state-of-the-art recommendation methods consistently.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 02:32:15 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Li", "Shihao", "", "Alibaba Inc"], ["Yang", "Dekun", "", "Alibaba Inc"], ["Zhang", "Bufeng", "", "Alibaba Inc"]]}, {"id": "2007.07085", "submitter": "Wenhui Yu", "authors": "Wenhui Yu and Xiao Lin and Junfeng Ge and Wenwu Ou and Zheng Qin", "title": "Semi-supervised Collaborative Filtering by Text-enhanced Domain\n  Adaptation", "comments": "KDD 2020 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data sparsity is an inherent challenge in the recommender systems, where most\nof the data is collected from the implicit feedbacks of users. This causes two\ndifficulties in designing effective algorithms: first, the majority of users\nonly have a few interactions with the system and there is no enough data for\nlearning; second, there are no negative samples in the implicit feedbacks and\nit is a common practice to perform negative sampling to generate negative\nsamples. However, this leads to a consequence that many potential positive\nsamples are mislabeled as negative ones and data sparsity would exacerbate the\nmislabeling problem. To solve these difficulties, we regard the problem of\nrecommendation on sparse implicit feedbacks as a semi-supervised learning task,\nand explore domain adaption to solve it. We transfer the knowledge learned from\ndense data to sparse data and we focus on the most challenging case -- there is\nno user or item overlap. In this extreme case, aligning embeddings of two\ndatasets directly is rather sub-optimal since the two latent spaces encode very\ndifferent information. As such, we adopt domain-invariant textual features as\nthe anchor points to align the latent spaces. To align the embeddings, we\nextract the textual features for each user and item and feed them into a domain\nclassifier with the embeddings of users and items. The embeddings are trained\nto puzzle the classifier and textual features are fixed as anchor points. By\ndomain adaptation, the distribution pattern in the source domain is transferred\nto the target domain. As the target part can be supervised by domain\nadaptation, we abandon negative sampling in target dataset to avoid label\nnoise. We adopt three pairs of real-world datasets to validate the\neffectiveness of our transfer strategy. Results show that our models outperform\nexisting models significantly.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 05:28:05 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Yu", "Wenhui", ""], ["Lin", "Xiao", ""], ["Ge", "Junfeng", ""], ["Ou", "Wenwu", ""], ["Qin", "Zheng", ""]]}, {"id": "2007.07090", "submitter": "Christoph Raab", "authors": "Christoph Raab and Frank-Michael Schleif", "title": "Transfer learning extensions for the probabilistic classification vector\n  machine", "comments": "arXiv admin note: text overlap with arXiv:1907.01343", "journal-ref": "Neurocomputing, Volume 397, 2020, Pages 320-330, ISSN 0925-2312", "doi": "10.1016/j.neucom.2019.09.104", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Transfer learning is focused on the reuse of supervised learning models in a\nnew context. Prominent applications can be found in robotics, image processing\nor web mining. In these fields, the learning scenarios are naturally changing\nbut often remain related to each other motivating the reuse of existing\nsupervised models. Current transfer learning models are neither sparse nor\ninterpretable. Sparsity is very desirable if the methods have to be used in\ntechnically limited environments and interpretability is getting more critical\ndue to privacy regulations. In this work, we propose two transfer learning\nextensions integrated into the sparse and interpretable probabilistic\nclassification vector machine. They are compared to standard benchmarks in the\nfield and show their relevance either by sparsity or performance improvements.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 08:35:10 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Raab", "Christoph", ""], ["Schleif", "Frank-Michael", ""]]}, {"id": "2007.07105", "submitter": "Samuel Cohen", "authors": "Samuel Cohen, Michael Arbel, Marc Peter Deisenroth", "title": "Estimating Barycenters of Measures in High Dimensions", "comments": "In submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Barycentric averaging is a principled way of summarizing populations of\nmeasures. Existing algorithms for estimating barycenters typically parametrize\nthem as weighted sums of Diracs and optimize their weights and/or locations.\nHowever, these approaches do not scale to high-dimensional settings due to the\ncurse of dimensionality. In this paper, we propose a scalable and general\nalgorithm for estimating barycenters of measures in high dimensions. The key\nidea is to turn the optimization over measures into an optimization over\ngenerative models, introducing inductive biases that allow the method to scale\nwhile still accurately estimating barycenters. We prove local convergence under\nmild assumptions on the discrepancy showing that the approach is well-posed. We\ndemonstrate that our method is fast, achieves good performance on\nlow-dimensional problems, and scales to high-dimensional settings. In\nparticular, our approach is the first to be used to estimate barycenters in\nthousands of dimensions.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 15:24:41 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2021 09:00:46 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Cohen", "Samuel", ""], ["Arbel", "Michael", ""], ["Deisenroth", "Marc Peter", ""]]}, {"id": "2007.07124", "submitter": "Yaniv Yacoby", "authors": "Yaniv Yacoby, Weiwei Pan, Finale Doshi-Velez", "title": "Failure Modes of Variational Autoencoders and Their Effects on\n  Downstream Tasks", "comments": "Accepted at the International Conference on Machine Learning (ICML)\n  Workshop on Uncertainty and Robustness in Deep Learning (UDL) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Auto-encoders (VAEs) are deep generative latent variable models\nthat are widely used for a number of downstream tasks. While it has been\ndemonstrated that VAE training can suffer from a number of pathologies,\nexisting literature lacks characterizations of exactly when these pathologies\noccur and how they impact down-stream task performance. In this paper we\nconcretely characterize conditions under which VAE training exhibits\npathologies and connect these failure modes to undesirable effects on specific\ndownstream tasks, such as learning compressed and disentangled representations,\nadversarial robustness and semi-supervised learning.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 15:44:18 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 01:14:10 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2021 23:59:19 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Yacoby", "Yaniv", ""], ["Pan", "Weiwei", ""], ["Doshi-Velez", "Finale", ""]]}, {"id": "2007.07127", "submitter": "Sam Witty", "authors": "Sam Witty, Kenta Takatsu, David Jensen, Vikash Mansinghka", "title": "Causal Inference using Gaussian Processes with Structured Latent\n  Confounders", "comments": "to be published at ICML2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Latent confounders---unobserved variables that influence both treatment and\noutcome---can bias estimates of causal effects. In some cases, these\nconfounders are shared across observations, e.g. all students taking a course\nare influenced by the course's difficulty in addition to any educational\ninterventions they receive individually. This paper shows how to\nsemiparametrically model latent confounders that have this structure and\nthereby improve estimates of causal effects. The key innovations are a\nhierarchical Bayesian model, Gaussian processes with structured latent\nconfounders (GP-SLC), and a Monte Carlo inference algorithm for this model\nbased on elliptical slice sampling. GP-SLC provides principled Bayesian\nuncertainty estimates of individual treatment effect with minimal assumptions\nabout the functional forms relating confounders, covariates, treatment, and\noutcome. Finally, this paper shows GP-SLC is competitive with or more accurate\nthan widely used causal inference techniques on three benchmark datasets,\nincluding the Infant Health and Development Program and a dataset showing the\neffect of changing temperatures on state-wide energy consumption across New\nEngland.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 15:45:28 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Witty", "Sam", ""], ["Takatsu", "Kenta", ""], ["Jensen", "David", ""], ["Mansinghka", "Vikash", ""]]}, {"id": "2007.07142", "submitter": "Andres Felipe Duque Correa", "authors": "Andr\\'es F. Duque, Sacha Morin, Guy Wolf, Kevin R. Moon", "title": "Extendable and invertible manifold learning with geometry regularized\n  autoencoders", "comments": "10 pages, 6 figures", "journal-ref": "IEEE International Conference on Big Data, pp. 5027-5036, Dec.\n  2020", "doi": "10.1109/BigData50022.2020.9378049", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental task in data exploration is to extract simplified low\ndimensional representations that capture intrinsic geometry in data, especially\nfor faithfully visualizing data in two or three dimensions. Common approaches\nto this task use kernel methods for manifold learning. However, these methods\ntypically only provide an embedding of fixed input data and cannot extend to\nnew data points. Autoencoders have also recently become popular for\nrepresentation learning. But while they naturally compute feature extractors\nthat are both extendable to new data and invertible (i.e., reconstructing\noriginal features from latent representation), they have limited capabilities\nto follow global intrinsic geometry compared to kernel-based manifold learning.\nWe present a new method for integrating both approaches by incorporating a\ngeometric regularization term in the bottleneck of the autoencoder. Our\nregularization, based on the diffusion potential distances from the\nrecently-proposed PHATE visualization method, encourages the learned latent\nrepresentation to follow intrinsic data geometry, similar to manifold learning\nalgorithms, while still enabling faithful extension to new data and\nreconstruction of data in the original feature space from latent coordinates.\nWe compare our approach with leading kernel methods and autoencoder models for\nmanifold learning to provide qualitative and quantitative evidence of our\nadvantages in preserving intrinsic structure, out of sample extension, and\nreconstruction. Our method is easily implemented for big-data applications,\nwhereas other methods are limited in this regard.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 15:59:10 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2020 23:24:04 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Duque", "Andr\u00e9s F.", ""], ["Morin", "Sacha", ""], ["Wolf", "Guy", ""], ["Moon", "Kevin R.", ""]]}, {"id": "2007.07151", "submitter": "Kundan Krishna", "authors": "Kundan Krishna, Amy Pavel, Benjamin Schloss, Jeffrey P. Bigham,\n  Zachary C. Lipton", "title": "Extracting Structured Data from Physician-Patient Conversations By\n  Predicting Noteworthy Utterances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite diverse efforts to mine various modalities of medical data, the\nconversations between physicians and patients at the time of care remain an\nuntapped source of insights. In this paper, we leverage this data to extract\nstructured information that might assist physicians with post-visit\ndocumentation in electronic health records, potentially lightening the clerical\nburden. In this exploratory study, we describe a new dataset consisting of\nconversation transcripts, post-visit summaries, corresponding supporting\nevidence (in the transcript), and structured labels. We focus on the tasks of\nrecognizing relevant diagnoses and abnormalities in the review of organ systems\n(RoS). One methodological challenge is that the conversations are long (around\n1500 words), making it difficult for modern deep-learning models to use them as\ninput. To address this challenge, we extract noteworthy utterances---parts of\nthe conversation likely to be cited as evidence supporting some summary\nsentence. We find that by first filtering for (predicted) noteworthy\nutterances, we can significantly boost predictive performance for recognizing\nboth diagnoses and RoS abnormalities.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 16:10:37 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Krishna", "Kundan", ""], ["Pavel", "Amy", ""], ["Schloss", "Benjamin", ""], ["Bigham", "Jeffrey P.", ""], ["Lipton", "Zachary C.", ""]]}, {"id": "2007.07170", "submitter": "Suraj Nair", "authors": "Suraj Nair, Silvio Savarese, Chelsea Finn", "title": "Goal-Aware Prediction: Learning to Model What Matters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learned dynamics models combined with both planning and policy learning\nalgorithms have shown promise in enabling artificial agents to learn to perform\nmany diverse tasks with limited supervision. However, one of the fundamental\nchallenges in using a learned forward dynamics model is the mismatch between\nthe objective of the learned model (future state reconstruction), and that of\nthe downstream planner or policy (completing a specified task). This issue is\nexacerbated by vision-based control tasks in diverse real-world environments,\nwhere the complexity of the real world dwarfs model capacity. In this paper, we\npropose to direct prediction towards task relevant information, enabling the\nmodel to be aware of the current task and encouraging it to only model relevant\nquantities of the state space, resulting in a learning objective that more\nclosely matches the downstream task. Further, we do so in an entirely\nself-supervised manner, without the need for a reward function or image labels.\nWe find that our method more effectively models the relevant parts of the scene\nconditioned on the goal, and as a result outperforms standard task-agnostic\ndynamics models and model-free reinforcement learning.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 16:42:59 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 23:15:15 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Nair", "Suraj", ""], ["Savarese", "Silvio", ""], ["Finn", "Chelsea", ""]]}, {"id": "2007.07172", "submitter": "Alireza Abedin", "authors": "Alireza Abedin, Mahsa Ehsanpour, Qinfeng Shi, Hamid Rezatofighi,\n  Damith C. Ranasinghe", "title": "Attend And Discriminate: Beyond the State-of-the-Art for Human Activity\n  Recognition using Wearable Sensors", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wearables are fundamental to improving our understanding of human activities,\nespecially for an increasing number of healthcare applications from\nrehabilitation to fine-grained gait analysis. Although our collective know-how\nto solve Human Activity Recognition (HAR) problems with wearables has\nprogressed immensely with end-to-end deep learning paradigms, several\nfundamental opportunities remain overlooked. We rigorously explore these new\nopportunities to learn enriched and highly discriminating activity\nrepresentations. We propose: i) learning to exploit the latent relationships\nbetween multi-channel sensor modalities and specific activities; ii)\ninvestigating the effectiveness of data-agnostic augmentation for multi-modal\nsensor data streams to regularize deep HAR models; and iii) incorporating a\nclassification loss criterion to encourage minimal intra-class representation\ndifferences whilst maximising inter-class differences to achieve more\ndiscriminative features. Our contributions achieves new state-of-the-art\nperformance on four diverse activity recognition problem benchmarks with large\nmargins -- with up to 6% relative margin improvement. We extensively validate\nthe contributions from our design concepts through extensive experiments,\nincluding activity misalignment measures, ablation studies and insights shared\nthrough both quantitative and qualitative studies.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 16:44:16 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Abedin", "Alireza", ""], ["Ehsanpour", "Mahsa", ""], ["Shi", "Qinfeng", ""], ["Rezatofighi", "Hamid", ""], ["Ranasinghe", "Damith C.", ""]]}, {"id": "2007.07176", "submitter": "Kai Liang Tan", "authors": "Kai Liang Tan, Yasaman Esfandiari, Xian Yeow Lee, Aakanksha, Soumik\n  Sarkar", "title": "Robustifying Reinforcement Learning Agents via Action Space Adversarial\n  Training", "comments": "Accepted for publication in American Control Conference 2020, 6 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adoption of machine learning (ML)-enabled cyber-physical systems (CPS) are\nbecoming prevalent in various sectors of modern society such as transportation,\nindustrial, and power grids. Recent studies in deep reinforcement learning\n(DRL) have demonstrated its benefits in a large variety of data-driven\ndecisions and control applications. As reliance on ML-enabled systems grows, it\nis imperative to study the performance of these systems under malicious state\nand actuator attacks. Traditional control systems employ\nresilient/fault-tolerant controllers that counter these attacks by correcting\nthe system via error observations. However, in some applications, a resilient\ncontroller may not be sufficient to avoid a catastrophic failure. Ideally, a\nrobust approach is more useful in these scenarios where a system is inherently\nrobust (by design) to adversarial attacks. While robust control has a long\nhistory of development, robust ML is an emerging research area that has already\ndemonstrated its relevance and urgency. However, the majority of robust ML\nresearch has focused on perception tasks and not on decision and control tasks,\nalthough the ML (specifically RL) models used for control applications are\nequally vulnerable to adversarial attacks. In this paper, we show that a\nwell-performing DRL agent that is initially susceptible to action space\nperturbations (e.g. actuator attacks) can be robustified against similar\nperturbations through adversarial training.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 16:50:02 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Tan", "Kai Liang", ""], ["Esfandiari", "Yasaman", ""], ["Lee", "Xian Yeow", ""], ["Aakanksha", "", ""], ["Sarkar", "Soumik", ""]]}, {"id": "2007.07177", "submitter": "Mark Hamilton", "authors": "Mark Hamilton, Stephanie Fu, Mindren Lu, Johnny Bui, Darius Bopp,\n  Zhenbang Chen, Felix Tran, Margaret Wang, Marina Rogers, Lei Zhang, Chris\n  Hoder, William T. Freeman", "title": "MosAIc: Finding Artistic Connections across Culture with Conditional\n  Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce MosAIc, an interactive web app that allows users to find pairs\nof semantically related artworks that span different cultures, media, and\nmillennia. To create this application, we introduce Conditional Image Retrieval\n(CIR) which combines visual similarity search with user supplied filters or\n\"conditions\". This technique allows one to find pairs of similar images that\nspan distinct subsets of the image corpus. We provide a generic way to adapt\nexisting image retrieval data-structures to this new domain and provide\ntheoretical bounds on our approach's efficiency. To quantify the performance of\nCIR systems, we introduce new datasets for evaluating CIR methods and show that\nCIR performs non-parametric style transfer. Finally, we demonstrate that our\nCIR data-structures can identify \"blind spots\" in Generative Adversarial\nNetworks (GAN) where they fail to properly model the true data distribution.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 16:50:29 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 18:25:23 GMT"}, {"version": "v3", "created": "Sun, 28 Feb 2021 01:08:22 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Hamilton", "Mark", ""], ["Fu", "Stephanie", ""], ["Lu", "Mindren", ""], ["Bui", "Johnny", ""], ["Bopp", "Darius", ""], ["Chen", "Zhenbang", ""], ["Tran", "Felix", ""], ["Wang", "Margaret", ""], ["Rogers", "Marina", ""], ["Zhang", "Lei", ""], ["Hoder", "Chris", ""], ["Freeman", "William T.", ""]]}, {"id": "2007.07181", "submitter": "Harshit Sharma", "authors": "Harshit Sharma, Harsh K. Gandhi, Apoorv Jain", "title": "Towards Credit-Fraud Detection via Sparsely Varying Gaussian\n  Approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fraudulent activities are an expensive problem for many financial\ninstitutions, costing billions of dollars to corporations annually. More\ncommonly occurring activities in this regard are credit card frauds. In this\ncontext, the credit card fraud detection concept has been developed over the\nlines of incorporating the uncertainty in our prediction system to ensure\nbetter judgment in such a crucial task. We propose to use a sparse Gaussian\nclassification method to work with the large data-set and use the concept of\npseudo or inducing inputs. We perform the same with different sets of kernels\nand the different number of inducing data points to show the best accuracy was\nobtained with the selection of RBF kernel with a higher number of inducing\npoints. Our approach was able to work over large financial data given the\nstochastic nature of our method employed and also good test accuracy with low\nvariance over the prediction suggesting confidence and robustness in our model.\nUsing the methodologies of Bayesian learning techniques with the incorporated\ninducing points phenomenon, are successfully able to obtain a healthy accuracy\nand a high confidence score.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 16:56:06 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Sharma", "Harshit", ""], ["Gandhi", "Harsh K.", ""], ["Jain", "Apoorv", ""]]}, {"id": "2007.07183", "submitter": "Tineke Blom", "authors": "Tineke Blom and Mirthe M. van Diepen and Joris M. Mooij", "title": "Conditional independences and causal relations implied by sets of\n  equations", "comments": "60 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world complex systems are often modelled by sets of equations with\nendogenous and exogenous variables. What can we say about the causal and\nprobabilistic aspects of variables that appear in these equations without\nexplicitly solving the equations? We make use of Simon's causal ordering\nalgorithm (Simon, 1953) to construct a causal ordering graph and prove that it\nexpresses the effects of soft and perfect interventions on the equations under\ncertain unique solvability assumptions. We further construct a Markov ordering\ngraph and prove that it encodes conditional independences in the distribution\nimplied by the equations with independent random exogenous variables, under a\nsimilar unique solvability assumption. We discuss how this approach reveals and\naddresses some of the limitations of existing causal modelling frameworks, such\nas causal Bayesian networks and structural causal models.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 17:00:28 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 17:37:16 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Blom", "Tineke", ""], ["van Diepen", "Mirthe M.", ""], ["Mooij", "Joris M.", ""]]}, {"id": "2007.07195", "submitter": "Hao Liu", "authors": "Hao Liu, Ying Li, Yanjie Fu, Huaibo Mei, Jingbo Zhou, Xu Ma, Hui Xiong", "title": "Polestar: An Intelligent, Efficient and National-Wide Public\n  Transportation Routing Engine", "comments": null, "journal-ref": "KDD 2020 applied data science track", "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Public transportation plays a critical role in people's daily life. It has\nbeen proven that public transportation is more environmentally sustainable,\nefficient, and economical than any other forms of travel. However, due to the\nincreasing expansion of transportation networks and more complex travel\nsituations, people are having difficulties in efficiently finding the most\npreferred route from one place to another through public transportation\nsystems. To this end, in this paper, we present Polestar, a data-driven engine\nfor intelligent and efficient public transportation routing. Specifically, we\nfirst propose a novel Public Transportation Graph (PTG) to model public\ntransportation system in terms of various travel costs, such as time or\ndistance. Then, we introduce a general route search algorithm coupled with an\nefficient station binding method for efficient route candidate generation.\nAfter that, we propose a two-pass route candidate ranking module to capture\nuser preferences under dynamic travel situations. Finally, experiments on two\nreal-world data sets demonstrate the advantages of Polestar in terms of both\nefficiency and effectiveness. Indeed, in early 2019, Polestar has been deployed\non Baidu Maps, one of the world's largest map services. To date, Polestar is\nservicing over 330 cities, answers over a hundred millions of queries each day,\nand achieves substantial improvement of user click ratio.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 05:14:52 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Liu", "Hao", ""], ["Li", "Ying", ""], ["Fu", "Yanjie", ""], ["Mei", "Huaibo", ""], ["Zhou", "Jingbo", ""], ["Ma", "Xu", ""], ["Xiong", "Hui", ""]]}, {"id": "2007.07203", "submitter": "Weihao Gao", "authors": "Weihao Gao, Xiangjun Fan, Chong Wang, Jiankai Sun, Kai Jia, Wenzhi\n  Xiao, Ruofan Ding, Xingyan Bin, Hui Yang, Xiaobing Liu", "title": "Deep Retrieval: Learning A Retrievable Structure for Large-Scale\n  Recommendations", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the core problems in large-scale recommendations is to retrieve top\nrelevant candidates accurately and efficiently, preferably in sub-linear time.\nPrevious approaches are mostly based on a two-step procedure: first learn an\ninner-product model, and then use some approximate nearest neighbor (ANN)\nsearch algorithm to find top candidates. In this paper, we present Deep\nRetrieval (DR), to learn a retrievable structure directly with user-item\ninteraction data (e.g. clicks) without resorting to the Euclidean space\nassumption in ANN algorithms. DR's structure encodes all candidate items into a\ndiscrete latent space. Those latent codes for the candidates are model\nparameters and learnt together with other neural network parameters to maximize\nthe same objective function. With the model learnt, a beam search over the\nstructure is performed to retrieve the top candidates for reranking.\nEmpirically, we first demonstrate that DR, with sub-linear computational\ncomplexity, can achieve almost the same accuracy as the brute-force baseline on\ntwo public datasets. Moreover, we show that, in a live production\nrecommendation system, a deployed DR approach significantly outperforms a\nwell-tuned ANN baseline in terms of engagement metrics. To the best of our\nknowledge, DR is among the first non-ANN algorithms successfully deployed at\nthe scale of hundreds of millions of items for industrial recommendation\nsystems.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 06:23:51 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 05:45:30 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Gao", "Weihao", ""], ["Fan", "Xiangjun", ""], ["Wang", "Chong", ""], ["Sun", "Jiankai", ""], ["Jia", "Kai", ""], ["Xiao", "Wenzhi", ""], ["Ding", "Ruofan", ""], ["Bin", "Xingyan", ""], ["Yang", "Hui", ""], ["Liu", "Xiaobing", ""]]}, {"id": "2007.07204", "submitter": "Wenhui Yu", "authors": "Wenhui Yu and Zheng Qin", "title": "Sampler Design for Implicit Feedback Data by Noisy-label Robust Learning", "comments": "SIGIR 2020 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit feedback data is extensively explored in recommendation as it is\neasy to collect and generally applicable. However, predicting users' preference\non implicit feedback data is a challenging task since we can only observe\npositive (voted) samples and unvoted samples. It is difficult to distinguish\nbetween the negative samples and unlabeled positive samples from the unvoted\nones. Existing works, such as Bayesian Personalized Ranking (BPR), sample\nunvoted items as negative samples uniformly, therefore suffer from a critical\nnoisy-label issue. To address this gap, we design an adaptive sampler based on\nnoisy-label robust learning for implicit feedback data.\n  To formulate the issue, we first introduce Bayesian Point-wise Optimization\n(BPO) to learn a model, e.g., Matrix Factorization (MF), by maximum likelihood\nestimation. We predict users' preferences with the model and learn it by\nmaximizing likelihood of observed data labels, i.e., a user prefers her\npositive samples and has no interests in her unvoted samples. However, in\nreality, a user may have interests in some of her unvoted samples, which are\nindeed positive samples mislabeled as negative ones. We then consider the risk\nof these noisy labels, and propose a Noisy-label Robust BPO (NBPO). NBPO also\nmaximizes the observation likelihood while connects users' preference and\nobserved labels by the likelihood of label flipping based on the Bayes'\ntheorem. In NBPO, a user prefers her true positive samples and shows no\ninterests in her true negative samples, hence the optimization quality is\ndramatically improved. Extensive experiments on two public real-world datasets\nshow the significant improvement of our proposed optimization methods.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 05:31:53 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Yu", "Wenhui", ""], ["Qin", "Zheng", ""]]}, {"id": "2007.07205", "submitter": "Ivan Evtimov", "authors": "Ivan Evtimov, Weidong Cui, Ece Kamar, Emre Kiciman, Tadayoshi Kohno,\n  Jerry Li", "title": "Security and Machine Learning in the Real World", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) models deployed in many safety- and business-critical\nsystems are vulnerable to exploitation through adversarial examples. A large\nbody of academic research has thoroughly explored the causes of these blind\nspots, developed sophisticated algorithms for finding them, and proposed a few\npromising defenses. A vast majority of these works, however, study standalone\nneural network models. In this work, we build on our experience evaluating the\nsecurity of a machine learning software product deployed on a large scale to\nbroaden the conversation to include a systems security view of these\nvulnerabilities. We describe novel challenges to implementing systems security\nbest practices in software with ML components. In addition, we propose a list\nof short-term mitigation suggestions that practitioners deploying machine\nlearning modules can use to secure their systems. Finally, we outline\ndirections for new research into machine learning attacks and defenses that can\nserve to advance the state of ML systems security.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 16:57:12 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Evtimov", "Ivan", ""], ["Cui", "Weidong", ""], ["Kamar", "Ece", ""], ["Kiciman", "Emre", ""], ["Kohno", "Tadayoshi", ""], ["Li", "Jerry", ""]]}, {"id": "2007.07206", "submitter": "Amy Zhang", "authors": "Amy Zhang, Shagun Sodhani, Khimya Khetarpal, Joelle Pineau", "title": "Learning Robust State Abstractions for Hidden-Parameter Block MDPs", "comments": "Accepted at the 9th International Conference on Learning\n  Representations. 22 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many control tasks exhibit similar dynamics that can be modeled as having\ncommon latent structure. Hidden-Parameter Markov Decision Processes (HiP-MDPs)\nexplicitly model this structure to improve sample efficiency in multi-task\nsettings. However, this setting makes strong assumptions on the observability\nof the state that limit its application in real-world scenarios with rich\nobservation spaces. In this work, we leverage ideas of common structure from\nthe HiP-MDP setting, and extend it to enable robust state abstractions inspired\nby Block MDPs. We derive instantiations of this new framework for both\nmulti-task reinforcement learning (MTRL) and meta-reinforcement learning\n(Meta-RL) settings. Further, we provide transfer and generalization bounds\nbased on task and state similarity, along with sample complexity bounds that\ndepend on the aggregate number of samples across tasks, rather than the number\nof tasks, a significant improvement over prior work that use the same\nenvironment assumptions. To further demonstrate the efficacy of the proposed\nmethod, we empirically compare and show improvement over multi-task and\nmeta-reinforcement learning baselines.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 17:25:27 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 14:37:00 GMT"}, {"version": "v3", "created": "Thu, 14 Jan 2021 18:08:05 GMT"}, {"version": "v4", "created": "Fri, 12 Feb 2021 04:40:14 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Zhang", "Amy", ""], ["Sodhani", "Shagun", ""], ["Khetarpal", "Khimya", ""], ["Pineau", "Joelle", ""]]}, {"id": "2007.07210", "submitter": "Satya Narayan Shukla", "authors": "Satya Narayan Shukla, Anit Kumar Sahu, Devin Willmott, J. Zico Kolter", "title": "Simple and Efficient Hard Label Black-box Adversarial Attacks in Low\n  Query Budget Regimes", "comments": "Accepted at KDD 2021. arXiv admin note: substantial text overlap with\n  arXiv:1909.13857", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the problem of black-box adversarial attacks, where the aim is to\ngenerate adversarial examples for deep learning models solely based on\ninformation limited to output label~(hard label) to a queried data input. We\npropose a simple and efficient Bayesian Optimization~(BO) based approach for\ndeveloping black-box adversarial attacks. Issues with BO's performance in high\ndimensions are avoided by searching for adversarial examples in a structured\nlow-dimensional subspace. We demonstrate the efficacy of our proposed attack\nmethod by evaluating both $\\ell_\\infty$ and $\\ell_2$ norm constrained\nuntargeted and targeted hard label black-box attacks on three standard datasets\n- MNIST, CIFAR-10 and ImageNet. Our proposed approach consistently achieves 2x\nto 10x higher attack success rate while requiring 10x to 20x fewer queries\ncompared to the current state-of-the-art black-box adversarial attacks.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 04:34:57 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 17:31:02 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Shukla", "Satya Narayan", ""], ["Sahu", "Anit Kumar", ""], ["Willmott", "Devin", ""], ["Kolter", "J. Zico", ""]]}, {"id": "2007.07213", "submitter": "Yeonjong Shin", "authors": "Mark Ainsworth and Yeonjong Shin", "title": "Plateau Phenomenon in Gradient Descent Training of ReLU networks:\n  Explanation, Quantification and Avoidance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The ability of neural networks to provide `best in class' approximation\nacross a wide range of applications is well-documented. Nevertheless, the\npowerful expressivity of neural networks comes to naught if one is unable to\neffectively train (choose) the parameters defining the network. In general,\nneural networks are trained by gradient descent type optimization methods, or a\nstochastic variant thereof. In practice, such methods result in the loss\nfunction decreases rapidly at the beginning of training but then, after a\nrelatively small number of steps, significantly slow down. The loss may even\nappear to stagnate over the period of a large number of epochs, only to then\nsuddenly start to decrease fast again for no apparent reason. This so-called\nplateau phenomenon manifests itself in many learning tasks.\n  The present work aims to identify and quantify the root causes of plateau\nphenomenon. No assumptions are made on the number of neurons relative to the\nnumber of training data, and our results hold for both the lazy and adaptive\nregimes. The main findings are: plateaux correspond to periods during which\nactivation patterns remain constant, where activation pattern refers to the\nnumber of data points that activate a given neuron; quantification of\nconvergence of the gradient flow dynamics; and, characterization of stationary\npoints in terms solutions of local least squares regression lines over subsets\nof the training data. Based on these conclusions, we propose a new iterative\ntraining method, the Active Neuron Least Squares (ANLS), characterised by the\nexplicit adjustment of the activation pattern at each step, which is designed\nto enable a quick exit from a plateau. Illustrative numerical examples are\nincluded throughout.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 17:33:26 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Ainsworth", "Mark", ""], ["Shin", "Yeonjong", ""]]}, {"id": "2007.07224", "submitter": "Ting-Hsiang Wang", "authors": "Ting-Hsiang Wang, Qingquan Song, Xiaotian Han, Zirui Liu, Haifeng Jin,\n  Xia Hu", "title": "AutoRec: An Automated Recommender System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Realistic recommender systems are often required to adapt to ever-changing\ndata and tasks or to explore different models systematically. To address the\nneed, we present AutoRec, an open-source automated machine learning (AutoML)\nplatform extended from the TensorFlow ecosystem and, to our knowledge, the\nfirst framework to leverage AutoML for model search and hyperparameter tuning\nin deep recommendation models. AutoRec also supports a highly flexible pipeline\nthat accommodates both sparse and dense inputs, rating prediction and\nclick-through rate (CTR) prediction tasks, and an array of recommendation\nmodels. Lastly, AutoRec provides a simple, user-friendly API. Experiments\nconducted on the benchmark datasets reveal AutoRec is reliable and can identify\nmodels which resemble the best model without prior knowledge.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 17:04:53 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Wang", "Ting-Hsiang", ""], ["Song", "Qingquan", ""], ["Han", "Xiaotian", ""], ["Liu", "Zirui", ""], ["Jin", "Haifeng", ""], ["Hu", "Xia", ""]]}, {"id": "2007.07276", "submitter": "Pavan Inguva", "authors": "Pavan Inguva, Lachlan Mason, Indranil Pan, Miselle Hengardi, Omar K.\n  Matar", "title": "Numerical simulation, clustering and prediction of multi-component\n  polymer precipitation", "comments": "18 pages, 10 figures, supporting info in anc, fixed typos and\n  references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cond-mat.soft cs.LG physics.flu-dyn stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-component polymer systems are of interest in organic photovoltaic and\ndrug delivery applications, among others where diverse morphologies influence\nperformance. An improved understanding of morphology classification, driven by\ncomposition-informed prediction tools, will aid polymer engineering practice.\nWe use a modified Cahn-Hilliard model to simulate polymer precipitation. Such\nphysics-based models require high-performance computations that prevent rapid\nprototyping and iteration in engineering settings. To reduce the required\ncomputational costs, we apply machine learning techniques for clustering and\nconsequent prediction of the simulated polymer blend images in conjunction with\nsimulations. Integrating ML and simulations in such a manner reduces the number\nof simulations needed to map out the morphology of polymer blends as a function\nof input parameters and also generates a data set which can be used by others\nto this end. We explore dimensionality reduction, via principal component\nanalysis and autoencoder techniques, and analyse the resulting morphology\nclusters. Supervised machine learning using Gaussian process classification was\nsubsequently used to predict morphology clusters according to species molar\nfraction and interaction parameter inputs. Manual pattern clustering yielded\nthe best results, but machine learning techniques were able to predict the\nmorphology of polymer blends with $\\geq$ 90 $\\%$ accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 09:10:17 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 05:52:10 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Inguva", "Pavan", ""], ["Mason", "Lachlan", ""], ["Pan", "Indranil", ""], ["Hengardi", "Miselle", ""], ["Matar", "Omar K.", ""]]}, {"id": "2007.07293", "submitter": "Yikun Ban", "authors": "Yikun Ban and Jingrui He", "title": "Generic Outlier Detection in Multi-Armed Bandit", "comments": "Published in SIGKDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of outlier arm detection in multi-armed\nbandit settings, which finds plenty of applications in many high-impact domains\nsuch as finance, healthcare, and online advertising. For this problem, a\nlearner aims to identify the arms whose expected rewards deviate significantly\nfrom most of the other arms. Different from existing work, we target the\ngeneric outlier arms or outlier arm groups whose expected rewards can be\nlarger, smaller, or even in between those of normal arms. To this end, we start\nby providing a comprehensive definition of such generic outlier arms and\noutlier arm groups. Then we propose a novel pulling algorithm named GOLD to\nidentify such generic outlier arms. It builds a real-time neighborhood graph\nbased on upper confidence bounds and catches the behavior pattern of outliers\nfrom normal arms. We also analyze its performance from various aspects. In the\nexperiments conducted on both synthetic and real-world data sets, the proposed\nalgorithm achieves 98 % accuracy while saving 83 % exploration cost on average\ncompared with state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 18:42:44 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Ban", "Yikun", ""], ["He", "Jingrui", ""]]}, {"id": "2007.07302", "submitter": "Bart Paul Gerard Van Parys", "authors": "Bart P.G. Van Parys, Negin Golrezaei", "title": "Optimal Learning for Structured Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study structured multi-armed bandits, which is the problem of online\ndecision-making under uncertainty in the presence of structural information. In\nthis problem, the decision-maker needs to discover the best course of action\ndespite observing only uncertain rewards over time. The decision-maker is aware\nof certain structural information regarding the reward distributions and would\nlike to minimize his regret by exploiting this information, where the regret is\nits performance difference against a benchmark policy which knows the best\naction ahead of time. In the absence of structural information, the classical\nUCB and Thomson sampling algorithms are well known to suffer only minimal\nregret. As recently pointed out, neither algorithms is, however, capable of\nexploiting structural information which is commonly available in practice. We\npropose a novel learning algorithm which we call \"DUSA\" whose worst-case regret\nmatches the information-theoretic regret lower bound up to a constant factor\nand can handle a wide range of structural information. Our algorithm DUSA\nsolves a dual counterpart of regret lower bound at the empirical reward\ndistribution and follows the suggestion made by the dual problem. Our proposed\nalgorithm is the first computationally viable learning policy for structured\nbandit problems that suffers asymptotic minimal regret.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 18:56:44 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Van Parys", "Bart P. G.", ""], ["Golrezaei", "Negin", ""]]}, {"id": "2007.07307", "submitter": "Matthew Willetts", "authors": "Matthew Willetts, Xenia Miscouridou, Stephen Roberts, Chris Holmes", "title": "Relaxed-Responsibility Hierarchical Discrete VAEs", "comments": "10 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successfully training Variational Autoencoders (VAEs) with a hierarchy of\ndiscrete latent variables remains an area of active research.\n  Vector-Quantised VAEs are a powerful approach to discrete VAEs, but naive\nhierarchical extensions can be unstable when training. Leveraging insights from\nclassical methods of inference we introduce \\textit{Relaxed-Responsibility\nVector-Quantisation}, a novel way to parameterise discrete latent variables, a\nrefinement of relaxed Vector-Quantisation that gives better performance and\nmore stable training. This enables a novel approach to hierarchical discrete\nvariational autoencoders with numerous layers of latent variables (here up to\n32) that we train end-to-end. Within hierarchical probabilistic deep generative\nmodels with discrete latent variables trained end-to-end, we achieve\nstate-of-the-art bits-per-dim results for various standard datasets. % Unlike\ndiscrete VAEs with a single layer of latent variables, we can produce samples\nby ancestral sampling: it is not essential to train a second autoregressive\ngenerative model over the learnt latent representations to then sample from and\nthen decode. % Moreover, that latter approach in these deep hierarchical models\nwould require thousands of forward passes to generate a single sample. Further,\nwe observe different layers of our model become associated with different\naspects of the data.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 19:10:05 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 18:59:59 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Willetts", "Matthew", ""], ["Miscouridou", "Xenia", ""], ["Roberts", "Stephen", ""], ["Holmes", "Chris", ""]]}, {"id": "2007.07314", "submitter": "Sadeep Jayasumana", "authors": "Aditya Krishna Menon and Sadeep Jayasumana and Ankit Singh Rawat and\n  Himanshu Jain and Andreas Veit and Sanjiv Kumar", "title": "Long-tail learning via logit adjustment", "comments": "Published as a conference paper in ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Real-world classification problems typically exhibit an imbalanced or\nlong-tailed label distribution, wherein many labels are associated with only a\nfew samples. This poses a challenge for generalisation on such labels, and also\nmakes na\\\"ive learning biased towards dominant labels. In this paper, we\npresent two simple modifications of standard softmax cross-entropy training to\ncope with these challenges. Our techniques revisit the classic idea of logit\nadjustment based on the label frequencies, either applied post-hoc to a trained\nmodel, or enforced in the loss during training. Such adjustment encourages a\nlarge relative margin between logits of rare versus dominant labels. These\ntechniques unify and generalise several recent proposals in the literature,\nwhile possessing firmer statistical grounding and empirical performance.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 19:27:13 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 21:23:25 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Menon", "Aditya Krishna", ""], ["Jayasumana", "Sadeep", ""], ["Rawat", "Ankit Singh", ""], ["Jain", "Himanshu", ""], ["Veit", "Andreas", ""], ["Kumar", "Sanjiv", ""]]}, {"id": "2007.07319", "submitter": "Antonio Briola", "authors": "Antonio Briola, Jeremy Turiel, Tomaso Aste", "title": "Deep Learning modeling of Limit Order Book: a comparative perspective", "comments": "16 pages, 4 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR cs.LG q-fin.CP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The present work addresses theoretical and practical questions in the domain\nof Deep Learning for High Frequency Trading. State-of-the-art models such as\nRandom models, Logistic Regressions, LSTMs, LSTMs equipped with an Attention\nmask, CNN-LSTMs and MLPs are reviewed and compared on the same tasks, feature\nspace and dataset, and then clustered according to pairwise similarity and\nperformance metrics. The underlying dimensions of the modeling techniques are\nhence investigated to understand whether these are intrinsic to the Limit Order\nBook's dynamics. We observe that the Multilayer Perceptron performs comparably\nto or better than state-of-the-art CNN-LSTM architectures indicating that\ndynamic spatial and temporal dimensions are a good approximation of the LOB's\ndynamics, but not necessarily the true underlying dimensions.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 17:06:30 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 14:27:15 GMT"}, {"version": "v3", "created": "Sun, 18 Oct 2020 15:44:44 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Briola", "Antonio", ""], ["Turiel", "Jeremy", ""], ["Aste", "Tomaso", ""]]}, {"id": "2007.07320", "submitter": "Tiansi Dong", "authors": "Tiansi Dong, Chengjiang Li, Christian Bauckhage, Juanzi Li, Stefan\n  Wrobel, Armin B. Cremers", "title": "Learning Syllogism with Euler Neural-Networks", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional neural networks represent everything as a vector, and are able to\napproximate a subset of logical reasoning to a certain degree. As basic logic\nrelations are better represented by topological relations between regions, we\npropose a novel neural network that represents everything as a ball and is able\nto learn topological configuration as an Euler diagram. So comes the name Euler\nNeural-Network (ENN). The central vector of a ball is a vector that can inherit\nrepresentation power of traditional neural network. ENN distinguishes four\nspatial statuses between balls, namely, being disconnected, being partially\noverlapped, being part of, being inverse part of. Within each status, ideal\nvalues are defined for efficient reasoning. A novel back-propagation algorithm\nwith six Rectified Spatial Units (ReSU) can optimize an Euler diagram\nrepresenting logical premises, from which logical conclusion can be deduced. In\ncontrast to traditional neural network, ENN can precisely represent all 24\ndifferent structures of Syllogism. Two large datasets are created: one\nextracted from WordNet-3.0 covers all types of Syllogism reasoning, the other\nextracted all family relations from DBpedia. Experiment results approve the\nsuperior power of ENN in logical representation and reasoning. Datasets and\nsource code are available upon request.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 19:35:35 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 09:58:24 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Dong", "Tiansi", ""], ["Li", "Chengjiang", ""], ["Bauckhage", "Christian", ""], ["Li", "Juanzi", ""], ["Wrobel", "Stefan", ""], ["Cremers", "Armin B.", ""]]}, {"id": "2007.07324", "submitter": "Michael Rotman", "authors": "Michael Rotman and Lior Wolf", "title": "Shuffling Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel recurrent neural network model, where the hidden state\n$h_t$ is obtained by permuting the vector elements of the previous hidden state\n$h_{t-1}$ and adding the output of a learned function $b(x_t)$ of the input\n$x_t$ at time $t$. In our model, the prediction is given by a second learned\nfunction, which is applied to the hidden state $s(h_t)$. The method is easy to\nimplement, extremely efficient, and does not suffer from vanishing nor\nexploding gradients. In an extensive set of experiments, the method shows\ncompetitive results, in comparison to the leading literature baselines.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 19:36:10 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Rotman", "Michael", ""], ["Wolf", "Lior", ""]]}, {"id": "2007.07336", "submitter": "Andrew Kirby", "authors": "Andrew C. Kirby, Siddharth Samsi, Michael Jones, Albert Reuther,\n  Jeremy Kepner, Vijay Gadepally", "title": "Layer-Parallel Training with GPU Concurrency of Deep Residual Neural\n  Networks via Nonlinear Multigrid", "comments": "7 pages, 6 figures, 27 citations. Accepted to 2020 IEEE High\n  Performance Extreme Computing Conference - Outstanding Paper Award", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Multigrid Full Approximation Storage algorithm for solving Deep Residual\nNetworks is developed to enable neural network parallelized layer-wise training\nand concurrent computational kernel execution on GPUs. This work demonstrates a\n10.2x speedup over traditional layer-wise model parallelism techniques using\nthe same number of compute units.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 20:15:36 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2020 18:34:56 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Kirby", "Andrew C.", ""], ["Samsi", "Siddharth", ""], ["Jones", "Michael", ""], ["Reuther", "Albert", ""], ["Kepner", "Jeremy", ""], ["Gadepally", "Vijay", ""]]}, {"id": "2007.07358", "submitter": "Youngmin Oh", "authors": "Youngmin Oh, Kimin Lee, Jinwoo Shin, Eunho Yang, and Sung Ju Hwang", "title": "Learning to Sample with Local and Global Contexts in Experience Replay\n  Buffer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experience replay, which enables the agents to remember and reuse experience\nfrom the past, has played a significant role in the success of off-policy\nreinforcement learning (RL). To utilize the experience replay efficiently, the\nexisting sampling methods allow selecting out more meaningful experiences by\nimposing priorities on them based on certain metrics (e.g. TD-error). However,\nthey may result in sampling highly biased, redundant transitions since they\ncompute the sampling rate for each transition independently, without\nconsideration of its importance in relation to other transitions. In this\npaper, we aim to address the issue by proposing a new learning-based sampling\nmethod that can compute the relative importance of transition. To this end, we\ndesign a novel permutation-equivariant neural architecture that takes contexts\nfrom not only features of each transition (local) but also those of others\n(global) as inputs. We validate our framework, which we refer to as Neural\nExperience Replay Sampler (NERS), on multiple benchmark tasks for both\ncontinuous and discrete control tasks and show that it can significantly\nimprove the performance of various off-policy RL methods. Further analysis\nconfirms that the improvements of the sample efficiency indeed are due to\nsampling diverse and meaningful transitions by NERS that considers both local\nand global contexts.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 21:12:56 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 15:10:58 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Oh", "Youngmin", ""], ["Lee", "Kimin", ""], ["Shin", "Jinwoo", ""], ["Yang", "Eunho", ""], ["Hwang", "Sung Ju", ""]]}, {"id": "2007.07361", "submitter": "George Petrides", "authors": "George Petrides and Wouter Verbeke", "title": "Misclassification cost-sensitive ensemble learning: A unifying framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the years, a plethora of cost-sensitive methods have been proposed for\nlearning on data when different types of misclassification errors incur\ndifferent costs. Our contribution is a unifying framework that provides a\ncomprehensive and insightful overview on cost-sensitive ensemble methods,\npinpointing their differences and similarities via a fine-grained\ncategorization. Our framework contains natural extensions and generalisations\nof ideas across methods, be it AdaBoost, Bagging or Random Forest, and as a\nresult not only yields all methods known to date but also some not previously\nconsidered.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 21:18:33 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Petrides", "George", ""], ["Verbeke", "Wouter", ""]]}, {"id": "2007.07365", "submitter": "Matthew Willetts", "authors": "Alexander Camuto, Matthew Willetts, Stephen Roberts, Chris Holmes, Tom\n  Rainforth", "title": "Towards a Theoretical Understanding of the Robustness of Variational\n  Autoencoders", "comments": "8 pages", "journal-ref": "AISTATS 2021", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We make inroads into understanding the robustness of Variational Autoencoders\n(VAEs) to adversarial attacks and other input perturbations. While previous\nwork has developed algorithmic approaches to attacking and defending VAEs,\nthere remains a lack of formalization for what it means for a VAE to be robust.\nTo address this, we develop a novel criterion for robustness in probabilistic\nmodels: $r$-robustness. We then use this to construct the first theoretical\nresults for the robustness of VAEs, deriving margins in the input space for\nwhich we can provide guarantees about the resulting reconstruction. Informally,\nwe are able to define a region within which any perturbation will produce a\nreconstruction that is similar to the original reconstruction. To support our\nanalysis, we show that VAEs trained using disentangling methods not only score\nwell under our robustness metrics, but that the reasons for this can be\ninterpreted through our theoretical results.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 21:22:29 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 20:14:58 GMT"}, {"version": "v3", "created": "Fri, 29 Jan 2021 18:33:07 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Camuto", "Alexander", ""], ["Willetts", "Matthew", ""], ["Roberts", "Stephen", ""], ["Holmes", "Chris", ""], ["Rainforth", "Tom", ""]]}, {"id": "2007.07366", "submitter": "Clive Cox", "authors": "Clive Cox, Dan Sun, Ellis Tarn, Animesh Singh, Rakesh Kelkar, David\n  Goodwin", "title": "Serverless inferencing on Kubernetes", "comments": "4 pages, 1 figure, presented at workshop on \"Challenges in Deploying\n  and Monitoring Machine Learning System\" at ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Organisations are increasingly putting machine learning models into\nproduction at scale. The increasing popularity of serverless scale-to-zero\nparadigms presents an opportunity for deploying machine learning models to help\nmitigate infrastructure costs when many models may not be in continuous use. We\nwill discuss the KFServing project which builds on the KNative serverless\nparadigm to provide a serverless machine learning inference solution that\nallows a consistent and simple interface for data scientists to deploy their\nmodels. We will show how it solves the challenges of autoscaling GPU based\ninference and discuss some of the lessons learnt from using it in production.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 21:23:59 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 07:18:25 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Cox", "Clive", ""], ["Sun", "Dan", ""], ["Tarn", "Ellis", ""], ["Singh", "Animesh", ""], ["Kelkar", "Rakesh", ""], ["Goodwin", "David", ""]]}, {"id": "2007.07367", "submitter": "Shikai Fang", "authors": "Shikai Fang, Zheng Wang, Zhimeng Pan, Ji Liu, Shandian Zhe", "title": "Streaming Probabilistic Deep Tensor Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of existing tensor factorization methods, most of them\nconduct a multilinear decomposition, and rarely exploit powerful modeling\nframeworks, like deep neural networks, to capture a variety of complicated\ninteractions in data. More important, for highly expressive, deep\nfactorization, we lack an effective approach to handle streaming data, which\nare ubiquitous in real-world applications. To address these issues, we propose\nSPIDER, a Streaming ProbabilistIc Deep tEnsoR factorization method. We first\nuse Bayesian neural networks (NNs) to construct a deep tensor factorization\nmodel. We assign a spike-and-slab prior over the NN weights to encourage\nsparsity and prevent overfitting. We then use Taylor expansions and moment\nmatching to approximate the posterior of the NN output and calculate the\nrunning model evidence, based on which we develop an efficient streaming\nposterior inference algorithm in the assumed-density-filtering and expectation\npropagation framework. Our algorithm provides responsive incremental updates\nfor the posterior of the latent factors and NN weights upon receiving new\ntensor entries, and meanwhile select and inhibit redundant/useless weights. We\nshow the advantages of our approach in four real-world applications.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 21:25:39 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Fang", "Shikai", ""], ["Wang", "Zheng", ""], ["Pan", "Zhimeng", ""], ["Liu", "Ji", ""], ["Zhe", "Shandian", ""]]}, {"id": "2007.07368", "submitter": "Matthew Willetts", "authors": "Alexander Camuto, Matthew Willetts, Umut \\c{S}im\\c{s}ekli, Stephen\n  Roberts, Chris Holmes", "title": "Explicit Regularisation in Gaussian Noise Injections", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 34 (2020)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the regularisation induced in neural networks by Gaussian noise\ninjections (GNIs). Though such injections have been extensively studied when\napplied to data, there have been few studies on understanding the regularising\neffect they induce when applied to network activations. Here we derive the\nexplicit regulariser of GNIs, obtained by marginalising out the injected noise,\nand show that it penalises functions with high-frequency components in the\nFourier domain; particularly in layers closer to a neural network's output. We\nshow analytically and empirically that such regularisation produces calibrated\nclassifiers with large classification margins.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 21:29:46 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 18:08:04 GMT"}, {"version": "v3", "created": "Fri, 18 Dec 2020 17:00:17 GMT"}, {"version": "v4", "created": "Wed, 13 Jan 2021 18:30:13 GMT"}, {"version": "v5", "created": "Mon, 18 Jan 2021 16:28:53 GMT"}, {"version": "v6", "created": "Tue, 19 Jan 2021 16:41:43 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Camuto", "Alexander", ""], ["Willetts", "Matthew", ""], ["\u015eim\u015fekli", "Umut", ""], ["Roberts", "Stephen", ""], ["Holmes", "Chris", ""]]}, {"id": "2007.07369", "submitter": "Joonas P\\\"a\\\"akk\\\"onen", "authors": "Joonas P\\\"a\\\"akk\\\"onen", "title": "Ordinal Regression with Fenton-Wilkinson Order Statistics: A Case Study\n  of an Orienteering Race", "comments": "10 pages, 5 figures, to be published in Proc. ICSDMS 2020:\n  International Conference on Sports Data Mining and Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sports, individuals and teams are typically interested in final rankings.\nFinal results, such as times or distances, dictate these rankings, also known\nas places. Places can be further associated with ordered random variables,\ncommonly referred to as order statistics. In this work, we introduce a simple,\nyet accurate order statistical ordinal regression function that predicts relay\nrace places with changeover-times. We call this function the Fenton-Wilkinson\nOrder Statistics model. This model is built on the following educated\nassumption: individual leg-times follow log-normal distributions. Moreover, our\nkey idea is to utilize Fenton-Wilkinson approximations of changeover-times\nalongside an estimator for the total number of teams as in the notorious German\ntank problem. This original place regression function is sigmoidal and thus\ncorrectly predicts the existence of a small number of elite teams that\nsignificantly outperform the rest of the teams. Our model also describes how\nplace increases linearly with changeover-time at the inflection point of the\nlog-normal distribution function. With real-world data from Jukola 2019, a\nmassive orienteering relay race, the model is shown to be highly accurate even\nwhen the size of the training set is only 5% of the whole data set. Numerical\nresults also show that our model exhibits smaller place prediction\nroot-mean-square-errors than linear regression, mord regression and Gaussian\nprocess regression.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 21:37:23 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["P\u00e4\u00e4kk\u00f6nen", "Joonas", ""]]}, {"id": "2007.07375", "submitter": "Maria Brbic", "authors": "Kaidi Cao, Maria Brbic, Jure Leskovec", "title": "Concept Learners for Few-Shot Learning", "comments": "Published at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing algorithms that are able to generalize to a novel task given only\na few labeled examples represents a fundamental challenge in closing the gap\nbetween machine- and human-level performance. The core of human cognition lies\nin the structured, reusable concepts that help us to rapidly adapt to new tasks\nand provide reasoning behind our decisions. However, existing meta-learning\nmethods learn complex representations across prior labeled tasks without\nimposing any structure on the learned representations. Here we propose COMET, a\nmeta-learning method that improves generalization ability by learning to learn\nalong human-interpretable concept dimensions. Instead of learning a joint\nunstructured metric space, COMET learns mappings of high-level concepts into\nsemi-structured metric spaces, and effectively combines the outputs of\nindependent concept learners. We evaluate our model on few-shot tasks from\ndiverse domains, including fine-grained image classification, document\ncategorization and cell type annotation on a novel dataset from a biological\ndomain developed in our work. COMET significantly outperforms strong\nmeta-learning baselines, achieving 6-15% relative improvement on the most\nchallenging 1-shot learning tasks, while unlike existing methods providing\ninterpretations behind the model's predictions.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 22:04:17 GMT"}, {"version": "v2", "created": "Sat, 16 Jan 2021 03:52:19 GMT"}, {"version": "v3", "created": "Sat, 20 Mar 2021 05:19:10 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Cao", "Kaidi", ""], ["Brbic", "Maria", ""], ["Leskovec", "Jure", ""]]}, {"id": "2007.07383", "submitter": "Georg Gottwald A.", "authors": "Georg A. Gottwald and Sebastian Reich", "title": "Supervised learning from noisy observations: Combining machine-learning\n  techniques with data assimilation", "comments": null, "journal-ref": null, "doi": "10.1016/j.physd.2021.132911", "report-no": null, "categories": "physics.data-an cs.LG physics.comp-ph stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven prediction and physics-agnostic machine-learning methods have\nattracted increased interest in recent years achieving forecast horizons going\nwell beyond those to be expected for chaotic dynamical systems. In a separate\nstrand of research data-assimilation has been successfully used to optimally\ncombine forecast models and their inherent uncertainty with incoming noisy\nobservations. The key idea in our work here is to achieve increased forecast\ncapabilities by judiciously combining machine-learning algorithms and data\nassimilation. We combine the physics-agnostic data-driven approach of random\nfeature maps as a forecast model within an ensemble Kalman filter data\nassimilation procedure. The machine-learning model is learned sequentially by\nincorporating incoming noisy observations. We show that the obtained forecast\nmodel has remarkably good forecast skill while being computationally cheap once\ntrained. Going beyond the task of forecasting, we show that our method can be\nused to generate reliable ensembles for probabilistic forecasting as well as to\nlearn effective model closure in multi-scale systems.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 22:29:37 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 07:44:28 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 11:45:38 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Gottwald", "Georg A.", ""], ["Reich", "Sebastian", ""]]}, {"id": "2007.07384", "submitter": "Brian Brubach", "authors": "Brian Brubach, Darshan Chakrabarti, John P. Dickerson, Samir Khuller,\n  Aravind Srinivasan, Leonidas Tsepenekas", "title": "A Pairwise Fair and Community-preserving Approach to k-Center Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is a foundational problem in machine learning with numerous\napplications. As machine learning increases in ubiquity as a backend for\nautomated systems, concerns about fairness arise. Much of the current\nliterature on fairness deals with discrimination against protected classes in\nsupervised learning (group fairness). We define a different notion of fair\nclustering wherein the probability that two points (or a community of points)\nbecome separated is bounded by an increasing function of their pairwise\ndistance (or community diameter). We capture the situation where data points\nrepresent people who gain some benefit from being clustered together.\nUnfairness arises when certain points are deterministically separated, either\narbitrarily or by someone who intends to harm them as in the case of\ngerrymandering election districts. In response, we formally define two new\ntypes of fairness in the clustering setting, pairwise fairness and community\npreservation. To explore the practicality of our fairness goals, we devise an\napproach for extending existing $k$-center algorithms to satisfy these fairness\nconstraints. Analysis of this approach proves that reasonable approximations\ncan be achieved while maintaining fairness. In experiments, we compare the\neffectiveness of our approach to classical $k$-center algorithms/heuristics and\nexplore the tradeoff between optimal clustering and fairness.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 22:32:27 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Brubach", "Brian", ""], ["Chakrabarti", "Darshan", ""], ["Dickerson", "John P.", ""], ["Khuller", "Samir", ""], ["Srinivasan", "Aravind", ""], ["Tsepenekas", "Leonidas", ""]]}, {"id": "2007.07400", "submitter": "Vinay Ramasesh", "authors": "Vinay V. Ramasesh, Ethan Dyer, Maithra Raghu", "title": "Anatomy of Catastrophic Forgetting: Hidden Representations and Task\n  Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central challenge in developing versatile machine learning systems is\ncatastrophic forgetting: a model trained on tasks in sequence will suffer\nsignificant performance drops on earlier tasks. Despite the ubiquity of\ncatastrophic forgetting, there is limited understanding of the underlying\nprocess and its causes. In this paper, we address this important knowledge gap,\ninvestigating how forgetting affects representations in neural network models.\nThrough representational analysis techniques, we find that deeper layers are\ndisproportionately the source of forgetting. Supporting this, a study of\nmethods to mitigate forgetting illustrates that they act to stabilize deeper\nlayers. These insights enable the development of an analytic argument and\nempirical picture relating the degree of forgetting to representational\nsimilarity between tasks. Consistent with this picture, we observe maximal\nforgetting occurs for task sequences with intermediate similarity. We perform\nempirical studies on the standard split CIFAR-10 setup and also introduce a\nnovel CIFAR-100 based task approximating realistic input distribution shift.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 23:31:14 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Ramasesh", "Vinay V.", ""], ["Dyer", "Ethan", ""], ["Raghu", "Maithra", ""]]}, {"id": "2007.07435", "submitter": "Hadi Mohaghegh Dolatabadi", "authors": "Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie", "title": "AdvFlow: Inconspicuous Black-box Adversarial Attacks using Normalizing\n  Flows", "comments": "Accepted to the 34th Conference on Neural Information Processing\n  Systems (NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning classifiers are susceptible to well-crafted, imperceptible\nvariations of their inputs, known as adversarial attacks. In this regard, the\nstudy of powerful attack models sheds light on the sources of vulnerability in\nthese classifiers, hopefully leading to more robust ones. In this paper, we\nintroduce AdvFlow: a novel black-box adversarial attack method on image\nclassifiers that exploits the power of normalizing flows to model the density\nof adversarial examples around a given target image. We see that the proposed\nmethod generates adversaries that closely follow the clean data distribution, a\nproperty which makes their detection less likely. Also, our experimental\nresults show competitive performance of the proposed approach with some of the\nexisting attack methods on defended classifiers. The code is available at\nhttps://github.com/hmdolatabadi/AdvFlow.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 02:13:49 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 00:36:25 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Dolatabadi", "Hadi M.", ""], ["Erfani", "Sarah", ""], ["Leckie", "Christopher", ""]]}, {"id": "2007.07443", "submitter": "Sinong Geng", "authors": "Sinong Geng, Houssam Nassif, Carlos A. Manzanares, A. Max Reppen,\n  Ronnie Sircar", "title": "Deep PQR: Solving Inverse Reinforcement Learning using Anchor Actions", "comments": null, "journal-ref": "In Proceedings of the 37th ICML, Vienna, Austria, PMLR 119, pp.\n  3431-3441, 2020", "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a reward function estimation framework for inverse reinforcement\nlearning with deep energy-based policies. We name our method PQR, as it\nsequentially estimates the Policy, the $Q$-function, and the Reward function by\ndeep learning. PQR does not assume that the reward solely depends on the state,\ninstead it allows for a dependency on the choice of action. Moreover, PQR\nallows for stochastic state transitions. To accomplish this, we assume the\nexistence of one anchor action whose reward is known, typically the action of\ndoing nothing, yielding no reward. We present both estimators and algorithms\nfor the PQR method. When the environment transition is known, we prove that the\nPQR reward estimator uniquely recovers the true reward. With unknown\ntransitions, we bound the estimation error of PQR. Finally, the performance of\nPQR is demonstrated by synthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 02:35:01 GMT"}, {"version": "v2", "created": "Sat, 15 Aug 2020 02:26:40 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Geng", "Sinong", ""], ["Nassif", "Houssam", ""], ["Manzanares", "Carlos A.", ""], ["Reppen", "A. Max", ""], ["Sircar", "Ronnie", ""]]}, {"id": "2007.07448", "submitter": "Xu Wang", "authors": "Xu Wang, Mladen Kolar and Ali Shojaie", "title": "Statistical Inference for Networks of High-Dimensional Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fueled in part by recent applications in neuroscience, the multivariate\nHawkes process has become a popular tool for modeling the network of\ninteractions among high-dimensional point process data. While evaluating the\nuncertainty of the network estimates is critical in scientific applications,\nexisting methodological and theoretical work has primarily addressed\nestimation. To bridge this gap, this paper develops a new statistical inference\nprocedure for high-dimensional Hawkes processes. The key ingredient for this\ninference procedure is a new concentration inequality on the first- and\nsecond-order statistics for integrated stochastic processes, which summarize\nthe entire history of the process. Combining recent results on martingale\ncentral limit theory with the new concentration inequality, we then\ncharacterize the convergence rate of the test statistics. We illustrate finite\nsample validity of our inferential tools via extensive simulations and\ndemonstrate their utility by applying them to a neuron spike train data set.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 02:46:36 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Wang", "Xu", ""], ["Kolar", "Mladen", ""], ["Shojaie", "Ali", ""]]}, {"id": "2007.07459", "submitter": "Alistair Shilton", "authors": "Alistair Shilton, Sunil Gupta, Santu Rana, Svetha Venkatesh", "title": "From deep to Shallow: Equivalent Forms of Deep Networks in Reproducing\n  Kernel Krein Space and Indefinite Support Vector Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore a connection between deep networks and learning in\nreproducing kernel Krein space. Our approach is based on the concept of\npush-forward - that is, taking a fixed non-linear transform on a linear\nprojection and converting it to a linear projection on the output of a fixed\nnon-linear transform, pushing the weights forward through the non-linearity.\nApplying this repeatedly from the input to the output of a deep network, the\nweights can be progressively \"pushed\" to the output layer, resulting in a flat\nnetwork that has the form of a fixed non-linear map (whose form is determined\nby the structure of the deep network) followed by a linear projection\ndetermined by the weight matrices - that is, we take a deep network and convert\nit to an equivalent (indefinite) kernel machine. We then investigate the\nimplications of this transformation for capacity control and uniform\nconvergence, and provide a Rademacher complexity bound on the deep network in\nterms of Rademacher complexity in reproducing kernel Krein space. Finally, we\nanalyse the sparsity properties of the flat representation, showing that the\nflat weights are (effectively) Lp-\"norm\" regularised with 0<p<1 (bridge\nregression).\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 03:21:35 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 07:27:16 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Shilton", "Alistair", ""], ["Gupta", "Sunil", ""], ["Rana", "Santu", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "2007.07461", "submitter": "Kaiqing Zhang", "authors": "Kaiqing Zhang, Sham M. Kakade, Tamer Ba\\c{s}ar, Lin F. Yang", "title": "Model-Based Multi-Agent RL in Zero-Sum Markov Games with Near-Optimal\n  Sample Complexity", "comments": "Addressed minor comments from NeurIPS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT cs.MA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based reinforcement learning (RL), which finds an optimal policy using\nan empirical model, has long been recognized as one of the corner stones of RL.\nIt is especially suitable for multi-agent RL (MARL), as it naturally decouples\nthe learning and the planning phases, and avoids the non-stationarity problem\nwhen all agents are improving their policies simultaneously using samples.\nThough intuitive and widely-used, the sample complexity of model-based MARL\nalgorithms has not been fully investigated. In this paper, our goal is to\naddress the fundamental question about its sample complexity. We study arguably\nthe most basic MARL setting: two-player discounted zero-sum Markov games, given\nonly access to a generative model. We show that model-based MARL achieves a\nsample complexity of $\\tilde O(|S||A||B|(1-\\gamma)^{-3}\\epsilon^{-2})$ for\nfinding the Nash equilibrium (NE) value up to some $\\epsilon$ error, and the\n$\\epsilon$-NE policies with a smooth planning oracle, where $\\gamma$ is the\ndiscount factor, and $S,A,B$ denote the state space, and the action spaces for\nthe two agents. We further show that such a sample bound is minimax-optimal (up\nto logarithmic factors) if the algorithm is reward-agnostic, where the\nalgorithm queries state transition samples without reward knowledge, by\nestablishing a matching lower bound. This is in contrast to the usual\nreward-aware setting, with a\n$\\tilde\\Omega(|S|(|A|+|B|)(1-\\gamma)^{-3}\\epsilon^{-2})$ lower bound, where\nthis model-based approach is near-optimal with only a gap on the $|A|,|B|$\ndependence. Our results not only demonstrate the sample-efficiency of this\nbasic model-based approach in MARL, but also elaborate on the fundamental\ntradeoff between its power (easily handling the more challenging\nreward-agnostic case) and limitation (less adaptive and suboptimal in\n$|A|,|B|$), particularly arises in the multi-agent context.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 03:25:24 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 03:34:56 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Zhang", "Kaiqing", ""], ["Kakade", "Sham M.", ""], ["Ba\u015far", "Tamer", ""], ["Yang", "Lin F.", ""]]}, {"id": "2007.07467", "submitter": "Shunki Kyoya", "authors": "Shunki Kyoya and Kenji Yamanishi", "title": "Mixture Complexity and Its Application to Gradual Clustering Change\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In model-based clustering using finite mixture models, it is a significant\nchallenge to determine the number of clusters (cluster size). It used to be\nequal to the number of mixture components (mixture size); however, this may not\nbe valid in the presence of overlaps or weight biases. In this study, we\npropose to continuously measure the cluster size in a mixture model by a new\nconcept called mixture complexity (MC). It is formally defined from the\nviewpoint of information theory and can be seen as a natural extension of the\ncluster size considering overlap and weight bias. Subsequently, we apply MC to\nthe issue of gradual clustering change detection. Conventionally, clustering\nchanges has been considered to be abrupt, induced by the changes in the mixture\nsize or cluster size. Meanwhile, we consider the clustering changes to be\ngradual in terms of MC; it has the benefits of finding the changes earlier and\ndiscerning the significant and insignificant changes. We further demonstrate\nthat the MC can be decomposed according to the hierarchical structures of the\nmixture models; it helps us to analyze the detail of substructures.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 03:49:38 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Kyoya", "Shunki", ""], ["Yamanishi", "Kenji", ""]]}, {"id": "2007.07481", "submitter": "Jianyu Wang", "authors": "Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, H. Vincent Poor", "title": "Tackling the Objective Inconsistency Problem in Heterogeneous Federated\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In federated optimization, heterogeneity in the clients' local datasets and\ncomputation speeds results in large variations in the number of local updates\nperformed by each client in each communication round. Naive weighted\naggregation of such models causes objective inconsistency, that is, the global\nmodel converges to a stationary point of a mismatched objective function which\ncan be arbitrarily different from the true objective. This paper provides a\ngeneral framework to analyze the convergence of federated heterogeneous\noptimization algorithms. It subsumes previously proposed methods such as FedAvg\nand FedProx and provides the first principled understanding of the solution\nbias and the convergence slowdown due to objective inconsistency. Using\ninsights from this analysis, we propose FedNova, a normalized averaging method\nthat eliminates objective inconsistency while preserving fast error\nconvergence.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 05:01:23 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Wang", "Jianyu", ""], ["Liu", "Qinghua", ""], ["Liang", "Hao", ""], ["Joshi", "Gauri", ""], ["Poor", "H. Vincent", ""]]}, {"id": "2007.07484", "submitter": "Jihun Yun", "authors": "Jihun Yun, Aurelie C. Lozano, Eunho Yang", "title": "A General Family of Stochastic Proximal Gradient Methods for Deep\n  Learning", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the training of regularized neural networks where the regularizer\ncan be non-smooth and non-convex. We propose a unified framework for stochastic\nproximal gradient descent, which we term ProxGen, that allows for arbitrary\npositive preconditioners and lower semi-continuous regularizers. Our framework\nencompasses standard stochastic proximal gradient methods without\npreconditioners as special cases, which have been extensively studied in\nvarious settings. Not only that, we present two important update rules beyond\nthe well-known standard methods as a byproduct of our approach: (i) the first\nclosed-form proximal mappings of $\\ell_q$ regularization ($0 \\leq q \\leq 1$)\nfor adaptive stochastic gradient methods, and (ii) a revised version of\nProxQuant that fixes a caveat of the original approach for\nquantization-specific regularizers. We analyze the convergence of ProxGen and\nshow that the whole family of ProxGen enjoys the same convergence rate as\nstochastic proximal gradient descent without preconditioners. We also\nempirically show the superiority of proximal methods compared to\nsubgradient-based approaches via extensive experiments. Interestingly, our\nresults indicate that proximal methods with non-convex regularizers are more\neffective than those with convex regularizers.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 05:13:33 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Yun", "Jihun", ""], ["Lozano", "Aurelie C.", ""], ["Yang", "Eunho", ""]]}, {"id": "2007.07495", "submitter": "Julaiti Alafate", "authors": "Julaiti Alafate, Yoav Freund, David T. Sandwell, Brook Tozer", "title": "Experimental Design for Bathymetry Editing", "comments": "Published as a workshop paper at ICML 2020 Workshop on Real World\n  Experiment Design and Active Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an application of machine learning to a real-world computer\nassisted labeling task. Our experimental results expose significant deviations\nfrom the IID assumption commonly used in machine learning. These results\nsuggest that the common random split of all data into training and testing can\noften lead to poor performance.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 05:58:09 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Alafate", "Julaiti", ""], ["Freund", "Yoav", ""], ["Sandwell", "David T.", ""], ["Tozer", "Brook", ""]]}, {"id": "2007.07497", "submitter": "Zhiqin Xu", "authors": "Tao Luo, Zhi-Qin John Xu, Zheng Ma, Yaoyu Zhang", "title": "Phase diagram for two-layer ReLU neural networks at infinite-width limit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How neural network behaves during the training over different choices of\nhyperparameters is an important question in the study of neural networks. In\nthis work, inspired by the phase diagram in statistical mechanics, we draw the\nphase diagram for the two-layer ReLU neural network at the infinite-width limit\nfor a complete characterization of its dynamical regimes and their dependence\non hyperparameters related to initialization. Through both experimental and\ntheoretical approaches, we identify three regimes in the phase diagram, i.e.,\nlinear regime, critical regime and condensed regime, based on the relative\nchange of input weights as the width approaches infinity, which tends to $0$,\n$O(1)$ and $+\\infty$, respectively. In the linear regime, NN training dynamics\nis approximately linear similar to a random feature model with an exponential\nloss decay. In the condensed regime, we demonstrate through experiments that\nactive neurons are condensed at several discrete orientations. The critical\nregime serves as the boundary between above two regimes, which exhibits an\nintermediate nonlinear behavior with the mean-field model as a typical example.\nOverall, our phase diagram for the two-layer ReLU NN serves as a map for the\nfuture studies and is a first step towards a more systematical investigation of\nthe training behavior and the implicit regularization of NNs of different\nstructures.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 06:04:35 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 05:35:06 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Luo", "Tao", ""], ["Xu", "Zhi-Qin John", ""], ["Ma", "Zheng", ""], ["Zhang", "Yaoyu", ""]]}, {"id": "2007.07498", "submitter": "Zheng Tracy Ke", "authors": "Zhirui Hu, Zheng Tracy Ke, Jun S Liu", "title": "Measurement error models: from nonparametric methods to deep neural\n  networks", "comments": "37 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep learning has inspired recent interests in applying neural\nnetworks in statistical inference. In this paper, we investigate the use of\ndeep neural networks for nonparametric regression with measurement errors. We\npropose an efficient neural network design for estimating measurement error\nmodels, in which we use a fully connected feed-forward neural network (FNN) to\napproximate the regression function $f(x)$, a normalizing flow to approximate\nthe prior distribution of $X$, and an inference network to approximate the\nposterior distribution of $X$. Our method utilizes recent advances in\nvariational inference for deep neural networks, such as the importance weight\nautoencoder, doubly reparametrized gradient estimator, and non-linear\nindependent components estimation. We conduct an extensive numerical study to\ncompare the neural network approach with classical nonparametric methods and\nobserve that the neural network approach is more flexible in accommodating\ndifferent classes of regression functions and performs superior or comparable\nto the best available method in nearly all settings.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 06:05:37 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Hu", "Zhirui", ""], ["Ke", "Zheng Tracy", ""], ["Liu", "Jun S", ""]]}, {"id": "2007.07501", "submitter": "Nikolay Prudkovskiy", "authors": "Nikolay Prudkovskiy", "title": "Static analysis of executable files by machine learning methods", "comments": "36 pages, 13 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper describes how to detect malicious executable files based on static\nanalysis of their binary content. The stages of pre-processing and cleaning\ndata extracted from different areas of executable files are analyzed. Methods\nof encoding categorical attributes of executable files are considered, as are\nways to reduce the feature field dimension and select characteristic features\nin order to effectively represent samples of binary executable files for\nfurther training classifiers. An ensemble training approach was applied in\norder to aggregate forecasts from each classifier, and an ensemble of\nclassifiers of various feature groups of executable file attributes was created\nin order to subsequently develop a system for detecting malicious files in an\nuninsulated environment.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 06:15:15 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Prudkovskiy", "Nikolay", ""]]}, {"id": "2007.07518", "submitter": "Cedric Schockaert", "authors": "Cedric Schockaert, Henri Hoyez", "title": "MTS-CycleGAN: An Adversarial-based Deep Mapping Learning Network for\n  Multivariate Time Series Domain Adaptation Applied to the Ironmaking Industry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the current era, an increasing number of machine learning models is\ngenerated for the automation of industrial processes. To that end, machine\nlearning models are trained using historical data of each single asset leading\nto the development of asset-based models. To elevate machine learning models to\na higher level of learning capability, domain adaptation has opened the door\nfor extracting relevant patterns from several assets combined together. In this\nresearch we are focusing on translating the specific asset-based historical\ndata (source domain) into data corresponding to one reference asset (target\ndomain), leading to the creation of a multi-assets global dataset required for\ntraining domain invariant generic machine learning models. This research is\nconducted to apply domain adaptation to the ironmaking industry, and\nparticularly for the creation of a domain invariant dataset by gathering data\nfrom different blast furnaces. The blast furnace data is characterized by\nmultivariate time series. Domain adaptation for multivariate time series data\nhasn't been covered extensively in the literature. We propose MTS-CycleGAN, an\nalgorithm for Multivariate Time Series data based on CycleGAN. To the best of\nour knowledge, this is the first time CycleGAN is applied on multivariate time\nseries data. Our contribution is the integration in the CycleGAN architecture\nof a Long Short-Term Memory (LSTM)-based AutoEncoder (AE) for the generator and\na stacked LSTM-based discriminator, together with dedicated extended features\nextraction mechanisms. MTS-CycleGAN is validated using two artificial datasets\nembedding the complex temporal relations between variables reflecting the blast\nfurnace process. MTS-CycleGAN is successfully learning the mapping between both\nartificial multivariate time series datasets, allowing an efficient translation\nfrom a source to a target artificial blast furnace dataset.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 07:33:25 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Schockaert", "Cedric", ""], ["Hoyez", "Henri", ""]]}, {"id": "2007.07559", "submitter": "Rodrigo de Medrano", "authors": "Rodrigo de Medrano, Jos\\'e L. Aznarte", "title": "On the Inclusion of Spatial Information for Spatio-Temporal Neural\n  Networks", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When confronting a spatio-temporal regression, it is sensible to feed the\nmodel with any available prior information about the spatial dimension. For\nexample, it is common to define the architecture of neural networks based on\nspatial closeness, adjacency, or correlation. A common alternative, if spatial\ninformation is not available or is too costly to introduce it in the model, is\nto learn it as an extra step of the model. While the use of prior spatial\nknowledge, given or learnt, might be beneficial, in this work we question this\nprinciple by comparing spatial agnostic neural networks with state of the art\nmodels. Our results show that the typical inclusion of prior spatial\ninformation is not really needed in most cases. In order to validate this\ncounterintuitive result, we perform thorough experiments over ten different\ndatasets related to sustainable mobility and air quality, substantiating our\nconclusions on real world problems with direct implications for public health\nand economy.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 09:18:47 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 13:12:46 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["de Medrano", "Rodrigo", ""], ["Aznarte", "Jos\u00e9 L.", ""]]}, {"id": "2007.07582", "submitter": "Sabrina Hoppe", "authors": "Sabrina Hoppe and Marc Toussaint", "title": "Qgraph-bounded Q-learning: Stabilizing Model-Free Off-Policy Deep\n  Reinforcement Learning", "comments": "15 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In state of the art model-free off-policy deep reinforcement learning, a\nreplay memory is used to store past experience and derive all network updates.\nEven if both state and action spaces are continuous, the replay memory only\nholds a finite number of transitions. We represent these transitions in a data\ngraph and link its structure to soft divergence. By selecting a subgraph with a\nfavorable structure, we construct a simplified Markov Decision Process for\nwhich exact Q-values can be computed efficiently as more data comes in. The\nsubgraph and its associated Q-values can be represented as a QGraph. We show\nthat the Q-value for each transition in the simplified MDP is a lower bound of\nthe Q-value for the same transition in the original continuous Q-learning\nproblem. By using these lower bounds in temporal difference learning, our\nmethod QG-DDPG is less prone to soft divergence and exhibits increased sample\nefficiency while being more robust to hyperparameters. QGraphs also retain\ninformation from transitions that have already been overwritten in the replay\nmemory, which can decrease the algorithm's sensitivity to the replay memory\ncapacity.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 10:01:32 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Hoppe", "Sabrina", ""], ["Toussaint", "Marc", ""]]}, {"id": "2007.07584", "submitter": "An-Phi Nguyen", "authors": "An-phi Nguyen, Mar\\'ia Rodr\\'iguez Mart\\'inez", "title": "On quantitative aspects of model interpretability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the growing body of work in interpretable machine learning, it\nremains unclear how to evaluate different explainability methods without\nresorting to qualitative assessment and user-studies. While interpretability is\nan inherently subjective matter, previous works in cognitive science and\nepistemology have shown that good explanations do possess aspects that can be\nobjectively judged apart from fidelity), such assimplicity and broadness. In\nthis paper we propose a set of metrics to programmatically evaluate\ninterpretability methods along these dimensions. In particular, we argue that\nthe performance of methods along these dimensions can be orthogonally imputed\nto two conceptual parts, namely the feature extractor and the actual\nexplainability method. We experimentally validate our metrics on different\nbenchmark tasks and show how they can be used to guide a practitioner in the\nselection of the most appropriate method for the task at hand.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 10:05:05 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Nguyen", "An-phi", ""], ["Mart\u00ednez", "Mar\u00eda Rodr\u00edguez", ""]]}, {"id": "2007.07588", "submitter": "Hilde Weerts", "authors": "Hilde J.P. Weerts, Andreas C. Mueller, Joaquin Vanschoren", "title": "Importance of Tuning Hyperparameters of Machine Learning Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of many machine learning algorithms depends on their\nhyperparameter settings. The goal of this study is to determine whether it is\nimportant to tune a hyperparameter or whether it can be safely set to a default\nvalue. We present a methodology to determine the importance of tuning a\nhyperparameter based on a non-inferiority test and tuning risk: the performance\nloss that is incurred when a hyperparameter is not tuned, but set to a default\nvalue. Because our methods require the notion of a default parameter, we\npresent a simple procedure that can be used to determine reasonable default\nparameters. We apply our methods in a benchmark study using 59 datasets from\nOpenML. Our results show that leaving particular hyperparameters at their\ndefault value is non-inferior to tuning these hyperparameters. In some cases,\nleaving the hyperparameter at its default value even outperforms tuning it\nusing a search procedure with a limited number of iterations.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 10:06:59 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Weerts", "Hilde J. P.", ""], ["Mueller", "Andreas C.", ""], ["Vanschoren", "Joaquin", ""]]}, {"id": "2007.07591", "submitter": "An-Phi Nguyen", "authors": "An-phi Nguyen, Mar\\'ia Rodr\\'iguez Mart\\'inez", "title": "Learning Invariances for Interpretability using Supervised VAE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to learn model invariances as a means of interpreting a model.\nThis is motivated by a reverse engineering principle. If we understand a\nproblem, we may introduce inductive biases in our model in the form of\ninvariances. Conversely, when interpreting a complex supervised model, we can\nstudy its invariances to understand how that model solves a problem. To this\nend we propose a supervised form of variational auto-encoders (VAEs).\nCrucially, only a subset of the dimensions in the latent space contributes to\nthe supervised task, allowing the remaining dimensions to act as nuisance\nparameters. By sampling solely the nuisance dimensions, we are able to generate\nsamples that have undergone transformations that leave the classification\nunchanged, revealing the invariances of the model. Our experimental results\nshow the capability of our proposed model both in terms of classification, and\ngeneration of invariantly transformed samples. Finally we show how combining\nour model with feature attribution methods it is possible to reach a more\nfine-grained understanding about the decision process of the model.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 10:14:16 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Nguyen", "An-phi", ""], ["Mart\u00ednez", "Mar\u00eda Rodr\u00edguez", ""]]}, {"id": "2007.07606", "submitter": "Felix Mujkanovic", "authors": "Felix Mujkanovic, Vanja Dosko\\v{c}, Martin Schirneck, Patrick\n  Sch\\\"afer, Tobias Friedrich", "title": "timeXplain -- A Framework for Explaining the Predictions of Time Series\n  Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern time series classifiers display impressive predictive capabilities,\nyet their decision-making processes mostly remain black boxes to the user. At\nthe same time, model-agnostic explainers, such as the recently proposed SHAP,\npromise to make the predictions of machine learning models interpretable,\nprovided there are well-designed domain mappings. We bring both worlds together\nin our timeXplain framework, extending the reach of explainable artificial\nintelligence to time series classification and value prediction. We present\nnovel domain mappings for the time and the frequency domain as well as series\nstatistics and analyze their explicative power as well as their limits. We\nemploy timeXplain in a large-scale experimental comparison of several\nstate-of-the-art time series classifiers and discover similarities between\nseemingly distinct classification concepts such as residual neural networks and\nelastic ensembles.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 10:32:43 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Mujkanovic", "Felix", ""], ["Dosko\u010d", "Vanja", ""], ["Schirneck", "Martin", ""], ["Sch\u00e4fer", "Patrick", ""], ["Friedrich", "Tobias", ""]]}, {"id": "2007.07617", "submitter": "Ghada Sokar", "authors": "Ghada Sokar, Decebal Constantin Mocanu, Mykola Pechenizkiy", "title": "SpaceNet: Make Free Space For Continual Learning", "comments": "Published in Neurocomputing Journal", "journal-ref": "Neurocomputing, 439: 1-11, 2021", "doi": "10.1016/j.neucom.2021.01.078", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The continual learning (CL) paradigm aims to enable neural networks to learn\ntasks continually in a sequential fashion. The fundamental challenge in this\nlearning paradigm is catastrophic forgetting previously learned tasks when the\nmodel is optimized for a new task, especially when their data is not\naccessible. Current architectural-based methods aim at alleviating the\ncatastrophic forgetting problem but at the expense of expanding the capacity of\nthe model. Regularization-based methods maintain a fixed model capacity;\nhowever, previous studies showed the huge performance degradation of these\nmethods when the task identity is not available during inference (e.g. class\nincremental learning scenario). In this work, we propose a novel\narchitectural-based method referred as SpaceNet for class incremental learning\nscenario where we utilize the available fixed capacity of the model\nintelligently. SpaceNet trains sparse deep neural networks from scratch in an\nadaptive way that compresses the sparse connections of each task in a compact\nnumber of neurons. The adaptive training of the sparse connections results in\nsparse representations that reduce the interference between the tasks.\nExperimental results show the robustness of our proposed method against\ncatastrophic forgetting old tasks and the efficiency of SpaceNet in utilizing\nthe available capacity of the model, leaving space for more tasks to be\nlearned. In particular, when SpaceNet is tested on the well-known benchmarks\nfor CL: split MNIST, split Fashion-MNIST, and CIFAR-10/100, it outperforms\nregularization-based methods by a big performance gap. Moreover, it achieves\nbetter performance than architectural-based methods without model expansion and\nachieved comparable results with rehearsal-based methods, while offering a huge\nmemory reduction.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 11:21:31 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 18:09:38 GMT"}, {"version": "v3", "created": "Wed, 14 Apr 2021 08:39:33 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Sokar", "Ghada", ""], ["Mocanu", "Decebal Constantin", ""], ["Pechenizkiy", "Mykola", ""]]}, {"id": "2007.07677", "submitter": "Jonas Rauber", "authors": "Jonas Rauber, Matthias Bethge", "title": "Fast Differentiable Clipping-Aware Normalization and Rescaling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rescaling a vector $\\vec{\\delta} \\in \\mathbb{R}^n$ to a desired length is a\ncommon operation in many areas such as data science and machine learning. When\nthe rescaled perturbation $\\eta \\vec{\\delta}$ is added to a starting point\n$\\vec{x} \\in D$ (where $D$ is the data domain, e.g. $D = [0, 1]^n$), the\nresulting vector $\\vec{v} = \\vec{x} + \\eta \\vec{\\delta}$ will in general not be\nin $D$. To enforce that the perturbed vector $v$ is in $D$, the values of\n$\\vec{v}$ can be clipped to $D$. This subsequent element-wise clipping to the\ndata domain does however reduce the effective perturbation size and thus\ninterferes with the rescaling of $\\vec{\\delta}$. The optimal rescaling $\\eta$\nto obtain a perturbation with the desired norm after the clipping can be\niteratively approximated using a binary search. However, such an iterative\napproach is slow and non-differentiable. Here we show that the optimal\nrescaling can be found analytically using a fast and differentiable algorithm.\nOur algorithm works for any p-norm and can be used to train neural networks on\ninputs with normalized perturbations. We provide native implementations for\nPyTorch, TensorFlow, JAX, and NumPy based on EagerPy.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 13:43:22 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Rauber", "Jonas", ""], ["Bethge", "Matthias", ""]]}, {"id": "2007.07682", "submitter": "Daniel Rothchild", "authors": "Daniel Rothchild, Ashwinee Panda, Enayat Ullah, Nikita Ivkin, Ion\n  Stoica, Vladimir Braverman, Joseph Gonzalez, and Raman Arora", "title": "FetchSGD: Communication-Efficient Federated Learning with Sketching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches to federated learning suffer from a communication\nbottleneck as well as convergence issues due to sparse client participation. In\nthis paper we introduce a novel algorithm, called FetchSGD, to overcome these\nchallenges. FetchSGD compresses model updates using a Count Sketch, and then\ntakes advantage of the mergeability of sketches to combine model updates from\nmany workers. A key insight in the design of FetchSGD is that, because the\nCount Sketch is linear, momentum and error accumulation can both be carried out\nwithin the sketch. This allows the algorithm to move momentum and error\naccumulation from clients to the central aggregator, overcoming the challenges\nof sparse client participation while still achieving high compression rates and\ngood convergence. We prove that FetchSGD has favorable convergence guarantees,\nand we demonstrate its empirical effectiveness by training two residual\nnetworks and a transformer model.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 13:46:34 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 00:37:29 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Rothchild", "Daniel", ""], ["Panda", "Ashwinee", ""], ["Ullah", "Enayat", ""], ["Ivkin", "Nikita", ""], ["Stoica", "Ion", ""], ["Braverman", "Vladimir", ""], ["Gonzalez", "Joseph", ""], ["Arora", "Raman", ""]]}, {"id": "2007.07684", "submitter": "Philipp Marquetand", "authors": "Julia Westermayr, Philipp Marquetand", "title": "Deep Learning for UV Absorption Spectra with SchNarc: First Steps\n  Towards Transferability in Chemical Compound Space", "comments": null, "journal-ref": "J. Chem. Phys., 153, 154112 (2020)", "doi": "10.1063/5.0021915", "report-no": null, "categories": "physics.chem-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) has shown to advance the research field of quantum\nchemistry in almost any possible direction and has recently also entered the\nexcited states to investigate the multifaceted photochemistry of molecules. In\nthis paper, we pursue two goals: i) We show how ML can be used to model\npermanent dipole moments for excited states and transition dipole moments by\nadapting the charge model of [Chem. Sci., 2017, 8, 6924-6935], which was\noriginally proposed for the permanent dipole moment vector of the electronic\nground state. ii) We investigate the transferability of our excited-state ML\nmodels in chemical space, i.e., whether an ML model can predict properties of\nmolecules that it has never been trained on and whether it can learn the\ndifferent excited states of two molecules simultaneously. To this aim, we\nemploy and extend our previously reported SchNarc approach for excited-state\nML. We calculate UV absorption spectra from excited-state energies and\ntransition dipole moments as well as electrostatic potentials from latent\ncharges inferred by the ML model of the permanent dipole moment vectors. We\ntrain our ML models on CH$_2$NH$_2^+$ and C$_2$H$_4$, while predictions are\ncarried out for these molecules and additionally for CHNH$_2$, CH$_2$NH, and\nC$_2$H$_5^+$. The results indicate that transferability is possible for the\nexcited states.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 13:51:39 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Westermayr", "Julia", ""], ["Marquetand", "Philipp", ""]]}, {"id": "2007.07695", "submitter": "Yabin Zhang", "authors": "Yabin Zhang, Bin Deng, Kui Jia and Lei Zhang", "title": "Label Propagation with Augmented Anchors: A Simple Semi-Supervised\n  Learning baseline for Unsupervised Domain Adaptation", "comments": "ECCV2020 spotlight. Investigating SSL techniques for UDA. Codes are\n  available at https://github.com/YBZh/Label-Propagation-with-Augmented-Anchors", "journal-ref": "ECCV2020", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the problem relatedness between unsupervised domain adaptation\n(UDA) and semi-supervised learning (SSL), many state-of-the-art UDA methods\nadopt SSL principles (e.g., the cluster assumption) as their learning\ningredients. However, they tend to overlook the very domain-shift nature of\nUDA. In this work, we take a step further to study the proper extensions of SSL\ntechniques for UDA. Taking the algorithm of label propagation (LP) as an\nexample, we analyze the challenges of adopting LP to UDA and theoretically\nanalyze the conditions of affinity graph/matrix construction in order to\nachieve better propagation of true labels to unlabeled instances. Our analysis\nsuggests a new algorithm of Label Propagation with Augmented Anchors (A$^2$LP),\nwhich could potentially improve LP via generation of unlabeled virtual\ninstances (i.e., the augmented anchors) with high-confidence label predictions.\nTo make the proposed A$^2$LP useful for UDA, we propose empirical schemes to\ngenerate such virtual instances. The proposed schemes also tackle the\ndomain-shift challenge of UDA by alternating between pseudo labeling via\nA$^2$LP and domain-invariant feature learning. Experiments show that such a\nsimple SSL extension improves over representative UDA methods of\ndomain-invariant feature learning, and could empower two state-of-the-art\nmethods on benchmark UDA datasets. Our results show the value of further\ninvestigation on SSL techniques for UDA problems.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 14:09:31 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Zhang", "Yabin", ""], ["Deng", "Bin", ""], ["Jia", "Kui", ""], ["Zhang", "Lei", ""]]}, {"id": "2007.07698", "submitter": "Max Kochurov", "authors": "Max Kochurov, Sergey Ivanov, Eugeny Burnaev", "title": "Are Hyperbolic Representations in Graphs Created Equal?", "comments": "Proceedings of the 37th International Conference on Machine Learning,\n  Vienna, Austria, PMLR 108, 2020, GRLB Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there was an increasing interest in applications of graph neural\nnetworks in non-Euclidean geometry; however, are non-Euclidean representations\nalways useful for graph learning tasks? For different problems such as node\nclassification and link prediction we compute hyperbolic embeddings and\nconclude that for tasks that require global prediction consistency it might be\nuseful to use non-Euclidean embeddings, while for other tasks Euclidean models\nare superior. To do so we first fix an issue of the existing models associated\nwith the optimization process at zero curvature. Current hyperbolic models deal\nwith gradients at the origin in ad-hoc manner, which is inefficient and can\nlead to numerical instabilities. We solve the instabilities of\nkappa-Stereographic model at zero curvature cases and evaluate the approach of\nembedding graphs into the manifold in several graph representation learning\ntasks.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 14:14:14 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Kochurov", "Max", ""], ["Ivanov", "Sergey", ""], ["Burnaev", "Eugeny", ""]]}, {"id": "2007.07704", "submitter": "Anastasia Borovykh", "authors": "Anastasia Borovykh, Nikolas Kantas, Panos Parpas, Grigorios A.\n  Pavliotis", "title": "On stochastic mirror descent with interacting particles: convergence\n  properties and variance reduction", "comments": null, "journal-ref": null, "doi": "10.1016/j.physd.2021.132844", "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An open problem in optimization with noisy information is the computation of\nan exact minimizer that is independent of the amount of noise. A standard\npractice in stochastic approximation algorithms is to use a decreasing\nstep-size. This however leads to a slower convergence. A second alternative is\nto use a fixed step-size and run independent replicas of the algorithm and\naverage these. A third option is to run replicas of the algorithm and allow\nthem to interact. It is unclear which of these options works best. To address\nthis question, we reduce the problem of the computation of an exact minimizer\nwith noisy gradient information to the study of stochastic mirror descent with\ninteracting particles. We study the convergence of stochastic mirror descent\nand make explicit the tradeoffs between communication and variance reduction.\nWe provide theoretical and numerical evidence to suggest that interaction helps\nto improve convergence and reduce the variance of the estimate.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 14:21:27 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 17:40:17 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Borovykh", "Anastasia", ""], ["Kantas", "Nikolas", ""], ["Parpas", "Panos", ""], ["Pavliotis", "Grigorios A.", ""]]}, {"id": "2007.07723", "submitter": "Moab Arar", "authors": "Moab Arar, Noa Fish, Dani Daniel, Evgeny Tenetov, Ariel Shamir, Amit\n  Bermano", "title": "Focus-and-Expand: Training Guidance Through Gradual Manipulation of\n  Input Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple and intuitive Focus-and-eXpand (\\fax) method to guide the\ntraining process of a neural network towards a specific solution. Optimizing a\nneural network is a highly non-convex problem. Typically, the space of\nsolutions is large, with numerous possible local minima, where reaching a\nspecific minimum depends on many factors. In many cases, however, a solution\nwhich considers specific aspects, or features, of the input is desired. For\nexample, in the presence of bias, a solution that disregards the biased feature\nis a more robust and accurate one. Drawing inspiration from Parameter\nContinuation methods, we propose steering the training process to consider\nspecific features in the input more than others, through gradual shifts in the\ninput domain. \\fax extracts a subset of features from each input data-point,\nand exposes the learner to these features first, Focusing the solution on them.\nThen, by using a blending/mixing parameter $\\alpha$ it gradually eXpands the\nlearning process to include all features of the input. This process encourages\nthe consideration of the desired features more than others. Though not\nrestricted to this field, we quantitatively evaluate the effectiveness of our\napproach on various Computer Vision tasks, and achieve state-of-the-art bias\nremoval, improvements to an established augmentation method, and two examples\nof improvements to image classification tasks. Through these few examples we\ndemonstrate the impact this approach potentially carries for a wide variety of\nproblems, which stand to gain from understanding the solution landscape.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 14:49:56 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Arar", "Moab", ""], ["Fish", "Noa", ""], ["Daniel", "Dani", ""], ["Tenetov", "Evgeny", ""], ["Shamir", "Ariel", ""], ["Bermano", "Amit", ""]]}, {"id": "2007.07732", "submitter": "Jorge A Mendez", "authors": "Jorge A. Mendez and Eric Eaton", "title": "Lifelong Learning of Compositional Structures", "comments": "ICLR-2021. Code:\n  https://github.com/Lifelong-ML/Mendez2020Compositional.git", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A hallmark of human intelligence is the ability to construct self-contained\nchunks of knowledge and adequately reuse them in novel combinations for solving\ndifferent yet structurally related problems. Learning such compositional\nstructures has been a significant challenge for artificial systems, due to the\ncombinatorial nature of the underlying search problem. To date, research into\ncompositional learning has largely proceeded separately from work on lifelong\nor continual learning. We integrate these two lines of work to present a\ngeneral-purpose framework for lifelong learning of compositional structures\nthat can be used for solving a stream of related tasks. Our framework separates\nthe learning process into two broad stages: learning how to best combine\nexisting components in order to assimilate a novel problem, and learning how to\nadapt the set of existing components to accommodate the new problem. This\nseparation explicitly handles the trade-off between the stability required to\nremember how to solve earlier tasks and the flexibility required to solve new\ntasks, as we show empirically in an extensive evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 14:58:48 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 12:12:16 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Mendez", "Jorge A.", ""], ["Eaton", "Eric", ""]]}, {"id": "2007.07740", "submitter": "Marin Bilo\\v{s}", "authors": "Nick Harmening, Marin Bilo\\v{s}, Stephan G\\\"unnemann", "title": "Deep Representation Learning and Clustering of Traffic Scenarios", "comments": "Workshop on AI for Autonomous Driving, International Conference on\n  Machine Learning (ICML) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the traffic scenario space is a major challenge for the\nhomologation and coverage assessment of automated driving functions. In\ncontrast to current approaches that are mainly scenario-based and rely on\nexpert knowledge, we introduce two data driven autoencoding models that learn a\nlatent representation of traffic scenes. First is a CNN based spatio-temporal\nmodel that autoencodes a grid of traffic participants' positions. Secondly, we\ndevelop a pure temporal RNN based model that auto-encodes a sequence of sets.\nTo handle the unordered set data, we had to incorporate the permutation\ninvariance property. Finally, we show how the latent scenario embeddings can be\nused for clustering traffic scenarios and similarity retrieval.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 15:12:23 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Harmening", "Nick", ""], ["Bilo\u0161", "Marin", ""], ["G\u00fcnnemann", "Stephan", ""]]}, {"id": "2007.07762", "submitter": "Yun Yuan", "authors": "Yun Yuan, Zhao Zhang, Xianfeng Terry Yang", "title": "Highway Traffic State Estimation Using Physics Regularized Gaussian\n  Process: Discretized Formulation", "comments": "26 pages, 13 figures. arXiv admin note: text overlap with\n  arXiv:2002.02374", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of classical traffic flow (e.g., second-order\nmacroscopic) models and data-driven (e.g., Machine Learning - ML) approaches in\ntraffic state estimation, those approaches either require great efforts for\nparameter calibrations or lack theoretical interpretation. To fill this\nresearch gap, this study presents a new modeling framework, named physics\nregularized Gaussian process (PRGP). This novel approach can encode physics\nmodels, i.e., classical traffic flow models, into the Gaussian process\narchitecture and so as to regularize the ML training process. Particularly,\nthis study aims to discuss how to develop a PRGP model when the original\nphysics model is with discrete formulations. Then based on the posterior\nregularization inference framework, an efficient stochastic optimization\nalgorithm is developed to maximize the evidence lowerbound of the system\nlikelihood. To prove the effectiveness of the proposed model, this paper\nconducts empirical studies on a real-world dataset that is collected from a\nstretch of I-15 freeway, Utah. Results show the new PRGP model can outperform\nthe previous compatible methods, such as calibrated physics models and pure\nmachine learning methods, in estimation precision and input robustness.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 17:27:23 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Yuan", "Yun", ""], ["Zhang", "Zhao", ""], ["Yang", "Xianfeng Terry", ""]]}, {"id": "2007.07781", "submitter": "Sokbae Lee", "authors": "Sokbae Lee, Serena Ng", "title": "Sketching for Two-Stage Least Squares Estimation", "comments": "32 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When there is so much data that they become a computation burden, it is not\nuncommon to compute quantities of interest using a sketch of data of size $m$\ninstead of the full sample of size $n$. This paper investigates the\nimplications for two-stage least squares (2SLS) estimation when the sketches\nare obtained by a computationally efficient method known as CountSketch. We\nobtain three results. First, we establish conditions under which given the full\nsample, a sketched 2SLS estimate can be arbitrarily close to the full-sample\n2SLS estimate with high probability. Second, we give conditions under which the\nsketched 2SLS estimator converges in probability to the true parameter at a\nrate of $m^{-1/2}$ and is asymptotically normal. Third, we show that the\nasymptotic variance can be consistently estimated using the sketched sample and\nsuggest methods for determining an inference-conscious sketch size $m$. The\nsketched 2SLS estimator is used to estimate returns to education.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 15:58:27 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Lee", "Sokbae", ""], ["Ng", "Serena", ""]]}, {"id": "2007.07796", "submitter": "George Chen", "authors": "Linhong Li, Ren Zuo, Amanda Coston, Jeremy C. Weiss, George H. Chen", "title": "Neural Topic Models with Survival Supervision: Jointly Predicting\n  Time-to-Event Outcomes and Learning How Clinical Features Relate", "comments": "International Conference on Artificial Intelligence in Medicine (AIME\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In time-to-event prediction problems, a standard approach to estimating an\ninterpretable model is to use Cox proportional hazards, where features are\nselected based on lasso regularization or stepwise regression. However, these\nCox-based models do not learn how different features relate. As an alternative,\nwe present an interpretable neural network approach to jointly learn a survival\nmodel to predict time-to-event outcomes while simultaneously learning how\nfeatures relate in terms of a topic model. In particular, we model each subject\nas a distribution over \"topics\", which are learned from clinical features as to\nhelp predict a time-to-event outcome. From a technical standpoint, we extend\nexisting neural topic modeling approaches to also minimize a survival analysis\nloss function. We study the effectiveness of this approach on seven healthcare\ndatasets on predicting time until death as well as hospital ICU length of stay,\nwhere we find that neural survival-supervised topic models achieves competitive\naccuracy with existing approaches while yielding interpretable clinical\n\"topics\" that explain feature relationships.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 16:20:04 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Li", "Linhong", ""], ["Zuo", "Ren", ""], ["Coston", "Amanda", ""], ["Weiss", "Jeremy C.", ""], ["Chen", "George H.", ""]]}, {"id": "2007.07804", "submitter": "Giorgia Ramponi", "authors": "Giorgia Ramponi and Marcello Restelli", "title": "Newton Optimization on Helmholtz Decomposition for Continuous Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many learning problems involve multiple agents optimizing different\ninteractive functions. In these problems, the standard policy gradient\nalgorithms fail due to the non-stationarity of the setting and the different\ninterests of each agent. In fact, algorithms must take into account the complex\ndynamics of these systems to guarantee rapid convergence towards a (local) Nash\nequilibrium. In this paper, we propose NOHD (Newton Optimization on Helmholtz\nDecomposition), a Newton-like algorithm for multi-agent learning problems based\non the decomposition of the dynamics of the system in its irrotational\n(Potential) and solenoidal (Hamiltonian) component. This method ensures\nquadratic convergence in purely irrotational systems and pure solenoidal\nsystems. Furthermore, we show that NOHD is attracted to stable fixed points in\ngeneral multi-agent systems and repelled by strict saddle ones. Finally, we\nempirically compare the NOHD's performance with that of state-of-the-art\nalgorithms on some bimatrix games and in a continuous Gridworld environment.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 16:26:33 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 11:14:41 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Ramponi", "Giorgia", ""], ["Restelli", "Marcello", ""]]}, {"id": "2007.07812", "submitter": "Giorgia Ramponi", "authors": "Giorgia Ramponi and Gianluca Drappo and Marcello Restelli", "title": "Inverse Reinforcement Learning from a Gradient-based Learner", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse Reinforcement Learning addresses the problem of inferring an expert's\nreward function from demonstrations. However, in many applications, we not only\nhave access to the expert's near-optimal behavior, but we also observe part of\nher learning process. In this paper, we propose a new algorithm for this\nsetting, in which the goal is to recover the reward function being optimized by\nan agent, given a sequence of policies produced during learning. Our approach\nis based on the assumption that the observed agent is updating her policy\nparameters along the gradient direction. Then we extend our method to deal with\nthe more realistic scenario where we only have access to a dataset of learning\ntrajectories. For both settings, we provide theoretical insights into our\nalgorithms' performance. Finally, we evaluate the approach in a simulated\nGridWorld environment and on the MuJoCo environments, comparing it with the\nstate-of-the-art baseline.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 16:41:00 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Ramponi", "Giorgia", ""], ["Drappo", "Gianluca", ""], ["Restelli", "Marcello", ""]]}, {"id": "2007.07838", "submitter": "Deepak P", "authors": "Deepak P", "title": "Whither Fair Clustering?", "comments": "Accepted at the AI for Social Good Workshop, Harvard, July 20-21,\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the relatively busy area of fair machine learning that has been\ndominated by classification fairness research, fairness in clustering has\nstarted to see some recent attention. In this position paper, we assess the\nexisting work in fair clustering and observe that there are several directions\nthat are yet to be explored, and postulate that the state-of-the-art in fair\nclustering has been quite parochial in outlook. We posit that widening the\nnormative principles to target for, characterizing shortfalls where the target\ncannot be achieved fully, and making use of knowledge of downstream processes\ncan significantly widen the scope of research in fair clustering research. At a\ntime when clustering and unsupervised learning are being increasingly used to\nmake and influence decisions that matter significantly to human lives, we\nbelieve that widening the ambit of fair clustering is of immense significance.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 19:41:25 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["P", "Deepak", ""]]}, {"id": "2007.07853", "submitter": "Kun Ho Kim", "authors": "Kuno Kim, Megumi Sano, Julian De Freitas, Nick Haber, Daniel Yamins", "title": "Active World Model Learning with Progress Curiosity", "comments": "ICML 2020. Video of results at https://bit.ly/31vg7v1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  World models are self-supervised predictive models of how the world evolves.\nHumans learn world models by curiously exploring their environment, in the\nprocess acquiring compact abstractions of high bandwidth sensory inputs, the\nability to plan across long temporal horizons, and an understanding of the\nbehavioral patterns of other agents. In this work, we study how to design such\na curiosity-driven Active World Model Learning (AWML) system. To do so, we\nconstruct a curious agent building world models while visually exploring a 3D\nphysical environment rich with distillations of representative real-world\nagents. We propose an AWML system driven by $\\gamma$-Progress: a scalable and\neffective learning progress-based curiosity signal. We show that\n$\\gamma$-Progress naturally gives rise to an exploration policy that directs\nattention to complex but learnable dynamics in a balanced manner, thus\novercoming the \"white noise problem\". As a result, our $\\gamma$-Progress-driven\ncontroller achieves significantly higher AWML performance than baseline\ncontrollers equipped with state-of-the-art exploration strategies such as\nRandom Network Distillation and Model Disagreement.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 17:19:17 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Kim", "Kuno", ""], ["Sano", "Megumi", ""], ["De Freitas", "Julian", ""], ["Haber", "Nick", ""], ["Yamins", "Daniel", ""]]}, {"id": "2007.07869", "submitter": "Paul Micaelli", "authors": "Paul Micaelli and Amos Storkey", "title": "Non-greedy Gradient-based Hyperparameter Optimization Over Long Horizons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient-based hyperparameter optimization is an attractive way to perform\nmeta-learning across a distribution of tasks, or improve the performance of an\noptimizer on a single task. However, this approach has been unpopular for tasks\nrequiring long horizons (many gradient steps), due to memory scaling and\ngradient degradation issues. A common workaround is to learn hyperparameters\nonline or split the horizon into smaller chunks. However, this introduces\ngreediness which comes with a large performance drop, since the best local\nhyperparameters can make for poor global solutions. In this work, we enable\nnon-greediness over long horizons with a two-fold solution. First, we share\nhyperparameters that are contiguous in time, and show that this drastically\nmitigates gradient degradation issues. Then, we derive a forward-mode\ndifferentiation algorithm for the popular momentum-based SGD optimizer, which\nallows for a memory cost that is constant with horizon size. When put together,\nthese solutions allow us to learn hyperparameters without any prior knowledge.\nCompared to the baseline of hand-tuned off-the-shelf hyperparameters, our\nmethod compares favorably on simple datasets like SVHN. On CIFAR-10 we match\nthe baseline performance, and demonstrate for the first time that learning\nrate, momentum and weight decay schedules can be learned with gradients on a\ndataset of this size. Code is available at\nhttps://github.com/polo5/NonGreedyGradientHPO\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 17:44:07 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Micaelli", "Paul", ""], ["Storkey", "Amos", ""]]}, {"id": "2007.07876", "submitter": "Yunbei Xu", "authors": "Yunbei Xu and Assaf Zeevi", "title": "Upper Counterfactual Confidence Bounds: a New Optimism Principle for\n  Contextual Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The principle of optimism in the face of uncertainty is one of the most\nwidely used and successful ideas in multi-armed bandits and reinforcement\nlearning. However, existing optimistic algorithms (primarily UCB and its\nvariants) are often unable to deal with large context spaces. Essentially all\nexisting well performing algorithms for general contextual bandit problems rely\non weighted action allocation schemes; and theoretical guarantees for\noptimism-based algorithms are only known for restricted formulations. In this\npaper we study general contextual bandits under the realizability condition,\nand propose a simple generic principle to design optimistic algorithms, dubbed\n\"Upper Counterfactual Confidence Bounds\" (UCCB). We show that these algorithms\nare provably optimal and efficient in the presence of large context spaces. Key\ncomponents of UCCB include: 1) a systematic analysis of confidence bounds in\npolicy space rather than in action space; and 2) the potential function\nperspective that is used to express the power of optimism in the contextual\nsetting. We further show how the UCCB principle can be extended to infinite\naction spaces, by constructing confidence bounds via the newly introduced\nnotion of \"counterfactual action divergence.\"\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 17:50:46 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 04:03:41 GMT"}, {"version": "v3", "created": "Fri, 12 Feb 2021 19:05:14 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Xu", "Yunbei", ""], ["Zeevi", "Assaf", ""]]}, {"id": "2007.07878", "submitter": "Uthsav Chitra", "authors": "Uthsav Chitra, Kimberly Ding, Jasper C.H. Lee, Benjamin J. Raphael", "title": "Quantifying and Reducing Bias in Maximum Likelihood Estimation of\n  Structured Anomalies", "comments": "Accepted to ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly estimation, or the problem of finding a subset of a dataset that\ndiffers from the rest of the dataset, is a classic problem in machine learning\nand data mining. In both theoretical work and in applications, the anomaly is\nassumed to have a specific structure defined by membership in an\n$\\textit{anomaly family}$. For example, in temporal data the anomaly family may\nbe time intervals, while in network data the anomaly family may be connected\nsubgraphs. The most prominent approach for anomaly estimation is to compute the\nMaximum Likelihood Estimator (MLE) of the anomaly; however, it was recently\nobserved that for normally distributed data, the MLE is a $\\textit{biased}$\nestimator for some anomaly families. In this work, we demonstrate that in the\nnormal means setting, the bias of the MLE depends on the size of the anomaly\nfamily. We prove that if the number of sets in the anomaly family that contain\nthe anomaly is sub-exponential, then the MLE is asymptotically unbiased. We\nalso provide empirical evidence that the converse is true: if the number of\nsuch sets is exponential, then the MLE is asymptotically biased. Our analysis\nunifies a number of earlier results on the bias of the MLE for specific anomaly\nfamilies. Next, we derive a new anomaly estimator using a mixture model, and we\nprove that our anomaly estimator is asymptotically unbiased regardless of the\nsize of the anomaly family. We illustrate the advantages of our estimator\nversus the MLE on disease outbreak and highway traffic data.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 17:54:27 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 16:54:00 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Chitra", "Uthsav", ""], ["Ding", "Kimberly", ""], ["Lee", "Jasper C. H.", ""], ["Raphael", "Benjamin J.", ""]]}, {"id": "2007.07935", "submitter": "Jos\\'e Daniel Pascual-Triana", "authors": "Jos\\'e Daniel Pascual-Triana, David Charte, Marta Andr\\'es Arroyo,\n  Alberto Fern\\'andez and Francisco Herrera", "title": "Revisiting Data Complexity Metrics Based on Morphology for Overlap and\n  Imbalance: Snapshot, New Overlap Number of Balls Metrics and Singular\n  Problems Prospect", "comments": "23 pages, 9 figures, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data Science and Machine Learning have become fundamental assets for\ncompanies and research institutions alike. As one of its fields, supervised\nclassification allows for class prediction of new samples, learning from given\ntraining data. However, some properties can cause datasets to be problematic to\nclassify.\n  In order to evaluate a dataset a priori, data complexity metrics have been\nused extensively. They provide information regarding different intrinsic\ncharacteristics of the data, which serve to evaluate classifier compatibility\nand a course of action that improves performance. However, most complexity\nmetrics focus on just one characteristic of the data, which can be insufficient\nto properly evaluate the dataset towards the classifiers' performance. In fact,\nclass overlap, a very detrimental feature for the classification process\n(especially when imbalance among class labels is also present) is hard to\nassess.\n  This research work focuses on revisiting complexity metrics based on data\nmorphology. In accordance to their nature, the premise is that they provide\nboth good estimates for class overlap, and great correlations with the\nclassification performance. For that purpose, a novel family of metrics have\nbeen developed. Being based on ball coverage by classes, they are named after\nOverlap Number of Balls. Finally, some prospects for the adaptation of the\nformer family of metrics to singular (more complex) problems are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 18:21:13 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Pascual-Triana", "Jos\u00e9 Daniel", ""], ["Charte", "David", ""], ["Arroyo", "Marta Andr\u00e9s", ""], ["Fern\u00e1ndez", "Alberto", ""], ["Herrera", "Francisco", ""]]}, {"id": "2007.07967", "submitter": "Dario Malchiodi", "authors": "Giosu\\`e Cataldo Marin\\`o, Gregorio Ghidoli, Marco Frasca and Dario\n  Malchiodi", "title": "Compression strategies and space-conscious representations for deep\n  neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning have made available large, powerful\nconvolutional neural networks (CNN) with state-of-the-art performance in\nseveral real-world applications. Unfortunately, these large-sized models have\nmillions of parameters, thus they are not deployable on resource-limited\nplatforms (e.g. where RAM is limited). Compression of CNNs thereby becomes a\ncritical problem to achieve memory-efficient and possibly computationally\nfaster model representations. In this paper, we investigate the impact of lossy\ncompression of CNNs by weight pruning and quantization, and lossless weight\nmatrix representations based on source coding. We tested several combinations\nof these techniques on four benchmark datasets for classification and\nregression problems, achieving compression rates up to $165$ times, while\npreserving or improving the model performance.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 19:41:19 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Marin\u00f2", "Giosu\u00e8 Cataldo", ""], ["Ghidoli", "Gregorio", ""], ["Frasca", "Marco", ""], ["Malchiodi", "Dario", ""]]}, {"id": "2007.07979", "submitter": "Ramon Gomes da Silva", "authors": "Ramon Gomes da Silva, Matheus Henrique Dal Molin Ribeiro, Viviana\n  Cocco Mariani and Leandro dos Santos Coelho", "title": "Short-term forecasting of Amazon rainforest fires based on ensemble\n  decomposition model", "comments": "6 pages with 3 figures; Comments edited", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Accurate forecasting is important for decision-makers. Recently, the Amazon\nrainforest is reaching record levels of the number of fires, a situation that\nconcerns both climate and public health problems. Obtaining the desired\nforecasting accuracy becomes difficult and challenging. In this paper were\ndeveloped a novel heterogeneous decomposition-ensemble model by using Seasonal\nand Trend decomposition based on Loess in combination with algorithms for\nshort-term load forecasting multi-month-ahead, to explore temporal patterns of\nAmazon rainforest fires in Brazil. The results demonstrate the proposed\ndecomposition-ensemble models can provide more accurate forecasting evaluated\nby performance measures. Diebold-Mariano statistical test showed the proposed\nmodels are better than other compared models, but it is statistically equal to\none of them.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 20:21:22 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 14:48:07 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["da Silva", "Ramon Gomes", ""], ["Ribeiro", "Matheus Henrique Dal Molin", ""], ["Mariani", "Viviana Cocco", ""], ["Coelho", "Leandro dos Santos", ""]]}, {"id": "2007.07981", "submitter": "Jordi Nin", "authors": "Irene Unceta, Jordi Nin and Oriol Pujol", "title": "Differential Replication in Machine Learning", "comments": "8 pages, 1 Figure, 34 References", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When deployed in the wild, machine learning models are usually confronted\nwith data and requirements that constantly vary, either because of changes in\nthe generating distribution or because external constraints change the\nenvironment where the model operates. To survive in such an ecosystem, machine\nlearning models need to adapt to new conditions by evolving over time. The idea\nof model adaptability has been studied from different perspectives. In this\npaper, we propose a solution based on reusing the knowledge acquired by the\nalready deployed machine learning models and leveraging it to train future\ngenerations. This is the idea behind differential replication of machine\nlearning models.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 20:26:49 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Unceta", "Irene", ""], ["Nin", "Jordi", ""], ["Pujol", "Oriol", ""]]}, {"id": "2007.07985", "submitter": "Ali Siahkoohi", "authors": "Ali Siahkoohi, Gabrio Rizzuti, Philipp A. Witte, Felix J. Herrmann", "title": "Faster Uncertainty Quantification for Inverse Problems with Conditional\n  Normalizing Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.geo-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In inverse problems, we often have access to data consisting of paired\nsamples $(x,y)\\sim p_{X,Y}(x,y)$ where $y$ are partial observations of a\nphysical system, and $x$ represents the unknowns of the problem. Under these\ncircumstances, we can employ supervised training to learn a solution $x$ and\nits uncertainty from the observations $y$. We refer to this problem as the\n\"supervised\" case. However, the data $y\\sim p_{Y}(y)$ collected at one point\ncould be distributed differently than observations $y'\\sim p_{Y}'(y')$,\nrelevant for a current set of problems. In the context of Bayesian inference,\nwe propose a two-step scheme, which makes use of normalizing flows and joint\ndata to train a conditional generator $q_{\\theta}(x|y)$ to approximate the\ntarget posterior density $p_{X|Y}(x|y)$. Additionally, this preliminary phase\nprovides a density function $q_{\\theta}(x|y)$, which can be recast as a prior\nfor the \"unsupervised\" problem, e.g.~when only the observations $y'\\sim\np_{Y}'(y')$, a likelihood model $y'|x$, and a prior on $x'$ are known. We then\ntrain another invertible generator with output density $q'_{\\phi}(x|y')$\nspecifically for $y'$, allowing us to sample from the posterior\n$p_{X|Y}'(x|y')$. We present some synthetic results that demonstrate\nconsiderable training speedup when reusing the pretrained network\n$q_{\\theta}(x|y')$ as a warm start or preconditioning for approximating\n$p_{X|Y}'(x|y')$, instead of learning from scratch. This training modality can\nbe interpreted as an instance of transfer learning. This result is particularly\nrelevant for large-scale inverse problems that employ expensive numerical\nsimulations.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 20:36:30 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Siahkoohi", "Ali", ""], ["Rizzuti", "Gabrio", ""], ["Witte", "Philipp A.", ""], ["Herrmann", "Felix J.", ""]]}, {"id": "2007.08009", "submitter": "Akshay Kumar", "authors": "Akshay Kumar and Jarvis Haupt", "title": "Convexifying Sparse Interpolation with Infinitely Wide Neural Networks:\n  An Atomic Norm Approach", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": "10.1109/LSP.2020.3039479", "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work examines the problem of exact data interpolation via sparse (neuron\ncount), infinitely wide, single hidden layer neural networks with leaky\nrectified linear unit activations. Using the atomic norm framework of\n[Chandrasekaran et al., 2012], we derive simple characterizations of the convex\nhulls of the corresponding atomic sets for this problem under several different\nconstraints on the weights and biases of the network, thus obtaining equivalent\nconvex formulations for these problems. A modest extension of our proposed\nframework to a binary classification problem is also presented. We explore the\nefficacy of the resulting formulations experimentally, and compare with\nnetworks trained via gradient descent.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 21:40:51 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Kumar", "Akshay", ""], ["Haupt", "Jarvis", ""]]}, {"id": "2007.08025", "submitter": "Hakim Hafidi", "authors": "Hakim Hafidi, Mounir Ghogho, Philippe Ciblat and Ananthram Swami", "title": "GraphCL: Contrastive Self-Supervised Learning of Graph Representations", "comments": "Under review for Neurips 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Graph Contrastive Learning (GraphCL), a general framework for\nlearning node representations in a self supervised manner. GraphCL learns node\nembeddings by maximizing the similarity between the representations of two\nrandomly perturbed versions of the intrinsic features and link structure of the\nsame node's local subgraph. We use graph neural networks to produce two\nrepresentations of the same node and leverage a contrastive learning loss to\nmaximize agreement between them. In both transductive and inductive learning\nsetups, we demonstrate that our approach significantly outperforms the\nstate-of-the-art in unsupervised learning on a number of node classification\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 22:36:53 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Hafidi", "Hakim", ""], ["Ghogho", "Mounir", ""], ["Ciblat", "Philippe", ""], ["Swami", "Ananthram", ""]]}, {"id": "2007.08034", "submitter": "Anupriya Vysala", "authors": "Anupriya Vysala and Dr. Joseph Gomes", "title": "Evaluating and Validating Cluster Results", "comments": "47 pages, 9th International Conference on Data Mining & Knowledge\n  Management Process (CDKP 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is the technique to partition data according to their\ncharacteristics. Data that are similar in nature belong to the same cluster\n[1]. There are two types of evaluation methods to evaluate clustering quality.\nOne is an external evaluation where the truth labels in the data sets are known\nin advance and the other is internal evaluation in which the evaluation is done\nwith data set itself without true labels. In this paper, both external\nevaluation and internal evaluation are performed on the cluster results of the\nIRIS dataset. In the case of external evaluation Homogeneity, Correctness and\nV-measure scores are calculated for the dataset. For internal performance\nmeasures, the Silhouette Index and Sum of Square Errors are used. These\ninternal performance measures along with the dendrogram (graphical tool from\nhierarchical Clustering) are used first to validate the number of clusters.\nFinally, as a statistical tool, we used the frequency distribution method to\ncompare and provide a visual representation of the distribution of observations\nwithin a clustering result and the original data.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 23:14:48 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Vysala", "Anupriya", ""], ["Gomes", "Dr. Joseph", ""]]}, {"id": "2007.08053", "submitter": "Yu Hao", "authors": "Yu Hao, Xin Cao, Yixiang Fang, Xike Xie, Sibo Wang", "title": "Inductive Link Prediction for Nodes Having Only Attribute Information", "comments": "IJCAI2020", "journal-ref": null, "doi": "10.24963/ijcai.2020/168", "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the link between two nodes is a fundamental problem for graph data\nanalytics. In attributed graphs, both the structure and attribute information\ncan be utilized for link prediction. Most existing studies focus on\ntransductive link prediction where both nodes are already in the graph.\nHowever, many real-world applications require inductive prediction for new\nnodes having only attribute information. It is more challenging since the new\nnodes do not have structure information and cannot be seen during the model\ntraining. To solve this problem, we propose a model called DEAL, which consists\nof three components: two node embedding encoders and one alignment mechanism.\nThe two encoders aim to output the attribute-oriented node embedding and the\nstructure-oriented node embedding, and the alignment mechanism aligns the two\ntypes of embeddings to build the connections between the attributes and links.\nOur model DEAL is versatile in the sense that it works for both inductive and\ntransductive link prediction. Extensive experiments on several benchmark\ndatasets show that our proposed model significantly outperforms existing\ninductive link prediction methods, and also outperforms the state-of-the-art\nmethods on transductive link prediction.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 00:51:51 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Hao", "Yu", ""], ["Cao", "Xin", ""], ["Fang", "Yixiang", ""], ["Xie", "Xike", ""], ["Wang", "Sibo", ""]]}, {"id": "2007.08060", "submitter": "Simeon Spasov Mr", "authors": "Simeon Spasov, Alessandro Di Stefano, Pietro Lio, Jian Tang", "title": "GRADE: Graph Dynamic Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning of static and more recently dynamically evolving\ngraphs has gained noticeable attention. Existing approaches for modelling graph\ndynamics focus extensively on the evolution of individual nodes independently\nof the evolution of mesoscale community structures. As a result, current\nmethods do not provide useful tools to study and cannot explicitly capture\ntemporal community dynamics. To address this challenge, we propose GRADE - a\nprobabilistic model that learns to generate evolving node and community\nrepresentations by imposing a random walk prior over their trajectories. Our\nmodel also learns node community membership which is updated between time steps\nvia a transition matrix. At each time step link generation is performed by\nfirst assigning node membership from a distribution over the communities, and\nthen sampling a neighbor from a distribution over the nodes for the assigned\ncommunity. We parametrize the node and community distributions with neural\nnetworks and learn their parameters via variational inference. Experiments\ndemonstrate GRADE outperforms baselines in dynamic link prediction, shows\nfavourable performance on dynamic community detection, and identifies coherent\nand interpretable evolving communities.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 01:17:24 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 17:01:07 GMT"}, {"version": "v3", "created": "Mon, 10 May 2021 19:59:06 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Spasov", "Simeon", ""], ["Di Stefano", "Alessandro", ""], ["Lio", "Pietro", ""], ["Tang", "Jian", ""]]}, {"id": "2007.08063", "submitter": "Boris Rubinstein", "authors": "Boris Rubinstein", "title": "A fast noise filtering algorithm for time series prediction using\n  recurrent neural networks", "comments": "15 pages, 10 figures; typos corrected; the notation table removed; an\n  appendix added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research demonstrate that prediction of time series by recurrent\nneural networks (RNNs) based on the noisy input generates a smooth anticipated\ntrajectory. We examine the internal dynamics of RNNs and establish a set of\nconditions required for such behavior. Based on this analysis we propose a new\napproximate algorithm and show that it significantly speeds up the predictive\nprocess without loss of accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 01:32:48 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 23:44:50 GMT"}, {"version": "v3", "created": "Tue, 6 Oct 2020 14:53:56 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Rubinstein", "Boris", ""]]}, {"id": "2007.08076", "submitter": "Darshana Priyasad Madduma Kankanamalage Don Mr", "authors": "Darshana Priyasad, Tharindu Fernando, Simon Denman, Sridha Sridharan,\n  Clinton Fookes", "title": "Memory based fusion for multi-modal deep learning", "comments": "Pre-print submitted to Information Fusion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of multi-modal data for deep machine learning has shown promise when\ncompared to uni-modal approaches with fusion of multi-modal features resulting\nin improved performance in several applications. However, most state-of-the-art\nmethods use naive fusion which processes feature streams independently,\nignoring possible long-term dependencies within the data during fusion. In this\npaper, we present a novel Memory based Attentive Fusion layer, which fuses\nmodes by incorporating both the current features and longterm dependencies in\nthe data, thus allowing the model to understand the relative importance of\nmodes over time. We introduce an explicit memory block within the fusion layer\nwhich stores features containing long-term dependencies of the fused data. The\nfeature inputs from uni-modal encoders are fused through attentive composition\nand transformation followed by naive fusion of the resultant memory derived\nfeatures with layer inputs. Following state-of-the-art methods, we have\nevaluated the performance and the generalizability of the proposed fusion\napproach on two different datasets with different modalities. In our\nexperiments, we replace the naive fusion layer in benchmark networks with our\nproposed layer to enable a fair comparison. Experimental results indicate that\nthe MBAF layer can generalise across different modalities and networks to\nenhance fusion and improve performance.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 02:05:54 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 00:48:17 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 05:22:34 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Priyasad", "Darshana", ""], ["Fernando", "Tharindu", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "2007.08082", "submitter": "Yasuhiro Fujita", "authors": "Yasuhiro Fujita, Kota Uenishi, Avinash Ummadisingu, Prabhat Nagarajan,\n  Shimpei Masuda, and Mario Ynocente Castro", "title": "Distributed Reinforcement Learning of Targeted Grasping with Active\n  Vision for Mobile Manipulators", "comments": "Accepted at IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing personal robots that can perform a diverse range of manipulation\ntasks in unstructured environments necessitates solving several challenges for\nrobotic grasping systems. We take a step towards this broader goal by\npresenting the first RL-based system, to our knowledge, for a mobile\nmanipulator that can (a) achieve targeted grasping generalizing to unseen\ntarget objects, (b) learn complex grasping strategies for cluttered scenes with\noccluded objects, and (c) perform active vision through its movable wrist\ncamera to better locate objects. The system is informed of the desired target\nobject in the form of a single, arbitrary-pose RGB image of that object,\nenabling the system to generalize to unseen objects without retraining. To\nachieve such a system, we combine several advances in deep reinforcement\nlearning and present a large-scale distributed training system using\nsynchronous SGD that seamlessly scales to multi-node, multi-GPU infrastructure\nto make rapid prototyping easier. We train and evaluate our system in a\nsimulated environment, identify key components for improving performance,\nanalyze its behaviors, and transfer to a real-world setup.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 02:47:48 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 08:59:39 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Fujita", "Yasuhiro", ""], ["Uenishi", "Kota", ""], ["Ummadisingu", "Avinash", ""], ["Nagarajan", "Prabhat", ""], ["Masuda", "Shimpei", ""], ["Castro", "Mario Ynocente", ""]]}, {"id": "2007.08092", "submitter": "Rayan Krishnan", "authors": "Langston Nashold, Rayan Krishnan", "title": "Using LSTM and SARIMA Models to Forecast Cluster CPU Usage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As large scale cloud computing centers become more popular than individual\nservers, predicting future resource demand need has become an important\nproblem. Forecasting resource need allows public cloud providers to proactively\nallocate or deallocate resources for cloud services. This work seeks to predict\none resource, CPU usage, over both a short term and long term time scale.\n  To gain insight into the model characteristics that best support specific\ntasks, we consider two vastly different architectures: the historically\nrelevant SARIMA model and the more modern neural network, LSTM model. We apply\nthese models to Azure data resampled to 20 minutes per data point with the goal\nof predicting usage over the next hour for the short-term task and for the next\nthree days for the long-term task. The SARIMA model outperformed the LSTM for\nthe long term prediction task, but performed poorer on the short term task.\nFurthermore, the LSTM model was more robust, whereas the SARIMA model relied on\nthe data meeting certain assumptions about seasonality.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 03:29:13 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Nashold", "Langston", ""], ["Krishnan", "Rayan", ""]]}, {"id": "2007.08093", "submitter": "Haiping Huang", "authors": "Wenxuan Zou and Haiping Huang", "title": "Data-driven effective model shows a liquid-like deep learning", "comments": "43 pages, 19 figures, 2nd revision for the journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn cond-mat.stat-mech stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The geometric structure of an optimization landscape is argued to be\nfundamentally important to support the success of deep neural network learning.\nA direct computation of the landscape beyond two layers is hard. Therefore, to\ncapture the global view of the landscape, an interpretable model of the\nnetwork-parameter (or weight) space must be established. However, the model is\nlacking so far. Furthermore, it remains unknown what the landscape looks like\nfor deep networks of binary synapses, which plays a key role in robust and\nenergy efficient neuromorphic computation. Here, we propose a statistical\nmechanics framework by directly building a least structured model of the\nhigh-dimensional weight space, considering realistic structured data,\nstochastic gradient descent training, and the computational depth of neural\nnetworks. We also consider whether the number of network parameters outnumbers\nthe number of supplied training data, namely, over- or under-parametrization.\nOur least structured model reveals that the weight spaces of the\nunder-parametrization and over-parameterization cases belong to the same class,\nin the sense that these weight spaces are well-connected without any\nhierarchical clustering structure. In contrast, the shallow-network has a\nbroken weight space, characterized by a discontinuous phase transition, thereby\nclarifying the benefit of depth in deep learning from the angle of high\ndimensional geometry. Our effective model also reveals that inside a deep\nnetwork, there exists a liquid-like central part of the architecture in the\nsense that the weights in this part behave as randomly as possible, providing\nalgorithmic implications. Our data-driven model thus provides a statistical\nmechanics insight about why deep learning is unreasonably effective in terms of\nthe high-dimensional weight space, and how deep networks are different from\nshallow ones.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 04:02:48 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 07:36:57 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Zou", "Wenxuan", ""], ["Huang", "Haiping", ""]]}, {"id": "2007.08095", "submitter": "Xinyun Chen", "authors": "Kavi Gupta, Peter Ebert Christensen, Xinyun Chen, Dawn Song", "title": "Synthesize, Execute and Debug: Learning to Repair for Neural Program\n  Synthesis", "comments": "Published in NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of deep learning techniques has achieved significant progress for\nprogram synthesis from input-output examples. However, when the program\nsemantics become more complex, it still remains a challenge to synthesize\nprograms that are consistent with the specification. In this work, we propose\nSED, a neural program generation framework that incorporates synthesis,\nexecution, and debugging stages. Instead of purely relying on the neural\nprogram synthesizer to generate the final program, SED first produces initial\nprograms using the neural program synthesizer component, then utilizes a neural\nprogram debugger to iteratively repair the generated programs. The integration\nof the debugger component enables SED to modify the programs based on the\nexecution results and specification, which resembles the coding process of\nhuman programmers. On Karel, a challenging input-output program synthesis\nbenchmark, SED reduces the error rate of the neural program synthesizer itself\nby a considerable margin, and outperforms the standard beam search for\ndecoding.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 04:15:47 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 07:23:09 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Gupta", "Kavi", ""], ["Christensen", "Peter Ebert", ""], ["Chen", "Xinyun", ""], ["Song", "Dawn", ""]]}, {"id": "2007.08101", "submitter": "Spencer Gordon", "authors": "Spencer Gordon, Bijan Mazaheri, Leonard J. Schulman, Yuval Rabani", "title": "The Sparse Hausdorff Moment Problem, with Application to Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of identifying, from its first $m$ noisy moments, a\nprobability distribution on $[0,1]$ of support $k<\\infty$. This is equivalent\nto the problem of learning a distribution on $m$ observable binary random\nvariables $X_1,X_2,\\dots,X_m$ that are iid conditional on a hidden random\nvariable $U$ taking values in $\\{1,2,\\dots,k\\}$. Our focus is on accomplishing\nthis with $m=2k$, which is the minimum $m$ for which verifying that the source\nis a $k$-mixture is possible (even with exact statistics). This problem, so\nsimply stated, is quite useful: e.g., by a known reduction, any algorithm for\nit lifts to an algorithm for learning pure topic models.\n  We give an algorithm for identifying a $k$-mixture using samples of $m=2k$\niid binary random variables using a sample of size $\\left(1/w_{\\min}\\right)^2\n\\cdot\\left(1/\\zeta\\right)^{O(k)}$ and post-sampling runtime of only\n$O(k^{2+o(1)})$ arithmetic operations. Here $w_{\\min}$ is the minimum\nprobability of an outcome of $U$, and $\\zeta$ is the minimum separation between\nthe distinct success probabilities of the $X_i$s. Stated in terms of the moment\nproblem, it suffices to know the moments to additive accuracy\n$w_{\\min}\\cdot\\zeta^{O(k)}$. It is known that the sample complexity of any\nsolution to the identification problem must be at least exponential in $k$.\nPrevious results demonstrated either worse sample complexity and worse $O(k^c)$\nruntime for some $c$ substantially larger than $2$, or similar sample\ncomplexity and much worse $k^{O(k^2)}$ runtime.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 04:23:57 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 20:42:46 GMT"}, {"version": "v3", "created": "Mon, 7 Sep 2020 17:24:41 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Gordon", "Spencer", ""], ["Mazaheri", "Bijan", ""], ["Schulman", "Leonard J.", ""], ["Rabani", "Yuval", ""]]}, {"id": "2007.08128", "submitter": "Quanying Liu", "authors": "Xuming Ran, Mingkun Xu, Lingrui Mei, Qi Xu, Quanying Liu", "title": "Detecting Out-of-distribution Samples via Variational Auto-encoder with\n  Reliable Uncertainty Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In unsupervised learning, variational auto-encoders (VAEs) are an influential\nclass of deep generative models with rich representational power of neural\nnetworks and Bayesian methods. However, VAEs suffer from assigning higher\nlikelihood to out-of-distribution (OOD) inputs than in-distribution (ID)\ninputs. Recent studies advise that the deep generative models with reliable\nuncertainty estimation is critical to a deep understanding of OOD inputs.\nMeanwhile, noise contrastive prior (NCP) is an emerging promising method for\nobtaining uncertainty, with the advantages of easy to scale, being trainable,\nand compatibility with extensive models. Inspired by these ideas, We propose an\nimproved noise contrastive prior (INCP) to acquire reliable uncertainty\nestimate for standard VAEs. By combining INCP with the encoder of VAE, patterns\nbetween OOD and ID inputs can be well captured and distinguished. Our method\noutperforms standard VAEs on the FashionMNIST and CIFAR10 datasets. We also\ndemonstrate the preferred robustness of our model by the extensive experiments\non anomaly detection tasks.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 06:02:18 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Ran", "Xuming", ""], ["Xu", "Mingkun", ""], ["Mei", "Lingrui", ""], ["Xu", "Qi", ""], ["Liu", "Quanying", ""]]}, {"id": "2007.08129", "submitter": "Yunxiao Qin", "authors": "Yunxiao Qin, Weiguo Zhang, Zezheng Wang, Chenxu Zhao, Jingping Shi", "title": "Layer-Wise Adaptive Updating for Few-Shot Image Classification", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2020.3036348", "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot image classification (FSIC), which requires a model to recognize new\ncategories via learning from few images of these categories, has attracted lots\nof attention. Recently, meta-learning based methods have been shown as a\npromising direction for FSIC. Commonly, they train a meta-learner\n(meta-learning model) to learn easy fine-tuning weight, and when solving an\nFSIC task, the meta-learner efficiently fine-tunes itself to a task-specific\nmodel by updating itself on few images of the task. In this paper, we propose a\nnovel meta-learning based layer-wise adaptive updating (LWAU) method for FSIC.\nLWAU is inspired by an interesting finding that compared with common deep\nmodels, the meta-learner pays much more attention to update its top layer when\nlearning from few images. According to this finding, we assume that the\nmeta-learner may greatly prefer updating its top layer to updating its bottom\nlayers for better FSIC performance. Therefore, in LWAU, the meta-learner is\ntrained to learn not only the easy fine-tuning model but also its favorite\nlayer-wise adaptive updating rule to improve its learning efficiency. Extensive\nexperiments show that with the layer-wise adaptive updating rule, the proposed\nLWAU: 1) outperforms existing few-shot classification methods with a clear\nmargin; 2) learns from few images more efficiently by at least 5 times than\nexisting meta-learners when solving FSIC.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 06:02:44 GMT"}], "update_date": "2020-12-30", "authors_parsed": [["Qin", "Yunxiao", ""], ["Zhang", "Weiguo", ""], ["Wang", "Zezheng", ""], ["Zhao", "Chenxu", ""], ["Shi", "Jingping", ""]]}, {"id": "2007.08133", "submitter": "Haolin Chen", "authors": "Haolin Chen, Luis Rademacher", "title": "Overcomplete order-3 tensor decomposition, blind deconvolution and\n  Gaussian mixture models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm for tensor decomposition, based on Jennrich's\nalgorithm, and apply our new algorithmic ideas to blind deconvolution and\nGaussian mixture models. Our first contribution is a simple and efficient\nalgorithm to decompose certain symmetric overcomplete order-3 tensors, that is,\nthree dimensional arrays of the form $T = \\sum_{i=1}^n a_i \\otimes a_i \\otimes\na_i$ where the $a_i$s are not linearly independent.Our algorithm comes with a\ndetailed robustness analysis. Our second contribution builds on top of our\ntensor decomposition algorithm to expand the family of Gaussian mixture models\nwhose parameters can be estimated efficiently. These ideas are also presented\nin a more general framework of blind deconvolution that makes them applicable\nto mixture models of identical but very general distributions, including all\ncentrally symmetric distributions with finite 6th moment.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 06:23:37 GMT"}, {"version": "v2", "created": "Sat, 20 Feb 2021 00:58:35 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Chen", "Haolin", ""], ["Rademacher", "Luis", ""]]}, {"id": "2007.08137", "submitter": "Yeshwanth Cherapanamjeri", "authors": "Yeshwanth Cherapanamjeri, Efe Aras, Nilesh Tripuraneni, Michael I.\n  Jordan, Nicolas Flammarion, Peter L. Bartlett", "title": "Optimal Robust Linear Regression in Nearly Linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of high-dimensional robust linear regression where a\nlearner is given access to $n$ samples from the generative model $Y = \\langle\nX,w^* \\rangle + \\epsilon$ (with $X \\in \\mathbb{R}^d$ and $\\epsilon$\nindependent), in which an $\\eta$ fraction of the samples have been\nadversarially corrupted. We propose estimators for this problem under two\nsettings: (i) $X$ is L4-L2 hypercontractive, $\\mathbb{E} [XX^\\top]$ has bounded\ncondition number and $\\epsilon$ has bounded variance and (ii) $X$ is\nsub-Gaussian with identity second moment and $\\epsilon$ is sub-Gaussian. In\nboth settings, our estimators: (a) Achieve optimal sample complexities and\nrecovery guarantees up to log factors and (b) Run in near linear time\n($\\tilde{O}(nd / \\eta^6)$). Prior to our work, polynomial time algorithms\nachieving near optimal sample complexities were only known in the setting where\n$X$ is Gaussian with identity covariance and $\\epsilon$ is Gaussian, and no\nlinear time estimators were known for robust linear regression in any setting.\nOur estimators and their analysis leverage recent developments in the\nconstruction of faster algorithms for robust mean estimation to improve\nruntimes, and refined concentration of measure arguments alongside Gaussian\nrounding techniques to improve statistical sample complexities.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 06:44:44 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Cherapanamjeri", "Yeshwanth", ""], ["Aras", "Efe", ""], ["Tripuraneni", "Nilesh", ""], ["Jordan", "Michael I.", ""], ["Flammarion", "Nicolas", ""], ["Bartlett", "Peter L.", ""]]}, {"id": "2007.08140", "submitter": "Ron Shoham", "authors": "Ron Shoham and Haim Permuter", "title": "Amended Cross Entropy Cost: Framework For Explicit Diversity\n  Encouragement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross Entropy (CE) has an important role in machine learning and, in\nparticular, in neural networks. It is commonly used in neural networks as the\ncost between the known distribution of the label and the Softmax/Sigmoid\noutput. In this paper we present a new cost function called the Amended Cross\nEntropy (ACE). Its novelty lies in its affording the capability to train\nmultiple classifiers while explicitly controlling the diversity between them.\nWe derived the new cost by mathematical analysis and \"reverse engineering\" of\nthe way we wish the gradients to behave, and produced a tailor-made, elegant\nand intuitive cost function to achieve the desired result. This process is\nsimilar to the way that CE cost is picked as a cost function for the\nSoftmax/Sigmoid classifiers for obtaining linear derivatives. By choosing the\noptimal diversity factor we produce an ensemble which yields better results\nthan the vanilla one. We demonstrate two potential usages of this outcome, and\npresent empirical results. Our method works for classification problems\nanalogously to Negative Correlation Learning (NCL) for regression problems.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 06:50:29 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Shoham", "Ron", ""], ["Permuter", "Haim", ""]]}, {"id": "2007.08145", "submitter": "Eyke H\\\"ullermeier", "authors": "Eyke H\\\"ullermeier and Johannes F\\\"urnkranz and Eneldo Loza Mencia", "title": "Conformal Rule-Based Multi-label Classification", "comments": null, "journal-ref": "Draft of a paper presented at KI 2020, 43. German Conference on\n  Artificial Intelligence, Bamberg, Germany", "doi": "10.1007/978-3-030-58285-2_25", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We advocate the use of conformal prediction (CP) to enhance rule-based\nmulti-label classification (MLC). In particular, we highlight the mutual\nbenefit of CP and rule learning: Rules have the ability to provide natural\n(non-)conformity scores, which are required by CP, while CP suggests a way to\ncalibrate the assessment of candidate rules, thereby supporting better\npredictions and more elaborate decision making. We illustrate the potential\nusefulness of calibrated conformity scores in a case study on lazy multi-label\nrule learning.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 07:08:35 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["H\u00fcllermeier", "Eyke", ""], ["F\u00fcrnkranz", "Johannes", ""], ["Mencia", "Eneldo Loza", ""]]}, {"id": "2007.08159", "submitter": "Sandeep Madireddy", "authors": "Sandeep Madireddy, Angel Yanguas-Gil, Prasanna Balaprakash", "title": "Neuromodulated Neural Architectures with Local Error Signals for\n  Memory-Constrained Online Continual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to learn continuously from an incoming data stream without\ncatastrophic forgetting is critical for designing intelligent systems. Many\nexisting approaches to continual learning rely on stochastic gradient descent\nand its variants. However, these algorithms have to implement various\nstrategies, such as memory buffers or replay, to overcome well-known\nshortcomings of stochastic gradient descent methods in terms of stability,\ngreed, and short-term memory.\n  To that end, we develop a biologically-inspired light weight neural network\narchitecture that incorporates local learning and neuromodulation to enable\ninput processing over data streams and online learning. Next, we address the\nchallenge of hyperparameter selection for tasks that are not known in advance\nby implementing transfer metalearning: using a Bayesian optimization to explore\na design space spanning multiple local learning rules and their\nhyperparameters, we identify high performing configurations in classical single\ntask online learning and we transfer them to continual learning tasks with\ntask-similarity considerations.\n  We demonstrate the efficacy of our approach on both single task and continual\nlearning setting. For the single task learning setting, we demonstrate superior\nperformance over other local learning approaches on the MNIST, Fashion MNIST,\nand CIFAR-10 datasets. Using high performing configurations metalearned in the\nsingle task learning setting, we achieve superior continual learning\nperformance on Split-MNIST, and Split-CIFAR-10 data as compared with other\nmemory-constrained learning approaches, and match that of the state-of-the-art\nmemory-intensive replay-based approaches.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 07:41:23 GMT"}, {"version": "v2", "created": "Sat, 13 Mar 2021 19:10:17 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Madireddy", "Sandeep", ""], ["Yanguas-Gil", "Angel", ""], ["Balaprakash", "Prasanna", ""]]}, {"id": "2007.08165", "submitter": "Boqing Zhu", "authors": "Boqing Zhu, Kele Xu, Qiuqiang Kong, Huaimin Wang, Yuxing Peng", "title": "Audio Tagging by Cross Filtering Noisy Labels", "comments": "Accepted by IEEE/ACM Transactions on Audio, Speech and Language\n  Processing", "journal-ref": null, "doi": "10.1109/TASLP.2020.3008832", "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High quality labeled datasets have allowed deep learning to achieve\nimpressive results on many sound analysis tasks. Yet, it is labor-intensive to\naccurately annotate large amount of audio data, and the dataset may contain\nnoisy labels in the practical settings. Meanwhile, the deep neural networks are\nsusceptive to those incorrect labeled data because of their outstanding\nmemorization ability. In this paper, we present a novel framework, named\nCrossFilter, to combat the noisy labels problem for audio tagging. Multiple\nrepresentations (such as, Logmel and MFCC) are used as the input of our\nframework for providing more complementary information of the audio. Then,\nthough the cooperation and interaction of two neural networks, we divide the\ndataset into curated and noisy subsets by incrementally pick out the possibly\ncorrectly labeled data from the noisy data. Moreover, our approach leverages\nthe multi-task learning on curated and noisy subsets with different loss\nfunction to fully utilize the entire dataset. The noisy-robust loss function is\nemployed to alleviate the adverse effects of incorrect labels. On both the\naudio tagging datasets FSDKaggle2018 and FSDKaggle2019, empirical results\ndemonstrate the performance improvement compared with other competing\napproaches. On FSDKaggle2018 dataset, our method achieves state-of-the-art\nperformance and even surpasses the ensemble models.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 07:55:04 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Zhu", "Boqing", ""], ["Xu", "Kele", ""], ["Kong", "Qiuqiang", ""], ["Wang", "Huaimin", ""], ["Peng", "Yuxing", ""]]}, {"id": "2007.08176", "submitter": "Sangwoo Mo", "authors": "Jihoon Tack, Sangwoo Mo, Jongheon Jeong, Jinwoo Shin", "title": "CSI: Novelty Detection via Contrastive Learning on Distributionally\n  Shifted Instances", "comments": "NeurIPS 2020. First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novelty detection, i.e., identifying whether a given sample is drawn from\noutside the training distribution, is essential for reliable machine learning.\nTo this end, there have been many attempts at learning a representation\nwell-suited for novelty detection and designing a score based on such\nrepresentation. In this paper, we propose a simple, yet effective method named\ncontrasting shifted instances (CSI), inspired by the recent success on\ncontrastive learning of visual representations. Specifically, in addition to\ncontrasting a given sample with other instances as in conventional contrastive\nlearning methods, our training scheme contrasts the sample with\ndistributionally-shifted augmentations of itself. Based on this, we propose a\nnew detection score that is specific to the proposed training scheme. Our\nexperiments demonstrate the superiority of our method under various novelty\ndetection scenarios, including unlabeled one-class, unlabeled multi-class and\nlabeled multi-class settings, with various image benchmark datasets. Code and\npre-trained models are available at https://github.com/alinlab/CSI.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 08:32:56 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 08:09:43 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Tack", "Jihoon", ""], ["Mo", "Sangwoo", ""], ["Jeong", "Jongheon", ""], ["Shin", "Jinwoo", ""]]}, {"id": "2007.08199", "submitter": "Hwanjun Song", "authors": "Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, Jae-Gil Lee", "title": "Learning from Noisy Labels with Deep Neural Networks: A Survey", "comments": "If your paper is missing, contact me: ghkswns91@gmail.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has achieved remarkable success in numerous domains with help\nfrom large amounts of big data. However, the quality of data labels is a\nconcern because of the lack of high-quality labels in many real-world\nscenarios. As noisy labels severely degrade the generalization performance of\ndeep neural networks, learning from noisy labels (robust training) is becoming\nan important task in modern deep learning applications. In this survey, we\nfirst describe the problem of learning with label noise from a supervised\nlearning perspective. Next, we provide a comprehensive review of 57\nstate-of-the-art robust training methods, all of which are categorized into\nfive groups according to their methodological difference, followed by a\nsystematic comparison of six properties used to evaluate their superiority.\nSubsequently, we perform an in-depth analysis of noise rate estimation and\nsummarize the typically used evaluation methodology, including public noisy\ndatasets and evaluation metrics. Finally, we present several promising research\ndirections that can serve as a guideline for future studies. All the contents\nwill be available at https://github.com/songhwanjun/Awesome-Noisy-Labels.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 09:23:13 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 14:09:46 GMT"}, {"version": "v3", "created": "Wed, 28 Oct 2020 03:38:50 GMT"}, {"version": "v4", "created": "Mon, 5 Apr 2021 04:43:20 GMT"}, {"version": "v5", "created": "Tue, 8 Jun 2021 11:32:13 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Song", "Hwanjun", ""], ["Kim", "Minseok", ""], ["Park", "Dongmin", ""], ["Shin", "Yooju", ""], ["Lee", "Jae-Gil", ""]]}, {"id": "2007.08202", "submitter": "Yao Liu", "authors": "Yao Liu, Adith Swaminathan, Alekh Agarwal, Emma Brunskill", "title": "Provably Good Batch Reinforcement Learning Without Great Exploration", "comments": "36 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch reinforcement learning (RL) is important to apply RL algorithms to many\nhigh stakes tasks. Doing batch RL in a way that yields a reliable new policy in\nlarge domains is challenging: a new decision policy may visit states and\nactions outside the support of the batch data, and function approximation and\noptimization with limited samples can further increase the potential of\nlearning policies with overly optimistic estimates of their future performance.\nRecent algorithms have shown promise but can still be overly optimistic in\ntheir expected outcomes. Theoretical work that provides strong guarantees on\nthe performance of the output policy relies on a strong concentrability\nassumption, that makes it unsuitable for cases where the ratio between\nstate-action distributions of behavior policy and some candidate policies is\nlarge. This is because in the traditional analysis, the error bound scales up\nwith this ratio. We show that a small modification to Bellman optimality and\nevaluation back-up to take a more conservative update can have much stronger\nguarantees. In certain settings, they can find the approximately best policy\nwithin the state-action space explored by the batch data, without requiring a\npriori assumptions of concentrability. We highlight the necessity of our\nconservative update and the limitations of previous algorithms and analyses by\nillustrative MDP examples, and demonstrate an empirical comparison of our\nalgorithm and other state-of-the-art batch RL baselines in standard benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 09:25:54 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 08:48:10 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Liu", "Yao", ""], ["Swaminathan", "Adith", ""], ["Agarwal", "Alekh", ""], ["Brunskill", "Emma", ""]]}, {"id": "2007.08216", "submitter": "Carlos Eduardo Rosar Kos Lassance", "authors": "Carlos Lassance and Vincent Gripon and Gonzalo Mateos", "title": "Graph topology inference benchmarks for machine learning", "comments": "To appear in 2020 Machine Learning for Signal Processing. Code\n  available at https://github.com/cadurosar/benchmark_graphinference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs are nowadays ubiquitous in the fields of signal processing and machine\nlearning. As a tool used to express relationships between objects, graphs can\nbe deployed to various ends: I) clustering of vertices, II) semi-supervised\nclassification of vertices, III) supervised classification of graph signals,\nand IV) denoising of graph signals. However, in many practical cases graphs are\nnot explicitly available and must therefore be inferred from data. Validation\nis a challenging endeavor that naturally depends on the downstream task for\nwhich the graph is learnt. Accordingly, it has often been difficult to compare\nthe efficacy of different algorithms. In this work, we introduce several\nease-to-use and publicly released benchmarks specifically designed to reveal\nthe relative merits and limitations of graph inference methods. We also\ncontrast some of the most prominent techniques in the literature.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 09:40:32 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Lassance", "Carlos", ""], ["Gripon", "Vincent", ""], ["Mateos", "Gonzalo", ""]]}, {"id": "2007.08220", "submitter": "Luke Harries Mr", "authors": "Luke Harries, Rebekah Storan Clarke, Timothy Chapman, Swamy V. P. L.\n  N. Nallamalli, Levent Ozgur, Shuktika Jain, Alex Leung, Steve Lim, Aaron\n  Dietrich, Jos\\'e Miguel Hern\\'andez-Lobato, Tom Ellis, Cheng Zhang, Kamil\n  Ciosek", "title": "DRIFT: Deep Reinforcement Learning for Functional Software Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient software testing is essential for productive software development\nand reliable user experiences. As human testing is inefficient and expensive,\nautomated software testing is needed. In this work, we propose a Reinforcement\nLearning (RL) framework for functional software testing named DRIFT. DRIFT\noperates on the symbolic representation of the user interface. It uses\nQ-learning through Batch-RL and models the state-action value function with a\nGraph Neural Network. We apply DRIFT to testing the Windows 10 operating system\nand show that DRIFT can robustly trigger the desired software functionality in\na fully automated manner. Our experiments test the ability to perform single\nand combined tasks across different applications, demonstrating that our\nframework can efficiently test software with a large range of testing\nobjectives.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 09:46:59 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Harries", "Luke", ""], ["Clarke", "Rebekah Storan", ""], ["Chapman", "Timothy", ""], ["Nallamalli", "Swamy V. P. L. N.", ""], ["Ozgur", "Levent", ""], ["Jain", "Shuktika", ""], ["Leung", "Alex", ""], ["Lim", "Steve", ""], ["Dietrich", "Aaron", ""], ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""], ["Ellis", "Tom", ""], ["Zhang", "Cheng", ""], ["Ciosek", "Kamil", ""]]}, {"id": "2007.08229", "submitter": "PoHan Chiang", "authors": "Po-Han Chiang, Hsuan-Kung Yang, Zhang-Wei Hong and Chun-Yi Lee", "title": "Mixture of Step Returns in Bootstrapped DQN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of utilizing multi-step returns for updating value functions has\nbeen adopted in deep reinforcement learning (DRL) for a number of years.\nUpdating value functions with different backup lengths provides advantages in\ndifferent aspects, including bias and variance of value estimates, convergence\nspeed, and exploration behavior of the agent. Conventional methods such as\nTD-lambda leverage these advantages by using a target value equivalent to an\nexponential average of different step returns. Nevertheless, integrating step\nreturns into a single target sacrifices the diversity of the advantages offered\nby different step return targets. To address this issue, we propose Mixture\nBootstrapped DQN (MB-DQN) built on top of bootstrapped DQN, and uses different\nbackup lengths for different bootstrapped heads. MB-DQN enables heterogeneity\nof the target values that is unavailable in approaches relying only on a single\ntarget value. As a result, it is able to maintain the advantages offered by\ndifferent backup lengths. In this paper, we first discuss the motivational\ninsights through a simple maze environment. In order to validate the\neffectiveness of MB-DQN, we perform experiments on the Atari 2600 benchmark\nenvironments, and demonstrate the performance improvement of MB-DQN over a\nnumber of baseline methods. We further provide a set of ablation studies to\nexamine the impacts of different design configurations of MB-DQN.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 10:00:16 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Chiang", "Po-Han", ""], ["Yang", "Hsuan-Kung", ""], ["Hong", "Zhang-Wei", ""], ["Lee", "Chun-Yi", ""]]}, {"id": "2007.08233", "submitter": "Karl Thurnhofer-Hemsi", "authors": "Karl Thurnhofer-Hemsi, Ezequiel L\\'opez-Rubio, Miguel A.\n  Molina-Cabello, Kayvan Najarian", "title": "Radial basis function kernel optimization for Support Vector Machine\n  classifiers", "comments": "9 pages, 5 figures, 1 table (main paper), 8 pages, 6 figures, 2\n  tables (supplementary material). To be submitted to IEEE Transactions on\n  Neural Networks and Learning Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support Vector Machines (SVMs) are still one of the most popular and precise\nclassifiers. The Radial Basis Function (RBF) kernel has been used in SVMs to\nseparate among classes with considerable success. However, there is an\nintrinsic dependence on the initial value of the kernel hyperparameter. In this\nwork, we propose OKSVM, an algorithm that automatically learns the RBF kernel\nhyperparameter and adjusts the SVM weights simultaneously. The proposed\noptimization technique is based on a gradient descent method. We analyze the\nperformance of our approach with respect to the classical SVM for\nclassification on synthetic and real data. Experimental results show that OKSVM\nperforms better irrespective of the initial values of the RBF hyperparameter.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 10:09:15 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Thurnhofer-Hemsi", "Karl", ""], ["L\u00f3pez-Rubio", "Ezequiel", ""], ["Molina-Cabello", "Miguel A.", ""], ["Najarian", "Kayvan", ""]]}, {"id": "2007.08243", "submitter": "Bryn Elesedy", "authors": "Bryn Elesedy, Varun Kanade and Yee Whye Teh", "title": "Lottery Tickets in Linear Models: An Analysis of Iterative Magnitude\n  Pruning", "comments": "Updated for Sparsity in Neural Networks Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We analyse the pruning procedure behind the lottery ticket hypothesis\narXiv:1803.03635v5, iterative magnitude pruning (IMP), when applied to linear\nmodels trained by gradient flow. We begin by presenting sufficient conditions\non the statistical structure of the features under which IMP prunes those\nfeatures that have smallest projection onto the data. Following this, we\nexplore IMP as a method for sparse estimation.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 10:33:13 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 10:00:33 GMT"}, {"version": "v3", "created": "Mon, 5 Jul 2021 10:28:43 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Elesedy", "Bryn", ""], ["Kanade", "Varun", ""], ["Teh", "Yee Whye", ""]]}, {"id": "2007.08259", "submitter": "Ximei Wang", "authors": "Ximei Wang, Mingsheng Long, Jianmin Wang, and Michael I. Jordan", "title": "Transferable Calibration with Lower Bias and Variance in Domain\n  Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain Adaptation (DA) enables transferring a learning machine from a labeled\nsource domain to an unlabeled target one. While remarkable advances have been\nmade, most of the existing DA methods focus on improving the target accuracy at\ninference. How to estimate the predictive uncertainty of DA models is vital for\ndecision-making in safety-critical scenarios but remains the boundary to\nexplore. In this paper, we delve into the open problem of Calibration in DA,\nwhich is extremely challenging due to the coexistence of domain shift and the\nlack of target labels. We first reveal the dilemma that DA models learn higher\naccuracy at the expense of well-calibrated probabilities. Driven by this\nfinding, we propose Transferable Calibration (TransCal) to achieve more\naccurate calibration with lower bias and variance in a unified\nhyperparameter-free optimization framework. As a general post-hoc calibration\nmethod, TransCal can be easily applied to recalibrate existing DA methods. Its\nefficacy has been justified both theoretically and empirically.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 11:09:36 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 11:00:52 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Wang", "Ximei", ""], ["Long", "Mingsheng", ""], ["Wang", "Jianmin", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2007.08283", "submitter": "Gunnar K\\\"onig", "authors": "Gunnar K\\\"onig, Christoph Molnar, Bernd Bischl, Moritz Grosse-Wentrup", "title": "Relative Feature Importance", "comments": null, "journal-ref": "In Proceedings of the 2020 25th International Conference on\n  Pattern Recognition (ICPR), 9318--9325", "doi": "10.1007/978-3-030-68787-8", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Interpretable Machine Learning (IML) methods are used to gain insight into\nthe relevance of a feature of interest for the performance of a model. Commonly\nused IML methods differ in whether they consider features of interest in\nisolation, e.g., Permutation Feature Importance (PFI), or in relation to all\nremaining feature variables, e.g., Conditional Feature Importance (CFI). As\nsuch, the perturbation mechanisms inherent to PFI and CFI represent extreme\nreference points. We introduce Relative Feature Importance (RFI), a\ngeneralization of PFI and CFI that allows for a more nuanced feature importance\ncomputation beyond the PFI versus CFI dichotomy. With RFI, the importance of a\nfeature relative to any other subset of features can be assessed, including\nvariables that were not available at training time. We derive general\ninterpretation rules for RFI based on a detailed theoretical analysis of the\nimplications of relative feature relevance, and demonstrate the method's\nusefulness on simulated examples.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 12:20:22 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["K\u00f6nig", "Gunnar", ""], ["Molnar", "Christoph", ""], ["Bischl", "Bernd", ""], ["Grosse-Wentrup", "Moritz", ""]]}, {"id": "2007.08294", "submitter": "Dasol Hwang", "authors": "Dasol Hwang, Jinyoung Park, Sunyoung Kwon, Kyung-Min Kim, Jung-Woo Ha,\n  Hyunwoo J. Kim", "title": "Self-supervised Auxiliary Learning with Meta-paths for Heterogeneous\n  Graphs", "comments": "Neural Information Processing Systems (NeurIPS), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks have shown superior performance in a wide range of\napplications providing a powerful representation of graph-structured data.\nRecent works show that the representation can be further improved by auxiliary\ntasks. However, the auxiliary tasks for heterogeneous graphs, which contain\nrich semantic information with various types of nodes and edges, have less\nexplored in the literature. In this paper, to learn graph neural networks on\nheterogeneous graphs we propose a novel self-supervised auxiliary learning\nmethod using meta-paths, which are composite relations of multiple edge types.\nOur proposed method is learning to learn a primary task by predicting\nmeta-paths as auxiliary tasks. This can be viewed as a type of meta-learning.\nThe proposed method can identify an effective combination of auxiliary tasks\nand automatically balance them to improve the primary task. Our methods can be\napplied to any graph neural networks in a plug-in manner without manual\nlabeling or additional data. The experiments demonstrate that the proposed\nmethod consistently improves the performance of link prediction and node\nclassification on heterogeneous graphs.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 12:32:11 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 05:22:35 GMT"}, {"version": "v3", "created": "Tue, 18 Aug 2020 08:17:54 GMT"}, {"version": "v4", "created": "Tue, 3 Nov 2020 08:10:08 GMT"}, {"version": "v5", "created": "Mon, 8 Feb 2021 04:19:05 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Hwang", "Dasol", ""], ["Park", "Jinyoung", ""], ["Kwon", "Sunyoung", ""], ["Kim", "Kyung-Min", ""], ["Ha", "Jung-Woo", ""], ["Kim", "Hyunwoo J.", ""]]}, {"id": "2007.08322", "submitter": "Mengxin Yu", "authors": "Jianqing Fan, Zhuoran Yang, Mengxin Yu", "title": "Understanding Implicit Regularization in Over-Parameterized Nonlinear\n  Statistical Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the implicit regularization phenomenon induced by simple\noptimization algorithms in over-parameterized nonlinear statistical models.\nSpecifically, we study both vector and matrix single index models where the\nlink function is nonlinear and unknown, the signal parameter is either a sparse\nvector or a low-rank symmetric matrix, and the response variable can be\nheavy-tailed. To gain a better understanding of the role played by implicit\nregularization in the nonlinear models without excess technicality, we assume\nthat the distribution of the covariates is known a priori. For both the vector\nand matrix settings, we construct an over-parameterized least-squares loss\nfunction by employing the score function transform and a robust truncation step\ndesigned specifically for heavy-tailed data. We propose to estimate the true\nparameter by applying regularization-free gradient descent to the loss\nfunction. When the initialization is close to the origin and the stepsize is\nsufficiently small, we prove that the obtained solution achieves minimax\noptimal statistical rates of convergence in both the vector and matrix cases.\nIn particular, for the vector single index model with Gaussian covariates, our\nproposed estimator is shown to further enjoy the oracle statistical rate. Our\nresults capture the implicit regularization phenomenon in over-parameterized\nnonlinear and noisy statistical models with possibly heavy-tailed data.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 13:27:47 GMT"}, {"version": "v2", "created": "Sun, 10 Jan 2021 16:01:33 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Fan", "Jianqing", ""], ["Yang", "Zhuoran", ""], ["Yu", "Mengxin", ""]]}, {"id": "2007.08349", "submitter": "Pim de Haan", "authors": "Pim de Haan, Taco Cohen, Max Welling", "title": "Natural Graph Networks", "comments": "Published at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key requirement for graph neural networks is that they must process a graph\nin a way that does not depend on how the graph is described. Traditionally this\nhas been taken to mean that a graph network must be equivariant to node\npermutations. Here we show that instead of equivariance, the more general\nconcept of naturality is sufficient for a graph network to be well-defined,\nopening up a larger class of graph networks. We define global and local natural\ngraph networks, the latter of which are as scalable as conventional message\npassing graph neural networks while being more flexible. We give one practical\ninstantiation of a natural network on graphs which uses an equivariant message\nnetwork parameterization, yielding good performance on several benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 14:19:06 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 15:38:46 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["de Haan", "Pim", ""], ["Cohen", "Taco", ""], ["Welling", "Max", ""]]}, {"id": "2007.08377", "submitter": "Simon Bernard", "authors": "Simon Bernard, Hongliu Cao, Robert Sabourin, Laurent Heutte", "title": "Random Forest for Dissimilarity-based Multi-view Learning", "comments": "Published in Handbook of Pattern Recognition and Computer Vision,\n  2020 (preprint)", "journal-ref": null, "doi": "10.1142/9789811211072_0007", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many classification problems are naturally multi-view in the sense their data\nare described through multiple heterogeneous descriptions. For such tasks,\ndissimilarity strategies are effective ways to make the different descriptions\ncomparable and to easily merge them, by (i) building intermediate dissimilarity\nrepresentations for each view and (ii) fusing these representations by\naveraging the dissimilarities over the views. In this work, we show that the\nRandom Forest proximity measure can be used to build the dissimilarity\nrepresentations, since this measure reflects similarities between features but\nalso class membership. We then propose a Dynamic View Selection method to\nbetter combine the view-specific dissimilarity representations. This allows to\ntake a decision, on each instance to predict, with only the most relevant views\nfor that instance. Experiments are conducted on several real-world multi-view\ndatasets, and show that the Dynamic View Selection offers a significant\nimprovement in performance compared to the simple average combination and two\nstate-of-the-art static view combinations.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 14:52:52 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Bernard", "Simon", ""], ["Cao", "Hongliu", ""], ["Sabourin", "Robert", ""], ["Heutte", "Laurent", ""]]}, {"id": "2007.08387", "submitter": "Richard Combes", "authors": "Richard Combes and Mikael Touati", "title": "Solving Random Parity Games in Polynomial Time", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of solving random parity games. We prove that parity\ngames exibit a phase transition threshold above $d_P$, so that when the degree\nof the graph that defines the game has a degree $d > d_P$ then there exists a\npolynomial time algorithm that solves the game with high probability when the\nnumber of nodes goes to infinity. We further propose the SWCP (Self-Winning\nCycles Propagation) algorithm and show that, when the degree is large enough,\nSWCP solves the game with high probability. Furthermore, the complexity of SWCP\nis polynomial $O\\Big(|{\\cal V}|^2 + |{\\cal V}||{\\cal E}|\\Big)$. The design of\nSWCP is based on the threshold for the appearance of particular types of cycles\nin the players' respective subgraphs. We further show that non-sparse games can\nbe solved in time $O(|{\\cal V}|)$ with high probability, and emit a conjecture\nconcerning the hardness of the $d=2$ case.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 15:05:51 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Combes", "Richard", ""], ["Touati", "Mikael", ""]]}, {"id": "2007.08428", "submitter": "Chaitanya Devaguptapu", "authors": "Chaitanya Devaguptapu, Devansh Agarwal, Gaurav Mittal, Vineeth N\n  Balasubramanian", "title": "On Adversarial Robustness: A Neural Architecture Search perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial robustness of deep learning models has gained much traction in\nthe last few years. Various attacks and defenses are proposed to improve the\nadversarial robustness of modern-day deep learning architectures. While all\nthese approaches help improve the robustness, one promising direction for\nimproving adversarial robustness is un-explored, i.e., the complex topology of\nthe neural network architecture. In this work, we answer the following\nquestion: \"Can the complex topology of a neural network give adversarial\nrobustness without any form of adversarial training?\" empirically by\nexperimenting with different hand-crafted and NAS based architectures. Our\nfindings show that, for small-scale attacks, NAS-based architectures are more\nrobust for small-scale datasets and simple tasks than hand-crafted\narchitectures. However, as the dataset's size or the task's complexity\nincrease, hand-crafted architectures are more robust than NAS-based\narchitectures. We perform the first large scale study to understand adversarial\nrobustness purely from an architectural perspective. Our results show that\nrandom sampling in the search space of DARTS (a popular NAS method) with simple\nensembling can improve the robustness to PGD attack by nearly ~12\\%. We show\nthat NAS, which is popular for SoTA accuracy, can provide adversarial accuracy\nas a free add-on without any form of adversarial training. Our results show\nthat leveraging the power of neural network topology with methods like\nensembles can be an excellent way to achieve adversarial robustness without any\nform of adversarial training. We also introduce a metric that can be used to\ncalculate the trade-off between clean accuracy and adversarial robustness.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 16:07:10 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 14:34:28 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Devaguptapu", "Chaitanya", ""], ["Agarwal", "Devansh", ""], ["Mittal", "Gaurav", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "2007.08432", "submitter": "Stacey Truex", "authors": "Vale Tolpegin, Stacey Truex, Mehmet Emre Gursoy, and Ling Liu", "title": "Data Poisoning Attacks Against Federated Learning Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) is an emerging paradigm for distributed training of\nlarge-scale deep neural networks in which participants' data remains on their\nown devices with only model updates being shared with a central server.\nHowever, the distributed nature of FL gives rise to new threats caused by\npotentially malicious participants. In this paper, we study targeted data\npoisoning attacks against FL systems in which a malicious subset of the\nparticipants aim to poison the global model by sending model updates derived\nfrom mislabeled data. We first demonstrate that such data poisoning attacks can\ncause substantial drops in classification accuracy and recall, even with a\nsmall percentage of malicious participants. We additionally show that the\nattacks can be targeted, i.e., they have a large negative impact only on\nclasses that are under attack. We also study attack longevity in early/late\nround training, the impact of malicious participant availability, and the\nrelationships between the two. Finally, we propose a defense strategy that can\nhelp identify malicious participants in FL to circumvent poisoning attacks, and\ndemonstrate its effectiveness.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 16:16:57 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 19:10:13 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Tolpegin", "Vale", ""], ["Truex", "Stacey", ""], ["Gursoy", "Mehmet Emre", ""], ["Liu", "Ling", ""]]}, {"id": "2007.08433", "submitter": "Zhongwen Xu", "authors": "Zhongwen Xu, Hado van Hasselt, Matteo Hessel, Junhyuk Oh, Satinder\n  Singh, David Silver", "title": "Meta-Gradient Reinforcement Learning with an Objective Discovered Online", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning includes a broad family of algorithms that\nparameterise an internal representation, such as a value function or policy, by\na deep neural network. Each algorithm optimises its parameters with respect to\nan objective, such as Q-learning or policy gradient, that defines its\nsemantics. In this work, we propose an algorithm based on meta-gradient descent\nthat discovers its own objective, flexibly parameterised by a deep neural\nnetwork, solely from interactive experience with its environment. Over time,\nthis allows the agent to learn how to learn increasingly effectively.\nFurthermore, because the objective is discovered online, it can adapt to\nchanges over time. We demonstrate that the algorithm discovers how to address\nseveral important issues in RL, such as bootstrapping, non-stationarity, and\noff-policy learning. On the Atari Learning Environment, the meta-gradient\nalgorithm adapts over time to learn with greater efficiency, eventually\noutperforming the median score of a strong actor-critic baseline.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 16:17:09 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Xu", "Zhongwen", ""], ["van Hasselt", "Hado", ""], ["Hessel", "Matteo", ""], ["Oh", "Junhyuk", ""], ["Singh", "Satinder", ""], ["Silver", "David", ""]]}, {"id": "2007.08448", "submitter": "Dirk Van Der Hoeven", "authors": "Dirk van der Hoeven and Ashok Cutkosky and Haipeng Luo", "title": "Comparator-adaptive Convex Bandits", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study bandit convex optimization methods that adapt to the norm of the\ncomparator, a topic that has only been studied before for its full-information\ncounterpart. Specifically, we develop convex bandit algorithms with regret\nbounds that are small whenever the norm of the comparator is small. We first\nuse techniques from the full-information setting to develop comparator-adaptive\nalgorithms for linear bandits. Then, we extend the ideas to convex bandits with\nLipschitz or smooth loss functions, using a new single-point gradient estimator\nand carefully designed surrogate losses.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 16:33:35 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["van der Hoeven", "Dirk", ""], ["Cutkosky", "Ashok", ""], ["Luo", "Haipeng", ""]]}, {"id": "2007.08450", "submitter": "Eric Wong", "authors": "Eric Wong and J. Zico Kolter", "title": "Learning perturbation sets for robust machine learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although much progress has been made towards robust deep learning, a\nsignificant gap in robustness remains between real-world perturbations and more\nnarrowly defined sets typically studied in adversarial defenses. In this paper,\nwe aim to bridge this gap by learning perturbation sets from data, in order to\ncharacterize real-world effects for robust training and evaluation.\nSpecifically, we use a conditional generator that defines the perturbation set\nover a constrained region of the latent space. We formulate desirable\nproperties that measure the quality of a learned perturbation set, and\ntheoretically prove that a conditional variational autoencoder naturally\nsatisfies these criteria. Using this framework, our approach can generate a\nvariety of perturbations at different complexities and scales, ranging from\nbaseline spatial transformations, through common image corruptions, to lighting\nvariations. We measure the quality of our learned perturbation sets both\nquantitatively and qualitatively, finding that our models are capable of\nproducing a diverse set of meaningful perturbations beyond the limited data\nseen during training. Finally, we leverage our learned perturbation sets to\ntrain models which are empirically and certifiably robust to adversarial image\ncorruptions and adversarial lighting variations, while improving generalization\non non-adversarial data. All code and configuration files for reproducing the\nexperiments as well as pretrained model weights can be found at\nhttps://github.com/locuslab/perturbation_learning.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 16:39:54 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 13:03:48 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Wong", "Eric", ""], ["Kolter", "J. Zico", ""]]}, {"id": "2007.08459", "submitter": "Wen Sun", "authors": "Alekh Agarwal, Mikael Henaff, Sham Kakade, Wen Sun", "title": "PC-PG: Policy Cover Directed Exploration for Provable Policy Gradient\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct policy gradient methods for reinforcement learning are a successful\napproach for a variety of reasons: they are model free, they directly optimize\nthe performance metric of interest, and they allow for richly parameterized\npolicies. Their primary drawback is that, by being local in nature, they fail\nto adequately explore the environment. In contrast, while model-based\napproaches and Q-learning directly handle exploration through the use of\noptimism, their ability to handle model misspecification and function\napproximation is far less evident. This work introduces the the Policy\nCover-Policy Gradient (PC-PG) algorithm, which provably balances the\nexploration vs. exploitation tradeoff using an ensemble of learned policies\n(the policy cover). PC-PG enjoys polynomial sample complexity and run time for\nboth tabular MDPs and, more generally, linear MDPs in an infinite dimensional\nRKHS. Furthermore, PC-PG also has strong guarantees under model\nmisspecification that go beyond the standard worst case $\\ell_{\\infty}$\nassumptions; this includes approximation guarantees for state aggregation under\nan average case error assumption, along with guarantees under a more general\nassumption where the approximation error under distribution shift is\ncontrolled. We complement the theory with empirical evaluation across a variety\nof domains in both reward-free and reward-driven settings.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 16:57:41 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 17:59:40 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Agarwal", "Alekh", ""], ["Henaff", "Mikael", ""], ["Kakade", "Sham", ""], ["Sun", "Wen", ""]]}, {"id": "2007.08473", "submitter": "Julian Bitterwolf", "authors": "Julian Bitterwolf, Alexander Meinke and Matthias Hein", "title": "Certifiably Adversarially Robust Detection of Out-of-Distribution Data", "comments": "Published and presented at NeurIPS 2020. Code available at\n  https://gitlab.com/Bitterwolf/GOOD v3: added missing acknowledgement", "journal-ref": "Advances in Neural Information Processing Systems 33 (NeurIPS\n  2020)", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are known to be overconfident when applied to\nout-of-distribution (OOD) inputs which clearly do not belong to any class. This\nis a problem in safety-critical applications since a reliable assessment of the\nuncertainty of a classifier is a key property, allowing the system to trigger\nhuman intervention or to transfer into a safe state. In this paper, we aim for\ncertifiable worst case guarantees for OOD detection by enforcing not only low\nconfidence at the OOD point but also in an $l_\\infty$-ball around it. For this\npurpose, we use interval bound propagation (IBP) to upper bound the maximal\nconfidence in the $l_\\infty$-ball and minimize this upper bound during training\ntime. We show that non-trivial bounds on the confidence for OOD data\ngeneralizing beyond the OOD dataset seen at training time are possible.\nMoreover, in contrast to certified adversarial robustness which typically comes\nwith significant loss in prediction performance, certified guarantees for worst\ncase OOD detection are possible without much loss in accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 17:16:47 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 16:12:57 GMT"}, {"version": "v3", "created": "Wed, 10 Mar 2021 15:55:00 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Bitterwolf", "Julian", ""], ["Meinke", "Alexander", ""], ["Hein", "Matthias", ""]]}, {"id": "2007.08477", "submitter": "Min-hwan Oh", "authors": "Min-hwan Oh, Garud Iyengar, Assaf Zeevi", "title": "Sparsity-Agnostic Lasso Bandit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We consider a stochastic contextual bandit problem where the dimension $d$ of\nthe feature vectors is potentially large, however, only a sparse subset of\nfeatures of cardinality $s_0 \\ll d$ affect the reward function. Essentially all\nexisting algorithms for sparse bandits require a priori knowledge of the value\nof the sparsity index $s_0$. This knowledge is almost never available in\npractice, and misspecification of this parameter can lead to severe\ndeterioration in the performance of existing methods. The main contribution of\nthis paper is to propose an algorithm that does not require prior knowledge of\nthe sparsity index $s_0$ and establish tight regret bounds on its performance\nunder mild conditions. We also comprehensively evaluate our proposed algorithm\nnumerically and show that it consistently outperforms existing methods, even\nwhen the correct sparsity index is revealed to them but is kept hidden from our\nalgorithm.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 17:24:12 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 06:04:34 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Oh", "Min-hwan", ""], ["Iyengar", "Garud", ""], ["Zeevi", "Assaf", ""]]}, {"id": "2007.08479", "submitter": "Eric Zhao", "authors": "Eric Zhao, Anqi Liu, Animashree Anandkumar, Yisong Yue", "title": "Active Learning under Label Shift", "comments": "18 pages, 9 figures, to appear at the 2021 International Conference\n  on Artificial Intelligence and Statistics (AIStats)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of active learning under label shift: when the class\nproportions of source and target domains differ. We introduce a \"medial\ndistribution\" to incorporate a tradeoff between importance weighting and\nclass-balanced sampling and propose their combined usage in active learning.\nOur method is known as Mediated Active Learning under Label Shift (MALLS). It\nbalances the bias from class-balanced sampling and the variance from importance\nweighting. We prove sample complexity and generalization guarantees for MALLS\nwhich show active learning reduces asymptotic sample complexity even under\narbitrary label shift. We empirically demonstrate MALLS scales to\nhigh-dimensional datasets and can reduce the sample complexity of active\nlearning by 60% in deep active learning tasks.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 17:30:02 GMT"}, {"version": "v2", "created": "Sun, 6 Dec 2020 23:52:58 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2021 20:38:03 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Zhao", "Eric", ""], ["Liu", "Anqi", ""], ["Anandkumar", "Animashree", ""], ["Yue", "Yisong", ""]]}, {"id": "2007.08483", "submitter": "Ekaterina Lobacheva Ms", "authors": "Ekaterina Lobacheva, Nadezhda Chirkova, Maxim Kodryan, Dmitry Vetrov", "title": "On Power Laws in Deep Ensembles", "comments": "Published in NeurIPS 2020 and Workshop on Uncertainty and Robustness\n  in Deep Learning at ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensembles of deep neural networks are known to achieve state-of-the-art\nperformance in uncertainty estimation and lead to accuracy improvement. In this\nwork, we focus on a classification problem and investigate the behavior of both\nnon-calibrated and calibrated negative log-likelihood (CNLL) of a deep ensemble\nas a function of the ensemble size and the member network size. We indicate the\nconditions under which CNLL follows a power law w.r.t. ensemble size or member\nnetwork size, and analyze the dynamics of the parameters of the discovered\npower laws. Our important practical finding is that one large network may\nperform worse than an ensemble of several medium-size networks with the same\ntotal number of parameters (we call this ensemble a memory split). Using the\ndetected power law-like dependencies, we can predict (1) the possible gain from\nthe ensembling of networks with given structure, (2) the optimal memory split\ngiven a memory budget, based on a relatively small number of trained networks.\n  We describe the memory split advantage effect in more details in\narXiv:2005.07292\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 17:35:32 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 13:19:55 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Lobacheva", "Ekaterina", ""], ["Chirkova", "Nadezhda", ""], ["Kodryan", "Maxim", ""], ["Vetrov", "Dmitry", ""]]}, {"id": "2007.08489", "submitter": "Andrew Ilyas", "authors": "Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, Aleksander\n  Madry", "title": "Do Adversarially Robust ImageNet Models Transfer Better?", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning is a widely-used paradigm in deep learning, where models\npre-trained on standard datasets can be efficiently adapted to downstream\ntasks. Typically, better pre-trained models yield better transfer results,\nsuggesting that initial accuracy is a key aspect of transfer learning\nperformance. In this work, we identify another such aspect: we find that\nadversarially robust models, while less accurate, often perform better than\ntheir standard-trained counterparts when used for transfer learning.\nSpecifically, we focus on adversarially robust ImageNet classifiers, and show\nthat they yield improved accuracy on a standard suite of downstream\nclassification tasks. Further analysis uncovers more differences between robust\nand standard models in the context of transfer learning. Our results are\nconsistent with (and in fact, add to) recent hypotheses stating that robustness\nleads to improved feature representations. Our code and models are available at\nhttps://github.com/Microsoft/robust-models-transfer .\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 17:42:40 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 01:56:10 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Salman", "Hadi", ""], ["Ilyas", "Andrew", ""], ["Engstrom", "Logan", ""], ["Kapoor", "Ashish", ""], ["Madry", "Aleksander", ""]]}, {"id": "2007.08491", "submitter": "Fernando Andreotti", "authors": "Fernando Andreotti, Frank S. Heldt, Basel Abu-Jamous, Ming Li, Avelino\n  Javer, Oliver Carr, Stojan Jovanovic, Nadezda Lipunova, Benjamin Irving,\n  Rabia T. Khan, Robert D\\\"urichen", "title": "Prediction of the onset of cardiovascular diseases from electronic\n  health records using multi-task gated recurrent units", "comments": "5 pages, 2 figures, 2 tables, submitted at Healthcare Systems,\n  Population Health, and the Role of Health-Tech - ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a multi-task recurrent neural network with attention\nmechanism for predicting cardiovascular events from electronic health records\n(EHRs) at different time horizons. The proposed approach is compared to a\nstandard clinical risk predictor (QRISK) and machine learning alternatives\nusing 5-year data from a NHS Foundation Trust. The proposed model outperforms\nstandard clinical risk scores in predicting stroke (AUC=0.85) and myocardial\ninfarction (AUC=0.89), considering the largest time horizon. Benefit of using\nan \\gls{mt} setting becomes visible for very short time horizons, which results\nin an AUC increase between 2-6%. Further, we explored the importance of\nindividual features and attention weights in predicting cardiovascular events.\nOur results indicate that the recurrent neural network approach benefits from\nthe hospital longitudinal information and demonstrates how machine learning\ntechniques can be applied to secondary care.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 17:43:13 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Andreotti", "Fernando", ""], ["Heldt", "Frank S.", ""], ["Abu-Jamous", "Basel", ""], ["Li", "Ming", ""], ["Javer", "Avelino", ""], ["Carr", "Oliver", ""], ["Jovanovic", "Stojan", ""], ["Lipunova", "Nadezda", ""], ["Irving", "Benjamin", ""], ["Khan", "Rabia T.", ""], ["D\u00fcrichen", "Robert", ""]]}, {"id": "2007.08506", "submitter": "Ari Seff", "authors": "Ari Seff, Yaniv Ovadia, Wenda Zhou, Ryan P. Adams", "title": "SketchGraphs: A Large-Scale Dataset for Modeling Relational Geometry in\n  Computer-Aided Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric computer-aided design (CAD) is the dominant paradigm in mechanical\nengineering for physical design. Distinguished by relational geometry,\nparametric CAD models begin as two-dimensional sketches consisting of geometric\nprimitives (e.g., line segments, arcs) and explicit constraints between them\n(e.g., coincidence, perpendicularity) that form the basis for three-dimensional\nconstruction operations. Training machine learning models to reason about and\nsynthesize parametric CAD designs has the potential to reduce design time and\nenable new design workflows. Additionally, parametric CAD designs can be viewed\nas instances of constraint programming and they offer a well-scoped test bed\nfor exploring ideas in program synthesis and induction. To facilitate this\nresearch, we introduce SketchGraphs, a collection of 15 million sketches\nextracted from real-world CAD models coupled with an open-source data\nprocessing pipeline. Each sketch is represented as a geometric constraint graph\nwhere edges denote designer-imposed geometric relationships between primitives,\nthe nodes of the graph. We demonstrate and establish benchmarks for two use\ncases of the dataset: generative modeling of sketches and conditional\ngeneration of likely constraints given unconstrained geometry.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 17:56:25 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Seff", "Ari", ""], ["Ovadia", "Yaniv", ""], ["Zhou", "Wenda", ""], ["Adams", "Ryan P.", ""]]}, {"id": "2007.08520", "submitter": "Wenjie Wan", "authors": "Wenjie Wan, Zhaodi Zhang, Yiwei Zhu, Min Zhang, Fu Song", "title": "Accelerating Robustness Verification of Deep Neural Networks Guided by\n  Target Labels", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have become key components of many\nsafety-critical applications such as autonomous driving and medical diagnosis.\nHowever, DNNs have been shown suffering from poor robustness because of their\nsusceptibility to adversarial examples such that small perturbations to an\ninput result in misprediction. Addressing to this concern, various approaches\nhave been proposed to formally verify the robustness of DNNs. Most of these\napproaches reduce the verification problem to optimization problems of\nsearching an adversarial example for a given input so that it is not correctly\nclassified to the original label. However, they are limited in accuracy and\nscalability. In this paper, we propose a novel approach that can accelerate the\nrobustness verification techniques by guiding the verification with target\nlabels. The key insight of our approach is that the robustness verification\nproblem of DNNs can be solved by verifying sub-problems of DNNs, one per target\nlabel. Fixing the target label during verification can drastically reduce the\nsearch space and thus improve the efficiency. We also propose an approach by\nleveraging symbolic interval propagation and linear relaxation techniques to\nsort the target labels in terms of chances that adversarial examples exist.\nThis often allows us to quickly falsify the robustness of DNNs and the\nverification for remaining target labels could be avoided. Our approach is\northogonal to, and can be integrated with, many existing verification\ntechniques. For evaluation purposes, we integrate it with three recent\npromising DNN verification tools, i.e., MipVerify, DeepZ, and Neurify.\nExperimental results show that our approach can significantly improve these\ntools by 36X speedup when the perturbation distance is set in a reasonable\nrange.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 00:51:52 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 00:04:38 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Wan", "Wenjie", ""], ["Zhang", "Zhaodi", ""], ["Zhu", "Yiwei", ""], ["Zhang", "Min", ""], ["Song", "Fu", ""]]}, {"id": "2007.08557", "submitter": "Lili Mou", "authors": "Jingjing Li, Zichao Li, Lili Mou, Xin Jiang, Michael R. Lyu, Irwin\n  King", "title": "Unsupervised Text Generation by Learning from Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present TGLS, a novel framework to unsupervised Text\nGeneration by Learning from Search. We start by applying a strong search\nalgorithm (in particular, simulated annealing) towards a heuristically defined\nobjective that (roughly) estimates the quality of sentences. Then, a\nconditional generative model learns from the search results, and meanwhile\nsmooth out the noise of search. The alternation between search and learning can\nbe repeated for performance bootstrapping. We demonstrate the effectiveness of\nTGLS on two real-world natural language generation tasks, paraphrase generation\nand text formalization. Our model significantly outperforms unsupervised\nbaseline methods in both tasks. Especially, it achieves comparable performance\nwith the state-of-the-art supervised methods in paraphrase generation.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 04:34:48 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Li", "Jingjing", ""], ["Li", "Zichao", ""], ["Mou", "Lili", ""], ["Jiang", "Xin", ""], ["Lyu", "Michael R.", ""], ["King", "Irwin", ""]]}, {"id": "2007.08561", "submitter": "Zhiyuan Liu", "authors": "Zhiyuan Liu, Huazheng Wang, Bo Waggoner, Youjian (Eugene) Liu, Lijun\n  Chen", "title": "A Smoothed Analysis of Online Lasso for the Sparse Linear Contextual\n  Bandit Problem", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the sparse linear contextual bandit problem where the\nparameter $\\theta$ is sparse. To relieve the sampling inefficiency, we utilize\nthe \"perturbed adversary\" where the context is generated adversarilly but with\nsmall random non-adaptive perturbations. We prove that the simple online Lasso\nsupports sparse linear contextual bandit with regret bound\n$\\mathcal{O}(\\sqrt{kT\\log d})$ even when $d \\gg T$ where $k$ and $d$ are the\nnumber of effective and ambient dimension, respectively. Compared to the recent\nwork from Sivakumar et al. (2020), our analysis does not rely on the\nprecondition processing, adaptive perturbation (the adaptive perturbation\nviolates the i.i.d perturbation setting) or truncation on the error set.\nMoreover, the special structures in our results explicitly characterize how the\nperturbation affects exploration length, guide the design of perturbation\ntogether with the fundamental performance limit of perturbation method.\nNumerical experiments are provided to complement the theoretical analysis.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 18:48:45 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Liu", "Zhiyuan", "", "Eugene"], ["Wang", "Huazheng", "", "Eugene"], ["Waggoner", "Bo", "", "Eugene"], ["Youjian", "", "", "Eugene"], ["Liu", "", ""], ["Chen", "Lijun", ""]]}, {"id": "2007.08569", "submitter": "Daniele Durante", "authors": "Sirio Legramanti, Tommaso Rigon, Daniele Durante and David B. Dunson", "title": "Extended Stochastic Block Models with Application to Criminal Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliably learning group structure among nodes in network data is challenging\nin modern applications. We are motivated by covert networks encoding\nrelationships among criminals. These data are subject to measurement errors and\nexhibit a complex combination of an unknown number of core-periphery,\nassortative and disassortative structures that may unveil the internal\narchitecture of the criminal organization. The coexistence of such noisy block\nstructures limits the reliability of community detection algorithms routinely\napplied to criminal networks, and requires extensions of model-based solutions\nto realistically characterize the node partition process, incorporate\ninformation from node attributes, and provide improved strategies for\nestimation, uncertainty quantification, model selection and prediction. To\naddress these goals, we develop a novel class of extended stochastic block\nmodels (ESBM) that infer groups of nodes having common connectivity patterns\nvia Gibbs-type priors on the partition process. This choice encompasses several\nrealistic priors for criminal networks, covering solutions with fixed, random\nand infinite number of possible groups, and facilitates inclusion of node\nattributes in a principled manner. Among the new alternatives in our class, we\nfocus on the Gnedin process as a realistic prior that allows the number of\ngroups to be finite, random and subject to a reinforcement process coherent\nwith the modular structures in organized crime. A collapsed Gibbs sampler is\nproposed for the whole ESBM class, and refined strategies for estimation,\nprediction, uncertainty quantification and model selection are outlined. ESBM\nperformance is illustrated in realistic simulations and in an application to an\nItalian Mafia network, where we learn key block patterns revealing a complex\nhierarchical structure of the organization, mostly hidden from state-of-the-art\nalternative solutions.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 19:06:16 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 11:51:40 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Legramanti", "Sirio", ""], ["Rigon", "Tommaso", ""], ["Durante", "Daniele", ""], ["Dunson", "David B.", ""]]}, {"id": "2007.08584", "submitter": "Joseph Suk", "authors": "Joseph Suk and Samory Kpotufe", "title": "Self-Tuning Bandits over Unknown Covariate-Shifts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bandits with covariates, a.k.a. contextual bandits, address situations where\noptimal actions (or arms) at a given time $t$, depend on a context $x_t$, e.g.,\na new patient's medical history, a consumer's past purchases. While it is\nunderstood that the distribution of contexts might change over time, e.g., due\nto seasonalities, or deployment to new environments, the bulk of studies\nconcern the most adversarial such changes, resulting in regret bounds that are\noften worst-case in nature.\n  Covariate-shift on the other hand has been considered in classification as a\nmiddle-ground formalism that can capture mild to relatively severe changes in\ndistributions. We consider nonparametric bandits under such middle-ground\nscenarios, and derive new regret bounds that tightly capture a continuum of\nchanges in context distribution. Furthermore, we show that these rates can be\nadaptively attained without knowledge of the time of shift nor the amount of\nshift.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 19:40:16 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 06:42:45 GMT"}, {"version": "v3", "created": "Wed, 16 Dec 2020 05:52:49 GMT"}, {"version": "v4", "created": "Sun, 21 Feb 2021 04:40:44 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Suk", "Joseph", ""], ["Kpotufe", "Samory", ""]]}, {"id": "2007.08601", "submitter": "Sushant Veer", "authors": "Sushant Veer, Anirudha Majumdar", "title": "CoNES: Convex Natural Evolutionary Strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel algorithm -- convex natural evolutionary strategies\n(CoNES) -- for optimizing high-dimensional blackbox functions by leveraging\ntools from convex optimization and information geometry. CoNES is formulated as\nan efficiently-solvable convex program that adapts the evolutionary strategies\n(ES) gradient estimate to promote rapid convergence. The resulting algorithm is\ninvariant to the parameterization of the belief distribution. Our numerical\nresults demonstrate that CoNES vastly outperforms conventional blackbox\noptimization methods on a suite of functions used for benchmarking blackbox\noptimizers. Furthermore, CoNES demonstrates the ability to converge faster than\nconventional blackbox methods on a selection of OpenAI's MuJoCo reinforcement\nlearning tasks for locomotion.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 19:59:57 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 04:31:46 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Veer", "Sushant", ""], ["Majumdar", "Anirudha", ""]]}, {"id": "2007.08616", "submitter": "Abhiram Iyer", "authors": "Abhiram Iyer, Aravind Mahadevan", "title": "Collision Avoidance Robotics Via Meta-Learning (CARML)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach to exploring a multi-objective reinforcement\nlearning problem with Model-Agnostic Meta-Learning. The environment we used\nconsists of a 2D vehicle equipped with a LIDAR sensor. The goal of the\nenvironment is to reach some pre-determined target location but also\neffectively avoid any obstacles it may find along its path. We also compare\nthis approach against a baseline TD3 solution that attempts to solve the same\nproblem.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 20:31:34 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Iyer", "Abhiram", ""], ["Mahadevan", "Aravind", ""]]}, {"id": "2007.08620", "submitter": "Sylvain Le Corff", "authors": "Alice Martin (CMAP, IP Paris, CITI, TIPIC-SAMOVAR), Charles Ollion\n  (CMAP), Florian Strub, Sylvain Le Corff (IP Paris, CITI, TIPIC-SAMOVAR),\n  Olivier Pietquin", "title": "The Monte Carlo Transformer: a stochastic self-attention model for\n  sequence prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the Sequential Monte Carlo Transformer, an original\napproach that naturally captures the observations distribution in a transformer\narchitecture. The keys, queries, values and attention vectors of the network\nare considered as the unobserved stochastic states of its hidden structure.\nThis generative model is such that at each time step the received observation\nis a random function of its past states in a given attention window. In this\ngeneral state-space setting, we use Sequential Monte Carlo methods to\napproximate the posterior distributions of the states given the observations,\nand to estimate the gradient of the log-likelihood. We hence propose a\ngenerative model giving a predictive distribution, instead of a single-point\nestimate.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 10:01:48 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 14:27:22 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Martin", "Alice", "", "CMAP, IP Paris, CITI, TIPIC-SAMOVAR"], ["Ollion", "Charles", "", "CMAP"], ["Strub", "Florian", "", "IP Paris, CITI, TIPIC-SAMOVAR"], ["Corff", "Sylvain Le", "", "IP Paris, CITI, TIPIC-SAMOVAR"], ["Pietquin", "Olivier", ""]]}, {"id": "2007.08649", "submitter": "Christoph Heindl", "authors": "Christoph Heindl", "title": "Graph Neural Networks for Node-Level Predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep learning has revolutionized many fields of research\nincluding areas of computer vision, text and speech processing. Enormous\nresearch efforts have led to numerous methods that are capable of efficiently\nanalyzing data, especially in the Euclidean space. However, many problems are\nposed in non-Euclidean domains modeled as general graphs with complex\nconnection patterns. Increased problem complexity and computational power\nconstraints have limited early approaches to static and small-sized graphs. In\nrecent years, a rising interest in machine learning on graph-structured data\nhas been accompanied by improved methods that overcome the limitations of their\npredecessors. These methods paved the way for dealing with large-scale and\ntime-dynamic graphs. This work aims to provide an overview of early and modern\ngraph neural network based machine learning methods for node-level prediction\ntasks. Under the umbrella of taxonomies already established in the literature,\nwe explain the core concepts and provide detailed explanations for\nconvolutional methods that have had strong impact. In addition, we introduce\ncommon benchmarks and present selected applications from various areas.\nFinally, we discuss open problems for further research.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 11:57:03 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Heindl", "Christoph", ""]]}, {"id": "2007.08652", "submitter": "Kalpdrum Passi", "authors": "Parth Patel, Kalpdrum Passi, Chakresh Kumar Jain", "title": "Prediction of Cancer Microarray and DNA Methylation Data using\n  Non-negative Matrix Factorization", "comments": "9th International Conference on Data Mining & Knowledge Management\n  Process (CDKP 2020)", "journal-ref": null, "doi": "10.5121/csit.2020.100906", "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years, there has been a considerable spread of microarray\ntechnology in many biological patterns, particularly in those pertaining to\ncancer diseases like leukemia, prostate, colon cancer, etc. The primary\nbottleneck that one experiences in the proper understanding of such datasets\nlies in their dimensionality, and thus for an efficient and effective means of\nstudying the same, a reduction in their dimension to a large extent is deemed\nnecessary. This study is a bid to suggesting different algorithms and\napproaches for the reduction of dimensionality of such microarray datasets.\nThis study exploits the matrix-like structure of such microarray data and uses\na popular technique called Non-Negative Matrix Factorization (NMF) to reduce\nthe dimensionality, primarily in the field of biological data. Classification\naccuracies are then compared for these algorithms. This technique gives an\naccuracy of 98%.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 15:45:08 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Patel", "Parth", ""], ["Passi", "Kalpdrum", ""], ["Jain", "Chakresh Kumar", ""]]}, {"id": "2007.08657", "submitter": "Dominik \\.Zurek", "authors": "Dominik \\.Zurek and Marcin Pietro\\'n", "title": "Training with reduced precision of a support vector machine model for\n  text classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the impact of using quantization on the efficiency of\nmulti-class text classification in the training process of a support vector\nmachine (SVM). This work is focused on comparing the efficiency of SVM model\ntrained using reduced precision with its original form. The main advantage of\nusing quantization is decrease in computation time and in memory footprint on\nthe dedicated hardware platform which supports low precision computation like\nGPU (16-bit) or FPGA (any bit-width). The paper presents the impact of a\nprecision reduction of the SVM training process on text classification\naccuracy. The implementation of the CPU was performed using the OpenMP library.\nAdditionally, the results of the implementation of the GPU using double, single\nand half precision are presented.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 11:59:30 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["\u017burek", "Dominik", ""], ["Pietro\u0144", "Marcin", ""]]}, {"id": "2007.08658", "submitter": "Federico Errica", "authors": "Federico Errica, Marco Giulini, Davide Bacciu, Roberto Menichetti,\n  Alessio Micheli, Raffaello Potestio", "title": "Accelerating the identification of informative reduced representations\n  of proteins with deep learning for graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cond-mat.soft cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The limits of molecular dynamics (MD) simulations of macromolecules are\nsteadily pushed forward by the relentless developments of computer\narchitectures and algorithms. This explosion in the number and extent (in size\nand time) of MD trajectories induces the need of automated and transferable\nmethods to rationalise the raw data and make quantitative sense out of them.\nRecently, an algorithmic approach was developed by some of us to identify the\nsubset of a protein's atoms, or mapping, that enables the most informative\ndescription of it. This method relies on the computation, for a given reduced\nrepresentation, of the associated mapping entropy, that is, a measure of the\ninformation loss due to the simplification. Albeit relatively straightforward,\nthis calculation can be time consuming. Here, we describe the implementation of\na deep learning approach aimed at accelerating the calculation of the mapping\nentropy. The method relies on deep graph networks, which provide extreme\nflexibility in the input format. We show that deep graph networks are accurate\nand remarkably efficient, with a speedup factor as large as $10^5$ with respect\nto the algorithmic computation of the mapping entropy. Applications of this\nmethod, which entails a great potential in the study of biomolecules when used\nto reconstruct its mapping entropy landscape, reach much farther than this,\nbeing the scheme easily transferable to the computation of arbitrary functions\nof a molecule's structure.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 21:22:27 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Errica", "Federico", ""], ["Giulini", "Marco", ""], ["Bacciu", "Davide", ""], ["Menichetti", "Roberto", ""], ["Micheli", "Alessio", ""], ["Potestio", "Raffaello", ""]]}, {"id": "2007.08663", "submitter": "Christopher Morris", "authors": "Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting,\n  Petra Mutzel, Marion Neumann", "title": "TUDataset: A collection of benchmark datasets for learning with graphs", "comments": "ICML 2020 workshop \"Graph Representation Learning and Beyond\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been an increasing interest in (supervised) learning with\ngraph data, especially using graph neural networks. However, the development of\nmeaningful benchmark datasets and standardized evaluation procedures is\nlagging, consequently hindering advancements in this area. To address this, we\nintroduce the TUDataset for graph classification and regression. The collection\nconsists of over 120 datasets of varying sizes from a wide range of\napplications. We provide Python-based data loaders, kernel and graph neural\nnetwork baseline implementations, and evaluation tools. Here, we give an\noverview of the datasets, standardized evaluation procedures, and provide\nbaseline experiments. All datasets are available at www.graphlearning.io. The\nexperiments are fully reproducible from the code available at\nwww.github.com/chrsmrrs/tudataset.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 21:46:33 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Morris", "Christopher", ""], ["Kriege", "Nils M.", ""], ["Bause", "Franka", ""], ["Kersting", "Kristian", ""], ["Mutzel", "Petra", ""], ["Neumann", "Marion", ""]]}, {"id": "2007.08668", "submitter": "{\\L}ukasz Dudziak", "authors": "{\\L}ukasz Dudziak, Thomas Chau, Mohamed S. Abdelfattah, Royson Lee,\n  Hyeji Kim, Nicholas D. Lane", "title": "BRP-NAS: Prediction-based NAS using GCNs", "comments": "Published at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search (NAS) enables researchers to automatically explore\nbroad design spaces in order to improve efficiency of neural networks. This\nefficiency is especially important in the case of on-device deployment, where\nimprovements in accuracy should be balanced out with computational demands of a\nmodel. In practice, performance metrics of model are computationally expensive\nto obtain. Previous work uses a proxy (e.g., number of operations) or a\nlayer-wise measurement of neural network layers to estimate end-to-end hardware\nperformance but the imprecise prediction diminishes the quality of NAS. To\naddress this problem, we propose BRP-NAS, an efficient hardware-aware NAS\nenabled by an accurate performance predictor-based on graph convolutional\nnetwork (GCN). What is more, we investigate prediction quality on different\nmetrics and show that sample efficiency of the predictor-based NAS can be\nimproved by considering binary relations of models and an iterative data\nselection strategy. We show that our proposed method outperforms all prior\nmethods on NAS-Bench-101 and NAS-Bench-201, and that our predictor can\nconsistently learn to extract useful features from the DARTS search space,\nimproving upon the second-order baseline. Finally, to raise awareness of the\nfact that accurate latency estimation is not a trivial task, we release\nLatBench -- a latency dataset of NAS-Bench-201 models running on a broad range\nof devices.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 21:58:43 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 22:03:08 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 21:05:03 GMT"}, {"version": "v4", "created": "Tue, 19 Jan 2021 17:29:16 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Dudziak", "\u0141ukasz", ""], ["Chau", "Thomas", ""], ["Abdelfattah", "Mohamed S.", ""], ["Lee", "Royson", ""], ["Kim", "Hyeji", ""], ["Lane", "Nicholas D.", ""]]}, {"id": "2007.08687", "submitter": "Isadora Cardoso-Pereira", "authors": "I. Cardoso-Pereira, J. B. Borges, P. H. Barros, A. F. Loureiro, O. A.\n  Rosso, H. S. Ramos", "title": "Leveraging the Self-Transition Probability of Ordinal Pattern Transition\n  Graph for Transportation Mode Classification", "comments": null, "journal-ref": null, "doi": "10.5753/sbrc.2019.7391", "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The analysis of GPS trajectories is a well-studied problem in Urban Computing\nand has been used to track people. Analyzing people mobility and identifying\nthe transportation mode used by them is essential for cities that want to\nreduce traffic jams and travel time between their points, thus helping to\nimprove the quality of life of citizens. The trajectory data of a moving object\nis represented by a discrete collection of points through time, i.e., a time\nseries. Regarding its interdisciplinary and broad scope of real-world\napplications, it is evident the need of extracting knowledge from time series\ndata. Mining this type of data, however, faces several complexities due to its\nunique properties. Different representations of data may overcome this. In this\nwork, we propose the use of a feature retained from the Ordinal Pattern\nTransition Graph, called the probability of self-transition for transportation\nmode classification. The proposed feature presents better accuracy results than\nPermutation Entropy and Statistical Complexity, even when these two are\ncombined. This is the first work, to the best of our knowledge, that uses\nInformation Theory quantifiers to transportation mode classification, showing\nthat it is a feasible approach to this kind of problem.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 23:25:09 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Cardoso-Pereira", "I.", ""], ["Borges", "J. B.", ""], ["Barros", "P. H.", ""], ["Loureiro", "A. F.", ""], ["Rosso", "O. A.", ""], ["Ramos", "H. S.", ""]]}, {"id": "2007.08703", "submitter": "Tianyu Wang", "authors": "Tianyu Wang and Cynthia Rudin", "title": "Bandits for BMO Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the bandit problem where the underlying expected reward is a Bounded\nMean Oscillation (BMO) function. BMO functions are allowed to be discontinuous\nand unbounded, and are useful in modeling signals with infinities in the\ndo-main. We develop a toolset for BMO bandits, and provide an algorithm that\ncan achieve poly-log $\\delta$-regret -- a regret measured against an arm that\nis optimal after removing a $\\delta$-sized portion of the arm space.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 00:46:30 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Wang", "Tianyu", ""], ["Rudin", "Cynthia", ""]]}, {"id": "2007.08711", "submitter": "Yu Liang", "authors": "Yu Liang, Arin Chaudhuri, and Haoyu Wang", "title": "Visualizing the Finer Cluster Structure of Large-Scale and\n  High-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimension reduction and visualization of high-dimensional data have become\nvery important research topics because of the rapid growth of large databases\nin data science. In this paper, we propose using a generalized sigmoid function\nto model the distance similarity in both high- and low-dimensional spaces. In\nparticular, the parameter b is introduced to the generalized sigmoid function\nin low-dimensional space, so that we can adjust the heaviness of the function\ntail by changing the value of b. Using both simulated and real-world data sets,\nwe show that our proposed method can generate visualization results comparable\nto those of uniform manifold approximation and projection (UMAP), which is a\nnewly developed manifold learning technique with fast running speed, better\nglobal structure, and scalability to massive data sets. In addition, according\nto the purpose of the study and the data structure, we can decrease or increase\nthe value of b to either reveal the finer cluster structure of the data or\nmaintain the neighborhood continuity of the embedding for better visualization.\nFinally, we use domain knowledge to demonstrate that the finer subclusters\nrevealed with small values of b are meaningful.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 01:36:45 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Liang", "Yu", ""], ["Chaudhuri", "Arin", ""], ["Wang", "Haoyu", ""]]}, {"id": "2007.08714", "submitter": "Yun-Yun Tsai", "authors": "Yun-Yun Tsai and Pin-Yu Chen and Tsung-Yi Ho", "title": "Transfer Learning without Knowing: Reprogramming Black-box Machine\n  Learning Models with Scarce Data and Limited Resources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current transfer learning methods are mainly based on finetuning a pretrained\nmodel with target-domain data. Motivated by the techniques from adversarial\nmachine learning (ML) that are capable of manipulating the model prediction via\ndata perturbations, in this paper we propose a novel approach, black-box\nadversarial reprogramming (BAR), that repurposes a well-trained black-box ML\nmodel (e.g., a prediction API or a proprietary software) for solving different\nML tasks, especially in the scenario with scarce data and constrained\nresources. The rationale lies in exploiting high-performance but unknown ML\nmodels to gain learning capability for transfer learning. Using zeroth order\noptimization and multi-label mapping techniques, BAR can reprogram a black-box\nML model solely based on its input-output responses without knowing the model\narchitecture or changing any parameter. More importantly, in the limited\nmedical data setting, on autism spectrum disorder classification, diabetic\nretinopathy detection, and melanoma detection tasks, BAR outperforms\nstate-of-the-art methods and yields comparable performance to the vanilla\nadversarial reprogramming method requiring complete knowledge of the target ML\nmodel. BAR also outperforms baseline transfer learning approaches by a\nsignificant margin, demonstrating cost-effective means and new insights for\ntransfer learning.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 01:52:34 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 12:12:30 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Tsai", "Yun-Yun", ""], ["Chen", "Pin-Yu", ""], ["Ho", "Tsung-Yi", ""]]}, {"id": "2007.08746", "submitter": "Anurag Sarkar", "authors": "Anurag Sarkar, Seth Cooper", "title": "Sequential Segment-based Level Generation and Blending using Variational\n  Autoencoders", "comments": "8 pages, 9 figures, 11th Workshop on Procedural Content Generation\n  (PCG 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods of level generation using latent variable models such as\nVAEs and GANs do so in segments and produce the final level by stitching these\nseparately generated segments together. In this paper, we build on these\nmethods by training VAEs to learn a sequential model of segment generation such\nthat generated segments logically follow from prior segments. By further\ncombining the VAE with a classifier that determines whether to place the\ngenerated segment to the top, bottom, left or right of the previous segment, we\nobtain a pipeline that enables the generation of arbitrarily long levels that\nprogress in any of these four directions and are composed of segments that\nlogically follow one another. In addition to generating more coherent levels of\nnon-fixed length, this method also enables implicit blending of levels from\nseparate games that do not have similar orientation. We demonstrate our\napproach using levels from Super Mario Bros., Kid Icarus and Mega Man, showing\nthat our method produces levels that are more coherent than previous latent\nvariable-based approaches and are capable of blending levels across games.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 04:11:51 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Sarkar", "Anurag", ""], ["Cooper", "Seth", ""]]}, {"id": "2007.08789", "submitter": "Vahid Yaghoubi", "authors": "Vahid Yaghoubi, Liangliang Cheng, Wim Van Paepegem, Mathias Kersemans", "title": "An ensemble classifier for vibration-based quality monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vibration-based quality monitoring of manufactured components often employs\npattern recognition methods. Albeit developing several classification methods,\nthey usually provide high accuracy for specific types of datasets, but not for\ngeneral cases. In this paper, this issue has been addressed by developing a\nnovel ensemble classifier based on the Dempster-Shafer theory of evidence. To\ndeal with conflicting evidences, three remedies are proposed prior to\ncombination: (i) selection of proper classifiers by evaluating the relevancy\nbetween the predicted and target outputs, (ii) devising an optimization method\nto minimize the distance between the predicted and target outputs, (iii)\nutilizing five different weighting factors, including a new one, to enhance the\nfusion performance. The effectiveness of the proposed framework is validated by\nits application to 15 UCI and KEEL machine learning datasets. It is then\napplied to two vibration-based datasets to detect defected samples: one\nsynthetic dataset generated from the finite element model of a dogbone\ncylinder, and one real experimental dataset generated by collecting broadband\nvibrational response of polycrystalline Nickel alloy first-stage turbine\nblades. The investigation is made through statistical analysis in presence of\ndifferent levels of noise-to-signal ratio. Comparing the results with those of\nfour state-of-the-art fusion techniques reveals the good performance of the\nproposed ensemble method.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 07:23:56 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 20:01:51 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Yaghoubi", "Vahid", ""], ["Cheng", "Liangliang", ""], ["Van Paepegem", "Wim", ""], ["Kersemans", "Mathias", ""]]}, {"id": "2007.08792", "submitter": "Alexandre Thiery", "authors": "Rahul Rahaman and Alexandre H. Thiery", "title": "Uncertainty Quantification and Deep Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning methods are known to suffer from calibration issues: they\ntypically produce over-confident estimates. These problems are exacerbated in\nthe low data regime. Although the calibration of probabilistic models is well\nstudied, calibrating extremely over-parametrized models in the low-data regime\npresents unique challenges. We show that deep-ensembles do not necessarily lead\nto improved calibration properties. In fact, we show that standard ensembling\nmethods, when used in conjunction with modern techniques such as mixup\nregularization, can lead to less calibrated models. In this text, we examine\nthe interplay between three of the most simple and commonly used approaches to\nleverage deep learning when data is scarce: data-augmentation, ensembling, and\npost-processing calibration methods. We demonstrate that, although standard\nensembling techniques certainly help to boost accuracy, the calibration of\ndeep-ensembles relies on subtle trade-offs. Our main finding is that\ncalibration methods such as temperature scaling need to be slightly tweaked\nwhen used with deep-ensembles and, crucially, need to be executed after the\naveraging process. Our simulations indicate that, in the low data regime, this\nsimple strategy can halve the Expected Calibration Error (ECE) on a range of\nbenchmark classification problems when compared to standard deep-ensembles.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 07:32:24 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 14:31:29 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Rahaman", "Rahul", ""], ["Thiery", "Alexandre H.", ""]]}, {"id": "2007.08799", "submitter": "Antonio Macaluso", "authors": "Ricardo \\~Nanculef, Francisco Mena, Antonio Macaluso, Stefano Lodi,\n  Claudio Sartori", "title": "Self-Supervised Bernoulli Autoencoders for Semi-Supervised Hashing", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic hashing is an emerging technique for large-scale similarity search\nbased on representing high-dimensional data using similarity-preserving binary\ncodes used for efficient indexing and search. It has recently been shown that\nvariational autoencoders, with Bernoulli latent representations parametrized by\nneural nets, can be successfully trained to learn such codes in supervised and\nunsupervised scenarios, improving on more traditional methods thanks to their\nability to handle the binary constraints architecturally. However, the scenario\nwhere labels are scarce has not been studied yet.\n  This paper investigates the robustness of hashing methods based on\nvariational autoencoders to the lack of supervision, focusing on two\nsemi-supervised approaches currently in use. The first augments the variational\nautoencoder's training objective to jointly model the distribution over the\ndata and the class labels. The second approach exploits the annotations to\ndefine an additional pairwise loss that enforces consistency between the\nsimilarity in the code (Hamming) space and the similarity in the label space.\nOur experiments show that both methods can significantly increase the hash\ncodes' quality. The pairwise approach can exhibit an advantage when the number\nof labelled points is large. However, we found that this method degrades\nquickly and loses its advantage when labelled samples decrease. To circumvent\nthis problem, we propose a novel supervision method in which the model uses its\nlabel distribution predictions to implement the pairwise objective. Compared to\nthe best baseline, this procedure yields similar performance in fully\nsupervised settings but improves the results significantly when labelled data\nis scarce. Our code is made publicly available at\nhttps://github.com/amacaluso/SSB-VAE.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 07:47:10 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["\u00d1anculef", "Ricardo", ""], ["Mena", "Francisco", ""], ["Macaluso", "Antonio", ""], ["Lodi", "Stefano", ""], ["Sartori", "Claudio", ""]]}, {"id": "2007.08803", "submitter": "Mahdi Soleymani", "authors": "Mahdi Soleymani, Hessam Mahdavifar, A. Salman Avestimehr", "title": "Privacy-Preserving Distributed Learning in the Analog Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the critical problem of distributed learning over data while\nkeeping it private from the computational servers. The state-of-the-art\napproaches to this problem rely on quantizing the data into a finite field, so\nthat the cryptographic approaches for secure multiparty computing can then be\nemployed. These approaches, however, can result in substantial accuracy losses\ndue to fixed-point representation of the data and computation overflows. To\naddress these critical issues, we propose a novel algorithm to solve the\nproblem when data is in the analog domain, e.g., the field of real/complex\nnumbers. We characterize the privacy of the data from both\ninformation-theoretic and cryptographic perspectives, while establishing a\nconnection between the two notions in the analog domain. More specifically, the\nwell-known connection between the distinguishing security (DS) and the mutual\ninformation security (MIS) metrics is extended from the discrete domain to the\ncontinues domain. This is then utilized to bound the amount of information\nabout the data leaked to the servers in our protocol, in terms of the DS\nmetric, using well-known results on the capacity of single-input\nmultiple-output (SIMO) channel with correlated noise. It is shown how the\nproposed framework can be adopted to do computation tasks when data is\nrepresented using floating-point numbers. We then show that this leads to a\nfundamental trade-off between the privacy level of data and accuracy of the\nresult. As an application, we also show how to train a machine learning model\nwhile keeping the data as well as the trained model private. Then numerical\nresults are shown for experiments on the MNIST dataset. Furthermore,\nexperimental advantages are shown comparing to fixed-point implementations over\nfinite fields.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 07:56:39 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Soleymani", "Mahdi", ""], ["Mahdavifar", "Hessam", ""], ["Avestimehr", "A. Salman", ""]]}, {"id": "2007.08840", "submitter": "Alina Ene", "authors": "Alina Ene, Huy L. Nguyen, Adrian Vladu", "title": "Adaptive Gradient Methods for Constrained Convex Optimization and\n  Variational Inequalities", "comments": "Full version of AAAI-21 paper. The current version adds an\n  experimental evaluation and revises the exposition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide new adaptive first-order methods for constrained convex\noptimization. Our main algorithms AdaACSA and AdaAGD+ are accelerated methods,\nwhich are universal in the sense that they achieve nearly-optimal convergence\nrates for both smooth and non-smooth functions, even when they only have access\nto stochastic gradients. In addition, they do not require any prior knowledge\non how the objective function is parametrized, since they automatically adjust\ntheir per-coordinate learning rate. These can be seen as truly accelerated\nAdagrad methods for constrained optimization.\n  We complement them with a simpler algorithm AdaGrad+ which enjoys the same\nfeatures, and achieves the standard non-accelerated convergence rate. We also\npresent a set of new results involving adaptive methods for unconstrained\noptimization and monotone operators.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 09:10:21 GMT"}, {"version": "v2", "created": "Sun, 16 Aug 2020 14:27:04 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2021 19:47:21 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Ene", "Alina", ""], ["Nguyen", "Huy L.", ""], ["Vladu", "Adrian", ""]]}, {"id": "2007.08844", "submitter": "Jaehyung Kim", "authors": "Jaehyung Kim, Youngbum Hur, Sejun Park, Eunho Yang, Sung Ju Hwang and\n  Jinwoo Shin", "title": "Distribution Aligning Refinery of Pseudo-label for Imbalanced\n  Semi-supervised Learning", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While semi-supervised learning (SSL) has proven to be a promising way for\nleveraging unlabeled data when labeled data is scarce, the existing SSL\nalgorithms typically assume that training class distributions are balanced.\nHowever, these SSL algorithms trained under imbalanced class distributions can\nseverely suffer when generalizing to a balanced testing criterion, since they\nutilize biased pseudo-labels of unlabeled data toward majority classes. To\nalleviate this issue, we formulate a convex optimization problem to softly\nrefine the pseudo-labels generated from the biased model, and develop a simple\nalgorithm, named Distribution Aligning Refinery of Pseudo-label (DARP) that\nsolves it provably and efficiently. Under various class-imbalanced\nsemi-supervised scenarios, we demonstrate the effectiveness of DARP and its\ncompatibility with state-of-the-art SSL schemes.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 09:16:05 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Kim", "Jaehyung", ""], ["Hur", "Youngbum", ""], ["Park", "Sejun", ""], ["Yang", "Eunho", ""], ["Hwang", "Sung Ju", ""], ["Shin", "Jinwoo", ""]]}, {"id": "2007.08848", "submitter": "Xinyu Ma", "authors": "Liantao Ma, Xinyu Ma, Junyi Gao, Chaohe Zhang, Zhihao Yu, Xianfeng\n  Jiao, Wenjie Ruan, Yasha Wang, Wen Tang, Jiangtao Wang", "title": "CovidCare: Transferring Knowledge from Existing EMR to Emerging Epidemic\n  for Interpretable Prognosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the characteristics of COVID-19, the epidemic develops rapidly and\noverwhelms health service systems worldwide. Many patients suffer from systemic\nlife-threatening problems and need to be carefully monitored in ICUs. Thus the\nintelligent prognosis is in an urgent need to assist physicians to take an\nearly intervention, prevent the adverse outcome, and optimize the medical\nresource allocation. However, in the early stage of the epidemic outbreak, the\ndata available for analysis is limited due to the lack of effective diagnostic\nmechanisms, rarity of the cases, and privacy concerns. In this paper, we\npropose a deep-learning-based approach, CovidCare, which leverages the existing\nelectronic medical records to enhance the prognosis for inpatients with\nemerging infectious diseases. It learns to embed the COVID-19-related medical\nfeatures based on massive existing EMR data via transfer learning. The\ntransferred parameters are further trained to imitate the teacher model's\nrepresentation behavior based on knowledge distillation, which embeds the\nhealth status more comprehensively in the source dataset. We conduct the length\nof stay prediction experiments for patients on a real-world COVID-19 dataset.\nThe experiment results indicate that our proposed model consistently\noutperforms the comparative baseline methods. CovidCare also reveals that, 1)\nhs-cTnI, hs-CRP and Platelet Counts are the most fatal biomarkers, whose\nabnormal values usually indicate emergency adverse outcome. 2) Normal values of\ngamma-GT, AP and eGFR indicate the overall improvement of health. The medical\nfindings extracted by CovidCare are empirically confirmed by human experts and\nmedical literatures.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 09:20:56 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Ma", "Liantao", ""], ["Ma", "Xinyu", ""], ["Gao", "Junyi", ""], ["Zhang", "Chaohe", ""], ["Yu", "Zhihao", ""], ["Jiao", "Xianfeng", ""], ["Ruan", "Wenjie", ""], ["Wang", "Yasha", ""], ["Tang", "Wen", ""], ["Wang", "Jiangtao", ""]]}, {"id": "2007.08864", "submitter": "Vineet Nair", "authors": "Nir Ailon, Omer Leibovich, Vineet Nair", "title": "Sparse Linear Networks with a Fixed Butterfly Structure: Theory and\n  Practice", "comments": "Accepted to UAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A butterfly network consists of logarithmically many layers, each with a\nlinear number of non-zero weights (pre-specified). The fast\nJohnson-Lindenstrauss transform (FJLT) can be represented as a butterfly\nnetwork followed by a projection onto a random subset of the coordinates.\nMoreover, a random matrix based on FJLT with high probability approximates the\naction of any matrix on a vector. Motivated by these facts, we propose to\nreplace a dense linear layer in any neural network by an architecture based on\nthe butterfly network. The proposed architecture significantly improves upon\nthe quadratic number of weights required in a standard dense layer to nearly\nlinear with little compromise in expressibility of the resulting operator. In a\ncollection of wide variety of experiments, including supervised prediction on\nboth the NLP and vision data, we show that this not only produces results that\nmatch and at times outperform existing well-known architectures, but it also\noffers faster training and prediction in deployment. To understand the\noptimization problems posed by neural networks with a butterfly network, we\nalso study the optimization landscape of the encoder-decoder network, where the\nencoder is replaced by a butterfly network followed by a dense linear layer in\nsmaller dimension. Theoretical result presented in the paper explains why the\ntraining speed and outcome are not compromised by our proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 09:45:03 GMT"}, {"version": "v2", "created": "Sun, 4 Jul 2021 11:12:29 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Ailon", "Nir", ""], ["Leibovich", "Omer", ""], ["Nair", "Vineet", ""]]}, {"id": "2007.08880", "submitter": "Guan-Horng Liu", "authors": "Guan-Horng Liu, Tianrong Chen and Evangelos A. Theodorou", "title": "A Differential Game Theoretic Neural Optimizer for Training Residual\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connections between Deep Neural Networks (DNNs) training and optimal control\ntheory has attracted considerable attention as a principled tool of algorithmic\ndesign. Differential Dynamic Programming (DDP) neural optimizer is a recently\nproposed method along this line. Despite its empirical success, the\napplicability has been limited to feedforward networks and whether such a\ntrajectory-optimization inspired framework can be extended to modern\narchitectures remains unclear. In this work, we derive a generalized DDP\noptimizer that accepts both residual connections and convolution layers. The\nresulting optimal control representation admits a game theoretic perspective,\nin which training residual networks can be interpreted as cooperative\ntrajectory optimization on state-augmented dynamical systems. This Game\nTheoretic DDP (GT-DDP) optimizer enjoys the same theoretic connection in\nprevious work, yet generates a much complex update rule that better leverages\navailable information during network propagation. Evaluation on image\nclassification datasets (e.g. MNIST and CIFAR100) shows an improvement in\ntraining convergence and variance reduction over existing methods. Our approach\nhighlights the benefit gained from architecture-aware optimization.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 10:19:17 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Liu", "Guan-Horng", ""], ["Chen", "Tianrong", ""], ["Theodorou", "Evangelos A.", ""]]}, {"id": "2007.08893", "submitter": "Antonio Ferrara", "authors": "Vito Walter Anelli, Yashar Deldjoo, Tommaso Di Noia, Antonio Ferrara", "title": "Prioritized Multi-Criteria Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Machine Learning scenarios, privacy is a crucial concern when models have\nto be trained with private data coming from users of a service, such as a\nrecommender system, a location-based mobile service, a mobile phone text\nmessaging service providing next word prediction, or a face image\nclassification system. The main issue is that, often, data are collected,\ntransferred, and processed by third parties. These transactions violate new\nregulations, such as GDPR. Furthermore, users usually are not willing to share\nprivate data such as their visited locations, the text messages they wrote, or\nthe photo they took with a third party. On the other hand, users appreciate\nservices that work based on their behaviors and preferences. In order to\naddress these issues, Federated Learning (FL) has been recently proposed as a\nmeans to build ML models based on private datasets distributed over a large\nnumber of clients, while preventing data leakage. A federation of users is\nasked to train a same global model on their private data, while a central\ncoordinating server receives locally computed updates by clients and aggregate\nthem to obtain a better global model, without the need to use clients' actual\ndata. In this work, we extend the FL approach by pushing forward the\nstate-of-the-art approaches in the aggregation step of FL, which we deem\ncrucial for building a high-quality global model. Specifically, we propose an\napproach that takes into account a suite of client-specific criteria that\nconstitute the basis for assigning a score to each client based on a priority\nof criteria defined by the service provider. Extensive experiments on two\npublicly available datasets indicate the merits of the proposed approach\ncompared to standard FL baseline.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 10:49:47 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Anelli", "Vito Walter", ""], ["Deldjoo", "Yashar", ""], ["Di Noia", "Tommaso", ""], ["Ferrara", "Antonio", ""]]}, {"id": "2007.08902", "submitter": "Dmitry Kobak", "authors": "Jan Niklas B\\\"ohm, Philipp Berens, Dmitry Kobak", "title": "A Unifying Perspective on Neighbor Embeddings along the\n  Attraction-Repulsion Spectrum", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neighbor embeddings are a family of methods for visualizing complex\nhigh-dimensional datasets using kNN graphs. To find the low-dimensional\nembedding, these algorithms combine an attractive force between neighboring\npairs of points with a repulsive force between all points. One of the most\npopular examples of such algorithms is t-SNE. Here we show that changing the\nbalance between the attractive and the repulsive forces in t-SNE yields a\nspectrum of embeddings, which is characterized by a simple trade-off: stronger\nattraction can better represent continuous manifold structures, while stronger\nrepulsion can better represent discrete cluster structures. We show that UMAP\nembeddings correspond to t-SNE with increased attraction; this happens because\nthe negative sampling optimisation strategy employed by UMAP strongly lowers\nthe effective repulsion. Likewise, ForceAtlas2, commonly used for visualizing\ndevelopmental single-cell transcriptomic data, yields embeddings corresponding\nto t-SNE with the attraction increased even more. At the extreme of this\nspectrum lies Laplacian Eigenmaps, corresponding to zero repulsion. Our results\ndemonstrate that many prominent neighbor embedding algorithms can be placed\nonto this attraction-repulsion spectrum, and highlight the inherent trade-offs\nbetween them.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 11:10:04 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["B\u00f6hm", "Jan Niklas", ""], ["Berens", "Philipp", ""], ["Kobak", "Dmitry", ""]]}, {"id": "2007.08911", "submitter": "Ehsan Toreini", "authors": "Ehsan Toreini, Mhairi Aitken, Kovila P. L. Coopamootoo, Karen Elliott,\n  Vladimiro Gonzalez Zelaya, Paolo Missier, Magdalene Ng, Aad van Moorsel", "title": "Technologies for Trustworthy Machine Learning: A Survey in a\n  Socio-Technical Context", "comments": "We are updating some sections to include more recent advances", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concerns about the societal impact of AI-based services and systems has\nencouraged governments and other organisations around the world to propose AI\npolicy frameworks to address fairness, accountability, transparency and related\ntopics. To achieve the objectives of these frameworks, the data and software\nengineers who build machine-learning systems require knowledge about a variety\nof relevant supporting tools and techniques. In this paper we provide an\noverview of technologies that support building trustworthy machine learning\nsystems, i.e., systems whose properties justify that people place trust in\nthem. We argue that four categories of system properties are instrumental in\nachieving the policy objectives, namely fairness, explainability, auditability\nand safety & security (FEAS). We discuss how these properties need to be\nconsidered across all stages of the machine learning life cycle, from data\ncollection through run-time model inference. As a consequence, we survey in\nthis paper the main technologies with respect to all four of the FEAS\nproperties, for data-centric as well as model-centric stages of the machine\nlearning system life cycle. We conclude with an identification of open research\nproblems, with a particular focus on the connection between trustworthy machine\nlearning technologies and their implications for individuals and society.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 11:39:20 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 09:40:31 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Toreini", "Ehsan", ""], ["Aitken", "Mhairi", ""], ["Coopamootoo", "Kovila P. L.", ""], ["Elliott", "Karen", ""], ["Zelaya", "Vladimiro Gonzalez", ""], ["Missier", "Paolo", ""], ["Ng", "Magdalene", ""], ["van Moorsel", "Aad", ""]]}, {"id": "2007.08929", "submitter": "Lei Feng", "authors": "Lei Feng, Jiaqi Lv, Bo Han, Miao Xu, Gang Niu, Xin Geng, Bo An,\n  Masashi Sugiyama", "title": "Provably Consistent Partial-Label Learning", "comments": "NeurIPS 2020 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial-label learning (PLL) is a multi-class classification problem, where\neach training example is associated with a set of candidate labels. Even though\nmany practical PLL methods have been proposed in the last two decades, there\nlacks a theoretical understanding of the consistency of those methods-none of\nthe PLL methods hitherto possesses a generation process of candidate label\nsets, and then it is still unclear why such a method works on a specific\ndataset and when it may fail given a different dataset. In this paper, we\npropose the first generation model of candidate label sets, and develop two\nnovel PLL methods that are guaranteed to be provably consistent, i.e., one is\nrisk-consistent and the other is classifier-consistent. Our methods are\nadvantageous, since they are compatible with any deep network or stochastic\noptimizer. Furthermore, thanks to the generation model, we would be able to\nanswer the two questions above by testing if the generation model matches given\ncandidate label sets. Experiments on benchmark and real-world datasets validate\nthe effectiveness of the proposed generation model and two PLL methods.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 12:19:16 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 11:22:28 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Feng", "Lei", ""], ["Lv", "Jiaqi", ""], ["Han", "Bo", ""], ["Xu", "Miao", ""], ["Niu", "Gang", ""], ["Geng", "Xin", ""], ["An", "Bo", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "2007.08949", "submitter": "Jean Kaddour", "authors": "Jean Kaddour, Steind\\'or S{\\ae}mundsson, Marc Peter Deisenroth", "title": "Probabilistic Active Meta-Learning", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-efficient learning algorithms are essential in many practical\napplications where data collection is expensive, e.g., in robotics due to the\nwear and tear. To address this problem, meta-learning algorithms use prior\nexperience about tasks to learn new, related tasks efficiently. Typically, a\nset of training tasks is assumed given or randomly chosen. However, this\nsetting does not take into account the sequential nature that naturally arises\nwhen training a model from scratch in real-life: how do we collect a set of\ntraining tasks in a data-efficient manner? In this work, we introduce task\nselection based on prior experience into a meta-learning algorithm by\nconceptualizing the learner and the active meta-learning setting using a\nprobabilistic latent variable model. We provide empirical evidence that our\napproach improves data-efficiency when compared to strong baselines on\nsimulated robotic experiments.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 12:51:42 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 23:17:28 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Kaddour", "Jean", ""], ["S\u00e6mundsson", "Steind\u00f3r", ""], ["Deisenroth", "Marc Peter", ""]]}, {"id": "2007.08986", "submitter": "Ilker Birbil", "authors": "Gurhan Ceylan and S. Ilker Birbil", "title": "Low-dimensional Interpretable Kernels with Conic Discriminant Functions\n  for Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Kernels are often developed and used as implicit mapping functions that show\nimpressive predictive power due to their high-dimensional feature space\nrepresentations. In this study, we gradually construct a series of simple\nfeature maps that lead to a collection of interpretable low-dimensional\nkernels. At each step, we keep the original features and make sure that the\nincrease in the dimension of input data is extremely low, so that the resulting\ndiscriminant functions remain interpretable and amenable to fast training.\nDespite our persistence on interpretability, we obtain high accuracy results\neven without in-depth hyperparameter tuning. Comparison of our results against\nseveral well-known kernels on benchmark datasets show that the proposed kernels\nare competitive in terms of prediction accuracy, while the training times are\nsignificantly lower than those obtained with state-of-the-art kernel\nimplementations.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 13:58:54 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Ceylan", "Gurhan", ""], ["Birbil", "S. Ilker", ""]]}, {"id": "2007.09024", "submitter": "Arnab Auddy", "authors": "Arnab Auddy, Ming Yuan", "title": "Perturbation Bounds for Orthogonally Decomposable Tensors and Their\n  Applications in High Dimensional Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop deterministic perturbation bounds for singular values and vectors\nof orthogonally decomposable tensors, in a spirit similar to classical results\nfor matrices. Our bounds exhibit intriguing differences between matrices and\nhigher-order tensors. Most notably, they indicate that for higher-order tensors\nperturbation affects each singular value/vector in isolation. In particular,\nits effect on a singular vector does not depend on the multiplicity of its\ncorresponding singular value or its distance from other singular values. Our\nresults can be readily applied and provide a unified treatment to many\ndifferent problems involving higher-order orthogonally decomposable tensors. In\nparticular, we illustrate the implications of our bounds through three\nconnected yet seemingly different high dimensional data analysis tasks: tensor\nSVD, tensor regression and estimation of latent variable models, leading to new\ninsights in each of these settings.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 14:34:58 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Auddy", "Arnab", ""], ["Yuan", "Ming", ""]]}, {"id": "2007.09028", "submitter": "Arnold Yeung", "authors": "Arnold YS Yeung, Shalmali Joshi, Joseph Jay Williams, Frank Rudzicz", "title": "Sequential Explanations with Mental Model-Based Policies", "comments": "Accepted into ICML 2020 Workshop on Human Interpretability in Machine\n  Learning (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The act of explaining across two parties is a feedback loop, where one\nprovides information on what needs to be explained and the other provides an\nexplanation relevant to this information. We apply a reinforcement learning\nframework which emulates this format by providing explanations based on the\nexplainee's current mental model. We conduct novel online human experiments\nwhere explanations generated by various explanation methods are selected and\npresented to participants, using policies which observe participants' mental\nmodels, in order to optimize an interpretability proxy. Our results suggest\nthat mental model-based policies (anchored in our proposed state\nrepresentation) may increase interpretability over multiple sequential\nexplanations, when compared to a random selection baseline. This work provides\ninsight into how to select explanations which increase relevant information for\nusers, and into conducting human-grounded experimentation to understand\ninterpretability.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 14:43:46 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Yeung", "Arnold YS", ""], ["Joshi", "Shalmali", ""], ["Williams", "Joseph Jay", ""], ["Rudzicz", "Frank", ""]]}, {"id": "2007.09029", "submitter": "Irfan Ahmad", "authors": "Abdolmaged Alkhulaifi, Fahad Alsahli, Irfan Ahmad", "title": "Knowledge Distillation in Deep Learning and its Applications", "comments": "9 pages", "journal-ref": "PeerJ Comput.Sci. 7:e474 (2021)", "doi": "10.7717/peerj-cs.474", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based models are relatively large, and it is hard to deploy\nsuch models on resource-limited devices such as mobile phones and embedded\ndevices. One possible solution is knowledge distillation whereby a smaller\nmodel (student model) is trained by utilizing the information from a larger\nmodel (teacher model). In this paper, we present a survey of knowledge\ndistillation techniques applied to deep learning models. To compare the\nperformances of different techniques, we propose a new metric called\ndistillation metric. Distillation metric compares different knowledge\ndistillation algorithms based on sizes and accuracy scores. Based on the\nsurvey, some interesting conclusions are drawn and presented in this paper.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 14:43:52 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Alkhulaifi", "Abdolmaged", ""], ["Alsahli", "Fahad", ""], ["Ahmad", "Irfan", ""]]}, {"id": "2007.09055", "submitter": "Tom Paine", "authors": "Tom Le Paine, Cosmin Paduraru, Andrea Michi, Caglar Gulcehre, Konrad\n  Zolna, Alexander Novikov, Ziyu Wang, Nando de Freitas", "title": "Hyperparameter Selection for Offline Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Offline reinforcement learning (RL purely from logged data) is an important\navenue for deploying RL techniques in real-world scenarios. However, existing\nhyperparameter selection methods for offline RL break the offline assumption by\nevaluating policies corresponding to each hyperparameter setting in the\nenvironment. This online execution is often infeasible and hence undermines the\nmain aim of offline RL. Therefore, in this work, we focus on \\textit{offline\nhyperparameter selection}, i.e. methods for choosing the best policy from a set\nof many policies trained using different hyperparameters, given only logged\ndata. Through large-scale empirical evaluation we show that: 1) offline RL\nalgorithms are not robust to hyperparameter choices, 2) factors such as the\noffline RL algorithm and method for estimating Q values can have a big impact\non hyperparameter selection, and 3) when we control those factors carefully, we\ncan reliably rank policies across hyperparameter choices, and therefore choose\npolicies which are close to the best policy in the set. Overall, our results\npresent an optimistic view that offline hyperparameter selection is within\nreach, even in challenging tasks with pixel observations, high dimensional\naction spaces, and long horizon.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 15:30:38 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Paine", "Tom Le", ""], ["Paduraru", "Cosmin", ""], ["Michi", "Andrea", ""], ["Gulcehre", "Caglar", ""], ["Zolna", "Konrad", ""], ["Novikov", "Alexander", ""], ["Wang", "Ziyu", ""], ["de Freitas", "Nando", ""]]}, {"id": "2007.09070", "submitter": "Hao Liu", "authors": "Hao Liu, Pieter Abbeel", "title": "Hybrid Discriminative-Generative Training via Contrastive Learning", "comments": "Code: https://github.com/lhao499/HDGE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive learning and supervised learning have both seen significant\nprogress and success. However, thus far they have largely been treated as two\nseparate objectives, brought together only by having a shared neural network.\nIn this paper we show that through the perspective of hybrid\ndiscriminative-generative training of energy-based models we can make a direct\nconnection between contrastive learning and supervised learning. Beyond\npresenting this unified view, we show our specific choice of approximation of\nthe energy-based loss outperforms the existing practice in terms of\nclassification accuracy of WideResNet on CIFAR-10 and CIFAR-100. It also leads\nto improved performance on robustness, out-of-distribution detection, and\ncalibration.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 15:50:34 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 07:34:31 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Liu", "Hao", ""], ["Abbeel", "Pieter", ""]]}, {"id": "2007.09081", "submitter": "Hongge Chen", "authors": "Hongge Chen, Si Si, Yang Li, Ciprian Chelba, Sanjiv Kumar, Duane\n  Boning, Cho-Jui Hsieh", "title": "Multi-Stage Influence Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-stage training and knowledge transfer, from a large-scale pretraining\ntask to various finetuning tasks, have revolutionized natural language\nprocessing and computer vision resulting in state-of-the-art performance\nimprovements. In this paper, we develop a multi-stage influence function score\nto track predictions from a finetuned model all the way back to the pretraining\ndata. With this score, we can identify the pretraining examples in the\npretraining task that contribute most to a prediction in the finetuning task.\nThe proposed multi-stage influence function generalizes the original influence\nfunction for a single model in (Koh & Liang, 2017), thereby enabling influence\ncomputation through both pretrained and finetuned models. We study two\ndifferent scenarios with the pretrained embeddings fixed or updated in the\nfinetuning tasks. We test our proposed method in various experiments to show\nits effectiveness and potential applications.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 16:03:11 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Chen", "Hongge", ""], ["Si", "Si", ""], ["Li", "Yang", ""], ["Chelba", "Ciprian", ""], ["Kumar", "Sanjiv", ""], ["Boning", "Duane", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "2007.09087", "submitter": "Weiwen Jiang", "authors": "Weiwen Jiang, Lei Yang, Sakyasingha Dasgupta, Jingtong Hu, Yiyu Shi", "title": "Standing on the Shoulders of Giants: Hardware and Neural Architecture\n  Co-Search with Hot Start", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hardware and neural architecture co-search that automatically generates\nArtificial Intelligence (AI) solutions from a given dataset is promising to\npromote AI democratization; however, the amount of time that is required by\ncurrent co-search frameworks is in the order of hundreds of GPU hours for one\ntarget hardware. This inhibits the use of such frameworks on commodity\nhardware. The root cause of the low efficiency in existing co-search frameworks\nis the fact that they start from a \"cold\" state (i.e., search from scratch). In\nthis paper, we propose a novel framework, namely HotNAS, that starts from a\n\"hot\" state based on a set of existing pre-trained models (a.k.a. model zoo) to\navoid lengthy training time. As such, the search time can be reduced from 200\nGPU hours to less than 3 GPU hours. In HotNAS, in addition to hardware design\nspace and neural architecture search space, we further integrate a compression\nspace to conduct model compressing during the co-search, which creates new\nopportunities to reduce latency but also brings challenges. One of the key\nchallenges is that all of the above search spaces are coupled with each other,\ne.g., compression may not work without hardware design support. To tackle this\nissue, HotNAS builds a chain of tools to design hardware to support\ncompression, based on which a global optimizer is developed to automatically\nco-search all the involved search spaces. Experiments on ImageNet dataset and\nXilinx FPGA show that, within the timing constraint of 5ms, neural\narchitectures generated by HotNAS can achieve up to 5.79% Top-1 and 3.97% Top-5\naccuracy gain, compared with the existing ones.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 16:09:06 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Jiang", "Weiwen", ""], ["Yang", "Lei", ""], ["Dasgupta", "Sakyasingha", ""], ["Hu", "Jingtong", ""], ["Shi", "Yiyu", ""]]}, {"id": "2007.09091", "submitter": "Daniele Musso", "authors": "Daniele Musso", "title": "Partial local entropy and anisotropy in deep weight spaces", "comments": "v3: added analysis of convolutional networks", "journal-ref": "Phys. Rev. E 103, 042303 (2021)", "doi": "10.1103/PhysRevE.103.042303", "report-no": null, "categories": "cs.LG cond-mat.dis-nn stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We refine a recently-proposed class of local entropic loss functions by\nrestricting the smoothening regularization to only a subset of weights. The new\nloss functions are referred to as partial local entropies. They can adapt to\nthe weight-space anisotropy, thus outperforming their isotropic counterparts.\nWe support the theoretical analysis with experiments on image classification\ntasks performed with multi-layer, fully-connected and convolutional neural\nnetworks. The present study suggests how to better exploit the anisotropic\nnature of deep landscapes and provides direct probes of the shape of the minima\nencountered by stochastic gradient descent algorithms. As a by-product, we\nobserve an asymptotic dynamical regime at late training times where the\ntemperature of all the layers obeys a common cooling behavior.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 16:16:18 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 10:02:28 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 10:59:10 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Musso", "Daniele", ""]]}, {"id": "2007.09114", "submitter": "Alvaro Tejero-Cantero", "authors": "Alvaro Tejero-Cantero (1), Jan Boelts (1), Michael Deistler (1),\n  Jan-Matthis Lueckmann (1), Conor Durkan (2), Pedro J. Gon\\c{c}alves (1, 3),\n  David S. Greenberg (1, 4) and Jakob H. Macke (1, 5, 6) ((1) Computational\n  Neuroengineering, Department of Electrical and Computer Engineering,\n  Technical University of Munich, (2) School of Informatics, University of\n  Edinburgh, (3) Neural Systems Analysis, Center of Advanced European Studies\n  and Research (caesar), Bonn, (4) Model-Driven Machine Learning, Centre for\n  Materials and Coastal Research, Helmholtz-Zentrum Geesthacht, (5) Machine\n  Learning in Science, University of T\\\"ubingen, (6) Empirical Inference, Max\n  Planck Institute for Intelligent Systems, T\\\"ubingen)", "title": "SBI -- A toolkit for simulation-based inference", "comments": "Alvaro Tejero-Cantero, Jan Boelts, Michael Deistler, Jan-Matthis\n  Lueckmann and Conor Durkan contributed equally in shared first authorship.\n  This manuscript has been submitted for consideration to the Journal of Open\n  Source Software (JOSS). 4 pages, no figures; v2: added link to sbi home", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Scientists and engineers employ stochastic numerical simulators to model\nempirically observed phenomena. In contrast to purely statistical models,\nsimulators express scientific principles that provide powerful inductive\nbiases, improve generalization to new data or scenarios and allow for fewer,\nmore interpretable and domain-relevant parameters. Despite these advantages,\ntuning a simulator's parameters so that its outputs match data is challenging.\nSimulation-based inference (SBI) seeks to identify parameter sets that a) are\ncompatible with prior knowledge and b) match empirical observations.\nImportantly, SBI does not seek to recover a single 'best' data-compatible\nparameter set, but rather to identify all high probability regions of parameter\nspace that explain observed data, and thereby to quantify parameter\nuncertainty. In Bayesian terminology, SBI aims to retrieve the posterior\ndistribution over the parameters of interest. In contrast to conventional\nBayesian inference, SBI is also applicable when one can run model simulations,\nbut no formula or algorithm exists for evaluating the probability of data given\nparameters, i.e. the likelihood. We present $\\texttt{sbi}$, a PyTorch-based\npackage that implements SBI algorithms based on neural networks. $\\texttt{sbi}$\nfacilitates inference on black-box simulators for practising scientists and\nengineers by providing a unified interface to state-of-the-art algorithms\ntogether with documentation and tutorials.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 16:53:51 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 15:43:36 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Tejero-Cantero", "Alvaro", ""], ["Boelts", "Jan", ""], ["Deistler", "Michael", ""], ["Lueckmann", "Jan-Matthis", ""], ["Durkan", "Conor", ""], ["Gon\u00e7alves", "Pedro J.", ""], ["Greenberg", "David S.", ""], ["Macke", "Jakob H.", ""]]}, {"id": "2007.09118", "submitter": "Hans Kersting", "authors": "Hans Kersting, Maren Mahsereci", "title": "A Fourier State Space Model for Bayesian ODE Filters", "comments": "5 pages, 2 figures, ICML Workshop on Invertible Neural Networks,\n  Normalizing Flows, and Explicit Likelihood Models", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian ODE filtering is a probabilistic numerical method to solve ordinary\ndifferential equations (ODEs). It computes a Bayesian posterior over the\nsolution from evaluations of the vector field defining the ODE. Its most\npopular version, which employs an integrated Brownian motion prior, uses Taylor\nexpansions of the mean to extrapolate forward and has the same convergence\nrates as classical numerical methods. As the solution of many important ODEs\nare periodic functions (oscillators), we raise the question whether Fourier\nexpansions can also be brought to bear within the framework of Gaussian ODE\nfiltering. To this end, we construct a Fourier state space model for ODEs and a\n`hybrid' model that combines a Taylor (Brownian motion) and Fourier state space\nmodel. We show by experiments how the hybrid model might become useful in\ncheaply predicting until the end of the time domain.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 17:14:45 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 12:04:02 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Kersting", "Hans", ""], ["Mahsereci", "Maren", ""]]}, {"id": "2007.09121", "submitter": "Pablo de Castro", "authors": "Tommaso Dorigo and Pablo de Castro", "title": "Dealing with Nuisance Parameters using Machine Learning in High Energy\n  Physics: a Review", "comments": "43 pages, 5 figures. v1: original review manuscript. v2: text\n  improvement/fixes from review process", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG hep-ex hep-ph physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we discuss the impact of nuisance parameters on the\neffectiveness of machine learning in high-energy physics problems, and provide\na review of techniques that allow to include their effect and reduce their\nimpact in the search for optimal selection criteria and variable\ntransformations. The introduction of nuisance parameters complicates the\nsupervised learning task and its correspondence with the data analysis goal,\ndue to their contribution degrading the model performances in real data, and\nthe necessary addition of uncertainties in the resulting statistical inference.\nThe approaches discussed include nuisance-parameterized models, modified or\nadversary losses, semi-supervised learning approaches, and inference-aware\ntechniques.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 17:20:39 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 16:34:28 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Dorigo", "Tommaso", ""], ["de Castro", "Pablo", ""]]}, {"id": "2007.09167", "submitter": "Antoine H\\'ebert", "authors": "Antoine H\\'ebert, Ian Marineau, Gilles Gervais, Tristan Glatard,\n  Brigitte Jaumard", "title": "Can we Estimate Truck Accident Risk from Telemetric Data using Machine\n  Learning?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Road accidents have a high societal cost that could be reduced through\nimproved risk predictions using machine learning. This study investigates\nwhether telemetric data collected on long-distance trucks can be used to\npredict the risk of accidents associated with a driver. We use a dataset\nprovided by a truck transportation company containing the driving data of 1,141\ndrivers for 18 months.\n  We evaluate two different machine learning approaches to perform this task.\nIn the first approach, features are extracted from the time series data using\nthe FRESH algorithm and then used to estimate the risk using Random Forests. In\nthe second approach, we use a convolutional neural network to directly estimate\nthe risk from the time-series data. We find that neither approach is able to\nsuccessfully estimate the risk of accidents on this dataset, in spite of many\nmethodological attempts. We discuss the difficulties of using telemetric data\nfor the estimation of the risk of accidents that could explain this negative\nresult.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 18:12:32 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["H\u00e9bert", "Antoine", ""], ["Marineau", "Ian", ""], ["Gervais", "Gilles", ""], ["Glatard", "Tristan", ""], ["Jaumard", "Brigitte", ""]]}, {"id": "2007.09200", "submitter": "Yujia Huang", "authors": "Yujia Huang, James Gornet, Sihui Dai, Zhiding Yu, Tan Nguyen, Doris Y.\n  Tsao, Anima Anandkumar", "title": "Neural Networks with Recurrent Generative Feedback", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are vulnerable to input perturbations such as additive noise\nand adversarial attacks. In contrast, human perception is much more robust to\nsuch perturbations. The Bayesian brain hypothesis states that human brains use\nan internal generative model to update the posterior beliefs of the sensory\ninput. This mechanism can be interpreted as a form of self-consistency between\nthe maximum a posteriori (MAP) estimation of an internal generative model and\nthe external environment. Inspired by such hypothesis, we enforce\nself-consistency in neural networks by incorporating generative recurrent\nfeedback. We instantiate this design on convolutional neural networks (CNNs).\nThe proposed framework, termed Convolutional Neural Networks with Feedback\n(CNN-F), introduces a generative feedback with latent variables to existing CNN\narchitectures, where consistent predictions are made through alternating MAP\ninference under a Bayesian framework. In the experiments, CNN-F shows\nconsiderably improved adversarial robustness over conventional feedforward CNNs\non standard benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 19:32:48 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 08:29:39 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Huang", "Yujia", ""], ["Gornet", "James", ""], ["Dai", "Sihui", ""], ["Yu", "Zhiding", ""], ["Nguyen", "Tan", ""], ["Tsao", "Doris Y.", ""], ["Anandkumar", "Anima", ""]]}, {"id": "2007.09208", "submitter": "Nhuong Nguyen", "authors": "Marten van Dijk, Nhuong V. Nguyen, Toan N. Nguyen, Lam M. Nguyen, Quoc\n  Tran-Dinh, Phuong Ha Nguyen", "title": "Asynchronous Federated Learning with Reduced Number of Rounds and with\n  Differential Privacy from Less Aggregated Gaussian Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The feasibility of federated learning is highly constrained by the\nserver-clients infrastructure in terms of network communication. Most newly\nlaunched smartphones and IoT devices are equipped with GPUs or sufficient\ncomputing hardware to run powerful AI models. However, in case of the original\nsynchronous federated learning, client devices suffer waiting times and regular\ncommunication between clients and server is required. This implies more\nsensitivity to local model training times and irregular or missed updates,\nhence, less or limited scalability to large numbers of clients and convergence\nrates measured in real time will suffer. We propose a new algorithm for\nasynchronous federated learning which eliminates waiting times and reduces\noverall network communication - we provide rigorous theoretical analysis for\nstrongly convex objective functions and provide simulation results. By adding\nGaussian noise we show how our algorithm can be made differentially private --\nnew theorems show how the aggregated added Gaussian noise is significantly\nreduced.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 19:47:16 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["van Dijk", "Marten", ""], ["Nguyen", "Nhuong V.", ""], ["Nguyen", "Toan N.", ""], ["Nguyen", "Lam M.", ""], ["Tran-Dinh", "Quoc", ""], ["Nguyen", "Phuong Ha", ""]]}, {"id": "2007.09236", "submitter": "Jed Mills", "authors": "Jed Mills, Jia Hu, Geyong Min", "title": "Multi-Task Federated Learning for Personalised Deep Neural Networks in\n  Edge Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) is an emerging approach for collaboratively training\nDeep Neural Networks (DNNs) on mobile devices, without private user data\nleaving the devices. Previous works have shown that non-Independent and\nIdentically Distributed (non-IID) user data harms the convergence speed of the\nFL algorithms. Furthermore, most existing work on FL measures global-model\naccuracy, but in many cases, such as user content-recommendation, improving\nindividual User model Accuracy (UA) is the real objective. To address these\nissues, we propose a Multi-Task FL (MTFL) algorithm that introduces\nnon-federated Batch-Normalization (BN) layers into the federated DNN. MTFL\nbenefits UA and convergence speed by allowing users to train models\npersonalised to their own data. MTFL is compatible with popular iterative FL\noptimisation algorithms such as Federated Averaging (FedAvg), and we show\nempirically that a distributed form of Adam optimisation (FedAvg-Adam) benefits\nconvergence speed even further when used as the optimisation strategy within\nMTFL. Experiments using MNIST and CIFAR10 demonstrate that MTFL is able to\nsignificantly reduce the number of rounds required to reach a target UA, by up\nto $5\\times$ when using existing FL optimisation strategies, and with a further\n$3\\times$ improvement when using FedAvg-Adam. We compare MTFL to competing\npersonalised FL algorithms, showing that it is able to achieve the best UA for\nMNIST and CIFAR10 in all considered scenarios. Finally, we evaluate MTFL with\nFedAvg-Adam on an edge-computing testbed, showing that its convergence and UA\nbenefits outweigh its overhead.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 21:11:01 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 14:45:01 GMT"}, {"version": "v3", "created": "Thu, 22 Jul 2021 09:42:08 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Mills", "Jed", ""], ["Hu", "Jia", ""], ["Min", "Geyong", ""]]}, {"id": "2007.09240", "submitter": "Jascha Sohl-Dickstein", "authors": "Jascha Sohl-Dickstein, Peter Battaglino, Michael R. DeWeese", "title": "A new method for parameter estimation in probabilistic models: Minimum\n  probability flow", "comments": "Originally published 2011. Uploaded to arXiv 2020. arXiv admin note:\n  text overlap with arXiv:0906.4779, arXiv:1205.4295", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fitting probabilistic models to data is often difficult, due to the general\nintractability of the partition function. We propose a new parameter fitting\nmethod, Minimum Probability Flow (MPF), which is applicable to any parametric\nmodel. We demonstrate parameter estimation using MPF in two cases: a continuous\nstate space model, and an Ising spin glass. In the latter case it outperforms\ncurrent techniques by at least an order of magnitude in convergence time with\nlower error in the recovered coupling parameters.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 21:19:44 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Sohl-Dickstein", "Jascha", ""], ["Battaglino", "Peter", ""], ["DeWeese", "Michael R.", ""]]}, {"id": "2007.09250", "submitter": "Grigorios Chrysos", "authors": "Grigorios G Chrysos, Jean Kossaifi, Zhiding Yu, Anima Anandkumar", "title": "Unsupervised Controllable Generation with Self-Training", "comments": "Accepted in IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent generative adversarial networks (GANs) are able to generate impressive\nphoto-realistic images. However, controllable generation with GANs remains a\nchallenging research problem. Achieving controllable generation requires\nsemantically interpretable and disentangled factors of variation. It is\nchallenging to achieve this goal using simple fixed distributions such as\nGaussian distribution. Instead, we propose an unsupervised framework to learn a\ndistribution of latent codes that control the generator through self-training.\nSelf-training provides an iterative feedback in the GAN training, from the\ndiscriminator to the generator, and progressively improves the proposal of the\nlatent codes as training proceeds. The latent codes are sampled from a latent\nvariable model that is learned in the feature space of the discriminator. We\nconsider a normalized independent component analysis model and learn its\nparameters through tensor factorization of the higher-order moments. Our\nframework exhibits better disentanglement compared to other variants such as\nthe variational autoencoder, and is able to discover semantically meaningful\nlatent codes without any supervision. We demonstrate empirically on both cars\nand faces datasets that each group of elements in the learned code controls a\nmode of variation with a semantic meaning, e.g. pose or background change. We\nalso demonstrate with quantitative metrics that our method generates better\nresults compared to other approaches.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 21:50:35 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 06:59:29 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Chrysos", "Grigorios G", ""], ["Kossaifi", "Jean", ""], ["Yu", "Zhiding", ""], ["Anandkumar", "Anima", ""]]}, {"id": "2007.09256", "submitter": "Asaf Shabtai", "authors": "Yoni Birman, Ziv Ido, Gilad Katz and Asaf Shabtai", "title": "Hierarchical Deep Reinforcement Learning Approach for Multi-Objective\n  Scheduling With Varying Queue Sizes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-objective task scheduling (MOTS) is the task scheduling while\noptimizing multiple and possibly contradicting constraints. A challenging\nextension of this problem occurs when every individual task is a\nmulti-objective optimization problem by itself. While deep reinforcement\nlearning (DRL) has been successfully applied to complex sequential problems,\nits application to the MOTS domain has been stymied by two challenges. The\nfirst challenge is the inability of the DRL algorithm to ensure that every item\nis processed identically regardless of its position in the queue. The second\nchallenge is the need to manage large queues, which results in large neural\narchitectures and long training times. In this study we present MERLIN, a\nrobust, modular and near-optimal DRL-based approach for multi-objective task\nscheduling. MERLIN applies a hierarchical approach to the MOTS problem by\ncreating one neural network for the processing of individual tasks and another\nfor the scheduling of the overall queue. In addition to being smaller and with\nshorted training times, the resulting architecture ensures that an item is\nprocessed in the same manner regardless of its position in the queue.\nAdditionally, we present a novel approach for efficiently applying DRL-based\nsolutions on very large queues, and demonstrate how we effectively scale MERLIN\nto process queue sizes that are larger by orders of magnitude than those on\nwhich it was trained. Extensive evaluation on multiple queue sizes show that\nMERLIN outperforms multiple well-known baselines by a large margin (>22%).\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 21:59:06 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Birman", "Yoni", ""], ["Ido", "Ziv", ""], ["Katz", "Gilad", ""], ["Shabtai", "Asaf", ""]]}, {"id": "2007.09286", "submitter": "Boris Ginsburg", "authors": "Boris Ginsburg", "title": "On regularization of gradient descent, layer imbalance and flat minima", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the training dynamics for deep linear networks using a new metric\n- layer imbalance - which defines the flatness of a solution. We demonstrate\nthat different regularization methods, such as weight decay or noise data\naugmentation, behave in a similar way. Training has two distinct phases: 1)\noptimization and 2) regularization. First, during the optimization phase, the\nloss function monotonically decreases, and the trajectory goes toward a minima\nmanifold. Then, during the regularization phase, the layer imbalance decreases,\nand the trajectory goes along the minima manifold toward a flat area. Finally,\nwe extend the analysis for stochastic gradient descent and show that SGD works\nsimilarly to noise regularization.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 00:09:14 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Ginsburg", "Boris", ""]]}, {"id": "2007.09293", "submitter": "Ismael Araujo", "authors": "Ismael C. S. Araujo and Adenilton J. da Silva", "title": "Quantum ensemble of trained classifiers", "comments": "Article in the field of Quantum machine learning, accepted on IJCNN\n  2020 conference with 8 pages, 12 figures and 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Through superposition, a quantum computer is capable of representing an\nexponentially large set of states, according to the number of qubits available.\nQuantum machine learning is a subfield of quantum computing that explores the\npotential of quantum computing to enhance machine learning algorithms. An\napproach of quantum machine learning named quantum ensembles of quantum\nclassifiers consists of using superposition to build an exponentially large\nensemble of classifiers to be trained with an optimization-free learning\nalgorithm. In this work, we investigate how the quantum ensemble works with the\naddition of an optimization method. Experiments using benchmark datasets show\nthe improvements obtained with the addition of the optimization step.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 01:01:33 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Araujo", "Ismael C. S.", ""], ["da Silva", "Adenilton J.", ""]]}, {"id": "2007.09294", "submitter": "Evan Racah Mr.", "authors": "Evan Racah, Sarath Chandar", "title": "Slot Contrastive Networks: A Contrastive Approach for Representing\n  Objects", "comments": "Presented at ICML 2020 Workshop: Object-Oriented Learning (OOL):\n  Perception, Representation, and Reasoning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised extraction of objects from low-level visual data is an important\ngoal for further progress in machine learning. Existing approaches for\nrepresenting objects without labels use structured generative models with\nstatic images. These methods focus a large amount of their capacity on\nreconstructing unimportant background pixels, missing low contrast or small\nobjects. Conversely, we present a new method that avoids losses in pixel space\nand over-reliance on the limited signal a static image provides. Our approach\ntakes advantage of objects' motion by learning a discriminative,\ntime-contrastive loss in the space of slot representations, attempting to force\neach slot to not only capture entities that move, but capture distinct objects\nfrom the other slots. Moreover, we introduce a new quantitative evaluation\nmetric to measure how \"diverse\" a set of slot vectors are, and use it to\nevaluate our model on 20 Atari games.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 01:01:39 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Racah", "Evan", ""], ["Chandar", "Sarath", ""]]}, {"id": "2007.09296", "submitter": "Hongyang Gao", "authors": "Meng Liu, Hongyang Gao, Shuiwang Ji", "title": "Towards Deeper Graph Neural Networks", "comments": "11 pages, KDD2020", "journal-ref": null, "doi": "10.1145/3394486.3403076", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks have shown significant success in the field of graph\nrepresentation learning. Graph convolutions perform neighborhood aggregation\nand represent one of the most important graph operations. Nevertheless, one\nlayer of these neighborhood aggregation methods only consider immediate\nneighbors, and the performance decreases when going deeper to enable larger\nreceptive fields. Several recent studies attribute this performance\ndeterioration to the over-smoothing issue, which states that repeated\npropagation makes node representations of different classes indistinguishable.\nIn this work, we study this observation systematically and develop new insights\ntowards deeper graph neural networks. First, we provide a systematical analysis\non this issue and argue that the key factor compromising the performance\nsignificantly is the entanglement of representation transformation and\npropagation in current graph convolution operations. After decoupling these two\noperations, deeper graph neural networks can be used to learn graph node\nrepresentations from larger receptive fields. We further provide a theoretical\nanalysis of the above observation when building very deep models, which can\nserve as a rigorous and gentle description of the over-smoothing issue. Based\non our theoretical and empirical analysis, we propose Deep Adaptive Graph\nNeural Network (DAGNN) to adaptively incorporate information from large\nreceptive fields. A set of experiments on citation, co-authorship, and\nco-purchase datasets have confirmed our analysis and insights and demonstrated\nthe superiority of our proposed methods.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 01:11:14 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Liu", "Meng", ""], ["Gao", "Hongyang", ""], ["Ji", "Shuiwang", ""]]}, {"id": "2007.09297", "submitter": "Alvaro Ovalle", "authors": "Alvaro Ovalle and Simon M. Lucas", "title": "Modulation of viability signals for self-regulatory control", "comments": "Accepted at the International Workshop on Active Inference 2020\n  (camera-ready version). Extended from 6 to 13 pages to include appendices and\n  a more comprehensive reference list", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We revisit the role of instrumental value as a driver of adaptive behavior.\nIn active inference, instrumental or extrinsic value is quantified by the\ninformation-theoretic surprisal of a set of observations measuring the extent\nto which those observations conform to prior beliefs or preferences. That is,\nan agent is expected to seek the type of evidence that is consistent with its\nown model of the world. For reinforcement learning tasks, the distribution of\npreferences replaces the notion of reward. We explore a scenario in which the\nagent learns this distribution in a self-supervised manner. In particular, we\nhighlight the distinction between observations induced by the environment and\nthose pertaining more directly to the continuity of an agent in time. We\nevaluate our methodology in a dynamic environment with discrete time and\nactions. First with a surprisal minimizing model-free agent (in the RL sense)\nand then expanding to the model-based case to minimize the expected free\nenergy.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 01:11:51 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 11:57:40 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Ovalle", "Alvaro", ""], ["Lucas", "Simon M.", ""]]}, {"id": "2007.09303", "submitter": "Xin Deng", "authors": "Xin Deng, Ross Smith, Genevieve Quintin", "title": "Semi-Supervised Learning Approach to Discover Enterprise User Insights\n  from Feedback and Support", "comments": "7 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the evolution of the cloud and customer centric culture, we inherently\naccumulate huge repositories of textual reviews, feedback, and support\ndata.This has driven enterprises to seek and research engagement patterns, user\nnetwork analysis, topic detections, etc.However, huge manual work is still\nnecessary to mine data to be able to mine actionable outcomes. In this paper,\nwe proposed and developed an innovative Semi-Supervised Learning approach by\nutilizing Deep Learning and Topic Modeling to have a better understanding of\nthe user voice.This approach combines a BERT-based multiclassification\nalgorithm through supervised learning combined with a novel Probabilistic and\nSemantic Hybrid Topic Inference (PSHTI) Model through unsupervised learning,\naiming at automating the process of better identifying the main topics or areas\nas well as the sub-topics from the textual feedback and support.There are three\nmajor break-through: 1. As the advancement of deep learning technology, there\nhave been tremendous innovations in the NLP field, yet the traditional topic\nmodeling as one of the NLP applications lag behind the tide of deep learning.\nIn the methodology and technical perspective, we adopt transfer learning to\nfine-tune a BERT-based multiclassification system to categorize the main topics\nand then utilize the novel PSHTI model to infer the sub-topics under the\npredicted main topics. 2. The traditional unsupervised learning-based topic\nmodels or clustering methods suffer from the difficulty of automatically\ngenerating a meaningful topic label, but our system enables mapping the top\nwords to the self-help issues by utilizing domain knowledge about the product\nthrough web-crawling. 3. This work provides a prominent showcase by leveraging\nthe state-of-the-art methodology in the real production to help shed light to\ndiscover user insights and drive business investment priorities.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 01:18:00 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 01:38:07 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 00:20:23 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Deng", "Xin", ""], ["Smith", "Ross", ""], ["Quintin", "Genevieve", ""]]}, {"id": "2007.09312", "submitter": "Rongzhe Wei", "authors": "Rongzhe Wei, Fa Zhang, Bo Dong and Qinghua Zheng", "title": "DWMD: Dimensional Weighted Orderwise Moment Discrepancy for\n  Domain-specific Hidden Representation Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge transfer from a source domain to a different but semantically\nrelated target domain has long been an important topic in the context of\nunsupervised domain adaptation (UDA). A key challenge in this field is\nestablishing a metric that can exactly measure the data distribution\ndiscrepancy between two homogeneous domains and adopt it in distribution\nalignment, especially in the matching of feature representations in the hidden\nactivation space. Existing distribution matching approaches can be interpreted\nas failing to either explicitly orderwise align higher-order moments or satisfy\nthe prerequisite of certain assumptions in practical uses. We propose a novel\nmoment-based probability distribution metric termed dimensional weighted\norderwise moment discrepancy (DWMD) for feature representation matching in the\nUDA scenario. Our metric function takes advantage of a series for high-order\nmoment alignment, and we theoretically prove that our DWMD metric function is\nerror-free, which means that it can strictly reflect the distribution\ndifferences between domains and is valid without any feature distribution\nassumption. In addition, since the discrepancies between probability\ndistributions in each feature dimension are different, dimensional weighting is\nconsidered in our function. We further calculate the error bound of the\nempirical estimate of the DWMD metric in practical applications. Comprehensive\nexperiments on benchmark datasets illustrate that our method yields\nstate-of-the-art distribution metrics.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 02:37:32 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Wei", "Rongzhe", ""], ["Zhang", "Fa", ""], ["Dong", "Bo", ""], ["Zheng", "Qinghua", ""]]}, {"id": "2007.09330", "submitter": "Raj Kishore", "authors": "Raj Kishore, Zohar Nussinov, Kisor Kumar Sahu", "title": "A new nature inspired modularity function adapted for unsupervised\n  learning involving spatially embedded networks: A comparative analysis", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised machine learning methods can be of great help in many\ntraditional engineering disciplines, where huge amount of labeled data is not\nreadily available or is extremely difficult or costly to generate. Two specific\nexamples include the structure of granular materials and atomic structure of\nmetallic glasses. While the former is critically important for several hundreds\nof billion dollars global industries, the latter is still a big puzzle in\nfundamental science. One thing is common in both the examples is that the\nparticles are the elements of the ensembles that are embedded in Euclidean\nspace and one can create a spatially embedded network to represent their key\nfeatures. Some recent studies show that clustering, which generically refers to\nunsupervised learning, holds great promise in partitioning these networks. In\nmany complex networks, the spatial information of nodes play very important\nrole in determining the network properties. So understanding the structure of\nsuch networks is very crucial. We have compared the performance of our newly\ndeveloped modularity function with some of the well-known modularity functions.\nWe performed this comparison by finding the best partition in 2D and 3D\ngranular assemblies. We show that for the class of networks considered in this\narticle, our method produce much better results than the competing methods.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 04:32:14 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Kishore", "Raj", ""], ["Nussinov", "Zohar", ""], ["Sahu", "Kisor Kumar", ""]]}, {"id": "2007.09334", "submitter": "Yi Liu", "authors": "Yi Liu, Hao Yuan, Lei Cai and Shuiwang Ji", "title": "Deep Learning of High-Order Interactions for Protein Interface\n  Prediction", "comments": "10 pages, 3 figures, 4 tables. KDD2020", "journal-ref": null, "doi": "10.1145/3394486.3403110", "report-no": null, "categories": "cs.LG q-bio.MN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protein interactions are important in a broad range of biological processes.\nTraditionally, computational methods have been developed to automatically\npredict protein interface from hand-crafted features. Recent approaches employ\ndeep neural networks and predict the interaction of each amino acid pair\nindependently. However, these methods do not incorporate the important\nsequential information from amino acid chains and the high-order pairwise\ninteractions. Intuitively, the prediction of an amino acid pair should depend\non both their features and the information of other amino acid pairs. In this\nwork, we propose to formulate the protein interface prediction as a 2D dense\nprediction problem. In addition, we propose a novel deep model to incorporate\nthe sequential information and high-order pairwise interactions to perform\ninterface predictions. We represent proteins as graphs and employ graph neural\nnetworks to learn node features. Then we propose the sequential modeling method\nto incorporate the sequential information and reorder the feature matrix. Next,\nwe incorporate high-order pairwise interactions to generate a 3D tensor\ncontaining different pairwise interactions. Finally, we employ convolutional\nneural networks to perform 2D dense predictions. Experimental results on\nmultiple benchmarks demonstrate that our proposed method can consistently\nimprove the protein interface prediction performance.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 05:39:35 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Liu", "Yi", ""], ["Yuan", "Hao", ""], ["Cai", "Lei", ""], ["Ji", "Shuiwang", ""]]}, {"id": "2007.09335", "submitter": "Hexiang Hu", "authors": "Hexiang Hu, Ozan Sener, Fei Sha, Vladlen Koltun", "title": "Drinking from a Firehose: Continual Learning with Web-scale Natural\n  Language", "comments": "Dataset Downloader: https://github.com/firehose-dataset/downloader\n  Source Code: https://github.com/firehose-dataset/congrad", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning systems will interact with humans, with each other, and\nwith the physical world through time -- and continue to learn and adapt as they\ndo. An important open problem for continual learning is a large-scale benchmark\nthat enables realistic evaluation of algorithms. In this paper, we study a\nnatural setting for continual learning on a massive scale. We introduce the\nproblem of personalized online language learning (POLL), which involves fitting\npersonalized language models to a population of users that evolves over time.\nTo facilitate research on POLL, we collect massive datasets of Twitter posts.\nThese datasets, Firehose10M and Firehose100M, comprise 100 million tweets,\nposted by one million users over six years. Enabled by the Firehose datasets,\nwe present a rigorous evaluation of continual learning algorithms on an\nunprecedented scale. Based on this analysis, we develop a simple algorithm for\ncontinual gradient descent (ConGraD) that outperforms prior continual learning\nmethods on the Firehose datasets as well as earlier benchmarks. Collectively,\nthe POLL problem setting, the Firehose datasets, and the ConGraD algorithm\nenable a complete benchmark for reproducible research on web-scale continual\nlearning.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 05:40:02 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 02:34:21 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Hu", "Hexiang", ""], ["Sener", "Ozan", ""], ["Sha", "Fei", ""], ["Koltun", "Vladlen", ""]]}, {"id": "2007.09343", "submitter": "Nicholas Kuo", "authors": "Nicholas I-Hsien Kuo, Mehrtash Harandi, Nicolas Fourrier, Christian\n  Walder, Gabriela Ferraro, Hanna Suominen", "title": "MTL2L: A Context Aware Neural Optimiser", "comments": "Published in the ICML workshop of Automated Machine Learning (AutoML)\n  2020. Also see\n  https://www.automl.org/wp-content/uploads/2020/07/AutoML_2020_paper_5.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning to learn (L2L) trains a meta-learner to assist the learning of a\ntask-specific base learner. Previously, it was shown that a meta-learner could\nlearn the direct rules to update learner parameters; and that the learnt neural\noptimiser updated learners more rapidly than handcrafted gradient-descent\nmethods. However, we demonstrate that previous neural optimisers were limited\nto update learners on one designated dataset. In order to address input-domain\nheterogeneity, we introduce Multi-Task Learning to Learn (MTL2L), a context\naware neural optimiser which self-modifies its optimisation rules based on\ninput data. We show that MTL2L is capable of updating learners to classify on\ndata of an unseen input-domain at the meta-testing phase.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 06:37:30 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Kuo", "Nicholas I-Hsien", ""], ["Harandi", "Mehrtash", ""], ["Fourrier", "Nicolas", ""], ["Walder", "Christian", ""], ["Ferraro", "Gabriela", ""], ["Suominen", "Hanna", ""]]}, {"id": "2007.09370", "submitter": "Lingjuan Lyu", "authors": "Lingjuan Lyu, Yitong Li, Karthik Nandakumar, Jiangshan Yu, Xingjun Ma", "title": "How to Democratise and Protect AI: Fair and Differentially Private\n  Decentralised Deep Learning", "comments": "Accepted for publication in TDSC", "journal-ref": null, "doi": "10.1109/TDSC.2020.3006287", "report-no": null, "categories": "cs.CR cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper firstly considers the research problem of fairness in\ncollaborative deep learning, while ensuring privacy. A novel reputation system\nis proposed through digital tokens and local credibility to ensure fairness, in\ncombination with differential privacy to guarantee privacy. In particular, we\nbuild a fair and differentially private decentralised deep learning framework\ncalled FDPDDL, which enables parties to derive more accurate local models in a\nfair and private manner by using our developed two-stage scheme: during the\ninitialisation stage, artificial samples generated by Differentially Private\nGenerative Adversarial Network (DPGAN) are used to mutually benchmark the local\ncredibility of each party and generate initial tokens; during the update stage,\nDifferentially Private SGD (DPSGD) is used to facilitate collaborative\nprivacy-preserving deep learning, and local credibility and tokens of each\nparty are updated according to the quality and quantity of individually\nreleased gradients. Experimental results on benchmark datasets under three\nrealistic settings demonstrate that FDPDDL achieves high fairness, yields\ncomparable accuracy to the centralised and distributed frameworks, and delivers\nbetter accuracy than the standalone framework.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 09:06:10 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Lyu", "Lingjuan", ""], ["Li", "Yitong", ""], ["Nandakumar", "Karthik", ""], ["Yu", "Jiangshan", ""], ["Ma", "Xingjun", ""]]}, {"id": "2007.09371", "submitter": "Fengxiang He", "authors": "Fengxiang He, Bohan Wang, Dacheng Tao", "title": "Tighter Generalization Bounds for Iterative Differentially Private\n  Learning Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the relationship between generalization and privacy\npreservation in iterative learning algorithms by two sequential steps. We first\nestablish an alignment between generalization and privacy preservation for any\nlearning algorithm. We prove that $(\\varepsilon, \\delta)$-differential privacy\nimplies an on-average generalization bound for multi-database learning\nalgorithms which further leads to a high-probability bound for any learning\nalgorithm. This high-probability bound also implies a PAC-learnable guarantee\nfor differentially private learning algorithms. We then investigate how the\niterative nature shared by most learning algorithms influence privacy\npreservation and further generalization. Three composition theorems are\nproposed to approximate the differential privacy of any iterative algorithm\nthrough the differential privacy of its every iteration. By integrating the\nabove two steps, we eventually deliver generalization bounds for iterative\nlearning algorithms, which suggest one can simultaneously enhance privacy\npreservation and generalization. Our results are strictly tighter than the\nexisting works. Particularly, our generalization bounds do not rely on the\nmodel size which is prohibitively large in deep learning. This sheds light to\nunderstanding the generalizability of deep learning. These results apply to a\nwide spectrum of learning algorithms. In this paper, we apply them to\nstochastic gradient Langevin dynamics and agnostic federated learning as\nexamples.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 09:12:03 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 04:41:55 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["He", "Fengxiang", ""], ["Wang", "Bohan", ""], ["Tao", "Dacheng", ""]]}, {"id": "2007.09390", "submitter": "Ricardo Pio Monti", "authors": "Ricardo Pio Monti, Ilyes Khemakhem, Aapo Hyvarinen", "title": "Autoregressive flow-based causal discovery and inference", "comments": "6 pages, 3 figures. Accepted at the 2nd ICML Workshop on Invertible\n  Neural Networks, Normalizing Flows, and Explicit Likelihood Models", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We posit that autoregressive flow models are well-suited to performing a\nrange of causal inference tasks - ranging from causal discovery to making\ninterventional and counterfactual predictions. In particular, we exploit the\nfact that autoregressive architectures define an ordering over variables,\nanalogous to a causal ordering, in order to propose a single flow architecture\nto perform all three aforementioned tasks. We first leverage the fact that flow\nmodels estimate normalized log-densities of data to derive a bivariate measure\nof causal direction based on likelihood ratios. Whilst traditional measures of\ncausal direction often require restrictive assumptions on the nature of causal\nrelationships (e.g., linearity),the flexibility of flow models allows for\narbitrary causal dependencies. Our approach compares favourably against\nalternative methods on synthetic data as well as on the Cause-Effect Pairs\nbench-mark dataset. Subsequently, we demonstrate that the invertible nature of\nflows naturally allows for direct evaluation of both interventional and\ncounterfactual predictions, which require marginalization and conditioning over\nlatent variables respectively. We present examples over synthetic data where\nautoregressive flows, when trained under the correct causal ordering, are able\nto make accurate interventional and counterfactual predictions\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 10:02:59 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 21:34:07 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Monti", "Ricardo Pio", ""], ["Khemakhem", "Ilyes", ""], ["Hyvarinen", "Aapo", ""]]}, {"id": "2007.09392", "submitter": "Yuguang Wang", "authors": "Guido Mont\\'ufar, Yu Guang Wang", "title": "Distributed Learning via Filtered Hyperinterpolation on Manifolds", "comments": "50 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning mappings of data on manifolds is an important topic in contemporary\nmachine learning, with applications in astrophysics, geophysics, statistical\nphysics, medical diagnosis, biochemistry, 3D object analysis. This paper\nstudies the problem of learning real-valued functions on manifolds through\nfiltered hyperinterpolation of input-output data pairs where the inputs may be\nsampled deterministically or at random and the outputs may be clean or noisy.\nMotivated by the problem of handling large data sets, it presents a parallel\ndata processing approach which distributes the data-fitting task among multiple\nservers and synthesizes the fitted sub-models into a global estimator. We prove\nquantitative relations between the approximation quality of the learned\nfunction over the entire manifold, the type of target function, the number of\nservers, and the number and type of available samples. We obtain the\napproximation rates of convergence for distributed and non-distributed\napproaches. For the non-distributed case, the approximation order is optimal.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 10:05:18 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Mont\u00fafar", "Guido", ""], ["Wang", "Yu Guang", ""]]}, {"id": "2007.09426", "submitter": "Ralf M\\\"oller", "authors": "Ralf M\\\"oller", "title": "Improved Convergence Speed of Fully Symmetric Learning Rules for\n  Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully symmetric learning rules for principal component analysis can be\nderived from a novel objective function suggested in our previous work. We\nobserved that these learning rules suffer from slow convergence for covariance\nmatrices where some principal eigenvalues are close to each other. Here we\ndescribe a modified objective function with an additional term which mitigates\nthis convergence problem. We show that the learning rule derived from the\nmodified objective function inherits all fixed points from the original\nlearning rule (but may introduce additional ones). Also the stability of the\ninherited fixed points remains unchanged. Only the steepness of the objective\nfunction is increased in some directions. Simulations confirm that the\nconvergence speed can be noticeably improved, depending on the weight factor of\nthe additional term.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 13:41:35 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["M\u00f6ller", "Ralf", ""]]}, {"id": "2007.09445", "submitter": "Purva Pruthi", "authors": "Purva Pruthi, Javier Gonz\\'alez, Xiaoyu Lu, Madalina Fiterau", "title": "Structure Mapping for Transferability of Causal Models", "comments": "Presented at the Inductive Biases, Invariances and Generalization in\n  Reinforcement Learning Workshop, ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human beings learn causal models and constantly use them to transfer\nknowledge between similar environments. We use this intuition to design a\ntransfer-learning framework using object-oriented representations to learn the\ncausal relationships between objects. A learned causal dynamics model can be\nused to transfer between variants of an environment with exchangeable\nperceptual features among objects but with the same underlying causal dynamics.\nWe adapt continuous optimization for structure learning techniques to\nexplicitly learn the cause and effects of the actions in an interactive\nenvironment and transfer to the target domain by categorization of the objects\nbased on causal knowledge. We demonstrate the advantages of our approach in a\ngridworld setting by combining causal model-based approach with model-free\napproach in reinforcement learning.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 14:59:54 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Pruthi", "Purva", ""], ["Gonz\u00e1lez", "Javier", ""], ["Lu", "Xiaoyu", ""], ["Fiterau", "Madalina", ""]]}, {"id": "2007.09456", "submitter": "Guillem Ramirez", "authors": "Guillem Ram\\'irez, Rumen Dangovski, Preslav Nakov, Marin\n  Solja\\v{c}i\\'c", "title": "On a Novel Application of Wasserstein-Procrustes for Unsupervised\n  Cross-Lingual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of unsupervised word embeddings, pre-trained on very large\nmonolingual text corpora, is at the core of the ongoing neural revolution in\nNatural Language Processing (NLP). Initially introduced for English, such\npre-trained word embeddings quickly emerged for a number of other languages.\nSubsequently, there have been a number of attempts to align the embedding\nspaces across languages, which could enable a number of cross-language NLP\napplications. Performing the alignment using unsupervised cross-lingual\nlearning (UCL) is especially attractive as it requires little data and often\nrivals supervised and semi-supervised approaches. Here, we analyze popular\nmethods for UCL and we find that often their objectives are, intrinsically,\nversions of the Wasserstein-Procrustes problem. Hence, we devise an approach to\nsolve Wasserstein-Procrustes in a direct way, which can be used to refine and\nto improve popular UCL methods such as iterative closest point (ICP),\nmultilingual unsupervised and supervised embeddings (MUSE) and supervised\nProcrustes methods. Our evaluation experiments on standard datasets show\nsizable improvements over these approaches. We believe that our rethinking of\nthe Wasserstein-Procrustes problem could enable further research, thus helping\nto develop better algorithms for aligning word embeddings across languages. Our\ncode and instructions to reproduce the experiments are available at\nhttps://github.com/guillemram97/wp-hungarian.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 15:35:09 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Ram\u00edrez", "Guillem", ""], ["Dangovski", "Rumen", ""], ["Nakov", "Preslav", ""], ["Solja\u010di\u0107", "Marin", ""]]}, {"id": "2007.09457", "submitter": "Simon Vary", "authors": "Jared Tanner and Simon Vary", "title": "Compressed sensing of low-rank plus sparse matrices", "comments": "35 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expressing a matrix as the sum of a low-rank matrix plus a sparse matrix is a\nflexible model capturing global and local features in data. This model is the\nfoundation of robust principle component analysis (Candes et al., 2011)\n(Chandrasekaran et al., 2009), and popularized by\ndynamic-foreground/static-background separation (Bouwmans et al., 2016) amongst\nother applications. Compressed sensing, matrix completion, and their variants\n(Eldar and Kutyniok, 2012) (Foucart and Rauhut, 2013) have established that\ndata satisfying low complexity models can be efficiently measured and recovered\nfrom a number of measurements proportional to the model complexity rather than\nthe ambient dimension. This manuscript develops similar guarantees showing that\n$m\\times n$ matrices that can be expressed as the sum of a rank-$r$ matrix and\na $s$-sparse matrix can be recovered by computationally tractable methods from\n$\\mathcal{O}(r(m+n-r)+s)\\log(mn/s)$ linear measurements. More specifically, we\nestablish that the restricted isometry constants for the aforementioned\nmatrices remain bounded independent of problem size provided $p/mn$, $s/p$, and\n$r(m+n-r)/p$ reman fixed. Additionally, we show that semidefinite programming\nand two hard threshold gradient descent algorithms, NIHT and NAHT, converge to\nthe measured matrix provided the measurement operator's RIC's are sufficiently\nsmall. Numerical experiments illustrating these results are shown for synthetic\nproblems, dynamic-foreground/static-background separation, and multispectral\nimaging.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 15:36:11 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Tanner", "Jared", ""], ["Vary", "Simon", ""]]}, {"id": "2007.09483", "submitter": "Emma Rocheteau", "authors": "Emma Rocheteau and Pietro Li\\`o and Stephanie Hyland", "title": "Temporal Pointwise Convolutional Networks for Length of Stay Prediction\n  in the Intensive Care Unit", "comments": "ACM CHIL 2021 Proceedings. arXiv admin note: substantial text overlap\n  with arXiv:2006.16109", "journal-ref": null, "doi": "10.1145/3450439.3451860", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The pressure of ever-increasing patient demand and budget restrictions make\nhospital bed management a daily challenge for clinical staff. Most critical is\nthe efficient allocation of resource-heavy Intensive Care Unit (ICU) beds to\nthe patients who need life support. Central to solving this problem is knowing\nfor how long the current set of ICU patients are likely to stay in the unit. In\nthis work, we propose a new deep learning model based on the combination of\ntemporal convolution and pointwise (1x1) convolution, to solve the length of\nstay prediction task on the eICU and MIMIC-IV critical care datasets. The model\n- which we refer to as Temporal Pointwise Convolution (TPC) - is specifically\ndesigned to mitigate common challenges with Electronic Health Records, such as\nskewness, irregular sampling and missing data. In doing so, we have achieved\nsignificant performance benefits of 18-68% (metric and dataset dependent) over\nthe commonly used Long-Short Term Memory (LSTM) network, and the multi-head\nself-attention network known as the Transformer. By adding mortality prediction\nas a side-task, we can improve performance further still, resulting in a mean\nabsolute deviation of 1.55 days (eICU) and 2.28 days (MIMIC-IV) on predicting\nremaining length of stay.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 17:30:10 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 16:27:09 GMT"}, {"version": "v3", "created": "Fri, 30 Oct 2020 15:34:14 GMT"}, {"version": "v4", "created": "Wed, 24 Feb 2021 12:10:39 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Rocheteau", "Emma", ""], ["Li\u00f2", "Pietro", ""], ["Hyland", "Stephanie", ""]]}, {"id": "2007.09524", "submitter": "Shiqian Ma", "authors": "Zhongruo Wang, Bingyuan Liu, Shixiang Chen, Shiqian Ma, Lingzhou Xue,\n  Hongyu Zhao", "title": "A Manifold Proximal Linear Method for Sparse Spectral Clustering with\n  Application to Single-Cell RNA Sequencing Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering is one of the fundamental unsupervised learning methods\nwidely used in data analysis. Sparse spectral clustering (SSC) imposes sparsity\nto the spectral clustering and it improves the interpretability of the model.\nThis paper considers a widely adopted model for SSC, which can be formulated as\nan optimization problem over the Stiefel manifold with nonsmooth and nonconvex\nobjective. Such an optimization problem is very challenging to solve. Existing\nmethods usually solve its convex relaxation or need to smooth its nonsmooth\npart using certain smoothing techniques. In this paper, we propose a manifold\nproximal linear method (ManPL) that solves the original SSC formulation. We\nalso extend the algorithm to solve the multiple-kernel SSC problems, for which\nan alternating ManPL algorithm is proposed. Convergence and iteration\ncomplexity results of the proposed methods are established. We demonstrate the\nadvantage of our proposed methods over existing methods via the single-cell RNA\nsequencing data analysis.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 22:05:00 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 22:28:39 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Wang", "Zhongruo", ""], ["Liu", "Bingyuan", ""], ["Chen", "Shixiang", ""], ["Ma", "Shiqian", ""], ["Xue", "Lingzhou", ""], ["Zhao", "Hongyu", ""]]}, {"id": "2007.09530", "submitter": "Viet Anh Nguyen", "authors": "Bahar Taskesen and Viet Anh Nguyen and Daniel Kuhn and Jose Blanchet", "title": "A Distributionally Robust Approach to Fair Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a distributionally robust logistic regression model with an\nunfairness penalty that prevents discrimination with respect to sensitive\nattributes such as gender or ethnicity. This model is equivalent to a tractable\nconvex optimization problem if a Wasserstein ball centered at the empirical\ndistribution on the training data is used to model distributional uncertainty\nand if a new convex unfairness measure is used to incentivize equalized\nopportunities. We demonstrate that the resulting classifier improves fairness\nat a marginal loss of predictive accuracy on both synthetic and real datasets.\nWe also derive linear programming-based confidence bounds on the level of\nunfairness of any pre-trained classifier by leveraging techniques from optimal\nuncertainty quantification over Wasserstein balls.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 22:34:48 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Taskesen", "Bahar", ""], ["Nguyen", "Viet Anh", ""], ["Kuhn", "Daniel", ""], ["Blanchet", "Jose", ""]]}, {"id": "2007.09541", "submitter": "Xinwei Chen", "authors": "Xinwei Chen, Tong Wang, Barrett W. Thomas, Marlin W. Ulmer", "title": "Same-Day Delivery with Fairness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The demand for same-day delivery (SDD) has increased rapidly in the last few\nyears and has particularly boomed during the COVID-19 pandemic. Existing\nliterature on the problem has focused on maximizing the utility, represented as\nthe total number of expected requests served. However, a utility-driven\nsolution results in unequal opportunities for customers to receive delivery\nservice, raising questions about fairness. In this paper, we study the problem\nof achieving fairness in SDD. We construct a regional-level fairness constraint\nthat ensures customers from different regions have an equal chance of being\nserved. We develop a reinforcement learning model to learn policies that focus\non both overall utility and fairness. Experimental results demonstrate the\nability of our approach to mitigate the unfairness caused by geographic\ndifferences and constraints of resources, at both coarser and finer-grained\nlevel and with a small cost to utility. In addition, we simulate a real-world\nsituation where the system is suddenly overwhelmed by a surge of requests,\nmimicking the COVID-19 scenario. Our model is robust to the systematic pressure\nand is able to maintain fairness with little compromise to the utility.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 00:25:09 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Chen", "Xinwei", ""], ["Wang", "Tong", ""], ["Thomas", "Barrett W.", ""], ["Ulmer", "Marlin W.", ""]]}, {"id": "2007.09601", "submitter": "Stefano Massaroli", "authors": "Michael Poli, Stefano Massaroli, Atsushi Yamashita, Hajime Asama,\n  Jinkyoo Park", "title": "Hypersolvers: Toward Fast Continuous-Depth Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The infinite-depth paradigm pioneered by Neural ODEs has launched a\nrenaissance in the search for novel dynamical system-inspired deep learning\nprimitives; however, their utilization in problems of non-trivial size has\noften proved impossible due to poor computational scalability. This work paves\nthe way for scalable Neural ODEs with time-to-prediction comparable to\ntraditional discrete networks. We introduce hypersolvers, neural networks\ndesigned to solve ODEs with low overhead and theoretical guarantees on\naccuracy. The synergistic combination of hypersolvers and Neural ODEs allows\nfor cheap inference and unlocks a new frontier for practical application of\ncontinuous-depth models. Experimental evaluations on standard benchmarks, such\nas sampling for continuous normalizing flows, reveal consistent pareto\nefficiency over classical numerical methods.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 06:31:31 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 06:20:08 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Poli", "Michael", ""], ["Massaroli", "Stefano", ""], ["Yamashita", "Atsushi", ""], ["Asama", "Hajime", ""], ["Park", "Jinkyoo", ""]]}, {"id": "2007.09605", "submitter": "J\\'er\\'emy Emile Cohen", "authors": "Carla Schenker, Jeremy E. Cohen and Evrim Acar", "title": "A Flexible Optimization Framework for Regularized Matrix-Tensor\n  Factorizations with Linear Couplings", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2020.3045848", "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coupled matrix and tensor factorizations (CMTF) are frequently used to\njointly analyze data from multiple sources, also called data fusion. However,\ndifferent characteristics of datasets stemming from multiple sources pose many\nchallenges in data fusion and require to employ various regularizations,\nconstraints, loss functions and different types of coupling structures between\ndatasets. In this paper, we propose a flexible algorithmic framework for\ncoupled matrix and tensor factorizations which utilizes Alternating\nOptimization (AO) and the Alternating Direction Method of Multipliers (ADMM).\nThe framework facilitates the use of a variety of constraints, loss functions\nand couplings with linear transformations in a seamless way. Numerical\nexperiments on simulated and real datasets demonstrate that the proposed\napproach is accurate, and computationally efficient with comparable or better\nperformance than available CMTF methods for Frobenius norm loss, while being\nmore flexible. Using Kullback-Leibler divergence on count data, we demonstrate\nthat the algorithm yields accurate results also for other loss functions.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 06:49:59 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Schenker", "Carla", ""], ["Cohen", "Jeremy E.", ""], ["Acar", "Evrim", ""]]}, {"id": "2007.09644", "submitter": "Kristian Gundersen", "authors": "Kristian Gundersen, Anna Oleynik, Nello Blaser, Guttorm Alendal", "title": "Semi Conditional Variational Auto-Encoder for Flow Reconstruction and\n  Uncertainty Quantification from Limited Observations", "comments": null, "journal-ref": null, "doi": "10.1063/5.0025779", "report-no": null, "categories": "stat.ML cs.LG physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new data-driven model to reconstruct nonlinear flow from\nspatially sparse observations. The model is a version of a conditional\nvariational auto-encoder (CVAE), which allows for probabilistic reconstruction\nand thus uncertainty quantification of the prediction. We show that in our\nmodel, conditioning on the measurements from the complete flow data leads to a\nCVAE where only the decoder depends on the measurements. For this reason we\ncall the model as Semi-Conditional Variational Autoencoder (SCVAE). The method,\nreconstructions and associated uncertainty estimates are illustrated on the\nvelocity data from simulations of 2D flow around a cylinder and bottom currents\nfrom the Bergen Ocean Model. The reconstruction errors are compared to those of\nthe Gappy Proper Orthogonal Decomposition (GPOD) method.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 10:15:56 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Gundersen", "Kristian", ""], ["Oleynik", "Anna", ""], ["Blaser", "Nello", ""], ["Alendal", "Guttorm", ""]]}, {"id": "2007.09647", "submitter": "Shuchang Tao", "authors": "Shuchang Tao, Huawei Shen, Qi Cao, Liang Hou, Xueqi Cheng", "title": "Adversarial Immunization for Certifiable Robustness on Graphs", "comments": "Accepted by the WSDM 2021; Code:\n  https://github.com/TaoShuchang/AdvImmune", "journal-ref": null, "doi": "10.1145/3437963.3441782", "report-no": null, "categories": "cs.LG cs.CR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite achieving strong performance in semi-supervised node classification\ntask, graph neural networks (GNNs) are vulnerable to adversarial attacks,\nsimilar to other deep learning models. Existing researches focus on developing\neither robust GNN models or attack detection methods against adversarial\nattacks on graphs. However, little research attention is paid to the potential\nand practice of immunization to adversarial attacks on graphs. In this paper,\nwe propose and formulate the graph adversarial immunization problem, i.e.,\nvaccinating an affordable fraction of node pairs, connected or unconnected, to\nimprove the certifiable robustness of graph against any admissible adversarial\nattack. We further propose an effective algorithm, called AdvImmune, which\noptimizes with meta-gradient in a discrete way to circumvent the\ncomputationally expensive combinatorial optimization when solving the\nadversarial immunization problem. Experiments are conducted on two citation\nnetworks and one social network. Experimental results demonstrate that the\nproposed AdvImmune method remarkably improves the ratio of robust nodes by 12%,\n42%, 65%, with an affordable immune budget of only 5% edges.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 10:41:10 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 09:25:59 GMT"}, {"version": "v3", "created": "Tue, 20 Oct 2020 01:28:32 GMT"}, {"version": "v4", "created": "Fri, 28 May 2021 06:54:55 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Tao", "Shuchang", ""], ["Shen", "Huawei", ""], ["Cao", "Qi", ""], ["Hou", "Liang", ""], ["Cheng", "Xueqi", ""]]}, {"id": "2007.09651", "submitter": "Changyi Xiao", "authors": "Changyi Xiao, Ligang Liu", "title": "Generative Flows with Matrix Exponential", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative flows models enjoy the properties of tractable exact likelihood\nand efficient sampling, which are composed of a sequence of invertible\nfunctions. In this paper, we incorporate matrix exponential into generative\nflows. Matrix exponential is a map from matrices to invertible matrices, this\nproperty is suitable for generative flows. Based on matrix exponential, we\npropose matrix exponential coupling layers that are a general case of affine\ncoupling layers and matrix exponential invertible 1 x 1 convolutions that do\nnot collapse during training. And we modify the networks architecture to make\ntrainingstable andsignificantly speed up the training process. Our experiments\nshow that our model achieves great performance on density estimation amongst\ngenerative flows models.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 11:18:47 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Xiao", "Changyi", ""], ["Liu", "Ligang", ""]]}, {"id": "2007.09668", "submitter": "Denis Lukovnikov", "authors": "Denis Lukovnikov, Jens Lehmann, Asja Fischer", "title": "Improving the Long-Range Performance of Gated Graph Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many popular variants of graph neural networks (GNNs) that are capable of\nhandling multi-relational graphs may suffer from vanishing gradients. In this\nwork, we propose a novel GNN architecture based on the Gated Graph Neural\nNetwork with an improved ability to handle long-range dependencies in\nmulti-relational graphs. An experimental analysis on different synthetic tasks\ndemonstrates that the proposed architecture outperforms several popular GNN\nmodels.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 13:25:38 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Lukovnikov", "Denis", ""], ["Lehmann", "Jens", ""], ["Fischer", "Asja", ""]]}, {"id": "2007.09670", "submitter": "T\\'arik S. Salem", "authors": "T\\'arik S. Salem, Helge Langseth, Heri Ramampiaro", "title": "Prediction Intervals: Split Normal Mixture from Quality-Driven Deep\n  Ensembles", "comments": null, "journal-ref": "Proceedings of the 36th Conference on Uncertainty in Artificial\n  Intelligence (UAI), PMLR volume 124, 2020", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prediction intervals are a machine- and human-interpretable way to represent\npredictive uncertainty in a regression analysis. In this paper, we present a\nmethod for generating prediction intervals along with point estimates from an\nensemble of neural networks. We propose a multi-objective loss function fusing\nquality measures related to prediction intervals and point estimates, and a\npenalty function, which enforces semantic integrity of the results and\nstabilizes the training process of the neural networks. The ensembled\nprediction intervals are aggregated as a split normal mixture accounting for\npossible multimodality and asymmetricity of the posterior predictive\ndistribution, and resulting in prediction intervals that capture aleatoric and\nepistemic uncertainty. Our results show that both our quality-driven loss\nfunction and our aggregation method contribute to well-calibrated prediction\nintervals and point estimates.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 13:46:34 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Salem", "T\u00e1rik S.", ""], ["Langseth", "Helge", ""], ["Ramampiaro", "Heri", ""]]}, {"id": "2007.09684", "submitter": "Haiyang He", "authors": "Haiyang He, Jay Pathak", "title": "An unsupervised learning approach to solving heat equations on chip\n  based on Auto Encoder and Image Gradient", "comments": "18 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.app-ph physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving heat transfer equations on chip becomes very critical in the upcoming\n5G and AI chip-package-systems. However, batches of simulations have to be\nperformed for data driven supervised machine learning models. Data driven\nmethods are data hungry, to address this, Physics Informed Neural Networks\n(PINN) have been proposed. However, vanilla PINN models solve one fixed heat\nequation at a time, so the models have to be retrained for heat equations with\ndifferent source terms. Additionally, issues related to multi-objective\noptimization have to be resolved while using PINN to minimize the PDE residual,\nsatisfy boundary conditions and fit the observed data etc. Therefore, this\npaper investigates an unsupervised learning approach for solving heat transfer\nequations on chip without using solution data and generalizing the trained\nnetwork for predicting solutions for heat equations with unseen source terms.\nSpecifically, a hybrid framework of Auto Encoder (AE) and Image Gradient (IG)\nbased network is designed. The AE is used to encode different source terms of\nthe heat equations. The IG based network implements a second order central\ndifference algorithm for structured grids and minimizes the PDE residual. The\neffectiveness of the designed network is evaluated by solving heat equations\nfor various use cases. It is proved that with limited number of source terms to\ntrain the AE network, the framework can not only solve the given heat transfer\nproblems with a single training process, but also make reasonable predictions\nfor unseen cases (heat equations with new source terms) without retraining.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 15:01:01 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["He", "Haiyang", ""], ["Pathak", "Jay", ""]]}, {"id": "2007.09712", "submitter": "Yi Liu", "authors": "Yi Liu, Sahil Garg, Jiangtian Nie, Yang Zhang, Zehui Xiong, Jiawen\n  Kang, M. Shamim Hossain", "title": "Deep Anomaly Detection for Time-series Data in Industrial IoT: A\n  Communication-Efficient On-device Federated Learning Approach", "comments": "IEEE Internet of Things Journal", "journal-ref": null, "doi": "10.1109/JIOT.2020.3011726", "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since edge device failures (i.e., anomalies) seriously affect the production\nof industrial products in Industrial IoT (IIoT), accurately and timely\ndetecting anomalies is becoming increasingly important. Furthermore, data\ncollected by the edge device may contain the user's private data, which is\nchallenging the current detection approaches as user privacy is calling for the\npublic concern in recent years. With this focus, this paper proposes a new\ncommunication-efficient on-device federated learning (FL)-based deep anomaly\ndetection framework for sensing time-series data in IIoT. Specifically, we\nfirst introduce a FL framework to enable decentralized edge devices to\ncollaboratively train an anomaly detection model, which can improve its\ngeneralization ability. Second, we propose an Attention Mechanism-based\nConvolutional Neural Network-Long Short Term Memory (AMCNN-LSTM) model to\naccurately detect anomalies. The AMCNN-LSTM model uses attention\nmechanism-based CNN units to capture important fine-grained features, thereby\npreventing memory loss and gradient dispersion problems. Furthermore, this\nmodel retains the advantages of LSTM unit in predicting time series data.\nThird, to adapt the proposed framework to the timeliness of industrial anomaly\ndetection, we propose a gradient compression mechanism based on Top-\\textit{k}\nselection to improve communication efficiency. Extensive experiment studies on\nfour real-world datasets demonstrate that the proposed framework can accurately\nand timely detect anomalies and also reduce the communication overhead by 50\\%\ncompared to the federated learning framework that does not use a gradient\ncompression scheme.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 16:47:26 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Liu", "Yi", ""], ["Garg", "Sahil", ""], ["Nie", "Jiangtian", ""], ["Zhang", "Yang", ""], ["Xiong", "Zehui", ""], ["Kang", "Jiawen", ""], ["Hossain", "M. Shamim", ""]]}, {"id": "2007.09762", "submitter": "Ananda Theertha Suresh", "authors": "Yishay Mansour and Mehryar Mohri and Jae Ro and Ananda Theertha Suresh\n  and Ke Wu", "title": "A Theory of Multiple-Source Adaptation with Limited Target Labeled Data", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a theoretical and algorithmic study of the multiple-source domain\nadaptation problem in the common scenario where the learner has access only to\na limited amount of labeled target data, but where the learner has at disposal\na large amount of labeled data from multiple source domains. We show that a new\nfamily of algorithms based on model selection ideas benefits from very\nfavorable guarantees in this scenario and discuss some theoretical obstacles\naffecting some alternative techniques. We also report the results of several\nexperiments with our algorithms that demonstrate their practical effectiveness.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 19:34:48 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 16:41:50 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Mansour", "Yishay", ""], ["Mohri", "Mehryar", ""], ["Ro", "Jae", ""], ["Suresh", "Ananda Theertha", ""], ["Wu", "Ke", ""]]}, {"id": "2007.09811", "submitter": "Liangyu Zhu", "authors": "Liangyu Zhu, Wenbin Lu, Michael R. Kosorok, Rui Song", "title": "Kernel Assisted Learning for Personalized Dose Finding", "comments": "Accepted for KDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An individualized dose rule recommends a dose level within a continuous safe\ndose range based on patient level information such as physical conditions,\ngenetic factors and medication histories. Traditionally, personalized dose\nfinding process requires repeating clinical visits of the patient and frequent\nadjustments of the dosage. Thus the patient is constantly exposed to the risk\nof underdosing and overdosing during the process. Statistical methods for\nfinding an optimal individualized dose rule can lower the costs and risks for\npatients. In this article, we propose a kernel assisted learning method for\nestimating the optimal individualized dose rule. The proposed methodology can\nalso be applied to all other continuous decision-making problems. Advantages of\nthe proposed method include robustness to model misspecification and capability\nof providing statistical inference for the estimated parameters. In the\nsimulation studies, we show that this method is capable of identifying the\noptimal individualized dose rule and produces favorable expected outcomes in\nthe population. Finally, we illustrate our approach using data from a warfarin\ndosing study for thrombosis patients.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 23:03:26 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zhu", "Liangyu", ""], ["Lu", "Wenbin", ""], ["Kosorok", "Michael R.", ""], ["Song", "Rui", ""]]}, {"id": "2007.09814", "submitter": "Harish S. Bhat", "authors": "Harish S. Bhat and Karnamohit Ranka and Christine M. Isborn", "title": "Machine Learning a Molecular Hamiltonian for Predicting Electron\n  Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.LG physics.chem-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a computational method to learn a molecular Hamiltonian matrix\nfrom matrix-valued time series of the electron density. As we demonstrate for\nthree small molecules, the resulting Hamiltonians can be used for electron\ndensity evolution, producing highly accurate results even when propagating 1000\ntime steps beyond the training data. As a more rigorous test, we use the\nlearned Hamiltonians to simulate electron dynamics in the presence of an\napplied electric field, extrapolating to a problem that is beyond the\nfield-free training data. We find that the resulting electron dynamics\npredicted by our learned Hamiltonian are in close quantitative agreement with\nthe ground truth. Our method relies on combining a reduced-dimensional, linear\nstatistical model of the Hamiltonian with a time-discretization of the quantum\nLiouville equation within time-dependent Hartree Fock theory. We train the\nmodel using a least-squares solver, avoiding numerous, CPU-intensive\noptimization steps. For both field-free and field-on problems, we quantify\ntraining and propagation errors, highlighting areas for future development.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 23:18:08 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 05:39:36 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Bhat", "Harish S.", ""], ["Ranka", "Karnamohit", ""], ["Isborn", "Christine M.", ""]]}, {"id": "2007.09818", "submitter": "Hassan Dbouk", "authors": "Hassan Dbouk, Hetul Sanghvi, Mahesh Mehendale, Naresh Shanbhag", "title": "DBQ: A Differentiable Branch Quantizer for Lightweight Deep Neural\n  Networks", "comments": "Published as a conference paper in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have achieved state-of-the art performance on various\ncomputer vision tasks. However, their deployment on resource-constrained\ndevices has been hindered due to their high computational and storage\ncomplexity. While various complexity reduction techniques, such as lightweight\nnetwork architecture design and parameter quantization, have been successful in\nreducing the cost of implementing these networks, these methods have often been\nconsidered orthogonal. In reality, existing quantization techniques fail to\nreplicate their success on lightweight architectures such as MobileNet. To this\nend, we present a novel fully differentiable non-uniform quantizer that can be\nseamlessly mapped onto efficient ternary-based dot product engines. We conduct\ncomprehensive experiments on CIFAR-10, ImageNet, and Visual Wake Words\ndatasets. The proposed quantizer (DBQ) successfully tackles the daunting task\nof aggressively quantizing lightweight networks such as MobileNetV1,\nMobileNetV2, and ShuffleNetV2. DBQ achieves state-of-the art results with\nminimal training overhead and provides the best (pareto-optimal)\naccuracy-complexity trade-off.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 23:50:09 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Dbouk", "Hassan", ""], ["Sanghvi", "Hetul", ""], ["Mehendale", "Mahesh", ""], ["Shanbhag", "Naresh", ""]]}, {"id": "2007.09852", "submitter": "Jiaming Song", "authors": "Jiaming Song and Stefano Ermon", "title": "Multi-label Contrastive Predictive Coding", "comments": "Post camera-ready version. Reorganized the theorems in the last\n  version as corollaries of more general theorems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational mutual information (MI) estimators are widely used in\nunsupervised representation learning methods such as contrastive predictive\ncoding (CPC). A lower bound on MI can be obtained from a multi-class\nclassification problem, where a critic attempts to distinguish a positive\nsample drawn from the underlying joint distribution from $(m-1)$ negative\nsamples drawn from a suitable proposal distribution. Using this approach, MI\nestimates are bounded above by $\\log m$, and could thus severely underestimate\nunless $m$ is very large. To overcome this limitation, we introduce a novel\nestimator based on a multi-label classification problem, where the critic needs\nto jointly identify multiple positive samples at the same time. We show that\nusing the same amount of negative samples, multi-label CPC is able to exceed\nthe $\\log m$ bound, while still being a valid lower bound of mutual\ninformation. We demonstrate that the proposed approach is able to lead to\nbetter mutual information estimation, gain empirical improvements in\nunsupervised representation learning, and beat a current state-of-the-art\nknowledge distillation method over 10 out of 13 tasks.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 02:46:21 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 20:05:39 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Song", "Jiaming", ""], ["Ermon", "Stefano", ""]]}, {"id": "2007.09855", "submitter": "Michael Horrell PhD", "authors": "Michael T. Horrell", "title": "Wide Boosting", "comments": "Gradient Boosting, Wide Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient boosting (GB) is a popular methodology used to solve prediction\nproblems through minimization of a differentiable loss function, $L$. GB is\nespecially performant in low and medium dimension problems. This paper presents\na simple adjustment to GB motivated in part by artificial neural networks.\nSpecifically, our adjustment inserts a square or rectangular matrix\nmultiplication between the output of a GB model and the loss, $L$. This allows\nthe output of a GB model to have increased dimension prior to being fed into\nthe loss and is thus \"wider\" than standard GB implementations. We provide\nperformance comparisons on several publicly available datasets. When using the\nsame tuning methodology and same maximum boosting rounds, Wide Boosting\noutperforms standard GB in every dataset we try.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 02:54:50 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 03:44:22 GMT"}, {"version": "v3", "created": "Sat, 22 Aug 2020 21:21:31 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Horrell", "Michael T.", ""]]}, {"id": "2007.09868", "submitter": "Mohamed Ragab", "authors": "Mohamed Ragab, Zhenghua Chen, Min Wu, Chee-Keong Kwoh, Ruqiang Yan,\n  and Xiaoli Li", "title": "Attention Sequence to Sequence Model for Machine Remaining Useful Life\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate estimation of remaining useful life (RUL) of industrial equipment\ncan enable advanced maintenance schedules, increase equipment availability and\nreduce operational costs. However, existing deep learning methods for RUL\nprediction are not completely successful due to the following two reasons.\nFirst, relying on a single objective function to estimate the RUL will limit\nthe learned representations and thus affect the prediction accuracy. Second,\nwhile longer sequences are more informative for modelling the sensor dynamics\nof equipment, existing methods are less effective to deal with very long\nsequences, as they mainly focus on the latest information. To address these two\nproblems, we develop a novel attention-based sequence to sequence with\nauxiliary task (ATS2S) model. In particular, our model jointly optimizes both\nreconstruction loss to empower our model with predictive capabilities (by\npredicting next input sequence given current input sequence) and RUL prediction\nloss to minimize the difference between the predicted RUL and actual RUL.\nFurthermore, to better handle longer sequence, we employ the attention\nmechanism to focus on all the important input information during training\nprocess. Finally, we propose a new dual-latent feature representation to\nintegrate the encoder features and decoder hidden states, to capture rich\nsemantic information in data. We conduct extensive experiments on four real\ndatasets to evaluate the efficacy of the proposed method. Experimental results\nshow that our proposed method can achieve superior performance over 13\nstate-of-the-art methods consistently.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 03:40:51 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Ragab", "Mohamed", ""], ["Chen", "Zhenghua", ""], ["Wu", "Min", ""], ["Kwoh", "Chee-Keong", ""], ["Yan", "Ruqiang", ""], ["Li", "Xiaoli", ""]]}, {"id": "2007.09880", "submitter": "Yeganeh Marghi", "authors": "Yeganeh M. Marghi, Rohan Gala, Uygar S\\\"umb\\\"ul", "title": "Mixture Representation Learning with Coupled Autoencoders", "comments": "10 pages, 6 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Jointly identifying a mixture of discrete and continuous factors of\nvariability without supervision is a key problem in unraveling complex\nphenomena. Variational inference has emerged as a promising method to learn\ninterpretable mixture representations. However, posterior approximation in\nhigh-dimensional latent spaces, particularly for discrete factors remains\nchallenging. Here, we propose an unsupervised variational framework using\nmultiple interacting networks called cpl-mixVAE that scales well to\nhigh-dimensional discrete settings. In this framework, the mixture\nrepresentation of each network is regularized by imposing a consensus\nconstraint on the discrete factor. We justify the use of this framework by\nproviding both theoretical and experimental results. Finally, we use the\nproposed method to jointly uncover discrete and continuous factors of\nvariability describing gene expression in a single-cell transcriptomic dataset\nprofiling more than a hundred cortical neuron types.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 04:12:04 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 18:37:08 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 02:02:27 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Marghi", "Yeganeh M.", ""], ["Gala", "Rohan", ""], ["S\u00fcmb\u00fcl", "Uygar", ""]]}, {"id": "2007.09881", "submitter": "Wenpeng Wei", "authors": "Wenpeng Wei, Toshiko Aizono", "title": "DeepCO: Offline Combinatorial Optimization Framework Utilizing Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Combinatorial optimization serves as an essential part in many modern\nindustrial applications. A great number of the problems are offline setting due\nto safety and/or cost issues. While simulation-based approaches appear\ndifficult to realise for complicated systems, in this research, we propose\nDeepCO, an offline combinatorial optimization framework utilizing deep\nlearning. We also design an offline variation of Travelling Salesman Problem\n(TSP) to model warehouse operation sequence optimization problem for\nevaluation. With only limited historical data, novel proposed distribution\nregularized optimization method outperforms existing baseline method in offline\nTSP experiment reducing route length by 5.7% averagely and shows great\npotential in real world problems.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 04:17:30 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Wei", "Wenpeng", ""], ["Aizono", "Toshiko", ""]]}, {"id": "2007.09890", "submitter": "Ali Vakilian", "authors": "Simin Liu, Tianrui Liu, Ali Vakilian, Yulin Wan, David P. Woodruff", "title": "Learning the Positions in CountSketch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider sketching algorithms which first quickly compress data by\nmultiplication with a random sketch matrix, and then apply the sketch to\nquickly solve an optimization problem, e.g., low rank approximation. In the\nlearning-based sketching paradigm proposed by Indyk et al. [2019], the sketch\nmatrix is found by choosing a random sparse matrix, e.g., the CountSketch, and\nthen updating the values of the non-zero entries by running gradient descent on\na training data set. Despite the growing body of work on this paradigm, a\nnoticeable omission is that the locations of the non-zero entries of previous\nalgorithms were fixed, and only their values were learned. In this work we\npropose the first learning algorithm that also optimizes the locations of the\nnon-zero entries. We show this algorithm gives better accuracy for low rank\napproximation than previous work, and apply it to other problems such as\n$k$-means clustering for the first time. We show that our algorithm is provably\nbetter in the spiked covariance model and for Zipfian matrices. We also show\nthe importance of the sketch monotonicity property for combining learned\nsketches. Our empirical results show the importance of optimizing not only the\nvalues of the non-zero entries but also their positions.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 05:06:29 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 01:43:01 GMT"}, {"version": "v3", "created": "Mon, 7 Jun 2021 07:30:46 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Liu", "Simin", ""], ["Liu", "Tianrui", ""], ["Vakilian", "Ali", ""], ["Wan", "Yulin", ""], ["Woodruff", "David P.", ""]]}, {"id": "2007.09952", "submitter": "Roy Jennings", "authors": "Hai Victor Habi, Roy H. Jennings, Arnon Netzer", "title": "HMQ: Hardware Friendly Mixed Precision Quantization Block for CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in network quantization produced state-of-the-art results using\nmixed precision quantization. An imperative requirement for many efficient edge\ndevice hardware implementations is that their quantizers are uniform and with\npower-of-two thresholds. In this work, we introduce the Hardware Friendly Mixed\nPrecision Quantization Block (HMQ) in order to meet this requirement. The HMQ\nis a mixed precision quantization block that repurposes the Gumbel-Softmax\nestimator into a smooth estimator of a pair of quantization parameters, namely,\nbit-width and threshold. HMQs use this to search over a finite space of\nquantization schemes. Empirically, we apply HMQs to quantize classification\nmodels trained on CIFAR10 and ImageNet. For ImageNet, we quantize four\ndifferent architectures and show that, in spite of the added restrictions to\nour quantization scheme, we achieve competitive and, in some cases,\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 09:02:09 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Habi", "Hai Victor", ""], ["Jennings", "Roy H.", ""], ["Netzer", "Arnon", ""]]}, {"id": "2007.09953", "submitter": "Yang Yang", "authors": "Yang Yang, Ke Deng, Michael Zhu", "title": "Multi-level Training and Bayesian Optimization for Economical\n  Hyperparameter Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperparameters play a critical role in the performances of many machine\nlearning methods. Determining their best settings or Hyperparameter\nOptimization (HPO) faces difficulties presented by the large number of\nhyperparameters as well as the excessive training time. In this paper, we\ndevelop an effective approach to reducing the total amount of required training\ntime for HPO. In the initialization, the nested Latin hypercube design is used\nto select hyperparameter configurations for two types of training, which are,\nrespectively, heavy training and light training. We propose a truncated\nadditive Gaussian process model to calibrate approximate performance\nmeasurements generated by light training, using accurate performance\nmeasurements generated by heavy training. Based on the model, a sequential\nmodel-based algorithm is developed to generate the performance profile of the\nconfiguration space as well as find optimal ones. Our proposed approach\ndemonstrates competitive performance when applied to optimize synthetic\nexamples, support vector machines, fully connected networks and convolutional\nneural networks.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 09:03:02 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Yang", "Yang", ""], ["Deng", "Ke", ""], ["Zhu", "Michael", ""]]}, {"id": "2007.09966", "submitter": "James Grant", "authors": "James A. Grant, and Roberto Szechtman", "title": "Filtered Poisson Process Bandit on a Continuum", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a version of the continuum armed bandit where an action induces a\nfiltered realisation of a non-homogeneous Poisson process. Point data in the\nfiltered sample are then revealed to the decision-maker, whose reward is the\ntotal number of revealed points. Using knowledge of the function governing the\nfiltering, but without knowledge of the Poisson intensity function, the\ndecision-maker seeks to maximise the expected number of revealed points over T\nrounds. We propose an upper confidence bound algorithm for this problem\nutilising data-adaptive discretisation of the action space. This approach\nenjoys O(T^(2/3)) regret under a Lipschitz assumption on the reward function.\nWe provide lower bounds on the regret of any algorithm for the problem, via new\nlower bounds for related finite-armed bandits, and show that the orders of the\nupper and lower bounds match up to a logarithmic factor.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 09:39:17 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Grant", "James A.", ""], ["Szechtman", "Roberto", ""]]}, {"id": "2007.09969", "submitter": "Christopher J. Anders", "authors": "Christopher J. Anders, Plamen Pasliev, Ann-Kathrin Dombrowski,\n  Klaus-Robert M\\\"uller and Pan Kessel", "title": "Fairwashing Explanations with Off-Manifold Detergent", "comments": "22 pages with 43 figures, to be published in ICML2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explanation methods promise to make black-box classifiers more transparent.\nAs a result, it is hoped that they can act as proof for a sensible, fair and\ntrustworthy decision-making process of the algorithm and thereby increase its\nacceptance by the end-users. In this paper, we show both theoretically and\nexperimentally that these hopes are presently unfounded. Specifically, we show\nthat, for any classifier $g$, one can always construct another classifier\n$\\tilde{g}$ which has the same behavior on the data (same train, validation,\nand test error) but has arbitrarily manipulated explanation maps. We derive\nthis statement theoretically using differential geometry and demonstrate it\nexperimentally for various explanation methods, architectures, and datasets.\nMotivated by our theoretical insights, we then propose a modification of\nexisting explanation methods which makes them significantly more robust.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 09:42:06 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Anders", "Christopher J.", ""], ["Pasliev", "Plamen", ""], ["Dombrowski", "Ann-Kathrin", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Kessel", "Pan", ""]]}, {"id": "2007.09982", "submitter": "Ivano Lauriola", "authors": "Ivano Lauriola and Fabio Aiolli", "title": "MKLpy: a python-based framework for Multiple Kernel Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multiple Kernel Learning is a recent and powerful paradigm to learn the\nkernel function from data. In this paper, we introduce MKLpy, a python-based\nframework for Multiple Kernel Learning. The library provides Multiple Kernel\nLearning algorithms for classification tasks, mechanisms to compute kernel\nfunctions for different data types, and evaluation strategies. The library is\nmeant to maximize the usability and to simplify the development of novel\nsolutions.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 10:10:13 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Lauriola", "Ivano", ""], ["Aiolli", "Fabio", ""]]}, {"id": "2007.09989", "submitter": "Pedro F Da Costa", "authors": "Pedro F. da Costa, Romy Lorenz, Ricardo Pio Monti, Emily Jones, Robert\n  Leech", "title": "Bayesian optimization for automatic design of face stimuli", "comments": "Accepted at ICML2020 workshop track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investigating the cognitive and neural mechanisms involved with face\nprocessing is a fundamental task in modern neuroscience and psychology. To\ndate, the majority of such studies have focused on the use of pre-selected\nstimuli. The absence of personalized stimuli presents a serious limitation as\nit fails to account for how each individual face processing system is tuned to\ncultural embeddings or how it is disrupted in disease. In this work, we propose\na novel framework which combines generative adversarial networks (GANs) with\nBayesian optimization to identify individual response patterns to many\ndifferent faces. Formally, we employ Bayesian optimization to efficiently\nsearch the latent space of state-of-the-art GAN models, with the aim to\nautomatically generate novel faces, to maximize an individual subject's\nresponse. We present results from a web-based proof-of-principle study, where\nparticipants rated images of themselves generated via performing Bayesian\noptimization over the latent space of a GAN. We show how the algorithm can\nefficiently locate an individual's optimal face while mapping out their\nresponse across different semantic transformations of a face; inter-individual\nanalyses suggest how the approach can provide rich information about individual\ndifferences in face processing.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 10:27:18 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["da Costa", "Pedro F.", ""], ["Lorenz", "Romy", ""], ["Monti", "Ricardo Pio", ""], ["Jones", "Emily", ""], ["Leech", "Robert", ""]]}, {"id": "2007.09998", "submitter": "Pranay Pasula", "authors": "Pranay Pasula", "title": "Lagrangian Duality in Reinforcement Learning", "comments": "8 pages, 0 figures; fixed typo in abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although duality is used extensively in certain fields, such as supervised\nlearning in machine learning, it has been much less explored in others, such as\nreinforcement learning (RL). In this paper, we show how duality is involved in\na variety of RL work, from that which spearheaded the field, such as Richard\nBellman's value iteration, to that which was done within just the past few\nyears yet has already had significant impact, such as TRPO, A3C, and GAIL. We\nshow that duality is not uncommon in reinforcement learning, especially when\nvalue iteration, or dynamic programming, is used or when first or second order\napproximations are made to transform initially intractable problems into\ntractable convex programs.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 10:55:12 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 01:01:50 GMT"}, {"version": "v3", "created": "Sat, 25 Jul 2020 01:17:10 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Pasula", "Pranay", ""]]}, {"id": "2007.10047", "submitter": "Valdecy Pereira", "authors": "Gabriela Montenegro de Barros (Federal Fluminense University) and\n  Valdecy Pereira (Federal Fluminense University)", "title": "Electre Tree A Machine Learning Approach to Infer Electre Tri B\n  Parameters", "comments": "21 pages, 8 figures, 8 tables", "journal-ref": "Data Technologies and Applications 2021", "doi": "10.1108/DTA-10-2020-0256", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Purpose: This paper presents an algorithm that can elicitate (infer) all or\nany combination of ELECTRE Tri-B parameters. For example, a decision-maker can\nmaintain the values for indifference, preference, and veto thresholds, and our\nalgorithm can find the criteria weights, reference profiles, and the lambda\ncutting level. Our approach is inspired by a Machine Learning ensemble\ntechnique, the Random Forest, and for that, we named our approach as ELECTRE\nTree algorithm. Methodology: First, we generate a set of ELECTRE Tri-B models,\nwhere each model solves a random sample of criteria and alternatives. Each\nsample is made with replacement, having at least two criteria and between 10%\nto 25% of alternatives. Each model has its parameters optimized by a genetic\nalgorithm that can use an ordered cluster or an assignment example as a\nreference to the optimization. Finally, after the optimization phase, two\nprocedures can be performed, the first one will merge all models, finding in\nthis way the elicitated parameters, and in the second procedure each\nalternative is classified (voted) by each separated model, and the majority\nvote decides the final class. Findings: We have noted that concerning the\nvoting procedure, non-linear decision boundaries are generated, and they can be\nsuitable in analyzing problems with the same nature. In contrast, the merged\nmodel generates linear decision boundaries. Originality: The elicitation of\nELECTRE Tri-B parameters is made by an ensemble technique that is composed of a\nset of multicriteria models that are engaged in generating robust solutions.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 12:39:56 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["de Barros", "Gabriela Montenegro", "", "Federal Fluminense University"], ["Pereira", "Valdecy", "", "Federal Fluminense University"]]}, {"id": "2007.10050", "submitter": "Seth Strimas-Mackey", "authors": "Xin Bing, Florentina Bunea, Seth Strimas-Mackey, Marten Wegkamp", "title": "Prediction in latent factor regression: Adaptive PCR and beyond", "comments": "46 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is devoted to the finite sample prediction risk analysis of a class\nof linear predictors of a response $Y\\in \\mathbb{R}$ from a high-dimensional\nrandom vector $X\\in \\mathbb{R}^p$ when $(X,Y)$ follows a latent factor\nregression model generated by a unobservable latent vector $Z$ of dimension\nless than $p$. Our primary contribution is in establishing finite sample risk\nbounds for prediction with the ubiquitous Principal Component Regression (PCR)\nmethod, under the factor regression model, with the number of principal\ncomponents adaptively selected from the data -- a form of theoretical guarantee\nthat is surprisingly lacking from the PCR literature. To accomplish this, we\nprove a master theorem that establishes a risk bound for a large class of\npredictors, including the PCR predictor as a special case. This approach has\nthe benefit of providing a unified framework for the analysis of a wide range\nof linear prediction methods, under the factor regression setting. In\nparticular, we use our main theorem to recover known risk bounds for the\nminimum-norm interpolating predictor, which has received renewed attention in\nthe past two years, and a prediction method tailored to a subclass of factor\nregression models with identifiable parameters. This model-tailored method can\nbe interpreted as prediction via clusters with latent centers.\n  To address the problem of selecting among a set of candidate predictors, we\nanalyze a simple model selection procedure based on data-splitting, providing\nan oracle inequality under the factor model to prove that the performance of\nthe selected predictor is close to the optimal candidate. We conclude with a\ndetailed simulation study to support and complement our theoretical results.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 12:42:47 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 16:34:47 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Bing", "Xin", ""], ["Bunea", "Florentina", ""], ["Strimas-Mackey", "Seth", ""], ["Wegkamp", "Marten", ""]]}, {"id": "2007.10098", "submitter": "Alexey Zaytsev", "authors": "Victoria Snorovikhina and Alexey Zaytsev", "title": "Unsupervised anomaly detection for discrete sequence healthcare data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fraud in healthcare is widespread, as doctors could prescribe unnecessary\ntreatments to increase bills. Insurance companies want to detect these\nanomalous fraudulent bills and reduce their losses. Traditional fraud detection\nmethods use expert rules and manual data processing.\n  Recently, machine learning techniques automate this process, but hand-labeled\ndata is extremely costly and usually out of date. We propose a machine learning\nmodel that automates fraud detection in an unsupervised way. Two deep learning\napproaches include LSTM neural network for prediction next patient visit and a\nseq2seq model. For normalization of produced anomaly scores, we propose\nEmpirical Distribution Function (EDF) approach. So, the algorithm works with\nhigh class imbalance problems.\n  We use real data on sequences of patients' visits data from Allianz company\nfor the validation. The models provide state-of-the-art results for\nunsupervised anomaly detection for fraud detection in healthcare. Our EDF\napproach further improves the quality of LSTM model.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 13:42:21 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 17:38:44 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Snorovikhina", "Victoria", ""], ["Zaytsev", "Alexey", ""]]}, {"id": "2007.10099", "submitter": "Reinhard Heckel", "authors": "Reinhard Heckel and Fatih Furkan Yilmaz", "title": "Early Stopping in Deep Networks: Double Descent and How to Eliminate it", "comments": "37 pages, 8 figures; changes from version 1: additional numerical\n  results and clarifications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over-parameterized models, such as large deep networks, often exhibit a\ndouble descent phenomenon, whereas a function of model size, error first\ndecreases, increases, and decreases at last. This intriguing double descent\nbehavior also occurs as a function of training epochs and has been conjectured\nto arise because training epochs control the model complexity. In this paper,\nwe show that such epoch-wise double descent arises for a different reason: It\nis caused by a superposition of two or more bias-variance tradeoffs that arise\nbecause different parts of the network are learned at different epochs, and\neliminating this by proper scaling of stepsizes can significantly improve the\nearly stopping performance. We show this analytically for i) linear regression,\nwhere differently scaled features give rise to a superposition of bias-variance\ntradeoffs, and for ii) a two-layer neural network, where the first and second\nlayer each govern a bias-variance tradeoff. Inspired by this theory, we study\ntwo standard convolutional networks empirically and show that eliminating\nepoch-wise double descent through adjusting stepsizes of different layers\nimproves the early stopping performance significantly.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 13:43:33 GMT"}, {"version": "v2", "created": "Sat, 19 Sep 2020 22:21:12 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Heckel", "Reinhard", ""], ["Yilmaz", "Fatih Furkan", ""]]}, {"id": "2007.10106", "submitter": "Ghouthi Boukli Hacene", "authors": "Guillaume Coiffier, Ghouthi Boukli Hacene, Vincent Gripon", "title": "ThriftyNets : Convolutional Neural Networks with Tiny Parameter Budget", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical deep convolutional architectures present an increasing number of\nfeature maps as we go deeper in the network, whereas spatial resolution of\ninputs is decreased through downsampling operations. This means that most of\nthe parameters lay in the final layers, while a large portion of the\ncomputations are performed by a small fraction of the total parameters in the\nfirst layers. In an effort to use every parameter of a network at its maximum,\nwe propose a new convolutional neural network architecture, called ThriftyNet.\nIn ThriftyNet, only one convolutional layer is defined and used recursively,\nleading to a maximal parameter factorization. In complement, normalization,\nnon-linearities, downsamplings and shortcut ensure sufficient expressivity of\nthe model. ThriftyNet achieves competitive performance on a tiny parameters\nbudget, exceeding 91% accuracy on CIFAR-10 with less than 40K parameters in\ntotal, and 74.3% on CIFAR-100 with less than 600K parameters.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 13:50:51 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Coiffier", "Guillaume", ""], ["Hacene", "Ghouthi Boukli", ""], ["Gripon", "Vincent", ""]]}, {"id": "2007.10109", "submitter": "Yun Yuan", "authors": "Yun Yuan, Qinzheng Wang, Xianfeng Terry Yang", "title": "Modeling Stochastic Microscopic Traffic Behaviors: a Physics Regularized\n  Gaussian Process Approach", "comments": "31 pages, 10 figures. arXiv admin note: text overlap with\n  arXiv:2002.02374", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling stochastic traffic behaviors at the microscopic level, such as\ncar-following and lane-changing, is a crucial task to understand the\ninteractions between individual vehicles in traffic streams. Leveraging a\nrecently developed theory named physics regularized Gaussian process (PRGP),\nthis study presents a stochastic microscopic traffic model that can capture the\nrandomness and measure errors in the real world. Physical knowledge from\nclassical car-following models is converted as physics regularizers, in the\nform of shadow Gaussian process (GP), of a multivariate PRGP for improving the\nmodeling accuracy. More specifically, a Bayesian inference algorithm is\ndeveloped to estimate the mean and kernel of GPs, and an enhanced latent force\nmodel is formulated to encode physical knowledge into stochastic processes.\nAlso, based on the posterior regularization inference framework, an efficient\nstochastic optimization algorithm is developed to maximize the evidence\nlower-bound of the system likelihood. To evaluate the performance of the\nproposed models, this study conducts empirical studies on real-world vehicle\ntrajectories from the NGSIM dataset. Since one unique feature of the proposed\nframework is the capability of capturing both car-following and lane-changing\nbehaviors with one single model, numerical tests are carried out with two\nseparated datasets, one contains lane-changing maneuvers and the other doesn't.\nThe results show the proposed method outperforms the previous influential\nmethods in estimation precision.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 06:03:32 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Yuan", "Yun", ""], ["Wang", "Qinzheng", ""], ["Yang", "Xianfeng Terry", ""]]}, {"id": "2007.10112", "submitter": "Hanwen Huang", "authors": "Hanwen Huang and Qinglong Yang", "title": "Large scale analysis of generalization error in learning using margin\n  based classification methods", "comments": "31 pages, 7 figures. arXiv admin note: substantial text overlap with\n  arXiv:1901.08057", "journal-ref": null, "doi": "10.1088/1742-5468/abbed5", "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-margin classifiers are popular methods for classification. We derive\nthe asymptotic expression for the generalization error of a family of\nlarge-margin classifiers in the limit of both sample size $n$ and dimension $p$\ngoing to $\\infty$ with fixed ratio $\\alpha=n/p$. This family covers a broad\nrange of commonly used classifiers including support vector machine, distance\nweighted discrimination, and penalized logistic regression. Our result can be\nused to establish the phase transition boundary for the separability of two\nclasses. We assume that the data are generated from a single multivariate\nGaussian distribution with arbitrary covariance structure. We explore two\nspecial choices for the covariance matrix: spiked population model and two\nlayer neural networks with random first layer weights. The method we used for\nderiving the closed-form expression is from statistical physics known as the\nreplica method. Our asymptotic results match simulations already when $n,p$ are\nof the order of a few hundreds. For two layer neural networks, we reproduce the\nrecently developed `double descent' phenomenology for several classification\nmodels. We also discuss some statistical insights that can be drawn from these\nanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 20:31:26 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Huang", "Hanwen", ""], ["Yang", "Qinglong", ""]]}, {"id": "2007.10129", "submitter": "Xianfu Chen", "authors": "Xianfu Chen and Celimuge Wu and Tao Chen and Zhi Liu and Honggang\n  Zhang and Mehdi Bennis and Hang Liu and Yusheng Ji", "title": "Information Freshness-Aware Task Offloading in Air-Ground Integrated\n  Edge Computing Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper studies the problem of information freshness-aware task offloading\nin an air-ground integrated multi-access edge computing system, which is\ndeployed by an infrastructure provider (InP). A third-party real-time\napplication service provider provides computing services to the subscribed\nmobile users (MUs) with the limited communication and computation resources\nfrom the InP based on a long-term business agreement. Due to the dynamic\ncharacteristics, the interactions among the MUs are modelled by a\nnon-cooperative stochastic game, in which the control policies are coupled and\neach MU aims to selfishly maximize its own expected long-term payoff. To\naddress the Nash equilibrium solutions, we propose that each MU behaves in\naccordance with the local system states and conjectures, based on which the\nstochastic game is transformed into a single-agent Markov decision process.\nMoreover, we derive a novel online deep reinforcement learning (RL) scheme that\nadopts two separate double deep Q-networks for each MU to approximate the\nQ-factor and the post-decision Q-factor. Using the proposed deep RL scheme,\neach MU in the system is able to make decisions without a priori statistical\nknowledge of dynamics. Numerical experiments examine the potentials of the\nproposed scheme in balancing the age of information and the energy consumption.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 21:32:43 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Chen", "Xianfu", ""], ["Wu", "Celimuge", ""], ["Chen", "Tao", ""], ["Liu", "Zhi", ""], ["Zhang", "Honggang", ""], ["Bennis", "Mehdi", ""], ["Liu", "Hang", ""], ["Ji", "Yusheng", ""]]}, {"id": "2007.10135", "submitter": "Sara Mohammadinejad", "authors": "Sara Mohammadinejad, Brandon Paulsen, Chao Wang, Jyotirmoy V. Deshmukh", "title": "DiffRNN: Differential Verification of Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) such as Long Short Term Memory (LSTM)\nnetworks have become popular in a variety of applications such as image\nprocessing, data classification, speech recognition, and as controllers in\nautonomous systems. In practical settings, there is often a need to deploy such\nRNNs on resource-constrained platforms such as mobile phones or embedded\ndevices. As the memory footprint and energy consumption of such components\nbecome a bottleneck, there is interest in compressing and optimizing such\nnetworks using a range of heuristic techniques. However, these techniques do\nnot guarantee the safety of the optimized network, e.g., against adversarial\ninputs, or equivalence of the optimized and original networks. To address this\nproblem, we propose DIFFRNN, the first differential verification method for\nRNNs to certify the equivalence of two structurally similar neural networks.\nExisting work on differential verification for ReLUbased feed-forward neural\nnetworks does not apply to RNNs where nonlinear activation functions such as\nSigmoid and Tanh cannot be avoided. RNNs also pose unique challenges such as\nhandling sequential inputs, complex feedback structures, and interactions\nbetween the gates and states. In DIFFRNN, we overcome these challenges by\nbounding nonlinear activation functions with linear constraints and then\nsolving constrained optimization problems to compute tight bounding boxes on\nnonlinear surfaces in a high-dimensional space. The soundness of these bounding\nboxes is then proved using the dReal SMT solver. We demonstrate the practical\nefficacy of our technique on a variety of benchmarks and show that DIFFRNN\noutperforms state-of-the-art RNN verification tools such as POPQORN.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 14:14:35 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Mohammadinejad", "Sara", ""], ["Paulsen", "Brandon", ""], ["Wang", "Chao", ""], ["Deshmukh", "Jyotirmoy V.", ""]]}, {"id": "2007.10143", "submitter": "Rumen Dangovski", "authors": "Evan Vogelbaum and Rumen Dangovski and Li Jing and Marin\n  Solja\\v{c}i\\'c", "title": "Contextualizing Enhances Gradient Based Meta Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta learning methods have found success when applied to few shot\nclassification problems, in which they quickly adapt to a small number of\nlabeled examples. Prototypical representations, each representing a particular\nclass, have been of particular importance in this setting, as they provide a\ncompact form to convey information learned from the labeled examples. However,\nthese prototypes are just one method of representing this information, and they\nare narrow in their scope and ability to classify unseen examples. We propose\nthe implementation of contextualizers, which are generalizable prototypes that\nadapt to given examples and play a larger role in classification for\ngradient-based models. We demonstrate how to equip meta learning methods with\ncontextualizers and show that their use can significantly boost performance on\na range of few shot learning datasets. We also present figures of merit\ndemonstrating the potential benefits of contextualizers, along with analysis of\nhow models make use of them. Our approach is particularly apt for low-data\nenvironments where it is difficult to update parameters without overfitting.\nOur implementation and instructions to reproduce the experiments are available\nat https://github.com/naveace/proto-context.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 04:01:56 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Vogelbaum", "Evan", ""], ["Dangovski", "Rumen", ""], ["Jing", "Li", ""], ["Solja\u010di\u0107", "Marin", ""]]}, {"id": "2007.10181", "submitter": "Nicholas Halmagyi", "authors": "Nick Halmagyi and Shailesh Lal", "title": "Mixed Moments for the Product of Ginibre Matrices", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math-ph cond-mat.stat-mech hep-th math.MP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the ensemble of a product of n complex Gaussian i.i.d. matrices. We\nfind this ensemble is Gaussian with a variance matrix which is averaged over a\nmulti-Wishart ensemble. We compute the mixed moments and find that at large\n$N$, they are given by an enumeration of non-crossing pairings weighted by\nFuss-Catalan numbers.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 15:13:18 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Halmagyi", "Nick", ""], ["Lal", "Shailesh", ""]]}, {"id": "2007.10182", "submitter": "Edouard Pineau", "authors": "Edouard Pineau, S\\'ebastien Razakarivony, Thomas Bonald", "title": "Time Series Source Separation with Slow Flows", "comments": "INNF+ Workshop, ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show that slow feature analysis (SFA), a common time series\ndecomposition method, naturally fits into the flow-based models (FBM)\nframework, a type of invertible neural latent variable models. Building upon\nrecent advances on blind source separation, we show that such a fit makes the\ntime series decomposition identifiable.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 15:15:27 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Pineau", "Edouard", ""], ["Razakarivony", "S\u00e9bastien", ""], ["Bonald", "Thomas", ""]]}, {"id": "2007.10185", "submitter": "Matthew McDermott", "authors": "Matthew B.A. McDermott (1), Bret Nestor (2), Evan Kim (1), Wancong\n  Zhang (3), Anna Goldenberg (2, 4, 5), Peter Szolovits (1), Marzyeh Ghassemi\n  (2, 4) ((1) CSAIL, MIT, (2) University of Toronto, (3) NYU, (4) Vector\n  Institute, (5) SickKids)", "title": "A Comprehensive Evaluation of Multi-task Learning and Multi-task\n  Pre-training on EHR Time-series Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning (MTL) is a machine learning technique aiming to improve\nmodel performance by leveraging information across many tasks. It has been used\nextensively on various data modalities, including electronic health record\n(EHR) data. However, despite significant use on EHR data, there has been little\nsystematic investigation of the utility of MTL across the diverse set of\npossible tasks and training schemes of interest in healthcare. In this work, we\nexamine MTL across a battery of tasks on EHR time-series data. We find that\nwhile MTL does suffer from common negative transfer, we can realize significant\ngains via MTL pre-training combined with single-task fine-tuning. We\ndemonstrate that these gains can be achieved in a task-independent manner and\noffer not only minor improvements under traditional learning, but also notable\ngains in a few-shot learning context, thereby suggesting this could be a\nscalable vehicle to offer improved performance in important healthcare\ncontexts.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 15:19:28 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["McDermott", "Matthew B. A.", ""], ["Nestor", "Bret", ""], ["Kim", "Evan", ""], ["Zhang", "Wancong", ""], ["Goldenberg", "Anna", ""], ["Szolovits", "Peter", ""], ["Ghassemi", "Marzyeh", ""]]}, {"id": "2007.10205", "submitter": "Nir Sochen", "authors": "Ido Ben-Shaul, Leah Bar and Nir Sochen", "title": "Solving the functional Eigen-Problem using Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we explore the ability of NN (Neural Networks) to serve as a\ntool for finding eigen-pairs of ordinary differential equations. The question\nwe aime to address is whether, given a self-adjoint operator, we can learn what\nare the eigenfunctions, and their matching eigenvalues. The topic of solving\nthe eigen-problem is widely discussed in Image Processing, as many image\nprocessing algorithms can be thought of as such operators. We suggest an\nalternative to numeric methods of finding eigenpairs, which may potentially be\nmore robust and have the ability to solve more complex problems. In this work,\nwe focus on simple problems for which the analytical solution is known. This\nway, we are able to make initial steps in discovering the capabilities and\nshortcomings of DNN (Deep Neural Networks) in the given setting.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 15:41:22 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Ben-Shaul", "Ido", ""], ["Bar", "Leah", ""], ["Sochen", "Nir", ""]]}, {"id": "2007.10229", "submitter": "Neil Walton", "authors": "Denis Denisov and Neil Walton", "title": "Regret Analysis of a Markov Policy Gradient Algorithm for Multi-arm\n  Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a policy gradient algorithm applied to a finite-arm bandit\nproblem with Bernoulli rewards. We allow learning rates to depend on the\ncurrent state of the algorithm, rather than use a deterministic time-decreasing\nlearning rate. The state of the algorithm forms a Markov chain on the\nprobability simplex. We apply Foster-Lyapunov techniques to analyse the\nstability of this Markov chain. We prove that if learning rates are well chosen\nthen the policy gradient algorithm is a transient Markov chain and the state of\nthe chain converges on the optimal arm with logarithmic or poly-logarithmic\nregret.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 16:19:50 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 11:27:56 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Denisov", "Denis", ""], ["Walton", "Neil", ""]]}, {"id": "2007.10231", "submitter": "Sambaran Bandyopadhyay", "authors": "Sambaran Bandyopadhyay, Saley Vishal Vivek, M. N. Murty", "title": "Integrating Network Embedding and Community Outlier Detection via\n  Multiclass Graph Description", "comments": "This work is accepted at the 24th European Conference on Artificial\n  Intelligence (ECAI 2020) as a full research paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network (or graph) embedding is the task to map the nodes of a graph to a\nlower dimensional vector space, such that it preserves the graph properties and\nfacilitates the downstream network mining tasks. Real world networks often come\nwith (community) outlier nodes, which behave differently from the regular nodes\nof the community. These outlier nodes can affect the embedding of the regular\nnodes, if not handled carefully. In this paper, we propose a novel unsupervised\ngraph embedding approach (called DMGD) which integrates outlier and community\ndetection with node embedding. We extend the idea of deep support vector data\ndescription to the framework of graph embedding when there are multiple\ncommunities present in the given network, and an outlier is characterized\nrelative to its community. We also show the theoretical bounds on the number of\noutliers detected by DMGD. Our formulation boils down to an interesting minimax\ngame between the outliers, community assignments and the node embedding\nfunction. We also propose an efficient algorithm to solve this optimization\nframework. Experimental results on both synthetic and real world networks show\nthe merit of our approach compared to state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 16:21:07 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Bandyopadhyay", "Sambaran", ""], ["Vivek", "Saley Vishal", ""], ["Murty", "M. N.", ""]]}, {"id": "2007.10237", "submitter": "Giosu\\'e Lo Bosco", "authors": "Domenico Amato, Giosu\\'e Lo Bosco, Raffaele Giancarlo", "title": "Learning from Data to Speed-up Sorted Table Search Procedures:\n  Methodology and Practical Guidelines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorted Table Search Procedures are the quintessential query-answering tool,\nwith widespread usage that now includes also Web Applications, e.g, Search\nEngines (Google Chrome) and ad Bidding Systems (AppNexus). Speeding them up, at\nvery little cost in space, is still a quite significant achievement. Here we\nstudy to what extend Machine Learning Techniques can contribute to obtain such\na speed-up via a systematic experimental comparison of known efficient\nimplementations of Sorted Table Search procedures, with different Data Layouts,\nand their Learned counterparts developed here. We characterize the scenarios in\nwhich those latter can be profitably used with respect to the former,\naccounting for both CPU and GPU computing. Our approach contributes also to the\nstudy of Learned Data Structures, a recent proposal to improve the time/space\nperformance of fundamental Data Structures, e.g., B-trees, Hash Tables, Bloom\nFilters. Indeed, we also formalize an Algorithmic Paradigm of Learned\nDichotomic Sorted Table Search procedures that naturally complements the\nLearned one proposed here and that characterizes most of the known Sorted Table\nSearch Procedures as having a \"learning phase\" that approximates Simple Linear\nRegression.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 16:26:54 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 16:10:43 GMT"}, {"version": "v3", "created": "Thu, 30 Jul 2020 11:56:44 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Amato", "Domenico", ""], ["Bosco", "Giosu\u00e9 Lo", ""], ["Giancarlo", "Raffaele", ""]]}, {"id": "2007.10252", "submitter": "Xingjian Li", "authors": "Xingjian Li, Haoyi Xiong, Haozhe An, Chengzhong Xu, Dejing Dou", "title": "XMixup: Efficient Transfer Learning with Auxiliary Samples by\n  Cross-domain Mixup", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transferring knowledge from large source datasets is an effective way to\nfine-tune the deep neural networks of the target task with a small sample size.\nA great number of algorithms have been proposed to facilitate deep transfer\nlearning, and these techniques could be generally categorized into two groups -\nRegularized Learning of the target task using models that have been pre-trained\nfrom source datasets, and Multitask Learning with both source and target\ndatasets to train a shared backbone neural network. In this work, we aim to\nimprove the multitask paradigm for deep transfer learning via Cross-domain\nMixup (XMixup). While the existing multitask learning algorithms need to run\nbackpropagation over both the source and target datasets and usually consume a\nhigher gradient complexity, XMixup transfers the knowledge from source to\ntarget tasks more efficiently: for every class of the target task, XMixup\nselects the auxiliary samples from the source dataset and augments training\nsamples via the simple mixup strategy. We evaluate XMixup over six real world\ntransfer learning datasets. Experiment results show that XMixup improves the\naccuracy by 1.9% on average. Compared with other state-of-the-art transfer\nlearning approaches, XMixup costs much less training time while still obtains\nhigher accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 16:42:29 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Li", "Xingjian", ""], ["Xiong", "Haoyi", ""], ["An", "Haozhe", ""], ["Xu", "Chengzhong", ""], ["Dou", "Dejing", ""]]}, {"id": "2007.10256", "submitter": "Cedric Schockaert", "authors": "Cedric Schockaert, Vadim Macher, Alexander Schmitz", "title": "VAE-LIME: Deep Generative Model Based Approach for Local Data-Driven\n  Model Interpretability Applied to the Ironmaking Industry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning applied to generate data-driven models are lacking of\ntransparency leading the process engineer to lose confidence in relying on the\nmodel predictions to optimize his industrial process. Bringing processes in the\nindustry to a certain level of autonomy using data-driven models is\nparticularly challenging as the first user of those models, is the expert in\nthe process with often decades of experience. It is necessary to expose to the\nprocess engineer, not solely the model predictions, but also their\ninterpretability. To that end, several approaches have been proposed in the\nliterature. The Local Interpretable Model-agnostic Explanations (LIME) method\nhas gained a lot of interest from the research community recently. The\nprinciple of this method is to train a linear model that is locally\napproximating the black-box model, by generating randomly artificial data\npoints locally. Model-agnostic local interpretability solutions based on LIME\nhave recently emerged to improve the original method. We present in this paper\na novel approach, VAE-LIME, for local interpretability of data-driven models\nforecasting the temperature of the hot metal produced by a blast furnace. Such\nironmaking process data is characterized by multivariate time series with high\ninter-correlation representing the underlying process in a blast furnace. Our\ncontribution is to use a Variational Autoencoder (VAE) to learn the complex\nblast furnace process characteristics from the data. The VAE is aiming at\ngenerating optimal artificial samples to train a local interpretable model\nbetter representing the black-box model in the neighborhood of the input sample\nprocessed by the black-box model to make a prediction. In comparison with LIME,\nVAE-LIME is showing a significantly improved local fidelity of the local\ninterpretable linear model with the black-box model resulting in robust model\ninterpretability.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 07:07:07 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Schockaert", "Cedric", ""], ["Macher", "Vadim", ""], ["Schmitz", "Alexander", ""]]}, {"id": "2007.10261", "submitter": "Vassilis N. Ioannidis", "authors": "Vassilis N. Ioannidis, Da Zheng, George Karypis", "title": "Few-shot link prediction via graph neural networks for Covid-19\n  drug-repurposing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting interactions among heterogenous graph structured data has numerous\napplications such as knowledge graph completion, recommendation systems and\ndrug discovery. Often times, the links to be predicted belong to rare types\nsuch as the case in repurposing drugs for novel diseases. This motivates the\ntask of few-shot link prediction. Typically, GCNs are ill-equipped in learning\nsuch rare link types since the relation embedding is not learned in an\ninductive fashion. This paper proposes an inductive RGCN for learning\ninformative relation embeddings even in the few-shot learning regime. The\nproposed inductive model significantly outperforms the RGCN and\nstate-of-the-art KGE models in few-shot learning tasks. Furthermore, we apply\nour method on the drug-repurposing knowledge graph (DRKG) for discovering drugs\nfor Covid-19. We pose the drug discovery task as link prediction and learn\nembeddings for the biological entities that partake in the DRKG. Our initial\nresults corroborate that several drugs used in clinical trials were identified\nas possible drug candidates. The method in this paper are implemented using the\nefficient deep graph learning (DGL)\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 16:48:51 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Ioannidis", "Vassilis N.", ""], ["Zheng", "Da", ""], ["Karypis", "George", ""]]}, {"id": "2007.10263", "submitter": "Peyton Greenside", "authors": "Vivek Myers and Peyton Greenside", "title": "A Hierarchical Approach to Scaling Batch Active Search Over Structured\n  Data", "comments": "Presented at the 2020 ICML Workshop on Real World Experiment Design\n  and Active Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Active search is the process of identifying high-value data points in a large\nand often high-dimensional parameter space that can be expensive to evaluate.\nTraditional active search techniques like Bayesian optimization trade off\nexploration and exploitation over consecutive evaluations, and have\nhistorically focused on single or small (<5) numbers of examples evaluated per\nround. As modern data sets grow, so does the need to scale active search to\nlarge data sets and batch sizes. In this paper, we present a general\nhierarchical framework based on bandit algorithms to scale active search to\nlarge batch sizes by maximizing information derived from the unique structure\nof each dataset. Our hierarchical framework, Hierarchical Batch Bandit Search\n(HBBS), strategically distributes batch selection across a learned embedding\nspace by facilitating wide exploration of different structural elements within\na dataset. We focus our application of HBBS on modern biology, where large\nbatch experimentation is often fundamental to the research process, and\ndemonstrate batch design of biological sequences (protein and DNA). We also\npresent a new Gym environment to easily simulate diverse biological sequences\nand to enable more comprehensive evaluation of active search methods across\nheterogeneous data sets. The HBBS framework improves upon standard performance,\nwall-clock, and scalability benchmarks for batch search by using a broad\nexploration strategy across coarse partitions and fine-grained exploitation\nwithin each partition of structured data.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 16:50:25 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Myers", "Vivek", ""], ["Greenside", "Peyton", ""]]}, {"id": "2007.10281", "submitter": "Pranay Pasula", "authors": "Pranay Pasula", "title": "Complex Skill Acquisition Through Simple Skill Imitation Learning", "comments": "7 pages, 2 figures; Fixed typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans often think of complex tasks as combinations of simpler subtasks in\norder to learn those complex tasks more efficiently. For example, a backflip\ncould be considered a combination of four subskills: jumping, tucking knees,\nrolling backwards, and thrusting arms downwards. Motivated by this line of\nreasoning, we propose a new algorithm that trains neural network policies on\nsimple, easy-to-learn skills in order to cultivate latent spaces that\naccelerate imitation learning of complex, hard-to-learn skills. We focus on the\ncase in which the complex task comprises a concurrent (and possibly sequential)\ncombination of the simpler subtasks, and therefore our algorithm can be seen as\na novel approach to concurrent hierarchical imitation learning. We evaluate our\nalgorithm on difficult tasks in a high-dimensional environment and find that it\nconsistently outperforms a state-of-the-art baseline in training speed and\noverall performance.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 17:06:26 GMT"}, {"version": "v2", "created": "Sun, 13 Sep 2020 20:27:38 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 17:45:38 GMT"}, {"version": "v4", "created": "Mon, 19 Oct 2020 19:43:49 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Pasula", "Pranay", ""]]}, {"id": "2007.10297", "submitter": "Neil Walton", "authors": "Neil Walton", "title": "A Short Note on Soft-max and Policy Gradients in Bandits Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a short communication on a Lyapunov function argument for softmax in\nbandit problems. There are a number of excellent papers coming out using\ndifferential equations for policy gradient algorithms in reinforcement learning\n\\cite{agarwal2019optimality,bhandari2019global,mei2020global}. We give a short\nargument that gives a regret bound for the soft-max ordinary differential\nequation for bandit problems. We derive a similar result for a different policy\ngradient algorithm, again for bandit problems. For this second algorithm, it is\npossible to prove regret bounds in the stochastic case \\cite{DW20}. At the end,\nwe summarize some ideas and issues on deriving stochastic regret bounds for\npolicy gradients.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 17:30:27 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Walton", "Neil", ""]]}, {"id": "2007.10306", "submitter": "Stephen Pfohl", "authors": "Stephen R. Pfohl, Agata Foryciarz, Nigam H. Shah", "title": "An Empirical Characterization of Fair Machine Learning For Clinical Risk\n  Prediction", "comments": "Published in the Journal of Biomedical Informatics\n  (https://doi.org/10.1016/j.jbi.2020.103621). Version 3 updates\n  acknowledgements and fixes typos", "journal-ref": "Journal of Biomedical Informatics, Volume 113, January 2021,\n  103621", "doi": "10.1016/j.jbi.2020.103621", "report-no": null, "categories": "stat.ML cs.CY cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of machine learning to guide clinical decision making has the\npotential to worsen existing health disparities. Several recent works frame the\nproblem as that of algorithmic fairness, a framework that has attracted\nconsiderable attention and criticism. However, the appropriateness of this\nframework is unclear due to both ethical as well as technical considerations,\nthe latter of which include trade-offs between measures of fairness and model\nperformance that are not well-understood for predictive models of clinical\noutcomes. To inform the ongoing debate, we conduct an empirical study to\ncharacterize the impact of penalizing group fairness violations on an array of\nmeasures of model performance and group fairness. We repeat the analyses across\nmultiple observational healthcare databases, clinical outcomes, and sensitive\nattributes. We find that procedures that penalize differences between the\ndistributions of predictions across groups induce nearly-universal degradation\nof multiple performance metrics within groups. On examining the secondary\nimpact of these procedures, we observe heterogeneity of the effect of these\nprocedures on measures of fairness in calibration and ranking across\nexperimental conditions. Beyond the reported trade-offs, we emphasize that\nanalyses of algorithmic fairness in healthcare lack the contextual grounding\nand causal awareness necessary to reason about the mechanisms that lead to\nhealth disparities, as well as about the potential of algorithmic fairness\nmethods to counteract those mechanisms. In light of these limitations, we\nencourage researchers building predictive models for clinical use to step\noutside the algorithmic fairness frame and engage critically with the broader\nsociotechnical context surrounding the use of machine learning in healthcare.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 17:46:31 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 19:25:57 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 15:28:53 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Pfohl", "Stephen R.", ""], ["Foryciarz", "Agata", ""], ["Shah", "Nigam H.", ""]]}, {"id": "2007.10307", "submitter": "Arvind Mahankali", "authors": "Arvind V. Mahankali (1), David P. Woodruff (1) ((1) Carnegie Mellon\n  University)", "title": "Optimal $\\ell_1$ Column Subset Selection and a Fast PTAS for Low Rank\n  Approximation", "comments": "To appear in SODA 2021. Changes: (1) Fixed errors in hardness proof\n  for constrained $\\ell_1$ low rank approximation. (2) Simplified analysis of\n  column subset selection algorithm. (3) Improved runtime of\n  $\\text{poly}(k)$-approximation algorithm with output rank $k$ from\n  $2^{O(k\\log k)} + \\text{poly}(nd)$ to $\\text{poly}(nd)$. Results are\n  unchanged aside from (3)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of entrywise $\\ell_1$ low rank approximation. We give\nthe first polynomial time column subset selection-based $\\ell_1$ low rank\napproximation algorithm sampling $\\tilde{O}(k)$ columns and achieving an\n$\\tilde{O}(k^{1/2})$-approximation for any $k$, improving upon the previous\nbest $\\tilde{O}(k)$-approximation and matching a prior lower bound for column\nsubset selection-based $\\ell_1$-low rank approximation which holds for any\n$\\text{poly}(k)$ number of columns. We extend our results to obtain tight upper\nand lower bounds for column subset selection-based $\\ell_p$ low rank\napproximation for any $1 < p < 2$, closing a long line of work on this problem.\n  We next give a $(1 + \\varepsilon)$-approximation algorithm for entrywise\n$\\ell_p$ low rank approximation, for $1 \\leq p < 2$, that is not a column\nsubset selection algorithm. First, we obtain an algorithm which, given a matrix\n$A \\in \\mathbb{R}^{n \\times d}$, returns a rank-$k$ matrix $\\hat{A}$ in\n$2^{\\text{poly}(k/\\varepsilon)} + \\text{poly}(nd)$ running time such that:\n$$\\|A - \\hat{A}\\|_p \\leq (1 + \\varepsilon) \\cdot OPT +\n\\frac{\\varepsilon}{\\text{poly}(k)}\\|A\\|_p$$ where $OPT = \\min_{A_k \\text{ rank\n}k} \\|A - A_k\\|_p$. Using this algorithm, in the same running time we give an\nalgorithm which obtains error at most $(1 + \\varepsilon) \\cdot OPT$ and outputs\na matrix of rank at most $3k$ -- these algorithms significantly improve upon\nall previous $(1 + \\varepsilon)$- and $O(1)$-approximation algorithms for the\n$\\ell_p$ low rank approximation problem, which required at least\n$n^{\\text{poly}(k/\\varepsilon)}$ or $n^{\\text{poly}(k)}$ running time, and\neither required strong bit complexity assumptions (our algorithms do not) or\nhad bicriteria rank $3k$. Finally, we show hardness results which nearly match\nour $2^{\\text{poly}(k)} + \\text{poly}(nd)$ running time and the above additive\nerror guarantee.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 17:50:30 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 07:22:43 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Mahankali", "Arvind V.", ""], ["Woodruff", "David P.", ""]]}, {"id": "2007.10329", "submitter": "Woojay Jeon", "authors": "Woojay Jeon", "title": "Acoustic Neighbor Embeddings", "comments": "Anonymized version submitted to ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel acoustic word embedding called Acoustic Neighbor\nEmbeddings where speech or text of arbitrary length are mapped to a vector\nspace of fixed, reduced dimensions by adapting stochastic neighbor embedding\n(SNE) to sequential inputs. The Euclidean distance between coordinates in the\nembedding space reflects the phonetic confusability between their corresponding\nsequences. Two encoder neural networks are trained: an acoustic encoder that\naccepts speech signals in the form of frame-wise subword posterior\nprobabilities obtained from an acoustic model and a text encoder that accepts\ntext in the form of subword transcriptions. Compared to a triplet loss\ncriterion, the proposed method is shown to have more effective gradients for\nneural network training. Experimentally, it also gives more accurate results\nwith low-dimensional embeddings when the two encoder networks are used in\ntandem in a word (name) recognition task, and when the text encoder network is\nused standalone in an approximate phonetic matching task. In particular, in an\nisolated name recognition task depending solely on Euclidean nearest-neighbor\nsearch between the proposed embedding vectors, the recognition accuracy is\nidentical to that of conventional finite state transducer(FST)-based decoding\nusing test data with up to 1 million names in the vocabulary and 40 dimensions\nin the embeddings.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 05:33:07 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 22:47:23 GMT"}, {"version": "v3", "created": "Fri, 2 Oct 2020 05:28:25 GMT"}, {"version": "v4", "created": "Thu, 26 Nov 2020 17:53:39 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Jeon", "Woojay", ""]]}, {"id": "2007.10333", "submitter": "Karan Yang", "authors": "Karan Yang, Chengxi Zang, Fei Wang", "title": "Visualizing Deep Graph Generative Models for Drug Discovery", "comments": "4 pages, 2020 KDD Workshop on Applied Data Science for Healthcare", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drug discovery aims at designing novel molecules with specific desired\nproperties for clinical trials. Over past decades, drug discovery and\ndevelopment have been a costly and time consuming process. Driven by big\nchemical data and AI, deep generative models show great potential to accelerate\nthe drug discovery process. Existing works investigate different deep\ngenerative frameworks for molecular generation, however, less attention has\nbeen paid to the visualization tools to quickly demo and evaluate model's\nresults. Here, we propose a visualization framework which provides interactive\nvisualization tools to visualize molecules generated during the encoding and\ndecoding process of deep graph generative models, and provide real time\nmolecular optimization functionalities. Our work tries to empower black box AI\ndriven drug discovery models with some visual interpretabilities.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 18:49:10 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Yang", "Karan", ""], ["Zang", "Chengxi", ""], ["Wang", "Fei", ""]]}, {"id": "2007.10389", "submitter": "Wei Cheng", "authors": "Wei Cheng, Gregory Darnell, Sohini Ramachandran, Lorin Crawford", "title": "Generalizing Variational Autoencoders with Hierarchical Empirical Bayes", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Autoencoders (VAEs) have experienced recent success as\ndata-generating models by using simple architectures that do not require\nsignificant fine-tuning of hyperparameters. However, VAEs are known to suffer\nfrom over-regularization which can lead to failure to escape local maxima. This\nphenomenon, known as posterior collapse, prevents learning a meaningful latent\nencoding of the data. Recent methods have mitigated this issue by\ndeterministically moment-matching an aggregated posterior distribution to an\naggregate prior. However, abandoning a probabilistic framework (and thus\nrelying on point estimates) can both lead to a discontinuous latent space and\ngenerate unrealistic samples. Here we present Hierarchical Empirical Bayes\nAutoencoder (HEBAE), a computationally stable framework for probabilistic\ngenerative models. Our key contributions are two-fold. First, we make gains by\nplacing a hierarchical prior over the encoding distribution, enabling us to\nadaptively balance the trade-off between minimizing the reconstruction loss\nfunction and avoiding over-regularization. Second, we show that assuming a\ngeneral dependency structure between variables in the latent space produces\nbetter convergence onto the mean-field assumption for improved posterior\ninference. Overall, HEBAE is more robust to a wide-range of hyperparameter\ninitializations than an analogous VAE. Using data from MNIST and CelebA, we\nillustrate the ability of HEBAE to generate higher quality samples based on FID\nscore than existing autoencoder-based approaches.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 18:18:39 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Cheng", "Wei", ""], ["Darnell", "Gregory", ""], ["Ramachandran", "Sohini", ""], ["Crawford", "Lorin", ""]]}, {"id": "2007.10394", "submitter": "Tsuyoshi Okita", "authors": "Tsuyoshi Okita and Hirotaka Hachiya and Sozo Inoue and Naonori Ueda", "title": "Translation Between Waves, wave2wave", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The understanding of sensor data has been greatly improved by advanced deep\nlearning methods with big data. However, available sensor data in the real\nworld are still limited, which is called the opportunistic sensor problem. This\npaper proposes a new variant of neural machine translation seq2seq to deal with\ncontinuous signal waves by introducing the window-based (inverse-)\nrepresentation to adaptively represent partial shapes of waves and the\niterative back-translation model for high-dimensional data. Experimental\nresults are shown for two real-life data: earthquake and activity translation.\nThe performance improvements of one-dimensional data was about 46% in test loss\nand that of high-dimensional data was about 1625% in perplexity with regard to\nthe original seq2seq.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 18:29:09 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Okita", "Tsuyoshi", ""], ["Hachiya", "Hirotaka", ""], ["Inoue", "Sozo", ""], ["Ueda", "Naonori", ""]]}, {"id": "2007.10412", "submitter": "Deniz Oktay", "authors": "Deniz Oktay, Nick McGreivy, Joshua Aduol, Alex Beatson, Ryan P. Adams", "title": "Randomized Automatic Differentiation", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The successes of deep learning, variational inference, and many other fields\nhave been aided by specialized implementations of reverse-mode automatic\ndifferentiation (AD) to compute gradients of mega-dimensional objectives. The\nAD techniques underlying these tools were designed to compute exact gradients\nto numerical precision, but modern machine learning models are almost always\ntrained with stochastic gradient descent. Why spend computation and memory on\nexact (minibatch) gradients only to use them for stochastic optimization? We\ndevelop a general framework and approach for randomized automatic\ndifferentiation (RAD), which can allow unbiased gradient estimates to be\ncomputed with reduced memory in return for variance. We examine limitations of\nthe general approach, and argue that we must leverage problem specific\nstructure to realize benefits. We develop RAD techniques for a variety of\nsimple neural network architectures, and show that for a fixed memory budget,\nRAD converges in fewer iterations than using a small batch size for feedforward\nnetworks, and in a similar number for recurrent networks. We also show that RAD\ncan be applied to scientific computing, and use it to develop a low-memory\nstochastic gradient method for optimizing the control parameters of a linear\nreaction-diffusion PDE representing a fission reactor.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 19:03:44 GMT"}, {"version": "v2", "created": "Sat, 13 Mar 2021 18:28:19 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Oktay", "Deniz", ""], ["McGreivy", "Nick", ""], ["Aduol", "Joshua", ""], ["Beatson", "Alex", ""], ["Adams", "Ryan P.", ""]]}, {"id": "2007.10417", "submitter": "Jake Snell", "authors": "Jake Snell, Richard Zemel", "title": "Bayesian Few-Shot Classification with One-vs-Each P\\'olya-Gamma\n  Augmented Gaussian Processes", "comments": "Extended version of accepted ICLR 2021 submission. 34 pages, 9\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot classification (FSC), the task of adapting a classifier to unseen\nclasses given a small labeled dataset, is an important step on the path toward\nhuman-like machine learning. Bayesian methods are well-suited to tackling the\nfundamental issue of overfitting in the few-shot scenario because they allow\npractitioners to specify prior beliefs and update those beliefs in light of\nobserved data. Contemporary approaches to Bayesian few-shot classification\nmaintain a posterior distribution over model parameters, which is slow and\nrequires storage that scales with model size. Instead, we propose a Gaussian\nprocess classifier based on a novel combination of P\\'olya-Gamma augmentation\nand the one-vs-each softmax approximation that allows us to efficiently\nmarginalize over functions rather than model parameters. We demonstrate\nimproved accuracy and uncertainty quantification on both standard few-shot\nclassification benchmarks and few-shot domain transfer tasks.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 19:10:41 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2021 19:54:57 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Snell", "Jake", ""], ["Zemel", "Richard", ""]]}, {"id": "2007.10445", "submitter": "Vassilis N. Ioannidis", "authors": "Vassilis N. Ioannidis, Da Zheng, George Karypis", "title": "PanRep: Graph neural networks for extracting universal node embeddings\n  in heterogeneous graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning unsupervised node embeddings facilitates several downstream tasks\nsuch as node classification and link prediction. A node embedding is universal\nif it is designed to be used by and benefit various downstream tasks. This work\nintroduces PanRep, a graph neural network (GNN) model, for unsupervised\nlearning of universal node representations for heterogenous graphs. PanRep\nconsists of a GNN encoder that obtains node embeddings and four decoders, each\ncapturing different topological and node feature properties. Abiding to these\nproperties the novel unsupervised framework learns universal embeddings\napplicable to different downstream tasks. PanRep can be furthered fine-tuned to\naccount for possible limited labels. In this operational setting PanRep is\nconsidered as a pretrained model for extracting node embeddings of heterogenous\ngraph data. PanRep outperforms all unsupervised and certain supervised methods\nin node classification and link prediction, especially when the labeled data\nfor the supervised methods is small. PanRep-FT (with fine-tuning) outperforms\nall other supervised approaches, which corroborates the merits of pretraining\nmodels. Finally, we apply PanRep-FT for discovering novel drugs for Covid-19.\nWe showcase the advantage of universal embeddings in drug repurposing and\nidentify several drugs used in clinical trials as possible drug candidates.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 20:14:10 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 15:49:05 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Ioannidis", "Vassilis N.", ""], ["Zheng", "Da", ""], ["Karypis", "George", ""]]}, {"id": "2007.10449", "submitter": "Zebang Shen", "authors": "Zebang Shen, Zhenfu Wang, Alejandro Ribeiro, Hamed Hassani", "title": "Sinkhorn Barycenter via Functional Gradient Descent", "comments": "submitted to NIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of computing the barycenter of a set\nof probability distributions under the Sinkhorn divergence.\n  This problem has recently found applications across various domains,\nincluding graphics, learning, and vision, as it provides a meaningful mechanism\nto aggregate knowledge.\n  Unlike previous approaches which directly operate in the space of probability\nmeasures, we recast the Sinkhorn barycenter problem as an instance of\nunconstrained functional optimization and develop a novel functional gradient\ndescent method named Sinkhorn Descent (SD).\n  We prove that SD converges to a stationary point at a sublinear rate, and\nunder reasonable assumptions, we further show that it asymptotically finds a\nglobal minimizer of the Sinkhorn barycenter problem. Moreover, by providing a\nmean-field analysis, we show that SD preserves the weak convergence of\nempirical measures.\n  Importantly, the computational complexity of SD scales linearly in the\ndimension $d$ and we demonstrate its scalability by solving a $100$-dimensional\nSinkhorn barycenter problem.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 20:16:47 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Shen", "Zebang", ""], ["Wang", "Zhenfu", ""], ["Ribeiro", "Alejandro", ""], ["Hassani", "Hamed", ""]]}, {"id": "2007.10455", "submitter": "Andrew Jones", "authors": "Andrew Jones and Patrick Rubin-Delanchy", "title": "The multilayer random dot product graph", "comments": "45 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a comprehensive extension of the latent position network model\nknown as the random dot product graph to accommodate multiple graphs -- both\nundirected and directed -- which share a common subset of nodes, and propose a\nmethod for jointly embedding the associated adjacency matrices, or submatrices\nthereof, into a suitable latent space. Theoretical results concerning the\nasymptotic behaviour of the node representations thus obtained are established,\nshowing that after the application of a linear transformation these converge\nuniformly in the Euclidean norm to the latent positions with Gaussian error.\nWithin this framework, we present a generalisation of the stochastic block\nmodel to a number of different multiple graph settings, and demonstrate the\neffectiveness of our joint embedding method through several statistical\ninference tasks in which we achieve comparable or better results than rival\nspectral methods. Empirical improvements in link prediction over single graph\nembeddings are exhibited in a cyber-security example.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 20:31:39 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 15:17:45 GMT"}, {"version": "v3", "created": "Mon, 25 Jan 2021 10:00:28 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Jones", "Andrew", ""], ["Rubin-Delanchy", "Patrick", ""]]}, {"id": "2007.10463", "submitter": "Ying Wang", "authors": "Ying Wang, Yadong Lu and Tijmen Blankevoort", "title": "Differentiable Joint Pruning and Quantization for Hardware Efficiency", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a differentiable joint pruning and quantization (DJPQ) scheme. We\nframe neural network compression as a joint gradient-based optimization\nproblem, trading off between model pruning and quantization automatically for\nhardware efficiency. DJPQ incorporates variational information bottleneck based\nstructured pruning and mixed-bit precision quantization into a single\ndifferentiable loss function. In contrast to previous works which consider\npruning and quantization separately, our method enables users to find the\noptimal trade-off between both in a single training procedure. To utilize the\nmethod for more efficient hardware inference, we extend DJPQ to integrate\nstructured pruning with power-of-two bit-restricted quantization. We show that\nDJPQ significantly reduces the number of Bit-Operations (BOPs) for several\nnetworks while maintaining the top-1 accuracy of original floating-point models\n(e.g., 53x BOPs reduction in ResNet18 on ImageNet, 43x in MobileNetV2).\nCompared to the conventional two-stage approach, which optimizes pruning and\nquantization independently, our scheme outperforms in terms of both accuracy\nand BOPs. Even when considering bit-restricted quantization, DJPQ achieves\nlarger compression ratios and better accuracy than the two-stage approach.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 20:45:47 GMT"}, {"version": "v2", "created": "Sun, 4 Apr 2021 18:45:08 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wang", "Ying", ""], ["Lu", "Yadong", ""], ["Blankevoort", "Tijmen", ""]]}, {"id": "2007.10493", "submitter": "Lai Wei", "authors": "Lai Wei and Vaibhav Srivastava", "title": "Minimax Policy for Heavy-tailed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the stochastic Multi-Armed Bandit (MAB) problem under worst-case\nregret and heavy-tailed reward distribution. We modify the minimax policy MOSS\nfor the sub-Gaussian reward distribution by using saturated empirical mean to\ndesign a new algorithm called Robust MOSS. We show that if the moment of order\n$1+\\epsilon$ for the reward distribution exists, then the refined strategy has\na worst-case regret matching the lower bound while maintaining a\ndistribution-dependent logarithm regret.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 21:43:57 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 01:34:32 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Wei", "Lai", ""], ["Srivastava", "Vaibhav", ""]]}, {"id": "2007.10504", "submitter": "Jonathan Chung", "authors": "Jonathan Chung, Anna Luo, Xavier Raffin, Scott Perry", "title": "Battlesnake Challenge: A Multi-agent Reinforcement Learning Playground\n  with Human-in-the-loop", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Battlesnake Challenge, a framework for multi-agent\nreinforcement learning with Human-In-the-Loop Learning (HILL). It is developed\nupon Battlesnake, a multiplayer extension of the traditional Snake game in\nwhich 2 or more snakes compete for the final survival. The Battlesnake\nChallenge consists of an offline module for model training and an online module\nfor live competitions. We develop a simulated game environment for the offline\nmulti-agent model training and identify a set of baseline heuristics that can\nbe instilled to improve learning. Our framework is agent-agnostic and\nheuristics-agnostic such that researchers can design their own algorithms,\ntrain their models, and demonstrate in the online Battlesnake competition. We\nvalidate the framework and baseline heuristics with our preliminary\nexperiments. Our results show that agents with the proposed HILL methods\nconsistently outperform agents without HILL. Besides, heuristics of reward\nmanipulation had the best performance in the online competition. We open source\nour framework at https://github.com/awslabs/sagemaker-battlesnake-ai.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 21:59:53 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Chung", "Jonathan", ""], ["Luo", "Anna", ""], ["Raffin", "Xavier", ""], ["Perry", "Scott", ""]]}, {"id": "2007.10505", "submitter": "Sarath Shekkizhar", "authors": "Sarath Shekkizhar, Antonio Ortega", "title": "DeepNNK: Explaining deep models and their generalization using polytope\n  interpolation", "comments": "Submitted for review at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern machine learning systems based on neural networks have shown great\nsuccess in learning complex data patterns while being able to make good\npredictions on unseen data points. However, the limited interpretability of\nthese systems hinders further progress and application to several domains in\nthe real world. This predicament is exemplified by time consuming model\nselection and the difficulties faced in predictive explainability, especially\nin the presence of adversarial examples. In this paper, we take a step towards\nbetter understanding of neural networks by introducing a local polytope\ninterpolation method. The proposed Deep Non Negative Kernel regression (NNK)\ninterpolation framework is non parametric, theoretically simple and\ngeometrically intuitive. We demonstrate instance based explainability for deep\nlearning models and develop a method to identify models with good\ngeneralization properties using leave one out estimation. Finally, we draw a\nrationalization to adversarial and generative examples which are inevitable\nfrom an interpolation view of machine learning.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 22:05:24 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Shekkizhar", "Sarath", ""], ["Ortega", "Antonio", ""]]}, {"id": "2007.10507", "submitter": "Michael Park", "authors": "Michael Park", "title": "Moment-Matching Graph-Networks for Causal Inference", "comments": "18 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we explore a fully unsupervised deep-learning framework for\nsimulating non-linear structural equation models from observational training\ndata. The main contribution of this note is an architecture for applying\nmoment-matching loss functions to the edges of a causal Bayesian graph,\nresulting in a generative conditional-moment-matching graph-neural-network.\nThis framework thus enables automated sampling of latent space conditional\nprobability distributions for various graphical interventions, and is capable\nof generating out-of-sample interventional probabilities that are often\nfaithful to the ground truth distributions well beyond the range contained in\nthe training set. These methods could in principle be used in conjunction with\nany existing autoencoder that produces a latent space representation containing\ncausal graph structures.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 22:07:43 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 13:13:40 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Park", "Michael", ""]]}, {"id": "2007.10527", "submitter": "Sebastian Musslick", "authors": "Sachin Ravi and Sebastian Musslick and Maia Hamin and Theodore L.\n  Willke and Jonathan D. Cohen", "title": "Navigating the Trade-Off between Multi-Task Learning and Learning to\n  Multitask in Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The terms multi-task learning and multitasking are easily confused.\nMulti-task learning refers to a paradigm in machine learning in which a network\nis trained on various related tasks to facilitate the acquisition of tasks. In\ncontrast, multitasking is used to indicate, especially in the cognitive science\nliterature, the ability to execute multiple tasks simultaneously. While\nmulti-task learning exploits the discovery of common structure between tasks in\nthe form of shared representations, multitasking is promoted by separating\nrepresentations between tasks to avoid processing interference. Here, we build\non previous work involving shallow networks and simple task settings suggesting\nthat there is a trade-off between multi-task learning and multitasking,\nmediated by the use of shared versus separated representations. We show that\nthe same tension arises in deep networks and discuss a meta-learning algorithm\nfor an agent to manage this trade-off in an unfamiliar environment. We display\nthrough different experiments that the agent is able to successfully optimize\nits training strategy as a function of the environment.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 23:26:16 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 18:16:35 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Ravi", "Sachin", ""], ["Musslick", "Sebastian", ""], ["Hamin", "Maia", ""], ["Willke", "Theodore L.", ""], ["Cohen", "Jonathan D.", ""]]}, {"id": "2007.10532", "submitter": "Ke Xu", "authors": "Brian Barr, Ke Xu, Claudio Silva, Enrico Bertini, Robert Reilly, C.\n  Bayan Bruss, Jason D. Wittenbach", "title": "Towards Ground Truth Explainability on Tabular Data", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In data science, there is a long history of using synthetic data for method\ndevelopment, feature selection and feature engineering. Our current interest in\nsynthetic data comes from recent work in explainability. Today's datasets are\ntypically larger and more complex - requiring less interpretable models. In the\nsetting of \\textit{post hoc} explainability, there is no ground truth for\nexplanations. Inspired by recent work in explaining image classifiers that does\nprovide ground truth, we propose a similar solution for tabular data. Using\ncopulas, a concise specification of the desired statistical properties of a\ndataset, users can build intuition around explainability using controlled data\nsets and experimentation. The current capabilities are demonstrated on three\nuse cases: one dimensional logistic regression, impact of correlation from\ninformative features, impact of correlation from redundant variables.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 23:42:51 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Barr", "Brian", ""], ["Xu", "Ke", ""], ["Silva", "Claudio", ""], ["Bertini", "Enrico", ""], ["Reilly", "Robert", ""], ["Bruss", "C. Bayan", ""], ["Wittenbach", "Jason D.", ""]]}, {"id": "2007.10567", "submitter": "Da Yu", "authors": "Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, Tie-Yan Liu", "title": "How Does Data Augmentation Affect Privacy in Machine Learning?", "comments": "AAAI Conference on Artificial Intelligence (AAAI-21). Source code\n  available at: https://github.com/dayu11/MI_with_DA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is observed in the literature that data augmentation can significantly\nmitigate membership inference (MI) attack. However, in this work, we challenge\nthis observation by proposing new MI attacks to utilize the information of\naugmented data. MI attack is widely used to measure the model's information\nleakage of the training set. We establish the optimal membership inference when\nthe model is trained with augmented data, which inspires us to formulate the MI\nattack as a set classification problem, i.e., classifying a set of augmented\ninstances instead of a single data point, and design input permutation\ninvariant features. Empirically, we demonstrate that the proposed approach\nuniversally outperforms original methods when the model is trained with data\naugmentation. Even further, we show that the proposed approach can achieve\nhigher MI attack success rates on models trained with some data augmentation\nthan the existing methods on models trained without data augmentation. Notably,\nwe achieve a 70.1% MI attack success rate on CIFAR10 against a wide residual\nnetwork while the previous best approach only attains 61.9%. This suggests the\nprivacy risk of models trained with data augmentation could be largely\nunderestimated.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 02:21:10 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 12:37:36 GMT"}, {"version": "v3", "created": "Fri, 26 Feb 2021 05:21:51 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Yu", "Da", ""], ["Zhang", "Huishuai", ""], ["Chen", "Wei", ""], ["Yin", "Jian", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2007.10593", "submitter": "Nannan Li", "authors": "Nannan Li and Zhenzhong Chen", "title": "Towards Visual Distortion in Black-Box Attacks", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2021.3092822", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constructing adversarial examples in a black-box threat model injures the\noriginal images by introducing visual distortion. In this paper, we propose a\nnovel black-box attack approach that can directly minimize the induced\ndistortion by learning the noise distribution of the adversarial example,\nassuming only loss-oracle access to the black-box network. The quantified\nvisual distortion, which measures the perceptual distance between the\nadversarial example and the original image, is introduced in our loss whilst\nthe gradient of the corresponding non-differentiable loss function is\napproximated by sampling noise from the learned noise distribution. We validate\nthe effectiveness of our attack on ImageNet. Our attack results in much lower\ndistortion when compared to the state-of-the-art black-box attacks and achieves\n$100\\%$ success rate on InceptionV3, ResNet50 and VGG16bn. The code is\navailable at https://github.com/Alina-1997/visual-distortion-in-attack.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 04:42:43 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 07:35:56 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Li", "Nannan", ""], ["Chen", "Zhenzhong", ""]]}, {"id": "2007.10596", "submitter": "Yang Liu", "authors": "Yang Liu and Jiaheng Wei", "title": "Incentives for Federated Learning: a Hypothesis Elicitation Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning provides a promising paradigm for collecting machine\nlearning models from distributed data sources without compromising users' data\nprivacy. The success of a credible federated learning system builds on the\nassumption that the decentralized and self-interested users will be willing to\nparticipate to contribute their local models in a trustworthy way. However,\nwithout proper incentives, users might simply opt out the contribution cycle,\nor will be mis-incentivized to contribute spam/false information. This paper\nintroduces solutions to incentivize truthful reporting of a local, user-side\nmachine learning model for federated learning. Our results build on the\nliterature of information elicitation, but focus on the questions of eliciting\nhypothesis (rather than eliciting human predictions). We provide a scoring rule\nbased framework that incentivizes truthful reporting of local hypotheses at a\nBayesian Nash Equilibrium. We study the market implementation, accuracy as well\nas robustness properties of our proposed solution too. We verify the\neffectiveness of our methods using MNIST and CIFAR-10 datasets. Particularly we\nshow that by reporting low-quality hypotheses, users will receive decreasing\nscores (rewards, or payments).\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 04:55:31 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Liu", "Yang", ""], ["Wei", "Jiaheng", ""]]}, {"id": "2007.10623", "submitter": "Katsuhiko Ishiguro", "authors": "Ruixiang Zhang, Masanori Koyama, Katsuhiko Ishiguro", "title": "Learning Structured Latent Factors from Dependent Data:A Generative\n  Model Framework from Information-Theoretic Perspective", "comments": "ICML2020 accepted paper. Author name fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning controllable and generalizable representation of multivariate data\nwith desired structural properties remains a fundamental problem in machine\nlearning. In this paper, we present a novel framework for learning generative\nmodels with various underlying structures in the latent space. We represent the\ninductive bias in the form of mask variables to model the dependency structure\nin the graphical model and extend the theory of multivariate information\nbottleneck to enforce it. Our model provides a principled approach to learn a\nset of semantically meaningful latent factors that reflect various types of\ndesired structures like capturing correlation or encoding invariance, while\nalso offering the flexibility to automatically estimate the dependency\nstructure from data. We show that our framework unifies many existing\ngenerative models and can be applied to a variety of tasks including\nmulti-modal data modeling, algorithmic fairness, and invariant risk\nminimization.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 06:59:29 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 04:22:54 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Zhang", "Ruixiang", ""], ["Koyama", "Masanori", ""], ["Ishiguro", "Katsuhiko", ""]]}, {"id": "2007.10626", "submitter": "Xiongjun Zhang", "authors": "Xiongjun Zhang and Michael K. Ng", "title": "Sparse Nonnegative Tensor Factorization and Completion with Noisy\n  Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the sparse nonnegative tensor factorization and\ncompletion problem from partial and noisy observations for third-order tensors.\nBecause of sparsity and nonnegativity, the underling tensor is decomposed into\nthe tensor-tensor product of one sparse nonnegative tensor and one nonnegative\ntensor. We propose to minimize the sum of the maximum likelihood estimate for\nthe observations with nonnegativity constraints and the tensor $\\ell_0$ norm\nfor the sparse factor. We show that the error bounds of the estimator of the\nproposed model can be established under general noise observations. The\ndetailed error bounds under specific noise distributions including additive\nGaussian noise, additive Laplace noise, and Poisson observations can be\nderived. Moreover, the minimax lower bounds are shown to be matched with the\nestablished upper bounds up to a logarithmic factor of the sizes of the\nunderlying tensor. These theoretical results for tensors are better than those\nobtained for matrices, and this illustrates the advantage of the use of\nnonnegative sparse tensor models for completion and denoising. Numerical\nexperiments are provided to validate the superiority of the proposed\ntensor-based method compared with the matrix-based approach.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 07:17:52 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 04:07:11 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Zhang", "Xiongjun", ""], ["Ng", "Michael K.", ""]]}, {"id": "2007.10637", "submitter": "Taewon Park", "authors": "Taewon Park, Inchul Choi, Minho Lee", "title": "Distributed Associative Memory Network with Memory Refreshing Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent progress in memory augmented neural network (MANN) research,\nassociative memory networks with a single external memory still show limited\nperformance on complex relational reasoning tasks. Especially the content-based\naddressable memory networks often fail to encode input data into rich enough\nrepresentation for relational reasoning and this limits the relation modeling\nperformance of MANN for long temporal sequence data. To address these problems,\nhere we introduce a novel Distributed Associative Memory architecture (DAM)\nwith Memory Refreshing Loss (MRL) which enhances the relation reasoning\nperformance of MANN. Inspired by how the human brain works, our framework\nencodes data with distributed representation across multiple memory blocks and\nrepeatedly refreshes the contents for enhanced memorization similar to the\nrehearsal process of the brain. For this procedure, we replace a single\nexternal memory with a set of multiple smaller associative memory blocks and\nupdate these sub-memory blocks simultaneously and independently for the\ndistributed representation of input data. Moreover, we propose MRL which\nassists a task's target objective while learning relational information\nexisting in data. MRL enables MANN to reinforce an association between input\ndata and task objective by reproducing stochastically sampled input data from\nstored memory contents. With this procedure, MANN further enriches the stored\nrepresentations with relational information. In experiments, we apply our\napproaches to Differential Neural Computer (DNC), which is one of the\nrepresentative content-based addressing memory models and achieves the\nstate-of-the-art performance on both memorization and relational reasoning\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 07:34:33 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 08:21:09 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Park", "Taewon", ""], ["Choi", "Inchul", ""], ["Lee", "Minho", ""]]}, {"id": "2007.10653", "submitter": "Alexis Bellot", "authors": "Alexis Bellot and Mihaela van der Schaar", "title": "Accounting for Unobserved Confounding in Domain Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to generalize from observed to new related environments is\ncentral to any form of reliable machine learning, yet most methods fail when\nmoving beyond i.i.d data. This work argues that in some cases the reason lies\nin a misapreciation of the causal structure in data; and in particular due to\nthe influence of unobserved confounders which void many of the invariances and\nprinciples of minimum error between environments presently used for the problem\nof domain generalization. This observation leads us to study generalization in\nthe context of a broader class of interventions in an underlying causal model\n(including changes in observed, unobserved and target variable distributions)\nand to connect this causal intuition with an explicit distributionally robust\noptimization problem. From this analysis derives a new proposal for model\nlearning with explicit generalization guarantees that is based on the partial\nequality of error derivatives with respect to model parameters. We demonstrate\nthe empirical performance of our approach on healthcare data from different\nmodalities, including image, speech and tabular data.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 08:18:06 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 15:53:05 GMT"}, {"version": "v3", "created": "Mon, 18 Jan 2021 14:47:21 GMT"}, {"version": "v4", "created": "Tue, 2 Feb 2021 09:31:32 GMT"}, {"version": "v5", "created": "Tue, 25 May 2021 09:08:49 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Bellot", "Alexis", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "2007.10685", "submitter": "Robert Schwarzenberg", "authors": "Robert Schwarzenberg, Steffen Castle", "title": "Pattern-Guided Integrated Gradients", "comments": "Presented at the ICML 2020 Workshop on Human Interpretability in\n  Machine Learning (WHI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrated Gradients (IG) and PatternAttribution (PA) are two established\nexplainability methods for neural networks. Both methods are theoretically\nwell-founded. However, they were designed to overcome different challenges. In\nthis work, we combine the two methods into a new method, Pattern-Guided\nIntegrated Gradients (PGIG). PGIG inherits important properties from both\nparent methods and passes stress tests that the originals fail. In addition, we\nbenchmark PGIG against nine alternative explainability approaches (including\nits parent methods) in a large-scale image degradation experiment and find that\nit outperforms all of them.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 09:51:33 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 11:33:25 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Schwarzenberg", "Robert", ""], ["Castle", "Steffen", ""]]}, {"id": "2007.10695", "submitter": "Yudhik Agrawal", "authors": "Yudhik Agrawal, Samyak Jain, Emily Carlson, Petri Toiviainen, Vinoo\n  Alluri", "title": "Towards Multimodal MIR: Predicting individual differences from\n  music-induced movement", "comments": "Appearing in the proceedings of the 21st International Society for\n  Music Information Retrieval Conference (ISMIR 2020) (camera-ready version)", "journal-ref": "International Society for Music Information Retrieval Conference,\n  (2020) 54-61", "doi": "10.5281/zenodo.4245368", "report-no": null, "categories": "cs.LG cs.MM stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the field of Music Information Retrieval grows, it is important to take\ninto consideration the multi-modality of music and how aspects of musical\nengagement such as movement and gesture might be taken into account. Bodily\nmovement is universally associated with music and reflective of important\nindividual features related to music preference such as personality, mood, and\nempathy. Future multimodal MIR systems may benefit from taking these aspects\ninto account. The current study addresses this by identifying individual\ndifferences, specifically Big Five personality traits, and scores on the\nEmpathy and Systemizing Quotients (EQ/SQ) from participants' free dance\nmovements. Our model successfully explored the unseen space for personality as\nwell as EQ, SQ, which has not previously been accomplished for the latter. R2\nscores for personality, EQ, and SQ were 76.3%, 77.1%, and 86.7% respectively.\nAs a follow-up, we investigated which bodily joints were most important in\ndefining these traits. We discuss how further research may explore how the\nmapping of these traits to movement patterns can be used to build a more\npersonalized, multi-modal recommendation system, as well as potential\ntherapeutic applications.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 10:10:58 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Agrawal", "Yudhik", ""], ["Jain", "Samyak", ""], ["Carlson", "Emily", ""], ["Toiviainen", "Petri", ""], ["Alluri", "Vinoo", ""]]}, {"id": "2007.10717", "submitter": "Irem Cetin", "authors": "Irem Cetin, Steffen E. Petersen, Sandy Napel, Oscar Camara, Miguel\n  Angel Gonzalez Ballester, Karim Lekadir", "title": "A radiomics approach to analyze cardiac alterations in hypertension", "comments": null, "journal-ref": null, "doi": "10.1109/ISBI.2019.8759440", "report-no": null, "categories": "physics.med-ph cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hypertension is a medical condition that is well-established as a risk factor\nfor many major diseases. For example, it can cause alterations in the cardiac\nstructure and function over time that can lead to heart related morbidity and\nmortality. However, at the subclinical stage, these changes are subtle and\ncannot be easily captured using conventional cardiovascular indices calculated\nfrom clinical cardiac imaging. In this paper, we describe a radiomics approach\nfor identifying intermediate imaging phenotypes associated with hypertension.\nThe method combines feature selection and machine learning techniques to\nidentify the most subtle as well as complex structural and tissue changes in\nhypertensive subgroups as compared to healthy individuals. Validation based on\na sample of asymptomatic hearts that include both hypertensive and\nnon-hypertensive cases demonstrate that the proposed radiomics model is capable\nof detecting intensity and textural changes well beyond the capabilities of\nconventional imaging phenotypes, indicating its potential for improved\nunderstanding of the longitudinal effects of hypertension on cardiovascular\nhealth and disease.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 11:21:14 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Cetin", "Irem", ""], ["Petersen", "Steffen E.", ""], ["Napel", "Sandy", ""], ["Camara", "Oscar", ""], ["Ballester", "Miguel Angel Gonzalez", ""], ["Lekadir", "Karim", ""]]}, {"id": "2007.10720", "submitter": "Longbing Cao", "authors": "Chengzhang Zhu, Longbing Cao, and Jianping Yin", "title": "Unsupervised Heterogeneous Coupling Learning for Categorical\n  Representation", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2020", "doi": "10.1109/TPAMI.2020.3010953", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex categorical data is often hierarchically coupled with heterogeneous\nrelationships between attributes and attribute values and the couplings between\nobjects. Such value-to-object couplings are heterogeneous with complementary\nand inconsistent interactions and distributions. Limited research exists on\nunlabeled categorical data representations, ignores the heterogeneous and\nhierarchical couplings, underestimates data characteristics and complexities,\nand overuses redundant information, etc. The deep representation learning of\nunlabeled categorical data is challenging, overseeing such value-to-object\ncouplings, complementarity and inconsistency, and requiring large data,\ndisentanglement, and high computational power. This work introduces a shallow\nbut powerful UNsupervised heTerogeneous couplIng lEarning (UNTIE) approach for\nrepresenting coupled categorical data by untying the interactions between\ncouplings and revealing heterogeneous distributions embedded in each type of\ncouplings. UNTIE is efficiently optimized w.r.t. a kernel k-means objective\nfunction for unsupervised representation learning of heterogeneous and\nhierarchical value-to-object couplings. Theoretical analysis shows that UNTIE\ncan represent categorical data with maximal separability while effectively\nrepresent heterogeneous couplings and disclose their roles in categorical data.\nThe UNTIE-learned representations make significant performance improvement\nagainst the state-of-the-art categorical representations and deep\nrepresentation models on 25 categorical data sets with diversified\ncharacteristics.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 11:23:27 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Zhu", "Chengzhang", ""], ["Cao", "Longbing", ""], ["Yin", "Jianping", ""]]}, {"id": "2007.10731", "submitter": "Benjamin Guedj", "authors": "Arthur Leroy and Pierre Latouche and Benjamin Guedj and Servane Gey", "title": "MAGMA: Inference and Prediction with Multi-Task Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We investigate the problem of multiple time series forecasting, with the\nobjective to improve multiple-step-ahead predictions. We propose a multi-task\nGaussian process framework to simultaneously model batches of individuals with\na common mean function and a specific covariance structure. This common mean is\ndefined as a Gaussian process for which the hyper-posterior distribution is\ntractable. Therefore an EM algorithm can be derived for simultaneous\nhyper-parameters optimisation and hyper-posterior computation. Unlike previous\napproaches in the literature, we account for uncertainty and handle uncommon\ngrids of observations while maintaining explicit formulations, by modelling the\nmean process in a non-parametric probabilistic framework. We also provide\npredictive formulas integrating this common mean process. This approach greatly\nimproves the predictive performance far from observations, where information\nshared across individuals provides a relevant prior mean. Our overall algorithm\nis called \\textsc{Magma} (standing for Multi tAsk Gaussian processes with\ncommon MeAn), and publicly available as a R package. The quality of the mean\nprocess estimation, predictive performances, and comparisons to alternatives\nare assessed in various simulated scenarios and on real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 11:43:54 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Leroy", "Arthur", ""], ["Latouche", "Pierre", ""], ["Guedj", "Benjamin", ""], ["Gey", "Servane", ""]]}, {"id": "2007.10736", "submitter": "Florian Henkel", "authors": "Florian Henkel, Rainer Kelz, Gerhard Widmer", "title": "Learning to Read and Follow Music in Complete Score Sheet Images", "comments": "Published in the Proceedings of the 21th International Society for\n  Music Information Retrieval Conference, Montr\\'eal, Canada 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the task of score following in sheet music given as\nunprocessed images. While existing work either relies on OMR software to obtain\na computer-readable score representation, or crucially relies on prepared sheet\nimage excerpts, we propose the first system that directly performs score\nfollowing in full-page, completely unprocessed sheet images. Based on incoming\naudio and a given image of the score, our system directly predicts the most\nlikely position within the page that matches the audio, outperforming current\nstate-of-the-art image-based score followers in terms of alignment precision.\nWe also compare our method to an OMR-based approach and empirically show that\nit can be a viable alternative to such a system.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 11:53:22 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Henkel", "Florian", ""], ["Kelz", "Rainer", ""], ["Widmer", "Gerhard", ""]]}, {"id": "2007.10740", "submitter": "Jiawei Ren", "authors": "Jiawei Ren, Cunjun Yu, Shunan Sheng, Xiao Ma, Haiyu Zhao, Shuai Yi,\n  Hongsheng Li", "title": "Balanced Meta-Softmax for Long-Tailed Visual Recognition", "comments": "NeurIPS 2020 camera-ready; Code available at\n  https://github.com/jiawei-ren/BalancedMetaSoftmax", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep classifiers have achieved great success in visual recognition. However,\nreal-world data is long-tailed by nature, leading to the mismatch between\ntraining and testing distributions. In this paper, we show that the Softmax\nfunction, though used in most classification tasks, gives a biased gradient\nestimation under the long-tailed setup. This paper presents Balanced Softmax,\nan elegant unbiased extension of Softmax, to accommodate the label distribution\nshift between training and testing. Theoretically, we derive the generalization\nbound for multiclass Softmax regression and show our loss minimizes the bound.\nIn addition, we introduce Balanced Meta-Softmax, applying a complementary Meta\nSampler to estimate the optimal class sample rate and further improve\nlong-tailed learning. In our experiments, we demonstrate that Balanced\nMeta-Softmax outperforms state-of-the-art long-tailed classification solutions\non both visual recognition and instance segmentation tasks.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 12:05:00 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 04:00:11 GMT"}, {"version": "v3", "created": "Sun, 22 Nov 2020 05:27:41 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Ren", "Jiawei", ""], ["Yu", "Cunjun", ""], ["Sheng", "Shunan", ""], ["Ma", "Xiao", ""], ["Zhao", "Haiyu", ""], ["Yi", "Shuai", ""], ["Li", "Hongsheng", ""]]}, {"id": "2007.10756", "submitter": "Pankaj Pandey", "authors": "Pankaj Pandey, Raunak Swarnkar, Shobhit Kakaria and Krishna Prasad\n  Miyapuram", "title": "Understanding Consumer Preferences for Movie Trailers from EEG using\n  Machine Learning", "comments": "This paper presented at \"The 6th Annual Conference of Cognitive\n  Science\", 10-12 December, 2019 Link:\n  https://drive.google.com/file/d/13gcTFUNg_2jA3sokOXv2f0U1D8xd-hkS/view", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neuromarketing aims to understand consumer behavior using neuroscience. Brain\nimaging tools such as EEG have been used to better understand consumer behavior\nthat goes beyond self-report measures which can be a more accurate measure to\nunderstand how and why consumers prefer choosing one product over another.\nPrevious studies have shown that consumer preferences can be effectively\npredicted by understanding changes in evoked responses as captured by EEG.\nHowever, understanding ordered preference of choices was not studied earlier.\nIn this study, we try to decipher the evoked responses using EEG while\nparticipants were presented with naturalistic stimuli i.e. movie trailers.\nUsing Machine Learning tech niques to mine the patterns in EEG signals, we\npredicted the movie rating with more than above-chance, 72% accuracy. Our\nresearch shows that neural correlates can be an effective predictor of consumer\nchoices and can significantly enhance our understanding of consumer behavior.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 12:35:18 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Pandey", "Pankaj", ""], ["Swarnkar", "Raunak", ""], ["Kakaria", "Shobhit", ""], ["Miyapuram", "Krishna Prasad", ""]]}, {"id": "2007.10757", "submitter": "Christian Reinbold", "authors": "Christian Reinbold (1), R\\\"udiger Westermann (1) ((1) Chair of\n  Computer Graphics and Visualization, Technical University of Munich, Bavaria,\n  Germany)", "title": "Inverting the Feature Visualization Process for Feedforward Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work sheds light on the invertibility of feature visualization in neural\nnetworks. Since the input that is generated by feature visualization using\nactivation maximization does, in general, not yield the feature objective it\nwas optimized for, we investigate optimizing for the feature objective that\nyields this input. Given the objective function used in activation maximization\nthat measures how closely a given input resembles the feature objective, we\nexploit that the gradient of this function w.r.t. inputs is---up to a scaling\nfactor---linear in the objective. This observation is used to find the optimal\nfeature objective via computing a closed form solution that minimizes the\ngradient. By means of Inverse Feature Visualization, we intend to provide an\nalternative view on a networks sensitivity to certain inputs that considers\nfeature objectives rather than activations.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 12:44:46 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Reinbold", "Christian", ""], ["Westermann", "R\u00fcdiger", ""]]}, {"id": "2007.10781", "submitter": "Alaettin Zubaro\\u{g}lu", "authors": "Alaettin Zubaro\\u{g}lu and Volkan Atalay", "title": "Data Stream Clustering: A Review", "comments": "Has been accepted for publication in Artificial Intelligence Review", "journal-ref": null, "doi": "10.1007/s10462-020-09874-x", "report-no": null, "categories": "cs.LG cs.AI cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Number of connected devices is steadily increasing and these devices\ncontinuously generate data streams. Real-time processing of data streams is\narousing interest despite many challenges. Clustering is one of the most\nsuitable methods for real-time data stream processing, because it can be\napplied with less prior information about the data and it does not need labeled\ninstances. However, data stream clustering differs from traditional clustering\nin many aspects and it has several challenging issues. Here, we provide\ninformation regarding the concepts and common characteristics of data streams,\nsuch as concept drift, data structures for data streams, time window models and\noutlier detection. We comprehensively review recent data stream clustering\nalgorithms and analyze them in terms of the base clustering technique,\ncomputational complexity and clustering accuracy. A comparison of these\nalgorithms is given along with still open problems. We indicate popular data\nstream repositories and datasets, stream processing tools and platforms. Open\nproblems about data stream clustering are also discussed.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 20:35:09 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Zubaro\u011flu", "Alaettin", ""], ["Atalay", "Volkan", ""]]}, {"id": "2007.10784", "submitter": "Rumen Dangovski", "authors": "Allan Costa and Rumen Dangovski and Owen Dugan and Samuel Kim and\n  Pawan Goyal and Marin Solja\\v{c}i\\'c and Joseph Jacobson", "title": "Fast Neural Models for Symbolic Regression at Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning owes much of its success to the astonishing expressiveness of\nneural networks. However, this comes at the cost of complex, black-boxed models\nthat extrapolate poorly beyond the domain of the training dataset, conflicting\nwith goals of finding analytic expressions to describe science, engineering and\nreal world data. Under the hypothesis that the hierarchical modularity of such\nlaws can be captured by training a neural network, we introduce OccamNet, a\nneural network model that finds interpretable, compact, and sparse solutions\nfor fitting data, \\`{a} la Occam's razor. Our model defines a probability\ndistribution over a non-differentiable function space. We introduce a two-step\noptimization method that samples functions and updates the weights with\nbackpropagation based on cross-entropy matching in an evolutionary strategy: we\ntrain by biasing the probability mass toward better fitting solutions. OccamNet\nis able to fit a variety of symbolic laws including simple analytic functions,\nrecursive programs, implicit functions, simple image classification, and can\noutperform noticeably state-of-the-art symbolic regression methods on real\nworld regression datasets. Our method requires minimal memory footprint, does\nnot require AI accelerators for efficient training, fits complicated functions\nin minutes of training on a single CPU, and demonstrates significant\nperformance gains when scaled on a GPU. Our implementation, demonstrations and\ninstructions for reproducing the experiments are available at\nhttps://github.com/druidowm/OccamNet_Public.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 21:14:45 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 17:50:27 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Costa", "Allan", ""], ["Dangovski", "Rumen", ""], ["Dugan", "Owen", ""], ["Kim", "Samuel", ""], ["Goyal", "Pawan", ""], ["Solja\u010di\u0107", "Marin", ""], ["Jacobson", "Joseph", ""]]}, {"id": "2007.10791", "submitter": "Jindong Wang", "authors": "Chaohui Yu, Jindong Wang, Chang Liu, Tao Qin, Renjun Xu, Wenjie Feng,\n  Yiqiang Chen, Tie-Yan Liu", "title": "Learning to Match Distributions for Domain Adaptation", "comments": "Preprint. 20 Pages. Code available at\n  https://github.com/jindongwang/transferlearning/tree/master/code/deep/Learning-to-Match", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the training and test data are from different distributions, domain\nadaptation is needed to reduce dataset bias to improve the model's\ngeneralization ability. Since it is difficult to directly match the\ncross-domain joint distributions, existing methods tend to reduce the marginal\nor conditional distribution divergence using predefined distances such as MMD\nand adversarial-based discrepancies. However, it remains challenging to\ndetermine which method is suitable for a given application since they are built\nwith certain priors or bias. Thus they may fail to uncover the underlying\nrelationship between transferable features and joint distributions. This paper\nproposes Learning to Match (L2M) to automatically learn the cross-domain\ndistribution matching without relying on hand-crafted priors on the matching\nloss. Instead, L2M reduces the inductive bias by using a meta-network to learn\nthe distribution matching loss in a data-driven way. L2M is a general framework\nthat unifies task-independent and human-designed matching features. We design a\nnovel optimization algorithm for this challenging objective with\nself-supervised label propagation. Experiments on public datasets substantiate\nthe superiority of L2M over SOTA methods. Moreover, we apply L2M to transfer\nfrom pneumonia to COVID-19 chest X-ray images with remarkable performance. L2M\ncan also be extended in other distribution matching applications where we show\nin a trial experiment that L2M generates more realistic and sharper MNIST\nsamples.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 03:26:13 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 11:05:56 GMT"}, {"version": "v3", "created": "Mon, 27 Jul 2020 01:44:38 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Yu", "Chaohui", ""], ["Wang", "Jindong", ""], ["Liu", "Chang", ""], ["Qin", "Tao", ""], ["Xu", "Renjun", ""], ["Feng", "Wenjie", ""], ["Chen", "Yiqiang", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2007.10798", "submitter": "Hu Wang", "authors": "Congbo Ma, Xiaowei Yang, Hu Wang", "title": "Randomized Online CP Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CANDECOMP/PARAFAC (CP) decomposition has been widely used to deal with\nmulti-way data. For real-time or large-scale tensors, based on the ideas of\nrandomized-sampling CP decomposition algorithm and online CP decomposition\nalgorithm, a novel CP decomposition algorithm called randomized online CP\ndecomposition (ROCP) is proposed in this paper. The proposed algorithm can\navoid forming full Khatri-Rao product, which leads to boost the speed largely\nand reduce memory usage. The experimental results on synthetic data and\nreal-world data show the ROCP algorithm is able to cope with CP decomposition\nfor large-scale tensors with arbitrary number of dimensions. In addition, ROCP\ncan reduce the computing time and memory usage dramatically, especially for\nlarge-scale tensors.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 13:41:27 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Ma", "Congbo", ""], ["Yang", "Xiaowei", ""], ["Wang", "Hu", ""]]}, {"id": "2007.10800", "submitter": "Ankur Mallick", "authors": "Ankur Mallick, Chaitanya Dwivedi, Bhavya Kailkhura, Gauri Joshi, T.\n  Yong-Jin Han", "title": "Probabilistic Neighbourhood Component Analysis: Sample Efficient\n  Uncertainty Estimation in Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Deep Neural Networks (DNNs) achieve state-of-the-art accuracy in\nvarious applications, they often fall short in accurately estimating their\npredictive uncertainty and, in turn, fail to recognize when these predictions\nmay be wrong. Several uncertainty-aware models, such as Bayesian Neural Network\n(BNNs) and Deep Ensembles have been proposed in the literature for quantifying\npredictive uncertainty. However, research in this area has been largely\nconfined to the big data regime. In this work, we show that the uncertainty\nestimation capability of state-of-the-art BNNs and Deep Ensemble models\ndegrades significantly when the amount of training data is small. To address\nthe issue of accurate uncertainty estimation in the small-data regime, we\npropose a probabilistic generalization of the popular sample-efficient\nnon-parametric kNN approach. Our approach enables deep kNN classifier to\naccurately quantify underlying uncertainties in its prediction. We demonstrate\nthe usefulness of the proposed approach by achieving superior uncertainty\nquantification as compared to state-of-the-art on a real-world application of\nCOVID-19 diagnosis from chest X-Rays. Our code is available at\nhttps://github.com/ankurmallick/sample-efficient-uq\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 21:36:31 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Mallick", "Ankur", ""], ["Dwivedi", "Chaitanya", ""], ["Kailkhura", "Bhavya", ""], ["Joshi", "Gauri", ""], ["Han", "T. Yong-Jin", ""]]}, {"id": "2007.10868", "submitter": "Christoph M\\\"uller", "authors": "Christoph M\\\"uller, Fran\\c{c}ois Serre, Gagandeep Singh, Markus\n  P\\\"uschel, Martin Vechev", "title": "Scaling Polyhedral Neural Network Verification on GPUs", "comments": "M\\\"uller and Serre contributed equally to this work", "journal-ref": "Proc. MLSys, 2021", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Certifying the robustness of neural networks against adversarial attacks is\nessential to their reliable adoption in safety-critical systems such as\nautonomous driving and medical diagnosis. Unfortunately, state-of-the-art\nverifiers either do not scale to bigger networks or are too imprecise to prove\nrobustness, limiting their practical adoption. In this work, we introduce\nGPUPoly, a scalable verifier that can prove the robustness of significantly\nlarger deep neural networks than previously possible. The key technical insight\nbehind GPUPoly is the design of custom, sound polyhedra algorithms for neural\nnetwork verification on a GPU. Our algorithms leverage the available GPU\nparallelism and inherent sparsity of the underlying verification task. GPUPoly\nscales to large networks: for example, it can prove the robustness of a 1M\nneuron, 34-layer deep residual network in approximately 34.5 ms. We believe\nGPUPoly is a promising step towards practical verification of real-world neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 16:09:07 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 10:14:05 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["M\u00fcller", "Christoph", ""], ["Serre", "Fran\u00e7ois", ""], ["Singh", "Gagandeep", ""], ["P\u00fcschel", "Markus", ""], ["Vechev", "Martin", ""]]}, {"id": "2007.10909", "submitter": "Pascal Notin", "authors": "Pascal Notin, Aidan N. Gomez, Joanna Yoo, Yarin Gal", "title": "Improving compute efficacy frontiers with SliceOut", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pushing forward the compute efficacy frontier in deep learning is critical\nfor tasks that require frequent model re-training or workloads that entail\ntraining a large number of models. We introduce SliceOut -- a dropout-inspired\nscheme designed to take advantage of GPU memory layout to train deep learning\nmodels faster without impacting final test accuracy. By dropping contiguous\nsets of units at random, our method realises training speedups through (1) fast\nmemory access and matrix multiplication of smaller tensors, and (2) memory\nsavings by avoiding allocating memory to zero units in weight gradients and\nactivations. At test time, turning off SliceOut performs an implicit ensembling\nacross a linear number of architectures that preserves test accuracy. We\ndemonstrate 10-40% speedups and memory reduction with Wide ResNets,\nEfficientNets, and Transformer models, with minimal to no loss in accuracy.\nThis leads to faster processing of large computational workloads overall, and\nsignificantly reduce the resulting energy consumption and CO2emissions.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 15:59:09 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 23:06:40 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Notin", "Pascal", ""], ["Gomez", "Aidan N.", ""], ["Yoo", "Joanna", ""], ["Gal", "Yarin", ""]]}, {"id": "2007.10928", "submitter": "David Wolpert", "authors": "David H. Wolpert", "title": "What is important about the No Free Lunch theorems?", "comments": "15 pages, 11 of main text, to be published in \"Black Box\n  Optimization, Machine Learning and No-Free Lunch Theorems\", P. Pardalos, V.\n  Rasskazova, M.N. Vrahatis, Ed., Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The No Free Lunch theorems prove that under a uniform distribution over\ninduction problems (search problems or learning problems), all induction\nalgorithms perform equally. As I discuss in this chapter, the importance of the\ntheorems arises by using them to analyze scenarios involving {non-uniform}\ndistributions, and to compare different algorithms, without any assumption\nabout the distribution over problems at all. In particular, the theorems prove\nthat {anti}-cross-validation (choosing among a set of candidate algorithms\nbased on which has {worst} out-of-sample behavior) performs as well as\ncross-validation, unless one makes an assumption -- which has never been\nformalized -- about how the distribution over induction problems, on the one\nhand, is related to the set of algorithms one is choosing among using\n(anti-)cross validation, on the other. In addition, they establish strong\ncaveats concerning the significance of the many results in the literature which\nestablish the strength of a particular algorithm without assuming a particular\ndistribution. They also motivate a ``dictionary'' between supervised learning\nand improve blackbox optimization, which allows one to ``translate'' techniques\nfrom supervised learning into the domain of blackbox optimization, thereby\nstrengthening blackbox optimization algorithms. In addition to these topics, I\nalso briefly discuss their implications for philosophy of science.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 16:42:36 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Wolpert", "David H.", ""]]}, {"id": "2007.10929", "submitter": "Zhijian Li", "authors": "Zhijian Li, Yunling Zheng, Jack Xin, and Guofa Zhou", "title": "A Recurrent Neural Network and Differential Equation Based\n  Spatiotemporal Infectious Disease Model with Application to COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outbreaks of Coronavirus Disease 2019 (COVID-19) have impacted the world\nsignificantly. Modeling the trend of infection and real-time forecasting of\ncases can help decision making and control of the disease spread. However,\ndata-driven methods such as recurrent neural networks (RNN) can perform poorly\ndue to limited daily samples in time. In this work, we develop an integrated\nspatiotemporal model based on the epidemic differential equations (SIR) and\nRNN. The former after simplification and discretization is a compact model of\ntemporal infection trend of a region while the latter models the effect of\nnearest neighboring regions. The latter captures latent spatial information.\n%that is not publicly reported. We trained and tested our model on COVID-19\ndata in Italy, and show that it out-performs existing temporal models (fully\nconnected NN, SIR, ARIMA) in 1-day, 3-day, and 1-week ahead forecasting\nespecially in the regime of limited training data.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 07:04:57 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 08:26:13 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Li", "Zhijian", ""], ["Zheng", "Yunling", ""], ["Xin", "Jack", ""], ["Zhou", "Guofa", ""]]}, {"id": "2007.10930", "submitter": "Yash Sharma", "authors": "David Klindt, Lukas Schott, Yash Sharma, Ivan Ustyuzhaninov, Wieland\n  Brendel, Matthias Bethge, Dylan Paiton", "title": "Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse\n  Coding", "comments": "ICLR 2021. Code is available at\n  https://github.com/bethgelab/slow_disentanglement. The first three authors,\n  as well as the last two authors, contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct an unsupervised learning model that achieves nonlinear\ndisentanglement of underlying factors of variation in naturalistic videos.\nPrevious work suggests that representations can be disentangled if all but a\nfew factors in the environment stay constant at any point in time. As a result,\nalgorithms proposed for this problem have only been tested on carefully\nconstructed datasets with this exact property, leaving it unclear whether they\nwill transfer to natural scenes. Here we provide evidence that objects in\nsegmented natural movies undergo transitions that are typically small in\nmagnitude with occasional large jumps, which is characteristic of a temporally\nsparse distribution. We leverage this finding and present SlowVAE, a model for\nunsupervised representation learning that uses a sparse prior on temporally\nadjacent observations to disentangle generative factors without any assumptions\non the number of changing factors. We provide a proof of identifiability and\nshow that the model reliably learns disentangled representations on several\nestablished benchmark datasets, often surpassing the current state-of-the-art.\nWe additionally demonstrate transferability towards video datasets with natural\ndynamics, Natural Sprites and KITTI Masks, which we contribute as benchmarks\nfor guiding disentanglement research towards more natural data domains.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 16:46:05 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 14:20:05 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Klindt", "David", ""], ["Schott", "Lukas", ""], ["Sharma", "Yash", ""], ["Ustyuzhaninov", "Ivan", ""], ["Brendel", "Wieland", ""], ["Bethge", "Matthias", ""], ["Paiton", "Dylan", ""]]}, {"id": "2007.10960", "submitter": "Siavash Alemzadeh", "authors": "Siavash Alemzadeh, Ramin Moslemi, Ratnesh Sharma, and Mehran Mesbahi", "title": "Adaptive Traffic Control with Deep Reinforcement Learning: Towards\n  State-of-the-art and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study adaptive data-guided traffic planning and control\nusing Reinforcement Learning (RL). We shift from the plain use of classic\nmethods towards state-of-the-art in deep RL community. We embed several recent\ntechniques in our algorithm that improve the original Deep Q-Networks (DQN) for\ndiscrete control and discuss the traffic-related interpretations that follow.\nWe propose a novel DQN-based algorithm for Traffic Control (called TC-DQN+) as\na tool for fast and more reliable traffic decision-making. We introduce a new\nform of reward function which is further discussed using illustrative examples\nwith comparisons to traditional traffic control methods.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 17:26:20 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Alemzadeh", "Siavash", ""], ["Moslemi", "Ramin", ""], ["Sharma", "Ratnesh", ""], ["Mesbahi", "Mehran", ""]]}, {"id": "2007.11022", "submitter": "Ankur Sinha PhD", "authors": "Ankur Sinha, Tanmay Khandait, Raja Mohanty", "title": "A Gradient-based Bilevel Optimization Approach for Tuning\n  Hyperparameters in Machine Learning", "comments": "10 pages, 6 figures.\\", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperparameter tuning is an active area of research in machine learning,\nwhere the aim is to identify the optimal hyperparameters that provide the best\nperformance on the validation set. Hyperparameter tuning is often achieved\nusing naive techniques, such as random search and grid search. However, most of\nthese methods seldom lead to an optimal set of hyperparameters and often get\nvery expensive. In this paper, we propose a bilevel solution method for solving\nthe hyperparameter optimization problem that does not suffer from the drawbacks\nof the earlier studies. The proposed method is general and can be easily\napplied to any class of machine learning algorithms. The idea is based on the\napproximation of the lower level optimal value function mapping, which is an\nimportant mapping in bilevel optimization and helps in reducing the bilevel\nproblem to a single level constrained optimization task. The single-level\nconstrained optimization problem is solved using the augmented Lagrangian\nmethod. We discuss the theory behind the proposed algorithm and perform\nextensive computational study on two datasets that confirm the efficiency of\nthe proposed method. We perform a comparative study against grid search, random\nsearch and Bayesian optimization techniques that shows that the proposed\nalgorithm is multiple times faster on problems with one or two hyperparameters.\nThe computational gain is expected to be significantly higher as the number of\nhyperparameters increase. Corresponding to a given hyperparameter most of the\ntechniques in the literature often assume a unique optimal parameter set that\nminimizes loss on the training set. Such an assumption is often violated by\ndeep learning architectures and the proposed method does not require any such\nassumption.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 18:15:08 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Sinha", "Ankur", ""], ["Khandait", "Tanmay", ""], ["Mohanty", "Raja", ""]]}, {"id": "2007.11026", "submitter": "Stephen Becker", "authors": "Zhishen Huang and Stephen Becker", "title": "Spectral estimation from simulations via sketching", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketching is a stochastic dimension reduction method that preserves geometric\nstructures of data and has applications in high-dimensional regression, low\nrank approximation and graph sparsification. In this work, we show that\nsketching can be used to compress simulation data and still accurately estimate\ntime autocorrelation and power spectral density. For a given compression ratio,\nthe accuracy is much higher than using previously known methods. In addition to\nproviding theoretical guarantees, we apply sketching to a molecular dynamics\nsimulation of methanol and find that the estimate of spectral density is 90%\naccurate using only 10% of the data.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 18:17:50 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Huang", "Zhishen", ""], ["Becker", "Stephen", ""]]}, {"id": "2007.11045", "submitter": "Pranjal Awasthi", "authors": "Pranjal Awasthi, Natalie Frank, Mehryar Mohri", "title": "On the Rademacher Complexity of Linear Hypothesis Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear predictors form a rich class of hypotheses used in a variety of\nlearning algorithms. We present a tight analysis of the empirical Rademacher\ncomplexity of the family of linear hypothesis classes with weight vectors\nbounded in $\\ell_p$-norm for any $p \\geq 1$. This provides a tight analysis of\ngeneralization using these hypothesis sets and helps derive sharp\ndata-dependent learning guarantees. We give both upper and lower bounds on the\nRademacher complexity of these families and show that our bounds improve upon\nor match existing bounds, which are known only for $1 \\leq p \\leq 2$.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 19:08:21 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Awasthi", "Pranjal", ""], ["Frank", "Natalie", ""], ["Mohri", "Mehryar", ""]]}, {"id": "2007.11091", "submitter": "Seyed Kamyar Seyed Ghasemipour", "authors": "Seyed Kamyar Seyed Ghasemipour, Dale Schuurmans, Shixiang Shane Gu", "title": "EMaQ: Expected-Max Q-Learning Operator for Simple Yet Effective Offline\n  and Online RL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Off-policy reinforcement learning holds the promise of sample-efficient\nlearning of decision-making policies by leveraging past experience. However, in\nthe offline RL setting -- where a fixed collection of interactions are provided\nand no further interactions are allowed -- it has been shown that standard\noff-policy RL methods can significantly underperform. Recently proposed methods\noften aim to address this shortcoming by constraining learned policies to\nremain close to the given dataset of interactions. In this work, we closely\ninvestigate an important simplification of BCQ -- a prior approach for offline\nRL -- which removes a heuristic design choice and naturally restricts extracted\npolicies to remain exactly within the support of a given behavior policy.\nImportantly, in contrast to their original theoretical considerations, we\nderive this simplified algorithm through the introduction of a novel backup\noperator, Expected-Max Q-Learning (EMaQ), which is more closely related to the\nresulting practical algorithm. Specifically, in addition to the distribution\nsupport, EMaQ explicitly considers the number of samples and the proposal\ndistribution, allowing us to derive new sub-optimality bounds which can serve\nas a novel measure of complexity for offline RL problems. In the offline RL\nsetting -- the main focus of this work -- EMaQ matches and outperforms prior\nstate-of-the-art in the D4RL benchmarks. In the online RL setting, we\ndemonstrate that EMaQ is competitive with Soft Actor Critic. The key\ncontributions of our empirical findings are demonstrating the importance of\ncareful generative model design for estimating behavior policies, and an\nintuitive notion of complexity for offline RL problems. With its simple\ninterpretation and fewer moving parts, such as no explicit function\napproximator representing the policy, EMaQ serves as a strong yet easy to\nimplement baseline for future work.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 21:13:02 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 19:11:34 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Ghasemipour", "Seyed Kamyar Seyed", ""], ["Schuurmans", "Dale", ""], ["Gu", "Shixiang Shane", ""]]}, {"id": "2007.11115", "submitter": "Jinhyun So", "authors": "Jinhyun So, Basak Guler, A. Salman Avestimehr", "title": "Byzantine-Resilient Secure Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Secure federated learning is a privacy-preserving framework to improve\nmachine learning models by training over large volumes of data collected by\nmobile users. This is achieved through an iterative process where, at each\niteration, users update a global model using their local datasets. Each user\nthen masks its local model via random keys, and the masked models are\naggregated at a central server to compute the global model for the next\niteration. As the local models are protected by random masks, the server cannot\nobserve their true values. This presents a major challenge for the resilience\nof the model against adversarial (Byzantine) users, who can manipulate the\nglobal model by modifying their local models or datasets. Towards addressing\nthis challenge, this paper presents the first single-server Byzantine-resilient\nsecure aggregation framework (BREA) for secure federated learning. BREA is\nbased on an integrated stochastic quantization, verifiable outlier detection,\nand secure model aggregation approach to guarantee Byzantine-resilience,\nprivacy, and convergence simultaneously. We provide theoretical convergence and\nprivacy guarantees and characterize the fundamental trade-offs in terms of the\nnetwork size, user dropouts, and privacy protection. Our experiments\ndemonstrate convergence in the presence of Byzantine users, and comparable\naccuracy to conventional federated learning benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 22:15:11 GMT"}, {"version": "v2", "created": "Sat, 20 Feb 2021 21:57:10 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["So", "Jinhyun", ""], ["Guler", "Basak", ""], ["Avestimehr", "A. Salman", ""]]}, {"id": "2007.11117", "submitter": "Matteo Terzi", "authors": "Mattia Carletti, Matteo Terzi, Gian Antonio Susto", "title": "Interpretable Anomaly Detection with DIFFI: Depth-based Isolation Forest\n  Feature Importance", "comments": "Added pseudocode and overview diagram", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly Detection is an unsupervised learning task aimed at detecting\nanomalous behaviours with respect to historical data. In particular,\nmultivariate Anomaly Detection has an important role in many applications\nthanks to the capability of summarizing the status of a complex system or\nobserved phenomenon with a single indicator (typically called `Anomaly Score')\nand thanks to the unsupervised nature of the task that does not require human\ntagging. The Isolation Forest is one of the most commonly adopted algorithms in\nthe field of Anomaly Detection, due to its proven effectiveness and low\ncomputational complexity. A major problem affecting Isolation Forest is\nrepresented by the lack of interpretability, an effect of the inherent\nrandomness governing the splits performed by the Isolation Trees, the building\nblocks of the Isolation Forest. In this paper we propose effective, yet\ncomputationally inexpensive, methods to define feature importance scores at\nboth global and local level for the Isolation Forest. Moreover, we define a\nprocedure to perform unsupervised feature selection for Anomaly Detection\nproblems based on our interpretability method; such procedure also serves the\npurpose of tackling the challenging task of feature importance evaluation in\nunsupervised anomaly detection. We assess the performance on several synthetic\nand real-world datasets, including comparisons against state-of-the-art\ninterpretability techniques, and make the code publicly available to enhance\nreproducibility and foster research in the field.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 22:19:21 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 13:15:08 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Carletti", "Mattia", ""], ["Terzi", "Matteo", ""], ["Susto", "Gian Antonio", ""]]}, {"id": "2007.11120", "submitter": "Daniel Russo", "authors": "Jalaj Bhandari and Daniel Russo", "title": "A Note on the Linear Convergence of Policy Gradient Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the finite time analysis of policy gradient methods in the\nsimplest setting: finite state and action problems with a policy class\nconsisting of all stochastic policies and with exact gradient evaluations. Some\nrecent works have viewed these problems as instances of smooth nonlinear\noptimization problems, suggesting suggest small stepsizes and showing sublinear\nconvergence rates. This note instead takes a policy iteration perspective and\nhighlights that many versions of policy gradient succeed with extremely large\nstepsizes and attain a linear rate of convergence.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 22:35:37 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Bhandari", "Jalaj", ""], ["Russo", "Daniel", ""]]}, {"id": "2007.11121", "submitter": "Ankit Goyal", "authors": "Ankit Goyal and Jia Deng", "title": "PackIt: A Virtual Environment for Geometric Planning", "comments": "Accepted to ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to jointly understand the geometry of objects and plan actions\nfor manipulating them is crucial for intelligent agents. We refer to this\nability as geometric planning. Recently, many interactive environments have\nbeen proposed to evaluate intelligent agents on various skills, however, none\nof them cater to the needs of geometric planning. We present PackIt, a virtual\nenvironment to evaluate and potentially learn the ability to do geometric\nplanning, where an agent needs to take a sequence of actions to pack a set of\nobjects into a box with limited space. We also construct a set of challenging\npacking tasks using an evolutionary algorithm. Further, we study various\nbaselines for the task that include model-free learning-based and\nheuristic-based methods, as well as search-based optimization methods that\nassume access to the model of the environment. Code and data are available at\nhttps://github.com/princeton-vl/PackIt.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 22:51:17 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Goyal", "Ankit", ""], ["Deng", "Jia", ""]]}, {"id": "2007.11126", "submitter": "Kevin Miller", "authors": "Kevin Miller, Hao Li, and Andrea L. Bertozzi", "title": "Efficient Graph-Based Active Learning with Probit Likelihood via\n  Gaussian Approximations", "comments": "Accepted in ICML Workshop on Real World Experiment Design and Active\n  Learning 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel adaptation of active learning to graph-based\nsemi-supervised learning (SSL) under non-Gaussian Bayesian models. We present\nan approximation of non-Gaussian distributions to adapt previously\nGaussian-based acquisition functions to these more general cases. We develop an\nefficient rank-one update for applying \"look-ahead\" based methods as well as\nmodel retraining. We also introduce a novel \"model change\" acquisition function\nbased on these approximations that further expands the available collection of\nactive learning acquisition functions for such methods.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 23:03:11 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Miller", "Kevin", ""], ["Li", "Hao", ""], ["Bertozzi", "Andrea L.", ""]]}, {"id": "2007.11133", "submitter": "Dylan Randle", "authors": "Dylan Randle, Pavlos Protopapas, David Sondak", "title": "Unsupervised Learning of Solutions to Differential Equations with\n  Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solutions to differential equations are of significant scientific and\nengineering relevance. Recently, there has been a growing interest in solving\ndifferential equations with neural networks. This work develops a novel method\nfor solving differential equations with unsupervised neural networks that\napplies Generative Adversarial Networks (GANs) to \\emph{learn the loss\nfunction} for optimizing the neural network. We present empirical results\nshowing that our method, which we call Differential Equation GAN (DEQGAN), can\nobtain multiple orders of magnitude lower mean squared errors than an\nalternative unsupervised neural network method based on (squared) $L_2$, $L_1$,\nand Huber loss functions. Moreover, we show that DEQGAN achieves solution\naccuracy that is competitive with traditional numerical methods. Finally, we\nanalyze the stability of our approach and find it to be sensitive to the\nselection of hyperparameters, which we provide in the appendix.\n  Code available at https://github.com/dylanrandle/denn. Please address any\nelectronic correspondence to dylanrandle@alumni.harvard.edu.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 23:36:36 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Randle", "Dylan", ""], ["Protopapas", "Pavlos", ""], ["Sondak", "David", ""]]}, {"id": "2007.11136", "submitter": "Jacob Zavatone-Veth", "authors": "Jacob A. Zavatone-Veth and Cengiz Pehlevan", "title": "Activation function dependence of the storage capacity of treelike\n  neural networks", "comments": "5+23 pages, 2+4 figures. v3: accepted for publication as a Letter in\n  Physical Review E", "journal-ref": "Phys. Rev. E 103, 020301 (2021)", "doi": "10.1103/PhysRevE.103.L020301", "report-no": null, "categories": "cond-mat.dis-nn cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expressive power of artificial neural networks crucially depends on the\nnonlinearity of their activation functions. Though a wide variety of nonlinear\nactivation functions have been proposed for use in artificial neural networks,\na detailed understanding of their role in determining the expressive power of a\nnetwork has not emerged. Here, we study how activation functions affect the\nstorage capacity of treelike two-layer networks. We relate the boundedness or\ndivergence of the capacity in the infinite-width limit to the smoothness of the\nactivation function, elucidating the relationship between previously studied\nspecial cases. Our results show that nonlinearity can both increase capacity\nand decrease the robustness of classification, and provide simple estimates for\nthe capacity of networks with several commonly used activation functions.\nFurthermore, they generate a hypothesis for the functional benefit of dendritic\nspikes in branched neurons.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 23:51:45 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 14:04:18 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2021 19:06:18 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Zavatone-Veth", "Jacob A.", ""], ["Pehlevan", "Cengiz", ""]]}, {"id": "2007.11164", "submitter": "Yonghui Xu", "authors": "Yonghui Xu, Shengjie Sun, Yuan Miao, Dong Yang, Xiaonan Meng, Yi Hu,\n  Ke Wang, Hengjie Song, Chuanyan Miao", "title": "Time-aware Graph Embedding: A temporal smoothness and task-oriented\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph embedding, which aims to learn the low-dimensional\nrepresentations of entities and relationships, has attracted considerable\nresearch efforts recently. However, most knowledge graph embedding methods\nfocus on the structural relationships in fixed triples while ignoring the\ntemporal information. Currently, existing time-aware graph embedding methods\nonly focus on the factual plausibility, while ignoring the temporal smoothness\nwhich models the interactions between a fact and its contexts, and thus can\ncapture fine-granularity temporal relationships. This leads to the limited\nperformance of embedding related applications. To solve this problem, this\npaper presents a Robustly Time-aware Graph Embedding (RTGE) method by\nincorporating temporal smoothness. Two major innovations of our paper are\npresented here. At first, RTGE integrates a measure of temporal smoothness in\nthe learning process of the time-aware graph embedding. Via the proposed\nadditional smoothing factor, RTGE can preserve both structural information and\nevolutionary patterns of a given graph. Secondly, RTGE provides a general\ntask-oriented negative sampling strategy associated with temporally-aware\ninformation, which further improves the adaptive ability of the proposed\nalgorithm and plays an essential role in obtaining superior performance in\nvarious tasks. Extensive experiments conducted on multiple benchmark tasks show\nthat RTGE can increase performance in entity/relationship/temporal scoping\nprediction tasks.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 02:20:25 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Xu", "Yonghui", ""], ["Sun", "Shengjie", ""], ["Miao", "Yuan", ""], ["Yang", "Dong", ""], ["Meng", "Xiaonan", ""], ["Hu", "Yi", ""], ["Wang", "Ke", ""], ["Song", "Hengjie", ""], ["Miao", "Chuanyan", ""]]}, {"id": "2007.11168", "submitter": "Aramayis Dallakyan", "authors": "Aramayis Dallakyan and Mohsen Pourahmadi", "title": "Fused-Lasso Regularized Cholesky Factors of Large Nonstationary\n  Covariance Matrices of Longitudinal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smoothness of the subdiagonals of the Cholesky factor of large covariance\nmatrices is closely related to the degrees of nonstationarity of autoregressive\nmodels for time series and longitudinal data. Heuristically, one expects for a\nnearly stationary covariance matrix the entries in each subdiagonal of the\nCholesky factor of its inverse to be nearly the same in the sense that sum of\nabsolute values of successive terms is small. Statistically such smoothness is\nachieved by regularizing each subdiagonal using fused-type lasso penalties. We\nrely on the standard Cholesky factor as the new parameters within a regularized\nnormal likelihood setup which guarantees: (1) joint convexity of the likelihood\nfunction, (2) strict convexity of the likelihood function restricted to each\nsubdiagonal even when $n<p$, and (3) positive-definiteness of the estimated\ncovariance matrix. A block coordinate descent algorithm, where each block is a\nsubdiagonal, is proposed and its convergence is established under mild\nconditions. Lack of decoupling of the penalized likelihood function into a sum\nof functions involving individual subdiagonals gives rise to some computational\nchallenges and advantages relative to two recent algorithms for sparse\nestimation of the Cholesky factor which decouple row-wise. Simulation results\nand real data analysis show the scope and good performance of the proposed\nmethodology.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 02:38:16 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Dallakyan", "Aramayis", ""], ["Pourahmadi", "Mohsen", ""]]}, {"id": "2007.11192", "submitter": "Ping Wang", "authors": "Ping Wang, Khushbu Agarwal, Colby Ham, Sutanay Choudhury, Chandan K.\n  Reddy", "title": "Self-Supervised Learning of Contextual Embeddings for Link Prediction in\n  Heterogeneous Networks", "comments": null, "journal-ref": "Published in The Web Conference 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning methods for heterogeneous networks produce a\nlow-dimensional vector embedding for each node that is typically fixed for all\ntasks involving the node. Many of the existing methods focus on obtaining a\nstatic vector representation for a node in a way that is agnostic to the\ndownstream application where it is being used. In practice, however, downstream\ntasks such as link prediction require specific contextual information that can\nbe extracted from the subgraphs related to the nodes provided as input to the\ntask. To tackle this challenge, we develop SLiCE, a framework bridging static\nrepresentation learning methods using global information from the entire graph\nwith localized attention driven mechanisms to learn contextual node\nrepresentations. We first pre-train our model in a self-supervised manner by\nintroducing higher-order semantic associations and masking nodes, and then\nfine-tune our model for a specific link prediction task. Instead of training\nnode representations by aggregating information from all semantic neighbors\nconnected via metapaths, we automatically learn the composition of different\nmetapaths that characterize the context for a specific task without the need\nfor any pre-defined metapaths. SLiCE significantly outperforms both static and\ncontextual embedding learning methods on several publicly available benchmark\nnetwork datasets. We also interpret the semantic association matrix and provide\nits utility and relevance in making successful link predictions between\nheterogeneous nodes in the network.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 03:48:53 GMT"}, {"version": "v2", "created": "Sun, 23 Aug 2020 03:11:56 GMT"}, {"version": "v3", "created": "Sun, 21 Mar 2021 20:42:38 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Wang", "Ping", ""], ["Agarwal", "Khushbu", ""], ["Ham", "Colby", ""], ["Choudhury", "Sutanay", ""], ["Reddy", "Chandan K.", ""]]}, {"id": "2007.11202", "submitter": "Xuebin Zheng", "authors": "Xuebin Zheng, Bingxin Zhou, Ming Li, Yu Guang Wang, Junbin Gao", "title": "MathNet: Haar-Like Wavelet Multiresolution-Analysis for Graph\n  Representation and Learning", "comments": "32 pages, 6 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs) have recently caught great attention and\nachieved significant progress in graph-level applications. In this paper, we\npropose a framework for graph neural networks with multiresolution Haar-like\nwavelets, or MathNet, with interrelated convolution and pooling strategies. The\nunderlying method takes graphs in different structures as input and assembles\nconsistent graph representations for readout layers, which then accomplishes\nlabel prediction. To achieve this, the multiresolution graph representations\nare first constructed and fed into graph convolutional layers for processing.\nThe hierarchical graph pooling layers are then involved to downsample graph\nresolution while simultaneously remove redundancy within graph signals. The\nwhole workflow could be formed with a multi-level graph analysis, which not\nonly helps embed the intrinsic topological information of each graph into the\nGNN, but also supports fast computation of forward and adjoint graph\ntransforms. We show by extensive experiments that the proposed framework\nobtains notable accuracy gains on graph classification and regression tasks\nwith performance stability. The proposed MathNet outperforms various existing\nGNN models, especially on big data sets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 05:00:59 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 04:16:54 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Zheng", "Xuebin", ""], ["Zhou", "Bingxin", ""], ["Li", "Ming", ""], ["Wang", "Yu Guang", ""], ["Gao", "Junbin", ""]]}, {"id": "2007.11206", "submitter": "Hong Long Pham", "authors": "Long H. Pham, Jiaying Li and Jun Sun", "title": "SOCRATES: Towards a Unified Platform for Neural Network Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies show that neural networks, not unlike traditional programs, are\nsubject to bugs, e.g., adversarial samples that cause classification errors and\ndiscriminatory instances that demonstrate the lack of fairness. Given that\nneural networks are increasingly applied in critical applications (e.g.,\nself-driving cars, face recognition systems and personal credit rating\nsystems), it is desirable that systematic methods are developed to analyze\n(e.g., test or verify) neural networks against desirable properties. Recently,\na number of approaches have been developed for analyzing neural networks. These\nefforts are however scattered (i.e., each approach tackles some restricted\nclasses of neural networks against certain particular properties), incomparable\n(i.e., each approach has its own assumptions and input format) and thus hard to\napply, reuse or extend. In this project, we aim to build a unified framework\nfor developing techniques to analyze neural networks. Towards this goal, we\ndevelop a platform called SOCRATES which supports a standardized format for a\nvariety of neural network models, an assertion language for property\nspecification as well as multiple neural network analysis algorithms including\ntwo novel ones for falsifying and probabilistic verification of neural network\nmodels. SOCRATES is extensible and thus existing approaches can be easily\nintegrated. Experiment results show that our platform can handle a wide range\nof networks models and properties. More importantly, it provides a platform for\nsynergistic research on neural network analysis.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 05:18:57 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2021 03:37:56 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Pham", "Long H.", ""], ["Li", "Jiaying", ""], ["Sun", "Jun", ""]]}, {"id": "2007.11230", "submitter": "Kaushalya Madhawa Mr", "authors": "Kaushalya Madhawa and Tsuyoshi Murata", "title": "MetAL: Active Semi-Supervised Learning on Graphs via Meta Learning", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of active learning (AL) is to train classification models with\nless number of labeled instances by selecting only the most informative\ninstances for labeling. The AL algorithms designed for other data types such as\nimages and text do not perform well on graph-structured data. Although a few\nheuristics-based AL algorithms have been proposed for graphs, a principled\napproach is lacking. In this paper, we propose MetAL, an AL approach that\nselects unlabeled instances that directly improve the future performance of a\nclassification model. For a semi-supervised learning problem, we formulate the\nAL task as a bilevel optimization problem. Based on recent work in\nmeta-learning, we use the meta-gradients to approximate the impact of\nretraining the model with any unlabeled instance on the model performance.\nUsing multiple graph datasets belonging to different domains, we demonstrate\nthat MetAL efficiently outperforms existing state-of-the-art AL algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 06:59:49 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Madhawa", "Kaushalya", ""], ["Murata", "Tsuyoshi", ""]]}, {"id": "2007.11259", "submitter": "Matteo Terzi", "authors": "Matteo Terzi, Alessandro Achille, Marco Maggipinto, Gian Antonio Susto", "title": "Adversarial Training Reduces Information and Improves Transferability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent results show that features of adversarially trained networks for\nclassification, in addition to being robust, enable desirable properties such\nas invertibility. The latter property may seem counter-intuitive as it is\nwidely accepted by the community that classification models should only capture\nthe minimal information (features) required for the task. Motivated by this\ndiscrepancy, we investigate the dual relationship between Adversarial Training\nand Information Theory. We show that the Adversarial Training can improve\nlinear transferability to new tasks, from which arises a new trade-off between\ntransferability of representations and accuracy on the source task. We validate\nour results employing robust networks trained on CIFAR-10, CIFAR-100 and\nImageNet on several datasets. Moreover, we show that Adversarial Training\nreduces Fisher information of representations about the input and of the\nweights about the task, and we provide a theoretical argument which explains\nthe invertibility of deterministic networks without violating the principle of\nminimality. Finally, we leverage our theoretical insights to remarkably improve\nthe quality of reconstructed images through inversion.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 08:30:16 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 09:42:50 GMT"}, {"version": "v3", "created": "Fri, 21 Aug 2020 15:04:47 GMT"}, {"version": "v4", "created": "Tue, 15 Dec 2020 22:52:16 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Terzi", "Matteo", ""], ["Achille", "Alessandro", ""], ["Maggipinto", "Marco", ""], ["Susto", "Gian Antonio", ""]]}, {"id": "2007.11280", "submitter": "Yuhu Yan", "authors": "Bo-Jian Hou, Yu-Hu Yan, Peng Zhao and Zhi-Hua Zhou", "title": "Storage Fit Learning with Feature Evolvable Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature evolvable learning has been widely studied in recent years where old\nfeatures will vanish and new features will emerge when learning with streams.\nConventional methods usually assume that a label will be revealed after\nprediction at each time step. However, in practice, this assumption may not\nhold whereas no label will be given at most time steps. A good solution is to\nleverage the technique of manifold regularization to utilize the previous\nsimilar data to assist the refinement of the online model. Nevertheless, this\napproach needs to store all previous data which is impossible in learning with\nstreams that arrive sequentially in large volume. Thus we need a buffer to\nstore part of them. Considering that different devices may have different\nstorage budgets, the learning approaches should be flexible subject to the\nstorage budget limit. In this paper, we propose a new setting: Storage-Fit\nFeature-Evolvable streaming Learning (SF$^2$EL) which incorporates the issue of\nrarely-provided labels into feature evolution. Our framework is able to fit its\nbehavior to different storage budgets when learning with feature evolvable\nstreams with unlabeled data. Besides, both theoretical and empirical results\nvalidate that our approach can preserve the merit of the original feature\nevolvable learning i.e., can always track the best baseline and thus perform\nwell at any time step.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 09:08:42 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 11:24:10 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2021 07:27:01 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Hou", "Bo-Jian", ""], ["Yan", "Yu-Hu", ""], ["Zhao", "Peng", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "2007.11344", "submitter": "Patrick Hemmer", "authors": "Patrick Hemmer, Niklas K\\\"uhl and Jakob Sch\\\"offer", "title": "DEAL: Deep Evidential Active Learning for Image Classification", "comments": "Extended version of the paper \"DEAL: Deep Evidential Active Learning\n  for Image Classification\" accepted for publication at ICMLA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Convolutional Neural Networks (CNNs) have proven to be state-of-the-art\nmodels for supervised computer vision tasks, such as image classification.\nHowever, large labeled data sets are generally needed for the training and\nvalidation of such models. In many domains, unlabeled data is available but\nlabeling is expensive, for instance when specific expert knowledge is required.\nActive Learning (AL) is one approach to mitigate the problem of limited labeled\ndata. Through selecting the most informative and representative data instances\nfor labeling, AL can contribute to more efficient learning of the model. Recent\nAL methods for CNNs propose different solutions for the selection of instances\nto be labeled. However, they do not perform consistently well and are often\ncomputationally expensive. In this paper, we propose a novel AL algorithm that\nefficiently learns from unlabeled data by capturing high prediction\nuncertainty. By replacing the softmax standard output of a CNN with the\nparameters of a Dirichlet density, the model learns to identify data instances\nthat contribute efficiently to improving model performance during training. We\ndemonstrate in several experiments with publicly available data that our method\nconsistently outperforms other state-of-the-art AL approaches. It can be easily\nimplemented and does not require extensive computational resources for\ntraining. Additionally, we are able to show the benefits of the approach on a\nreal-world medical use case in the field of automated detection of visual\nsignals for pneumonia on chest radiographs.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 11:14:23 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 07:35:51 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Hemmer", "Patrick", ""], ["K\u00fchl", "Niklas", ""], ["Sch\u00f6ffer", "Jakob", ""]]}, {"id": "2007.11353", "submitter": "Andreas Hinterreiter", "authors": "Michael P\\\"uhringer, Andreas Hinterreiter, Marc Streit", "title": "InstanceFlow: Visualizing the Evolution of Classifier Confusion on the\n  Instance Level", "comments": "conditionally accepted for IEEE VIS 2020 short paper track; minor\n  revisions compared to original version", "journal-ref": null, "doi": "10.1109/VIS47514.2020.00065", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification is one of the most important supervised machine learning\ntasks. During the training of a classification model, the training instances\nare fed to the model multiple times (during multiple epochs) in order to\niteratively increase the classification performance. The increasing complexity\nof models has led to a growing demand for model interpretability through\nvisualizations. Existing approaches mostly focus on the visual analysis of the\nfinal model performance after training and are often limited to aggregate\nperformance measures. In this paper we introduce InstanceFlow, a novel\ndual-view visualization tool that allows users to analyze the learning behavior\nof classifiers over time on the instance-level. A Sankey diagram visualizes the\nflow of instances throughout epochs, with on-demand detailed glyphs and traces\nfor individual instances. A tabular view allows users to locate interesting\ninstances by ranking and filtering. In this way, InstanceFlow bridges the gap\nbetween class-level and instance-level performance evaluation while enabling\nusers to perform a full temporal analysis of the training process.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 11:59:28 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 22:01:56 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["P\u00fchringer", "Michael", ""], ["Hinterreiter", "Andreas", ""], ["Streit", "Marc", ""]]}, {"id": "2007.11362", "submitter": "In Huh", "authors": "In Huh, Eunho Yang, Sung Ju Hwang, Jinwoo Shin", "title": "Time-Reversal Symmetric ODE Network", "comments": "15 pages; accepted to NeurIPS 2020; Code is available at\n  https://github.com/inhuh/trs-oden; v3: references added, typo corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-reversal symmetry, which requires that the dynamics of a system should\nnot change with the reversal of time axis, is a fundamental property that\nfrequently holds in classical and quantum mechanics. In this paper, we propose\na novel loss function that measures how well our ordinary differential equation\n(ODE) networks comply with this time-reversal symmetry; it is formally defined\nby the discrepancy in the time evolutions of ODE networks between forward and\nbackward dynamics. Then, we design a new framework, which we name as\nTime-Reversal Symmetric ODE Networks (TRS-ODENs), that can learn the dynamics\nof physical systems more sample-efficiently by learning with the proposed loss\nfunction. We evaluate TRS-ODENs on several classical dynamics, and find they\ncan learn the desired time evolution from observed noisy and complex\ntrajectories. We also show that, even for systems that do not possess the full\ntime-reversal symmetry, TRS-ODENs can achieve better predictive performances\nover baselines.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 12:19:40 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 09:53:00 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2021 01:42:46 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Huh", "In", ""], ["Yang", "Eunho", ""], ["Hwang", "Sung Ju", ""], ["Shin", "Jinwoo", ""]]}, {"id": "2007.11391", "submitter": "Andreas Selmar Hauptmann", "authors": "Arttu Arjas, Lassi Roininen, Mikko J. Sillanp\\\"a\\\"a, Andreas Hauptmann", "title": "Blind hierarchical deconvolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deconvolution is a fundamental inverse problem in signal processing and the\nprototypical model for recovering a signal from its noisy measurement.\nNevertheless, the majority of model-based inversion techniques require\nknowledge on the convolution kernel to recover an accurate reconstruction and\nadditionally prior assumptions on the regularity of the signal are needed. To\novercome these limitations, we parametrise the convolution kernel and prior\nlength-scales, which are then jointly estimated in the inversion procedure. The\nproposed framework of blind hierarchical deconvolution enables accurate\nreconstructions of functions with varying regularity and unknown kernel size\nand can be solved efficiently with an empirical Bayes two-step procedure, where\nhyperparameters are first estimated by optimisation and other unknowns then by\nan analytical formula.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 12:54:19 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Arjas", "Arttu", ""], ["Roininen", "Lassi", ""], ["Sillanp\u00e4\u00e4", "Mikko J.", ""], ["Hauptmann", "Andreas", ""]]}, {"id": "2007.11412", "submitter": "Brooke Husic", "authors": "Brooke E. Husic, Nicholas E. Charron, Dominik Lemm, Jiang Wang,\n  Adri\\`a P\\'erez, Maciej Majewski, Andreas Kr\\\"amer, Yaoyi Chen, Simon Olsson,\n  Gianni de Fabritiis, Frank No\\'e, Cecilia Clementi", "title": "Coarse Graining Molecular Dynamics with Graph Neural Networks", "comments": "17 pages, 9 figures", "journal-ref": null, "doi": "10.1063/5.0026133", "report-no": null, "categories": "physics.comp-ph physics.bio-ph physics.chem-ph q-bio.BM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coarse graining enables the investigation of molecular dynamics for larger\nsystems and at longer timescales than is possible at atomic resolution.\nHowever, a coarse graining model must be formulated such that the conclusions\nwe draw from it are consistent with the conclusions we would draw from a model\nat a finer level of detail. It has been proven that a force matching scheme\ndefines a thermodynamically consistent coarse-grained model for an atomistic\nsystem in the variational limit. Wang et al. [ACS Cent. Sci. 5, 755 (2019)]\ndemonstrated that the existence of such a variational limit enables the use of\na supervised machine learning framework to generate a coarse-grained force\nfield, which can then be used for simulation in the coarse-grained space. Their\nframework, however, requires the manual input of molecular features upon which\nto machine learn the force field. In the present contribution, we build upon\nthe advance of Wang et al.and introduce a hybrid architecture for the machine\nlearning of coarse-grained force fields that learns their own features via a\nsubnetwork that leverages continuous filter convolutions on a graph neural\nnetwork architecture. We demonstrate that this framework succeeds at\nreproducing the thermodynamics for small biomolecular systems. Since the\nlearned molecular representations are inherently transferable, the architecture\npresented here sets the stage for the development of machine-learned,\ncoarse-grained force fields that are transferable across molecular systems.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 13:20:08 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 12:26:04 GMT"}, {"version": "v3", "created": "Fri, 6 Nov 2020 15:57:54 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Husic", "Brooke E.", ""], ["Charron", "Nicholas E.", ""], ["Lemm", "Dominik", ""], ["Wang", "Jiang", ""], ["P\u00e9rez", "Adri\u00e0", ""], ["Majewski", "Maciej", ""], ["Kr\u00e4mer", "Andreas", ""], ["Chen", "Yaoyi", ""], ["Olsson", "Simon", ""], ["de Fabritiis", "Gianni", ""], ["No\u00e9", "Frank", ""], ["Clementi", "Cecilia", ""]]}, {"id": "2007.11416", "submitter": "Djallel Bouneffouf", "authors": "Djallel Bouneffouf", "title": "Spectral Clustering using Eigenspectrum Shape Based Nystrom Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering has shown a superior performance in analyzing the cluster\nstructure. However, its computational complexity limits its application in\nanalyzing large-scale data. To address this problem, many low-rank matrix\napproximating algorithms are proposed, including the Nystrom method - an\napproach with proven approximate error bounds. There are several algorithms\nthat provide recipes to construct Nystrom approximations with variable\naccuracies and computing times. This paper proposes a scalable Nystrom-based\nclustering algorithm with a new sampling procedure, Centroid Minimum Sum of\nSquared Similarities (CMS3), and a heuristic on when to use it. Our heuristic\ndepends on the eigen spectrum shape of the dataset, and yields competitive\nlow-rank approximations in test datasets compared to the other state-of-the-art\nmethods\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 17:49:03 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Bouneffouf", "Djallel", ""]]}, {"id": "2007.11446", "submitter": "Nicolas Gillis", "authors": "Maryam Abdolali, Nicolas Gillis", "title": "Simplex-Structured Matrix Factorization: Sparsity-based Identifiability\n  and Provably Correct Algorithms", "comments": "38 pages", "journal-ref": "SIAM J. on Mathematics of Data Science 3 (2), 593-623, 2021", "doi": "10.1137/20M1354982", "report-no": null, "categories": "cs.LG eess.IV eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide novel algorithms with identifiability guarantees\nfor simplex-structured matrix factorization (SSMF), a generalization of\nnonnegative matrix factorization. Current state-of-the-art algorithms that\nprovide identifiability results for SSMF rely on the sufficiently scattered\ncondition (SSC) which requires the data points to be well spread within the\nconvex hull of the basis vectors. The conditions under which our proposed\nalgorithms recover the unique decomposition is in most cases much weaker than\nthe SSC. We only require to have $d$ points on each facet of the convex hull of\nthe basis vectors whose dimension is $d-1$. The key idea is based on extracting\nfacets containing the largest number of points. We illustrate the effectiveness\nof our approach on synthetic data sets and hyperspectral images, showing that\nit outperforms state-of-the-art SSMF algorithms as it is able to handle higher\nnoise levels, rank deficient matrices, outliers, and input data that highly\nviolates the SSC.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 14:01:58 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Abdolali", "Maryam", ""], ["Gillis", "Nicolas", ""]]}, {"id": "2007.11455", "submitter": "Amine Laghaout", "authors": "Amine Laghaout", "title": "Supervised learning on heterogeneous, attributed entities interacting\n  over time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most physical or social phenomena can be represented by ontologies where the\nconstituent entities are interacting in various ways with each other and with\ntheir environment. Furthermore, those entities are likely heterogeneous and\nattributed with features that evolve dynamically in time as a response to their\nsuccessive interactions. In order to apply machine learning on such entities,\ne.g., for classification purposes, one therefore needs to integrate the\ninteractions into the feature engineering in a systematic way. This proposal\nshows how, to this end, the current state of graph machine learning remains\ninadequate and needs to be be augmented with a comprehensive feature\nengineering paradigm in space and time.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 14:19:11 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Laghaout", "Amine", ""]]}, {"id": "2007.11465", "submitter": "Alexander Fuchs", "authors": "Alexander Fuchs, Franz Pernkopf", "title": "Wasserstein Routed Capsule Networks", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capsule networks offer interesting properties and provide an alternative to\ntoday's deep neural network architectures. However, recent approaches have\nfailed to consistently achieve competitive results across different image\ndatasets. We propose a new parameter efficient capsule architecture, that is\nable to tackle complex tasks by using neural networks trained with an\napproximate Wasserstein objective to dynamically select capsules throughout the\nentire architecture. This approach focuses on implementing a robust routing\nscheme, which can deliver improved results using little overhead. We perform\nseveral ablation studies verifying the proposed concepts and show that our\nnetwork is able to substantially outperform other capsule approaches by over\n1.2 % on CIFAR-10, using fewer parameters.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 14:38:05 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Fuchs", "Alexander", ""], ["Pernkopf", "Franz", ""]]}, {"id": "2007.11471", "submitter": "Leonardo Petrini", "authors": "Jonas Paccolat, Leonardo Petrini, Mario Geiger, Kevin Tyloo and\n  Matthieu Wyart", "title": "Geometric compression of invariant manifolds in neural nets", "comments": null, "journal-ref": "Journal of Statistical Mechanics: Theory and Experiment, Volume\n  2021, April 2021", "doi": "10.1088/1742-5468/abf1f3", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how neural networks compress uninformative input space in models\nwhere data lie in $d$ dimensions, but whose label only vary within a linear\nmanifold of dimension $d_\\parallel < d$. We show that for a one-hidden layer\nnetwork initialized with infinitesimal weights (i.e. in the feature learning\nregime) trained with gradient descent, the first layer of weights evolve to\nbecome nearly insensitive to the $d_\\perp=d-d_\\parallel$ uninformative\ndirections. These are effectively compressed by a factor $\\lambda\\sim\n\\sqrt{p}$, where $p$ is the size of the training set. We quantify the benefit\nof such a compression on the test error $\\epsilon$. For large initialization of\nthe weights (the lazy training regime), no compression occurs and for regular\nboundaries separating labels we find that $\\epsilon \\sim p^{-\\beta}$, with\n$\\beta_\\text{Lazy} = d / (3d-2)$. Compression improves the learning curves so\nthat $\\beta_\\text{Feature} = (2d-1)/(3d-2)$ if $d_\\parallel = 1$ and\n$\\beta_\\text{Feature} = (d + d_\\perp/2)/(3d-2)$ if $d_\\parallel > 1$. We test\nthese predictions for a stripe model where boundaries are parallel interfaces\n($d_\\parallel=1$) as well as for a cylindrical boundary ($d_\\parallel=2$). Next\nwe show that compression shapes the Neural Tangent Kernel (NTK) evolution in\ntime, so that its top eigenvectors become more informative and display a larger\nprojection on the labels. Consequently, kernel learning with the frozen NTK at\nthe end of training outperforms the initial NTK. We confirm these predictions\nboth for a one-hidden layer FC network trained on the stripe model and for a\n16-layers CNN trained on MNIST, for which we also find\n$\\beta_\\text{Feature}>\\beta_\\text{Lazy}$.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 14:43:49 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 16:12:40 GMT"}, {"version": "v3", "created": "Fri, 6 Nov 2020 13:52:29 GMT"}, {"version": "v4", "created": "Thu, 11 Mar 2021 08:58:04 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Paccolat", "Jonas", ""], ["Petrini", "Leonardo", ""], ["Geiger", "Mario", ""], ["Tyloo", "Kevin", ""], ["Wyart", "Matthieu", ""]]}, {"id": "2007.11482", "submitter": "Elad Romanov", "authors": "Elad Romanov, Tamir Bendory, Or Ordentlich", "title": "Multi-reference alignment in high dimensions: sample complexity and\n  phase transition", "comments": null, "journal-ref": null, "doi": "10.1137/20M1354994", "report-no": null, "categories": "cs.IT eess.SP math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-reference alignment entails estimating a signal in $\\mathbb{R}^L$ from\nits circularly-shifted and noisy copies. This problem has been studied\nthoroughly in recent years, focusing on the finite-dimensional setting (fixed\n$L$). Motivated by single-particle cryo-electron microscopy, we analyze the\nsample complexity of the problem in the high-dimensional regime $L\\to\\infty$.\nOur analysis uncovers a phase transition phenomenon governed by the parameter\n$\\alpha = L/(\\sigma^2\\log L)$, where $\\sigma^2$ is the variance of the noise.\nWhen $\\alpha>2$, the impact of the unknown circular shifts on the sample\ncomplexity is minor. Namely, the number of measurements required to achieve a\ndesired accuracy $\\varepsilon$ approaches $\\sigma^2/\\varepsilon$ for small\n$\\varepsilon$; this is the sample complexity of estimating a signal in additive\nwhite Gaussian noise, which does not involve shifts. In sharp contrast, when\n$\\alpha\\leq 2$, the problem is significantly harder and the sample complexity\ngrows substantially quicker with $\\sigma^2$.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 15:04:47 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Romanov", "Elad", ""], ["Bendory", "Tamir", ""], ["Ordentlich", "Or", ""]]}, {"id": "2007.11500", "submitter": "Mohammad Taha Bahadori", "authors": "Mohammad Taha Bahadori, David E. Heckerman", "title": "Debiasing Concept-based Explanations with Causal Analysis", "comments": "Accepted in ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concept-based explanation approach is a popular model interpertability tool\nbecause it expresses the reasons for a model's predictions in terms of concepts\nthat are meaningful for the domain experts. In this work, we study the problem\nof the concepts being correlated with confounding information in the features.\nWe propose a new causal prior graph for modeling the impacts of unobserved\nvariables and a method to remove the impact of confounding information and\nnoise using a two-stage regression technique borrowed from the instrumental\nvariable literature. We also model the completeness of the concepts set and\nshow that our debiasing method works when the concepts are not complete. Our\nsynthetic and real-world experiments demonstrate the success of our method in\nremoving biases and improving the ranking of the concepts in terms of their\ncontribution to the explanation of the predictions.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 15:42:46 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 02:18:46 GMT"}, {"version": "v3", "created": "Wed, 13 Jan 2021 17:13:26 GMT"}, {"version": "v4", "created": "Sat, 22 May 2021 04:57:28 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Bahadori", "Mohammad Taha", ""], ["Heckerman", "David E.", ""]]}, {"id": "2007.11524", "submitter": "Milad Nasr", "authors": "Milad Nasr, Reza Shokri and Amir houmansadr", "title": "Improving Deep Learning with Differential Privacy using Gradient\n  Encoding and Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models leak significant amounts of information about their\ntraining datasets. Previous work has investigated training models with\ndifferential privacy (DP) guarantees through adding DP noise to the gradients.\nHowever, such solutions (specifically, DPSGD), result in large degradations in\nthe accuracy of the trained models. In this paper, we aim at training deep\nlearning models with DP guarantees while preserving model accuracy much better\nthan previous works. Our key technique is to encode gradients to map them to a\nsmaller vector space, therefore enabling us to obtain DP guarantees for\ndifferent noise distributions. This allows us to investigate and choose noise\ndistributions that best preserve model accuracy for a target privacy budget. We\nalso take advantage of the post-processing property of differential privacy by\nintroducing the idea of denoising, which further improves the utility of the\ntrained models without degrading their DP guarantees. We show that our\nmechanism outperforms the state-of-the-art DPSGD; for instance, for the same\nmodel accuracy of $96.1\\%$ on MNIST, our technique results in a privacy bound\nof $\\epsilon=3.2$ compared to $\\epsilon=6$ of DPSGD, which is a significant\nimprovement.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 16:33:14 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Nasr", "Milad", ""], ["Shokri", "Reza", ""], ["houmansadr", "Amir", ""]]}, {"id": "2007.11572", "submitter": "Reg Dodds", "authors": "Kudakwashe Dandajena, Isabella M. Venter, Mehrdad Ghaziasgar and Reg\n  Dodds", "title": "Complex Sequential Data Analysis: A Systematic Literature Review of\n  Existing Algorithms", "comments": "7 pages, 4 figures, Saicsit 20, Cape Town, South Africa", "journal-ref": null, "doi": "10.1145/3410886.3410899", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a review of past approaches to the use of deep-learning\nframeworks for the analysis of discrete irregular-patterned complex sequential\ndatasets. A typical example of such a dataset is financial data where specific\nevents trigger sudden irregular changes in the sequence of the data.\nTraditional deep-learning methods perform poorly or even fail when trying to\nanalyse these datasets. The results of a systematic literature review reveal\nthe dominance of frameworks based on recurrent neural networks. The performance\nof deep-learning frameworks was found to be evaluated mainly using mean\nabsolute error and root mean square error accuracy metrics. Underlying\nchallenges that were identified are: lack of performance robustness,\nnon-transparency of the methodology, internal and external architectural design\nand configuration issues. These challenges provide an opportunity to improve\nthe framework for complex irregular-patterned sequential datasets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 17:53:00 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Dandajena", "Kudakwashe", ""], ["Venter", "Isabella M.", ""], ["Ghaziasgar", "Mehrdad", ""], ["Dodds", "Reg", ""]]}, {"id": "2007.11609", "submitter": "Shabnam Nazmi", "authors": "Shabnam Nazmi, Xuyang Yan, Abdollah Homaifar, Emily Doucette", "title": "Evolving Multi-label Classification Rules by Exploiting High-order Label\n  Correlation", "comments": "13 pages, 1 figure, 14 tables, accepted for publication in the\n  Neurocomputing journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-label classification tasks, each problem instance is associated with\nmultiple classes simultaneously. In such settings, the correlation between\nlabels contains valuable information that can be used to obtain more accurate\nclassification models. The correlation between labels can be exploited at\ndifferent levels such as capturing the pair-wise correlation or exploiting the\nhigher-order correlations. Even though the high-order approach is more capable\nof modeling the correlation, it is computationally more demanding and has\nscalability issues. This paper aims at exploiting the high-order label\ncorrelation within subsets of labels using a supervised learning classifier\nsystem (UCS). For this purpose, the label powerset (LP) strategy is employed\nand a prediction aggregation within the set of the relevant labels to an unseen\ninstance is utilized to increase the prediction capability of the LP method in\nthe presence of unseen labelsets. Exact match ratio and Hamming loss measures\nare considered to evaluate the rule performance and the expected fitness value\nof a classifier is investigated for both metrics. Also, a computational\ncomplexity analysis is provided for the proposed algorithm. The experimental\nresults of the proposed method are compared with other well-known LP-based\nmethods on multiple benchmark datasets and confirm the competitive performance\nof this method.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 18:13:12 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Nazmi", "Shabnam", ""], ["Yan", "Xuyang", ""], ["Homaifar", "Abdollah", ""], ["Doucette", "Emily", ""]]}, {"id": "2007.11612", "submitter": "Murat A. Erdogdu", "authors": "Murat A. Erdogdu, Rasa Hosseinzadeh, Matthew S. Zhang", "title": "Convergence of Langevin Monte Carlo in Chi-Squared and Renyi Divergence", "comments": "v1: There was an error in the proof of Lemma 1. Authors thank Andre\n  Wibisono for noticing this and letting us know. v2: Paper is updated with an\n  opaque condition, in order not to mislead researchers. v3: Opaque condition\n  in the previous version is proved under LSI and strong dissipativity. v4:\n  Results on Renyi divergence are added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study sampling from a target distribution $\\nu_* = e^{-f}$ using the\nunadjusted Langevin Monte Carlo (LMC) algorithm when the potential $f$\nsatisfies a strong dissipativity condition and it is first-order smooth with a\nLipschitz gradient. We prove that, initialized with a Gaussian random vector\nthat has sufficiently small variance, iterating the LMC algorithm for\n$\\widetilde{\\mathcal{O}}(\\lambda^2 d\\epsilon^{-1})$ steps is sufficient to\nreach $\\epsilon$-neighborhood of the target in both Chi-squared and Renyi\ndivergence, where $\\lambda$ is the logarithmic Sobolev constant of $\\nu_*$. Our\nresults do not require warm-start to deal with the exponential dimension\ndependency in Chi-squared divergence at initialization. In particular, for\nstrongly convex and first-order smooth potentials, we show that the LMC\nalgorithm achieves the rate estimate $\\widetilde{\\mathcal{O}}(d\\epsilon^{-1})$\nwhich improves the previously known rates in both of these metrics, under the\nsame assumptions. Translating this rate to other metrics, our results also\nrecover the state-of-the-art rate estimates in KL divergence, total variation\nand $2$-Wasserstein distance in the same setup. Finally, as we rely on the\nlogarithmic Sobolev inequality, our framework covers a range of non-convex\npotentials that are first-order smooth and exhibit strong convexity outside of\na compact region.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 18:18:28 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 16:23:22 GMT"}, {"version": "v3", "created": "Mon, 24 May 2021 17:57:14 GMT"}, {"version": "v4", "created": "Thu, 8 Jul 2021 06:45:09 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Erdogdu", "Murat A.", ""], ["Hosseinzadeh", "Rasa", ""], ["Zhang", "Matthew S.", ""]]}, {"id": "2007.11652", "submitter": "Morteza Haghir Chehreghani", "authors": "Carl Johnell, Morteza Haghir Chehreghani", "title": "Efficient Optimization of Dominant Set Clustering with Frank-Wolfe\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Frank-Wolfe algorithms -- standard, pairwise, and away-steps -- for\nefficient optimization of Dominant Set Clustering. We present a unified and\ncomputationally efficient framework to employ the different variants of\nFrank-Wolfe methods, and we investigate its effectiveness via several\nexperimental studies. In addition, we provide explicit convergence rates for\nthe algorithms in terms of the so-called Frank-Wolfe gap. The theoretical\nanalysis has been specialized to the problem of Dominant Set Clustering and is\nthus more easily accessible compared to prior work.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 20:08:41 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 10:55:21 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Johnell", "Carl", ""], ["Chehreghani", "Morteza Haghir", ""]]}, {"id": "2007.11684", "submitter": "Daniel Russo", "authors": "Daniel Russo", "title": "Approximation Benefits of Policy Gradient Methods with Aggregated States", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Folklore suggests that policy gradient can be more robust to misspecification\nthan its relative, approximate policy iteration. This paper studies the case of\nstate-aggregation, where the state space is partitioned and either the policy\nor value function approximation is held constant over partitions. This paper\nshows a policy gradient method converges to a policy whose regret per-period is\nbounded by $\\epsilon$, the largest difference between two elements of the\nstate-action value function belonging to a common partition. With the same\nrepresentation, both approximate policy iteration and approximate value\niteration can produce policies whose per-period regret scales as\n$\\epsilon/(1-\\gamma)$, where $\\gamma$ is a discount factor. Theoretical results\nsynthesize recent analysis of policy gradient methods with insights of Van Roy\n(2006) into the critical role of state-relevance weights in approximate dynamic\nprogramming.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 21:20:24 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 23:30:20 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Russo", "Daniel", ""]]}, {"id": "2007.11693", "submitter": "Ye Wang", "authors": "Ye Wang, Shuchin Aeron, Adnan Siraj Rakin, Toshiaki Koike-Akino,\n  Pierre Moulin", "title": "Robust Machine Learning via Privacy/Rate-Distortion Theory", "comments": "9 pages, 2 figures, accepted at 2021 IEEE International Symposium on\n  Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.GT cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust machine learning formulations have emerged to address the prevalent\nvulnerability of deep neural networks to adversarial examples. Our work draws\nthe connection between optimal robust learning and the privacy-utility tradeoff\nproblem, which is a generalization of the rate-distortion problem. The saddle\npoint of the game between a robust classifier and an adversarial perturbation\ncan be found via the solution of a maximum conditional entropy problem. This\ninformation-theoretic perspective sheds light on the fundamental tradeoff\nbetween robustness and clean data performance, which ultimately arises from the\ngeometric structure of the underlying data distribution and perturbation\nconstraints.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 21:34:59 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 21:13:24 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Wang", "Ye", ""], ["Aeron", "Shuchin", ""], ["Rakin", "Adnan Siraj", ""], ["Koike-Akino", "Toshiaki", ""], ["Moulin", "Pierre", ""]]}, {"id": "2007.11707", "submitter": "Wei-Ning Chen", "authors": "Wei-Ning Chen, Peter Kairouz, Ayfer \\\"Ozg\\\"ur", "title": "Breaking the Communication-Privacy-Accuracy Trilemma", "comments": "35 pages, 9 figures, submitted to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two major challenges in distributed learning and estimation are 1) preserving\nthe privacy of the local samples; and 2) communicating them efficiently to a\ncentral server, while achieving high accuracy for the end-to-end task. While\nthere has been significant interest in addressing each of these challenges\nseparately in the recent literature, treatments that simultaneously address\nboth challenges are still largely missing. In this paper, we develop novel\nencoding and decoding mechanisms that simultaneously achieve optimal privacy\nand communication efficiency in various canonical settings.\n  In particular, we consider the problems of mean estimation and frequency\nestimation under $\\varepsilon$-local differential privacy and $b$-bit\ncommunication constraints. For mean estimation, we propose a scheme based on\nKashin's representation and random sampling, with order-optimal estimation\nerror under both constraints. For frequency estimation, we present a mechanism\nthat leverages the recursive structure of Walsh-Hadamard matrices and achieves\norder-optimal estimation error for all privacy levels and communication\nbudgets. As a by-product, we also construct a distribution estimation mechanism\nthat is rate-optimal for all privacy regimes and communication constraints,\nextending recent work that is limited to $b=1$ and $\\varepsilon=O(1)$. Our\nresults demonstrate that intelligent encoding under joint privacy and\ncommunication constraints can yield a performance that matches the optimal\naccuracy achievable under either constraint alone.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 22:43:01 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 20:50:56 GMT"}, {"version": "v3", "created": "Tue, 20 Apr 2021 19:13:09 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Chen", "Wei-Ning", ""], ["Kairouz", "Peter", ""], ["\u00d6zg\u00fcr", "Ayfer", ""]]}, {"id": "2007.11730", "submitter": "Scott Mahan", "authors": "Scott Mahan, Emily King, Alex Cloninger", "title": "Nonclosedness of Sets of Neural Networks in Sobolev Spaces", "comments": null, "journal-ref": null, "doi": "10.1016/j.neunet.2021.01.007", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the closedness of sets of realized neural networks of a fixed\narchitecture in Sobolev spaces. For an exactly $m$-times differentiable\nactivation function $\\rho$, we construct a sequence of neural networks\n$(\\Phi_n)_{n \\in \\mathbb{N}}$ whose realizations converge in order-$(m-1)$\nSobolev norm to a function that cannot be realized exactly by a neural network.\nThus, sets of realized neural networks are not closed in order-$(m-1)$ Sobolev\nspaces $W^{m-1,p}$ for $p \\in [1,\\infty]$. We further show that these sets are\nnot closed in $W^{m,p}$ under slightly stronger conditions on the $m$-th\nderivative of $\\rho$. For a real analytic activation function, we show that\nsets of realized neural networks are not closed in $W^{k,p}$ for any $k \\in\n\\mathbb{N}$. The nonclosedness allows for approximation of non-network target\nfunctions with unbounded parameter growth. We partially characterize the rate\nof parameter growth for most activation functions by showing that a specific\nsequence of realized neural networks can approximate the activation function's\nderivative with weights increasing inversely proportional to the $L^p$\napproximation error. Finally, we present experimental results showing that\nnetworks are capable of closely approximating non-network target functions with\nincreasing parameters via training.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 00:57:25 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 02:33:47 GMT"}, {"version": "v3", "created": "Sat, 9 Jan 2021 01:14:50 GMT"}, {"version": "v4", "created": "Wed, 27 Jan 2021 20:10:39 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Mahan", "Scott", ""], ["King", "Emily", ""], ["Cloninger", "Alex", ""]]}, {"id": "2007.11752", "submitter": "Ting-Wu Chin", "authors": "Ting-Wu Chin, Ari S. Morcos, Diana Marculescu", "title": "Joslim: Joint Widths and Weights Optimization for Slimmable Neural\n  Networks", "comments": "Accepted at ECML-PKDD 2021 (Research Track), 4-page abridged versions\n  have been accepted at non-archival venues including RealML and DMMLSys\n  workshops at ICML'20 and DLP-KDD and AdvML workshops at KDD'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slimmable neural networks provide a flexible trade-off front between\nprediction error and computational requirement (such as the number of\nfloating-point operations or FLOPs) with the same storage requirement as a\nsingle model. They are useful for reducing maintenance overhead for deploying\nmodels to devices with different memory constraints and are useful for\noptimizing the efficiency of a system with many CNNs. However, existing\nslimmable network approaches either do not optimize layer-wise widths or\noptimize the shared-weights and layer-wise widths independently, thereby\nleaving significant room for improvement by joint width and weight\noptimization. In this work, we propose a general framework to enable joint\noptimization for both width configurations and weights of slimmable networks.\nOur framework subsumes conventional and NAS-based slimmable methods as special\ncases and provides flexibility to improve over existing methods. From a\npractical standpoint, we propose Joslim, an algorithm that jointly optimizes\nboth the widths and weights for slimmable nets, which outperforms existing\nmethods for optimizing slimmable networks across various networks, datasets,\nand objectives. Quantitatively, improvements up to 1.7% and 8% in top-1\naccuracy on the ImageNet dataset can be attained for MobileNetV2 considering\nFLOPs and memory footprint, respectively. Our results highlight the potential\nof optimizing the channel counts for different layers jointly with the weights\nfor slimmable networks. Code available at https://github.com/cmu-enyac/Joslim.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 02:05:03 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 01:33:08 GMT"}, {"version": "v3", "created": "Thu, 24 Jun 2021 21:24:17 GMT"}, {"version": "v4", "created": "Wed, 30 Jun 2021 14:38:29 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Chin", "Ting-Wu", ""], ["Morcos", "Ari S.", ""], ["Marculescu", "Diana", ""]]}, {"id": "2007.11771", "submitter": "Peng Liao", "authors": "Peng Liao, Zhengling Qi, Susan Murphy", "title": "Batch Policy Learning in Average Reward Markov Decision Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the batch (off-line) policy learning problem in the infinite\nhorizon Markov Decision Process. Motivated by mobile health applications, we\nfocus on learning a policy that maximizes the long-term average reward. We\npropose a doubly robust estimator for the average reward and show that it\nachieves semiparametric efficiency given multiple trajectories collected under\nsome behavior policy. Based on the proposed estimator, we develop an\noptimization algorithm to compute the optimal policy in a parameterized\nstochastic policy class. The performance of the estimated policy is measured by\nthe difference between the optimal average reward in the policy class and the\naverage reward of the estimated policy and we establish a finite-sample regret\nguarantee. To the best of our knowledge, this is the first regret bound for\nbatch policy learning in the infinite time horizon setting. The performance of\nthe method is illustrated by simulation studies.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 03:28:14 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Liao", "Peng", ""], ["Qi", "Zhengling", ""], ["Murphy", "Susan", ""]]}, {"id": "2007.11811", "submitter": "Xiaohan Yan", "authors": "Xiaohan Yan, Avleen S. Bijral", "title": "On a Bernoulli Autoregression Framework for Link Discovery and\n  Prediction", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a dynamic prediction framework for binary sequences that is based\non a Bernoulli generalization of the auto-regressive process. Our approach\nlends itself easily to variants of the standard link prediction problem for a\nsequence of time dependent networks. Focusing on this dynamic network link\nprediction/recommendation task, we propose a novel problem that exploits\nadditional information via a much larger sequence of auxiliary networks and has\nimportant real-world relevance. To allow discovery of links that do not exist\nin the available data, our model estimation framework introduces a\nregularization term that presents a trade-off between the conventional link\nprediction and this discovery task. In contrast to existing work our stochastic\ngradient based estimation approach is highly efficient and can scale to\nnetworks with millions of nodes. We show extensive empirical results on both\nactual product-usage based time dependent networks and also present results on\na Reddit based data set of time dependent sentiment sequences.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 05:58:22 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Yan", "Xiaohan", ""], ["Bijral", "Avleen S.", ""]]}, {"id": "2007.11819", "submitter": "Karoline Brucke", "authors": "Karoline Brucke, Stefan Arens, Jan-Simon Telle, Thomas Steens,\n  Benedikt Hanke, Karsten von Maydell, Carsten Agert", "title": "A Non-Intrusive Load Monitoring Approach for Very Short Term Power\n  Predictions in Commercial Buildings", "comments": "15 pages, 14 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new algorithm to extract device profiles fully\nunsupervised from three phases reactive and active aggregate power\nmeasurements. The extracted device profiles are applied for the disaggregation\nof the aggregate power measurements using particle swarm optimization. Finally,\nthis paper provides a new approach for short term power predictions using the\ndisaggregation data. For this purpose, a state changes forecast for every\ndevice is carried out by an artificial neural network and converted into a\npower prediction afterwards by reconstructing the power regarding the state\nchanges and the device profiles. The forecast horizon is 15 minutes. To\ndemonstrate the developed approaches, three phase reactive and active aggregate\npower measurements of a multi-tenant commercial building are used. The\ngranularity of data is 1 s. In this work, 52 device profiles are extracted from\nthe aggregate power data. The disaggregation shows a very accurate\nreconstruction of the measured power with a percentage energy error of\napproximately 1 %. The developed indirect power prediction method applied to\nthe measured power data outperforms two persistence forecasts and an artificial\nneural network, which is designed for 24h-day-ahead power predictions working\nin the power domain.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 06:37:55 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Brucke", "Karoline", ""], ["Arens", "Stefan", ""], ["Telle", "Jan-Simon", ""], ["Steens", "Thomas", ""], ["Hanke", "Benedikt", ""], ["von Maydell", "Karsten", ""], ["Agert", "Carsten", ""]]}, {"id": "2007.11826", "submitter": "Cong Han Lim", "authors": "Cong Han Lim, Raquel Urtasun, Ersin Yumer", "title": "Hierarchical Verification for Adversarial Robustness", "comments": "Published at ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new framework for the exact point-wise $\\ell_p$ robustness\nverification problem that exploits the layer-wise geometric structure of deep\nfeed-forward networks with rectified linear activations (ReLU networks). The\nactivation regions of the network partition the input space, and one can verify\nthe $\\ell_p$ robustness around a point by checking all the activation regions\nwithin the desired radius. The GeoCert algorithm (Jordan et al., NeurIPS 2019)\ntreats this partition as a generic polyhedral complex in order to detect which\nregion to check next. In contrast, our LayerCert framework considers the\n\\emph{nested hyperplane arrangement} structure induced by the layers of the\nReLU network and explores regions in a hierarchical manner. We show that, under\ncertain conditions on the algorithm parameters, LayerCert provably reduces the\nnumber and size of the convex programs that one needs to solve compared to\nGeoCert. Furthermore, our LayerCert framework allows the incorporation of lower\nbounding routines based on convex relaxations to further improve performance.\nExperimental results demonstrate that LayerCert can significantly reduce both\nthe number of convex programs solved and the running time over the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 07:03:05 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Lim", "Cong Han", ""], ["Urtasun", "Raquel", ""], ["Yumer", "Ersin", ""]]}, {"id": "2007.11831", "submitter": "Qing Ye", "authors": "Qing Ye, Yuhao Zhou, Mingjia Shi, Yanan Sun, Jiancheng Lv", "title": "DBS: Dynamic Batch Size For Distributed Deep Neural Network Training", "comments": "NIPS2020 under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synchronous strategies with data parallelism, such as the Synchronous\nStochasticGradient Descent (S-SGD) and the model averaging methods, are widely\nutilizedin distributed training of Deep Neural Networks (DNNs), largely owing\nto itseasy implementation yet promising performance. Particularly, each worker\nofthe cluster hosts a copy of the DNN and an evenly divided share of the\ndatasetwith the fixed mini-batch size, to keep the training of DNNs\nconvergence. In thestrategies, the workers with different computational\ncapability, need to wait foreach other because of the synchronization and\ndelays in network transmission,which will inevitably result in the\nhigh-performance workers wasting computation.Consequently, the utilization of\nthe cluster is relatively low. To alleviate thisissue, we propose the Dynamic\nBatch Size (DBS) strategy for the distributedtraining of DNNs. Specifically,\nthe performance of each worker is evaluatedfirst based on the fact in the\nprevious epoch, and then the batch size and datasetpartition are dynamically\nadjusted in consideration of the current performanceof the worker, thereby\nimproving the utilization of the cluster. To verify theeffectiveness of the\nproposed strategy, extensive experiments have been conducted,and the\nexperimental results indicate that the proposed strategy can fully utilizethe\nperformance of the cluster, reduce the training time, and have good\nrobustnesswith disturbance by irrelevant tasks. Furthermore, rigorous\ntheoretical analysis hasalso been provided to prove the convergence of the\nproposed strategy.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 07:31:55 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Ye", "Qing", ""], ["Zhou", "Yuhao", ""], ["Shi", "Mingjia", ""], ["Sun", "Yanan", ""], ["Lv", "Jiancheng", ""]]}, {"id": "2007.11836", "submitter": "Federico Amato", "authors": "Federico Amato, Fabian Guignard, Sylvain Robert, Mikhail Kanevski", "title": "A Novel Framework for Spatio-Temporal Prediction of Environmental Data\n  Using Deep Learning", "comments": "11 pages, 8 figures", "journal-ref": "Sci Rep 10, 22243 (2020)", "doi": "10.1038/s41598-020-79148-7", "report-no": null, "categories": "stat.ML cs.LG physics.ao-ph physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the role played by statistical and computational sciences in climate and\nenvironmental modelling and prediction becomes more important, Machine Learning\nresearchers are becoming more aware of the relevance of their work to help\ntackle the climate crisis. Indeed, being universal nonlinear function\napproximation tools, Machine Learning algorithms are efficient in analysing and\nmodelling spatially and temporally variable environmental data. While Deep\nLearning models have proved to be able to capture spatial, temporal, and\nspatio-temporal dependencies through their automatic feature representation\nlearning, the problem of the interpolation of continuous spatio-temporal fields\nmeasured on a set of irregular points in space is still under-investigated. To\nfill this gap, we introduce here a framework for spatio-temporal prediction of\nclimate and environmental data using deep learning. Specifically, we show how\nspatio-temporal processes can be decomposed in terms of a sum of products of\ntemporally referenced basis functions, and of stochastic spatial coefficients\nwhich can be spatially modelled and mapped on a regular grid, allowing the\nreconstruction of the complete spatio-temporal signal. Applications on two case\nstudies based on simulated and real-world data will show the effectiveness of\nthe proposed framework in modelling coherent spatio-temporal fields.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 07:44:04 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 14:46:24 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Amato", "Federico", ""], ["Guignard", "Fabian", ""], ["Robert", "Sylvain", ""], ["Kanevski", "Mikhail", ""]]}, {"id": "2007.11838", "submitter": "Alexander Lew", "authors": "Alexander K. Lew, Monica Agrawal, David Sontag, Vikash K. Mansinghka", "title": "PClean: Bayesian Data Cleaning at Scale with Domain-Specific\n  Probabilistic Programming", "comments": "Correct formatting error", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data cleaning can be naturally framed as probabilistic inference in a\ngenerative model, combining a prior distribution over ground-truth databases\nwith a likelihood that models the noisy channel by which the data are filtered\nand corrupted to yield incomplete, dirty, and denormalized datasets. Based on\nthis view, we present PClean, a probabilistic programming language for\nleveraging dataset-specific knowledge to clean and normalize dirty data. PClean\nis powered by three modeling and inference contributions: (1) a non-parametric\nmodel of relational database instances, customizable via probabilistic\nprograms, (2) a sequential Monte Carlo inference algorithm that exploits the\nmodel's structure, and (3) near-optimal SMC proposals and blocked Gibbs\nrejuvenation moves constructed on a per-dataset basis. We show empirically that\nshort (< 50-line) PClean programs can be faster and more accurate than generic\nPPL inference on multiple data-cleaning benchmarks; perform comparably in terms\nof accuracy and runtime to state-of-the-art data-cleaning systems (unlike\ngeneric PPL inference given the same runtime); and scale to real-world datasets\nwith millions of records.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 08:01:47 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 05:19:18 GMT"}, {"version": "v3", "created": "Sun, 25 Oct 2020 21:07:53 GMT"}, {"version": "v4", "created": "Tue, 27 Oct 2020 18:41:52 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Lew", "Alexander K.", ""], ["Agrawal", "Monica", ""], ["Sontag", "David", ""], ["Mansinghka", "Vikash K.", ""]]}, {"id": "2007.11847", "submitter": "Amila Silva", "authors": "Amila Silva, Shanika Karunasekera, Christopher Leckie, Ling Luo", "title": "METEOR: Learning Memory and Time Efficient Representations from\n  Multi-modal Data Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many learning tasks involve multi-modal data streams, where continuous data\nfrom different modes convey a comprehensive description about objects. A major\nchallenge in this context is how to efficiently interpret multi-modal\ninformation in complex environments. This has motivated numerous studies on\nlearning unsupervised representations from multi-modal data streams. These\nstudies aim to understand higher-level contextual information (e.g., a Twitter\nmessage) by jointly learning embeddings for the lower-level semantic units in\ndifferent modalities (e.g., text, user, and location of a Twitter message).\nHowever, these methods directly associate each low-level semantic unit with a\ncontinuous embedding vector, which results in high memory requirements. Hence,\ndeploying and continuously learning such models in low-memory devices (e.g.,\nmobile devices) becomes a problem. To address this problem, we present METEOR,\na novel MEmory and Time Efficient Online Representation learning technique,\nwhich: (1) learns compact representations for multi-modal data by sharing\nparameters within semantically meaningful groups and preserves the\ndomain-agnostic semantics; (2) can be accelerated using parallel processes to\naccommodate different stream rates while capturing the temporal changes of the\nunits; and (3) can be easily extended to capture implicit/explicit external\nknowledge related to multi-modal data streams. We evaluate METEOR using two\ntypes of multi-modal data streams (i.e., social media streams and shopping\ntransaction streams) to demonstrate its ability to adapt to different domains.\nOur results show that METEOR preserves the quality of the representations while\nreducing memory usage by around 80% compared to the conventional\nmemory-intensive embeddings.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 08:18:02 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Silva", "Amila", ""], ["Karunasekera", "Shanika", ""], ["Leckie", "Christopher", ""], ["Luo", "Ling", ""]]}, {"id": "2007.11849", "submitter": "Chen-Yu Wei", "authors": "Chen-Yu Wei, Mehdi Jafarnia-Jahromi, Haipeng Luo, Rahul Jain", "title": "Learning Infinite-horizon Average-reward MDPs with Linear Function\n  Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop several new algorithms for learning Markov Decision Processes in\nan infinite-horizon average-reward setting with linear function approximation.\nUsing the optimism principle and assuming that the MDP has a linear structure,\nwe first propose a computationally inefficient algorithm with optimal\n$\\widetilde{O}(\\sqrt{T})$ regret and another computationally efficient variant\nwith $\\widetilde{O}(T^{3/4})$ regret, where $T$ is the number of interactions.\nNext, taking inspiration from adversarial linear bandits, we develop yet\nanother efficient algorithm with $\\widetilde{O}(\\sqrt{T})$ regret under a\ndifferent set of assumptions, improving the best existing result by Hao et al.\n(2020) with $\\widetilde{O}(T^{2/3})$ regret. Moreover, we draw a connection\nbetween this algorithm and the Natural Policy Gradient algorithm proposed by\nKakade (2002), and show that our analysis improves the sample complexity bound\nrecently given by Agarwal et al. (2020).\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 08:23:44 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 09:12:03 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Wei", "Chen-Yu", ""], ["Jafarnia-Jahromi", "Mehdi", ""], ["Luo", "Haipeng", ""], ["Jain", "Rahul", ""]]}, {"id": "2007.11880", "submitter": "Frederic Loge", "authors": "Fr\\'ed\\'eric Log\\'e (CMAP), Erwan Le Pennec (XPOP, CMAP), Habiboulaye\n  Amadou-Boubacar", "title": "Challenging common bolus advisor for self-monitoring type-I diabetes\n  patients using Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patients with diabetes who are self-monitoring have to decide right before\neach meal how much insulin they should take. A standard bolus advisor exists,\nbut has never actually been proven to be optimal in any sense. We challenged\nthis rule applying Reinforcement Learning techniques on data simulated with\nT1DM, an FDA-approved simulator developed by Kovatchev et al. modeling the\ngluco-insulin interaction. Results show that the optimal bolus rule is fairly\ndifferent from the standard bolus advisor, and if followed can actually avoid\nhypoglycemia episodes.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 09:38:54 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Log\u00e9", "Fr\u00e9d\u00e9ric", "", "CMAP"], ["Pennec", "Erwan Le", "", "XPOP, CMAP"], ["Amadou-Boubacar", "Habiboulaye", ""]]}, {"id": "2007.11894", "submitter": "Hyeryung Jang", "authors": "Hyeryung Jang and Osvaldo Simeone", "title": "Multi-Sample Online Learning for Probabilistic Spiking Neural Networks", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNNs) capture some of the efficiency of biological\nbrains for inference and learning via the dynamic, online, event-driven\nprocessing of binary time series. Most existing learning algorithms for SNNs\nare based on deterministic neuronal models, such as leaky integrate-and-fire,\nand rely on heuristic approximations of backpropagation through time that\nenforce constraints such as locality. In contrast, probabilistic SNN models can\nbe trained directly via principled online, local, update rules that have proven\nto be particularly effective for resource-constrained systems. This paper\ninvestigates another advantage of probabilistic SNNs, namely their capacity to\ngenerate independent outputs when queried over the same input. It is shown that\nthe multiple generated output samples can be used during inference to robustify\ndecisions and to quantify uncertainty -- a feature that deterministic SNN\nmodels cannot provide. Furthermore, they can be leveraged for training in order\nto obtain more accurate statistical estimates of the log-loss training\ncriterion, as well as of its gradient. Specifically, this paper introduces an\nonline learning rule based on generalized expectation-maximization (GEM) that\nfollows a three-factor form with global learning signals and is referred to as\nGEM-SNN. Experimental results on structured output memorization and\nclassification on a standard neuromorphic data set demonstrate significant\nimprovements in terms of log-likelihood, accuracy, and calibration when\nincreasing the number of samples used for inference and training.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 10:03:58 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 11:44:25 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Jang", "Hyeryung", ""], ["Simeone", "Osvaldo", ""]]}, {"id": "2007.11926", "submitter": "Ferran Mazzanti", "authors": "Ferran Mazzanti and Enrique Romero", "title": "Efficient Evaluation of the Partition Function of RBMs with Annealed\n  Importance Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic models based on Restricted Boltzmann Machines (RBMs) imply the\nevaluation of normalized Boltzmann factors, which in turn require from the\nevaluation of the partition function Z. The exact evaluation of Z, though,\nbecomes a forbiddingly expensive task as the system size increases. This even\nworsens when one considers most usual learning algorithms for RBMs, where the\nexact evaluation of the gradient of the log-likelihood of the empirical\ndistribution of the data includes the computation of Z at each iteration. The\nAnnealed Importance Sampling (AIS) method provides a tool to stochastically\nestimate the partition function of the system. So far, the standard use of the\nAIS algorithm in the Machine Learning context has been done using a large\nnumber of Monte Carlo steps. In this work we show that this may not be required\nif a proper starting probability distribution is employed as the initialization\nof the AIS algorithm. We analyze the performance of AIS in both small- and\nlarge-sized problems, and show that in both cases a good estimation of Z can be\nobtained with little computational cost.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 10:59:04 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Mazzanti", "Ferran", ""], ["Romero", "Enrique", ""]]}, {"id": "2007.11934", "submitter": "Marcel Neunhoeffer", "authors": "Marcel Neunhoeffer, Zhiwei Steven Wu, Cynthia Dwork", "title": "Private Post-GAN Boosting", "comments": null, "journal-ref": "International Conference on Learning Representations, 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentially private GANs have proven to be a promising approach for\ngenerating realistic synthetic data without compromising the privacy of\nindividuals. Due to the privacy-protective noise introduced in the training,\nthe convergence of GANs becomes even more elusive, which often leads to poor\nutility in the output generator at the end of training. We propose Private\npost-GAN boosting (Private PGB), a differentially private method that combines\nsamples produced by the sequence of generators obtained during GAN training to\ncreate a high-quality synthetic dataset. To that end, our method leverages the\nPrivate Multiplicative Weights method (Hardt and Rothblum, 2010) to reweight\ngenerated samples. We evaluate Private PGB on two dimensional toy data, MNIST\nimages, US Census data and a standard machine learning prediction task. Our\nexperiments show that Private PGB improves upon a standard private GAN approach\nacross a collection of quality measures. We also provide a non-private variant\nof PGB that improves the data quality of standard GAN training.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 11:20:14 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 16:53:34 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Neunhoeffer", "Marcel", ""], ["Wu", "Zhiwei Steven", ""], ["Dwork", "Cynthia", ""]]}, {"id": "2007.11937", "submitter": "Joonas H\\\"am\\\"al\\\"ainen", "authors": "Joonas H\\\"am\\\"al\\\"ainen, Tommi K\\\"arkk\\\"ainen, Tuomo Rossi", "title": "Scalable Initialization Methods for Large-Scale Clustering", "comments": "11 pages, submitted to IEEE Transactions on Big Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, two new initialization methods for K-means clustering are\nproposed. Both proposals are based on applying a divide-and-conquer approach\nfor the K-means|| type of an initialization strategy. The second proposal also\nutilizes multiple lower-dimensional subspaces produced by the random projection\nmethod for the initialization. The proposed methods are scalable and can be run\nin parallel, which make them suitable for initializing large-scale problems. In\nthe experiments, comparison of the proposed methods to the K-means++ and\nK-means|| methods is conducted using an extensive set of reference and\nsynthetic large-scale datasets. Concerning the latter, a novel high-dimensional\nclustering data generation algorithm is given. The experiments show that the\nproposed methods compare favorably to the state-of-the-art. We also observe\nthat the currently most popular K-means++ initialization behaves like the\nrandom one in the very high-dimensional cases.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 11:29:53 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["H\u00e4m\u00e4l\u00e4inen", "Joonas", ""], ["K\u00e4rkk\u00e4inen", "Tommi", ""], ["Rossi", "Tuomo", ""]]}, {"id": "2007.11967", "submitter": "Djallel Bouneffouf", "authors": "Djallel Bouneffouf", "title": "Computing the Dirichlet-Multinomial Log-Likelihood Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dirichlet-multinomial (DMN) distribution is commonly used to model\nover-dispersion in count data. Precise and fast numerical computation of the\nDMN log-likelihood function is important for performing statistical inference\nusing this distribution, and remains a challenge. To address this, we use\nmathematical properties of the gamma function to derive a closed form\nexpression for the DMN log-likelihood function. Compared to existing methods,\ncalculation of the closed form has a lower computational complexity, hence is\nmuch faster without comprimising computational accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 20:31:01 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Bouneffouf", "Djallel", ""]]}, {"id": "2007.11972", "submitter": "Yuxiao Li", "authors": "Yuxiao Li, Ying Sun, Brian J Reich", "title": "DeepKriging: Spatially Dependent Deep Neural Networks for Spatial\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In spatial statistics, a common objective is to predict the values of a\nspatial process at unobserved locations by exploiting spatial dependence. In\ngeostatistics, Kriging provides the best linear unbiased predictor using\ncovariance functions and is often associated with Gaussian processes. However,\nwhen considering non-linear prediction for non-Gaussian and categorical data,\nthe Kriging prediction is not necessarily optimal, and the associated variance\nis often overly optimistic. We propose to use deep neural networks (DNNs) for\nspatial prediction. Although DNNs are widely used for general classification\nand prediction, they have not been studied thoroughly for data with spatial\ndependence. In this work, we propose a novel neural network structure for\nspatial prediction by adding an embedding layer of spatial coordinates with\nbasis functions. We show in theory that the proposed DeepKriging method has\nmultiple advantages over Kriging and classical DNNs only with spatial\ncoordinates as features. We also provide density prediction for uncertainty\nquantification without any distributional assumption and apply the method to\nPM$_{2.5}$ concentrations across the continental United States.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 12:38:53 GMT"}, {"version": "v2", "created": "Sat, 25 Jul 2020 08:15:15 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Li", "Yuxiao", ""], ["Sun", "Ying", ""], ["Reich", "Brian J", ""]]}, {"id": "2007.11975", "submitter": "Nataly Brukhim", "authors": "Nataly Brukhim and Elad Hazan", "title": "Online Boosting with Bandit Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of online boosting for regression tasks, when only\nlimited information is available to the learner. We give an efficient regret\nminimization method that has two implications: an online boosting algorithm\nwith noisy multi-point bandit feedback, and a new projection-free online convex\noptimization algorithm with stochastic gradient, that improves state-of-the-art\nguarantees in terms of efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 12:40:57 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Brukhim", "Nataly", ""], ["Hazan", "Elad", ""]]}, {"id": "2007.11994", "submitter": "Alexander Immer", "authors": "Alexander Immer", "title": "Disentangling the Gauss-Newton Method and Approximate Inference for\n  Neural Networks", "comments": "Master's thesis at EPFL", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this thesis, we disentangle the generalized Gauss-Newton and approximate\ninference for Bayesian deep learning. The generalized Gauss-Newton method is an\noptimization method that is used in several popular Bayesian deep learning\nalgorithms. Algorithms that combine the Gauss-Newton method with the Laplace\nand Gaussian variational approximation have recently led to state-of-the-art\nresults in Bayesian deep learning. While the Laplace and Gaussian variational\napproximation have been studied extensively, their interplay with the\nGauss-Newton method remains unclear. Recent criticism of priors and posterior\napproximations in Bayesian deep learning further urges the need for a deeper\nunderstanding of practical algorithms. The individual analysis of the\nGauss-Newton method and Laplace and Gaussian variational approximations for\nneural networks provides both theoretical insight and new practical algorithms.\nWe find that the Gauss-Newton method simplifies the underlying probabilistic\nmodel significantly. In particular, the combination of the Gauss-Newton method\nwith approximate inference can be cast as inference in a linear or Gaussian\nprocess model. The Laplace and Gaussian variational approximation can\nsubsequently provide a posterior approximation to these simplified models. This\nnew disentangled understanding of recent Bayesian deep learning algorithms also\nleads to new methods: first, the connection to Gaussian processes enables new\nfunction-space inference algorithms. Second, we present a marginal likelihood\napproximation of the underlying probabilistic model to tune neural network\nhyperparameters. Finally, the identified underlying models lead to different\nmethods to compute predictive distributions. In fact, we find that these\nprediction methods for Bayesian neural networks often work better than the\ndefault choice and solve a common issue with the Laplace approximation.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 17:42:58 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Immer", "Alexander", ""]]}, {"id": "2007.12000", "submitter": "Fei Mi", "authors": "Fei Mi, Xiaoyu Lin, and Boi Faltings", "title": "ADER: Adaptively Distilled Exemplar Replay Towards Continual Learning\n  for Session-based Recommendation", "comments": "Accepted at RecSys 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Session-based recommendation has received growing attention recently due to\nthe increasing privacy concern. Despite the recent success of neural\nsession-based recommenders, they are typically developed in an offline manner\nusing a static dataset. However, recommendation requires continual adaptation\nto take into account new and obsolete items and users, and requires \"continual\nlearning\" in real-life applications. In this case, the recommender is updated\ncontinually and periodically with new data that arrives in each update cycle,\nand the updated model needs to provide recommendations for user activities\nbefore the next model update. A major challenge for continual learning with\nneural models is catastrophic forgetting, in which a continually trained model\nforgets user preference patterns it has learned before. To deal with this\nchallenge, we propose a method called Adaptively Distilled Exemplar Replay\n(ADER) by periodically replaying previous training samples (i.e., exemplars) to\nthe current model with an adaptive distillation loss. Experiments are conducted\nbased on the state-of-the-art SASRec model using two widely used datasets to\nbenchmark ADER with several well-known continual learning techniques. We\nempirically demonstrate that ADER consistently outperforms other baselines, and\nit even outperforms the method using all historical data at every update cycle.\nThis result reveals that ADER is a promising solution to mitigate the\ncatastrophic forgetting issue towards building more realistic and scalable\nsession-based recommenders.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 13:19:53 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Mi", "Fei", ""], ["Lin", "Xiaoyu", ""], ["Faltings", "Boi", ""]]}, {"id": "2007.12002", "submitter": "Jonathan Halcrow", "authors": "Jonathan Halcrow, Alexandru Mo\\c{s}oi, Sam Ruth, Bryan Perozzi", "title": "Grale: Designing Networks for Graph Learning", "comments": "10 pages, 6 figures, to be published in KDD'20", "journal-ref": null, "doi": "10.1145/3394486.3403302", "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we find the right graph for semi-supervised learning? In real world\napplications, the choice of which edges to use for computation is the first\nstep in any graph learning process. Interestingly, there are often many types\nof similarity available to choose as the edges between nodes, and the choice of\nedges can drastically affect the performance of downstream semi-supervised\nlearning systems. However, despite the importance of graph design, most of the\nliterature assumes that the graph is static. In this work, we present Grale, a\nscalable method we have developed to address the problem of graph design for\ngraphs with billions of nodes. Grale operates by fusing together different\nmeasures of(potentially weak) similarity to create a graph which exhibits high\ntask-specific homophily between its nodes. Grale is designed for running on\nlarge datasets. We have deployed Grale in more than 20 different industrial\nsettings at Google, including datasets which have tens of billions of nodes,\nand hundreds of trillions of potential edges to score. By employing locality\nsensitive hashing techniques,we greatly reduce the number of pairs that need to\nbe scored, allowing us to learn a task specific model and build the associated\nnearest neighbor graph for such datasets in hours, rather than the days or even\nweeks that might be required otherwise. We illustrate this through a case study\nwhere we examine the application of Grale to an abuse classification problem on\nYouTube with hundreds of million of items. In this application, we find that\nGrale detects a large number of malicious actors on top of hard-coded rules and\ncontent classifiers, increasing the total recall by 89% over those approaches\nalone.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 13:25:36 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Halcrow", "Jonathan", ""], ["Mo\u015foi", "Alexandru", ""], ["Ruth", "Sam", ""], ["Perozzi", "Bryan", ""]]}, {"id": "2007.12020", "submitter": "Youngsung Kim", "authors": "Youngsung Kim, Jinwoo Shin, Eunho Yang, Sung Ju Hwang", "title": "Few-shot Visual Reasoning with Meta-analogical Contrastive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While humans can solve a visual puzzle that requires logical reasoning by\nobserving only few samples, it would require training over large amount of data\nfor state-of-the-art deep reasoning models to obtain similar performance on the\nsame task. In this work, we propose to solve such a few-shot (or low-shot)\nvisual reasoning problem, by resorting to analogical reasoning, which is a\nunique human ability to identify structural or relational similarity between\ntwo sets. Specifically, given training and test sets that contain the same type\nof visual reasoning problems, we extract the structural relationships between\nelements in both domains, and enforce them to be as similar as possible with\nanalogical learning. We repeatedly apply this process with slightly modified\nqueries of the same problem under the assumption that it does not affect the\nrelationship between a training and a test sample. This allows to learn the\nrelational similarity between the two samples in an effective manner even with\na single pair of samples. We validate our method on RAVEN dataset, on which it\noutperforms state-of-the-art method, with larger gains when the training data\nis scarce. We further meta-learn our analogical contrastive learning model over\nthe same tasks with diverse attributes, and show that it generalizes to the\nsame visual reasoning problem with unseen attributes.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 14:00:34 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Kim", "Youngsung", ""], ["Shin", "Jinwoo", ""], ["Yang", "Eunho", ""], ["Hwang", "Sung Ju", ""]]}, {"id": "2007.12035", "submitter": "Floris Geerts", "authors": "Floris Geerts", "title": "The expressive power of kth-order invariant graph networks", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expressive power of graph neural network formalisms is commonly measured\nby their ability to distinguish graphs. For many formalisms, the k-dimensional\nWeisfeiler-Leman (k-WL) graph isomorphism test is used as a yardstick. In this\npaper we consider the expressive power of kth-order invariant (linear) graph\nnetworks (k-IGNs). It is known that k-IGNs are expressive enough to simulate\nk-WL. This means that for any two graphs that can be distinguished by k-WL, one\ncan find a k-IGN which also distinguishes those graphs. The question remains\nwhether k-IGNs can distinguish more graphs than k-WL. This was recently shown\nto be false for k=2. Here, we generalise this result to arbitrary k. In other\nwords, we show that k-IGNs are bounded in expressive power by k-WL. This\nimplies that k-IGNs and k-WL are equally powerful in distinguishing graphs.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 14:30:51 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Geerts", "Floris", ""]]}, {"id": "2007.12036", "submitter": "Sergio Casas", "authors": "Sergio Casas, Cole Gulino, Simon Suo, Katie Luo, Renjie Liao, Raquel\n  Urtasun", "title": "Implicit Latent Variable Model for Scene-Consistent Motion Forecasting", "comments": "European Conference on Computer Vision (ECCV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to plan a safe maneuver an autonomous vehicle must accurately\nperceive its environment, and understand the interactions among traffic\nparticipants. In this paper, we aim to learn scene-consistent motion forecasts\nof complex urban traffic directly from sensor data. In particular, we propose\nto characterize the joint distribution over future trajectories via an implicit\nlatent variable model. We model the scene as an interaction graph and employ\npowerful graph neural networks to learn a distributed latent representation of\nthe scene. Coupled with a deterministic decoder, we obtain trajectory samples\nthat are consistent across traffic participants, achieving state-of-the-art\nresults in motion forecasting and interaction understanding. Last but not\nleast, we demonstrate that our motion forecasts result in safer and more\ncomfortable motion planning.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 14:31:25 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Casas", "Sergio", ""], ["Gulino", "Cole", ""], ["Suo", "Simon", ""], ["Luo", "Katie", ""], ["Liao", "Renjie", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2007.12070", "submitter": "Chuanshuai Chen", "authors": "Chuanshuai Chen, Jiazhu Dai", "title": "Mitigating backdoor attacks in LSTM-based Text Classification Systems by\n  Backdoor Keyword Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been proved that deep neural networks are facing a new threat called\nbackdoor attacks, where the adversary can inject backdoors into the neural\nnetwork model through poisoning the training dataset. When the input containing\nsome special pattern called the backdoor trigger, the model with backdoor will\ncarry out malicious task such as misclassification specified by adversaries. In\ntext classification systems, backdoors inserted in the models can cause spam or\nmalicious speech to escape detection. Previous work mainly focused on the\ndefense of backdoor attacks in computer vision, little attention has been paid\nto defense method for RNN backdoor attacks regarding text classification. In\nthis paper, through analyzing the changes in inner LSTM neurons, we proposed a\ndefense method called Backdoor Keyword Identification (BKI) to mitigate\nbackdoor attacks which the adversary performs against LSTM-based text\nclassification by data poisoning. This method can identify and exclude\npoisoning samples crafted to insert backdoor into the model from training data\nwithout a verified and trusted dataset. We evaluate our method on four\ndifferent text classification datset: IMDB, DBpedia ontology, 20 newsgroups and\nReuters-21578 dataset. It all achieves good performance regardless of the\ntrigger sentences.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 09:05:16 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 16:05:45 GMT"}, {"version": "v3", "created": "Mon, 15 Mar 2021 03:45:46 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Chen", "Chuanshuai", ""], ["Dai", "Jiazhu", ""]]}, {"id": "2007.12087", "submitter": "James Jordon", "authors": "James Jordon, Daniel Jarrett, Jinsung Yoon, Tavian Barnes, Paul\n  Elbers, Patrick Thoral, Ari Ercole, Cheng Zhang, Danielle Belgrave and\n  Mihaela van der Schaar", "title": "Hide-and-Seek Privacy Challenge", "comments": "19 pages, 5 figures. Part of the NeurIPS 2020 competition track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The clinical time-series setting poses a unique combination of challenges to\ndata modeling and sharing. Due to the high dimensionality of clinical time\nseries, adequate de-identification to preserve privacy while retaining data\nutility is difficult to achieve using common de-identification techniques. An\ninnovative approach to this problem is synthetic data generation. From a\ntechnical perspective, a good generative model for time-series data should\npreserve temporal dynamics, in the sense that new sequences respect the\noriginal relationships between high-dimensional variables across time. From the\nprivacy perspective, the model should prevent patient re-identification by\nlimiting vulnerability to membership inference attacks. The NeurIPS 2020\nHide-and-Seek Privacy Challenge is a novel two-tracked competition to\nsimultaneously accelerate progress in tackling both problems. In our\nhead-to-head format, participants in the synthetic data generation track (i.e.\n\"hiders\") and the patient re-identification track (i.e. \"seekers\") are directly\npitted against each other by way of a new, high-quality intensive care\ntime-series dataset: the AmsterdamUMCdb dataset. Ultimately, we seek to advance\ngenerative techniques for dense and high-dimensional temporal data streams that\nare (1) clinically meaningful in terms of fidelity and predictivity, as well as\n(2) capable of minimizing membership privacy risks in terms of the concrete\nnotion of patient re-identification.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 15:50:59 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 17:10:18 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Jordon", "James", ""], ["Jarrett", "Daniel", ""], ["Yoon", "Jinsung", ""], ["Barnes", "Tavian", ""], ["Elbers", "Paul", ""], ["Thoral", "Patrick", ""], ["Ercole", "Ari", ""], ["Zhang", "Cheng", ""], ["Belgrave", "Danielle", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "2007.12098", "submitter": "Neha Prasad", "authors": "Neha Prasad, Karren Yang, Caroline Uhler", "title": "Optimal Transport using GANs for Lineage Tracing", "comments": "4 pages excluding references, 2 figures, 3 tables. Accepted at ICML\n  2020 Workshop on Computational Biology for Spotlight Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present Super-OT, a novel approach to computational lineage\ntracing that combines a supervised learning framework with optimal transport\nbased on Generative Adversarial Networks (GANs). Unlike previous approaches to\nlineage tracing, Super-OT has the flexibility to integrate paired data. We\nbenchmark Super-OT based on single-cell RNA-seq data against Waddington-OT, a\npopular approach for lineage tracing that also employs optimal transport. We\nshow that Super-OT achieves gains over Waddington-OT in predicting the class\noutcome of cells during differentiation, since it allows the integration of\nadditional information during \\mbox{training.}\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 16:01:59 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 13:29:00 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Prasad", "Neha", ""], ["Yang", "Karren", ""], ["Uhler", "Caroline", ""]]}, {"id": "2007.12100", "submitter": "Qiang Liu", "authors": "Qiang Liu and Zhaocheng Liu and Xiaofang Zhu and Yeliang Xiu", "title": "Deep Active Learning by Model Interpretability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent successes of Deep Neural Networks (DNNs) in a variety of research\ntasks, however, heavily rely on the large amounts of labeled samples. This may\nrequire considerable annotation cost in real-world applications. Fortunately,\nactive learning is a promising methodology to train high-performing model with\nminimal annotation cost. In the deep learning context, the critical question of\nactive learning is how to precisely identify the informativeness of samples for\nDNN. In this paper, inspired by piece-wise linear interpretability in DNN, we\nintroduce the linearly separable regions of samples to the problem of active\nlearning, and propose a novel Deep Active learning approach by Model\nInterpretability (DAMI). To keep the maximal representativeness of the entire\nunlabeled data, DAMI tries to select and label samples on different linearly\nseparable regions introduced by the piece-wise linear interpretability in DNN.\nWe focus on modeling Multi-Layer Perception (MLP) for modeling tabular data.\nSpecifically, we use the local piece-wise interpretation in MLP as the\nrepresentation of each sample, and directly run K-Center clustering to select\nand label samples. To be noted, this whole process of DAMI does not require any\nhyper-parameters to tune manually. To verify the effectiveness of our approach,\nextensive experiments have been conducted on several tabular datasets. The\nexperimental results demonstrate that DAMI constantly outperforms several\nstate-of-the-art compared approaches.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 16:06:27 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 09:22:20 GMT"}, {"version": "v3", "created": "Wed, 26 Aug 2020 12:01:45 GMT"}, {"version": "v4", "created": "Sun, 6 Sep 2020 06:28:52 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Liu", "Qiang", ""], ["Liu", "Zhaocheng", ""], ["Zhu", "Xiaofang", ""], ["Xiu", "Yeliang", ""]]}, {"id": "2007.12101", "submitter": "Ameesh Shah", "authors": "Ameesh Shah, Eric Zhan, Jennifer J. Sun, Abhinav Verma, Yisong Yue,\n  Swarat Chaudhuri", "title": "Learning Differentiable Programs with Admissible Neural Heuristics", "comments": "9 pages, published in NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning differentiable functions expressed as\nprograms in a domain-specific language. Such programmatic models can offer\nbenefits such as composability and interpretability; however, learning them\nrequires optimizing over a combinatorial space of program \"architectures\". We\nframe this optimization problem as a search in a weighted graph whose paths\nencode top-down derivations of program syntax. Our key innovation is to view\nvarious classes of neural networks as continuous relaxations over the space of\nprograms, which can then be used to complete any partial program. This relaxed\nprogram is differentiable and can be trained end-to-end, and the resulting\ntraining loss is an approximately admissible heuristic that can guide the\ncombinatorial search. We instantiate our approach on top of the A-star\nalgorithm and an iteratively deepened branch-and-bound search, and use these\nalgorithms to learn programmatic classifiers in three sequence classification\ntasks. Our experiments show that the algorithms outperform state-of-the-art\nmethods for program learning, and that they discover programmatic classifiers\nthat yield natural interpretations and achieve competitive accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 16:07:39 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 03:41:24 GMT"}, {"version": "v3", "created": "Sat, 26 Sep 2020 00:24:44 GMT"}, {"version": "v4", "created": "Tue, 1 Dec 2020 02:11:06 GMT"}, {"version": "v5", "created": "Sun, 28 Mar 2021 01:15:33 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Shah", "Ameesh", ""], ["Zhan", "Eric", ""], ["Sun", "Jennifer J.", ""], ["Verma", "Abhinav", ""], ["Yue", "Yisong", ""], ["Chaudhuri", "Swarat", ""]]}, {"id": "2007.12133", "submitter": "Dimitar I. Dimitrov", "authors": "Dimitar I. Dimitrov, Gagandeep Singh, Timon Gehr, Martin Vechev", "title": "Scalable Inference of Symbolic Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for generating symbolic adversarial examples: input\nregions guaranteed to only contain adversarial examples for the given neural\nnetwork. These regions can generate real-world adversarial examples as they\nsummarize trillions of adversarial examples.\n  We theoretically show that computing optimal symbolic adversarial examples is\ncomputationally expensive. We present a method for approximating optimal\nexamples in a scalable manner. Our method first selectively uses adversarial\nattacks to generate a candidate region and then prunes this region with\nhyperplanes that fit points obtained via specialized sampling. It iterates\nuntil arriving at a symbolic adversarial example for which it can prove, via\nstate-of-the-art convex relaxation techniques, that the region only contains\nadversarial examples. Our experimental results demonstrate that our method is\npractically effective: it only needs a few thousand attacks to infer symbolic\nsummaries guaranteed to contain $\\approx 10^{258}$ adversarial examples.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 17:03:56 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 22:45:30 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Dimitrov", "Dimitar I.", ""], ["Singh", "Gagandeep", ""], ["Gehr", "Timon", ""], ["Vechev", "Martin", ""]]}, {"id": "2007.12158", "submitter": "Michael O'Keeffe", "authors": "Albert R. Gnadt, Joseph Belarge, Aaron Canciani, Lauren Conger, Joseph\n  Curro, Alan Edelman, Peter Morales, Michael F. O'Keeffe, Jonathan Taylor,\n  Christopher Rackauckas", "title": "Signal Enhancement for Magnetic Navigation Challenge Problem", "comments": "21 pages, 4 figures. See\n  https://github.com/MIT-AI-Accelerator/MagNav.jl for accompanying data and\n  code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.geo-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Harnessing the magnetic field of the earth for navigation has shown promise\nas a viable alternative to other navigation systems. A magnetic navigation\nsystem collects its own magnetic field data using a magnetometer and uses\nmagnetic anomaly maps to determine the current location. The greatest challenge\nwith magnetic navigation arises when the magnetic field data from the\nmagnetometer on the navigation system encompass the magnetic field from not\njust the earth, but also from the vehicle on which it is mounted. It is\ndifficult to separate the earth magnetic anomaly field magnitude, which is\ncrucial for navigation, from the total magnetic field magnitude reading from\nthe sensor. The purpose of this challenge problem is to decouple the earth and\naircraft magnetic signals in order to derive a clean signal from which to\nperform magnetic navigation. Baseline testing on the dataset shows that the\nearth magnetic field can be extracted from the total magnetic field using\nmachine learning (ML). The challenge is to remove the aircraft magnetic field\nfrom the total magnetic field using a trained neural network. These challenges\noffer an opportunity to construct an effective neural network for removing the\naircraft magnetic field from the dataset, using an ML algorithm integrated with\nphysics of magnetic navigation.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 17:44:02 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Gnadt", "Albert R.", ""], ["Belarge", "Joseph", ""], ["Canciani", "Aaron", ""], ["Conger", "Lauren", ""], ["Curro", "Joseph", ""], ["Edelman", "Alan", ""], ["Morales", "Peter", ""], ["O'Keeffe", "Michael F.", ""], ["Taylor", "Jonathan", ""], ["Rackauckas", "Christopher", ""]]}, {"id": "2007.12160", "submitter": "Shintaro Fukushima", "authors": "Shintaro Fukushima and Atsushi Nitanda and Kenji Yamanishi", "title": "Online Robust and Adaptive Learning from Data Streams", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In online learning from non-stationary data streams, it is both necessary to\nlearn robustly to outliers and to adapt to changes of underlying data\ngenerating mechanism quickly. In this paper, we refer to the former nature of\nonline learning algorithms as robustness and the latter as adaptivity. There is\nan obvious tradeoff between them. It is a fundamental issue to quantify and\nevaluate the tradeoff because it provides important information on the data\ngenerating mechanism. However, no previous work has considered the tradeoff\nquantitatively. We propose a novel algorithm called the Stochastic\napproximation-based Robustness-Adaptivity algorithm (SRA) to evaluate the\ntradeoff. The key idea of SRA is to update parameters of distribution or\nsufficient statistics with the biased stochastic approximation scheme, while\ndropping data points with large values of the stochastic update. We address the\nrelation between two parameters, one of which is the step size of the\nstochastic approximation, and the other is the threshold parameter of the norm\nof the stochastic update. The former controls the adaptivity and the latter\ndoes the robustness. We give a theoretical analysis for the non-asymptotic\nconvergence of SRA in the presence of outliers, which depends on both the step\nsize and the threshold parameter. Since SRA is formulated on the\nmajorization-minimization principle, it is a general algorithm including many\nalgorithms, such as the online EM algorithm and stochastic gradient descent.\nEmpirical experiments for both synthetic and real datasets demonstrated that\nSRA was superior to previous methods.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 17:49:04 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Fukushima", "Shintaro", ""], ["Nitanda", "Atsushi", ""], ["Yamanishi", "Kenji", ""]]}, {"id": "2007.12161", "submitter": "Morteza Noshad Iranzad", "authors": "Morteza Noshad, Ivana Jankovic, Jonathan H. Chen", "title": "Clinical Recommender System: Predicting Medical Specialty Diagnostic\n  Choices with Neural Network Ensembles", "comments": "Proceedings of 2020 KDD Workshop onApplied Data Science for\n  Healthcare (KDD 2020).ACM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing demand for key healthcare resources such as clinical expertise\nand facilities has motivated the emergence of artificial intelligence (AI)\nbased decision support systems. We address the problem of predicting clinical\nworkups for specialty referrals. As an alternative for manually-created\nclinical checklists, we propose a data-driven model that recommends the\nnecessary set of diagnostic procedures based on the patients' most recent\nclinical record extracted from the Electronic Health Record (EHR). This has the\npotential to enable health systems expand timely access to initial medical\nspecialty diagnostic workups for patients. The proposed approach is based on an\nensemble of feed-forward neural networks and achieves significantly higher\naccuracy compared to the conventional clinical checklists.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 17:50:15 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Noshad", "Morteza", ""], ["Jankovic", "Ivana", ""], ["Chen", "Jonathan H.", ""]]}, {"id": "2007.12167", "submitter": "Themistoklis Botsas", "authors": "Romit Maulik, Themistoklis Botsas, Nesar Ramachandra, Lachlan Robert\n  Mason and Indranil Pan", "title": "Latent-space time evolution of non-intrusive reduced-order models using\n  Gaussian process emulation", "comments": null, "journal-ref": null, "doi": "10.1016/j.physd.2020.132797", "report-no": null, "categories": "physics.comp-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-intrusive reduced-order models (ROMs) have recently generated\nconsiderable interest for constructing computationally efficient counterparts\nof nonlinear dynamical systems emerging from various domain sciences. They\nprovide a low-dimensional emulation framework for systems that may be\nintrinsically high-dimensional. This is accomplished by utilizing a\nconstruction algorithm that is purely data-driven. It is no surprise,\ntherefore, that the algorithmic advances of machine learning have led to\nnon-intrusive ROMs with greater accuracy and computational gains. However, in\nbypassing the utilization of an equation-based evolution, it is often seen that\nthe interpretability of the ROM framework suffers. This becomes more\nproblematic when black-box deep learning methods are used which are notorious\nfor lacking robustness outside the physical regime of the observed data. In\nthis article, we propose the use of a novel latent-space interpolation\nalgorithm based on Gaussian process regression. Notably, this reduced-order\nevolution of the system is parameterized by control parameters to allow for\ninterpolation in space. The use of this procedure also allows for a continuous\ninterpretation of time which allows for temporal interpolation. The latter\naspect provides information, with quantified uncertainty, about full-state\nevolution at a finer resolution than that utilized for training the ROMs. We\nassess the viability of this algorithm for an advection-dominated system given\nby the inviscid shallow water equations.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 17:56:47 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 08:58:59 GMT"}], "update_date": "2020-12-30", "authors_parsed": [["Maulik", "Romit", ""], ["Botsas", "Themistoklis", ""], ["Ramachandra", "Nesar", ""], ["Mason", "Lachlan Robert", ""], ["Pan", "Indranil", ""]]}, {"id": "2007.12173", "submitter": "Unnat Jain", "authors": "Luca Weihs, Unnat Jain, Jordi Salvador, Svetlana Lazebnik, Aniruddha\n  Kembhavi, Alexander Schwing", "title": "Bridging the Imitation Gap by Adaptive Insubordination", "comments": "The first two authors contributed equally. Project page:\n  https://unnat.github.io/advisor/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When expert supervision is available, practitioners often use imitation\nlearning with varying degrees of success. We show that when an expert has\naccess to privileged information that is unavailable to the student, this\ninformation is marginalized in the student policy during imitation learning\nresulting in an \"imitation gap\" and, potentially, poor results. Prior work\nbridges this gap via a progression from imitation learning to reinforcement\nlearning. While often successful, gradual progression fails for tasks that\nrequire frequent switches between exploration and memorization skills. To\nbetter address these tasks and alleviate the imitation gap we propose 'Adaptive\nInsubordination' (ADVISOR), which dynamically weights imitation and\nreward-based reinforcement learning losses during training, enabling switching\nbetween imitation and exploration. On a suite of challenging didactic and\nMiniGrid tasks, we show that ADVISOR outperforms pure imitation, pure\nreinforcement learning, as well as their sequential and parallel combinations.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 17:59:57 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 21:48:30 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Weihs", "Luca", ""], ["Jain", "Unnat", ""], ["Salvador", "Jordi", ""], ["Lazebnik", "Svetlana", ""], ["Kembhavi", "Aniruddha", ""], ["Schwing", "Alexander", ""]]}, {"id": "2007.12213", "submitter": "Marco A. Armenta", "authors": "Marco Antonio Armenta and Pierre-Marc Jodoin", "title": "The Representation Theory of Neural Networks", "comments": "52 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.RT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we show that neural networks can be represented via the\nmathematical theory of quiver representations. More specifically, we prove that\na neural network is a quiver representation with activation functions, a\nmathematical object that we represent using a network quiver. Also, we show\nthat network quivers gently adapt to common neural network concepts such as\nfully-connected layers, convolution operations, residual connections, batch\nnormalization, pooling operations and even randomly wired neural networks. We\nshow that this mathematical representation is by no means an approximation of\nwhat neural networks are as it exactly matches reality. This interpretation is\nalgebraic and can be studied with algebraic methods. We also provide a quiver\nrepresentation model to understand how a neural network creates representations\nfrom the data. We show that a neural network saves the data as quiver\nrepresentations, and maps it to a geometrical space called the moduli space,\nwhich is given in terms of the underlying oriented graph of the network, i.e.,\nits quiver. This results as a consequence of our defined objects and of\nunderstanding how the neural network computes a prediction in a combinatorial\nand algebraic way. Overall, representing neural networks through the quiver\nrepresentation theory leads to 9 consequences and 4 inquiries for future\nresearch that we believe are of great interest to better understand what neural\nnetworks are and how they work.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 19:02:14 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 19:20:35 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Armenta", "Marco Antonio", ""], ["Jodoin", "Pierre-Marc", ""]]}, {"id": "2007.12216", "submitter": "Zhi-Gang Liu", "authors": "Zhi-Gang Liu and Matthew Mattina", "title": "Efficient Residue Number System Based Winograd Convolution", "comments": "Accepted by ECCV2020 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior research has shown that Winograd algorithm can reduce the computational\ncomplexity of convolutional neural networks (CNN) with weights and activations\nrepresented in floating point. However it is difficult to apply the scheme to\nthe inference of low-precision quantized (e.g. INT8) networks. Our work extends\nthe Winograd algorithm to Residue Number System (RNS). The minimal complexity\nconvolution is computed precisely over large transformation tile (e.g. 10 x 10\nto 16 x 16) of filters and activation patches using the Winograd transformation\nand low cost (e.g. 8-bit) arithmetic without degrading the prediction accuracy\nof the networks during inference. The arithmetic complexity reduction is up to\n7.03x while the performance improvement is up to 2.30x to 4.69x for 3 x 3 and 5\nx 5 filters respectively.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 19:07:06 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Liu", "Zhi-Gang", ""], ["Mattina", "Matthew", ""]]}, {"id": "2007.12223", "submitter": "Tianlong Chen", "authors": "Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang,\n  Zhangyang Wang, Michael Carbin", "title": "The Lottery Ticket Hypothesis for Pre-trained BERT Networks", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In natural language processing (NLP), enormous pre-trained models like BERT\nhave become the standard starting point for training on a range of downstream\ntasks, and similar trends are emerging in other areas of deep learning. In\nparallel, work on the lottery ticket hypothesis has shown that models for NLP\nand computer vision contain smaller matching subnetworks capable of training in\nisolation to full accuracy and transferring to other tasks. In this work, we\ncombine these observations to assess whether such trainable, transferrable\nsubnetworks exist in pre-trained BERT models. For a range of downstream tasks,\nwe indeed find matching subnetworks at 40% to 90% sparsity. We find these\nsubnetworks at (pre-trained) initialization, a deviation from prior NLP\nresearch where they emerge only after some amount of training. Subnetworks\nfound on the masked language modeling task (the same task used to pre-train the\nmodel) transfer universally; those found on other tasks transfer in a limited\nfashion if at all. As large-scale pre-training becomes an increasingly central\nparadigm in deep learning, our results demonstrate that the main lottery ticket\nobservations remain relevant in this context. Codes available at\nhttps://github.com/VITA-Group/BERT-Tickets.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 19:35:39 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2020 20:10:29 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Chen", "Tianlong", ""], ["Frankle", "Jonathan", ""], ["Chang", "Shiyu", ""], ["Liu", "Sijia", ""], ["Zhang", "Yang", ""], ["Wang", "Zhangyang", ""], ["Carbin", "Michael", ""]]}, {"id": "2007.12248", "submitter": "Eric Chu", "authors": "Eric Chu, Deb Roy, Jacob Andreas", "title": "Are Visual Explanations Useful? A Case Study in Model-in-the-Loop\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a randomized controlled trial for a model-in-the-loop regression\ntask, with the goal of measuring the extent to which (1) good explanations of\nmodel predictions increase human accuracy, and (2) faulty explanations decrease\nhuman trust in the model. We study explanations based on visual saliency in an\nimage-based age prediction task for which humans and learned models are\nindividually capable but not highly proficient and frequently disagree. Our\nexperimental design separates model quality from explanation quality, and makes\nit possible to compare treatments involving a variety of explanations of\nvarying levels of quality. We find that presenting model predictions improves\nhuman accuracy. However, visual explanations of various kinds fail to\nsignificantly alter human accuracy or trust in the model - regardless of\nwhether explanations characterize an accurate model, an inaccurate one, or are\ngenerated randomly and independently of the input image. These findings suggest\nthe need for greater evaluation of explanations in downstream decision making\ntasks, better design-based tools for presenting explanations to users, and\nbetter approaches for generating explanations.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 20:39:40 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Chu", "Eric", ""], ["Roy", "Deb", ""], ["Andreas", "Jacob", ""]]}, {"id": "2007.12269", "submitter": "Muhammed Sit", "authors": "Muhammed Sit, Bekir Z. Demiray, Zhongrun Xiang, Gregory J. Ewing,\n  Yusuf Sermet and Ibrahim Demir", "title": "A Comprehensive Review of Deep Learning Applications in Hydrology and\n  Water Resources", "comments": "52 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The global volume of digital data is expected to reach 175 zettabytes by\n2025. The volume, variety, and velocity of water-related data are increasing\ndue to large-scale sensor networks and increased attention to topics such as\ndisaster response, water resources management, and climate change. Combined\nwith the growing availability of computational resources and popularity of deep\nlearning, these data are transformed into actionable and practical knowledge,\nrevolutionizing the water industry. In this article, a systematic review of\nliterature is conducted to identify existing research which incorporates deep\nlearning methods in the water sector, with regard to monitoring, management,\ngovernance and communication of water resources. The study provides a\ncomprehensive review of state-of-the-art deep learning approaches used in the\nwater industry for generation, prediction, enhancement, and classification\ntasks, and serves as a guide for how to utilize available deep learning methods\nfor future water resources challenges. Key issues and challenges in the\napplication of these techniques in the water domain are discussed, including\nthe ethics of these technologies for decision-making in water resources\nmanagement and governance. Finally, we provide recommendations and future\ndirections for the application of deep learning models in hydrology and water\nresources.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 16:57:17 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Sit", "Muhammed", ""], ["Demiray", "Bekir Z.", ""], ["Xiang", "Zhongrun", ""], ["Ewing", "Gregory J.", ""], ["Sermet", "Yusuf", ""], ["Demir", "Ibrahim", ""]]}, {"id": "2007.12285", "submitter": "Anthony David Blaom", "authors": "Anthony D. Blaom, Franz Kiraly, Thibaut Lienart, Yiannis Simillides,\n  Diego Arenas, Sebastian J. Vollmer", "title": "MLJ: A Julia package for composable machine learning", "comments": "Shortened version of previous version", "journal-ref": "Journal of Open Source Software, 2020, vol. 5(55), p. 2704", "doi": "10.21105/joss.02704", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MLJ (Machine Learing in Julia) is an open source software package providing a\ncommon interface for interacting with machine learning models written in Julia\nand other languages. It provides tools and meta-algorithms for selecting,\ntuning, evaluating, composing and comparing those models, with a focus on\nflexible model composition. In this design overview we detail chief novelties\nof the framework, together with the clear benefits of Julia over the dominant\nmulti-language alternatives.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 22:46:33 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 23:06:45 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Blaom", "Anthony D.", ""], ["Kiraly", "Franz", ""], ["Lienart", "Thibaut", ""], ["Simillides", "Yiannis", ""], ["Arenas", "Diego", ""], ["Vollmer", "Sebastian J.", ""]]}, {"id": "2007.12291", "submitter": "Sahin Lale", "authors": "Sahin Lale, Kamyar Azizzadenesheli, Babak Hassibi, Anima Anandkumar", "title": "Explore More and Improve Regret in Linear Quadratic Regulators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stabilizing the unknown dynamics of a control system and minimizing regret in\ncontrol of an unknown system are among the main goals in control theory and\nreinforcement learning. In this work, we pursue both these goals for adaptive\ncontrol of linear quadratic regulators (LQR). Prior works accomplish either one\nof these goals at the cost of the other one. The algorithms that are guaranteed\nto find a stabilizing controller suffer from high regret, whereas algorithms\nthat focus on achieving low regret assume the presence of a stabilizing\ncontroller at the early stages of agent-environment interaction. In the absence\nof such a stabilizing controller, at the early stages, the lack of reasonable\nmodel estimates needed for (i) strategic exploration and (ii) design of\ncontrollers that stabilize the system, results in regret that scales\nexponentially in the problem dimensions. We propose a framework for adaptive\ncontrol that exploits the characteristics of linear dynamical systems and\ndeploys additional exploration in the early stages of agent-environment\ninteraction to guarantee sooner design of stabilizing controllers. We show that\nfor the classes of controllable and stabilizable LQRs, where the latter is a\ngeneralization of prior work, these methods achieve\n$\\tilde{\\mathcal{O}}(\\sqrt{T})$ regret with a polynomial dependence in the\nproblem dimensions.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 23:06:40 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Lale", "Sahin", ""], ["Azizzadenesheli", "Kamyar", ""], ["Hassibi", "Babak", ""], ["Anandkumar", "Anima", ""]]}, {"id": "2007.12298", "submitter": "Jiwoong Im", "authors": "Daniel Jiwoong Im, Iljung Kwak, Kristin Branson", "title": "Evaluation metrics for behaviour modeling", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A primary difficulty with unsupervised discovery of structure in large data\nsets is a lack of quantitative evaluation criteria. In this work, we propose\nand investigate several metrics for evaluating and comparing generative models\nof behavior learned using imitation learning. Compared to the commonly-used\nmodel log-likelihood, these criteria look at longer temporal relationships in\nbehavior, are relevant if behavior has some properties that are inherently\nunpredictable, and highlight biases in the overall distribution of behaviors\nproduced by the model. Pointwise metrics compare real to model-predicted\ntrajectories given true past information. Distribution metrics compare\nstatistics of the model simulating behavior in open loop, and are inspired by\nhow experimental biologists evaluate the effects of manipulations on animal\nbehavior. We show that the proposed metrics correspond with biologists'\nintuitions about behavior, and allow us to evaluate models, understand their\nbiases, and enable us to propose new research directions.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 23:47:24 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Im", "Daniel Jiwoong", ""], ["Kwak", "Iljung", ""], ["Branson", "Kristin", ""]]}, {"id": "2007.12315", "submitter": "Daniel Brown", "authors": "Daniel S. Brown, Scott Niekum, Marek Petrik", "title": "Bayesian Robust Optimization for Imitation Learning", "comments": "In proceedings NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main challenges in imitation learning is determining what action\nan agent should take when outside the state distribution of the demonstrations.\nInverse reinforcement learning (IRL) can enable generalization to new states by\nlearning a parameterized reward function, but these approaches still face\nuncertainty over the true reward function and corresponding optimal policy.\nExisting safe imitation learning approaches based on IRL deal with this\nuncertainty using a maxmin framework that optimizes a policy under the\nassumption of an adversarial reward function, whereas risk-neutral IRL\napproaches either optimize a policy for the mean or MAP reward function. While\ncompletely ignoring risk can lead to overly aggressive and unsafe policies,\noptimizing in a fully adversarial sense is also problematic as it can lead to\noverly conservative policies that perform poorly in practice. To provide a\nbridge between these two extremes, we propose Bayesian Robust Optimization for\nImitation Learning (BROIL). BROIL leverages Bayesian reward function inference\nand a user specific risk tolerance to efficiently optimize a robust policy that\nbalances expected return and conditional value at risk. Our empirical results\nshow that BROIL provides a natural way to interpolate between return-maximizing\nand risk-minimizing behaviors and outperforms existing risk-sensitive and\nrisk-neutral inverse reinforcement learning algorithms. Code is available at\nhttps://github.com/dsbrown1331/broil.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 01:52:11 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 17:50:07 GMT"}, {"version": "v3", "created": "Sun, 8 Nov 2020 04:16:15 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Brown", "Daniel S.", ""], ["Niekum", "Scott", ""], ["Petrik", "Marek", ""]]}, {"id": "2007.12322", "submitter": "Tonghan Wang", "authors": "Yihan Wang, Beining Han, Tonghan Wang, Heng Dong, Chongjie Zhang", "title": "Off-Policy Multi-Agent Decomposed Policy Gradients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent policy gradient (MAPG) methods recently witness vigorous\nprogress. However, there is a significant performance discrepancy between MAPG\nmethods and state-of-the-art multi-agent value-based approaches. In this paper,\nwe investigate causes that hinder the performance of MAPG algorithms and\npresent a multi-agent decomposed policy gradient method (DOP). This method\nintroduces the idea of value function decomposition into the multi-agent\nactor-critic framework. Based on this idea, DOP supports efficient off-policy\nlearning and addresses the issue of centralized-decentralized mismatch and\ncredit assignment in both discrete and continuous action spaces. We formally\nshow that DOP critics have sufficient representational capability to guarantee\nconvergence. In addition, empirical evaluations on the StarCraft II\nmicromanagement benchmark and multi-agent particle environments demonstrate\nthat DOP significantly outperforms both state-of-the-art value-based and\npolicy-based multi-agent reinforcement learning algorithms. Demonstrative\nvideos are available at https://sites.google.com/view/dop-mapg/.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 02:21:55 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 08:07:44 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Wang", "Yihan", ""], ["Han", "Beining", ""], ["Wang", "Tonghan", ""], ["Dong", "Heng", ""], ["Zhang", "Chongjie", ""]]}, {"id": "2007.12325", "submitter": "Rami Mahdi", "authors": "Rami Mahdi", "title": "A Nonparametric Test of Dependence Based on Ensemble of Decision Trees", "comments": "35 pages, 6 figures, 1 table with 15 small figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a robust non-parametric measure of statistical dependence, or\ncorrelation, between two random variables is presented. The proposed\ncoefficient is a permutation-like statistic that quantifies how much the\nobserved sample S_n : {(X_i , Y_i), i = 1 . . . n} is discriminable from the\npermutated sample ^S_nn : {(X_i , Y_j), i, j = 1 . . . n}, where the two\nvariables are independent. The extent of discriminability is determined using\nthe predictions for the, interchangeable, leave-out sample from training an\naggregate of decision trees to discriminate between the two samples without\nmaterializing the permutated sample. The proposed coefficient is\ncomputationally efficient, interpretable, invariant to monotonic\ntransformations, and has a well-approximated distribution under independence.\nEmpirical results show the proposed method to have a high power for detecting\ncomplex relationships from noisy data.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 02:48:33 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Mahdi", "Rami", ""]]}, {"id": "2007.12335", "submitter": "Katie Everett", "authors": "Katie Everett, Ian Fischer", "title": "Cycles in Causal Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the causal learning setting, we wish to learn cause-and-effect\nrelationships between variables such that we can correctly infer the effect of\nan intervention. While the difference between a cyclic structure and an acyclic\nstructure may be just a single edge, cyclic causal structures have\nqualitatively different behavior under intervention: cycles cause feedback\nloops when the downstream effect of an intervention propagates back to the\nsource variable. We present three theoretical observations about probability\ndistributions with self-referential factorizations, i.e. distributions that\ncould be graphically represented with a cycle. First, we prove that\nself-referential distributions in two variables are, in fact, independent.\nSecond, we prove that self-referential distributions in N variables have zero\nmutual information. Lastly, we prove that self-referential distributions that\nfactorize in a cycle, also factorize as though the cycle were reversed. These\nresults suggest that cyclic causal dependence may exist even where\nobservational data suggest independence among variables. Methods based on\nestimating mutual information, or heuristics based on independent causal\nmechanisms, are likely to fail to learn cyclic casual structures. We encourage\nfuture work in causal learning that carefully considers cycles.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 03:58:02 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Everett", "Katie", ""], ["Fischer", "Ian", ""]]}, {"id": "2007.12336", "submitter": "Adnan Siraj Rakin", "authors": "Adnan Siraj Rakin, Zhezhi He, Jingtao Li, Fan Yao, Chaitali\n  Chakrabarti and Deliang Fan", "title": "T-BFA: Targeted Bit-Flip Adversarial Weight Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional Deep Neural Network (DNN) security is mostly related to the\nwell-known adversarial input example attack. Recently, another dimension of\nadversarial attack, namely, attack on DNN weight parameters, has been shown to\nbe very powerful. As a representative one, the Bit-Flip-based adversarial\nweight Attack (BFA) injects an extremely small amount of faults into weight\nparameters to hijack the executing DNN function. Prior works of BFA focus on\nun-targeted attack that can hack all inputs into a random output class by\nflipping a very small number of weight bits stored in computer memory. This\npaper proposes the first work of targeted BFA based (T-BFA) adversarial weight\nattack on DNNs, which can intentionally mislead selected inputs to a target\noutput class. The objective is achieved by identifying the weight bits that are\nhighly associated with classification of a targeted output through a\nclass-dependent weight bit ranking algorithm. Our proposed T-BFA performance is\nsuccessfully demonstrated on multiple DNN architectures for image\nclassification tasks. For example, by merely flipping 27 out of 88 million\nweight bits of ResNet-18, our T-BFA can misclassify all the images from 'Hen'\nclass into 'Goose' class (i.e., 100 % attack success rate) in ImageNet dataset,\nwhile maintaining 59.35 % validation accuracy. Moreover, we successfully\ndemonstrate our T-BFA attack in a real computer prototype system running DNN\ncomputation, with Ivy Bridge-based Intel i7 CPU and 8GB DDR3 memory.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 03:58:25 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 06:16:53 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2021 04:54:21 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Rakin", "Adnan Siraj", ""], ["He", "Zhezhi", ""], ["Li", "Jingtao", ""], ["Yao", "Fan", ""], ["Chakrabarti", "Chaitali", ""], ["Fan", "Deliang", ""]]}, {"id": "2007.12349", "submitter": "Zhuojian Xiao", "authors": "Zhuojian Xiao, Yunjiang jiang, Guoyu Tang, Lin Liu, Sulong Xu, Yun\n  Xiao, Weipeng Yan", "title": "Adversarial Mixture Of Experts with Category Hierarchy Soft Constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Product search is the most common way for people to satisfy their shopping\nneeds on e-commerce websites. Products are typically annotated with one of\nseveral broad categorical tags, such as \"Clothing\" or \"Electronics\", as well as\nfiner-grained categories like \"Refrigerator\" or \"TV\", both under \"Electronics\".\nThese tags are used to construct a hierarchy of query categories. Distributions\nof features such as price and brand popularity vary wildly across query\ncategories. In addition, feature importance for the purpose of CTR/CVR\npredictions differs from one category to another. In this work, we leverage the\nMixture of Expert (MoE) framework to learn a ranking model that specializes for\neach query category. In particular, our gate network relies solely on the\ncategory ids extracted from the user query. While classical MoE's pick expert\ntowers spontaneously for each input example, we explore two techniques to\nestablish more explicit and transparent connections between the experts and\nquery categories. To help differentiate experts on their domain specialties, we\nintroduce a form of adversarial regularization among the expert outputs,\nforcing them to disagree with one another. As a result, they tend to approach\neach prediction problem from different angles, rather than copying one another.\nThis is validated by a much stronger clustering effect of the gate output\nvectors under different categories. In addition, soft gating constraints based\non the categorical hierarchy are imposed to help similar products choose\nsimilar gate values. and make them more likely to share similar experts. This\nallows aggregation of training data among smaller sibling categories to\novercome data scarcity.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 04:47:40 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 13:43:14 GMT"}, {"version": "v3", "created": "Tue, 2 Mar 2021 13:32:37 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Xiao", "Zhuojian", ""], ["jiang", "Yunjiang", ""], ["Tang", "Guoyu", ""], ["Liu", "Lin", ""], ["Xu", "Sulong", ""], ["Xiao", "Yun", ""], ["Yan", "Weipeng", ""]]}, {"id": "2007.12354", "submitter": "Thanh Nguyen Tang", "authors": "Thanh Tang Nguyen, Sunil Gupta, Svetha Venkatesh", "title": "Distributional Reinforcement Learning via Moment Matching", "comments": "To appear in AAAI'21; code available at\n  https://github.com/thanhnguyentang/mmdrl", "journal-ref": "AAAI 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a set of probability distributions from\nthe empirical Bellman dynamics in distributional reinforcement learning (RL), a\nclass of state-of-the-art methods that estimate the distribution, as opposed to\nonly the expectation, of the total return. We formulate a method that learns a\nfinite set of statistics from each return distribution via neural networks, as\nin (Bellemare, Dabney, and Munos 2017; Dabney et al. 2018b). Existing\ndistributional RL methods however constrain the learned statistics to\n\\emph{predefined} functional forms of the return distribution which is both\nrestrictive in representation and difficult in maintaining the predefined\nstatistics. Instead, we learn \\emph{unrestricted} statistics, i.e.,\ndeterministic (pseudo-)samples, of the return distribution by leveraging a\ntechnique from hypothesis testing known as maximum mean discrepancy (MMD),\nwhich leads to a simpler objective amenable to backpropagation. Our method can\nbe interpreted as implicitly matching all orders of moments between a return\ndistribution and its Bellman target. We establish sufficient conditions for the\ncontraction of the distributional Bellman operator and provide finite-sample\nanalysis for the deterministic samples in distribution approximation.\nExperiments on the suite of Atari games show that our method outperforms the\nstandard distributional RL baselines and sets a new record in the Atari games\nfor non-distributed agents.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 05:18:17 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2020 06:43:28 GMT"}, {"version": "v3", "created": "Wed, 9 Dec 2020 00:38:36 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Nguyen", "Thanh Tang", ""], ["Gupta", "Sunil", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "2007.12355", "submitter": "Yiqin Yu", "authors": "Yiqin Yu, Xu Min, Shiwan Zhao, Jing Mei, Fei Wang, Dongsheng Li,\n  Kenney Ng, Shaochun Li", "title": "Dynamic Knowledge Distillation for Black-box Hypothesis Transfer\n  Learning", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real world applications like healthcare, it is usually difficult to build\na machine learning prediction model that works universally well across\ndifferent institutions. At the same time, the available model is often\nproprietary, i.e., neither the model parameter nor the data set used for model\ntraining is accessible. In consequence, leveraging the knowledge hidden in the\navailable model (aka. the hypothesis) and adapting it to a local data set\nbecomes extremely challenging. Motivated by this situation, in this paper we\naim to address such a specific case within the hypothesis transfer learning\nframework, in which 1) the source hypothesis is a black-box model and 2) the\nsource domain data is unavailable. In particular, we introduce a novel\nalgorithm called dynamic knowledge distillation for hypothesis transfer\nlearning (dkdHTL). In this method, we use knowledge distillation with\ninstance-wise weighting mechanism to adaptively transfer the \"dark\" knowledge\nfrom the source hypothesis to the target domain.The weighting coefficients of\nthe distillation loss and the standard loss are determined by the consistency\nbetween the predicted probability of the source hypothesis and the target\nground-truth label.Empirical results on both transfer learning benchmark\ndatasets and a healthcare dataset demonstrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 05:19:08 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 00:47:11 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Yu", "Yiqin", ""], ["Min", "Xu", ""], ["Zhao", "Shiwan", ""], ["Mei", "Jing", ""], ["Wang", "Fei", ""], ["Li", "Dongsheng", ""], ["Ng", "Kenney", ""], ["Li", "Shaochun", ""]]}, {"id": "2007.12364", "submitter": "Dana Lahat", "authors": "Dana Lahat, Yanbin Lang, Vincent Y. F. Tan, C\\'edric F\\'evotte", "title": "Positive Semidefinite Matrix Factorization: A Connection with Phase\n  Retrieval and Affine Rank Minimization", "comments": "18 pages (16 paper + 2 supplementary material), 9 figures, accepted\n  for publication in the IEEE Transactions on Signal Processing. This is a\n  revised version: there is a new additional PSDMF algorithm based on CGIHT,\n  more numerical experiments, and some background material moved to\n  Supplementary Material (pages 17 and 18 in this document). Supplementary\n  Material also contains some extra figures", "journal-ref": null, "doi": "10.1109/TSP.2021.3071293", "report-no": null, "categories": "eess.SP cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Positive semidefinite matrix factorization (PSDMF) expresses each entry of a\nnonnegative matrix as the inner product of two positive semidefinite (psd)\nmatrices. When all these psd matrices are constrained to be diagonal, this\nmodel is equivalent to nonnegative matrix factorization. Applications include\ncombinatorial optimization, quantum-based statistical models, and recommender\nsystems, among others. However, despite the increasing interest in PSDMF, only\na few PSDMF algorithms were proposed in the literature. In this work, we\nprovide a collection of tools for PSDMF, by showing that PSDMF algorithms can\nbe designed based on phase retrieval (PR) and affine rank minimization (ARM)\nalgorithms. This procedure allows a shortcut in designing new PSDMF algorithms,\nas it allows to leverage some of the useful numerical properties of existing PR\nand ARM methods to the PSDMF framework. Motivated by this idea, we introduce a\nnew family of PSDMF algorithms based on iterative hard thresholding (IHT). This\nfamily subsumes previously-proposed projected gradient PSDMF methods. We show\nthat there is high variability among PSDMF optimization problems that makes it\nbeneficial to try a number of methods based on different principles to tackle\ndifficult problems. In certain cases, our proposed methods are the only\nalgorithms able to find a solution. In certain other cases, they converge\nfaster. Our results support our claim that the PSDMF framework can inherit\ndesired numerical properties from PR and ARM algorithms, leading to more\nefficient PSDMF algorithms, and motivate further study of the links between\nthese models.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 06:10:19 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 21:26:06 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Lahat", "Dana", ""], ["Lang", "Yanbin", ""], ["Tan", "Vincent Y. F.", ""], ["F\u00e9votte", "C\u00e9dric", ""]]}, {"id": "2007.12371", "submitter": "Hans-Christian Ruiz Euler Dr.", "authors": "Hans-Christian Ruiz-Euler, Unai Alegre-Ibarra, Bram van de Ven, Hajo\n  Broersma, Peter A. Bobbert, Wilfred G. van der Wiel", "title": "Dopant Network Processing Units: Towards Efficient Neural-network\n  Emulators with High-capacity Nanoelectronic Nodes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.ET cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapidly growing computational demands of deep neural networks require\nnovel hardware designs. Recently, tunable nanoelectronic devices were developed\nbased on hopping electrons through a network of dopant atoms in silicon. These\n\"Dopant Network Processing Units\" (DNPUs) are highly energy-efficient and have\npotentially very high throughput. By adapting the control voltages applied to\nits terminals, a single DNPU can solve a variety of linearly non-separable\nclassification problems. However, using a single device has limitations due to\nthe implicit single-node architecture. This paper presents a promising novel\napproach to neural information processing by introducing DNPUs as high-capacity\nneurons and moving from a single to a multi-neuron framework. By implementing\nand testing a small multi-DNPU classifier in hardware, we show that\nfeed-forward DNPU networks improve the performance of a single DNPU from 77% to\n94% test accuracy on a binary classification task with concentric classes on a\nplane. Furthermore, motivated by the integration of DNPUs with memristor\narrays, we study the potential of using DNPUs in combination with linear\nlayers. We show by simulation that a single-layer MNIST classifier with only 10\nDNPUs achieves over 96% test accuracy. Our results pave the road towards\nhardware neural-network emulators that offer atomic-scale information\nprocessing with low latency and energy consumption.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 06:35:44 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Ruiz-Euler", "Hans-Christian", ""], ["Alegre-Ibarra", "Unai", ""], ["van de Ven", "Bram", ""], ["Broersma", "Hajo", ""], ["Bobbert", "Peter A.", ""], ["van der Wiel", "Wilfred G.", ""]]}, {"id": "2007.12375", "submitter": "Mei Wang", "authors": "Mei Wang, Jianwen Su, Haiqin Lu", "title": "Impact of Medical Data Imprecision on Learning Results", "comments": "2020 KDD Workshop on Applied Data Science for Healthcare", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Test data measured by medical instruments often carry imprecise ranges that\ninclude the true values. The latter are not obtainable in virtually all cases.\nMost learning algorithms, however, carry out arithmetical calculations that are\nsubject to uncertain influence in both the learning process to obtain models\nand applications of the learned models in, e.g. prediction. In this paper, we\ninitiate a study on the impact of imprecision on prediction results in a\nhealthcare application where a pre-trained model is used to predict future\nstate of hyperthyroidism for patients. We formulate a model for data\nimprecisions. Using parameters to control the degree of imprecision, imprecise\nsamples for comparison experiments can be generated using this model. Further,\na group of measures are defined to evaluate the different impacts\nquantitatively. More specifically, the statistics to measure the inconsistent\nprediction for individual patients are defined. We perform experimental\nevaluations to compare prediction results based on the data from the original\ndataset and the corresponding ones generated from the proposed precision model\nusing the long-short-term memories (LSTM) network. The results against a real\nworld hyperthyroidism dataset provide insights into how small imprecisions can\ncause large ranges of predicted results, which could cause mis-labeling and\ninappropriate actions (treatments or no treatments) for individual patients.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 06:54:57 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Wang", "Mei", ""], ["Su", "Jianwen", ""], ["Lu", "Haiqin", ""]]}, {"id": "2007.12377", "submitter": "Alexandre Capone", "authors": "Alexandre Capone, Sandra Hirche", "title": "Anticipating the Long-Term Effect of Online Learning in Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Control schemes that learn using measurement data collected online are\nincreasingly promising for the control of complex and uncertain systems.\nHowever, in most approaches of this kind, learning is viewed as a side effect\nthat passively improves control performance, e.g., by updating a model of the\nsystem dynamics. Determining how improvements in control performance due to\nlearning can be actively exploited in the control synthesis is still an open\nresearch question. In this paper, we present AntLer, a design algorithm for\nlearning-based control laws that anticipates learning, i.e., that takes the\nimpact of future learning in uncertain dynamic settings explicitly into\naccount. AntLer expresses system uncertainty using a non-parametric\nprobabilistic model. Given a cost function that measures control performance,\nAntLer chooses the control parameters such that the expected cost of the\nclosed-loop system is minimized approximately. We show that AntLer approximates\nan optimal solution arbitrarily accurately with probability one. Furthermore,\nwe apply AntLer to a nonlinear system, which yields better results compared to\nthe case where learning is not anticipated.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 07:00:14 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Capone", "Alexandre", ""], ["Hirche", "Sandra", ""]]}, {"id": "2007.12401", "submitter": "Kuang-Huei Lee", "authors": "Kuang-Huei Lee, Ian Fischer, Anthony Liu, Yijie Guo, Honglak Lee, John\n  Canny, Sergio Guadarrama", "title": "Predictive Information Accelerates Learning in RL", "comments": "To appear at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT cs.RO math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Predictive Information is the mutual information between the past and the\nfuture, I(X_past; X_future). We hypothesize that capturing the predictive\ninformation is useful in RL, since the ability to model what will happen next\nis necessary for success on many tasks. To test our hypothesis, we train Soft\nActor-Critic (SAC) agents from pixels with an auxiliary task that learns a\ncompressed representation of the predictive information of the RL environment\ndynamics using a contrastive version of the Conditional Entropy Bottleneck\n(CEB) objective. We refer to these as Predictive Information SAC (PI-SAC)\nagents. We show that PI-SAC agents can substantially improve sample efficiency\nover challenging baselines on tasks from the DM Control suite of continuous\ncontrol environments. We evaluate PI-SAC agents by comparing against\nuncompressed PI-SAC agents, other compressed and uncompressed agents, and SAC\nagents directly trained from pixels. Our implementation is given on GitHub.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 08:14:41 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 00:27:00 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Lee", "Kuang-Huei", ""], ["Fischer", "Ian", ""], ["Liu", "Anthony", ""], ["Guo", "Yijie", ""], ["Lee", "Honglak", ""], ["Canny", "John", ""], ["Guadarrama", "Sergio", ""]]}, {"id": "2007.12411", "submitter": "Yingzhen Li", "authors": "Chaochao Lu, Richard E. Turner, Yingzhen Li, Nate Kushman", "title": "Interpreting Spatially Infinite Generative Models", "comments": "ICML 2020 workshop on Human Interpretability in Machine Learning (WHI\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional deep generative models of images and other spatial modalities can\nonly generate fixed sized outputs. The generated images have exactly the same\nresolution as the training images, which is dictated by the number of layers in\nthe underlying neural network. Recent work has shown, however, that feeding\nspatial noise vectors into a fully convolutional neural network enables both\ngeneration of arbitrary resolution output images as well as training on\narbitrary resolution training images. While this work has provided impressive\nempirical results, little theoretical interpretation was provided to explain\nthe underlying generative process. In this paper we provide a firm theoretical\ninterpretation for infinite spatial generation, by drawing connections to\nspatial stochastic processes. We use the resulting intuition to improve upon\nexisting spatially infinite generative models to enable more efficient training\nthrough a model that we call an infinite generative adversarial network, or\n$\\infty$-GAN. Experiments on world map generation, panoramic images and texture\nsynthesis verify the ability of $\\infty$-GAN to efficiently generate images of\narbitrary size.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 09:00:41 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Lu", "Chaochao", ""], ["Turner", "Richard E.", ""], ["Li", "Yingzhen", ""], ["Kushman", "Nate", ""]]}, {"id": "2007.12415", "submitter": "Xi Li", "authors": "Hanbin Zhao, Hao Zeng, Xin Qin, Yongjian Fu, Hui Wang, Bourahla Omar,\n  and Xi Li", "title": "What and Where: Learn to Plug Adapters via NAS for Multi-Domain Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an important and challenging problem, multi-domain learning (MDL)\ntypically seeks for a set of effective lightweight domain-specific adapter\nmodules plugged into a common domain-agnostic network. Usually, existing ways\nof adapter plugging and structure design are handcrafted and fixed for all\ndomains before model learning, resulting in the learning inflexibility and\ncomputational intensiveness. With this motivation, we propose to learn a\ndata-driven adapter plugging strategy with Neural Architecture Search (NAS),\nwhich automatically determines where to plug for those adapter modules.\nFurthermore, we propose a NAS-adapter module for adapter structure design in a\nNAS-driven learning scheme, which automatically discovers effective adapter\nmodule structures for different domains. Experimental results demonstrate the\neffectiveness of our MDL model against existing approaches under the conditions\nof comparable performance.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 09:12:37 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 13:54:27 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Zhao", "Hanbin", ""], ["Zeng", "Hao", ""], ["Qin", "Xin", ""], ["Fu", "Yongjian", ""], ["Wang", "Hui", ""], ["Omar", "Bourahla", ""], ["Li", "Xi", ""]]}, {"id": "2007.12420", "submitter": "Lorena Romero-Medrano", "authors": "Lorena Romero-Medrano, Pablo Moreno-Mu\\~noz and Antonio\n  Art\\'es-Rodr\\'iguez", "title": "Multinomial Sampling for Hierarchical Change-Point Detection", "comments": "International Workshop on Machine Learning for Signal Processing\n  (MLSP), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian change-point detection, together with latent variable models, allows\nto perform segmentation over high-dimensional time-series. We assume that\nchange-points lie on a lower-dimensional manifold where we aim to infer subsets\nof discrete latent variables. For this model, full inference is computationally\nunfeasible and pseudo-observations based on point-estimates are used instead.\nHowever, if estimation is not certain enough, change-point detection gets\naffected. To circumvent this problem, we propose a multinomial sampling\nmethodology that improves the detection rate and reduces the delay while\nkeeping complexity stable and inference analytically tractable. Our experiments\nshow results that outperform the baseline method and we also provide an example\noriented to a human behavior study.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 09:18:17 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 17:45:26 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Romero-Medrano", "Lorena", ""], ["Moreno-Mu\u00f1oz", "Pablo", ""], ["Art\u00e9s-Rodr\u00edguez", "Antonio", ""]]}, {"id": "2007.12446", "submitter": "Yunzhen Feng", "authors": "Yunzhen Feng, Runtian Zhai, Di He, Liwei Wang, Bin Dong", "title": "Transferred Discrepancy: Quantifying the Difference Between\n  Representations", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding what information neural networks capture is an essential\nproblem in deep learning, and studying whether different models capture similar\nfeatures is an initial step to achieve this goal. Previous works sought to\ndefine metrics over the feature matrices to measure the difference between two\nmodels. However, different metrics sometimes lead to contradictory conclusions,\nand there has been no consensus on which metric is suitable to use in practice.\nIn this work, we propose a novel metric that goes beyond previous approaches.\nRecall that one of the most practical scenarios of using the learned\nrepresentations is to apply them to downstream tasks. We argue that we should\ndesign the metric based on a similar principle. For that, we introduce the\ntransferred discrepancy (TD), a new metric that defines the difference between\ntwo representations based on their downstream-task performance. Through an\nasymptotic analysis, we show how TD correlates with downstream tasks and the\nnecessity to define metrics in such a task-dependent fashion. In particular, we\nalso show that under specific conditions, the TD metric is closely related to\nprevious metrics. Our experiments show that TD can provide fine-grained\ninformation for varied downstream tasks, and for the models trained from\ndifferent initializations, the learned features are not the same in terms of\ndownstream-task predictions. We find that TD may also be used to evaluate the\neffectiveness of different training strategies. For example, we demonstrate\nthat the models trained with proper data augmentations that improve the\ngeneralization capture more similar features in terms of TD, while those with\ndata augmentations that hurt the generalization will not. This suggests a\ntraining strategy that leads to more robust representation also trains models\nthat generalize better.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 10:59:11 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Feng", "Yunzhen", ""], ["Zhai", "Runtian", ""], ["He", "Di", ""], ["Wang", "Liwei", ""], ["Dong", "Bin", ""]]}, {"id": "2007.12463", "submitter": "Gy\\\"orgy Kov\\'acs", "authors": "Attila Fazekas and Gy\\\"orgy Kov\\'acs", "title": "Approximately Optimal Binning for the Piecewise Constant Approximation\n  of the Normalized Unexplained Variance (nUV) Dissimilarity Measure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently introduced Matching by Tone Mapping (MTM) dissimilarity measure\nenables template matching under smooth non-linear distortions and also has a\nwell-established mathematical background. MTM operates by binning the template,\nbut the ideal binning for a particular problem is an open question. By pointing\nout an important analogy between the well known mutual information (MI) and\nMTM, we introduce the term \"normalized unexplained variance\" (nUV) for MTM to\nemphasize its relevance and applicability beyond image processing. Then, we\nprovide theoretical results on the optimal binning technique for the nUV\nmeasure and propose algorithms to find approximate solutions. The theoretical\nfindings are supported by numerical experiments. Using the proposed techniques\nfor binning shows 4-13% increase in terms of AUC scores with statistical\nsignificance, enabling us to conclude that the proposed binning techniques have\nthe potential to improve the performance of the nUV measure in real\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 11:55:28 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Fazekas", "Attila", ""], ["Kov\u00e1cs", "Gy\u00f6rgy", ""]]}, {"id": "2007.12475", "submitter": "Amir Mosavi Prof", "authors": "Mostafa Emadi, Ruhollah Taghizadeh-Mehrjardi, Ali Cherati, Majid\n  Danesh, Amir Mosavi, Thomas Scholten", "title": "Predicting and Mapping of Soil Organic Carbon Using Machine Learning\n  Algorithms in Northern Iran", "comments": "30pages, 9 figures", "journal-ref": "Remote Sens. 2020, 12, 2234", "doi": "10.3390/rs12142234", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimation of the soil organic carbon content is of utmost importance in\nunderstanding the chemical, physical, and biological functions of the soil.\nThis study proposes machine learning algorithms of support vector machines,\nartificial neural networks, regression tree, random forest, extreme gradient\nboosting, and conventional deep neural network for advancing prediction models\nof SOC. Models are trained with 1879 composite surface soil samples, and 105\nauxiliary data as predictors. The genetic algorithm is used as a feature\nselection approach to identify effective variables. The results indicate that\nprecipitation is the most important predictor driving 15 percent of SOC spatial\nvariability followed by the normalized difference vegetation index, day\ntemperature index of moderate resolution imaging spectroradiometer,\nmultiresolution valley bottom flatness and land use, respectively. Based on 10\nfold cross validation, the DNN model reported as a superior algorithm with the\nlowest prediction error and uncertainty. In terms of accuracy, DNN yielded a\nmean absolute error of 59 percent, a root mean squared error of 75 percent, a\ncoefficient of determination of 0.65, and Lins concordance correlation\ncoefficient of 0.83. The SOC content was the highest in udic soil moisture\nregime class with mean values of 4 percent, followed by the aquic and xeric\nclasses, respectively. Soils in dense forestlands had the highest SOC contents,\nwhereas soils of younger geological age and alluvial fans had lower SOC. The\nproposed DNN is a promising algorithm for handling large numbers of auxiliary\ndata at a province scale, and due to its flexible structure and the ability to\nextract more information from the auxiliary data surrounding the sampled\nobservations, it had high accuracy for the prediction of the SOC baseline map\nand minimal uncertainty.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 08:23:24 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Emadi", "Mostafa", ""], ["Taghizadeh-Mehrjardi", "Ruhollah", ""], ["Cherati", "Ali", ""], ["Danesh", "Majid", ""], ["Mosavi", "Amir", ""], ["Scholten", "Thomas", ""]]}, {"id": "2007.12499", "submitter": "Aditya Shrivastava", "authors": "Aditya Shrivastava", "title": "Adma: A Flexible Loss Function for Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Highly increased interest in Artificial Neural Networks (ANNs) have resulted\nin impressively wide-ranging improvements in its structure. In this work, we\ncome up with the idea that instead of static plugins that the currently\navailable loss functions are, they should by default be flexible in nature. A\nflexible loss function can be a more insightful navigator for neural networks\nleading to higher convergence rates and therefore reaching the optimum accuracy\nmore quickly. The insights to help decide the degree of flexibility can be\nderived from the complexity of ANNs, the data distribution, selection of\nhyper-parameters and so on. In the wake of this, we introduce a novel flexible\nloss function for neural networks. The function is shown to characterize a\nrange of fundamentally unique properties from which, much of the properties of\nother loss functions are only a subset and varying the flexibility parameter in\nthe function allows it to emulate the loss curves and the learning behavior of\nprevalent static loss functions. The extensive experimentation performed with\nthe loss function demonstrates that it is able to give state-of-the-art\nperformance on selected data sets. Thus, in all the idea of flexibility itself\nand the proposed function built upon it carry the potential to open to a new\ninteresting chapter in deep learning research.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 02:41:09 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Shrivastava", "Aditya", ""]]}, {"id": "2007.12509", "submitter": "Yunhao Tang", "authors": "Jean-Bastien Grill, Florent Altch\\'e, Yunhao Tang, Thomas Hubert,\n  Michal Valko, Ioannis Antonoglou, R\\'emi Munos", "title": "Monte-Carlo Tree Search as Regularized Policy Optimization", "comments": "Accepted to International Conference on Machine Learning (ICML), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of Monte-Carlo tree search (MCTS) with deep reinforcement\nlearning has led to significant advances in artificial intelligence. However,\nAlphaZero, the current state-of-the-art MCTS algorithm, still relies on\nhandcrafted heuristics that are only partially understood. In this paper, we\nshow that AlphaZero's search heuristics, along with other common ones such as\nUCT, are an approximation to the solution of a specific regularized policy\noptimization problem. With this insight, we propose a variant of AlphaZero\nwhich uses the exact solution to this policy optimization problem, and show\nexperimentally that it reliably outperforms the original algorithm in multiple\ndomains.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 13:01:34 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Grill", "Jean-Bastien", ""], ["Altch\u00e9", "Florent", ""], ["Tang", "Yunhao", ""], ["Hubert", "Thomas", ""], ["Valko", "Michal", ""], ["Antonoglou", "Ioannis", ""], ["Munos", "R\u00e9mi", ""]]}, {"id": "2007.12528", "submitter": "Luigi Malag\\`o", "authors": "Alexandra-Ioana Albu, Alina Enescu and Luigi Malag\\`o", "title": "Improved Slice-wise Tumour Detection in Brain MRIs by Computing\n  Dissimilarities between Latent Representations", "comments": "In 2020 KDD Workshop on Applied Data Science for Healthcare, August\n  24, 2020, San Diego, CA, USA. ACM, New York, NY, USA, 4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection for Magnetic Resonance Images (MRIs) can be solved with\nunsupervised methods by learning the distribution of healthy images and\nidentifying anomalies as outliers. In presence of an additional dataset of\nunlabelled data containing also anomalies, the task can be framed as a\nsemi-supervised task with negative and unlabelled sample points. Recently, in\nAlbu et al., 2020, we have proposed a slice-wise semi-supervised method for\ntumour detection based on the computation of a dissimilarity function in the\nlatent space of a Variational AutoEncoder, trained on unlabelled data. The\ndissimilarity is computed between the encoding of the image and the encoding of\nits reconstruction obtained through a different autoencoder trained only on\nhealthy images. In this paper we present novel and improved results for our\nmethod, obtained by training the Variational AutoEncoders on a subset of the\nHCP and BRATS-2018 datasets and testing on the remaining individuals. We show\nthat by training the models on higher resolution images and by improving the\nquality of the reconstructions, we obtain results which are comparable with\ndifferent baselines, which employ a single VAE trained on healthy individuals.\nAs expected, the performance of our method increases with the size of the\nthreshold used to determine the presence of an anomaly.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 14:02:09 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Albu", "Alexandra-Ioana", ""], ["Enescu", "Alina", ""], ["Malag\u00f2", "Luigi", ""]]}, {"id": "2007.12582", "submitter": "Wouter Verbeke", "authors": "Wouter Verbeke, Diego Olaya, Jeroen Berrevoets, Sam Verboven,\n  Sebasti\\'an Maldonado", "title": "The foundations of cost-sensitive causal classification", "comments": "New version: overall language edit - switch in notation (class 0:\n  negatives, class 1: positive) - typos corrected - proofs moved to appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification is a well-studied machine learning task which concerns the\nassignment of instances to a set of outcomes. Classification models support the\noptimization of managerial decision-making across a variety of operational\nbusiness processes. For instance, customer churn prediction models are adopted\nto increase the efficiency of retention campaigns by optimizing the selection\nof customers that are to be targeted. Cost-sensitive and causal classification\nmethods have independently been proposed to improve the performance of\nclassification models. The former considers the benefits and costs of correct\nand incorrect classifications, such as the benefit of a retained customer,\nwhereas the latter estimates the causal effect of an action, such as a\nretention campaign, on the outcome of interest. This study integrates\ncost-sensitive and causal classification by elaborating a unifying evaluation\nframework. The framework encompasses a range of existing and novel performance\nmeasures for evaluating both causal and conventional classification models in a\ncost-sensitive as well as a cost-insensitive manner. We proof that conventional\nclassification is a specific case of causal classification in terms of a range\nof performance measures when the number of actions is equal to one. The\nframework is shown to instantiate to application-specific cost-sensitive\nperformance measures that have been recently proposed for evaluating customer\nretention and response uplift models, and allows to maximize profitability when\nadopting a causal classification model for optimizing decision-making. The\nproposed framework paves the way toward the development of cost-sensitive\ncausal learning methods and opens a range of opportunities for improving\ndata-driven business decision-making.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 15:33:48 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 10:47:01 GMT"}, {"version": "v3", "created": "Wed, 19 Aug 2020 10:01:09 GMT"}, {"version": "v4", "created": "Wed, 27 Jan 2021 09:16:56 GMT"}, {"version": "v5", "created": "Tue, 20 Apr 2021 06:50:43 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Verbeke", "Wouter", ""], ["Olaya", "Diego", ""], ["Berrevoets", "Jeroen", ""], ["Verboven", "Sam", ""], ["Maldonado", "Sebasti\u00e1n", ""]]}, {"id": "2007.12618", "submitter": "Dirk van der Hoeven", "authors": "Dirk van der Hoeven", "title": "Exploiting the Surrogate Gap in Online Multiclass Classification", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Gaptron, a randomized first-order algorithm for online multiclass\nclassification. In the full information setting we show expected mistake bounds\nwith respect to the logistic loss, hinge loss, and the smooth hinge loss with\nconstant regret, where the expectation is with respect to the learner's\nrandomness. In the bandit classification setting we show that Gaptron is the\nfirst linear time algorithm with $O(K\\sqrt{T})$ expected regret, where $K$ is\nthe number of classes. Additionally, the expected mistake bound of Gaptron does\nnot depend on the dimension of the feature vector, contrary to previous\nalgorithms with $O(K\\sqrt{T})$ regret in the bandit classification setting. We\npresent a new proof technique that exploits the gap between the zero-one loss\nand surrogate losses rather than exploiting properties such as exp-concavity or\nmixability, which are traditionally used to prove logarithmic or constant\nregret bounds.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 16:19:02 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 11:51:23 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["van der Hoeven", "Dirk", ""]]}, {"id": "2007.12652", "submitter": "Emir Demirovi\\'c", "authors": "Emir Demirovi\\'c, Anna Lukina, Emmanuel Hebrard, Jeffrey Chan, James\n  Bailey, Christopher Leckie, Kotagiri Ramamohanarao, Peter J. Stuckey", "title": "MurTree: Optimal Classification Trees via Dynamic Programming and Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision tree learning is a widely used approach in machine learning,\nfavoured in applications that require concise and interpretable models.\nHeuristic methods are traditionally used to quickly produce models with\nreasonably high accuracy. A commonly criticised point, however, is that the\nresulting trees may not necessarily be the best representation of the data in\nterms of accuracy and size. In recent years, this motivated the development of\noptimal classification tree algorithms that globally optimise the decision tree\nin contrast to heuristic methods that perform a sequence of locally optimal\ndecisions. We follow this line of work and provide a novel algorithm for\nlearning optimal classification trees based on dynamic programming and search.\nOur algorithm supports constraints on the depth of the tree and number of\nnodes. The success of our approach is attributed to a series of specialised\ntechniques that exploit properties unique to classification trees. Whereas\nalgorithms for optimal classification trees have traditionally been plagued by\nhigh runtimes and limited scalability, we show in a detailed experimental study\nthat our approach uses only a fraction of the time required by the\nstate-of-the-art and can handle datasets with tens of thousands of instances,\nproviding several orders of magnitude improvements and notably contributing\ntowards the practical realisation of optimal decision trees.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 17:06:55 GMT"}, {"version": "v2", "created": "Sun, 9 May 2021 16:46:36 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Demirovi\u0107", "Emir", ""], ["Lukina", "Anna", ""], ["Hebrard", "Emmanuel", ""], ["Chan", "Jeffrey", ""], ["Bailey", "James", ""], ["Leckie", "Christopher", ""], ["Ramamohanarao", "Kotagiri", ""], ["Stuckey", "Peter J.", ""]]}, {"id": "2007.12669", "submitter": "Benjamin Priest", "authors": "Benjamin W. Priest, Alec Dunton, Geoffrey Sanders", "title": "Scaling Graph Clustering with Distributed Sketches", "comments": "9 pages, submitted to IEEE HPEC Graph Challenge 2020, comments\n  welcome", "journal-ref": null, "doi": null, "report-no": "LLNL-CONF-812693", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unsupervised learning of community structure, in particular the\npartitioning vertices into clusters or communities, is a canonical and\nwell-studied problem in exploratory graph analysis. However, like most graph\nanalyses the introduction of immense scale presents challenges to traditional\nmethods. Spectral clustering in distributed memory, for example, requires\nhundreds of expensive bulk-synchronous communication rounds to compute an\nembedding of vertices to a few eigenvectors of a graph associated matrix.\nFurthermore, the whole computation may need to be repeated if the underlying\ngraph changes some low percentage of edge updates. We present a method inspired\nby spectral clustering where we instead use matrix sketches derived from random\ndimension-reducing projections. We show that our method produces embeddings\nthat yield performant clustering results given a fully-dynamic stochastic block\nmodel stream using both the fast Johnson-Lindenstrauss and CountSketch\ntransforms. We also discuss the effects of stochastic block model parameters\nupon the required dimensionality of the subsequent embeddings, and show how\nrandom projections could significantly improve the performance of graph\nclustering in distributed memory.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 17:38:04 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Priest", "Benjamin W.", ""], ["Dunton", "Alec", ""], ["Sanders", "Geoffrey", ""]]}, {"id": "2007.12671", "submitter": "Pierre Bayle", "authors": "Pierre Bayle, Alexandre Bayle, Lucas Janson, Lester Mackey", "title": "Cross-validation Confidence Intervals for Test Error", "comments": "34th Conference on Neural Information Processing Systems (NeurIPS\n  2020); 40 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work develops central limit theorems for cross-validation and consistent\nestimators of its asymptotic variance under weak stability conditions on the\nlearning algorithm. Together, these results provide practical,\nasymptotically-exact confidence intervals for $k$-fold test error and valid,\npowerful hypothesis tests of whether one learning algorithm has smaller\n$k$-fold test error than another. These results are also the first of their\nkind for the popular choice of leave-one-out cross-validation. In our real-data\nexperiments with diverse learning algorithms, the resulting intervals and tests\noutperform the most popular alternative methods from the literature.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 17:40:06 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 17:24:26 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Bayle", "Pierre", ""], ["Bayle", "Alexandre", ""], ["Janson", "Lucas", ""], ["Mackey", "Lester", ""]]}, {"id": "2007.12678", "submitter": "Shengpu Tang", "authors": "Shengpu Tang, Aditya Modi, Michael W. Sjoding, Jenna Wiens", "title": "Clinician-in-the-Loop Decision Making: Reinforcement Learning with\n  Near-Optimal Set-Valued Policies", "comments": "ICML 2020. Code available at\n  https://github.com/shengpu1126/RL-Set-Valued-Policy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard reinforcement learning (RL) aims to find an optimal policy that\nidentifies the best action for each state. However, in healthcare settings,\nmany actions may be near-equivalent with respect to the reward (e.g.,\nsurvival). We consider an alternative objective -- learning set-valued policies\nto capture near-equivalent actions that lead to similar cumulative rewards. We\npropose a model-free algorithm based on temporal difference learning and a\nnear-greedy heuristic for action selection. We analyze the theoretical\nproperties of the proposed algorithm, providing optimality guarantees and\ndemonstrate our approach on simulated environments and a real clinical task.\nEmpirically, the proposed algorithm exhibits good convergence properties and\ndiscovers meaningful near-equivalent actions. Our work provides theoretical, as\nwell as practical, foundations for clinician/human-in-the-loop decision making,\nin which humans (e.g., clinicians, patients) can incorporate additional\nknowledge (e.g., side effects, patient preference) when selecting among\nnear-equivalent actions.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 17:50:58 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Tang", "Shengpu", ""], ["Modi", "Aditya", ""], ["Sjoding", "Michael W.", ""], ["Wiens", "Jenna", ""]]}, {"id": "2007.12702", "submitter": "Brandon Stewart", "authors": "Justin Grimmer, Dean Knox and Brandon M. Stewart", "title": "Na\\\"ive regression requires weaker assumptions than factor models to\n  adjust for multiple cause confounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The empirical practice of using factor models to adjust for shared,\nunobserved confounders, $\\mathbf{Z}$, in observational settings with multiple\ntreatments, $\\mathbf{A}$, is widespread in fields including genetics, networks,\nmedicine, and politics. Wang and Blei (2019, WB) formalizes these procedures\nand develops the \"deconfounder,\" a causal inference method using factor models\nof $\\mathbf{A}$ to estimate \"substitute confounders,\" $\\hat{\\mathbf{Z}}$, then\nestimating treatment effects by regressing the outcome, $\\mathbf{Y}$, on part\nof $\\mathbf{A}$ while adjusting for $\\hat{\\mathbf{Z}}$. WB claim the\ndeconfounder is unbiased when there are no single-cause confounders and\n$\\hat{\\mathbf{Z}}$ is \"pinpointed.\" We clarify pinpointing requires each\nconfounder to affect infinitely many treatments. We prove under these\nassumptions, a na\\\"ive semiparametric regression of $\\mathbf{Y}$ on\n$\\mathbf{A}$ is asymptotically unbiased. Deconfounder variants nesting this\nregression are therefore also asymptotically unbiased, but variants using\n$\\hat{\\mathbf{Z}}$ and subsets of causes require further untestable\nassumptions. We replicate every deconfounder analysis with available data and\nfind it fails to consistently outperform na\\\"ive regression. In practice, the\ndeconfounder produces implausible estimates in WB's case study to movie\nearnings: estimates suggest comic author Stan Lee's cameo appearances causally\ncontributed \\$15.5 billion, most of Marvel movie revenue. We conclude neither\napproach is a viable substitute for careful research design in real-world\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 18:00:02 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Grimmer", "Justin", ""], ["Knox", "Dean", ""], ["Stewart", "Brandon M.", ""]]}, {"id": "2007.12723", "submitter": "Thomas Bohnstingl", "authors": "Thomas Bohnstingl, Stanis{\\l}aw Wo\\'zniak, Wolfgang Maass, Angeliki\n  Pantazi and Evangelos Eleftheriou", "title": "Online Spatio-Temporal Learning in Deep Neural Networks", "comments": "Main manuscript: 9 pages, 3 figures, 1 table, Supplementary notes: 13\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological neural networks are equipped with an inherent capability to\ncontinuously adapt through online learning. This aspect remains in stark\ncontrast to learning with error backpropagation through time (BPTT) applied to\nrecurrent neural networks (RNNs), or recently to biologically-inspired spiking\nneural networks (SNNs). BPTT involves offline computation of the gradients due\nto the requirement to unroll the network through time. Online learning has\nrecently regained the attention of the research community, focusing either on\napproaches that approximate BPTT or on biologically-plausible schemes applied\nto SNNs. Here we present an alternative perspective that is based on a clear\nseparation of spatial and temporal gradient components. Combined with insights\nfrom biology, we derive from first principles a novel online learning algorithm\nfor deep SNNs, called online spatio-temporal learning (OSTL). For shallow\nnetworks, OSTL is gradient-equivalent to BPTT enabling for the first time\nonline training of SNNs with BPTT-equivalent gradients. In addition, the\nproposed formulation unveils a class of SNN architectures trainable online at\nlow time complexity. Moreover, we extend OSTL to a generic form, applicable to\na wide range of network architectures, including networks comprising long\nshort-term memory (LSTM) and gated recurrent units (GRU). We demonstrate the\noperation of our algorithm on various tasks from language modelling to speech\nrecognition and obtain results on par with the BPTT baselines. The proposed\nalgorithm provides a framework for developing succinct and efficient online\ntraining approaches for SNNs and in general deep RNNs.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 18:10:18 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 12:54:20 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Bohnstingl", "Thomas", ""], ["Wo\u017aniak", "Stanis\u0142aw", ""], ["Maass", "Wolfgang", ""], ["Pantazi", "Angeliki", ""], ["Eleftheriou", "Evangelos", ""]]}, {"id": "2007.12749", "submitter": "Xiaotong Liu", "authors": "Hong Xuan, Abby Stylianou, Xiaotong Liu, Robert Pless", "title": "Hard negative examples are hard, but useful", "comments": "CV, Triplet loss, Image embedding, 14 pages, 9 figures, ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Triplet loss is an extremely common approach to distance metric learning.\nRepresentations of images from the same class are optimized to be mapped closer\ntogether in an embedding space than representations of images from different\nclasses. Much work on triplet losses focuses on selecting the most useful\ntriplets of images to consider, with strategies that select dissimilar examples\nfrom the same class or similar examples from different classes. The consensus\nof previous research is that optimizing with the \\textit{hardest} negative\nexamples leads to bad training behavior. That's a problem -- these hardest\nnegatives are literally the cases where the distance metric fails to capture\nsemantic similarity. In this paper, we characterize the space of triplets and\nderive why hard negatives make triplet loss training fail. We offer a simple\nfix to the loss function and show that, with this fix, optimizing with hard\nnegative examples becomes feasible. This leads to more generalizable features,\nand image retrieval results that outperform state of the art for datasets with\nhigh intra-class variance.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 19:34:58 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 22:40:51 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Xuan", "Hong", ""], ["Stylianou", "Abby", ""], ["Liu", "Xiaotong", ""], ["Pless", "Robert", ""]]}, {"id": "2007.12769", "submitter": "Weijia Zhang", "authors": "Weijia Zhang, Jiuyong Li, Lin Liu", "title": "A unified survey of treatment effect heterogeneity modeling and uplift\n  modeling", "comments": "49 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central question in many fields of scientific research is to determine how\nan outcome would be affected by an action, or to measure the effect of an\naction (a.k.a treatment effect). In recent years, a need for estimating the\nheterogeneous treatment effects conditioning on the different characteristics\nof individuals has emerged from research fields such as personalized\nhealthcare, social science, and online marketing. To meet the need, researchers\nand practitioners from different communities have developed algorithms by\ntaking the treatment effect heterogeneity modeling approach and the uplift\nmodeling approach, respectively. In this paper, we provide a unified survey of\nthese two seemingly disconnected yet closely related approaches under the\npotential outcome framework. We then provide a structured survey of existing\nmethods by emphasizing on their inherent connections with a set of unified\nnotations to make comparisons of the different methods easy. We then review the\nmain applications of the surveyed methods in personalized marketing,\npersonalized medicine, and social studies. Finally, we summarize the existing\nsoftware packages and present discussions based on the use of methods on\nsynthetic, semi-synthetic and real world data sets and provide some general\nguidelines for choosing methods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 02:16:02 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 13:26:40 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Zhang", "Weijia", ""], ["Li", "Jiuyong", ""], ["Liu", "Lin", ""]]}, {"id": "2007.12778", "submitter": "Rafael Stern", "authors": "Rafael Izbicki, Gilson Shimizu, Rafael B. Stern", "title": "CD-split and HPD-split: efficient conformal regions in high dimensions", "comments": "31 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conformal methods create prediction bands that control average coverage\nassuming solely i.i.d. data. Although the literature has mostly focused on\nprediction intervals, more general regions can often better represent\nuncertainty. For instance, a bimodal target is better represented by the union\nof two intervals. Such prediction regions are obtained by CD-split , which\ncombines the split method and a data-driven partition of the feature space\nwhich scales to high dimensions. CD-split however contains many tuning\nparameters, and their role is not clear. In this paper, we provide new insights\non CD-split by exploring its theoretical properties. In particular, we show\nthat CD-split converges asymptotically to the oracle highest predictive density\nset and satisfies local and asymptotic conditional validity. We also present\nsimulations which show how to tune CD-split. Finally, we introduce HPD-split, a\nvariation of CD-split that requires less tuning, and show that it shares the\nsame theoretical guarantees as CD-split. In a wide variety of our simulations,\nCD-split and HPD-split have a better conditional coverage and yield smaller\nprediction regions than other methods.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 21:42:34 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 18:46:30 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Izbicki", "Rafael", ""], ["Shimizu", "Gilson", ""], ["Stern", "Rafael B.", ""]]}, {"id": "2007.12782", "submitter": "Marissa Masden", "authors": "Marissa Masden, Dev Sinha", "title": "Linear discriminant initialization for feed-forward neural networks", "comments": "6 pages, 7 figures. Added comparison to larger randomly initialized\n  networks. Updated tables to better illustrate trends in effect size", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.MG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Informed by the basic geometry underlying feed forward neural networks, we\ninitialize the weights of the first layer of a neural network using the linear\ndiscriminants which best distinguish individual classes. Networks initialized\nin this way take fewer training steps to reach the same level of training, and\nasymptotically have higher accuracy on training data.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 21:53:48 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 17:12:23 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Masden", "Marissa", ""], ["Sinha", "Dev", ""]]}, {"id": "2007.12786", "submitter": "Siva Rajesh Kasa", "authors": "Siva Rajesh Kasa, Vaibhav Rajan", "title": "Model-based Clustering using Automatic Differentiation: Confronting\n  Misspecification and High-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study two practically important cases of model based clustering using\nGaussian Mixture Models: (1) when there is misspecification and (2) on high\ndimensional data, in the light of recent advances in Gradient Descent (GD)\nbased optimization using Automatic Differentiation (AD). Our simulation studies\nshow that EM has better clustering performance, measured by Adjusted Rand\nIndex, compared to GD in cases of misspecification, whereas on high dimensional\ndata GD outperforms EM. We observe that both with EM and GD there are many\nsolutions with high likelihood but poor cluster interpretation. To address this\nproblem we design a new penalty term for the likelihood based on the Kullback\nLeibler divergence between pairs of fitted components. Closed form expressions\nfor the gradients of this penalized likelihood are difficult to derive but AD\ncan be done effortlessly, illustrating the advantage of AD-based optimization.\nExtensions of this penalty for high dimensional data and for model selection\nare discussed. Numerical experiments on synthetic and real datasets demonstrate\nthe efficacy of clustering using the proposed penalized likelihood approach.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 10:56:05 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Kasa", "Siva Rajesh", ""], ["Rajan", "Vaibhav", ""]]}, {"id": "2007.12792", "submitter": "Biswajit Khara", "authors": "Sergio Botelho, Ameya Joshi, Biswajit Khara, Soumik Sarkar, Chinmay\n  Hegde, Santi Adavani, Baskar Ganapathysubramanian", "title": "Deep Generative Models that Solve PDEs: Distributed Computing for\n  Training Large Data-Free Models", "comments": "10 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in scientific machine learning (SciML) has opened up the\npossibility of training novel neural network architectures that solve complex\npartial differential equations (PDEs). Several (nearly data free) approaches\nhave been recently reported that successfully solve PDEs, with examples\nincluding deep feed forward networks, generative networks, and deep\nencoder-decoder networks. However, practical adoption of these approaches is\nlimited by the difficulty in training these models, especially to make\npredictions at large output resolutions ($\\geq 1024 \\times 1024$). Here we\nreport on a software framework for data parallel distributed deep learning that\nresolves the twin challenges of training these large SciML models - training in\nreasonable time as well as distributing the storage requirements. Our framework\nprovides several out of the box functionality including (a) loss integrity\nindependent of number of processes, (b) synchronized batch normalization, and\n(c) distributed higher-order optimization methods. We show excellent\nscalability of this framework on both cloud as well as HPC clusters, and report\non the interplay between bandwidth, network topology and bare metal vs cloud.\nWe deploy this approach to train generative models of sizes hitherto not\npossible, showing that neural PDE solvers can be viably trained for practical\napplications. We also demonstrate that distributed higher-order optimization\nmethods are $2-3\\times$ faster than stochastic gradient-based methods and\nprovide minimal convergence drift with higher batch-size.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 22:42:35 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Botelho", "Sergio", ""], ["Joshi", "Ameya", ""], ["Khara", "Biswajit", ""], ["Sarkar", "Soumik", ""], ["Hegde", "Chinmay", ""], ["Adavani", "Santi", ""], ["Ganapathysubramanian", "Baskar", ""]]}, {"id": "2007.12804", "submitter": "Stefan Spalevic", "authors": "Stefan Spalevi\\'c, Petar Veli\\v{c}kovi\\'c, Jovana Kova\\v{c}evi\\'c,\n  Mladen Nikoli\\'c", "title": "Hierarchical Protein Function Prediction with Tail-GNNs", "comments": null, "journal-ref": "ICML 2020 - Workshops - Graph RepresGraph Representation Learning\n  and Beyond (GRL+)", "doi": null, "report-no": null, "categories": "cs.LG q-bio.BM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protein function prediction may be framed as predicting subgraphs (with\ncertain closure properties) of a directed acyclic graph describing the\nhierarchy of protein functions. Graph neural networks (GNNs), with their\nbuilt-in inductive bias for relational data, are hence naturally suited for\nthis task. However, in contrast with most GNN applications, the graph is not\nrelated to the input, but to the label space. Accordingly, we propose\nTail-GNNs, neural networks which naturally compose with the output space of any\nneural network for multi-task prediction, to provide relationally-reinforced\nlabels. For protein function prediction, we combine a Tail-GNN with a dilated\nconvolutional network which learns representations of the protein sequence,\nmaking significant improvement in F_1 score and demonstrating the ability of\nTail-GNNs to learn useful representations of labels and exploit them in\nreal-world problem solving.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 23:38:41 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Spalevi\u0107", "Stefan", ""], ["Veli\u010dkovi\u0107", "Petar", ""], ["Kova\u010devi\u0107", "Jovana", ""], ["Nikoli\u0107", "Mladen", ""]]}, {"id": "2007.12809", "submitter": "Hao Li", "authors": "Andrea L. Bertozzi, Bamdad Hosseini, Hao Li, Kevin Miller, Andrew M.\n  Stuart", "title": "Posterior Consistency of Semi-Supervised Regression on Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based semi-supervised regression (SSR) is the problem of estimating the\nvalue of a function on a weighted graph from its values (labels) on a small\nsubset of the vertices. This paper is concerned with the consistency of SSR in\nthe context of classification, in the setting where the labels have small noise\nand the underlying graph weighting is consistent with well-clustered nodes. We\npresent a Bayesian formulation of SSR in which the weighted graph defines a\nGaussian prior, using a graph Laplacian, and the labeled data defines a\nlikelihood. We analyze the rate of contraction of the posterior measure around\nthe ground truth in terms of parameters that quantify the small label error and\ninherent clustering in the graph. We obtain bounds on the rates of contraction\nand illustrate their sharpness through numerical experiments. The analysis also\ngives insight into the choice of hyperparameters that enter the definition of\nthe prior.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 00:00:19 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 13:25:21 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Bertozzi", "Andrea L.", ""], ["Hosseini", "Bamdad", ""], ["Li", "Hao", ""], ["Miller", "Kevin", ""], ["Stuart", "Andrew M.", ""]]}, {"id": "2007.12815", "submitter": "Surbhi Goel", "authors": "Surbhi Goel, Adam Klivans, Frederic Koehler", "title": "From Boltzmann Machines to Neural Networks and Back Again", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models are powerful tools for modeling high-dimensional data, but\nlearning graphical models in the presence of latent variables is well-known to\nbe difficult. In this work we give new results for learning Restricted\nBoltzmann Machines, probably the most well-studied class of latent variable\nmodels. Our results are based on new connections to learning two-layer neural\nnetworks under $\\ell_{\\infty}$ bounded input; for both problems, we give nearly\noptimal results under the conjectured hardness of sparse parity with noise.\nUsing the connection between RBMs and feedforward networks, we also initiate\nthe theoretical study of $supervised~RBMs$ [Hinton, 2012], a version of\nneural-network learning that couples distributional assumptions induced from\nthe underlying graphical model with the architecture of the unknown function\nclass. We then give an algorithm for learning a natural class of supervised\nRBMs with better runtime than what is possible for its related class of\nnetworks without distributional assumptions.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 00:42:50 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Goel", "Surbhi", ""], ["Klivans", "Adam", ""], ["Koehler", "Frederic", ""]]}, {"id": "2007.12817", "submitter": "Xiao Zhang", "authors": "Haonan Jia, Xiao Zhang, Jun Xu, Wei Zeng, Hao Jiang, Xiaohui Yan,\n  Ji-Rong Wen", "title": "Variance Reduction for Deep Q-Learning using Stochastic Recursive\n  Gradient", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Q-learning algorithms often suffer from poor gradient estimations with\nan excessive variance, resulting in unstable training and poor sampling\nefficiency. Stochastic variance-reduced gradient methods such as SVRG have been\napplied to reduce the estimation variance (Zhao et al. 2019). However, due to\nthe online instance generation nature of reinforcement learning, directly\napplying SVRG to deep Q-learning is facing the problem of the inaccurate\nestimation of the anchor points, which dramatically limits the potentials of\nSVRG. To address this issue and inspired by the recursive gradient variance\nreduction algorithm SARAH (Nguyen et al. 2017), this paper proposes to\nintroduce the recursive framework for updating the stochastic gradient\nestimates in deep Q-learning, achieving a novel algorithm called SRG-DQN.\nUnlike the SVRG-based algorithms, SRG-DQN designs a recursive update of the\nstochastic gradient estimate. The parameter update is along an accumulated\ndirection using the past stochastic gradient information, and therefore can get\nrid of the estimation of the full gradients as the anchors. Additionally,\nSRG-DQN involves the Adam process for further accelerating the training\nprocess. Theoretical analysis and the experimental results on well-known\nreinforcement learning tasks demonstrate the efficiency and effectiveness of\nthe proposed SRG-DQN algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 00:54:20 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Jia", "Haonan", ""], ["Zhang", "Xiao", ""], ["Xu", "Jun", ""], ["Zeng", "Wei", ""], ["Jiang", "Hao", ""], ["Yan", "Xiaohui", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "2007.12824", "submitter": "Douglas Cardoso D.Sc.", "authors": "Igor Q. Lordeiro, Diego B. Haddad, Douglas O. Cardoso", "title": "Multi-Armed Bandits for Minesweeper: Profiting from\n  Exploration-Exploitation Synergy", "comments": "To be published in IEEE Transactions on Games (ISSN 2475-1510 /\n  2475-1502)", "journal-ref": null, "doi": "10.1109/TG.2021.3082909", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular computer puzzle, the game of Minesweeper requires its human players\nto have a mix of both luck and strategy to succeed. Analyzing these aspects\nmore formally, in our research we assessed the feasibility of a novel\nmethodology based on Reinforcement Learning as an adequate approach to tackle\nthe problem presented by this game. For this purpose we employed Multi-Armed\nBandit algorithms which were carefully adapted in order to enable their use to\ndefine autonomous computational players, targeting to make the best use of some\ngame peculiarities. After experimental evaluation, results showed that this\napproach was indeed successful, especially in smaller game boards, such as the\nstandard beginner level. Despite this fact the main contribution of this work\nis a detailed examination of Minesweeper from a learning perspective, which led\nto various original insights which are thoroughly discussed.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 01:44:50 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 21:18:03 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Lordeiro", "Igor Q.", ""], ["Haddad", "Diego B.", ""], ["Cardoso", "Douglas O.", ""]]}, {"id": "2007.12826", "submitter": "Yiqiao Zhong", "authors": "Andrea Montanari and Yiqiao Zhong", "title": "The Interpolation Phase Transition in Neural Networks: Memorization and\n  Generalization under Lazy Training", "comments": "69 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern neural networks are often operated in a strongly overparametrized\nregime: they comprise so many parameters that they can interpolate the training\nset, even if actual labels are replaced by purely random ones. Despite this,\nthey achieve good prediction error on unseen data: interpolating the training\nset does not induce overfitting. Further, overparametrization appears to be\nbeneficial in that it simplifies the optimization landscape. Here we study\nthese phenomena in the context of two-layers neural networks in the neural\ntangent (NT) regime. We consider a simple data model, with isotropic feature\nvectors in $d$ dimensions, and $N$ hidden neurons. Under the assumption $N \\le\nCd$ (for $C$ a constant), we show that the network can exactly interpolate the\ndata as soon as the number of parameters is significantly larger than the\nnumber of samples: $Nd\\gg n$. Under these assumptions, we show that the\nempirical NT kernel has minimum eigenvalue bounded away from zero, and\ncharacterize the generalization error of min-$\\ell_2$ norm interpolants, when\nthe target function is linear. In particular, we show that the network\napproximately performs ridge regression in the raw features, with a strictly\npositive `self-induced' regularization.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 01:51:13 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Montanari", "Andrea", ""], ["Zhong", "Yiqiao", ""]]}, {"id": "2007.12829", "submitter": "Guo Zhong", "authors": "Shi-Xun Lina, Guo Zhongb, Ting Shu", "title": "Joint Featurewise Weighting and Lobal Structure Learning for Multi-view\n  Subspace Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view clustering integrates multiple feature sets, which reveal distinct\naspects of the data and provide complementary information to each other, to\nimprove the clustering performance. It remains challenging to effectively\nexploit complementary information across multiple views since the original data\noften contain noise and are highly redundant. Moreover, most existing\nmulti-view clustering methods only aim to explore the consistency of all views\nwhile ignoring the local structure of each view. However, it is necessary to\ntake the local structure of each view into consideration, because different\nviews would present different geometric structures while admitting the same\ncluster structure. To address the above issues, we propose a novel multi-view\nsubspace clustering method via simultaneously assigning weights for different\nfeatures and capturing local information of data in view-specific\nself-representation feature spaces. Especially, a common cluster structure\nregularization is adopted to guarantee consistency among different views. An\nefficient algorithm based on an augmented Lagrangian multiplier is also\ndeveloped to solve the associated optimization problem. Experiments conducted\non several benchmark datasets demonstrate that the proposed method achieves\nstate-of-the-art performance. We provide the Matlab code on\nhttps://github.com/Ekin102003/JFLMSC.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 01:57:57 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Lina", "Shi-Xun", ""], ["Zhongb", "Guo", ""], ["Shu", "Ting", ""]]}, {"id": "2007.12845", "submitter": "Chenguang Lu", "authors": "Chenguang Lu", "title": "Fair Marriage Principle and Initialization Map for the EM Algorithm", "comments": "13 pages and 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popular convergence theory of the EM algorithm explains that the observed\nincomplete data log-likelihood L and the complete data log-likelihood Q are\npositively correlated, and we can maximize L by maximizing Q. The Deterministic\nAnnealing EM (DAEM) algorithm was hence proposed for avoiding locally maximal\nQ. This paper provides different conclusions: 1) The popular convergence theory\nis wrong; 2) The locally maximal Q can affect the convergent speed, but cannot\nblock the global convergence; 3) Like marriage competition, unfair competition\nbetween two components may vastly decrease the globally convergent speed; 4)\nLocal convergence exists because the sample is too small, and unfair\ncompetition exists; 5) An improved EM algorithm, called the Channel Matching\n(CM) EM algorithm, can accelerate the global convergence. This paper provides\nan initialization map with two means as two axes for the example of a binary\nGaussian mixture studied by the authors of DAEM algorithm. This map can tell\nhow fast the convergent speeds are for different initial means and why points\nin some areas are not suitable as initial points. A two-dimensional example\nindicates that the big sample or the fair initialization can avoid global\nconvergence. For more complicated mixture models, we need further study to\nconvert the fair marriage principle to specific methods for the\ninitializations.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 03:23:49 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Lu", "Chenguang", ""]]}, {"id": "2007.12851", "submitter": "Shen Zhang", "authors": "Shen Zhang, Fei Ye, Bingnan Wang, Thomas G. Habetler", "title": "Few-Shot Bearing Fault Diagnosis Based on Model-Agnostic Meta-Learning", "comments": null, "journal-ref": null, "doi": "10.1109/TIA.2021.3091958", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development of artificial intelligence and deep learning has\nprovided many opportunities to further enhance the safety, stability, and\naccuracy of industrial Cyber-Physical Systems (CPS). As indispensable\ncomponents to many mission-critical CPS assets and equipment, mechanical\nbearings need to be monitored to identify any trace of abnormal conditions.\nMost of the data-driven approaches applied to bearing fault diagnosis\nup-to-date are trained using a large amount of fault data collected a priori.\nIn many practical applications, however, it can be unsafe and time-consuming to\ncollect sufficient data samples for each fault category, making it challenging\nto train a robust classifier. In this paper, we propose a few-shot learning\nframework for bearing fault diagnosis based on model-agnostic meta-learning\n(MAML), which targets for training an effective fault classifier using limited\ndata. In addition, it can leverage the training data and learn to identify new\nfault scenarios more efficiently. Case studies on the generalization to new\nartificial faults show that the proposed framework achieves an overall accuracy\nup to 25% higher than a Siamese network-based benchmark study. Finally, the\nrobustness and the generalization capability of the proposed framework are\nfurther validated by applying it to identify real bearing damages using data\nfrom artificial damages, which compares favorably against 6 state-of-the-art\nfew-shot learning algorithms using consistent test environments.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 04:03:18 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 00:30:11 GMT"}, {"version": "v3", "created": "Wed, 24 Feb 2021 07:15:55 GMT"}, {"version": "v4", "created": "Thu, 24 Jun 2021 06:39:43 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Zhang", "Shen", ""], ["Ye", "Fei", ""], ["Wang", "Bingnan", ""], ["Habetler", "Thomas G.", ""]]}, {"id": "2007.12852", "submitter": "Mingyuan Zhou", "authors": "Rahi Kalantari and Mingyuan Zhou", "title": "Graph Gamma Process Generalized Linear Dynamical Systems", "comments": "36 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce graph gamma process (GGP) linear dynamical systems to model\nreal-valued multivariate time series. For temporal pattern discovery, the\nlatent representation under the model is used to decompose the time series into\na parsimonious set of multivariate sub-sequences. In each sub-sequence,\ndifferent data dimensions often share similar temporal patterns but may exhibit\ndistinct magnitudes, and hence allowing the superposition of all sub-sequences\nto exhibit diverse behaviors at different data dimensions. We further\ngeneralize the proposed model by replacing the Gaussian observation layer with\nthe negative binomial distribution to model multivariate count time series.\nGenerated from the proposed GGP is an infinite dimensional directed sparse\nrandom graph, which is constructed by taking the logical OR operation of\ncountably infinite binary adjacency matrices that share the same set of\ncountably infinite nodes. Each of these adjacency matrices is associated with a\nweight to indicate its activation strength, and places a finite number of edges\nbetween a finite subset of nodes belonging to the same node community. We use\nthe generated random graph, whose number of nonzero-degree nodes is finite, to\ndefine both the sparsity pattern and dimension of the latent state transition\nmatrix of a (generalized) linear dynamical system. The activation strength of\neach node community relative to the overall activation strength is used to\nextract a multivariate sub-sequence, revealing the data pattern captured by the\ncorresponding community. On both synthetic and real-world time series, the\nproposed nonparametric Bayesian dynamic models, which are initialized at\nrandom, consistently exhibit good predictive performance in comparison to a\nvariety of baseline models, revealing interpretable latent state transition\npatterns and decomposing the time series into distinctly behaved sub-sequences.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 04:16:34 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Kalantari", "Rahi", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "2007.12858", "submitter": "Di Qiu", "authors": "Di Qiu, Lok Ming Lui", "title": "Modal Uncertainty Estimation via Discrete Latent Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many important problems in the real world don't have unique solutions. It is\nthus important for machine learning models to be capable of proposing different\nplausible solutions with meaningful probability measures. In this work we\nintroduce such a deep learning framework that learns the one-to-many mappings\nbetween the inputs and outputs, together with faithful uncertainty measures. We\ncall our framework {\\it modal uncertainty estimation} since we model the\none-to-many mappings to be generated through a set of discrete latent\nvariables, each representing a latent mode hypothesis that explains the\ncorresponding type of input-output relationship. The discrete nature of the\nlatent representations thus allows us to estimate for any input the conditional\nprobability distribution of the outputs very effectively. Both the discrete\nlatent space and its uncertainty estimation are jointly learned during\ntraining. We motivate our use of discrete latent space through the multi-modal\nposterior collapse problem in current conditional generative models, then\ndevelop the theoretical background, and extensively validate our method on both\nsynthetic and realistic tasks. Our framework demonstrates significantly more\naccurate uncertainty estimation than the current state-of-the-art methods, and\nis informative and convenient for practical use.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 05:29:34 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Qiu", "Di", ""], ["Lui", "Lok Ming", ""]]}, {"id": "2007.12865", "submitter": "Tiansheng Yao", "authors": "Tiansheng Yao, Xinyang Yi, Derek Zhiyuan Cheng, Felix Yu, Ting Chen,\n  Aditya Menon, Lichan Hong, Ed H. Chi, Steve Tjoa, Jieqi Kang, Evan Ettinger", "title": "Self-supervised Learning for Large-scale Item Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale recommender models find most relevant items from huge catalogs,\nand they play a critical role in modern search and recommendation systems. To\nmodel the input space with large-vocab categorical features, a typical\nrecommender model learns a joint embedding space through neural networks for\nboth queries and items from user feedback data. However, with millions to\nbillions of items in the corpus, users tend to provide feedback for a very\nsmall set of them, causing a power-law distribution. This makes the feedback\ndata for long-tail items extremely sparse.\n  Inspired by the recent success in self-supervised representation learning\nresearch in both computer vision and natural language understanding, we propose\na multi-task self-supervised learning (SSL) framework for large-scale item\nrecommendations. The framework is designed to tackle the label sparsity problem\nby learning better latent relationship of item features. Specifically, SSL\nimproves item representation learning as well as serving as additional\nregularization to improve generalization. Furthermore, we propose a novel data\naugmentation method that utilizes feature correlations within the proposed\nframework.\n  We evaluate our framework using two real-world datasets with 500M and 1B\ntraining examples respectively. Our results demonstrate the effectiveness of\nSSL regularization and show its superior performance over the state-of-the-art\nregularization techniques. We also have already launched the proposed\ntechniques to a web-scale commercial app-to-app recommendation system, with\nsignificant improvements top-tier business metrics demonstrated in A/B\nexperiments on live traffic. Our online results also verify our hypothesis that\nour framework indeed improves model performance even more on slices that lack\nsupervision.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 06:21:43 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 06:35:35 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 08:10:20 GMT"}, {"version": "v4", "created": "Thu, 25 Feb 2021 02:50:58 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Yao", "Tiansheng", ""], ["Yi", "Xinyang", ""], ["Cheng", "Derek Zhiyuan", ""], ["Yu", "Felix", ""], ["Chen", "Ting", ""], ["Menon", "Aditya", ""], ["Hong", "Lichan", ""], ["Chi", "Ed H.", ""], ["Tjoa", "Steve", ""], ["Kang", "Jieqi", ""], ["Ettinger", "Evan", ""]]}, {"id": "2007.12882", "submitter": "Stephane Chretien", "authors": "Emmanuel Caron and Stephane Chretien", "title": "A finite sample analysis of the benign overfitting phenomenon for ridge\n  function estimation", "comments": "Some typos were corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent extensive numerical experiments in high scale machine learning have\nallowed to uncover a quite counterintuitive phase transition, as a function of\nthe ratio between the sample size and the number of parameters in the model. As\nthe number of parameters $p$ approaches the sample size $n$, the generalisation\nerror (a.k.a. testing error) increases, but in many cases, it starts decreasing\nagain past the threshold $p=n$. This surprising phenomenon, brought to the\ntheoretical community attention in \\cite{belkin2019reconciling}, has been\nthoroughly investigated lately, more specifically for simpler models than deep\nneural networks, such as the linear model when the parameter is taken to be the\nminimum norm solution to the least-square problem, mostly in the asymptotic\nregime when $p$ and $n$ tend to $+\\infty$; see e.g. \\cite{hastie2019surprises}.\nIn the present paper, we propose a finite sample analysis of non-linear models\nof \\textit{ridge} type, where we investigate the \\textit{overparametrised\nregime} of the double descent phenomenon for both the \\textit{estimation\nproblem} and the \\textit{prediction} problem. Our results provide a precise\nanalysis of the distance of the best estimator from the true parameter as well\nas a generalisation bound which complements recent works of\n\\cite{bartlett2020benign} and \\cite{chinot2020benign}. Our analysis is based on\nefficient but elementary tools closely related to the continuous Newton method\n\\cite{neuberger2007continuous}.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 08:40:29 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 12:21:23 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2021 19:41:10 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Caron", "Emmanuel", ""], ["Chretien", "Stephane", ""]]}, {"id": "2007.12911", "submitter": "Maria Perez-Ortiz", "authors": "Mar\\'ia P\\'erez-Ortiz and Omar Rivasplata and John Shawe-Taylor and\n  Csaba Szepesv\\'ari", "title": "Tighter risk certificates for neural networks", "comments": "Preprint under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an empirical study regarding training probabilistic\nneural networks using training objectives derived from PAC-Bayes bounds. In the\ncontext of probabilistic neural networks, the output of training is a\nprobability distribution over network weights. We present two training\nobjectives, used here for the first time in connection with training neural\nnetworks. These two training objectives are derived from tight PAC-Bayes\nbounds. We also re-implement a previously used training objective based on a\nclassical PAC-Bayes bound, to compare the properties of the predictors learned\nusing the different training objectives. We compute risk certificates that are\nvalid on any unseen examples for the learnt predictors. We further experiment\nwith different types of priors on the weights (both data-free and\ndata-dependent priors) and neural network architectures. Our experiments on\nMNIST and CIFAR-10 show that our training methods produce competitive test set\nerrors and non-vacuous risk bounds with much tighter values than previous\nresults in the literature, showing promise not only to guide the learning\nalgorithm through bounding the risk but also for model selection. These\nobservations suggest that the methods studied here might be good candidates for\nself-certified learning, in the sense of certifying the risk on any unseen data\nwithout the need for data-splitting protocols.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 11:02:16 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 11:09:09 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["P\u00e9rez-Ortiz", "Mar\u00eda", ""], ["Rivasplata", "Omar", ""], ["Shawe-Taylor", "John", ""], ["Szepesv\u00e1ri", "Csaba", ""]]}, {"id": "2007.12919", "submitter": "Ly Antoine PhD", "authors": "Dimitri Delcaillau, Antoine Ly, Franck Vermet, Aliz\\'e Papp", "title": "Interpretabilit\\'e des mod\\`eles : \\'etat des lieux des m\\'ethodes et\n  application \\`a l'assurance", "comments": "25 pages without appendix, submitted to BFA, French preprint before\n  English paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since May 2018, the General Data Protection Regulation (GDPR) has introduced\nnew obligations to industries. By setting a legal framework, it notably imposes\nstrong transparency on the use of personal data. Thus, people must be informed\nof the use of their data and must consent the usage of it. Data is the raw\nmaterial of many models which today make it possible to increase the quality\nand performance of digital services. Transparency on the use of data also\nrequires a good understanding of its use through different models. The use of\nmodels, even if efficient, must be accompanied by an understanding at all\nlevels of the process that transform data (upstream and downstream of a model),\nthus making it possible to define the relationships between the individual's\ndata and the choice that an algorithm could make based on the analysis of the\nlatter. (For example, the recommendation of one product or one promotional\noffer or an insurance rate representative of the risk.) Models users must\nensure that models do not discriminate against and that it is also possible to\nexplain its result. The widening of the panel of predictive algorithms - made\npossible by the evolution of computing capacities -- leads scientists to be\nvigilant about the use of models and to consider new tools to better understand\nthe decisions deduced from them . Recently, the community has been particularly\nactive on model transparency with a marked intensification of publications over\nthe past three years. The increasingly frequent use of more complex algorithms\n(\\textit{deep learning}, Xgboost, etc.) presenting attractive performances is\nundoubtedly one of the causes of this interest. This article thus presents an\ninventory of methods of interpreting models and their uses in an insurance\ncontext.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 12:18:07 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Delcaillau", "Dimitri", ""], ["Ly", "Antoine", ""], ["Vermet", "Franck", ""], ["Papp", "Aliz\u00e9", ""]]}, {"id": "2007.12927", "submitter": "Jo\\~ao Sacramento", "authors": "Johannes von Oswald, Seijin Kobayashi, Jo\\~ao Sacramento, Alexander\n  Meulemans, Christian Henning, Benjamin F. Grewe", "title": "Neural networks with late-phase weights", "comments": "25 pages, 6 figures", "journal-ref": "Published as a conference paper at ICLR 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The largely successful method of training neural networks is to learn their\nweights using some variant of stochastic gradient descent (SGD). Here, we show\nthat the solutions found by SGD can be further improved by ensembling a subset\nof the weights in late stages of learning. At the end of learning, we obtain\nback a single model by taking a spatial average in weight space. To avoid\nincurring increased computational costs, we investigate a family of\nlow-dimensional late-phase weight models which interact multiplicatively with\nthe remaining parameters. Our results show that augmenting standard models with\nlate-phase weights improves generalization in established benchmarks such as\nCIFAR-10/100, ImageNet and enwik8. These findings are complemented with a\ntheoretical analysis of a noisy quadratic problem which provides a simplified\npicture of the late phases of neural network learning.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 13:23:37 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 16:41:19 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 18:38:41 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["von Oswald", "Johannes", ""], ["Kobayashi", "Seijin", ""], ["Sacramento", "Jo\u00e3o", ""], ["Meulemans", "Alexander", ""], ["Henning", "Christian", ""], ["Grewe", "Benjamin F.", ""]]}, {"id": "2007.12934", "submitter": "Reza Shokri", "authors": "Anshul Aggarwal, Trevor E. Carlson, Reza Shokri, Shruti Tople", "title": "SOTERIA: In Search of Efficient Neural Networks for Private Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ML-as-a-service is gaining popularity where a cloud server hosts a trained\nmodel and offers prediction (inference) service to users. In this setting, our\nobjective is to protect the confidentiality of both the users' input queries as\nwell as the model parameters at the server, with modest computation and\ncommunication overhead. Prior solutions primarily propose fine-tuning\ncryptographic methods to make them efficient for known fixed model\narchitectures. The drawback with this line of approach is that the model itself\nis never designed to operate with existing efficient cryptographic\ncomputations. We observe that the network architecture, internal functions, and\nparameters of a model, which are all chosen during training, significantly\ninfluence the computation and communication overhead of a cryptographic method,\nduring inference. Based on this observation, we propose SOTERIA -- a training\nmethod to construct model architectures that are by-design efficient for\nprivate inference. We use neural architecture search algorithms with the dual\nobjective of optimizing the accuracy of the model and the overhead of using\ncryptographic primitives for secure inference. Given the flexibility of\nmodifying a model during training, we find accurate models that are also\nefficient for private computation. We select garbled circuits as our underlying\ncryptographic primitive, due to their expressiveness and efficiency, but this\napproach can be extended to hybrid multi-party computation settings. We\nempirically evaluate SOTERIA on MNIST and CIFAR10 datasets, to compare with the\nprior work. Our results confirm that SOTERIA is indeed effective in balancing\nperformance and accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 13:53:02 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Aggarwal", "Anshul", ""], ["Carlson", "Trevor E.", ""], ["Shokri", "Reza", ""], ["Tople", "Shruti", ""]]}, {"id": "2007.12948", "submitter": "Amrith Setlur", "authors": "Amrith Setlur, Barnabas Poczos, Alan W Black", "title": "Nonlinear ISA with Auxiliary Variables for Learning Speech\n  Representations", "comments": "To be presented at Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper extends recent work on nonlinear Independent Component Analysis\n(ICA) by introducing a theoretical framework for nonlinear Independent Subspace\nAnalysis (ISA) in the presence of auxiliary variables. Observed high\ndimensional acoustic features like log Mel spectrograms can be considered as\nsurface level manifestations of nonlinear transformations over individual\nmultivariate sources of information like speaker characteristics, phonological\ncontent etc. Under assumptions of energy based models we use the theory of\nnonlinear ISA to propose an algorithm that learns unsupervised speech\nrepresentations whose subspaces are independent and potentially highly\ncorrelated with the original non-stationary multivariate sources. We show how\nnonlinear ICA with auxiliary variables can be extended to a generic\nidentifiable model for subspaces as well while also providing sufficient\nconditions for the identifiability of these high dimensional subspaces. Our\nproposed methodology is generic and can be integrated with standard\nunsupervised approaches to learn speech representations with subspaces that can\ntheoretically capture independent higher order speech signals. We evaluate the\ngains of our algorithm when integrated with the Autoregressive Predictive\nDecoding (APC) model by showing empirical results on the speaker verification\nand phoneme recognition tasks.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 14:53:09 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Setlur", "Amrith", ""], ["Poczos", "Barnabas", ""], ["Black", "Alan W", ""]]}, {"id": "2007.12975", "submitter": "George Chen", "authors": "George H. Chen", "title": "Deep Kernel Survival Analysis and Subject-Specific Survival Time\n  Prediction Intervals", "comments": "Machine Learning for Healthcare conference (MLHC 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel survival analysis methods predict subject-specific survival curves and\ntimes using information about which training subjects are most similar to a\ntest subject. These most similar training subjects could serve as forecast\nevidence. How similar any two subjects are is given by the kernel function. In\nthis paper, we present the first neural network framework that learns which\nkernel functions to use in kernel survival analysis. We also show how to use\nkernel functions to construct prediction intervals of survival time estimates\nthat are statistically valid for individuals similar to a test subject. These\nprediction intervals can use any kernel function, such as ones learned using\nour neural kernel learning framework or using random survival forests. Our\nexperiments show that our neural kernel survival estimators are competitive\nwith a variety of existing survival analysis methods, and that our prediction\nintervals can help compare different methods' uncertainties, even for\nestimators that do not use kernels. In particular, these prediction interval\nwidths can be used as a new performance metric for survival analysis methods.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 16:55:16 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Chen", "George H.", ""]]}, {"id": "2007.12986", "submitter": "Praveen Chandar", "authors": "James McInerney, Brian Brost, Praveen Chandar, Rishabh Mehrotra, Ben\n  Carterette", "title": "Counterfactual Evaluation of Slate Recommendations with Sequential\n  Reward Interactions", "comments": null, "journal-ref": null, "doi": "10.1145/3394486.3403229", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users of music streaming, video streaming, news recommendation, and\ne-commerce services often engage with content in a sequential manner. Providing\nand evaluating good sequences of recommendations is therefore a central problem\nfor these services. Prior reweighting-based counterfactual evaluation methods\neither suffer from high variance or make strong independence assumptions about\nrewards. We propose a new counterfactual estimator that allows for sequential\ninteractions in the rewards with lower variance in an asymptotically unbiased\nmanner. Our method uses graphical assumptions about the causal relationships of\nthe slate to reweight the rewards in the logging policy in a way that\napproximates the expected sum of rewards under the target policy. Extensive\nexperiments in simulation and on a live recommender system show that our\napproach outperforms existing methods in terms of bias and data efficiency for\nthe sequential track recommendations problem.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 17:58:01 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 01:34:40 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["McInerney", "James", ""], ["Brost", "Brian", ""], ["Chandar", "Praveen", ""], ["Mehrotra", "Rishabh", ""], ["Carterette", "Ben", ""]]}, {"id": "2007.12998", "submitter": "Sahithi Ankireddy", "authors": "Sahithi Ankireddy", "title": "A Novel Approach to the Diagnosis of Heart Disease using Machine\n  Learning and Deep Neural Networks", "comments": "Presented at 2019 IEEE MIT URTC Conference, 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heart disease is the leading cause of death worldwide. Currently, 33% of\ncases are misdiagnosed, and approximately half of myocardial infarctions occur\nin people who are not predicted to be at risk. The use of Artificial\nIntelligence could reduce the chance of error, leading to possible earlier\ndiagnoses, which could be the difference between life and death for some. The\nobjective of this project was to develop an application for assisted heart\ndisease diagnosis using Machine Learning (ML) and Deep Neural Network (DNN)\nalgorithms. The dataset was provided from the Cleveland Clinic Foundation, and\nthe models were built based on various optimization and hyper parametrization\ntechniques including a Grid Search algorithm. The application, running on\nFlask, and utilizing Bootstrap was developed using the DNN, as it performed\nhigher than the Random Forest ML model with a total accuracy rate of 92%.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 19:08:04 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Ankireddy", "Sahithi", ""]]}, {"id": "2007.13004", "submitter": "Daheng Wang", "authors": "Daheng Wang, Zhihan Zhang, Yihong Ma, Tong Zhao, Tianwen Jiang, Nitesh\n  V. Chawla, Meng Jiang", "title": "Learning Attribute-Structure Co-Evolutions in Dynamic Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most graph neural network models learn embeddings of nodes in static\nattributed graphs for predictive analysis. Recent attempts have been made to\nlearn temporal proximity of the nodes. We find that real dynamic attributed\ngraphs exhibit complex co-evolution of node attributes and graph structure.\nLearning node embeddings for forecasting change of node attributes and birth\nand death of links over time remains an open problem. In this work, we present\na novel framework called CoEvoGNN for modeling dynamic attributed graph\nsequence. It preserves the impact of earlier graphs on the current graph by\nembedding generation through the sequence. It has a temporal self-attention\nmechanism to model long-range dependencies in the evolution. Moreover, CoEvoGNN\noptimizes model parameters jointly on two dynamic tasks, attribute inference\nand link prediction over time. So the model can capture the co-evolutionary\npatterns of attribute change and link formation. This framework can adapt to\nany graph neural algorithms so we implemented and investigated three methods\nbased on it: CoEvoGCN, CoEvoGAT, and CoEvoSAGE. Experiments demonstrate the\nframework (and its methods) outperform strong baselines on predicting an entire\nunseen graph snapshot of personal attributes and interpersonal links in dynamic\nsocial graphs and financial graphs.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 20:07:28 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Wang", "Daheng", ""], ["Zhang", "Zhihan", ""], ["Ma", "Yihong", ""], ["Zhao", "Tong", ""], ["Jiang", "Tianwen", ""], ["Chawla", "Nitesh V.", ""], ["Jiang", "Meng", ""]]}, {"id": "2007.13018", "submitter": "Aaqib Saeed", "authors": "Aaqib Saeed, Flora D. Salim, Tanir Ozcelebi, and Johan Lukkien", "title": "Federated Self-Supervised Learning of Multi-Sensor Representations for\n  Embedded Intelligence", "comments": "Accepted for publication at IEEE Internet of Things Journal", "journal-ref": null, "doi": "10.1109/JIOT.2020.3009358", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Smartphones, wearables, and Internet of Things (IoT) devices produce a wealth\nof data that cannot be accumulated in a centralized repository for learning\nsupervised models due to privacy, bandwidth limitations, and the prohibitive\ncost of annotations. Federated learning provides a compelling framework for\nlearning models from decentralized data, but conventionally, it assumes the\navailability of labeled samples, whereas on-device data are generally either\nunlabeled or cannot be annotated readily through user interaction. To address\nthese issues, we propose a self-supervised approach termed\n\\textit{scalogram-signal correspondence learning} based on wavelet transform to\nlearn useful representations from unlabeled sensor inputs, such as\nelectroencephalography, blood volume pulse, accelerometer, and WiFi channel\nstate information. Our auxiliary task requires a deep temporal neural network\nto determine if a given pair of a signal and its complementary viewpoint (i.e.,\na scalogram generated with a wavelet transform) align with each other or not\nthrough optimizing a contrastive objective. We extensively assess the quality\nof learned features with our multi-view strategy on diverse public datasets,\nachieving strong performance in all domains. We demonstrate the effectiveness\nof representations learned from an unlabeled input collection on downstream\ntasks with training a linear classifier over pretrained network, usefulness in\nlow-data regime, transfer learning, and cross-validation. Our methodology\nachieves competitive performance with fully-supervised networks, and it\noutperforms pre-training with autoencoders in both central and federated\ncontexts. Notably, it improves the generalization in a semi-supervised setting\nas it reduces the volume of labeled data required through leveraging\nself-supervised learning.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 21:59:17 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Saeed", "Aaqib", ""], ["Salim", "Flora D.", ""], ["Ozcelebi", "Tanir", ""], ["Lukkien", "Johan", ""]]}, {"id": "2007.13023", "submitter": "Fang Liu", "authors": "Rahul Singh, Fang Liu, and Ness B. Shroff", "title": "A Partially Observable MDP Approach for Sequential Testing for\n  Infectious Diseases such as COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outbreak of the novel coronavirus (COVID-19) is unfolding as a major\ninternational crisis whose influence extends to every aspect of our daily\nlives. Effective testing allows infected individuals to be quarantined, thus\nreducing the spread of COVID-19, saving countless lives, and helping to restart\nthe economy safely and securely. Developing a good testing strategy can be\ngreatly aided by contact tracing that provides health care providers\ninformation about the whereabouts of infected patients in order to determine\nwhom to test. Countries that have been more successful in corralling the virus\ntypically use a ``test, treat, trace, test'' strategy that begins with testing\nindividuals with symptoms, traces contacts of positively tested individuals via\na combinations of patient memory, apps, WiFi, GPS, etc., followed by testing\ntheir contacts, and repeating this procedure. The problem is that such\nstrategies are myopic and do not efficiently use the testing resources. This is\nespecially the case with COVID-19, where symptoms may show up several days\nafter the infection (or not at all, there is evidence to suggest that many\nCOVID-19 carriers are asymptotic, but may spread the virus). Such greedy\nstrategies, miss out population areas where the virus may be dormant and flare\nup in the future.\n  In this paper, we show that the testing problem can be cast as a sequential\nlearning-based resource allocation problem with constraints, where the input to\nthe problem is provided by a time-varying social contact graph obtained through\nvarious contact tracing tools. We then develop efficient learning strategies\nthat minimize the number of infected individuals. These strategies are based on\npolicy iteration and look-ahead rules. We investigate fundamental performance\nbounds, and ensure that our solution is robust to errors in the input graph as\nwell as in the tests themselves.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 22:13:37 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Singh", "Rahul", ""], ["Liu", "Fang", ""], ["Shroff", "Ness B.", ""]]}, {"id": "2007.13040", "submitter": "Huaxiu Yao", "authors": "Huaxiu Yao, Longkai Huang, Linjun Zhang, Ying Wei, Li Tian, James Zou,\n  Junzhou Huang, Zhenhui Li", "title": "Improving Generalization in Meta-learning via Task Augmentation", "comments": "Accepted by ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning has proven to be a powerful paradigm for transferring the\nknowledge from previous tasks to facilitate the learning of a novel task.\nCurrent dominant algorithms train a well-generalized model initialization which\nis adapted to each task via the support set. The crux lies in optimizing the\ngeneralization capability of the initialization, which is measured by the\nperformance of the adapted model on the query set of each task. Unfortunately,\nthis generalization measure, evidenced by empirical results, pushes the\ninitialization to overfit the meta-training tasks, which significantly impairs\nthe generalization and adaptation to novel tasks. To address this issue, we\nactively augment a meta-training task with \"more data\" when evaluating the\ngeneralization. Concretely, we propose two task augmentation methods, including\nMetaMix and Channel Shuffle. MetaMix linearly combines features and labels of\nsamples from both the support and query sets. For each class of samples,\nChannel Shuffle randomly replaces a subset of their channels with the\ncorresponding ones from a different class. Theoretical studies show how task\naugmentation improves the generalization of meta-learning. Moreover, both\nMetaMix and Channel Shuffle outperform state-of-the-art results by a large\nmargin across many datasets and are compatible with existing meta-learning\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 01:50:42 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2021 00:52:20 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 19:19:01 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Yao", "Huaxiu", ""], ["Huang", "Longkai", ""], ["Zhang", "Linjun", ""], ["Wei", "Ying", ""], ["Tian", "Li", ""], ["Zou", "James", ""], ["Huang", "Junzhou", ""], ["Li", "Zhenhui", ""]]}, {"id": "2007.13067", "submitter": "Jie Xu", "authors": "Jie Xu, Yazhou Ren, Guofeng Li, Lili Pan, Ce Zhu, Zenglin Xu", "title": "Deep Embedded Multi-view Clustering with Collaborative Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view clustering has attracted increasing attentions recently by\nutilizing information from multiple views. However, existing multi-view\nclustering methods are either with high computation and space complexities, or\nlack of representation capability. To address these issues, we propose deep\nembedded multi-view clustering with collaborative training (DEMVC) in this\npaper. Firstly, the embedded representations of multiple views are learned\nindividually by deep autoencoders. Then, both consensus and complementary of\nmultiple views are taken into account and a novel collaborative training scheme\nis proposed. Concretely, the feature representations and cluster assignments of\nall views are learned collaboratively. A new consistency strategy for cluster\ncenters initialization is further developed to improve the multi-view\nclustering performance with collaborative training. Experimental results on\nseveral popular multi-view datasets show that DEMVC achieves significant\nimprovements over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 06:51:57 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Xu", "Jie", ""], ["Ren", "Yazhou", ""], ["Li", "Guofeng", ""], ["Pan", "Lili", ""], ["Zhu", "Ce", ""], ["Xu", "Zenglin", ""]]}, {"id": "2007.13077", "submitter": "Hossein Yazdani", "authors": "Hossein Yazdani", "title": "Bounded Fuzzy Possibilistic Method of Critical Objects Processing in\n  Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsatisfying accuracy of learning methods is mostly caused by omitting the\ninfluence of important parameters such as membership assignments, type of data\nobjects, and distance or similarity functions. The proposed method, called\nBounded Fuzzy Possibilistic Method (BFPM) addresses different issues that\nprevious clustering or classification methods have not sufficiently considered\nin their membership assignments. In fuzzy methods, the object's memberships\nshould sum to 1. Hence, any data object may obtain full membership in at most\none cluster or class. Possibilistic methods relax this condition, but the\nmethod can be satisfied with the results even if just an arbitrary object\nobtains the membership from just one cluster, which prevents the objects'\nmovement analysis. Whereas, BFPM differs from previous fuzzy and possibilistic\napproaches by removing these restrictions. Furthermore, BFPM provides the\nflexible search space for objects' movement analysis. Data objects are also\nconsidered as fundamental keys in learning methods, and knowing the exact type\nof objects results in providing a suitable environment for learning algorithms.\nThe Thesis introduces a new type of object, called critical, as well as\ncategorizing data objects into two different categories: structural-based and\nbehavioural-based. Critical objects are considered as causes of\nmiss-classification and miss-assignment in learning procedures. The Thesis also\nproposes new methodologies to study the behaviour of critical objects with the\naim of evaluating objects' movements (mutation) from one cluster or class to\nanother. The Thesis also introduces a new type of feature, called dominant,\nthat is considered as one of the causes of miss-classification and\nmiss-assignments. Then the Thesis proposes new sets of similarity functions,\ncalled Weighted Feature Distance (WFD) and Prioritized Weighted Feature\nDistance (PWFD).\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 08:12:33 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Yazdani", "Hossein", ""]]}, {"id": "2007.13087", "submitter": "Amit Livne", "authors": "Amit Livne, Roy Dor, Eyal Mazuz, Tamar Didi, Bracha Shapira, and Lior\n  Rokach", "title": "Iterative Boosting Deep Neural Networks for Predicting Click-Through\n  Rate", "comments": "16 Pages, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The click-through rate (CTR) reflects the ratio of clicks on a specific item\nto its total number of views. It has significant impact on websites'\nadvertising revenue. Learning sophisticated models to understand and predict\nuser behavior is essential for maximizing the CTR in recommendation systems.\nRecent works have suggested new methods that replace the expensive and\ntime-consuming feature engineering process with a variety of deep learning (DL)\nclassifiers capable of capturing complicated patterns from raw data; these\nmethods have shown significant improvement on the CTR prediction task. While DL\ntechniques can learn intricate user behavior patterns, it relies on a vast\namount of data and does not perform as well when there is a limited amount of\ndata. We propose XDBoost, a new DL method for capturing complex patterns that\nrequires just a limited amount of raw data. XDBoost is an iterative three-stage\nneural network model influenced by the traditional machine learning boosting\nmechanism. XDBoost's components operate sequentially similar to boosting;\nHowever, unlike conventional boosting, XDBoost does not sum the predictions\ngenerated by its components. Instead, it utilizes these predictions as new\nartificial features and enhances CTR prediction by retraining the model using\nthese features. Comprehensive experiments conducted to illustrate the\neffectiveness of XDBoost on two datasets demonstrated its ability to outperform\nexisting state-of-the-art (SOTA) models for CTR prediction.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 09:41:16 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Livne", "Amit", ""], ["Dor", "Roy", ""], ["Mazuz", "Eyal", ""], ["Didi", "Tamar", ""], ["Shapira", "Bracha", ""], ["Rokach", "Lior", ""]]}, {"id": "2007.13114", "submitter": "Mamoun Mardini", "authors": "Mamoun T. Mardini, Subhash Nerella Amal A. Wanigatunga, Santiago\n  Saldana, Ramon Casanova, Todd M. Manini", "title": "Deep CHORES: Estimating Hallmark Measures of Physical Activity Using\n  Deep Learning", "comments": null, "journal-ref": "Mardini MT, Nerella S, Wanigatunga AA, Saldana S, Casanova R,\n  Manini TM. Deep CHORES: Estimating Hallmark Measures of Physical Activity\n  Using Deep Learning. AMIA Annu Symp Proc. 2021 Jan 25;2020:803-812. PMID:\n  33936455; PMCID: PMC8075495", "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wrist accelerometers for assessing hallmark measures of physical activity\n(PA) are rapidly growing with the advent of smartwatch technology. Given the\ngrowing popularity of wrist-worn accelerometers, there needs to be a rigorous\nevaluation for recognizing (PA) type and estimating energy expenditure (EE)\nacross the lifespan. Participants (66% women, aged 20-89 yrs) performed a\nbattery of 33 daily activities in a standardized laboratory setting while a\ntri-axial accelerometer collected data from the right wrist. A portable\nmetabolic unit was worn to measure metabolic intensity. We built deep learning\nnetworks to extract spatial and temporal representations from the time-series\ndata, and used them to recognize PA type and estimate EE. The deep learning\nmodels resulted in high performance; the F1 score was: 0.82, 0.81, and 95 for\nrecognizing sedentary, locomotor, and lifestyle activities, respectively. The\nroot mean square error was 1.1 (+/-0.13) for the estimation of EE.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 12:15:50 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Mardini", "Mamoun T.", ""], ["Wanigatunga", "Subhash Nerella Amal A.", ""], ["Saldana", "Santiago", ""], ["Casanova", "Ramon", ""], ["Manini", "Todd M.", ""]]}, {"id": "2007.13123", "submitter": "Kerem Can Tezcan", "authors": "M\\'elanie Gaillochet and Kerem C. Tezcan and Ender Konukoglu", "title": "Joint reconstruction and bias field correction for undersampled MR\n  imaging", "comments": "Published at MICCAI 2020, 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Undersampling the k-space in MRI allows saving precious acquisition time, yet\nresults in an ill-posed inversion problem. Recently, many deep learning\ntechniques have been developed, addressing this issue of recovering the fully\nsampled MR image from the undersampled data. However, these learning based\nschemes are susceptible to differences between the training data and the image\nto be reconstructed at test time. One such difference can be attributed to the\nbias field present in MR images, caused by field inhomogeneities and coil\nsensitivities. In this work, we address the sensitivity of the reconstruction\nproblem to the bias field and propose to model it explicitly in the\nreconstruction, in order to decrease this sensitivity. To this end, we use an\nunsupervised learning based reconstruction algorithm as our basis and combine\nit with a N4-based bias field estimation method, in a joint optimization\nscheme. We use the HCP dataset as well as in-house measured images for the\nevaluations. We show that the proposed method improves the reconstruction\nquality, both visually and in terms of RMSE.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 12:58:34 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Gaillochet", "M\u00e9lanie", ""], ["Tezcan", "Kerem C.", ""], ["Konukoglu", "Ender", ""]]}, {"id": "2007.13140", "submitter": "Wenyang Wang", "authors": "Wenyang Wang, Dongchu Sun, Zhuoqiong He", "title": "Fully Bayesian Analysis of the Relevance Vector Machine Classification\n  for Imbalanced Data", "comments": "24 Pages, 3 figures, preprint to submit to Electronic Journal of\n  Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Relevance Vector Machine (RVM) is a supervised learning algorithm extended\nfrom Support Vector Machine (SVM) based on the Bayesian sparsity model.\nCompared with the regression problem, RVM classification is difficult to be\nconducted because there is no closed-form solution for the weight parameter\nposterior. Original RVM classification algorithm used Newton's method in\noptimization to obtain the mode of weight parameter posterior then approximated\nit by a Gaussian distribution in Laplace's method. It would work but just\napplied the frequency methods in a Bayesian framework. This paper proposes a\nGeneric Bayesian approach for the RVM classification. We conjecture that our\nalgorithm achieves convergent estimates of the quantities of interest compared\nwith the nonconvergent estimates of the original RVM classification algorithm.\nFurthermore, a Fully Bayesian approach with the hierarchical hyperprior\nstructure for RVM classification is proposed, which improves the classification\nperformance, especially in the imbalanced data problem. By the numeric studies,\nour proposed algorithms obtain high classification accuracy rates. The Fully\nBayesian hierarchical hyperprior method outperforms the Generic one for the\nimbalanced data classification.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 14:53:36 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Wang", "Wenyang", ""], ["Sun", "Dongchu", ""], ["He", "Zhuoqiong", ""]]}, {"id": "2007.13156", "submitter": "Anthony Bagnall Dr", "authors": "Alejandro Pasos Ruiz, Michael Flynn and Anthony Bagnall", "title": "Benchmarking Multivariate Time Series Classification Algorithms", "comments": "Data Min Knowl Disc (2020)", "journal-ref": null, "doi": "10.1007/s10618-020-00727-3", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time Series Classification (TSC) involved building predictive models for a\ndiscrete target variable from ordered, real valued, attributes. Over recent\nyears, a new set of TSC algorithms have been developed which have made\nsignificant improvement over the previous state of the art. The main focus has\nbeen on univariate TSC, i.e. the problem where each case has a single series\nand a class label. In reality, it is more common to encounter multivariate TSC\n(MTSC) problems where multiple series are associated with a single label.\nDespite this, much less consideration has been given to MTSC than the\nunivariate case. The UEA archive of 30 MTSC problems released in 2018 has made\ncomparison of algorithms easier. We review recently proposed bespoke MTSC\nalgorithms based on deep learning, shapelets and bag of words approaches. The\nsimplest approach to MTSC is to ensemble univariate classifiers over the\nmultivariate dimensions. We compare the bespoke algorithms to these dimension\nindependent approaches on the 26 of the 30 MTSC archive problems where the data\nare all of equal length. We demonstrate that the independent ensemble of\nHIVE-COTE classifiers is the most accurate, but that, unlike with univariate\nclassification, dynamic time warping is still competitive at MTSC.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 15:56:40 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Ruiz", "Alejandro Pasos", ""], ["Flynn", "Michael", ""], ["Bagnall", "Anthony", ""]]}, {"id": "2007.13171", "submitter": "Elizabeth Newman", "authors": "Elizabeth Newman, Lars Ruthotto, Joseph Hart, Bart van Bloemen\n  Waanders", "title": "Train Like a (Var)Pro: Efficient Training of Neural Networks with\n  Variable Projection", "comments": "33 pages, 14 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have achieved state-of-the-art performance across\na variety of traditional machine learning tasks, e.g., speech recognition,\nimage classification, and segmentation. The ability of DNNs to efficiently\napproximate high-dimensional functions has also motivated their use in\nscientific applications, e.g., to solve partial differential equations (PDE)\nand to generate surrogate models. In this paper, we consider the supervised\ntraining of DNNs, which arises in many of the above applications. We focus on\nthe central problem of optimizing the weights of the given DNN such that it\naccurately approximates the relation between observed input and target data.\nDevising effective solvers for this optimization problem is notoriously\nchallenging due to the large number of weights, non-convexity, data-sparsity,\nand non-trivial choice of hyperparameters. To solve the optimization problem\nmore efficiently, we propose the use of variable projection (VarPro), a method\noriginally designed for separable nonlinear least-squares problems. Our main\ncontribution is the Gauss-Newton VarPro method (GNvpro) that extends the reach\nof the VarPro idea to non-quadratic objective functions, most notably,\ncross-entropy loss functions arising in classification. These extensions make\nGNvpro applicable to all training problems that involve a DNN whose last layer\nis an affine mapping, which is common in many state-of-the-art architectures.\nIn our four numerical experiments from surrogate modeling, segmentation, and\nclassification GNvpro solves the optimization problem more efficiently than\ncommonly-used stochastic gradient descent (SGD) schemes. Also, GNvpro finds\nsolutions that generalize well, and in all but one example better than\nwell-tuned SGD methods, to unseen data points.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 16:29:39 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 22:09:21 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Newman", "Elizabeth", ""], ["Ruthotto", "Lars", ""], ["Hart", "Joseph", ""], ["Waanders", "Bart van Bloemen", ""]]}, {"id": "2007.13185", "submitter": "Neophytos Charalambides Mr", "authors": "Neophytos Charalambides", "title": "Dimensionality Reduction for $k$-means Clustering", "comments": "20 pages, 1 table, expository", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a study on how to effectively reduce the dimensions of the\n$k$-means clustering problem, so that provably accurate approximations are\nobtained. Four algorithms are presented, two \\textit{feature selection} and two\n\\textit{feature extraction} based algorithms, all of which are randomized.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 17:31:44 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Charalambides", "Neophytos", ""]]}, {"id": "2007.13197", "submitter": "Andrea Passerini", "authors": "Luca Di Liello, Pierfrancesco Ardino, Jacopo Gobbi, Paolo Morettin,\n  Stefano Teso, Andrea Passerini", "title": "Efficient Generation of Structured Objects with Constrained Adversarial\n  Networks", "comments": "34th Conference on Neural Information Processing Systems (NeurIPS\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) struggle to generate structured\nobjects like molecules and game maps. The issue is that structured objects must\nsatisfy hard requirements (e.g., molecules must be chemically valid) that are\ndifficult to acquire from examples alone. As a remedy, we propose Constrained\nAdversarial Networks (CANs), an extension of GANs in which the constraints are\nembedded into the model during training. This is achieved by penalizing the\ngenerator proportionally to the mass it allocates to invalid structures. In\ncontrast to other generative models, CANs support efficient inference of valid\nstructures (with high probability) and allows to turn on and off the learned\nconstraints at inference time. CANs handle arbitrary logical constraints and\nleverage knowledge compilation techniques to efficiently evaluate the\ndisagreement between the model and the constraints. Our setup is further\nextended to hybrid logical-neural constraints for capturing very complex\nconstraints, like graph reachability. An extensive empirical analysis shows\nthat CANs efficiently generate valid structures that are both high-quality and\nnovel.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 19:04:37 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2020 20:54:27 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Di Liello", "Luca", ""], ["Ardino", "Pierfrancesco", ""], ["Gobbi", "Jacopo", ""], ["Morettin", "Paolo", ""], ["Teso", "Stefano", ""], ["Passerini", "Andrea", ""]]}, {"id": "2007.13218", "submitter": "Denise Rava", "authors": "Denise Rava and Jelena Bradic", "title": "DeepHazard: neural network for time-varying risks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prognostic models in survival analysis are aimed at understanding the\nrelationship between patients' covariates and the distribution of survival\ntime. Traditionally, semi-parametric models, such as the Cox model, have been\nassumed. These often rely on strong proportionality assumptions of the hazard\nthat might be violated in practice. Moreover, they do not often include\ncovariate information updated over time. We propose a new flexible method for\nsurvival prediction: DeepHazard, a neural network for time-varying risks. Our\napproach is tailored for a wide range of continuous hazards forms, with the\nonly restriction of being additive in time. A flexible implementation, allowing\ndifferent optimization methods, along with any norm penalty, is developed.\nNumerical examples illustrate that our approach outperforms existing\nstate-of-the-art methodology in terms of predictive capability evaluated\nthrough the C-index metric. The same is revealed on the popular real datasets\nas METABRIC, GBSG, and ACTG.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 21:01:49 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 17:14:50 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Rava", "Denise", ""], ["Bradic", "Jelena", ""]]}, {"id": "2007.13221", "submitter": "Cong Xie", "authors": "Cong Xie, Shuai Zheng, Oluwasanmi Koyejo, Indranil Gupta, Mu Li,\n  Haibin Lin", "title": "CSER: Communication-efficient SGD with Error Reset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scalability of Distributed Stochastic Gradient Descent (SGD) is today\nlimited by communication bottlenecks. We propose a novel SGD variant:\nCommunication-efficient SGD with Error Reset, or CSER. The key idea in CSER is\nfirst a new technique called \"error reset\" that adapts arbitrary compressors\nfor SGD, producing bifurcated local models with periodic reset of resulting\nlocal residual errors. Second we introduce partial synchronization for both the\ngradients and the models, leveraging advantages from them. We prove the\nconvergence of CSER for smooth non-convex problems. Empirical results show that\nwhen combined with highly aggressive compressors, the CSER algorithms\naccelerate the distributed training by nearly 10x for CIFAR-100, and by 4.5x\nfor ImageNet.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 21:23:31 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 20:28:58 GMT"}, {"version": "v3", "created": "Sat, 5 Dec 2020 00:06:28 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Xie", "Cong", ""], ["Zheng", "Shuai", ""], ["Koyejo", "Oluwasanmi", ""], ["Gupta", "Indranil", ""], ["Li", "Mu", ""], ["Lin", "Haibin", ""]]}, {"id": "2007.13239", "submitter": "Aravind Ashok Nair", "authors": "Aravind Nair, Avijit Roy, Karl Meinke", "title": "funcGNN: A Graph Neural Network Approach to Program Similarity", "comments": "11 pages, 8 figures, 3 tables", "journal-ref": null, "doi": "10.1145/3382494.3410675", "report-no": null, "categories": "cs.LG cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Program similarity is a fundamental concept, central to the solution of\nsoftware engineering tasks such as software plagiarism, clone identification,\ncode refactoring and code search. Accurate similarity estimation between\nprograms requires an in-depth understanding of their structure, semantics and\nflow. A control flow graph (CFG), is a graphical representation of a program\nwhich captures its logical control flow and hence its semantics. A common\napproach is to estimate program similarity by analysing CFGs using graph\nsimilarity measures, e.g. graph edit distance (GED). However, graph edit\ndistance is an NP-hard problem and computationally expensive, making the\napplication of graph similarity techniques to complex software programs\nimpractical. This study intends to examine the effectiveness of graph neural\nnetworks to estimate program similarity, by analysing the associated control\nflow graphs. We introduce funcGNN, which is a graph neural network trained on\nlabeled CFG pairs to predict the GED between unseen program pairs by utilizing\nan effective embedding vector. To our knowledge, this is the first time graph\nneural networks have been applied on labeled CFGs for estimating the similarity\nbetween high-level language programs. Results: We demonstrate the effectiveness\nof funcGNN to estimate the GED between programs and our experimental analysis\ndemonstrates how it achieves a lower error rate (0.00194), with faster (23\ntimes faster than the quickest traditional GED approximation method) and better\nscalability compared with the state of the art methods. funcGNN posses the\ninductive learning ability to infer program structure and generalise to unseen\nprograms. The graph embedding of a program proposed by our methodology could be\napplied to several related software engineering problems (such as code\nplagiarism and clone identification) thus opening multiple research directions.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 23:16:24 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 14:27:33 GMT"}, {"version": "v3", "created": "Thu, 30 Jul 2020 17:19:16 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Nair", "Aravind", ""], ["Roy", "Avijit", ""], ["Meinke", "Karl", ""]]}, {"id": "2007.13241", "submitter": "Tim Roughgarden", "authors": "Tim Roughgarden", "title": "Beyond the Worst-Case Analysis of Algorithms (Introduction)", "comments": "Chapter 1 of the book Beyond the Worst-Case Analysis of Algorithms,\n  edited by Tim Roughgarden and published by Cambridge University Press (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the primary goals of the mathematical analysis of algorithms is to\nprovide guidance about which algorithm is the \"best\" for solving a given\ncomputational problem. Worst-case analysis summarizes the performance profile\nof an algorithm by its worst performance on any input of a given size,\nimplicitly advocating for the algorithm with the best-possible worst-case\nperformance. Strong worst-case guarantees are the holy grail of algorithm\ndesign, providing an application-agnostic certification of an algorithm's\nrobustly good performance. However, for many fundamental problems and\nperformance measures, such guarantees are impossible and a more nuanced\nanalysis approach is called for. This chapter surveys several alternatives to\nworst-case analysis that are discussed in detail later in the book.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 23:18:19 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Roughgarden", "Tim", ""]]}, {"id": "2007.13242", "submitter": "Renkun Ni", "authors": "Renkun Ni, Hong-min Chu, Oscar Casta\\~neda, Ping-yeh Chiang, Christoph\n  Studer, Tom Goldstein", "title": "WrapNet: Neural Net Inference with Ultra-Low-Resolution Arithmetic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-resolution neural networks represent both weights and activations with\nfew bits, drastically reducing the multiplication complexity. Nonetheless,\nthese products are accumulated using high-resolution (typically 32-bit)\nadditions, an operation that dominates the arithmetic complexity of inference\nwhen using extreme quantization (e.g., binary weights). To further optimize\ninference, we propose a method that adapts neural networks to use\nlow-resolution (8-bit) additions in the accumulators, achieving classification\naccuracy comparable to their 32-bit counterparts. We achieve resilience to\nlow-resolution accumulation by inserting a cyclic activation layer, as well as\nan overflow penalty regularizer. We demonstrate the efficacy of our approach on\nboth software and hardware platforms.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 23:18:38 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Ni", "Renkun", ""], ["Chu", "Hong-min", ""], ["Casta\u00f1eda", "Oscar", ""], ["Chiang", "Ping-yeh", ""], ["Studer", "Christoph", ""], ["Goldstein", "Tom", ""]]}, {"id": "2007.13250", "submitter": "Yichen Zhang", "authors": "Yichen Zhang and Jianzhe Liu and Feng Qiu and Tianqi Hong and Rui Yao", "title": "Deep Active Learning for Solvability Prediction in Power Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional methods for solvability region analysis can only have inner\napproximations with inconclusive conservatism. Machine learning methods have\nbeen proposed to approach the real region. In this letter, we propose a deep\nactive learning framework for power system solvability prediction. Compared\nwith the passive learning methods where the training is performed after all\ninstances are labeled, the active learning selects most informative instances\nto be label and therefore significantly reduce the size of labeled dataset for\ntraining. In the active learning framework, the acquisition functions, which\ncorrespond to different sampling strategies, are defined in terms of the\non-the-fly posterior probability from the classifier. The IEEE 39-bus system is\nemployed to validate the proposed framework, where a two-dimensional case is\nillustrated to visualize the effectiveness of the sampling method followed by\nthe full-dimensional numerical experiments.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 00:13:09 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 07:30:08 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Zhang", "Yichen", ""], ["Liu", "Jianzhe", ""], ["Qiu", "Feng", ""], ["Hong", "Tianqi", ""], ["Yao", "Rui", ""]]}, {"id": "2007.13288", "submitter": "Stefan Steinerberger", "authors": "Stefan Steinerberger", "title": "On the Regularization Effect of Stochastic Gradient Descent applied to\n  Least Squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the behavior of stochastic gradient descent applied to $\\|Ax -b\n\\|_2^2 \\rightarrow \\min$ for invertible $A \\in \\mathbb{R}^{n \\times n}$. We\nshow that there is an explicit constant $c_{A}$ depending (mildly) on $A$ such\nthat $$ \\mathbb{E} ~\\left\\| Ax_{k+1}-b\\right\\|^2_{2} \\leq \\left(1 +\n\\frac{c_{A}}{\\|A\\|_F^2}\\right) \\left\\|A x_k -b \\right\\|^2_{2} -\n\\frac{2}{\\|A\\|_F^2} \\left\\|A^T A (x_k - x)\\right\\|^2_{2}.$$ This is a curious\ninequality: the last term has one more matrix applied to the residual $u_k - u$\nthan the remaining terms: if $x_k - x$ is mainly comprised of large singular\nvectors, stochastic gradient descent leads to a quick regularization. For\nsymmetric matrices, this inequality has an extension to higher-order Sobolev\nspaces. This explains a (known) regularization phenomenon: an energy cascade\nfrom large singular values to small singular values smoothes.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 03:01:09 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 20:34:27 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Steinerberger", "Stefan", ""]]}, {"id": "2007.13322", "submitter": "Sauptik Dhar", "authors": "Sauptik Dhar, Unmesh Kurup, Mohak Shah", "title": "Stabilizing Bi-Level Hyperparameter Optimization using Moreau-Yosida\n  Regularization", "comments": "AutoML, Hyperparameter Optimization (HPO), Bi-Level Optimization,\n  Alternating Direction Method of Multipliers (ADMM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research proposes to use the Moreau-Yosida envelope to stabilize the\nconvergence behavior of bi-level Hyperparameter optimization solvers, and\nintroduces the new algorithm called Moreau-Yosida regularized Hyperparameter\nOptimization (MY-HPO) algorithm. Theoretical analysis on the correctness of the\nMY-HPO solution and initial convergence analysis is also provided. Our\nempirical results show significant improvement in loss values for a fixed\ncomputation budget, compared to the state-of-art bi-level HPO solvers.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 06:47:35 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Dhar", "Sauptik", ""], ["Kurup", "Unmesh", ""], ["Shah", "Mohak", ""]]}, {"id": "2007.13323", "submitter": "Ayaka Sakata", "authors": "Ayaka Sakata", "title": "Active pooling design in group testing based on Bayesian posterior\n  prediction", "comments": null, "journal-ref": "Phys. Rev. E 103, 022110 (2021)", "doi": "10.1103/PhysRevE.103.022110", "report-no": null, "categories": "stat.ML cond-mat.stat-mech cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In identifying infected patients in a population, group testing is an\neffective method to reduce the number of tests and correct the test errors. In\nthe group testing procedure, tests are performed on pools of specimens\ncollected from patients, where the number of pools is lower than that of\npatients. The performance of group testing heavily depends on the design of\npools and algorithms that are used in inferring the infected patients from the\ntest outcomes. In this paper, an adaptive design method of pools based on the\npredictive distribution is proposed in the framework of Bayesian inference. The\nproposed method executed using the belief propagation algorithm results in more\naccurate identification of the infected patients, as compared to the group\ntesting performed on random pools determined in advance.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 06:50:24 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 08:30:30 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Sakata", "Ayaka", ""]]}, {"id": "2007.13351", "submitter": "Subhadip Maji", "authors": "Subhadip Maji, Raghav Bali, Sree Harsha Ankem and Kishore V Ayyadevara", "title": "A Simple and Interpretable Predictive Model for Healthcare", "comments": "7 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning based models are currently dominating most state-of-the-art\nsolutions for disease prediction. Existing works employ RNNs along with\nmultiple levels of attention mechanisms to provide interpretability. These deep\nlearning models, with trainable parameters running into millions, require huge\namounts of compute and data to train and deploy. These requirements are\nsometimes so huge that they render usage of such models as unfeasible. We\naddress these challenges by developing a simpler yet interpretable non-deep\nlearning based model for application to EHR data. We model and showcase our\nwork's results on the task of predicting first occurrence of a diagnosis, often\noverlooked in existing works. We push the capabilities of a tree based model\nand come up with a strong baseline for more sophisticated models. Its\nperformance shows an improvement over deep learning based solutions (both, with\nand without the first-occurrence constraint) all the while maintaining\ninterpretability.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 08:13:37 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Maji", "Subhadip", ""], ["Bali", "Raghav", ""], ["Ankem", "Sree Harsha", ""], ["Ayyadevara", "Kishore V", ""]]}, {"id": "2007.13382", "submitter": "Fela Winkelmolen", "authors": "Fela Winkelmolen, Nikita Ivkin, H. Furkan Bozkurt, Zohar Karnin", "title": "Practical and sample efficient zero-shot HPO", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Zero-shot hyperparameter optimization (HPO) is a simple yet effective use of\ntransfer learning for constructing a small list of hyperparameter (HP)\nconfigurations that complement each other. That is to say, for any given\ndataset, at least one of them is expected to perform well. Current techniques\nfor obtaining this list are computationally expensive as they rely on running\ntraining jobs on a diverse collection of datasets and a large collection of\nrandomly drawn HPs. This cost is especially problematic in environments where\nthe space of HPs is regularly changing due to new algorithm versions, or\nchanging architectures of deep networks. We provide an overview of available\napproaches and introduce two novel techniques to handle the problem. The first\nis based on a surrogate model and adaptively chooses pairs of dataset,\nconfiguration to query. The second, for settings where finding, tuning and\ntesting a surrogate model is problematic, is a multi-fidelity technique\ncombining HyperBand with submodular optimization. We benchmark our methods\nexperimentally on five tasks (XGBoost, LightGBM, CatBoost, MLP and AutoML) and\nshow significant improvement in accuracy compared to standard zero-shot HPO\nwith the same training budget. In addition to contributing new algorithms, we\nprovide an extensive study of the zero-shot HPO technique resulting in (1)\ndefault hyper-parameters for popular algorithms that would benefit the\ncommunity using them, (2) massive lookup tables to further the research of\nhyper-parameter tuning.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 08:56:55 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Winkelmolen", "Fela", ""], ["Ivkin", "Nikita", ""], ["Bozkurt", "H. Furkan", ""], ["Karnin", "Zohar", ""]]}, {"id": "2007.13384", "submitter": "Manoj Vemparala", "authors": "Alexander Frickenstein, Manoj-Rohit Vemparala, Nael Fasfous, Laura\n  Hauenschild, Naveen-Shankar Nagaraja, Christian Unger, Walter Stechele", "title": "ALF: Autoencoder-based Low-rank Filter-sharing for Efficient\n  Convolutional Neural Networks", "comments": "Accepted by DAC'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Closing the gap between the hardware requirements of state-of-the-art\nconvolutional neural networks and the limited resources constraining embedded\napplications is the next big challenge in deep learning research. The\ncomputational complexity and memory footprint of such neural networks are\ntypically daunting for deployment in resource constrained environments. Model\ncompression techniques, such as pruning, are emphasized among other\noptimization methods for solving this problem. Most existing techniques require\ndomain expertise or result in irregular sparse representations, which increase\nthe burden of deploying deep learning applications on embedded hardware\naccelerators. In this paper, we propose the autoencoder-based low-rank\nfilter-sharing technique technique (ALF). When applied to various networks, ALF\nis compared to state-of-the-art pruning methods, demonstrating its efficient\ncompression capabilities on theoretical metrics as well as on an accurate,\ndeterministic hardware-model. In our experiments, ALF showed a reduction of\n70\\% in network parameters, 61\\% in operations and 41\\% in execution time, with\nminimal loss in accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 09:01:22 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Frickenstein", "Alexander", ""], ["Vemparala", "Manoj-Rohit", ""], ["Fasfous", "Nael", ""], ["Hauenschild", "Laura", ""], ["Nagaraja", "Naveen-Shankar", ""], ["Unger", "Christian", ""], ["Stechele", "Walter", ""]]}, {"id": "2007.13413", "submitter": "Vijay Pandey", "authors": "Vijay Pandey", "title": "Binary Search and First Order Gradient Based Method for Stochastic\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a novel stochastic optimization method, which uses\nthe binary search technique with first order gradient based optimization\nmethod, called Binary Search Gradient Optimization (BSG) or BiGrad. In this\noptimization setup, a non-convex surface is treated as a set of convex\nsurfaces. In BSG, at first, a region is defined, assuming region is convex. If\nregion is not convex, then the algorithm leaves the region very fast and\ndefines a new one, otherwise, it tries to converge at the optimal point of the\nregion. In BSG, core purpose of binary search is to decide, whether region is\nconvex or not in logarithmic time, whereas, first order gradient based method\nis primarily applied, to define a new region. In this paper, Adam is used as a\nfirst order gradient based method, nevertheless, other methods of this class\nmay also be considered. In deep neural network setup, it handles the problem of\nvanishing and exploding gradient efficiently. We evaluate BSG on the MNIST\nhandwritten digit, IMDB, and CIFAR10 data set, using logistic regression and\ndeep neural networks. We produce more promising results as compared to other\nfirst order gradient based optimization methods. Furthermore, proposed\nalgorithm generalizes significantly better on unseen data as compared to other\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 10:21:20 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Pandey", "Vijay", ""]]}, {"id": "2007.13414", "submitter": "Nupur Aggarwal Ms", "authors": "Nupur Aggarwal, Abhishek Bansal, Kushagra Manglik, Kedar Kulkarni,\n  Vikas Raykar", "title": "Hyper-local sustainable assortment planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assortment planning, an important seasonal activity for any retailer,\ninvolves choosing the right subset of products to stock in each store.While\nexisting approaches only maximize the expected revenue, we propose including\nthe environmental impact too, through the Higg Material Sustainability Index.\nThe trade-off between revenue and environmental impact is balanced through a\nmulti-objective optimization approach, that yields a Pareto-front of optimal\nassortments for merchandisers to choose from. Using the proposed approach on a\nfew product categories of a leading fashion retailer shows that choosing\nassortments with lower environmental impact with a minimal impact on revenue is\npossible.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 10:23:46 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Aggarwal", "Nupur", ""], ["Bansal", "Abhishek", ""], ["Manglik", "Kushagra", ""], ["Kulkarni", "Kedar", ""], ["Raykar", "Vikas", ""]]}, {"id": "2007.13435", "submitter": "Bingbing Xu", "authors": "Bingbing Xu, Junjie Huang, Liang Hou, Huawei Shen, Jinhua Gao, Xueqi\n  Cheng", "title": "Label-Consistency based Graph Neural Networks for Semi-supervised Node\n  Classification", "comments": null, "journal-ref": "SIGIR2020", "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNNs) achieve remarkable success in graph-based\nsemi-supervised node classification, leveraging the information from\nneighboring nodes to improve the representation learning of target node. The\nsuccess of GNNs at node classification depends on the assumption that connected\nnodes tend to have the same label. However, such an assumption does not always\nwork, limiting the performance of GNNs at node classification. In this paper,\nwe propose label-consistency based graph neural network(LC-GNN), leveraging\nnode pairs unconnected but with the same labels to enlarge the receptive field\nof nodes in GNNs. Experiments on benchmark datasets demonstrate the proposed\nLC-GNN outperforms traditional GNNs in graph-based semi-supervised node\nclassification.We further show the superiority of LC-GNN in sparse scenarios\nwith only a handful of labeled nodes.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 11:17:46 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Xu", "Bingbing", ""], ["Huang", "Junjie", ""], ["Hou", "Liang", ""], ["Shen", "Huawei", ""], ["Gao", "Jinhua", ""], ["Cheng", "Xueqi", ""]]}, {"id": "2007.13442", "submitter": "Michal Valko", "authors": "Pierre M\\'enard, Omar Darwiche Domingues, Anders Jonsson, Emilie\n  Kaufmann, Edouard Leurent, Michal Valko", "title": "Fast active learning for pure exploration in reinforcement learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Realistic environments often provide agents with very limited feedback. When\nthe environment is initially unknown, the feedback, in the beginning, can be\ncompletely absent, and the agents may first choose to devote all their effort\non exploring efficiently. The exploration remains a challenge while it has been\naddressed with many hand-tuned heuristics with different levels of generality\non one side, and a few theoretically-backed exploration strategies on the\nother. Many of them are incarnated by intrinsic motivation and in particular\nexplorations bonuses. A common rule of thumb for exploration bonuses is to use\n$1/\\sqrt{n}$ bonus that is added to the empirical estimates of the reward,\nwhere $n$ is a number of times this particular state (or a state-action pair)\nwas visited. We show that, surprisingly, for a pure-exploration objective of\nreward-free exploration, bonuses that scale with $1/n$ bring faster learning\nrates, improving the known upper bounds with respect to the dependence on the\nhorizon $H$. Furthermore, we show that with an improved analysis of the\nstopping time, we can improve by a factor $H$ the sample complexity in the\nbest-policy identification setting, which is another pure-exploration\nobjective, where the environment provides rewards but the agent is not\npenalized for its behavior during the exploration phase.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 11:28:32 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 17:15:28 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["M\u00e9nard", "Pierre", ""], ["Domingues", "Omar Darwiche", ""], ["Jonsson", "Anders", ""], ["Kaufmann", "Emilie", ""], ["Leurent", "Edouard", ""], ["Valko", "Michal", ""]]}, {"id": "2007.13444", "submitter": "Maximilian Bachl", "authors": "Fares Meghdouri, Maximilian Bachl, Tanja Zseby", "title": "EagerNet: Early Predictions of Neural Networks for Computationally\n  Efficient Intrusion Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully Connected Neural Networks (FCNNs) have been the core of most\nstate-of-the-art Machine Learning (ML) applications in recent years and also\nhave been widely used for Intrusion Detection Systems (IDSs). Experimental\nresults from the last years show that generally deeper neural networks with\nmore layers perform better than shallow models. Nonetheless, with the growing\nnumber of layers, obtaining fast predictions with less resources has become a\ndifficult task despite the use of special hardware such as GPUs. We propose a\nnew architecture to detect network attacks with minimal resources. The\narchitecture is able to deal with either binary or multiclass classification\nproblems and trades prediction speed for the accuracy of the network. We\nevaluate our proposal with two different network intrusion detection datasets.\nResults suggest that it is possible to obtain comparable accuracies to simple\nFCNNs without evaluating all layers for the majority of samples, thus obtaining\nearly predictions and saving energy and computational efforts.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 11:31:37 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 16:58:16 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Meghdouri", "Fares", ""], ["Bachl", "Maximilian", ""], ["Zseby", "Tanja", ""]]}, {"id": "2007.13454", "submitter": "Jan Markus Brauner", "authors": "Mrinank Sharma, S\\\"oren Mindermann, Jan Markus Brauner, Gavin Leech,\n  Anna B. Stephenson, Tom\\'a\\v{s} Gaven\\v{c}iak, Jan Kulveit, Yee Whye Teh,\n  Leonid Chindelevitch, Yarin Gal", "title": "How Robust are the Estimated Effects of Nonpharmaceutical Interventions\n  against COVID-19?", "comments": null, "journal-ref": "NeurIPS 2020, Advances in Neural Information Processing Systems 33", "doi": null, "report-no": null, "categories": "stat.AP cs.LG q-bio.PE q-bio.QM stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To what extent are effectiveness estimates of nonpharmaceutical interventions\n(NPIs) against COVID-19 influenced by the assumptions our models make? To\nanswer this question, we investigate 2 state-of-the-art NPI effectiveness\nmodels and propose 6 variants that make different structural assumptions. In\nparticular, we investigate how well NPI effectiveness estimates generalise to\nunseen countries, and their sensitivity to unobserved factors. Models that\naccount for noise in disease transmission compare favourably. We further\nevaluate how robust estimates are to different choices of epidemiological\nparameters and data. Focusing on models that assume transmission noise, we find\nthat previously published results are remarkably robust across these variables.\nFinally, we mathematically ground the interpretation of NPI effectiveness\nestimates when certain common assumptions do not hold.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 11:49:54 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 11:47:54 GMT"}, {"version": "v3", "created": "Sun, 20 Dec 2020 15:35:46 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Sharma", "Mrinank", ""], ["Mindermann", "S\u00f6ren", ""], ["Brauner", "Jan Markus", ""], ["Leech", "Gavin", ""], ["Stephenson", "Anna B.", ""], ["Gaven\u010diak", "Tom\u00e1\u0161", ""], ["Kulveit", "Jan", ""], ["Teh", "Yee Whye", ""], ["Chindelevitch", "Leonid", ""], ["Gal", "Yarin", ""]]}, {"id": "2007.13465", "submitter": "Yossi Adi", "authors": "Felix Kreuk, Joseph Keshet, Yossi Adi", "title": "Self-Supervised Contrastive Learning for Unsupervised Phoneme\n  Segmentation", "comments": "Interspeech 2020 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a self-supervised representation learning model for the task of\nunsupervised phoneme boundary detection. The model is a convolutional neural\nnetwork that operates directly on the raw waveform. It is optimized to identify\nspectral changes in the signal using the Noise-Contrastive Estimation\nprinciple. At test time, a peak detection algorithm is applied over the model\noutputs to produce the final boundaries. As such, the proposed model is trained\nin a fully unsupervised manner with no manual annotations in the form of target\nboundaries nor phonetic transcriptions. We compare the proposed approach to\nseveral unsupervised baselines using both TIMIT and Buckeye corpora. Results\nsuggest that our approach surpasses the baseline models and reaches\nstate-of-the-art performance on both data sets. Furthermore, we experimented\nwith expanding the training set with additional examples from the Librispeech\ncorpus. We evaluated the resulting model on distributions and languages that\nwere not seen during the training phase (English, Hebrew and German) and showed\nthat utilizing additional untranscribed data is beneficial for model\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 12:10:21 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 07:33:37 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Kreuk", "Felix", ""], ["Keshet", "Joseph", ""], ["Adi", "Yossi", ""]]}, {"id": "2007.13481", "submitter": "Ahmed Alaa", "authors": "Ahmed M. Alaa, Mihaela van der Schaar", "title": "Discriminative Jackknife: Quantifying Uncertainty in Deep Learning via\n  Higher-Order Influence Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models achieve high predictive accuracy across a broad spectrum\nof tasks, but rigorously quantifying their predictive uncertainty remains\nchallenging. Usable estimates of predictive uncertainty should (1) cover the\ntrue prediction targets with high probability, and (2) discriminate between\nhigh- and low-confidence prediction instances. Existing methods for uncertainty\nquantification are based predominantly on Bayesian neural networks; these may\nfall short of (1) and (2) -- i.e., Bayesian credible intervals do not guarantee\nfrequentist coverage, and approximate posterior inference undermines\ndiscriminative accuracy. In this paper, we develop the discriminative jackknife\n(DJ), a frequentist procedure that utilizes influence functions of a model's\nloss functional to construct a jackknife (or leave-one-out) estimator of\npredictive confidence intervals. The DJ satisfies (1) and (2), is applicable to\na wide range of deep learning models, is easy to implement, and can be applied\nin a post-hoc fashion without interfering with model training or compromising\nits accuracy. Experiments demonstrate that DJ performs competitively compared\nto existing Bayesian and non-Bayesian regression baselines.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 13:36:52 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Alaa", "Ahmed M.", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "2007.13487", "submitter": "Md. Abu Bakr Siddique", "authors": "Shadman Sakib, Md. Abu Bakr Siddique, Md. Abdur Rahman", "title": "Performance Evaluation of t-SNE and MDS Dimensionality Reduction\n  Techniques with KNN, ENN and SVM Classifiers", "comments": "2020 IEEE Region 10 Symposium (TENSYMP), 5-7 June 2020, Dhaka,\n  Bangladesh", "journal-ref": "2020 IEEE Region 10 Symposium (TENSYMP)", "doi": "10.1109/TENSYMP50017.2020.9230983", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The central goal of this paper is to establish two commonly available\ndimensionality reduction (DR) methods i.e. t-distributed Stochastic Neighbor\nEmbedding (t-SNE) and Multidimensional Scaling (MDS) in Matlab and to observe\ntheir application in several datasets. These DR techniques are applied to nine\ndifferent datasets namely CNAE9, Segmentation, Seeds, Pima Indians diabetes,\nParkinsons, Movement Libras, Mammographic Masses, Knowledge, and Ionosphere\nacquired from UCI machine learning repository. By applying t-SNE and MDS\nalgorithms, each dataset is transformed to the half of its original dimension\nby eliminating unnecessary features from the datasets. Subsequently, these\ndatasets with reduced dimensions are fed into three supervised classification\nalgorithms for classification. These classification algorithms are K Nearest\nNeighbors (KNN), Extended Nearest Neighbors (ENN), and Support Vector Machine\n(SVM). Again, all these algorithms are implemented in Matlab. The training and\ntest data ratios are maintained as ninety percent: ten percent for each\ndataset. Upon accuracy observation, the efficiency for every dimensionality\ntechnique with availed classification algorithms is analyzed and the\nperformance of each classifier is evaluated.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 08:13:42 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Sakib", "Shadman", ""], ["Siddique", "Md. Abu Bakr", ""], ["Rahman", "Md. Abdur", ""]]}, {"id": "2007.13489", "submitter": "Saavan Patel", "authors": "Saavan Patel, Philip Canoza, Sayeef Salahuddin", "title": "Logically Synthesized, Hardware-Accelerated, Restricted Boltzmann\n  Machines for Combinatorial Optimization and Integer Factorization", "comments": "14 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Restricted Boltzmann Machine (RBM) is a stochastic neural network capable\nof solving a variety of difficult tasks such as NP-Hard combinatorial\noptimization problems and integer factorization. The RBM architecture is also\nvery compact; requiring very few weights and biases. This, along with its\nsimple, parallelizable sampling algorithm for finding the ground state of such\nproblems, makes the RBM amenable to hardware acceleration. However, training of\nthe RBM on these problems can pose a significant challenge, as the training\nalgorithm tends to fail for large problem sizes and efficient mappings can be\nhard to find. Here, we propose a method of combining RBMs together that avoids\nthe need to train large problems in their full form. We also propose methods\nfor making the RBM more hardware amenable, allowing the algorithm to be\nefficiently mapped to an FPGA-based accelerator. Using this accelerator, we are\nable to show hardware accelerated factorization of 16 bit numbers with high\naccuracy with a speed improvement of 10000x and a power improvement of 32x.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 17:44:17 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 17:06:58 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Patel", "Saavan", ""], ["Canoza", "Philip", ""], ["Salahuddin", "Sayeef", ""]]}, {"id": "2007.13492", "submitter": "Daniel Buades Marcos", "authors": "Daniel Buades Marcos, Soumaya Yacout, Said Berriah", "title": "Self-Supervised Encoder for Fault Prediction in Electrochemical Cells", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting faults before they occur helps to avoid potential safety hazards.\nFurthermore, planning the required maintenance actions in advance reduces\noperation costs. In this article, the focus is on electrochemical cells. In\norder to predict a cell's fault, the typical approach is to estimate the\nexpected voltage that a healthy cell would present and compare it with the\ncell's measured voltage in real-time. This approach is possible because, when a\nfault is about to happen, the cell's measured voltage differs from the one\nexpected for the same operating conditions. However, estimating the expected\nvoltage is challenging, as the voltage of a healthy cell is also affected by\nits degradation -- an unknown parameter. Expert-defined parametric models are\ncurrently used for this estimation task. Instead, we propose the use of a\nneural network model based on an encoder-decoder architecture. The network\nreceives the operating conditions as input. The encoder's task is to find a\nfaithful representation of the cell's degradation and to pass it to the\ndecoder, which in turn predicts the expected cell's voltage. As no labeled\ndegradation data is given to the network, we consider our approach to be a\nself-supervised encoder. Results show that we were able to predict the voltage\nof multiple cells while diminishing the prediction error that was obtained by\nthe parametric models by 53%. This improvement enabled our network to predict a\nfault 31 hours before it happened, a 64% increase in reaction time compared to\nthe parametric model. Moreover, the output of the encoder can be plotted,\nadding interpretability to the neural network model.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 21:21:36 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Marcos", "Daniel Buades", ""], ["Yacout", "Soumaya", ""], ["Berriah", "Said", ""]]}, {"id": "2007.13494", "submitter": "Ionel Popescu", "authors": "Marian Petrica and Radu D. Stochitoiu and Marius Leordeanu and Ionel\n  Popescu", "title": "A regime switching on Covid19 analysis and prediction in Romania", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a regime separation for the analysis of Covid19 on\nRomania combined with mathematical models of SIR and SIRD. The main regimes we\nstudy are, the free spread of the virus, the quarantine and partial relaxation\nand the last one is the relaxation regime. The main model we use is SIR which\nis a classical model, but because we can not fully trust the numbers of\ninfected or recovered we base our analysis on the number of deceased people\nwhich is more reliable. To actually deal with this we introduce a simple\nmodification of the SIR model to account for the deceased separately. This in\nturn will be our base for fitting the parameters. The estimation of the\nparameters is done in two steps. The first one consists in training a neural\nnetwork based on SIR models to detect the regime changes. Once this is done we\nfit the main parameters of the SIRD model using a grid search. At the end, we\nmake some predictions on what the evolution will be in a timeframe of a month\nwith the fitted parameters.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 12:37:32 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 20:24:16 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Petrica", "Marian", ""], ["Stochitoiu", "Radu D.", ""], ["Leordeanu", "Marius", ""], ["Popescu", "Ionel", ""]]}, {"id": "2007.13505", "submitter": "G\\\"unter Klambauer", "authors": "Michael Widrich, Bernhard Sch\\\"afl, Hubert Ramsauer, Milena\n  Pavlovi\\'c, Lukas Gruber, Markus Holzleitner, Johannes Brandstetter, Geir\n  Kjetil Sandve, Victor Greiff, Sepp Hochreiter, G\\\"unter Klambauer", "title": "Modern Hopfield Networks and Attention for Immune Repertoire\n  Classification", "comments": "10 pages (+appendix); Source code and datasets:\n  https://github.com/ml-jku/DeepRC", "journal-ref": "Advances in Neural Information Processing Systems 33 (NeurIPS\n  2020)", "doi": null, "report-no": null, "categories": "cs.LG q-bio.BM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central mechanism in machine learning is to identify, store, and recognize\npatterns. How to learn, access, and retrieve such patterns is crucial in\nHopfield networks and the more recent transformer architectures. We show that\nthe attention mechanism of transformer architectures is actually the update\nrule of modern Hopfield networks that can store exponentially many patterns. We\nexploit this high storage capacity of modern Hopfield networks to solve a\nchallenging multiple instance learning (MIL) problem in computational biology:\nimmune repertoire classification. Accurate and interpretable machine learning\nmethods solving this problem could pave the way towards new vaccines and\ntherapies, which is currently a very relevant research topic intensified by the\nCOVID-19 crisis. Immune repertoire classification based on the vast number of\nimmunosequences of an individual is a MIL problem with an unprecedentedly\nmassive number of instances, two orders of magnitude larger than currently\nconsidered problems, and with an extremely low witness rate. In this work, we\npresent our novel method DeepRC that integrates transformer-like attention, or\nequivalently modern Hopfield networks, into deep learning architectures for\nmassive MIL such as immune repertoire classification. We demonstrate that\nDeepRC outperforms all other methods with respect to predictive performance on\nlarge-scale experiments, including simulated and real-world virus infection\ndata, and enables the extraction of sequence motifs that are connected to a\ngiven disease class. Source code and datasets: https://github.com/ml-jku/DeepRC\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 20:35:46 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Widrich", "Michael", ""], ["Sch\u00e4fl", "Bernhard", ""], ["Ramsauer", "Hubert", ""], ["Pavlovi\u0107", "Milena", ""], ["Gruber", "Lukas", ""], ["Holzleitner", "Markus", ""], ["Brandstetter", "Johannes", ""], ["Sandve", "Geir Kjetil", ""], ["Greiff", "Victor", ""], ["Hochreiter", "Sepp", ""], ["Klambauer", "G\u00fcnter", ""]]}, {"id": "2007.13512", "submitter": "Adrien Morisot", "authors": "Adrien Morisot", "title": "Add a SideNet to your MainNet", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the performance and popularity of deep neural networks has increased, so\ntoo has their computational cost. There are many effective techniques for\nreducing a network's computational footprint (quantisation, pruning, knowledge\ndistillation), but these lead to models whose computational cost is the same\nregardless of their input. Our human reaction times vary with the complexity of\nthe tasks we perform: easier tasks (e.g. telling apart dogs from boat) are\nexecuted much faster than harder ones (e.g. telling apart two similar looking\nbreeds of dogs). Driven by this observation, we develop a method for adaptive\nnetwork complexity by attaching a small classification layer, which we call\nSideNet, to a large pretrained network, which we call MainNet. Given an input,\nthe SideNet returns a classification if its confidence level, obtained via\nsoftmax, surpasses a user determined threshold, and only passes it along to the\nlarge MainNet for further processing if its confidence is too low. This allows\nus to flexibly trade off the network's performance with its computational cost.\nExperimental results show that simple single hidden layer perceptron SideNets\nadded onto pretrained ResNet and BERT MainNets allow for substantial decreases\nin compute with minimal drops in performance on image and text classification\ntasks. We also highlight three other desirable properties of our method, namely\nthat the classifications obtained by SideNets are calibrated, complementary to\nother compute reduction techniques, and that they enable the easy exploration\nof compute accuracy space.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 19:25:32 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Morisot", "Adrien", ""]]}, {"id": "2007.13518", "submitter": "Chaoyang He", "authors": "Chaoyang He, Songze Li, Jinhyun So, Xiao Zeng, Mi Zhang, Hongyi Wang,\n  Xiaoyang Wang, Praneeth Vepakomma, Abhishek Singh, Hang Qiu, Xinghua Zhu,\n  Jianzong Wang, Li Shen, Peilin Zhao, Yan Kang, Yang Liu, Ramesh Raskar, Qiang\n  Yang, Murali Annavaram, Salman Avestimehr", "title": "FedML: A Research Library and Benchmark for Federated Machine Learning", "comments": "This is FedML white paper V3. Homepage: https://fedml.ai; GitHub:\n  https://github.com/FedML-AI/FedML; In V3, More advanced algorithms and IoT\n  device training are supported, please check here:\n  https://github.com/FedML-AI/FedML/blob/master/fedml_iot/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated learning (FL) is a rapidly growing research field in machine\nlearning. However, existing FL libraries cannot adequately support diverse\nalgorithmic development; inconsistent dataset and model usage make fair\nalgorithm comparison challenging. In this work, we introduce FedML, an open\nresearch library and benchmark to facilitate FL algorithm development and fair\nperformance comparison. FedML supports three computing paradigms: on-device\ntraining for edge devices, distributed computing, and single-machine\nsimulation. FedML also promotes diverse algorithmic research with flexible and\ngeneric API design and comprehensive reference baseline implementations\n(optimizer, models, and datasets). We hope FedML could provide an efficient and\nreproducible means for developing and evaluating FL algorithms that would\nbenefit the FL research community. We maintain the source code, documents, and\nuser community at https://fedml.ai.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 13:02:08 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 03:38:00 GMT"}, {"version": "v3", "created": "Thu, 5 Nov 2020 13:41:12 GMT"}, {"version": "v4", "created": "Sun, 8 Nov 2020 19:34:25 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["He", "Chaoyang", ""], ["Li", "Songze", ""], ["So", "Jinhyun", ""], ["Zeng", "Xiao", ""], ["Zhang", "Mi", ""], ["Wang", "Hongyi", ""], ["Wang", "Xiaoyang", ""], ["Vepakomma", "Praneeth", ""], ["Singh", "Abhishek", ""], ["Qiu", "Hang", ""], ["Zhu", "Xinghua", ""], ["Wang", "Jianzong", ""], ["Shen", "Li", ""], ["Zhao", "Peilin", ""], ["Kang", "Yan", ""], ["Liu", "Yang", ""], ["Raskar", "Ramesh", ""], ["Yang", "Qiang", ""], ["Annavaram", "Murali", ""], ["Avestimehr", "Salman", ""]]}, {"id": "2007.13524", "submitter": "Ruichao Xiao", "authors": "Ruichao Xiao, Manish Kumar Singh, Rose Yu", "title": "Dynamic Relational Inference in Multi-Agent Trajectories", "comments": "submitted to ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring interactions from multi-agent trajectories has broad applications\nin physics, vision and robotics. Neural relational inference (NRI) is a deep\ngenerative model that can reason about relations in complex dynamics without\nsupervision. In this paper, we take a careful look at this approach for\nrelational inference in multi-agent trajectories. First, we discover that NRI\ncan be fundamentally limited without sufficient long-term observations. Its\nability to accurately infer interactions degrades drastically for short output\nsequences. Next, we consider a more general setting of relational inference\nwhen interactions are changing overtime. We propose an extension ofNRI, which\nwe call the DYnamic multi-AgentRelational Inference (DYARI) model that can\nreason about dynamic relations. We conduct exhaustive experiments to study the\neffect of model architecture, under-lying dynamics and training scheme on the\nperformance of dynamic relational inference using a simulated physics system.\nWe also showcase the usage of our model on real-world multi-agent basketball\ntrajectories.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 19:15:16 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 21:54:29 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Xiao", "Ruichao", ""], ["Singh", "Manish Kumar", ""], ["Yu", "Rose", ""]]}, {"id": "2007.13525", "submitter": "Lelin Zhang", "authors": "Lelin Zhang (1), Xi Nan (2), Eva Huang (2), Sidong Liu (3) ((1)\n  University of Technology Sydney, (2) The University of Sydney Business\n  School, (3) Macquarie University)", "title": "Detecting Transaction-based Tax Evasion Activities on Social Media\n  Platforms Using Multi-modal Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media platforms now serve billions of users by providing convenient\nmeans of communication, content sharing and even payment between different\nusers. Due to such convenient and anarchic nature, they have also been used\nrampantly to promote and conduct business activities between unregistered\nmarket participants without paying taxes. Tax authorities worldwide face\ndifficulties in regulating these hidden economy activities by traditional\nregulatory means. This paper presents a machine learning based Regtech tool for\ninternational tax authorities to detect transaction-based tax evasion\nactivities on social media platforms. To build such a tool, we collected a\ndataset of 58,660 Instagram posts and manually labelled 2,081 sampled posts\nwith multiple properties related to transaction-based tax evasion activities.\nBased on the dataset, we developed a multi-modal deep neural network to\nautomatically detect suspicious posts. The proposed model combines comments,\nhashtags and image modalities to produce the final output. As shown by our\nexperiments, the combined model achieved an AUC of 0.808 and F1 score of 0.762,\noutperforming any single modality models. This tool could help tax authorities\nto identify audit targets in an efficient and effective manner, and combat\nsocial e-commerce tax evasion in scale.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 13:05:39 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Zhang", "Lelin", ""], ["Nan", "Xi", ""], ["Huang", "Eva", ""], ["Liu", "Sidong", ""]]}, {"id": "2007.13531", "submitter": "Ioana Bica", "authors": "Ioana Bica, Daniel Jarrett, Alihan H\\\"uy\\\"uk, Mihaela van der Schaar", "title": "Learning \"What-if\" Explanations for Sequential Decision-Making", "comments": "In Proc. 9th International Conference on Learning Representations\n  (ICLR 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building interpretable parameterizations of real-world decision-making on the\nbasis of demonstrated behavior -- i.e. trajectories of observations and actions\nmade by an expert maximizing some unknown reward function -- is essential for\nintrospecting and auditing policies in different institutions. In this paper,\nwe propose learning explanations of expert decisions by modeling their reward\nfunction in terms of preferences with respect to \"what if\" outcomes: Given the\ncurrent history of observations, what would happen if we took a particular\naction? To learn these cost-benefit tradeoffs associated with the expert's\nactions, we integrate counterfactual reasoning into batch inverse reinforcement\nlearning. This offers a principled way of defining reward functions and\nexplaining expert behavior, and also satisfies the constraints of real-world\ndecision-making -- where active experimentation is often impossible (e.g. in\nhealthcare). Additionally, by estimating the effects of different actions,\ncounterfactuals readily tackle the off-policy nature of policy evaluation in\nthe batch setting, and can naturally accommodate settings where the expert\npolicies depend on histories of observations rather than just current states.\nThrough illustrative experiments in both real and simulated medical\nenvironments, we highlight the effectiveness of our batch, counterfactual\ninverse reinforcement learning approach in recovering accurate and\ninterpretable descriptions of behavior.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 14:24:17 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 13:14:44 GMT"}, {"version": "v3", "created": "Tue, 30 Mar 2021 17:32:17 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Bica", "Ioana", ""], ["Jarrett", "Daniel", ""], ["H\u00fcy\u00fck", "Alihan", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "2007.13532", "submitter": "Yevgeny Seldin", "authors": "Andr\\'es R. Masegosa and Stephan S. Lorenzen and Christian Igel and\n  Yevgeny Seldin", "title": "Second Order PAC-Bayesian Bounds for the Weighted Majority Vote", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel analysis of the expected risk of weighted majority vote in\nmulticlass classification. The analysis takes correlation of predictions by\nensemble members into account and provides a bound that is amenable to\nefficient minimization, which yields improved weighting for the majority vote.\nWe also provide a specialized version of our bound for binary classification,\nwhich allows to exploit additional unlabeled data for tighter risk estimation.\nIn experiments, we apply the bound to improve weighting of trees in random\nforests and show that, in contrast to the commonly used first order bound,\nminimization of the new bound typically does not lead to degradation of the\ntest error of the ensemble.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 16:25:13 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 12:26:53 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Masegosa", "Andr\u00e9s R.", ""], ["Lorenzen", "Stephan S.", ""], ["Igel", "Christian", ""], ["Seldin", "Yevgeny", ""]]}, {"id": "2007.13533", "submitter": "Jiazhou Chen", "authors": "Jiazhou Chen, Guoqiang Han, Hongmin Cai, Defu Yang, Paul J. Laurienti,\n  Martin Styner, Guorong Wu, and Alzheimer's Disease Neuroimaging Initiative\n  ADNI", "title": "Learning Common Harmonic Waves on Stiefel Manifold -- A New Mathematical\n  Approach for Brain Network Analyses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Converging evidence shows that disease-relevant brain alterations do not\nappear in random brain locations, instead, its spatial pattern follows large\nscale brain networks. In this context, a powerful network analysis approach\nwith a mathematical foundation is indispensable to understand the mechanism of\nneuropathological events spreading throughout the brain. Indeed, the topology\nof each brain network is governed by its native harmonic waves, which are a set\nof orthogonal bases derived from the Eigen-system of the underlying Laplacian\nmatrix. To that end, we propose a novel connectome harmonic analysis framework\nto provide enhanced mathematical insights by detecting frequency-based\nalterations relevant to brain disorders. The backbone of our framework is a\nnovel manifold algebra appropriate for inference across harmonic waves that\novercomes the limitations of using classic Euclidean operations on irregular\ndata structures. The individual harmonic difference is measured by a set of\ncommon harmonic waves learned from a population of individual Eigen systems,\nwhere each native Eigen-system is regarded as a sample drawn from the Stiefel\nmanifold. Specifically, a manifold optimization scheme is tailored to find the\ncommon harmonic waves which reside at the center of Stiefel manifold. To that\nend, the common harmonic waves constitute the new neuro-biological bases to\nunderstand disease progression. Each harmonic wave exhibits a unique\npropagation pattern of neuro-pathological burdens spreading across brain\nnetworks. The statistical power of our novel connectome harmonic analysis\napproach is evaluated by identifying frequency-based alterations relevant to\nAlzheimer's disease, where our learning-based manifold approach discovers more\nsignificant and reproducible network dysfunction patterns compared to Euclidian\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 20:16:43 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Chen", "Jiazhou", ""], ["Han", "Guoqiang", ""], ["Cai", "Hongmin", ""], ["Yang", "Defu", ""], ["Laurienti", "Paul J.", ""], ["Styner", "Martin", ""], ["Wu", "Guorong", ""], ["ADNI", "Alzheimer's Disease Neuroimaging Initiative", ""]]}, {"id": "2007.13547", "submitter": "Changsheng Li", "authors": "Changsheng Li and Chong Liu and Lixin Duan and Peng Gao and Kai Zheng", "title": "Reconstruction Regularized Deep Metric Learning for Multi-label Image\n  Classification", "comments": "Accepted by IEEE TNNLS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel deep metric learning method to tackle the\nmulti-label image classification problem. In order to better learn the\ncorrelations among images features, as well as labels, we attempt to explore a\nlatent space, where images and labels are embedded via two unique deep neural\nnetworks, respectively. To capture the relationships between image features and\nlabels, we aim to learn a \\emph{two-way} deep distance metric over the\nembedding space from two different views, i.e., the distance between one image\nand its labels is not only smaller than those distances between the image and\nits labels' nearest neighbors, but also smaller than the distances between the\nlabels and other images corresponding to the labels' nearest neighbors.\nMoreover, a reconstruction module for recovering correct labels is incorporated\ninto the whole framework as a regularization term, such that the label\nembedding space is more representative. Our model can be trained in an\nend-to-end manner. Experimental results on publicly available image datasets\ncorroborate the efficacy of our method compared with the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 13:28:50 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Li", "Changsheng", ""], ["Liu", "Chong", ""], ["Duan", "Lixin", ""], ["Gao", "Peng", ""], ["Zheng", "Kai", ""]]}, {"id": "2007.13553", "submitter": "Julien Bect", "authors": "R\\'emi Stroh (LNE, L2S), Julien Bect (L2S, GdR MASCOT-NUM), S\\'everine\n  Demeyer (LNE), Nicolas Fischer (LNE), Damien Marquis (LNE), Emmanuel Vazquez\n  (L2S, GdR MASCOT-NUM)", "title": "Sequential design of multi-fidelity computer experiments: maximizing the\n  rate of stepwise uncertainty reduction", "comments": "Technometrics, Taylor & Francis", "journal-ref": null, "doi": "10.1080/00401706.2021.1935324", "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article deals with the sequential design of experiments for\n(deterministic or stochastic) multi-fidelity numerical simulators, that is,\nsimulators that offer control over the accuracy of simulation of the physical\nphenomenon or system under study. Very often, accurate simulations correspond\nto high computational efforts whereas coarse simulations can be obtained at a\nsmaller cost. In this setting, simulation results obtained at several levels of\nfidelity can be combined in order to estimate quantities of interest (the\noptimal value of the output, the probability that the output exceeds a given\nthreshold...) in an efficient manner. To do so, we propose a new Bayesian\nsequential strategy called Maximal Rate of Stepwise Uncertainty Reduction\n(MR-SUR), that selects additional simulations to be performed by maximizing the\nratio between the expected reduction of uncertainty and the cost of simulation.\nThis generic strategy unifies several existing methods, and provides a\nprincipled approach to develop new ones. We assess its performance on several\nexamples, including a computationally intensive problem of fire safety analysis\nwhere the quantity of interest is the probability of exceeding a tenability\nthreshold during a building fire.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 13:34:12 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 13:37:24 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Stroh", "R\u00e9mi", "", "LNE, L2S"], ["Bect", "Julien", "", "L2S, GdR MASCOT-NUM"], ["Demeyer", "S\u00e9verine", "", "LNE"], ["Fischer", "Nicolas", "", "LNE"], ["Marquis", "Damien", "", "LNE"], ["Vazquez", "Emmanuel", "", "L2S, GdR MASCOT-NUM"]]}, {"id": "2007.13609", "submitter": "Ilya Kostrikov", "authors": "Ilya Kostrikov and Ofir Nachum", "title": "Statistical Bootstrapping for Uncertainty Estimation in Off-Policy\n  Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In reinforcement learning, it is typical to use the empirically observed\ntransitions and rewards to estimate the value of a policy via either\nmodel-based or Q-fitting approaches. Although straightforward, these techniques\nin general yield biased estimates of the true value of the policy. In this\nwork, we investigate the potential for statistical bootstrapping to be used as\na way to take these biased estimates and produce calibrated confidence\nintervals for the true value of the policy. We identify conditions -\nspecifically, sufficient data size and sufficient coverage - under which\nstatistical bootstrapping in this setting is guaranteed to yield correct\nconfidence intervals. In practical situations, these conditions often do not\nhold, and so we discuss and propose mechanisms that can be employed to mitigate\ntheir effects. We evaluate our proposed method and show that it can yield\naccurate confidence intervals in a variety of conditions, including challenging\ncontinuous control environments and small data regimes.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 14:49:22 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Kostrikov", "Ilya", ""], ["Nachum", "Ofir", ""]]}, {"id": "2007.13617", "submitter": "Tomaz Stepisnik", "authors": "Toma\\v{z} Stepi\\v{s}nik and Dragi Kocev", "title": "Oblique Predictive Clustering Trees", "comments": "26 pages, 11 figures. Submitted to Knowledge-based systems. Update:\n  fixed typos, general grammar improvements, updated 2 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive clustering trees (PCTs) are a well established generalization of\nstandard decision trees, which can be used to solve a variety of predictive\nmodeling tasks, including structured output prediction. Combining them into\nensembles yields state-of-the-art performance. Furthermore, the ensembles of\nPCTs can be interpreted by calculating feature importance scores from the\nlearned models. However, their learning time scales poorly with the\ndimensionality of the output space. This is often problematic, especially in\n(hierarchical) multi-label classification, where the output can consist of\nhundreds of potential labels. Also, learning of PCTs can not exploit the\nsparsity of data to improve the computational efficiency, which is common in\nboth input (molecular fingerprints, bag of words representations) and output\nspaces (in multi-label classification, examples are often labeled with only a\nfraction of possible labels). In this paper, we propose oblique predictive\nclustering trees, capable of addressing these limitations. We design and\nimplement two methods for learning oblique splits that contain linear\ncombinations of features in the tests, hence a split corresponds to an\narbitrary hyperplane in the input space. The methods are efficient for high\ndimensional data and capable of exploiting sparse data. We experimentally\nevaluate the proposed methods on 60 benchmark datasets for 6 predictive\nmodeling tasks. The results of the experiments show that oblique predictive\nclustering trees achieve performance on-par with state-of-the-art methods and\nare orders of magnitude faster than standard PCTs. We also show that meaningful\nfeature importance scores can be extracted from the models learned with the\nproposed methods.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 14:58:23 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 08:35:43 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Stepi\u0161nik", "Toma\u017e", ""], ["Kocev", "Dragi", ""]]}, {"id": "2007.13638", "submitter": "Yunpeng Shi", "authors": "Yunpeng Shi and Gilad Lerman", "title": "Message Passing Least Squares Framework and its Application to Rotation\n  Synchronization", "comments": "To Appear in ICML 2020 Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient algorithm for solving group synchronization under\nhigh levels of corruption and noise, while we focus on rotation\nsynchronization. We first describe our recent theoretically guaranteed message\npassing algorithm that estimates the corruption levels of the measured group\nratios. We then propose a novel reweighted least squares method to estimate the\ngroup elements, where the weights are initialized and iteratively updated using\nthe estimated corruption levels. We demonstrate the superior performance of our\nalgorithm over state-of-the-art methods for rotation synchronization using both\nsynthetic and real data.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 15:39:19 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 18:14:00 GMT"}, {"version": "v3", "created": "Sat, 15 Aug 2020 02:00:34 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Shi", "Yunpeng", ""], ["Lerman", "Gilad", ""]]}, {"id": "2007.13640", "submitter": "Eero Simoncelli", "authors": "Zahra Kadkhodaie and Eero P. Simoncelli", "title": "Solving Linear Inverse Problems Using the Prior Implicit in a Denoiser", "comments": "19 pages, 12 figures. Changes: more detailed description of\n  relationships to previous literature, including empirical comparisons for\n  super-resolution, debarring, and compressive sensing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prior probability models are a fundamental component of many image processing\nproblems, but density estimation is notoriously difficult for high-dimensional\nsignals such as photographic images. Deep neural networks have provided\nstate-of-the-art solutions for problems such as denoising, which implicitly\nrely on a prior probability model of natural images. Here, we develop a robust\nand general methodology for making use of this implicit prior. We rely on a\nstatistical result due to Miyasawa (1961), who showed that the least-squares\nsolution for removing additive Gaussian noise can be written directly in terms\nof the gradient of the log of the noisy signal density. We use this fact to\ndevelop a stochastic coarse-to-fine gradient ascent procedure for drawing\nhigh-probability samples from the implicit prior embedded within a CNN trained\nto perform blind (i.e., with unknown noise level) least-squares denoising. A\ngeneralization of this algorithm to constrained sampling provides a method for\nusing the implicit prior to solve any linear inverse problem, with no\nadditional training. We demonstrate this general form of transfer learning in\nmultiple applications, using the same algorithm to produce state-of-the-art\nlevels of unsupervised performance for deblurring, super-resolution,\ninpainting, and compressive sensing.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 15:40:46 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 23:49:27 GMT"}, {"version": "v3", "created": "Fri, 7 May 2021 02:34:03 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Kadkhodaie", "Zahra", ""], ["Simoncelli", "Eero P.", ""]]}, {"id": "2007.13648", "submitter": "Perry Gibson", "authors": "Perry Gibson, Jos\\'e Cano", "title": "Orpheus: A New Deep Learning Framework for Easy Deployment and\n  Evaluation of Edge Inference", "comments": "To be published as a poster in 2020 IEEE International Symposium on\n  Performance Analysis of Systems and Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV cs.LG cs.PF stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Optimising deep learning inference across edge devices and optimisation\ntargets such as inference time, memory footprint and power consumption is a key\nchallenge due to the ubiquity of neural networks. Today, production deep\nlearning frameworks provide useful abstractions to aid machine learning\nengineers and systems researchers. However, in exchange they can suffer from\ncompatibility challenges (especially on constrained platforms), inaccessible\ncode complexity, or design choices that otherwise limit research from a systems\nperspective. This paper presents Orpheus, a new deep learning framework for\neasy prototyping, deployment and evaluation of inference optimisations. Orpheus\nfeatures a small codebase, minimal dependencies, and a simple process for\nintegrating other third party systems. We present some preliminary evaluation\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 14:54:40 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 20:58:35 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Gibson", "Perry", ""], ["Cano", "Jos\u00e9", ""]]}, {"id": "2007.13657", "submitter": "Behnam Neyshabur", "authors": "Behnam Neyshabur", "title": "Towards Learning Convolutions from Scratch", "comments": "18 pages, 9 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution is one of the most essential components of architectures used in\ncomputer vision. As machine learning moves towards reducing the expert bias and\nlearning it from data, a natural next step seems to be learning\nconvolution-like structures from scratch. This, however, has proven elusive.\nFor example, current state-of-the-art architecture search algorithms use\nconvolution as one of the existing modules rather than learning it from data.\nIn an attempt to understand the inductive bias that gives rise to convolutions,\nwe investigate minimum description length as a guiding principle and show that\nin some settings, it can indeed be indicative of the performance of\narchitectures. To find architectures with small description length, we propose\n$\\beta$-LASSO, a simple variant of LASSO algorithm that, when applied on\nfully-connected networks for image classification tasks, learns architectures\nwith local connections and achieves state-of-the-art accuracies for training\nfully-connected nets on CIFAR-10 (85.19%), CIFAR-100 (59.56%) and SVHN (94.07%)\nbridging the gap between fully-connected and convolutional nets.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 16:13:13 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Neyshabur", "Behnam", ""]]}, {"id": "2007.13660", "submitter": "Yuhan Liu", "authors": "Yuhan Liu, Ananda Theertha Suresh, Felix Yu, Sanjiv Kumar, Michael\n  Riley", "title": "Learning discrete distributions: user vs item-level privacy", "comments": "NeurIPS 2020, 38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of the literature on differential privacy focuses on item-level privacy,\nwhere loosely speaking, the goal is to provide privacy per item or training\nexample. However, recently many practical applications such as federated\nlearning require preserving privacy for all items of a single user, which is\nmuch harder to achieve. Therefore understanding the theoretical limit of\nuser-level privacy becomes crucial.\n  We study the fundamental problem of learning discrete distributions over $k$\nsymbols with user-level differential privacy. If each user has $m$ samples, we\nshow that straightforward applications of Laplace or Gaussian mechanisms\nrequire the number of users to be $\\mathcal{O}(k/(m\\alpha^2) +\nk/\\epsilon\\alpha)$ to achieve an $\\ell_1$ distance of $\\alpha$ between the true\nand estimated distributions, with the privacy-induced penalty\n$k/\\epsilon\\alpha$ independent of the number of samples per user $m$. Moreover,\nwe show that any mechanism that only operates on the final aggregate counts\nshould require a user complexity of the same order. We then propose a mechanism\nsuch that the number of users scales as $\\tilde{\\mathcal{O}}(k/(m\\alpha^2) +\nk/\\sqrt{m}\\epsilon\\alpha)$ and hence the privacy penalty is\n$\\tilde{\\Theta}(\\sqrt{m})$ times smaller compared to the standard mechanisms in\ncertain settings of interest. We further show that the proposed mechanism is\nnearly-optimal under certain regimes.\n  We also propose general techniques for obtaining lower bounds on restricted\ndifferentially private estimators and a lower bound on the total variation\nbetween binomial distributions, both of which might be of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 16:15:14 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 17:42:38 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2021 22:15:07 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Liu", "Yuhan", ""], ["Suresh", "Ananda Theertha", ""], ["Yu", "Felix", ""], ["Kumar", "Sanjiv", ""], ["Riley", "Michael", ""]]}, {"id": "2007.13662", "submitter": "Elif Ecem Bas", "authors": "Elif Ecem Bas, Denis Aslangil, Mohamed A. Moustafa", "title": "Predicting Nonlinear Seismic Response of Structural Braces Using Machine\n  Learning", "comments": "6 pages, 6 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerical modeling of different structural materials that have highly\nnonlinear behaviors has always been a challenging problem in engineering\ndisciplines. Experimental data is commonly used to characterize this behavior.\nThis study aims to improve the modeling capabilities by using state of the art\nMachine Learning techniques, and attempts to answer several scientific\nquestions: (i) Which ML algorithm is capable and is more efficient to learn\nsuch a complex and nonlinear problem? (ii) Is it possible to artificially\nreproduce structural brace seismic behavior that can represent real physics?\n(iii) How can our findings be extended to the different engineering problems\nthat are driven by similar nonlinear dynamics? To answer these questions, the\npresented methods are validated by using experimental brace data. The paper\nshows that after proper data preparation, the long-short term memory (LSTM)\nmethod is highly capable of capturing the nonlinear behavior of braces.\nAdditionally, the effects of tuning the hyperparameters on the models, such as\nlayer numbers, neuron numbers, and the activation functions, are presented.\nFinally, the ability to learn nonlinear dynamics by using deep neural network\nalgorithms and their advantages are briefly discussed.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 16:16:03 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Bas", "Elif Ecem", ""], ["Aslangil", "Denis", ""], ["Moustafa", "Mohamed A.", ""]]}, {"id": "2007.13664", "submitter": "Gerrit Welper", "authors": "G. Welper", "title": "Universality of Gradient Descent Neural Network Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been observed that design choices of neural networks are often crucial\nfor their successful optimization. In this article, we therefore discuss the\nquestion if it is always possible to redesign a neural network so that it\ntrains well with gradient descent. This yields the following universality\nresult: If, for a given network, there is any algorithm that can find good\nnetwork weights for a classification task, then there exists an extension of\nthis network that reproduces these weights and the corresponding forward output\nby mere gradient descent training. The construction is not intended for\npractical computations, but it provides some orientation on the possibilities\nof meta-learning and related approaches.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 16:17:19 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Welper", "G.", ""]]}, {"id": "2007.13689", "submitter": "Barbara Benato", "authors": "Barbara Caroline Benato and Jancarlo Ferreira Gomes and Alexandru\n  Cristian Telea and Alexandre Xavier Falc\\~ao", "title": "Semi-Automatic Data Annotation guided by Feature Space Projection", "comments": "28 pages, 10 figures", "journal-ref": null, "doi": "10.1016/j.patcog.2020.107612", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data annotation using visual inspection (supervision) of each training sample\ncan be laborious. Interactive solutions alleviate this by helping experts\npropagate labels from a few supervised samples to unlabeled ones based solely\non the visual analysis of their feature space projection (with no further\nsample supervision). We present a semi-automatic data annotation approach based\non suitable feature space projection and semi-supervised label estimation. We\nvalidate our method on the popular MNIST dataset and on images of human\nintestinal parasites with and without fecal impurities, a large and diverse\ndataset that makes classification very hard. We evaluate two approaches for\nsemi-supervised learning from the latent and projection spaces, to choose the\none that best reduces user annotation effort and also increases classification\naccuracy on unseen data. Our results demonstrate the added-value of visual\nanalytics tools that combine complementary abilities of humans and machines for\nmore effective machine learning.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 17:03:50 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Benato", "Barbara Caroline", ""], ["Gomes", "Jancarlo Ferreira", ""], ["Telea", "Alexandru Cristian", ""], ["Falc\u00e3o", "Alexandre Xavier", ""]]}, {"id": "2007.13690", "submitter": "Karush Suri", "authors": "Karush Suri, Xiao Qi Shi, Konstantinos N. Plataniotis, Yuri A.\n  Lawryshyn", "title": "Maximum Mutation Reinforcement Learning for Scalable Control", "comments": "10+3 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in Reinforcement Learning (RL) have demonstrated data efficiency and\noptimal control over large state spaces at the cost of scalable performance.\nGenetic methods, on the other hand, provide scalability but depict\nhyperparameter sensitivity towards evolutionary operations. However, a\ncombination of the two methods has recently demonstrated success in scaling RL\nagents to high-dimensional action spaces. Parallel to recent developments, we\npresent the Evolution-based Soft Actor-Critic (ESAC), a scalable RL algorithm.\nWe abstract exploration from exploitation by combining Evolution Strategies\n(ES) with Soft Actor-Critic (SAC). Through this lens, we enable dominant skill\ntransfer between offsprings by making use of soft winner selections and genetic\ncrossovers in hindsight and simultaneously improve hyperparameter sensitivity\nin evolutions using the novel Automatic Mutation Tuning (AMT). AMT gradually\nreplaces the entropy framework of SAC allowing the population to succeed at the\ntask while acting as randomly as possible, without making use of\nbackpropagation updates. In a study of challenging locomotion tasks consisting\nof high-dimensional action spaces and sparse rewards, ESAC demonstrates\nimproved performance and sample efficiency in comparison to the Maximum Entropy\nframework. Additionally, ESAC presents efficacious use of hardware resources\nand algorithm overhead. A complete implementation of ESAC can be found at\nkarush17.github.io/esac-web/.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 16:29:19 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 01:51:07 GMT"}, {"version": "v3", "created": "Sat, 24 Oct 2020 18:08:24 GMT"}, {"version": "v4", "created": "Sun, 8 Nov 2020 02:57:01 GMT"}, {"version": "v5", "created": "Tue, 17 Nov 2020 22:21:26 GMT"}, {"version": "v6", "created": "Sat, 21 Nov 2020 23:02:54 GMT"}, {"version": "v7", "created": "Sat, 16 Jan 2021 23:51:53 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Suri", "Karush", ""], ["Shi", "Xiao Qi", ""], ["Plataniotis", "Konstantinos N.", ""], ["Lawryshyn", "Yuri A.", ""]]}, {"id": "2007.13716", "submitter": "Yuting Wei", "authors": "Michael Celentano, Andrea Montanari, Yuting Wei", "title": "The Lasso with general Gaussian designs with applications to hypothesis\n  testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lasso is a method for high-dimensional regression, which is now commonly\nused when the number of covariates $p$ is of the same order or larger than the\nnumber of observations $n$. Classical asymptotic normality theory is not\napplicable for this model due to two fundamental reasons: $(1)$ The regularized\nrisk is non-smooth; $(2)$ The distance between the estimator $\\bf\n\\widehat{\\theta}$ and the true parameters vector $\\bf \\theta^\\star$ cannot be\nneglected. As a consequence, standard perturbative arguments that are the\ntraditional basis for asymptotic normality fail.\n  On the other hand, the Lasso estimator can be precisely characterized in the\nregime in which both $n$ and $p$ are large, while $n/p$ is of order one. This\ncharacterization was first obtained in the case of standard Gaussian designs,\nand subsequently generalized to other high-dimensional estimation procedures.\nHere we extend the same characterization to Gaussian correlated designs with\nnon-singular covariance structure. This characterization is expressed in terms\nof a simpler ``fixed design'' model. We establish non-asymptotic bounds on the\ndistance between distributions of various quantities in the two models, which\nhold uniformly over signals $\\bf \\theta^\\star$ in a suitable sparsity class,\nand values of the regularization parameter.\n  As applications, we study the distribution of the debiased Lasso, and show\nthat a degrees-of-freedom correction is necessary for computing valid\nconfidence intervals.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 17:48:54 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Celentano", "Michael", ""], ["Montanari", "Andrea", ""], ["Wei", "Yuting", ""]]}, {"id": "2007.13723", "submitter": "Claudio Santos", "authors": "Claudio Filipi Goncalves do Santos, Danilo Colombo, Mateus Roder,\n  Jo\\~ao Paulo Papa", "title": "MaxDropout: Deep Neural Network Regularization Based on Maximum Output\n  Values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different techniques have emerged in the deep learning scenario, such as\nConvolutional Neural Networks, Deep Belief Networks, and Long Short-Term Memory\nNetworks, to cite a few. In lockstep, regularization methods, which aim to\nprevent overfitting by penalizing the weight connections, or turning off some\nunits, have been widely studied either. In this paper, we present a novel\napproach called MaxDropout, a regularizer for deep neural network models that\nworks in a supervised fashion by removing (shutting off) the prominent neurons\n(i.e., most active) in each hidden layer. The model forces fewer activated\nunits to learn more representative information, thus providing sparsity.\nRegarding the experiments, we show that it is possible to improve existing\nneural networks and provide better results in neural networks when Dropout is\nreplaced by MaxDropout. The proposed method was evaluated in image\nclassification, achieving comparable results to existing regularizers, such as\nCutout and RandomErasing, also improving the accuracy of neural networks that\nuses Dropout by replacing the existing layer by MaxDropout.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 17:55:54 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Santos", "Claudio Filipi Goncalves do", ""], ["Colombo", "Danilo", ""], ["Roder", "Mateus", ""], ["Papa", "Jo\u00e3o Paulo", ""]]}, {"id": "2007.13794", "submitter": "Joseph Enguehard", "authors": "Joseph Enguehard, Dan Busbridge, Adam Bozson, Claire Woodcock and Nils\n  Y. Hammerla", "title": "Neural Temporal Point Processes For Modelling Electronic Health Records", "comments": "Version accepted to Machine Learning for Health (ML4H) workshop at\n  NeurIPS 2020. 10 pages, 5 figures, 3 tables. Code, pre-trained models and\n  datasets available at https://github.com/babylonhealth/neuralTPPs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modelling of Electronic Health Records (EHRs) has the potential to drive\nmore efficient allocation of healthcare resources, enabling early intervention\nstrategies and advancing personalised healthcare. However, EHRs are challenging\nto model due to their realisation as noisy, multi-modal data occurring at\nirregular time intervals. To address their temporal nature, we treat EHRs as\nsamples generated by a Temporal Point Process (TPP), enabling us to model what\nhappened in an event with when it happened in a principled way. We gather and\npropose neural network parameterisations of TPPs, collectively referred to as\nNeural TPPs. We perform evaluations on synthetic EHRs as well as on a set of\nestablished benchmarks. We show that TPPs significantly outperform their\nnon-TPP counterparts on EHRs. We also show that an assumption of many Neural\nTPPs, that the class distribution is conditionally independent of time, reduces\nperformance on EHRs. Finally, our proposed attention-based Neural TPP performs\nfavourably compared to existing models, whilst aligning with real world\ninterpretability requirements, an important step towards a component of\nclinical decision support systems.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 18:16:54 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 14:47:03 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Enguehard", "Joseph", ""], ["Busbridge", "Dan", ""], ["Bozson", "Adam", ""], ["Woodcock", "Claire", ""], ["Hammerla", "Nils Y.", ""]]}, {"id": "2007.13819", "submitter": "Timothy Castiglia Mr.", "authors": "Timothy Castiglia, Anirban Das, and Stacy Patterson", "title": "Multi-Level Local SGD for Heterogeneous Hierarchical Networks", "comments": "36 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Multi-Level Local SGD, a distributed gradient method for learning\na smooth, non-convex objective in a heterogeneous multi-level network. Our\nnetwork model consists of a set of disjoint sub-networks, with a single hub and\nmultiple worker nodes; further, worker nodes may have different operating\nrates. The hubs exchange information with one another via a connected, but not\nnecessarily complete communication network. In our algorithm, sub-networks\nexecute a distributed SGD algorithm, using a hub-and-spoke paradigm, and the\nhubs periodically average their models with neighboring hubs. We first provide\na unified mathematical framework that describes the Multi-Level Local SGD\nalgorithm. We then present a theoretical analysis of the algorithm; our\nanalysis shows the dependence of the convergence error on the worker node\nheterogeneity, hub network topology, and the number of local, sub-network, and\nglobal iterations. We back up our theoretical results via simulation-based\nexperiments using both convex and non-convex objectives.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 19:14:23 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 21:17:04 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Castiglia", "Timothy", ""], ["Das", "Anirban", ""], ["Patterson", "Stacy", ""]]}, {"id": "2007.13825", "submitter": "Zhaozhi Qian", "authors": "Zhaozhi Qian and Ahmed M. Alaa and Mihaela van der Schaar", "title": "CPAS: the UK's National Machine Learning-based Hospital Capacity\n  Planning System for COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coronavirus disease 2019 (COVID-19) global pandemic poses the threat of\noverwhelming healthcare systems with unprecedented demands for intensive care\nresources. Managing these demands cannot be effectively conducted without a\nnationwide collective effort that relies on data to forecast hospital demands\non the national, regional, hospital and individual levels. To this end, we\ndeveloped the COVID-19 Capacity Planning and Analysis System (CPAS) - a machine\nlearning-based system for hospital resource planning that we have successfully\ndeployed at individual hospitals and across regions in the UK in coordination\nwith NHS Digital. In this paper, we discuss the main challenges of deploying a\nmachine learning-based decision support system at national scale, and explain\nhow CPAS addresses these challenges by (1) defining the appropriate learning\nproblem, (2) combining bottom-up and top-down analytical approaches, (3) using\nstate-of-the-art machine learning algorithms, (4) integrating heterogeneous\ndata sources, and (5) presenting the result with an interactive and transparent\ninterface. CPAS is one of the first machine learning-based systems to be\ndeployed in hospitals on a national scale to address the COVID-19 pandemic - we\nconclude the paper with a summary of the lessons learned from this experience.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 19:39:13 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Qian", "Zhaozhi", ""], ["Alaa", "Ahmed M.", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "2007.13843", "submitter": "Tyler Tomita", "authors": "Tyler M. Tomita and Joshua T. Vogelstein", "title": "Robust Similarity and Distance Learning via Decision Forests", "comments": "Submitted to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical distances such as Euclidean distance often fail to capture the\nappropriate relationships between items, subsequently leading to subpar\ninference and prediction. Many algorithms have been proposed for automated\nlearning of suitable distances, most of which employ linear methods to learn a\nglobal metric over the feature space. While such methods offer nice theoretical\nproperties, interpretability, and computationally efficient means for\nimplementing them, they are limited in expressive capacity. Methods which have\nbeen designed to improve expressiveness sacrifice one or more of the nice\nproperties of the linear methods. To bridge this gap, we propose a highly\nexpressive novel decision forest algorithm for the task of distance learning,\nwhich we call Similarity and Metric Random Forests (SMERF). We show that the\ntree construction procedure in SMERF is a proper generalization of standard\nclassification and regression trees. Thus, the mathematical driving forces of\nSMERF are examined via its direct connection to regression forests, for which\ntheory has been developed. Its ability to approximate arbitrary distances and\nidentify important features is empirically demonstrated on simulated data sets.\nLast, we demonstrate that it accurately predicts links in networks.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 20:17:42 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 15:41:38 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Tomita", "Tyler M.", ""], ["Vogelstein", "Joshua T.", ""]]}, {"id": "2007.13860", "submitter": "Shancong Mou", "authors": "Shancong Mou, Andi Wang, Chuck Zhang and Jianjun Shi", "title": "Additive Tensor Decomposition Considering Structural Data Information", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor data with rich structural information becomes increasingly important\nin process modeling, monitoring, and diagnosis. Here structural information is\nreferred to structural properties such as sparsity, smoothness, low-rank, and\npiecewise constancy. To reveal useful information from tensor data, we propose\nto decompose the tensor into the summation of multiple components based on\ndifferent structural information of them. In this paper, we provide a new\ndefinition of structural information in tensor data. Based on it, we propose an\nadditive tensor decomposition (ATD) framework to extract useful information\nfrom tensor data. This framework specifies a high dimensional optimization\nproblem to obtain the components with distinct structural information. An\nalternating direction method of multipliers (ADMM) algorithm is proposed to\nsolve it, which is highly parallelable and thus suitable for the proposed\noptimization problem. Two simulation examples and a real case study in medical\nimage analysis illustrate the versatility and effectiveness of the ATD\nframework.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 20:53:27 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Mou", "Shancong", ""], ["Wang", "Andi", ""], ["Zhang", "Chuck", ""], ["Shi", "Jianjun", ""]]}, {"id": "2007.13869", "submitter": "Ruda Zhang", "authors": "Ruda Zhang and Roger Ghanem", "title": "Normal-bundle Bootstrap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.DG math.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic models of data sets often exhibit salient geometric structure.\nSuch a phenomenon is summed up in the manifold distribution hypothesis, and can\nbe exploited in probabilistic learning. Here we present normal-bundle bootstrap\n(NBB), a method that generates new data which preserve the geometric structure\nof a given data set. Inspired by algorithms for manifold learning and concepts\nin differential geometry, our method decomposes the underlying probability\nmeasure into a marginalized measure on a learned data manifold and conditional\nmeasures on the normal spaces. The algorithm estimates the data manifold as a\ndensity ridge, and constructs new data by bootstrapping projection vectors and\nadding them to the ridge. We apply our method to the inference of density ridge\nand related statistics, and data augmentation to reduce overfitting.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 21:14:19 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Zhang", "Ruda", ""], ["Ghanem", "Roger", ""]]}, {"id": "2007.13875", "submitter": "Umberto Michelucci", "authors": "Umberto, Michelucci, Francesca Venturini", "title": "Multi-Task Learning for Multi-Dimensional Regression: Application to\n  Luminescence Sensing", "comments": "16 Pages", "journal-ref": null, "doi": "10.3390/app9224748", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical approach to non-linear regression in physics, is to take a\nmathematical model describing the functional dependence of the dependent\nvariable from a set of independent variables, and then, using non-linear\nfitting algorithms, extract the parameters used in the modeling. Particularly\nchallenging are real systems, characterized by several additional influencing\nfactors related to specific components, like electronics or optical parts. In\nsuch cases, to make the model reproduce the data, empirically determined terms\nare built-in the models to compensate for the impossibility of modeling things\nthat are, by construction, impossible to model. A new approach to solve this\nissue is to use neural networks, particularly feed-forward architectures with a\nsufficient number of hidden layers and an appropriate number of output neurons,\neach responsible for predicting the desired variables. Unfortunately,\nfeed-forward neural networks (FFNNs) usually perform less efficiently when\napplied to multi-dimensional regression problems, that is when they are\nrequired to predict simultaneously multiple variables that depend from the\ninput dataset in fundamentally different ways. To address this problem, we\npropose multi-task learning (MTL) architectures. These are characterized by\nmultiple branches of task-specific layers, which have as input the output of a\ncommon set of layers. To demonstrate the power of this approach for\nmulti-dimensional regression, the method is applied to luminescence sensing.\nHere the MTL architecture allows predicting multiple parameters, the oxygen\nconcentration and the temperature, from a single set of measurements.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 21:23:51 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Umberto", "", ""], ["Michelucci", "", ""], ["Venturini", "Francesca", ""]]}, {"id": "2007.13885", "submitter": "Christian Blakely", "authors": "Christian D. Blakely, Ole-Christoffer Granmo", "title": "Closed-Form Expressions for Global and Local Interpretation of Tsetlin\n  Machines with Applications to Explaining High-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tsetlin Machines (TMs) capture patterns using conjunctive clauses in\npropositional logic, thus facilitating interpretation. However, recent TM-based\napproaches mainly rely on inspecting the full range of clauses individually.\nSuch inspection does not necessarily scale to complex prediction problems that\nrequire a large number of clauses. In this paper, we propose closed-form\nexpressions for understanding why a TM model makes a specific prediction (local\ninterpretability). Additionally, the expressions capture the most important\nfeatures of the model overall (global interpretability). We further introduce\nexpressions for measuring the importance of feature value ranges for continuous\nfeatures. The expressions are formulated directly from the conjunctive clauses\nof the TM, making it possible to capture the role of features in real-time,\nalso during the learning process as the model evolves. Additionally, from the\nclosed-form expressions, we derive a novel data clustering algorithm for\nvisualizing high-dimensional data in three dimensions. Finally, we compare our\nproposed approach against SHAP and state-of-the-art interpretable machine\nlearning techniques. For both classification and regression, our evaluation\nshow correspondence with SHAP as well as competitive prediction accuracy in\ncomparison with XGBoost, Explainable Boosting Machines, and Neural Additive\nModels.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 21:47:24 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Blakely", "Christian D.", ""], ["Granmo", "Ole-Christoffer", ""]]}, {"id": "2007.13893", "submitter": "Ali Mousavi", "authors": "Andrew Bennett, Nathan Kallus, Lihong Li, Ali Mousavi", "title": "Off-policy Evaluation in Infinite-Horizon Reinforcement Learning with\n  Latent Confounders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Off-policy evaluation (OPE) in reinforcement learning is an important problem\nin settings where experimentation is limited, such as education and healthcare.\nBut, in these very same settings, observed actions are often confounded by\nunobserved variables making OPE even more difficult. We study an OPE problem in\nan infinite-horizon, ergodic Markov decision process with unobserved\nconfounders, where states and actions can act as proxies for the unobserved\nconfounders. We show how, given only a latent variable model for states and\nactions, policy value can be identified from off-policy data. Our method\ninvolves two stages. In the first, we show how to use proxies to estimate\nstationary distribution ratios, extending recent work on breaking the curse of\nhorizon to the confounded setting. In the second, we show optimal balancing can\nbe combined with such learned ratios to obtain policy value while avoiding\ndirect modeling of reward functions. We establish theoretical guarantees of\nconsistency, and benchmark our method empirically.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 22:19:01 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Bennett", "Andrew", ""], ["Kallus", "Nathan", ""], ["Li", "Lihong", ""], ["Mousavi", "Ali", ""]]}, {"id": "2007.13904", "submitter": "Karmesh Yadav", "authors": "Gunshi Gupta, Karmesh Yadav and Liam Paull", "title": "La-MAML: Look-ahead Meta Learning for Continual Learning", "comments": "Accepted for Oral Presentation at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The continual learning problem involves training models with limited capacity\nto perform well on a set of an unknown number of sequentially arriving tasks.\nWhile meta-learning shows great potential for reducing interference between old\nand new tasks, the current training procedures tend to be either slow or\noffline, and sensitive to many hyper-parameters. In this work, we propose\nLook-ahead MAML (La-MAML), a fast optimisation-based meta-learning algorithm\nfor online-continual learning, aided by a small episodic memory. Our proposed\nmodulation of per-parameter learning rates in our meta-learning update allows\nus to draw connections to prior work on hypergradients and meta-descent. This\nprovides a more flexible and efficient way to mitigate catastrophic forgetting\ncompared to conventional prior-based methods. La-MAML achieves performance\nsuperior to other replay-based, prior-based and meta-learning based approaches\nfor continual learning on real-world visual classification benchmarks. Source\ncode can be found here: https://github.com/montrealrobotics/La-MAML\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 23:07:01 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 02:08:10 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Gupta", "Gunshi", ""], ["Yadav", "Karmesh", ""], ["Paull", "Liam", ""]]}, {"id": "2007.13911", "submitter": "Anne Watson Draelos", "authors": "Anne Draelos, Eva A. Naumann, John M. Pearson", "title": "Online neural connectivity estimation with ensemble stimulation", "comments": "Revised and expanded version of the work that appeared in NeurIPS\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the primary goals of systems neuroscience is to relate the structure\nof neural circuits to their function, yet patterns of connectivity are\ndifficult to establish when recording from large populations in behaving\norganisms. Many previous approaches have attempted to estimate functional\nconnectivity between neurons using statistical modeling of observational data,\nbut these approaches rely heavily on parametric assumptions and are purely\ncorrelational. Recently, however, holographic photostimulation techniques have\nmade it possible to precisely target selected ensembles of neurons, offering\nthe possibility of establishing direct causal links. Here, we propose a method\nbased on noisy group testing that drastically increases the efficiency of this\nprocess in sparse networks. By stimulating small ensembles of neurons, we show\nthat it is possible to recover binarized network connectivity with a number of\ntests that grows only logarithmically with population size under minimal\nstatistical assumptions. Moreover, we prove that our approach, which reduces to\nan efficiently solvable convex optimization problem, can be related to\nVariational Bayesian inference on the binary connection weights, and we derive\nrigorous bounds on the posterior marginals. This allows us to extend our method\nto the streaming setting, where continuously updated posteriors allow for\noptional stopping, and we demonstrate the feasibility of inferring connectivity\nfor networks of up to tens of thousands of neurons online. Finally, we show how\nour work can be theoretically linked to compressed sensing approaches, and\ncompare results for connectivity inference in different settings.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 23:47:03 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 16:26:54 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Draelos", "Anne", ""], ["Naumann", "Eva A.", ""], ["Pearson", "John M.", ""]]}, {"id": "2007.13959", "submitter": "Changsheng Li", "authors": "Changsheng Li and Handong Ma and Zhao Kang and Ye Yuan and Xiao-Yu\n  Zhang and Guoren Wang", "title": "On Deep Unsupervised Active Learning", "comments": "Accepted by IJCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised active learning has attracted increasing attention in recent\nyears, where its goal is to select representative samples in an unsupervised\nsetting for human annotating. Most existing works are based on shallow linear\nmodels by assuming that each sample can be well approximated by the span (i.e.,\nthe set of all linear combinations) of certain selected samples, and then take\nthese selected samples as representative ones to label. However, in practice,\nthe data do not necessarily conform to linear models, and how to model\nnonlinearity of data often becomes the key point to success. In this paper, we\npresent a novel Deep neural network framework for Unsupervised Active Learning,\ncalled DUAL. DUAL can explicitly learn a nonlinear embedding to map each input\ninto a latent space through an encoder-decoder architecture, and introduce a\nselection block to select representative samples in the the learnt latent\nspace. In the selection block, DUAL considers to simultaneously preserve the\nwhole input patterns as well as the cluster structure of data. Extensive\nexperiments are performed on six publicly available datasets, and experimental\nresults clearly demonstrate the efficacy of our method, compared with\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 02:52:21 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Li", "Changsheng", ""], ["Ma", "Handong", ""], ["Kang", "Zhao", ""], ["Yuan", "Ye", ""], ["Zhang", "Xiao-Yu", ""], ["Wang", "Guoren", ""]]}, {"id": "2007.13982", "submitter": "Hongseok Namkoong", "authors": "John Duchi, Tatsunori Hashimoto, Hongseok Namkoong", "title": "Distributionally Robust Losses for Latent Covariate Mixtures", "comments": "First released in 2019 on a personal website", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While modern large-scale datasets often consist of heterogeneous\nsubpopulations---for example, multiple demographic groups or multiple text\ncorpora---the standard practice of minimizing average loss fails to guarantee\nuniformly low losses across all subpopulations. We propose a convex procedure\nthat controls the worst-case performance over all subpopulations of a given\nsize. Our procedure comes with finite-sample (nonparametric) convergence\nguarantees on the worst-off subpopulation. Empirically, we observe on lexical\nsimilarity, wine quality, and recidivism prediction tasks that our worst-case\nprocedure learns models that do well against unseen subpopulations.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 04:16:27 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Duchi", "John", ""], ["Hashimoto", "Tatsunori", ""], ["Namkoong", "Hongseok", ""]]}, {"id": "2007.13985", "submitter": "Zhao Shen-Yi", "authors": "Shen-Yi Zhao, Yin-Peng Xie, Wu-Jun Li", "title": "Stochastic Normalized Gradient Descent with Momentum for Large Batch\n  Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) and its variants have been the dominating\noptimization methods in machine learning. Compared with small batch training,\nSGD with large batch training can better utilize the computational power of\ncurrent multi-core systems like GPUs and can reduce the number of communication\nrounds in distributed training. Hence, SGD with large batch training has\nattracted more and more attention. However, existing empirical results show\nthat large batch training typically leads to a drop of generalization accuracy.\nAs a result, large batch training has also become a challenging topic. In this\npaper, we propose a novel method, called stochastic normalized gradient descent\nwith momentum (SNGM), for large batch training. We theoretically prove that\ncompared to momentum SGD (MSGD) which is one of the most widely used variants\nof SGD, SNGM can adopt a larger batch size to converge to the\n$\\epsilon$-stationary point with the same computation complexity (total number\nof gradient computation). Empirical results on deep learning also show that\nSNGM can achieve the state-of-the-art accuracy with a large batch size.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 04:34:43 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Zhao", "Shen-Yi", ""], ["Xie", "Yin-Peng", ""], ["Li", "Wu-Jun", ""]]}, {"id": "2007.14042", "submitter": "Jirong Yi", "authors": "Jirong Yi, Raghu Mudumbai, Weiyu Xu", "title": "Derivation of Information-Theoretically Optimal Adversarial Attacks with\n  Applications to Robust Machine Learning", "comments": "16 pages, 5 theorems, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the theoretical problem of designing an optimal adversarial\nattack on a decision system that maximally degrades the achievable performance\nof the system as measured by the mutual information between the degraded signal\nand the label of interest. This problem is motivated by the existence of\nadversarial examples for machine learning classifiers. By adopting an\ninformation theoretic perspective, we seek to identify conditions under which\nadversarial vulnerability is unavoidable i.e. even optimally designed\nclassifiers will be vulnerable to small adversarial perturbations. We present\nderivations of the optimal adversarial attacks for discrete and continuous\nsignals of interest, i.e., finding the optimal perturbation distributions to\nminimize the mutual information between the degraded signal and a signal\nfollowing a continuous or discrete distribution. In addition, we show that it\nis much harder to achieve adversarial attacks for minimizing mutual information\nwhen multiple redundant copies of the input signal are available. This provides\nadditional support to the recently proposed ``feature compression\" hypothesis\nas an explanation for the adversarial vulnerability of deep learning\nclassifiers. We also report on results from computational experiments to\nillustrate our theoretical results.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 07:45:25 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Yi", "Jirong", ""], ["Mudumbai", "Raghu", ""], ["Xu", "Weiyu", ""]]}, {"id": "2007.14052", "submitter": "Andr\\'es Felipe L\\'opez-Lopera", "authors": "A. F. L\\'opez-Lopera, D. Idier, J. Rohmer, F. Bachoc", "title": "Multi-Output Gaussian Processes with Functional Data: A Study on Coastal\n  Flood Hazard Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing coastal flood Forecast and Early-Warning Systems do not\nmodel the flood, but instead, rely on the prediction of hydrodynamic conditions\nat the coast and on expert judgment. Recent scientific contributions are now\ncapable to precisely model flood events, even in situations where wave\novertopping plays a significant role. Such models are nevertheless\ncostly-to-evaluate and surrogate ones need to be exploited for substantial\ncomputational savings. For the latter models, the hydro-meteorological forcing\nconditions (inputs) or flood events (outputs) are conveniently parametrised\ninto scalar representations. However, they neglect the fact that inputs are\nactually functions (more precisely, time series), and that floods spatially\npropagate inland. Here, we introduce a multi-output Gaussian process model\naccounting for both criteria. On various examples, we test its versatility for\nboth learning spatial maps and inferring unobserved ones. We demonstrate that\nefficient implementations are obtained by considering tensor-structured data\nand/or sparse-variational approximations. Finally, the proposed framework is\napplied on a coastal application aiming at predicting flood events. We conclude\nthat accurate predictions are obtained in the order of minutes rather than the\ncouples of days required by dedicated hydrodynamic simulators.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 08:15:17 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 10:34:30 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["L\u00f3pez-Lopera", "A. F.", ""], ["Idier", "D.", ""], ["Rohmer", "J.", ""], ["Bachoc", "F.", ""]]}, {"id": "2007.14062", "submitter": "Manzil Zaheer", "authors": "Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris\n  Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\n  Amr Ahmed", "title": "Big Bird: Transformers for Longer Sequences", "comments": null, "journal-ref": "Neural Information Processing Systems (NeurIPS) 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers-based models, such as BERT, have been one of the most successful\ndeep learning models for NLP. Unfortunately, one of their core limitations is\nthe quadratic dependency (mainly in terms of memory) on the sequence length due\nto their full attention mechanism. To remedy this, we propose, BigBird, a\nsparse attention mechanism that reduces this quadratic dependency to linear. We\nshow that BigBird is a universal approximator of sequence functions and is\nTuring complete, thereby preserving these properties of the quadratic, full\nattention model. Along the way, our theoretical analysis reveals some of the\nbenefits of having $O(1)$ global tokens (such as CLS), that attend to the\nentire sequence as part of the sparse attention mechanism. The proposed sparse\nattention can handle sequences of length up to 8x of what was previously\npossible using similar hardware. As a consequence of the capability to handle\nlonger context, BigBird drastically improves performance on various NLP tasks\nsuch as question answering and summarization. We also propose novel\napplications to genomics data.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 08:34:04 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 07:41:50 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Zaheer", "Manzil", ""], ["Guruganesh", "Guru", ""], ["Dubey", "Avinava", ""], ["Ainslie", "Joshua", ""], ["Alberti", "Chris", ""], ["Ontanon", "Santiago", ""], ["Pham", "Philip", ""], ["Ravula", "Anirudh", ""], ["Wang", "Qifan", ""], ["Yang", "Li", ""], ["Ahmed", "Amr", ""]]}, {"id": "2007.14082", "submitter": "Alexander Soen", "authors": "Alexander Soen, Alexander Mathews, Daniel Grixti-Cheng, Lexing Xie", "title": "UNIPoint: Universally Approximating Point Processes Intensities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point processes are a useful mathematical tool for describing events over\ntime, and so there are many recent approaches for representing and learning\nthem. One notable open question is how to precisely describe the flexibility of\npoint process models and whether there exists a general model that can\nrepresent all point processes. Our work bridges this gap. Focusing on the\nwidely used event intensity function representation of point processes, we\nprovide a proof that a class of learnable functions can universally approximate\nany valid intensity function. The proof connects the well known\nStone-Weierstrass Theorem for function approximation, the uniform density of\nnon-negative continuous functions using a transfer functions, the formulation\nof the parameters of a piece-wise continuous functions as a dynamic system, and\na recurrent neural network implementation for capturing the dynamics. Using\nthese insights, we design and implement UNIPoint, a novel neural point process\nmodel, using recurrent neural networks to parameterise sums of basis function\nupon each event. Evaluations on synthetic and real world datasets show that\nthis simpler representation performs better than Hawkes process variants and\nmore complex neural network-based approaches. We expect this result will\nprovide a practical basis for selecting and tuning models, as well as\nfurthering theoretical work on representational complexity and learnability.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 09:31:56 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 04:25:13 GMT"}, {"version": "v3", "created": "Wed, 16 Dec 2020 23:41:08 GMT"}, {"version": "v4", "created": "Wed, 3 Mar 2021 01:07:53 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Soen", "Alexander", ""], ["Mathews", "Alexander", ""], ["Grixti-Cheng", "Daniel", ""], ["Xie", "Lexing", ""]]}, {"id": "2007.14120", "submitter": "Anna-Kathrin Kopetzki", "authors": "Anna-Kathrin Kopetzki, Stephan G\\\"unnemann", "title": "Reachable Sets of Classifiers and Regression Models: (Non-)Robustness\n  Analysis and Robust Training", "comments": "Published as a journal paper at ECML PKDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks achieve outstanding accuracy in classification and regression\ntasks. However, understanding their behavior still remains an open challenge\nthat requires questions to be addressed on the robustness, explainability and\nreliability of predictions. We answer these questions by computing reachable\nsets of neural networks, i.e. sets of outputs resulting from continuous sets of\ninputs. We provide two efficient approaches that lead to over- and\nunder-approximations of the reachable set. This principle is highly versatile,\nas we show. First, we use it to analyze and enhance the robustness properties\nof both classifiers and regression models. This is in contrast to existing\nworks, which are mainly focused on classification. Specifically, we verify\n(non-)robustness, propose a robust training procedure, and show that our\napproach outperforms adversarial attacks as well as state-of-the-art methods of\nverifying classifiers for non-norm bound perturbations. Second, we provide\ntechniques to distinguish between reliable and non-reliable predictions for\nunlabeled inputs, to quantify the influence of each feature on a prediction,\nand compute a feature ranking.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 10:58:06 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 16:38:47 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Kopetzki", "Anna-Kathrin", ""], ["G\u00fcnnemann", "Stephan", ""]]}, {"id": "2007.14128", "submitter": "Martin Faj\\v{c}\\'ik", "authors": "Martin Fajcik, Josef Jon, Martin Docekal, Pavel Smrz", "title": "BUT-FIT at SemEval-2020 Task 5: Automatic detection of counterfactual\n  statements with deep pre-trained language representation models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper describes BUT-FIT's submission at SemEval-2020 Task 5: Modelling\nCausal Reasoning in Language: Detecting Counterfactuals. The challenge focused\non detecting whether a given statement contains a counterfactual (Subtask 1)\nand extracting both antecedent and consequent parts of the counterfactual from\nthe text (Subtask 2). We experimented with various state-of-the-art language\nrepresentation models (LRMs). We found RoBERTa LRM to perform the best in both\nsubtasks. We achieved the first place in both exact match and F1 for Subtask 2\nand ranked second for Subtask 1.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 11:16:11 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Fajcik", "Martin", ""], ["Jon", "Josef", ""], ["Docekal", "Martin", ""], ["Smrz", "Pavel", ""]]}, {"id": "2007.14132", "submitter": "Anatol Maier", "authors": "Anatol Maier, Benedikt Lorch, Christian Riess", "title": "Toward Reliable Models for Authenticating Multimedia Content: Detecting\n  Resampling Artifacts With Bayesian Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multimedia forensics, learning-based methods provide state-of-the-art\nperformance in determining origin and authenticity of images and videos.\nHowever, most existing methods are challenged by out-of-distribution data,\ni.e., with characteristics that are not covered in the training set. This makes\nit difficult to know when to trust a model, particularly for practitioners with\nlimited technical background.\n  In this work, we make a first step toward redesigning forensic algorithms\nwith a strong focus on reliability. To this end, we propose to use Bayesian\nneural networks (BNN), which combine the power of deep neural networks with the\nrigorous probabilistic formulation of a Bayesian framework. Instead of\nproviding a point estimate like standard neural networks, BNNs provide\ndistributions that express both the estimate and also an uncertainty range.\n  We demonstrate the usefulness of this framework on a classical forensic task:\nresampling detection. The BNN yields state-of-the-art detection performance,\nplus excellent capabilities for detecting out-of-distribution samples. This is\ndemonstrated for three pathologic issues in resampling detection, namely unseen\nresampling factors, unseen JPEG compression, and unseen resampling algorithms.\nWe hope that this proposal spurs further research toward reliability in\nmultimedia forensics.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 11:23:40 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Maier", "Anatol", ""], ["Lorch", "Benedikt", ""], ["Riess", "Christian", ""]]}, {"id": "2007.14166", "submitter": "Derya Soydaner", "authors": "Derya Soydaner", "title": "A Comparison of Optimization Algorithms for Deep Learning", "comments": "International Journal of Pattern Recognition and Artificial\n  Intelligence, 2020", "journal-ref": null, "doi": "10.1142/S0218001420520138", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, we have witnessed the rise of deep learning. Deep neural\nnetworks have proved their success in many areas. However, the optimization of\nthese networks has become more difficult as neural networks going deeper and\ndatasets becoming bigger. Therefore, more advanced optimization algorithms have\nbeen proposed over the past years. In this study, widely used optimization\nalgorithms for deep learning are examined in detail. To this end, these\nalgorithms called adaptive gradient methods are implemented for both supervised\nand unsupervised tasks. The behaviour of the algorithms during training and\nresults on four image datasets, namely, MNIST, CIFAR-10, Kaggle Flowers and\nLabeled Faces in the Wild are compared by pointing out their differences\nagainst basic optimization algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 12:42:28 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Soydaner", "Derya", ""]]}, {"id": "2007.14175", "submitter": "Mehdi Ali", "authors": "Mehdi Ali, Max Berrendorf, Charles Tapley Hoyt, Laurent Vermue, Sahand\n  Sharifzadeh, Volker Tresp, and Jens Lehmann", "title": "PyKEEN 1.0: A Python Library for Training and Evaluating Knowledge Graph\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, knowledge graph embeddings (KGEs) received significant attention,\nand several software libraries have been developed for training and evaluating\nKGEs. While each of them addresses specific needs, we re-designed and\nre-implemented PyKEEN, one of the first KGE libraries, in a community effort.\nPyKEEN 1.0 enables users to compose knowledge graph embedding models (KGEMs)\nbased on a wide range of interaction models, training approaches, loss\nfunctions, and permits the explicit modeling of inverse relations. Besides, an\nautomatic memory optimization has been realized in order to exploit the\nprovided hardware optimally, and through the integration of Optuna extensive\nhyper-parameter optimization (HPO) functionalities are provided.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 12:54:28 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 08:57:27 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Ali", "Mehdi", ""], ["Berrendorf", "Max", ""], ["Hoyt", "Charles Tapley", ""], ["Vermue", "Laurent", ""], ["Sharifzadeh", "Sahand", ""], ["Tresp", "Volker", ""], ["Lehmann", "Jens", ""]]}, {"id": "2007.14184", "submitter": "Francesco Locatello", "authors": "Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar R\\\"atsch,\n  Sylvain Gelly, Bernhard Sch\\\"olkopf, Olivier Bachem", "title": "A Commentary on the Unsupervised Learning of Disentangled\n  Representations", "comments": null, "journal-ref": "The Thirty-Fourth AAAI Conference on Artificial Intelligence 2020\n  (AAAI-20)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of the unsupervised learning of disentangled representations is to\nseparate the independent explanatory factors of variation in the data without\naccess to supervision. In this paper, we summarize the results of Locatello et\nal., 2019, and focus on their implications for practitioners. We discuss the\ntheoretical result showing that the unsupervised learning of disentangled\nrepresentations is fundamentally impossible without inductive biases and the\npractical challenges it entails. Finally, we comment on our experimental\nfindings, highlighting the limitations of state-of-the-art approaches and\ndirections for future research.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 13:13:45 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Locatello", "Francesco", ""], ["Bauer", "Stefan", ""], ["Lucic", "Mario", ""], ["R\u00e4tsch", "Gunnar", ""], ["Gelly", "Sylvain", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Bachem", "Olivier", ""]]}, {"id": "2007.14189", "submitter": "Seongjin Choi", "authors": "Seongjin Choi, Jiwon Kim, Hwasoo Yeo", "title": "TrajGAIL: Generating Urban Vehicle Trajectories using Generative\n  Adversarial Imitation Learning", "comments": "25 pages, 10 figures, 2 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, an abundant amount of urban vehicle trajectory data has been\ncollected in road networks. Many studies have used machine learning algorithms\nto analyze patterns in vehicle trajectories to predict location sequences of\nindividual travelers. Unlike the previous studies that used a discriminative\nmodeling approach, this research suggests a generative modeling approach to\nlearn the underlying distributions of urban vehicle trajectory data. A\ngenerative model for urban vehicle trajectories can better generalize from\ntraining data by learning the underlying distribution of the training data and,\nthus, produce synthetic vehicle trajectories similar to real vehicle\ntrajectories with limited observations. Synthetic trajectories can provide\nsolutions to data sparsity or data privacy issues in using location data. This\nresearch proposesTrajGAIL, a generative adversarial imitation learning\nframework for the urban vehicle trajectory generation. In TrajGAIL, learning\nlocation sequences in observed trajectories is formulated as an imitation\nlearning problem in a partially observable Markov decision process. The model\nis trained by the generative adversarial framework, which uses the reward\nfunction from the adversarial discriminator. The model is tested with both\nsimulation and real-world datasets, and the results show that the proposed\nmodel obtained significant performance gains compared to existing models in\nsequence modeling.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 13:17:51 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 07:47:02 GMT"}, {"version": "v3", "created": "Fri, 21 Aug 2020 01:54:38 GMT"}, {"version": "v4", "created": "Sat, 16 Jan 2021 02:41:58 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Choi", "Seongjin", ""], ["Kim", "Jiwon", ""], ["Yeo", "Hwasoo", ""]]}, {"id": "2007.14191", "submitter": "Nicolas Papernot", "authors": "Nicolas Papernot, Abhradeep Thakurta, Shuang Song, Steve Chien,\n  \\'Ulfar Erlingsson", "title": "Tempered Sigmoid Activations for Deep Learning with Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because learning sometimes involves sensitive data, machine learning\nalgorithms have been extended to offer privacy for training data. In practice,\nthis has been mostly an afterthought, with privacy-preserving models obtained\nby re-running training with a different optimizer, but using the model\narchitectures that already performed well in a non-privacy-preserving setting.\nThis approach leads to less than ideal privacy/utility tradeoffs, as we show\nhere. Instead, we propose that model architectures are chosen ab initio\nexplicitly for privacy-preserving training.\n  To provide guarantees under the gold standard of differential privacy, one\nmust bound as strictly as possible how individual training points can possibly\naffect model updates. In this paper, we are the first to observe that the\nchoice of activation function is central to bounding the sensitivity of\nprivacy-preserving deep learning. We demonstrate analytically and\nexperimentally how a general family of bounded activation functions, the\ntempered sigmoids, consistently outperform unbounded activation functions like\nReLU. Using this paradigm, we achieve new state-of-the-art accuracy on MNIST,\nFashionMNIST, and CIFAR10 without any modification of the learning procedure\nfundamentals or differential privacy analysis.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 13:19:45 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Papernot", "Nicolas", ""], ["Thakurta", "Abhradeep", ""], ["Song", "Shuang", ""], ["Chien", "Steve", ""], ["Erlingsson", "\u00dalfar", ""]]}, {"id": "2007.14209", "submitter": "Zhiyan Ding", "authors": "Zhiyan Ding and Qin Li", "title": "Langevin Monte Carlo: random coordinate descent and variance reduction", "comments": "arXiv admin note: text overlap with arXiv:2006.06068", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sampling from a log-concave distribution function on $\\mathbb{R}^d$ (with\n$d\\gg 1$) is a popular problem that has wide applications. In this paper we\nstudy the application of random coordinate descent method (RCD) on the Langevin\nMonte Carlo (LMC) sampling method, and we find two sides of the theory:\n  1. The direct application of RCD on LMC does reduce the number of finite\ndifferencing approximations per iteration, but it induces a large variance\nerror term. More iterations are then needed, and ultimately the method gains no\ncomputational advantage;\n  2. When variance reduction techniques (such as SAGA and SVRG) are\nincorporated in RCD-LMC, the variance error term is reduced. The new methods,\ncompared to the vanilla LMC, reduce the total computational cost by $d$ folds,\nand achieve the optimal cost rate.\n  We perform our investigations in both overdamped and underdamped settings.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 18:14:36 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 20:22:59 GMT"}, {"version": "v3", "created": "Fri, 12 Feb 2021 17:14:10 GMT"}, {"version": "v4", "created": "Tue, 9 Mar 2021 23:45:10 GMT"}, {"version": "v5", "created": "Sun, 4 Jul 2021 00:01:15 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Ding", "Zhiyan", ""], ["Li", "Qin", ""]]}, {"id": "2007.14216", "submitter": "Paul Platzer", "authors": "P Platzer, P. Yiou (LSCE), P. Naveau (LSCE), P Tandeo, Y Zhen, P\n  Ailliot (LMBA), J-F Filipot", "title": "Using local dynamics to explain analog forecasting of chaotic systems", "comments": "Article submitted to the Journal of Atmospheric Sciences", "journal-ref": null, "doi": "10.1175/JAS-D-20-0204.1", "report-no": null, "categories": "physics.data-an nlin.CD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analogs are nearest neighbors of the state of a system. By using analogs and\ntheir successors in time, one is able to produce empirical forecasts. Several\nanalog forecasting methods have been used in atmospheric applications and\ntested on well-known dynamical systems. Although efficient in practice,\ntheoretical connections between analog methods and dynamical systems have been\noverlooked. Analog forecasting can be related to the real dynamical equations\nof the system of interest. This study investigates the properties of different\nanalog forecasting strategies by taking local approximations of the system's\ndynamics. We find that analog forecasting performances are highly linked to the\nlocal Jacobian matrix of the flow map, and that analog forecasting combined\nwith linear regression allows to capture projections of this Jacobian matrix.\nThe proposed methodology allows to estimate analog forecasting errors, and to\ncompare different analog methods. These results are derived analytically and\ntested numerically on two simple chaotic dynamical systems.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 08:43:11 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Platzer", "P", "", "LSCE"], ["Yiou", "P.", "", "LSCE"], ["Naveau", "P.", "", "LSCE"], ["Tandeo", "P", "", "LMBA"], ["Zhen", "Y", "", "LMBA"], ["Ailliot", "P", "", "LMBA"], ["Filipot", "J-F", ""]]}, {"id": "2007.14229", "submitter": "Diego Marcondes", "authors": "Diego Marcondes", "title": "Parameter estimation in dynamical systems via Statistical Learning: a\n  reinterpretation of Approximate Bayesian Computation applied to COVID-19\n  spread", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a robust parameter estimation method for dynamical systems based\non Statistical Learning techniques which aims to estimate a set of parameters\nthat well fit the dynamics in order to obtain robust evidences about the\nqualitative behaviour of its trajectory. The method is quite general and\nflexible, since it does not rely on any specific property of the dynamical\nsystem, and represents a reinterpretation of Approximate Bayesian Computation\nmethods through the lens of Statistical Learning. The method is specially\nuseful for estimating parameters in epidemiological compartmental models in\norder to obtain qualitative properties of a disease evolution. We apply it to\nsimulated and real data about COVID-19 spread in the US in order to evaluate\nqualitatively its evolution over time, showing how one may assess the\neffectiveness of measures implemented to slow the spread and some qualitative\nfeatures of the disease current and future evolution.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 13:52:20 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 18:38:05 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Marcondes", "Diego", ""]]}, {"id": "2007.14245", "submitter": "Abhinav Sagar", "authors": "Abhinav Sagar", "title": "Bayesian Multi Scale Neural Network for Crowd Counting", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd Counting is a difficult but important problem in computer vision.\nConvolutional Neural Networks based on estimating the density map over the\nimage has been highly successful in this domain. However dense crowd counting\nremains an open problem because of severe occlusion and perspective view in\nwhich people can be present at various sizes. In this work, we propose a new\nnetwork which uses a ResNet based feature extractor, downsampling block which\nuses dilated convolutions and upsampling block using transposed convolutions.\nWe present a novel aggregation module which makes our network robust to the\nperspective view problem. We present the optimization details, loss functions\nand the algorithm used in our work. On evaluating on ShanghaiTech, UCF-CC-50\nand UCF-QNRF datasets using MSE and MAE as evaluation metrics, our network\noutperforms previous state of the art approaches while giving uncertainty\nestimates in a principled bayesian manner.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 21:43:20 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 19:57:51 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Sagar", "Abhinav", ""]]}, {"id": "2007.14254", "submitter": "Farzaneh Khoshnevisan", "authors": "Farzaneh Khoshnevisan, Zhewen Fan, Vitor R. Carvalho", "title": "Improving Robustness on Seasonality-Heavy Multivariate Time Series\n  Anomaly Detection", "comments": "arXiv admin note: substantial text overlap with arXiv:1911.07104", "journal-ref": "KDD Workshop on Mining and Learning from Time Series, 2020", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust Anomaly Detection (AD) on time series data is a key component for\nmonitoring many complex modern systems. These systems typically generate\nhigh-dimensional time series that can be highly noisy, seasonal, and\ninter-correlated. This paper explores some of the challenges in such data, and\nproposes a new approach that makes inroads towards increased robustness on\nseasonal and contaminated data, while providing a better root cause\nidentification of anomalies. In particular, we propose the use of Robust\nSeasonal Multivariate Generative Adversarial Network (RSM-GAN) that extends\nrecent advancements in GAN with the adoption of convolutional-LSTM layers and\nattention mechanisms to produce excellent performance on various settings. We\nconduct extensive experiments in which not only do this model displays more\nrobust behavior on complex seasonality patterns, but also shows increased\nresistance to training data contamination. We compare it with existing\nclassical and deep-learning AD models, and show that this architecture is\nassociated with the lowest false positive rate and improves precision by 30%\nand 16% in real-world and synthetic data, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 01:32:00 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Khoshnevisan", "Farzaneh", ""], ["Fan", "Zhewen", ""], ["Carvalho", "Vitor R.", ""]]}, {"id": "2007.14272", "submitter": "Or Yair", "authors": "Or Yair, Almog Lahav, and Ronen Talmon", "title": "Symmetric Positive Semi-definite Riemannian Geometry with Application to\n  Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present new results on the Riemannian geometry of symmetric\npositive semi-definite (SPSD) matrices. First, based on an existing\napproximation of the geodesic path, we introduce approximations of the\nlogarithmic and exponential maps. Second, we present a closed-form expression\nfor Parallel Transport (PT). Third, we derive a canonical representation for a\nset of SPSD matrices. Based on these results, we propose an algorithm for\nDomain Adaptation (DA) and demonstrate its performance in two applications:\nfusion of hyper-spectral images and motion identification.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 14:39:36 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 16:00:03 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Yair", "Or", ""], ["Lahav", "Almog", ""], ["Talmon", "Ronen", ""]]}, {"id": "2007.14281", "submitter": "Konstantinos Voulgaris", "authors": "Konstantinos A. Voulgaris, Mike E. Davies, Mehrdad Yaghoobi", "title": "DeepMP for Non-Negative Sparse Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-negative signals form an important class of sparse signals. Many\nalgorithms have already beenproposed to recover such non-negative\nrepresentations, where greedy and convex relaxed algorithms are among the most\npopular methods. The greedy techniques are low computational cost algorithms,\nwhich have also been modified to incorporate the non-negativity of the\nrepresentations. One such modification has been proposed for Matching Pursuit\n(MP) based algorithms, which first chooses positive coefficients and uses a\nnon-negative optimisation technique that guarantees the non-negativity of the\ncoefficients. The performance of greedy algorithms, like all non-exhaustive\nsearch methods, suffer from high coherence with the linear generative model,\ncalled the dictionary. We here first reformulate the non-negative matching\npursuit algorithm in the form of a deep neural network. We then show that the\nproposed model after training yields a significant improvement in terms of\nexact recovery performance, compared to other non-trained greedy algorithms,\nwhile keeping the complexity low.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 14:52:06 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Voulgaris", "Konstantinos A.", ""], ["Davies", "Mike E.", ""], ["Yaghoobi", "Mehrdad", ""]]}, {"id": "2007.14285", "submitter": "Han Feng", "authors": "Zhiying Fang, Han Feng, Shuo Huang, Ding-Xuan Zhou", "title": "Theory of Deep Convolutional Neural Networks II: Spherical Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.FA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based on deep neural networks of various structures and\narchitectures has been powerful in many practical applications, but it lacks\nenough theoretical verifications. In this paper, we consider a family of deep\nconvolutional neural networks applied to approximate functions on the unit\nsphere $\\mathbb{S}^{d-1}$ of $\\mathbb{R}^d$. Our analysis presents rates of\nuniform approximation when the approximated function lies in the Sobolev space\n$W^r_\\infty (\\mathbb{S}^{d-1})$ with $r>0$ or takes an additive ridge form. Our\nwork verifies theoretically the modelling and approximation ability of deep\nconvolutional neural networks followed by downsampling and one fully connected\nlayer or two. The key idea of our spherical analysis is to use the inner\nproduct form of the reproducing kernels of the spaces of spherical harmonics\nand then to apply convolutional factorizations of filters to realize the\ngenerated linear features.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 14:54:30 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Fang", "Zhiying", ""], ["Feng", "Han", ""], ["Huang", "Shuo", ""], ["Zhou", "Ding-Xuan", ""]]}, {"id": "2007.14294", "submitter": "Xiaoyu Li", "authors": "Xiaoyu Li, Francesco Orabona", "title": "A High Probability Analysis of Adaptive SGD with Momentum", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Descent (SGD) and its variants are the most used\nalgorithms in machine learning applications. In particular, SGD with adaptive\nlearning rates and momentum is the industry standard to train deep networks.\nDespite the enormous success of these methods, our theoretical understanding of\nthese variants in the nonconvex setting is not complete, with most of the\nresults only proving convergence in expectation and with strong assumptions on\nthe stochastic gradients. In this paper, we present a high probability analysis\nfor adaptive and momentum algorithms, under weak assumptions on the function,\nstochastic gradients, and learning rates. We use it to prove for the first time\nthe convergence of the gradients to zero in high probability in the smooth\nnonconvex setting for Delayed AdaGrad with momentum.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 15:06:22 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Li", "Xiaoyu", ""], ["Orabona", "Francesco", ""]]}, {"id": "2007.14313", "submitter": "Zhiqin Xu", "authors": "Zhi-Qin John Xu, Hanxu Zhou", "title": "Deep frequency principle towards understanding why deeper learning is\n  faster", "comments": null, "journal-ref": "AAAI-2021", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the effect of depth in deep learning is a critical problem. In\nthis work, we utilize the Fourier analysis to empirically provide a promising\nmechanism to understand why feedforward deeper learning is faster. To this end,\nwe separate a deep neural network, trained by normal stochastic gradient\ndescent, into two parts during analysis, i.e., a pre-condition component and a\nlearning component, in which the output of the pre-condition one is the input\nof the learning one. We use a filtering method to characterize the frequency\ndistribution of a high-dimensional function. Based on experiments of deep\nnetworks and real dataset, we propose a deep frequency principle, that is, the\neffective target function for a deeper hidden layer biases towards lower\nfrequency during the training. Therefore, the learning component effectively\nlearns a lower frequency function if the pre-condition component has more\nlayers. Due to the well-studied frequency principle, i.e., deep neural networks\nlearn lower frequency functions faster, the deep frequency principle provides a\nreasonable explanation to why deeper learning is faster. We believe these\nempirical studies would be valuable for future theoretical studies of the\neffect of depth in deep learning.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 15:35:49 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2020 05:24:13 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Xu", "Zhi-Qin John", ""], ["Zhou", "Hanxu", ""]]}, {"id": "2007.14321", "submitter": "Christopher A. Choquette-Choo", "authors": "Christopher A. Choquette-Choo, Florian Tramer, Nicholas Carlini, and\n  Nicolas Papernot", "title": "Label-Only Membership Inference Attacks", "comments": "16 pages, 11 figures, 2 tables Revision 2: 19 pages, 12 figures, 3\n  tables. Improved text and additional experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Membership inference attacks are one of the simplest forms of privacy leakage\nfor machine learning models: given a data point and model, determine whether\nthe point was used to train the model. Existing membership inference attacks\nexploit models' abnormal confidence when queried on their training data. These\nattacks do not apply if the adversary only gets access to models' predicted\nlabels, without a confidence measure. In this paper, we introduce label-only\nmembership inference attacks. Instead of relying on confidence scores, our\nattacks evaluate the robustness of a model's predicted labels under\nperturbations to obtain a fine-grained membership signal. These perturbations\ninclude common data augmentations or adversarial examples. We empirically show\nthat our label-only membership inference attacks perform on par with prior\nattacks that required access to model confidences. We further demonstrate that\nlabel-only attacks break multiple defenses against membership inference attacks\nthat (implicitly or explicitly) rely on a phenomenon we call confidence\nmasking. These defenses modify a model's confidence scores in order to thwart\nattacks, but leave the model's predicted labels unchanged. Our label-only\nattacks demonstrate that confidence-masking is not a viable defense strategy\nagainst membership inference. Finally, we investigate worst-case label-only\nattacks, that infer membership for a small number of outlier data points. We\nshow that label-only attacks also match confidence-based attacks in this\nsetting. We find that training models with differential privacy and (strong) L2\nregularization are the only known defense strategies that successfully prevents\nall attacks. This remains true even when the differential privacy budget is too\nhigh to offer meaningful provable guarantees.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 15:44:31 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 01:42:20 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Choquette-Choo", "Christopher A.", ""], ["Tramer", "Florian", ""], ["Carlini", "Nicholas", ""], ["Papernot", "Nicolas", ""]]}, {"id": "2007.14365", "submitter": "Weichi Wu", "authors": "Weichi Wu, Sofia Olhede, Patrick Wolfe", "title": "Tractably Modelling Dependence in Networks Beyond Exchangeability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework for modelling network data that is designed to\ndescribe aspects of non-exchangeable networks. Conditional on latent\n(unobserved) variables, the edges of the network are generated by their finite\ngrowth history (with latent orders) while the marginal probabilities of the\nadjacency matrix are modeled by a generalization of a graph limit function (or\na graphon). In particular, we study the estimation, clustering and degree\nbehavior of the network in our setting. We determine (i) the minimax estimator\nof a composite graphon with respect to squared error loss; (ii) that spectral\nclustering is able to consistently detect the latent membership when the\nblock-wise constant composite graphon is considered under additional\nconditions; and (iii) we are able to construct models with heavy-tailed\nempirical degrees under specific scenarios and parameter choices. This explores\nwhy and under which general conditions non-exchangeable network data can be\ndescribed by a stochastic block model. The new modelling framework is able to\ncapture empirically important characteristics of network data such as sparsity\ncombined with heavy tailed degree distribution, and add understanding as to\nwhat generative mechanisms will make them arise.\n  Keywords: statistical network analysis, exchangeable arrays, stochastic block\nmodel, nonlinear stochastic processes.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 17:13:59 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Wu", "Weichi", ""], ["Olhede", "Sofia", ""], ["Wolfe", "Patrick", ""]]}, {"id": "2007.14372", "submitter": "Weikai Yang", "authors": "Weikai Yang, Zhen Li, Mengchen Liu, Yafeng Lu, Kelei Cao, Ross\n  Maciejewski, Shixia Liu", "title": "Diagnosing Concept Drift with Visual Analytics", "comments": "Accepted for IEEE Conference on Visual Analytics Science and\n  Technology (VAST) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concept drift is a phenomenon in which the distribution of a data stream\nchanges over time in unforeseen ways, causing prediction models built on\nhistorical data to become inaccurate. While a variety of automated methods have\nbeen developed to identify when concept drift occurs, there is limited support\nfor analysts who need to understand and correct their models when drift is\ndetected. In this paper, we present a visual analytics method, DriftVis, to\nsupport model builders and analysts in the identification and correction of\nconcept drift in streaming data. DriftVis combines a distribution-based drift\ndetection method with a streaming scatterplot to support the analysis of drift\ncaused by the distribution changes of data streams and to explore the impact of\nthese changes on the model's accuracy. A quantitative experiment and two case\nstudies on weather prediction and text classification have been conducted to\ndemonstrate our proposed tool and illustrate how visual analytics can be used\nto support the detection, examination, and correction of concept drift.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 17:29:43 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 04:42:25 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2020 04:12:44 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Yang", "Weikai", ""], ["Li", "Zhen", ""], ["Liu", "Mengchen", ""], ["Lu", "Yafeng", ""], ["Cao", "Kelei", ""], ["Maciejewski", "Ross", ""], ["Liu", "Shixia", ""]]}, {"id": "2007.14381", "submitter": "Kensen Shi", "authors": "Augustus Odena, Kensen Shi, David Bieber, Rishabh Singh, Charles\n  Sutton", "title": "BUSTLE: Bottom-Up Program Synthesis Through Learning-Guided Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Program synthesis is challenging largely because of the difficulty of search\nin a large space of programs. Human programmers routinely tackle the task of\nwriting complex programs by writing sub-programs and then analyzing their\nintermediate results to compose them in appropriate ways. Motivated by this\nintuition, we present a new synthesis approach that leverages learning to guide\na bottom-up search over programs. In particular, we train a model to prioritize\ncompositions of intermediate values during search conditioned on a given set of\ninput-output examples. This is a powerful combination because of several\nemergent properties. First, in bottom-up search, intermediate programs can be\nexecuted, providing semantic information to the neural network. Second, given\nthe concrete values from those executions, we can exploit rich features based\non recent work on property signatures. Finally, bottom-up search allows the\nsystem substantial flexibility in what order to generate the solution, allowing\nthe synthesizer to build up a program from multiple smaller sub-programs.\nOverall, our empirical evaluation finds that the combination of learning and\nbottom-up search is remarkably effective, even with simple supervised learning\napproaches. We demonstrate the effectiveness of our technique on two datasets,\none from the SyGuS competition and one of our own creation.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 17:46:18 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 00:23:41 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Odena", "Augustus", ""], ["Shi", "Kensen", ""], ["Bieber", "David", ""], ["Singh", "Rishabh", ""], ["Sutton", "Charles", ""]]}, {"id": "2007.14390", "submitter": "Pedro Porto Buarque de Gusmao", "authors": "Daniel J. Beutel, Taner Topal, Akhil Mathur, Xinchi Qiu, Titouan\n  Parcollet, Pedro P. B. de Gusm\\~ao, Nicholas D. Lane", "title": "Flower: A Friendly Federated Learning Research Framework", "comments": "Open-Source, mobile-friendly Federated Learning framework", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) has emerged as a promising technique for edge devices\nto collaboratively learn a shared prediction model, while keeping their\ntraining data on the device, thereby decoupling the ability to do machine\nlearning from the need to store the data in the cloud. However, FL is difficult\nto implement and deploy in practice, considering the heterogeneity in mobile\ndevices, e.g., different programming languages, frameworks, and hardware\naccelerators. Although there are a few frameworks available to simulate FL\nalgorithms (e.g., TensorFlow Federated), they do not support implementing FL\nworkloads on mobile devices. Furthermore, these frameworks are designed to\nsimulate FL in a server environment and hence do not allow experimentation in\ndistributed mobile settings for a large number of clients. In this paper, we\npresent Flower (https://flower.dev/), a FL framework which is both agnostic\ntowards heterogeneous client environments and also scales to a large number of\nclients, including mobile and embedded devices. Flower's abstractions let\ndevelopers port existing mobile workloads with little overhead, regardless of\nthe programming language or ML framework used, while also allowing researchers\nflexibility to experiment with novel approaches to advance the\nstate-of-the-art. We describe the design goals and implementation\nconsiderations of Flower and show our experiences in evaluating the performance\nof FL across clients with heterogeneous computational and communication\ncapabilities.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 17:59:07 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 11:45:41 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 11:06:54 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Beutel", "Daniel J.", ""], ["Topal", "Taner", ""], ["Mathur", "Akhil", ""], ["Qiu", "Xinchi", ""], ["Parcollet", "Titouan", ""], ["de Gusm\u00e3o", "Pedro P. B.", ""], ["Lane", "Nicholas D.", ""]]}, {"id": "2007.14430", "submitter": "Nino Vieillard", "authors": "Nino Vieillard, Olivier Pietquin, Matthieu Geist", "title": "Munchausen Reinforcement Learning", "comments": "NeurIPS 2020. Code:\n  https://github.com/google-research/google-research/tree/master/munchausen_rl", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Bootstrapping is a core mechanism in Reinforcement Learning (RL). Most\nalgorithms, based on temporal differences, replace the true value of a\ntransiting state by their current estimate of this value. Yet, another estimate\ncould be leveraged to bootstrap RL: the current policy. Our core contribution\nstands in a very simple idea: adding the scaled log-policy to the immediate\nreward. We show that slightly modifying Deep Q-Network (DQN) in that way\nprovides an agent that is competitive with distributional methods on Atari\ngames, without making use of distributional RL, n-step returns or prioritized\nreplay. To demonstrate the versatility of this idea, we also use it together\nwith an Implicit Quantile Network (IQN). The resulting agent outperforms\nRainbow on Atari, installing a new State of the Art with very little\nmodifications to the original algorithm. To add to this empirical study, we\nprovide strong theoretical insights on what happens under the hood -- implicit\nKullback-Leibler regularization and increase of the action-gap.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 18:30:23 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 14:47:38 GMT"}, {"version": "v3", "created": "Wed, 4 Nov 2020 16:46:15 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Vieillard", "Nino", ""], ["Pietquin", "Olivier", ""], ["Geist", "Matthieu", ""]]}, {"id": "2007.14462", "submitter": "Veronica Sanz", "authors": "Charanjit K. Khosa and Veronica Sanz", "title": "Anomaly Awareness", "comments": "8 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG hep-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new Machine Learning algorithm called Anomaly Awareness. By\nmaking our algorithm aware of the presence of a range of different anomalies,\nwe improve its capability to detect anomalous events, even those it had not\nbeen exposed to. As an example of use, we apply this method to searches for new\nphenomena in the Large Hadron Collider. In particular, we analyze events with\nboosted jets where new physics could be hiding.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 16:39:15 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Khosa", "Charanjit K.", ""], ["Sanz", "Veronica", ""]]}, {"id": "2007.14469", "submitter": "Prem Seetharaman", "authors": "Prem Seetharaman, Gordon Wichern, Bryan Pardo, Jonathan Le Roux", "title": "AutoClip: Adaptive Gradient Clipping for Source Separation Networks", "comments": "Accepted at 2020 IEEE International Workshop on Machine Learning for\n  Signal Processing, Sept.\\ 21--24, 2020, Espoo, Finland", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clipping the gradient is a known approach to improving gradient descent, but\nrequires hand selection of a clipping threshold hyperparameter. We present\nAutoClip, a simple method for automatically and adaptively choosing a gradient\nclipping threshold, based on the history of gradient norms observed during\ntraining. Experimental results show that applying AutoClip results in improved\ngeneralization performance for audio source separation networks. Observation of\nthe training dynamics of a separation network trained with and without AutoClip\nshow that AutoClip guides optimization into smoother parts of the loss\nlandscape. AutoClip is very simple to implement and can be integrated readily\ninto a variety of applications across multiple domains.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 20:59:39 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Seetharaman", "Prem", ""], ["Wichern", "Gordon", ""], ["Pardo", "Bryan", ""], ["Roux", "Jonathan Le", ""]]}, {"id": "2007.14472", "submitter": "Li Chen", "authors": "Li Chen, Thomas Hatsukami, Jenq-Neng Hwang, Chun Yuan", "title": "Automated Intracranial Artery Labeling using a Graph Neural Network and\n  Hierarchical Refinement", "comments": "MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically labeling intracranial arteries (ICA) with their anatomical\nnames is beneficial for feature extraction and detailed analysis of\nintracranial vascular structures. There are significant variations in the ICA\ndue to natural and pathological causes, making it challenging for automated\nlabeling. However, the existing public dataset for evaluation of anatomical\nlabeling is limited. We construct a comprehensive dataset with 729 Magnetic\nResonance Angiography scans and propose a Graph Neural Network (GNN) method to\nlabel arteries by classifying types of nodes and edges in an attributed\nrelational graph. In addition, a hierarchical refinement framework is developed\nfor further improving the GNN outputs to incorporate structural and relational\nknowledge about the ICA. Our method achieved a node labeling accuracy of 97.5%,\nand 63.8% of scans were correctly labeled for all Circle of Willis nodes, on a\ntesting set of 105 scans with both healthy and diseased subjects. This is a\nsignificant improvement over available state-of-the-art methods. Automatic\nartery labeling is promising to minimize manual effort in characterizing the\ncomplicated ICA networks and provides valuable information for the\nidentification of geometric risk factors of vascular disease. Our code and\ndataset are available at https://github.com/clatfd/GNN-ARTLABEL.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 06:22:35 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Chen", "Li", ""], ["Hatsukami", "Thomas", ""], ["Hwang", "Jenq-Neng", ""], ["Yuan", "Chun", ""]]}, {"id": "2007.14495", "submitter": "Peter Rousseeuw", "authors": "Jakob Raymaekers, Peter J. Rousseeuw, Mia Hubert", "title": "Class maps for visualizing classification results", "comments": "Appeared online, Technometrics", "journal-ref": null, "doi": "10.1080/00401706.2021.1927849", "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification is a major tool of statistics and machine learning. A\nclassification method first processes a training set of objects with given\nclasses (labels), with the goal of afterward assigning new objects to one of\nthese classes. When running the resulting prediction method on the training\ndata or on test data, it can happen that an object is predicted to lie in a\nclass that differs from its given label. This is sometimes called label bias,\nand raises the question whether the object was mislabeled. The proposed class\nmap reflects the probability that an object belongs to an alternative class,\nhow far it is from the other objects in its given class, and whether some\nobjects lie far from all classes. The goal is to visualize aspects of the\nclassification results to obtain insight in the data. The display is\nconstructed for discriminant analysis, the k-nearest neighbor classifier,\nsupport vector machines, logistic regression, and coupling pairwise\nclassifications. It is illustrated on several benchmark datasets, including\nsome about images and texts.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 21:27:15 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2020 15:34:25 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 13:51:29 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Raymaekers", "Jakob", ""], ["Rousseeuw", "Peter J.", ""], ["Hubert", "Mia", ""]]}, {"id": "2007.14523", "submitter": "Yicun Liu", "authors": "Caojin Zhang, Yicun Liu, Yuanpu Xie, Sofia Ira Ktena, Alykhan Tejani,\n  Akshay Gupta, Pranay Kumar Myana, Deepak Dilipkumar, Suvadip Paul, Ikuhiro\n  Ihara, Prasang Upadhyaya, Ferenc Huszar, Wenzhe Shi", "title": "Model Size Reduction Using Frequency Based Double Hashing for\n  Recommender Systems", "comments": "Paper is accepted to RecSys 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) with sparse input features have been widely used\nin recommender systems in industry. These models have large memory requirements\nand need a huge amount of training data. The large model size usually entails a\ncost, in the range of millions of dollars, for storage and communication with\nthe inference services. In this paper, we propose a hybrid hashing method to\ncombine frequency hashing and double hashing techniques for model size\nreduction, without compromising performance. We evaluate the proposed models on\ntwo product surfaces. In both cases, experiment results demonstrated that we\ncan reduce the model size by around 90 % while keeping the performance on par\nwith the original baselines.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 23:26:17 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Zhang", "Caojin", ""], ["Liu", "Yicun", ""], ["Xie", "Yuanpu", ""], ["Ktena", "Sofia Ira", ""], ["Tejani", "Alykhan", ""], ["Gupta", "Akshay", ""], ["Myana", "Pranay Kumar", ""], ["Dilipkumar", "Deepak", ""], ["Paul", "Suvadip", ""], ["Ihara", "Ikuhiro", ""], ["Upadhyaya", "Prasang", ""], ["Huszar", "Ferenc", ""], ["Shi", "Wenzhe", ""]]}, {"id": "2007.14527", "submitter": "Sifan Wang", "authors": "Sifan Wang, Xinling Yu, Paris Perdikaris", "title": "When and why PINNs fail to train: A neural tangent kernel perspective", "comments": "29 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physics-informed neural networks (PINNs) have lately received great attention\nthanks to their flexibility in tackling a wide range of forward and inverse\nproblems involving partial differential equations. However, despite their\nnoticeable empirical success, little is known about how such constrained neural\nnetworks behave during their training via gradient descent. More importantly,\neven less is known about why such models sometimes fail to train at all. In\nthis work, we aim to investigate these questions through the lens of the Neural\nTangent Kernel (NTK); a kernel that captures the behavior of fully-connected\nneural networks in the infinite width limit during training via gradient\ndescent. Specifically, we derive the NTK of PINNs and prove that, under\nappropriate conditions, it converges to a deterministic kernel that stays\nconstant during training in the infinite-width limit. This allows us to analyze\nthe training dynamics of PINNs through the lens of their limiting NTK and find\na remarkable discrepancy in the convergence rate of the different loss\ncomponents contributing to the total training error. To address this\nfundamental pathology, we propose a novel gradient descent algorithm that\nutilizes the eigenvalues of the NTK to adaptively calibrate the convergence\nrate of the total training error. Finally, we perform a series of numerical\nexperiments to verify the correctness of our theory and the practical\neffectiveness of the proposed algorithms. The data and code accompanying this\nmanuscript are publicly available at\n\\url{https://github.com/PredictiveIntelligenceLab/PINNsNTK}.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 23:44:56 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Wang", "Sifan", ""], ["Yu", "Xinling", ""], ["Perdikaris", "Paris", ""]]}, {"id": "2007.14528", "submitter": "Jie Chen", "authors": "Linwei Hu, Jie Chen, Vijayan N. Nair, Agus Sudjianto", "title": "Surrogate Locally-Interpretable Models with Supervised Machine Learning\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised Machine Learning (SML) algorithms, such as Gradient Boosting,\nRandom Forest, and Neural Networks, have become popular in recent years due to\ntheir superior predictive performance over traditional statistical methods.\nHowever, their complexity makes the results hard to interpret without\nadditional tools. There has been a lot of recent work in developing global and\nlocal diagnostics for interpreting SML models. In this paper, we propose a\nlocally-interpretable model that takes the fitted ML response surface,\npartitions the predictor space using model-based regression trees, and fits\ninterpretable main-effects models at each of the nodes. We adapt the algorithm\nto be efficient in dealing with high-dimensional predictors. While the main\nfocus is on interpretability, the resulting surrogate model also has reasonably\ngood predictive performance.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 23:46:16 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Hu", "Linwei", ""], ["Chen", "Jie", ""], ["Nair", "Vijayan N.", ""], ["Sudjianto", "Agus", ""]]}, {"id": "2007.14535", "submitter": "Masashi Okada Dr", "authors": "Masashi Okada, Tadahiro Taniguchi", "title": "Dreaming: Model-based Reinforcement Learning by Latent Imagination\n  without Reconstruction", "comments": "Accepted to ICRA2021. Camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper, we propose a decoder-free extension of Dreamer, a\nleading model-based reinforcement learning (MBRL) method from pixels. Dreamer\nis a sample- and cost-efficient solution to robot learning, as it is used to\ntrain latent state-space models based on a variational autoencoder and to\nconduct policy optimization by latent trajectory imagination. However, this\nautoencoding based approach often causes object vanishing, in which the\nautoencoder fails to perceives key objects for solving control tasks, and thus\nsignificantly limiting Dreamer's potential. This work aims to relieve this\nDreamer's bottleneck and enhance its performance by means of removing the\ndecoder. For this purpose, we firstly derive a likelihood-free and InfoMax\nobjective of contrastive learning from the evidence lower bound of Dreamer.\nSecondly, we incorporate two components, (i) independent linear dynamics and\n(ii) the random crop data augmentation, to the learning scheme so as to improve\nthe training performance. In comparison to Dreamer and other recent model-free\nreinforcement learning methods, our newly devised Dreamer with InfoMax and\nwithout generative decoder (Dreaming) achieves the best scores on 5 difficult\nsimulated robotics tasks, in which Dreamer suffers from object vanishing.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 00:14:40 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 03:56:41 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Okada", "Masashi", ""], ["Taniguchi", "Tadahiro", ""]]}, {"id": "2007.14539", "submitter": "Dhruv Rohatgi", "authors": "Constantinos Daskalakis, Dhruv Rohatgi, Manolis Zampetakis", "title": "Truncated Linear Regression in High Dimensions", "comments": "30 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As in standard linear regression, in truncated linear regression, we are\ngiven access to observations $(A_i, y_i)_i$ whose dependent variable equals\n$y_i= A_i^{\\rm T} \\cdot x^* + \\eta_i$, where $x^*$ is some fixed unknown vector\nof interest and $\\eta_i$ is independent noise; except we are only given an\nobservation if its dependent variable $y_i$ lies in some \"truncation set\" $S\n\\subset \\mathbb{R}$. The goal is to recover $x^*$ under some favorable\nconditions on the $A_i$'s and the noise distribution. We prove that there\nexists a computationally and statistically efficient method for recovering\n$k$-sparse $n$-dimensional vectors $x^*$ from $m$ truncated samples, which\nattains an optimal $\\ell_2$ reconstruction error of $O(\\sqrt{(k \\log n)/m})$.\nAs a corollary, our guarantees imply a computationally efficient and\ninformation-theoretically optimal algorithm for compressed sensing with\ntruncation, which may arise from measurement saturation effects. Our result\nfollows from a statistical and computational analysis of the Stochastic\nGradient Descent (SGD) algorithm for solving a natural adaptation of the LASSO\noptimization problem that accommodates truncation. This generalizes the works\nof both: (1) [Daskalakis et al. 2018], where no regularization is needed due to\nthe low-dimensionality of the data, and (2) [Wainright 2009], where the\nobjective function is simple due to the absence of truncation. In order to deal\nwith both truncation and high-dimensionality at the same time, we develop new\ntechniques that not only generalize the existing ones but we believe are of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 00:31:34 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Rohatgi", "Dhruv", ""], ["Zampetakis", "Manolis", ""]]}, {"id": "2007.14546", "submitter": "Jun Shu", "authors": "Jun Shu, Yanwen Zhu, Qian Zhao, Zongben Xu, Deyu Meng", "title": "MLR-SNet: Transferable LR Schedules for Heterogeneous Tasks", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The learning rate (LR) is one of the most important hyper-parameters in\nstochastic gradient descent (SGD) algorithm for training deep neural networks\n(DNN). However, current hand-designed LR schedules need to manually pre-specify\na fixed form, which limits their ability to adapt practical non-convex\noptimization problems due to the significant diversification of training\ndynamics. Meanwhile, it always needs to search proper LR schedules from scratch\nfor new tasks, which, however, are often largely different with task\nvariations, like data modalities, network architectures, or training data\ncapacities. To address this learning-rate-schedule setting issues, we propose\nto parameterize LR schedules with an explicit mapping formulation, called\n\\textit{MLR-SNet}. The learnable parameterized structure brings more\nflexibility for MLR-SNet to learn a proper LR schedule to comply with the\ntraining dynamics of DNN. Image and text classification benchmark experiments\nsubstantiate the capability of our method for achieving proper LR schedules.\nMoreover, the explicit parameterized structure makes the meta-learned LR\nschedules capable of being transferable and plug-and-play, which can be easily\ngeneralized to new heterogeneous tasks. We transfer our meta-learned MLR-SNet\nto query tasks like different training epochs, network architectures, data\nmodalities, dataset sizes from the training ones, and achieve comparable or\neven better performance compared with hand-designed LR schedules specifically\ndesigned for the query tasks. The robustness of MLR-SNet is also substantiated\nwhen the training data are biased with corrupted noise. We further prove the\nconvergence of the SGD algorithm equipped with LR schedule produced by our\nMLR-Net, with the convergence rate comparable to the best-known ones of the\nalgorithm for solving the problem.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 01:18:58 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 01:21:06 GMT"}, {"version": "v3", "created": "Thu, 13 May 2021 15:39:27 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Shu", "Jun", ""], ["Zhu", "Yanwen", ""], ["Zhao", "Qian", ""], ["Xu", "Zongben", ""], ["Meng", "Deyu", ""]]}, {"id": "2007.14550", "submitter": "Hyeong Soo Chang", "authors": "Hyeong Soo Chang", "title": "An Index-based Deterministic Asymptotically Optimal Algorithm for\n  Constrained Multi-armed Bandit Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the model of constrained multi-armed bandit, we show that by construction\nthere exists an index-based deterministic asymptotically optimal algorithm. The\noptimality is achieved by the convergence of the probability of choosing an\noptimal feasible arm to one over infinite horizon. The algorithm is built upon\nLocatelli et al.'s \"anytime parameter-free thresholding\" algorithm under the\nassumption that the optimal value is known. We provide a finite-time bound to\nthe probability of the asymptotic optimality given as 1-O(|A|Te^{-T}) where T\nis the horizon size and A is the set of the arms in the bandit. We then study a\nrelaxed-version of the algorithm in a general form that estimates the optimal\nvalue and discuss the asymptotic optimality of the algorithm after a\nsufficiently large T with examples.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 01:54:22 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Chang", "Hyeong Soo", ""]]}, {"id": "2007.14573", "submitter": "Yaliang Li", "authors": "Yuexiang Xie, Zhen Wang, Yaliang Li, Bolin Ding, Nezihe Merve G\\\"urel,\n  Ce Zhang, Minlie Huang, Wei Lin, Jingren Zhou", "title": "FIVES: Feature Interaction Via Edge Search for Large-Scale Tabular Data", "comments": "Accepted by KDD-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-order interactive features capture the correlation between different\ncolumns and thus are promising to enhance various learning tasks on ubiquitous\ntabular data. To automate the generation of interactive features, existing\nworks either explicitly traverse the feature space or implicitly express the\ninteractions via intermediate activations of some designed models. These two\nkinds of methods show that there is essentially a trade-off between feature\ninterpretability and search efficiency. To possess both of their merits, we\npropose a novel method named Feature Interaction Via Edge Search (FIVES), which\nformulates the task of interactive feature generation as searching for edges on\nthe defined feature graph. Specifically, we first present our theoretical\nevidence that motivates us to search for useful interactive features with\nincreasing order. Then we instantiate this search strategy by optimizing both a\ndedicated graph neural network (GNN) and the adjacency tensor associated with\nthe defined feature graph. In this way, the proposed FIVES method simplifies\nthe time-consuming traversal as a typical training course of GNN and enables\nexplicit feature generation according to the learned adjacency tensor.\nExperimental results on both benchmark and real-world datasets show the\nadvantages of FIVES over several state-of-the-art methods. Moreover, the\ninteractive features identified by FIVES are deployed on the recommender system\nof Taobao, a worldwide leading e-commerce platform. Results of an online A/B\ntesting further verify the effectiveness of the proposed method FIVES, and we\nfurther provide FIVES as AI utilities for the customers of Alibaba Cloud.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 03:33:18 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 03:00:12 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Xie", "Yuexiang", ""], ["Wang", "Zhen", ""], ["Li", "Yaliang", ""], ["Ding", "Bolin", ""], ["G\u00fcrel", "Nezihe Merve", ""], ["Zhang", "Ce", ""], ["Huang", "Minlie", ""], ["Lin", "Wei", ""], ["Zhou", "Jingren", ""]]}, {"id": "2007.14581", "submitter": "Zhemin Li", "authors": "Zhemin Li, Zhi-Qin John Xu, Tao Luo, Hongxia Wang", "title": "A regularized deep matrix factorized model of matrix completion for\n  image restoration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been an important approach of using matrix completion to perform image\nrestoration. Most previous works on matrix completion focus on the low-rank\nproperty by imposing explicit constraints on the recovered matrix, such as the\nconstraint of the nuclear norm or limiting the dimension of the matrix\nfactorization component. Recently, theoretical works suggest that deep linear\nneural network has an implicit bias towards low rank on matrix completion.\nHowever, low rank is not adequate to reflect the intrinsic characteristics of a\nnatural image. Thus, algorithms with only the constraint of low rank are\ninsufficient to perform image restoration well. In this work, we propose a\nRegularized Deep Matrix Factorized (RDMF) model for image restoration, which\nutilizes the implicit bias of the low rank of deep neural networks and the\nexplicit bias of total variation. We demonstrate the effectiveness of our RDMF\nmodel with extensive experiments, in which our method surpasses the state of\nart models in common examples, especially for the restoration from very few\nobservations. Our work sheds light on a more general framework for solving\nother inverse problems by combining the implicit bias of deep learning with\nexplicit regularization.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 04:05:35 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Li", "Zhemin", ""], ["Xu", "Zhi-Qin John", ""], ["Luo", "Tao", ""], ["Wang", "Hongxia", ""]]}, {"id": "2007.14589", "submitter": "Xiaoxiao Li", "authors": "Xiaoxiao Li, Yuan Zhou, Nicha C. Dvornek, Muhan Zhang, Juntang Zhuang,\n  Pamela Ventola, and James S Duncan", "title": "Pooling Regularized Graph Neural Network for fMRI Biomarker Analysis", "comments": "11 pages, 4 figures", "journal-ref": "MICCAI 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how certain brain regions relate to a specific neurological\ndisorder has been an important area of neuroimaging research. A promising\napproach to identify the salient regions is using Graph Neural Networks (GNNs),\nwhich can be used to analyze graph structured data, e.g. brain networks\nconstructed by functional magnetic resonance imaging (fMRI). We propose an\ninterpretable GNN framework with a novel salient region selection mechanism to\ndetermine neurological brain biomarkers associated with disorders.\nSpecifically, we design novel regularized pooling layers that highlight salient\nregions of interests (ROIs) so that we can infer which ROIs are important to\nidentify a certain disease based on the node pooling scores calculated by the\npooling layers. Our proposed framework, Pooling Regularized-GNN (PR-GNN),\nencourages reasonable ROI-selection and provides flexibility to preserve either\nindividual- or group-level patterns. We apply the PR-GNN framework on a\nBiopoint Autism Spectral Disorder (ASD) fMRI dataset. We investigate different\nchoices of the hyperparameters and show that PR-GNN outperforms baseline\nmethods in terms of classification accuracy. The salient ROI detection results\nshow high correspondence with the previous neuroimaging-derived biomarkers for\nASD.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 04:19:36 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Li", "Xiaoxiao", ""], ["Zhou", "Yuan", ""], ["Dvornek", "Nicha C.", ""], ["Zhang", "Muhan", ""], ["Zhuang", "Juntang", ""], ["Ventola", "Pamela", ""], ["Duncan", "James S", ""]]}, {"id": "2007.14604", "submitter": "Lars Hertel", "authors": "Lars Hertel, Pierre Baldi, Daniel L. Gillen", "title": "Quantity vs. Quality: On Hyperparameter Optimization for Deep\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning algorithms can show strong variation in performance\nbetween training runs with different random seeds. In this paper we explore how\nthis affects hyperparameter optimization when the goal is to find\nhyperparameter settings that perform well across random seeds. In particular,\nwe benchmark whether it is better to explore a large quantity of hyperparameter\nsettings via pruning of bad performers, or if it is better to aim for quality\nof collected results by using repetitions. For this we consider the Successive\nHalving, Random Search, and Bayesian Optimization algorithms, the latter two\nwith and without repetitions. We apply these to tuning the PPO2 algorithm on\nthe Cartpole balancing task and the Inverted Pendulum Swing-up task. We\ndemonstrate that pruning may negatively affect the optimization and that\nrepeated sampling does not help in finding hyperparameter settings that perform\nbetter across random seeds. From our experiments we conclude that Bayesian\noptimization with a noise robust acquisition function is the best choice for\nhyperparameter optimization in reinforcement learning tasks.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 05:12:34 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 06:16:00 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Hertel", "Lars", ""], ["Baldi", "Pierre", ""], ["Gillen", "Daniel L.", ""]]}, {"id": "2007.14612", "submitter": "Feng Liu", "authors": "Yiyang Zhang, Feng Liu, Zhen Fang, Bo Yuan, Guangquan Zhang, Jie Lu", "title": "Clarinet: A One-step Approach Towards Budget-friendly Unsupervised\n  Domain Adaptation", "comments": "This paper has been accepted by IJCAI-PRICAI 2020. Yiyang Zhang, Feng\n  Liu and Zhen Fang equally contribute to this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In unsupervised domain adaptation (UDA), classifiers for the target domain\nare trained with massive true-label data from the source domain and unlabeled\ndata from the target domain. However, it may be difficult to collect\nfully-true-label data in a source domain given a limited budget. To mitigate\nthis problem, we consider a novel problem setting where the classifier for the\ntarget domain has to be trained with complementary-label data from the source\ndomain and unlabeled data from the target domain named budget-friendly UDA\n(BFUDA). The key benefit is that it is much less costly to collect\ncomplementary-label source data (required by BFUDA) than collecting the\ntrue-label source data (required by ordinary UDA). To this end, the\ncomplementary label adversarial network (CLARINET) is proposed to solve the\nBFUDA problem. CLARINET maintains two deep networks simultaneously, where one\nfocuses on classifying complementary-label source data and the other takes care\nof the source-to-target distributional adaptation. Experiments show that\nCLARINET significantly outperforms a series of competent baselines.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 05:31:58 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 06:09:13 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Zhang", "Yiyang", ""], ["Liu", "Feng", ""], ["Fang", "Zhen", ""], ["Yuan", "Bo", ""], ["Zhang", "Guangquan", ""], ["Lu", "Jie", ""]]}, {"id": "2007.14622", "submitter": "Yusuf Yazici", "authors": "Yusuf Yazici", "title": "Approaches to Fraud Detection on Credit Card Transactions Using\n  Artificial Intelligence Methods", "comments": "10 pages, 1 table, conference paper", "journal-ref": "pp. 235-244, 2020. CS & IT - CSCP 2020", "doi": "10.5121/csit.2020.101018", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Credit card fraud is an ongoing problem for almost all industries in the\nworld, and it raises millions of dollars to the global economy each year.\nTherefore, there is a number of research either completed or proceeding in\norder to detect these kinds of frauds in the industry. These researches\ngenerally use rule-based or novel artificial intelligence approaches to find\neligible solutions. The ultimate goal of this paper is to summarize\nstate-of-the-art approaches to fraud detection using artificial intelligence\nand machine learning techniques. While summarizing, we will categorize the\ncommon problems such as imbalanced dataset, real time working scenarios, and\nfeature engineering challenges that almost all research works encounter, and\nidentify general approaches to solve them. The imbalanced dataset problem\noccurs because the number of legitimate transactions is much higher than the\nfraudulent ones whereas applying the right feature engineering is substantial\nas the features obtained from the industries are limited, and applying feature\nengineering methods and reforming the dataset is crucial. Also, adapting the\ndetection system to real time scenarios is a challenge since the number of\ncredit card transactions in a limited time period is very high. In addition, we\nwill discuss how evaluation metrics and machine learning methods differentiate\namong each research.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 06:18:57 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Yazici", "Yusuf", ""]]}, {"id": "2007.14634", "submitter": "Tomas Geffner", "authors": "Tomas Geffner, Justin Domke", "title": "Approximation Based Variance Reduction for Reparameterization Gradients", "comments": "Neural Information Processing Systems (NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flexible variational distributions improve variational inference but are\nharder to optimize. In this work we present a control variate that is\napplicable for any reparameterizable distribution with known mean and\ncovariance matrix, e.g. Gaussians with any covariance structure. The control\nvariate is based on a quadratic approximation of the model, and its parameters\nare set using a double-descent scheme by minimizing the gradient estimator's\nvariance. We empirically show that this control variate leads to large\nimprovements in gradient variance and optimization convergence for inference\nwith non-factorized variational distributions.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 06:55:11 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 08:36:43 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Geffner", "Tomas", ""], ["Domke", "Justin", ""]]}, {"id": "2007.14641", "submitter": "Giulia Luise", "authors": "Giulia Luise, Massimiliano Pontil and Carlo Ciliberto", "title": "Generalization Properties of Optimal Transport GANs with Latent\n  Distribution Learning", "comments": "34 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Generative Adversarial Networks (GAN) framework is a well-established\nparadigm for probability matching and realistic sample generation. While recent\nattention has been devoted to studying the theoretical properties of such\nmodels, a full theoretical understanding of the main building blocks is still\nmissing. Focusing on generative models with Optimal Transport metrics as\ndiscriminators, in this work we study how the interplay between the latent\ndistribution and the complexity of the pushforward map (generator) affects\nperformance, from both statistical and modelling perspectives. Motivated by our\nanalysis, we advocate learning the latent distribution as well as the\npushforward map within the GAN paradigm. We prove that this can lead to\nsignificant advantages in terms of sample complexity.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 07:31:33 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Luise", "Giulia", ""], ["Pontil", "Massimiliano", ""], ["Ciliberto", "Carlo", ""]]}, {"id": "2007.14660", "submitter": "Zhenjie Ren", "authors": "Anna Kazeykina, Zhenjie Ren, Xiaolu Tan, Junjian Yang", "title": "Ergodicity of the underdamped mean-field Langevin dynamics", "comments": "29 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the long time behavior of an underdamped mean-field Langevin (MFL)\nequation, and provide a general convergence as well as an exponential\nconvergence rate result under different conditions. The results on the MFL\nequation can be applied to study the convergence of the Hamiltonian gradient\ndescent algorithm for the overparametrized optimization. We then provide a\nnumerical example of the algorithm to train a generative adversarial networks\n(GAN).\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 08:09:51 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Kazeykina", "Anna", ""], ["Ren", "Zhenjie", ""], ["Tan", "Xiaolu", ""], ["Yang", "Junjian", ""]]}, {"id": "2007.14677", "submitter": "Kostas Kolomvatsos", "authors": "Anna Karanika, Panagiotis Oikonomou, Kostas Kolomvatsos, Christos\n  Anagnostopoulos", "title": "On the Use of Interpretable Machine Learning for the Management of Data\n  Quality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data quality is a significant issue for any application that requests for\nanalytics to support decision making. It becomes very important when we focus\non Internet of Things (IoT) where numerous devices can interact to exchange and\nprocess data. IoT devices are connected to Edge Computing (EC) nodes to report\nthe collected data, thus, we have to secure data quality not only at the IoT\nbut also at the edge of the network. In this paper, we focus on the specific\nproblem and propose the use of interpretable machine learning to deliver the\nfeatures that are important to be based for any data processing activity. Our\naim is to secure data quality, at least, for those features that are detected\nas significant in the collected datasets. We have to notice that the selected\nfeatures depict the highest correlation with the remaining in every dataset,\nthus, they can be adopted for dimensionality reduction. We focus on multiple\nmethodologies for having interpretability in our learning models and adopt an\nensemble scheme for the final decision. Our scheme is capable of timely\nretrieving the final result and efficiently select the appropriate features. We\nevaluate our model through extensive simulations and present numerical results.\nOur aim is to reveal its performance under various experimental scenarios that\nwe create varying a set of parameters adopted in our mechanism.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 08:49:32 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Karanika", "Anna", ""], ["Oikonomou", "Panagiotis", ""], ["Kolomvatsos", "Kostas", ""], ["Anagnostopoulos", "Christos", ""]]}, {"id": "2007.14694", "submitter": "Michail Tsagris", "authors": "Ioanna Papadaki and Michail Tsagris", "title": "Estimating NBA players salary share according to their performance on\n  court: A machine learning approach", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is customary for researchers and practitioners to fit linear models in\norder to predict NBA player's salary based on the players' performance on\ncourt. On the contrary, we focus on the players salary share (with regards to\nthe team payroll) by first selecting the most important determinants or\nstatistics (years of experience in the league, games played, etc.) and then\nutilise them to predict the player salaries by employing a non linear Random\nForest machine learning algorithm. We externally evaluate our salary\npredictions, thus we avoid the phenomenon of over-fitting observed in most\npapers. Overall, using data from three distinct periods, 2017-2019 we identify\nthe important factors that achieve very satisfactory salary predictions and we\ndraw useful conclusions.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 09:21:27 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 19:15:30 GMT"}, {"version": "v3", "created": "Sat, 31 Oct 2020 20:15:18 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Papadaki", "Ioanna", ""], ["Tsagris", "Michail", ""]]}, {"id": "2007.14698", "submitter": "Yuka Hashimoto", "authors": "Yuka Hashimoto, Isao Ishikawa, Masahiro Ikeda, Fuyuta Komura,\n  Yoshinobu Kawahara", "title": "Kernel Mean Embeddings of Von Neumann-Algebra-Valued Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel mean embedding (KME) is a powerful tool to analyze probability\nmeasures for data, where the measures are conventionally embedded into a\nreproducing kernel Hilbert space (RKHS). In this paper, we generalize KME to\nthat of von Neumann-algebra-valued measures into reproducing kernel Hilbert\nmodules (RKHMs), which provides an inner product and distance between von\nNeumann-algebra-valued measures. Von Neumann-algebra-valued measures can, for\nexample, encode relations between arbitrary pairs of variables in a\nmultivariate distribution or positive operator-valued measures for quantum\nmechanics. Thus, this allows us to perform probabilistic analyses explicitly\nreflected with higher-order interactions among variables, and provides a way of\napplying machine learning frameworks to problems in quantum mechanics. We also\nshow that the injectivity of the existing KME and the universality of RKHS are\ngeneralized to RKHM, which confirms many useful features of the existing KME\nremain in our generalized KME. And, we investigate the empirical performance of\nour methods using synthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 09:26:39 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Hashimoto", "Yuka", ""], ["Ishikawa", "Isao", ""], ["Ikeda", "Masahiro", ""], ["Komura", "Fuyuta", ""], ["Kawahara", "Yoshinobu", ""]]}, {"id": "2007.14703", "submitter": "Luc Brogat-Motte", "authors": "Luc Brogat-Motte, Alessandro Rudi, C\\'eline Brouard, Juho Rousu,\n  Florence d'Alch\\'e-Buc", "title": "Learning Output Embeddings in Structured Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A powerful and flexible approach to structured prediction consists in\nembedding the structured objects to be predicted into a feature space of\npossibly infinite dimension by means of output kernels, and then, solving a\nregression problem in this output space. A prediction in the original space is\ncomputed by solving a pre-image problem. In such an approach, the embedding,\nlinked to the target loss, is defined prior to the learning phase. In this\nwork, we propose to jointly learn a finite approximation of the output\nembedding and the regression function into the new feature space. For that\npurpose, we leverage a priori information on the outputs and also unexploited\nunsupervised output data, which are both often available in structured\nprediction problems. We prove that the resulting structured predictor is a\nconsistent estimator, and derive an excess risk bound. Moreover, the novel\nstructured prediction tool enjoys a significantly smaller computational\ncomplexity than former output kernel methods. The approach empirically tested\non various structured prediction problems reveals to be versatile and able to\nhandle large datasets.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 09:32:53 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 14:26:17 GMT"}, {"version": "v3", "created": "Mon, 2 Nov 2020 11:39:29 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Brogat-Motte", "Luc", ""], ["Rudi", "Alessandro", ""], ["Brouard", "C\u00e9line", ""], ["Rousu", "Juho", ""], ["d'Alch\u00e9-Buc", "Florence", ""]]}, {"id": "2007.14706", "submitter": "J. Emmanuel Johnson", "authors": "J. Emmanuel Johnson, Valero Laparra, Adri\\'an P\\'erez-Suay, Miguel D.\n  Mahecha and Gustau Camps-Valls", "title": "Kernel Methods and their derivatives: Concept and perspectives for the\n  Earth system sciences", "comments": "21 pages, 10 figures, PLOS One Journal", "journal-ref": null, "doi": "10.1371/journal.pone.0235885", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel methods are powerful machine learning techniques which implement\ngeneric non-linear functions to solve complex tasks in a simple way. They Have\na solid mathematical background and exhibit excellent performance in practice.\nHowever, kernel machines are still considered black-box models as the feature\nmapping is not directly accessible and difficult to interpret.The aim of this\nwork is to show that it is indeed possible to interpret the functions learned\nby various kernel methods is intuitive despite their complexity. Specifically,\nwe show that derivatives of these functions have a simple mathematical\nformulation, are easy to compute, and can be applied to many different\nproblems. We note that model function derivatives in kernel machines is\nproportional to the kernel function derivative. We provide the explicit\nanalytic form of the first and second derivatives of the most common kernel\nfunctions with regard to the inputs as well as generic formulas to compute\nhigher order derivatives. We use them to analyze the most used supervised and\nunsupervised kernel learning methods: Gaussian Processes for regression,\nSupport Vector Machines for classification, Kernel Entropy Component Analysis\nfor density estimation, and the Hilbert-Schmidt Independence Criterion for\nestimating the dependency between random variables. For all cases we expressed\nthe derivative of the learned function as a linear combination of the kernel\nfunction derivative. Moreover we provide intuitive explanations through\nillustrative toy examples and show how to improve the interpretation of real\napplications in the context of spatiotemporal Earth system data cubes. This\nwork reflects on the observation that function derivatives may play a crucial\nrole in kernel methods analysis and understanding.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 09:36:42 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 16:18:51 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Johnson", "J. Emmanuel", ""], ["Laparra", "Valero", ""], ["P\u00e9rez-Suay", "Adri\u00e1n", ""], ["Mahecha", "Miguel D.", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "2007.14717", "submitter": "Konstantin Avrachenkov", "authors": "Konstantin Avrachenkov and Maximilien Dreveton", "title": "Almost exact recovery in noisy semi-supervised learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper investigates noisy graph-based semi-supervised learning or\ncommunity detection. We consider the Stochastic Block Model (SBM), where, in\naddition to the graph observation, an oracle gives a non-perfect information\nabout some nodes' cluster assignment. We derive the Maximum A Priori (MAP)\nestimator, and show that a continuous relaxation of the MAP performs almost\nexact recovery under non-restrictive conditions on the average degree and\namount of oracle noise. In particular, this method avoids some pitfalls of\nseveral graph-based semi-supervised learning methods such as the flatness of\nthe classification functions, appearing in the problems with a very large\namount of unlabeled data.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 09:56:05 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Avrachenkov", "Konstantin", ""], ["Dreveton", "Maximilien", ""]]}, {"id": "2007.14761", "submitter": "Sebastian Bruch", "authors": "Sebastian Bruch, Jan Pfeifer, Mathieu Guillame-bert", "title": "Learning Representations for Axis-Aligned Decision Forests through Input\n  Perturbation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Axis-aligned decision forests have long been the leading class of machine\nlearning algorithms for modeling tabular data. In many applications of machine\nlearning such as learning-to-rank, decision forests deliver remarkable\nperformance. They also possess other coveted characteristics such as\ninterpretability. Despite their widespread use and rich history, decision\nforests to date fail to consume raw structured data such as text, or learn\neffective representations for them, a factor behind the success of deep neural\nnetworks in recent years. While there exist methods that construct smoothed\ndecision forests to achieve representation learning, the resulting models are\ndecision forests in name only: They are no longer axis-aligned, use stochastic\ndecisions, or are not interpretable. Furthermore, none of the existing methods\nare appropriate for problems that require a Transfer Learning treatment. In\nthis work, we present a novel but intuitive proposal to achieve representation\nlearning for decision forests without imposing new restrictions or\nnecessitating structural changes. Our model is simply a decision forest,\npossibly trained using any forest learning algorithm, atop a deep neural\nnetwork. By approximating the gradients of the decision forest through input\nperturbation, a purely analytical procedure, the decision forest directs the\nneural network to learn or fine-tune representations. Our framework has the\nadvantage that it is applicable to any arbitrary decision forest and that it\nallows the use of arbitrary deep neural networks for representation learning.\nWe demonstrate the feasibility and effectiveness of our proposal through\nexperiments on synthetic and benchmark classification datasets.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 11:56:38 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 16:37:19 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Bruch", "Sebastian", ""], ["Pfeifer", "Jan", ""], ["Guillame-bert", "Mathieu", ""]]}, {"id": "2007.14805", "submitter": "Hagit Grushka-Cohen", "authors": "Hagit Grushka-Cohen, Raphael Cohen, Bracha Shapira, Jacob Moran-Gilad\n  and Lior Rokach", "title": "A framework for optimizing COVID-19 testing policy using a Multi Armed\n  Bandit approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing is an important part of tackling the COVID-19 pandemic. Availability\nof testing is a bottleneck due to constrained resources and effective\nprioritization of individuals is necessary. Here, we discuss the impact of\ndifferent prioritization policies on COVID-19 patient discovery and the ability\nof governments and health organizations to use the results for effective\ndecision making. We suggest a framework for testing that balances the maximal\ndiscovery of positive individuals with the need for population-based\nsurveillance aimed at understanding disease spread and characteristics. This\nframework draws from similar approaches to prioritization in the domain of\ncyber-security based on ranking individuals using a risk score and then\nreserving a portion of the capacity for random sampling. This approach is an\napplication of Multi-Armed-Bandits maximizing exploration/exploitation of the\nunderlying distribution. We find that individuals can be ranked for effective\ntesting using a few simple features, and that ranking them using such models we\ncan capture 65% (CI: 64.7%-68.3%) of the positive individuals using less than\n20% of the testing capacity or 92.1% (CI: 91.1%-93.2%) of positives individuals\nusing 70% of the capacity, allowing reserving a significant portion of the\ntests for population studies. Our approach allows experts and decision-makers\nto tailor the resulting policies as needed allowing transparency into the\nranking policy and the ability to understand the disease spread in the\npopulation and react quickly and in an informed manner.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 10:28:38 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Grushka-Cohen", "Hagit", ""], ["Cohen", "Raphael", ""], ["Shapira", "Bracha", ""], ["Moran-Gilad", "Jacob", ""], ["Rokach", "Lior", ""]]}, {"id": "2007.14861", "submitter": "Constance Beguier", "authors": "Constance Beguier and Eric W. Tramel", "title": "SAFER: Sparse Secure Aggregation for Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning enables one to train a common machine learning model\nacross separate, privately-held datasets via distributed model training. During\nfederated training, only intermediate model parameters are transmitted to a\ncentral server which aggregates these parameters to create a new common model,\nthus exposing only intermediate parameters rather than the training data\nitself. However, some attacks (e.g. membership inference) are able to infer\nproperties of local data from these intermediate model parameters. Hence,\nperforming the aggregation of these client-specific model parameters in a\nsecure way is required. Additionally, the communication cost is often the\nbottleneck of the federated systems, especially for large neural networks. So,\nlimiting the number and the size of communications is necessary to efficiently\ntrain large neural architectures. In this article, we present an efficient and\nsecure protocol for performing secure aggregation over compressed model updates\nin the context of collaborative, few-party federated learning, a context common\nin the medical, healthcare, and biotechnical use-cases of federated systems. By\nmaking compression-based federated techniques amenable to secure computation,\nwe develop a secure aggregation protocol between multiple servers with very low\ncommunication and computation costs and without preprocessing overhead. Our\nexperiments demonstrate the efficiency of this new approach for secure\nfederated training of deep convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 14:28:30 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 06:28:49 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Beguier", "Constance", ""], ["Tramel", "Eric W.", ""]]}, {"id": "2007.14870", "submitter": "Lucas Cardoso", "authors": "Lucas F. F. Cardoso, Vitor C. A. Santos, Regiane S. K. Franc\\^es,\n  Ricardo B. C. Prud\\^encio and Ronnie C. O. Alves", "title": "Decoding machine learning benchmarks", "comments": "Paper published at the BRACIS 2020 conference, 15 pages, 4 figures", "journal-ref": "BRACIS 2020", "doi": "10.1007/978-3-030-61380-8_28", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the availability of benchmark machine learning (ML) repositories\n(e.g., UCI, OpenML), there is no standard evaluation strategy yet capable of\npointing out which is the best set of datasets to serve as gold standard to\ntest different ML algorithms. In recent studies, Item Response Theory (IRT) has\nemerged as a new approach to elucidate what should be a good ML benchmark. This\nwork applied IRT to explore the well-known OpenML-CC18 benchmark to identify\nhow suitable it is on the evaluation of classifiers. Several classifiers\nranging from classical to ensembles ones were evaluated using IRT models, which\ncould simultaneously estimate dataset difficulty and classifiers' ability. The\nGlicko-2 rating system was applied on the top of IRT to summarize the innate\nability and aptitude of classifiers. It was observed that not all datasets from\nOpenML-CC18 are really useful to evaluate classifiers. Most datasets evaluated\nin this work (84%) contain easy instances in general (e.g., around 10% of\ndifficult instances only). Also, 80% of the instances in half of this benchmark\nare very discriminating ones, which can be of great use for pairwise algorithm\ncomparison, but not useful to push classifiers abilities. This paper presents\nthis new evaluation methodology based on IRT as well as the tool decodIRT,\ndeveloped to guide IRT estimation over ML benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 14:39:41 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 20:08:48 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Cardoso", "Lucas F. F.", ""], ["Santos", "Vitor C. A.", ""], ["Franc\u00eas", "Regiane S. K.", ""], ["Prud\u00eancio", "Ricardo B. C.", ""], ["Alves", "Ronnie C. O.", ""]]}, {"id": "2007.14917", "submitter": "James O' Neill", "authors": "James O' Neill, Greg Ver Steeg and Aram Galstyan", "title": "Compressing Deep Neural Networks via Layer Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes \\textit{layer fusion} - a model compression technique\nthat discovers which weights to combine and then fuses weights of similar\nfully-connected, convolutional and attention layers. Layer fusion can\nsignificantly reduce the number of layers of the original network with little\nadditional computation overhead, while maintaining competitive performance.\nFrom experiments on CIFAR-10, we find that various deep convolution neural\nnetworks can remain within 2\\% accuracy points of the original networks up to a\ncompression ratio of 3.33 when iteratively retrained with layer fusion. For\nexperiments on the WikiText-2 language modelling dataset where pretrained\ntransformer models are used, we achieve compression that leads to a network\nthat is 20\\% of its original size while being within 5 perplexity points of the\noriginal network. We also find that other well-established compression\ntechniques can achieve competitive performance when compared to their original\nnetworks given a sufficient number of retraining steps. Generally, we observe a\nclear inflection point in performance as the amount of compression increases,\nsuggesting a bound on the amount of compression that can be achieved before an\nexponential degradation in performance.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 15:43:19 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Neill", "James O'", ""], ["Steeg", "Greg Ver", ""], ["Galstyan", "Aram", ""]]}, {"id": "2007.14918", "submitter": "Bin Yang", "authors": "Thomas Buhl Andersen, R\\'ogvi Eliasen, Mikkel Jarlund, Bin Yang", "title": "Force myography benchmark data for hand gesture recognition and transfer\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Force myography has recently gained increasing attention for hand gesture\nrecognition tasks. However, there is a lack of publicly available benchmark\ndata, with most existing studies collecting their own data often with custom\nhardware and for varying sets of gestures. This limits the ability to compare\nvarious algorithms, as well as the possibility for research to be done without\nfirst needing to collect data oneself. We contribute to the advancement of this\nfield by making accessible a benchmark dataset collected using a commercially\navailable sensor setup from 20 persons covering 18 unique gestures, in the hope\nof allowing further comparison of results as well as easier entry into this\nfield of research. We illustrate one use-case for such data, showing how we can\nimprove gesture recognition accuracy by utilising transfer learning to\nincorporate data from multiple other persons. This also illustrates that the\ndataset can serve as a benchmark dataset to facilitate research on transfer\nlearning algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 15:43:59 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Andersen", "Thomas Buhl", ""], ["Eliasen", "R\u00f3gvi", ""], ["Jarlund", "Mikkel", ""], ["Yang", "Bin", ""]]}, {"id": "2007.14920", "submitter": "Dariusz Brzezinski", "authors": "Dariusz Brzezinski", "title": "Fibonacci and k-Subsecting Recursive Feature Elimination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Feature selection is a data mining task with the potential of speeding up\nclassification algorithms, enhancing model comprehensibility, and improving\nlearning accuracy. However, finding a subset of features that is optimal in\nterms of predictive accuracy is usually computationally intractable. Out of\nseveral heuristic approaches to dealing with this problem, the Recursive\nFeature Elimination (RFE) algorithm has received considerable interest from\ndata mining practitioners. In this paper, we propose two novel algorithms\ninspired by RFE, called Fibonacci- and k-Subsecting Recursive Feature\nElimination, which remove features in logarithmic steps, probing the wrapped\nclassifier more densely for the more promising feature subsets. The proposed\nalgorithms are experimentally compared against RFE on 28 highly\nmultidimensional datasets and evaluated in a practical case study involving 3D\nelectron density maps from the Protein Data Bank. The results show that\nFibonacci and k-Subsecting Recursive Feature Elimination are capable of\nselecting a smaller subset of features much faster than standard RFE, while\nachieving comparable predictive performance.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 15:53:04 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Brzezinski", "Dariusz", ""]]}, {"id": "2007.14983", "submitter": "Kai Steverson", "authors": "Kai Steverson, Jonathan Mullin, Metin Ahiskali", "title": "Adversarial Robustness for Machine Learning Cyber Defenses Using Log\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been considerable and growing interest in applying machine learning\nfor cyber defenses. One promising approach has been to apply natural language\nprocessing techniques to analyze logs data for suspicious behavior. A natural\nquestion arises to how robust these systems are to adversarial attacks. Defense\nagainst sophisticated attack is of particular concern for cyber defenses. In\nthis paper, we develop a testing framework to evaluate adversarial robustness\nof machine learning cyber defenses, particularly those focused on log data. Our\nframework uses techniques from deep reinforcement learning and adversarial\nnatural language processing. We validate our framework using a publicly\navailable dataset and demonstrate that our adversarial attack does succeed\nagainst the target systems, revealing a potential vulnerability. We apply our\nframework to analyze the influence of different levels of dropout\nregularization and find that higher dropout levels increases robustness.\nMoreover 90% dropout probability exhibited the highest level of robustness by a\nsignificant margin, which suggests unusually high dropout may be necessary to\nproperly protect against adversarial attacks.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 17:51:29 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Steverson", "Kai", ""], ["Mullin", "Jonathan", ""], ["Ahiskali", "Metin", ""]]}, {"id": "2007.15030", "submitter": "Eugenio Mart\\'inez-C\\'amara", "authors": "Nuria Rodr\\'iguez-Barroso, Eugenio Mart\\'inez-C\\'amara, M. Victoria\n  Luz\\'on, Gerardo Gonz\\'alez Seco, Miguel \\'Angel Veganzones, Francisco\n  Herrera", "title": "Dynamic Federated Learning Model for Identifying Adversarial Clients", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning, as a distributed learning that conducts the training on\nthe local devices without accessing to the training data, is vulnerable to\ndirty-label data poisoning adversarial attacks. We claim that the federated\nlearning model has to avoid those kind of adversarial attacks through filtering\nout the clients that manipulate the local data. We propose a dynamic federated\nlearning model that dynamically discards those adversarial clients, which\nallows to prevent the corruption of the global learning model. We evaluate the\ndynamic discarding of adversarial clients deploying a deep learning\nclassification model in a federated learning setting, and using the EMNIST\nDigits and Fashion MNIST image classification datasets. Likewise, we analyse\nthe capacity of detecting clients with poor data distribution and reducing the\nnumber of rounds of learning by selecting the clients to aggregate. The results\nshow that the dynamic selection of the clients to aggregate enhances the\nperformance of the global learning model, discards the adversarial and poor\nclients and reduces the rounds of learning.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 18:02:11 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Rodr\u00edguez-Barroso", "Nuria", ""], ["Mart\u00ednez-C\u00e1mara", "Eugenio", ""], ["Luz\u00f3n", "M. Victoria", ""], ["Seco", "Gerardo Gonz\u00e1lez", ""], ["Veganzones", "Miguel \u00c1ngel", ""], ["Herrera", "Francisco", ""]]}, {"id": "2007.15039", "submitter": "Elizabeth Pei-Ting Chou", "authors": "Elizabeth Chou, Catie McVey, Yin-Chen Hsieh, Sabrina Enriquez, Fushing\n  Hsieh", "title": "Extreme-K categorical samples problem", "comments": "20 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With histograms as its foundation, we develop Categorical Exploratory Data\nAnalysis (CEDA) under the extreme-$K$ sample problem, and illustrate its\nuniversal applicability through four 1D categorical datasets. Given a sizable\n$K$, CEDA's ultimate goal amounts to discover by data's information content via\ncarrying out two data-driven computational tasks: 1) establish a tree geometry\nupon $K$ populations as a platform for discovering a wide spectrum of patterns\namong populations; 2) evaluate each geometric pattern's reliability. In CEDA\ndevelopments, each population gives rise to a row vector of categories\nproportions. Upon the data matrix's row-axis, we discuss the pros and cons of\nEuclidean distance against its weighted version for building a binary\nclustering tree geometry. The criterion of choice rests on degrees of\nuniformness in column-blocks framed by this binary clustering tree. Each\ntree-leaf (population) is then encoded with a binary code sequence, so is\ntree-based pattern. For evaluating reliability, we adopt row-wise multinomial\nrandomness to generate an ensemble of matrix mimicries, so an ensemble of\nmimicked binary trees. Reliability of any observed pattern is its recurrence\nrate within the tree ensemble. A high reliability value means a deterministic\npattern. Our four applications of CEDA illuminate four significant aspects of\nextreme-$K$ sample problems.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 18:12:48 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Chou", "Elizabeth", ""], ["McVey", "Catie", ""], ["Hsieh", "Yin-Chen", ""], ["Enriquez", "Sabrina", ""], ["Hsieh", "Fushing", ""]]}, {"id": "2007.15047", "submitter": "Peter Gmeiner", "authors": "Peter Gmeiner", "title": "Information-Theoretic Approximation to Causal Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring the causal direction and causal effect between two discrete random\nvariables X and Y from a finite sample is often a crucial problem and a\nchallenging task. However, if we have access to observational and\ninterventional data, it is possible to solve that task. If X is causing Y, then\nit does not matter if we observe an effect in Y by observing changes in X or by\nintervening actively on X. This invariance principle creates a link between\nobservational and interventional distributions in a higher dimensional\nprobability space. We embed distributions that originate from samples of X and\nY into that higher dimensional space such that the embedded distribution is\nclosest to the distributions that follow the invariance principle, with respect\nto the relative entropy. This allows us to calculate the best\ninformation-theoretic approximation for a given empirical distribution, that\nfollows an assumed underlying causal model. We show that this\ninformation-theoretic approximation to causal models (IACM) can be done by\nsolving a linear optimization problem. In particular, by approximating the\nempirical distribution to a monotonic causal model, we can calculate\nprobabilities of causation. We can also use IACM for causal discovery problems\nin the bivariate, discrete case. However, experimental results on labeled\nsynthetic data from additive noise models show that our causal discovery\napproach is lagging behind state-of-the-art approaches because the invariance\nprinciple encodes only a necessary condition for causal relations.\nNevertheless, for synthetic multiplicative noise data and real-world data, our\napproach can compete in some cases with alternative methods.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 18:34:58 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 15:35:24 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Gmeiner", "Peter", ""]]}, {"id": "2007.15129", "submitter": "Abigail Azari", "authors": "Abigail R. Azari, John B. Biersteker, Ryan M. Dewey, Gary Doran, Emily\n  J. Forsberg, Camilla D. K. Harris, Hannah R. Kerner, Katherine A. Skinner,\n  Andy W. Smith, Rashied Amini, Saverio Cambioni, Victoria Da Poian, Tadhg M.\n  Garton, Michael D. Himes, Sarah Millholland, Suranga Ruhunusiri", "title": "Integrating Machine Learning for Planetary Science: Perspectives for the\n  Next Decade", "comments": "10 pages (expanded citations compared to 8 page submitted version for\n  decadal survey), 3 figures, white paper submitted to the Planetary Science\n  and Astrobiology Decadal Survey 2023-2032", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM astro-ph.EP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) methods can expand our ability to construct, and draw\ninsight from large datasets. Despite the increasing volume of planetary\nobservations, our field has seen few applications of ML in comparison to other\nsciences. To support these methods, we propose ten recommendations for\nbolstering a data-rich future in planetary science.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 21:58:42 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Azari", "Abigail R.", ""], ["Biersteker", "John B.", ""], ["Dewey", "Ryan M.", ""], ["Doran", "Gary", ""], ["Forsberg", "Emily J.", ""], ["Harris", "Camilla D. K.", ""], ["Kerner", "Hannah R.", ""], ["Skinner", "Katherine A.", ""], ["Smith", "Andy W.", ""], ["Amini", "Rashied", ""], ["Cambioni", "Saverio", ""], ["Da Poian", "Victoria", ""], ["Garton", "Tadhg M.", ""], ["Himes", "Michael D.", ""], ["Millholland", "Sarah", ""], ["Ruhunusiri", "Suranga", ""]]}, {"id": "2007.15130", "submitter": "Saeed Saremi", "authors": "Saeed Saremi", "title": "Unnormalized Variational Bayes", "comments": "Submitted to Journal of Machine Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We unify empirical Bayes and variational Bayes for approximating unnormalized\ndensities. This framework, named unnormalized variational Bayes (UVB), is based\non formulating a latent variable model for the random variable\n$Y=X+N(0,\\sigma^2 I_d)$ and using the evidence lower bound (ELBO), computed by\na variational autoencoder, as a parametrization of the energy function of $Y$\nwhich is then used to estimate $X$ with the empirical Bayes least-squares\nestimator. In this intriguing setup, the $\\textit{gradient}$ of the ELBO with\nrespect to noisy inputs plays the central role in learning the energy function.\nEmpirically, we demonstrate that UVB has a higher capacity to approximate\nenergy functions than the parametrization with MLPs as done in neural empirical\nBayes (DEEN). We especially showcase $\\sigma=1$, where the differences between\nUVB and DEEN become visible and qualitative in the denoising experiments. For\nthis high level of noise, the distribution of $Y$ is very smoothed and we\ndemonstrate that one can traverse in a single run $-$ without a restart $-$ all\nMNIST classes in a variety of styles via walk-jump sampling with a fast-mixing\nLangevin MCMC sampler. We finish by probing the encoder/decoder of the trained\nmodels and confirm UVB $\\neq$ VAE.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 21:58:54 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Saremi", "Saeed", ""]]}, {"id": "2007.15139", "submitter": "Yoshua Bengio", "authors": "Yoshua Bengio", "title": "Deriving Differential Target Propagation from Iterating Approximate\n  Inverses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that a particular form of target propagation, i.e., relying on\nlearned inverses of each layer, which is differential, i.e., where the target\nis a small perturbation of the forward propagation, gives rise to an update\nrule which corresponds to an approximate Gauss-Newton gradient-based\noptimization, without requiring the manipulation or inversion of large\nmatrices. What is interesting is that this is more biologically plausible than\nback-propagation yet may turn out to implicitly provide a stronger optimization\nprocedure. Extending difference target propagation, we consider several\niterative calculations based on local auto-encoders at each layer in order to\nachieve more precise inversions for more accurate target propagation and we\nshow that these iterative procedures converge exponentially fast if the\nauto-encoding function minus the identity function has a Lipschitz constant\nsmaller than one, i.e., the auto-encoder is coarsely succeeding at performing\nan inversion. We also propose a way to normalize the changes at each layer to\ntake into account the relative influence of each layer on the output, so that\nlarger weight changes are done on more influential layers, like would happen in\nordinary back-propagation with gradient descent.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 22:34:45 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 19:09:41 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Bengio", "Yoshua", ""]]}, {"id": "2007.15147", "submitter": "Varun Chandrasekaran", "authors": "Jayaram Raghuram, Varun Chandrasekaran, Somesh Jha, Suman Banerjee", "title": "A General Framework For Detecting Anomalous Inputs to DNN Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting anomalous inputs, such as adversarial and out-of-distribution (OOD)\ninputs, is critical for classifiers (including deep neural networks or DNNs)\ndeployed in real-world applications. While prior works have proposed various\nmethods to detect such anomalous samples using information from the internal\nlayer representations of a DNN, there is a lack of consensus on a principled\napproach for the different components of such a detection method. As a result,\noften heuristic and one-off methods are applied for different aspects of this\nproblem. We propose an unsupervised anomaly detection framework based on the\ninternal DNN layer representations in the form of a meta-algorithm with\nconfigurable components. We proceed to propose specific instantiations for each\ncomponent of the meta-algorithm based on ideas grounded in statistical testing\nand anomaly detection. We evaluate the proposed methods on well-known image\nclassification datasets with strong adversarial attacks and OOD inputs,\nincluding an adaptive attack that uses the internal layer representations of\nthe DNN (often not considered in prior work). Comparisons with five\nrecently-proposed competing detection methods demonstrates the effectiveness of\nour method in detecting adversarial and OOD inputs.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 22:57:57 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 03:37:27 GMT"}, {"version": "v3", "created": "Thu, 17 Jun 2021 15:04:47 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Raghuram", "Jayaram", ""], ["Chandrasekaran", "Varun", ""], ["Jha", "Somesh", ""], ["Banerjee", "Suman", ""]]}, {"id": "2007.15153", "submitter": "Divya Gopinath", "authors": "Divya Gopinath, Monica Agrawal, Luke Murray, Steven Horng, David\n  Karger, David Sontag", "title": "Fast, Structured Clinical Documentation via Contextual Autocomplete", "comments": "Published in Machine Learning for Healthcare 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system that uses a learned autocompletion mechanism to\nfacilitate rapid creation of semi-structured clinical documentation. We\ndynamically suggest relevant clinical concepts as a doctor drafts a note by\nleveraging features from both unstructured and structured medical data. By\nconstraining our architecture to shallow neural networks, we are able to make\nthese suggestions in real time. Furthermore, as our algorithm is used to write\na note, we can automatically annotate the documentation with clean labels of\nclinical concepts drawn from medical vocabularies, making notes more structured\nand readable for physicians, patients, and future algorithms. To our knowledge,\nthis system is the only machine learning-based documentation utility for\nclinical notes deployed in a live hospital setting, and it reduces keystroke\nburden of clinical concepts by 67% in real environments.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 23:43:15 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Gopinath", "Divya", ""], ["Agrawal", "Monica", ""], ["Murray", "Luke", ""], ["Horng", "Steven", ""], ["Karger", "David", ""], ["Sontag", "David", ""]]}, {"id": "2007.15159", "submitter": "Ken Kobayashi", "authors": "Tomokaze Shiratori and Ken Kobayashi and Yuichi Takano", "title": "Prediction of hierarchical time series using structured regularization\n  and its application to artificial neural networks", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0242099", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the prediction of hierarchical time series, where each\nupper-level time series is calculated by summing appropriate lower-level time\nseries. Forecasts for such hierarchical time series should be coherent, meaning\nthat the forecast for an upper-level time series equals the sum of forecasts\nfor corresponding lower-level time series. Previous methods for making coherent\nforecasts consist of two phases: first computing base (incoherent) forecasts\nand then reconciling those forecasts based on their inherent hierarchical\nstructure. With the aim of improving time series predictions, we propose a\nstructured regularization method for completing both phases simultaneously. The\nproposed method is based on a prediction model for bottom-level time series and\nuses a structured regularization term to incorporate upper-level forecasts into\nthe prediction model. We also develop a backpropagation algorithm specialized\nfor application of our method to artificial neural networks for time series\nprediction. Experimental results using synthetic and real-world datasets\ndemonstrate the superiority of our method in terms of prediction accuracy and\ncomputational efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 00:30:32 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Shiratori", "Tomokaze", ""], ["Kobayashi", "Ken", ""], ["Takano", "Yuichi", ""]]}, {"id": "2007.15174", "submitter": "Sui Tang", "authors": "Fei Lu, Mauro Maggioni, Sui Tang", "title": "Learning interaction kernels in stochastic systems of interacting\n  particles from multiple trajectories", "comments": "38 pages; 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider stochastic systems of interacting particles or agents, with\ndynamics determined by an interaction kernel which only depends on pairwise\ndistances. We study the problem of inferring this interaction kernel from\nobservations of the positions of the particles, in either continuous or\ndiscrete time, along multiple independent trajectories. We introduce a\nnonparametric inference approach to this inverse problem, based on a\nregularized maximum likelihood estimator constrained to suitable hypothesis\nspaces adaptive to data. We show that a coercivity condition enables us to\ncontrol the condition number of this problem and prove the consistency of our\nestimator, and that in fact it converges at a near-optimal learning rate, equal\nto the min-max rate of $1$-dimensional non-parametric regression. In\nparticular, this rate is independent of the dimension of the state space, which\nis typically very high. We also analyze the discretization errors in the case\nof discrete-time observations, showing that it is of order $1/2$ in terms of\nthe time gaps between observations. This term, when large, dominates the\nsampling error and the approximation error, preventing convergence of the\nestimator. Finally, we exhibit an efficient parallel algorithm to construct the\nestimator from data, and we demonstrate the effectiveness of our algorithm with\nnumerical tests on prototype systems including stochastic opinion dynamics and\na Lennard-Jones model.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 01:28:06 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Lu", "Fei", ""], ["Maggioni", "Mauro", ""], ["Tang", "Sui", ""]]}, {"id": "2007.15190", "submitter": "Akira Nakagawa", "authors": "Akira Nakagawa, Keizo Kato, Taiji Suzuki", "title": "Quantitative Understanding of VAE as a Non-linearly Scaled Isometric\n  Embedding", "comments": "40 pages, 29 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational autoencoder (VAE) estimates the posterior parameters (mean and\nvariance) of latent variables corresponding to each input data. While it is\nused for many tasks, the transparency of the model is still an underlying\nissue. This paper provides a quantitative understanding of VAE property through\nthe differential geometric and information-theoretic interpretations of VAE.\nAccording to the Rate-distortion theory, the optimal transform coding is\nachieved by using an orthonormal transform with PCA basis where the transform\nspace is isometric to the input. Considering the analogy of transform coding to\nVAE, we clarify theoretically and experimentally that VAE can be mapped to an\nimplicit isometric embedding with a scale factor derived from the posterior\nparameter. As a result, we can estimate the data probabilities in the input\nspace from the prior, loss metrics, and corresponding posterior parameters, and\nfurther, the quantitative importance of each latent variable can be evaluated\nlike the eigenvalue of PCA.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 02:37:46 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 07:53:54 GMT"}, {"version": "v3", "created": "Sat, 12 Jun 2021 04:51:47 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Nakagawa", "Akira", ""], ["Kato", "Keizo", ""], ["Suzuki", "Taiji", ""]]}, {"id": "2007.15197", "submitter": "Monica Ribero", "authors": "Monica Ribero, Haris Vikalo", "title": "Communication-Efficient Federated Learning via Optimal Client Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) ameliorates privacy concerns in settings where a\ncentral server coordinates learning from data distributed across many clients.\nThe clients train locally and communicate the models they learn to the server;\naggregation of local models requires frequent communication of large amounts of\ninformation between the clients and the central server. We propose a novel,\nsimple and efficient way of updating the central model in\ncommunication-constrained settings based on collecting models from clients with\ninformative updates and estimating local updates that were not communicated. In\nparticular, modeling the progression of model's weights by an\nOrnstein-Uhlenbeck process allows us to derive an optimal sampling strategy for\nselecting a subset of clients with significant weight updates. The central\nserver collects updated local models from only the selected clients and\ncombines them with estimated model updates of the clients that were not\nselected for communication. We test this policy on a synthetic dataset for\nlogistic regression and two FL benchmarks, namely, a classification task on\nEMNIST and a realistic language modeling task using the Shakespeare dataset.\nThe results demonstrate that the proposed framework provides significant\nreduction in communication while maintaining competitive or achieving superior\nperformance compared to a baseline. Our method represents a new line of\nstrategies for communication-efficient FL that is orthogonal to the existing\nuser-local methods such as quantization or sparsification, thus complementing\nrather than aiming to replace those existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 02:58:00 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 19:08:30 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Ribero", "Monica", ""], ["Vikalo", "Haris", ""]]}, {"id": "2007.15220", "submitter": "Pasin Manurangsi", "authors": "Ilias Diakonikolas, Daniel M. Kane, Pasin Manurangsi", "title": "The Complexity of Adversarially Robust Proper Learning of Halfspaces\n  with Agnostic Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational complexity of adversarially robust proper learning\nof halfspaces in the distribution-independent agnostic PAC model, with a focus\non $L_p$ perturbations. We give a computationally efficient learning algorithm\nand a nearly matching computational hardness result for this problem. An\ninteresting implication of our findings is that the $L_{\\infty}$ perturbations\ncase is provably computationally harder than the case $2 \\leq p < \\infty$.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 04:18:51 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Manurangsi", "Pasin", ""]]}, {"id": "2007.15222", "submitter": "Mahdi Nazemi", "authors": "Mahdi Nazemi, Amirhossein Esmaili, Arash Fayyazi, Massoud Pedram", "title": "SynergicLearning: Neural Network-Based Feature Extraction for\n  Highly-Accurate Hyperdimensional Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models differ in terms of accuracy, computational/memory\ncomplexity, training time, and adaptability among other characteristics. For\nexample, neural networks (NNs) are well-known for their high accuracy due to\nthe quality of their automatic feature extraction while brain-inspired\nhyperdimensional (HD) learning models are famous for their quick training,\ncomputational efficiency, and adaptability. This work presents a hybrid,\nsynergic machine learning model that excels at all the said characteristics and\nis suitable for incremental, on-line learning on a chip. The proposed model\ncomprises an NN and a classifier. The NN acts as a feature extractor and is\nspecifically trained to work well with the classifier that employs the HD\ncomputing framework. This work also presents a parameterized hardware\nimplementation of the said feature extraction and classification components\nwhile introducing a compiler that maps any arbitrary NN and/or classifier to\nthe aforementioned hardware. The proposed hybrid machine learning model has the\nsame level of accuracy (i.e. $\\pm$1%) as NNs while achieving at least 10%\nimprovement in accuracy compared to HD learning models. Additionally, the\nend-to-end hardware realization of the hybrid model improves power efficiency\nby 1.60x compared to state-of-the-art, high-performance HD learning\nimplementations while improving latency by 2.13x. These results have profound\nimplications for the application of such synergic models in challenging\ncognitive tasks.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 04:35:20 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 17:13:37 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Nazemi", "Mahdi", ""], ["Esmaili", "Amirhossein", ""], ["Fayyazi", "Arash", ""], ["Pedram", "Massoud", ""]]}, {"id": "2007.15241", "submitter": "Zhengxu Yu", "authors": "Zhengxu Yu, Pengfei Wang, Chao Xiang, Liang Xie, Zhongming Jin,\n  Jianqiang Huang, Xiaofei He, Deng Cai, Xian-Sheng Hua", "title": "Partial Feature Decorrelation for Non-I.I.D Image classification", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most deep-learning-based image classification methods follow a consistency\nhypothesis that all samples are generated under an independent and identically\ndistributed (I.I.D) setting. Such property can hardly be guaranteed in many\nreal-world applications, resulting in an agnostic context distribution shift\nbetween training and testing environments. It can misguide the model overfit\nsome statistical correlations between features in the training set. To address\nthis problem, we present a novel Partial Feature Decorrelation Learning (PFDL)\nAlgorithm, which jointly optimizes a feature decomposition network and the\ntarget image classification model. The feature decomposition network decomposes\nfeature embedding into independent and correlated parts such that the\ncorrelations between features will be highlighted. Then, the correlated\nfeatures are used to help learn a stable feature representation by\ndecorrelating the highlighted correlations while optimizing the image\nclassification model. Extensive experiments demonstrate that our proposal can\nimprove the backbone model's accuracy in non-i.i.d image classification\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 05:48:48 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 05:08:35 GMT"}, {"version": "v3", "created": "Wed, 9 Dec 2020 03:17:38 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Yu", "Zhengxu", ""], ["Wang", "Pengfei", ""], ["Xiang", "Chao", ""], ["Xie", "Liang", ""], ["Jin", "Zhongming", ""], ["Huang", "Jianqiang", ""], ["He", "Xiaofei", ""], ["Cai", "Deng", ""], ["Hua", "Xian-Sheng", ""]]}, {"id": "2007.15248", "submitter": "Nandan Kumar Jha", "authors": "Nandan Kumar Jha, Sparsh Mittal, Binod Kumar, and Govardhan Mattela", "title": "DeepPeep: Exploiting Design Ramifications to Decipher the Architecture\n  of Compact DNNs", "comments": "Accepted at The ACM Journal on Emerging Technologies in Computing\n  Systems (JETC), 2020. 25 pages, 11 tables, and 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The remarkable predictive performance of deep neural networks (DNNs) has led\nto their adoption in service domains of unprecedented scale and scope. However,\nthe widespread adoption and growing commercialization of DNNs have underscored\nthe importance of intellectual property (IP) protection. Devising techniques to\nensure IP protection has become necessary due to the increasing trend of\noutsourcing the DNN computations on the untrusted accelerators in cloud-based\nservices. The design methodologies and hyper-parameters of DNNs are crucial\ninformation, and leaking them may cause massive economic loss to the\norganization. Furthermore, the knowledge of DNN's architecture can increase the\nsuccess probability of an adversarial attack where an adversary perturbs the\ninputs and alter the prediction.\n  In this work, we devise a two-stage attack methodology \"DeepPeep\" which\nexploits the distinctive characteristics of design methodologies to\nreverse-engineer the architecture of building blocks in compact DNNs. We show\nthe efficacy of \"DeepPeep\" on P100 and P4000 GPUs. Additionally, we propose\nintelligent design maneuvering strategies for thwarting IP theft through the\nDeepPeep attack and proposed \"Secure MobileNet-V1\". Interestingly, compared to\nvanilla MobileNet-V1, secure MobileNet-V1 provides a significant reduction in\ninference latency ($\\approx$60%) and improvement in predictive performance\n($\\approx$2%) with very-low memory and computation overheads.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 06:01:41 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Jha", "Nandan Kumar", ""], ["Mittal", "Sparsh", ""], ["Kumar", "Binod", ""], ["Mattela", "Govardhan", ""]]}, {"id": "2007.15255", "submitter": "Terrance DeVries", "authors": "Terrance DeVries, Michal Drozdzal and Graham W. Taylor", "title": "Instance Selection for GANs", "comments": "Accepted to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Generative Adversarial Networks (GANs) have led to their\nwidespread adoption for the purposes of generating high quality synthetic\nimagery. While capable of generating photo-realistic images, these models often\nproduce unrealistic samples which fall outside of the data manifold. Several\nrecently proposed techniques attempt to avoid spurious samples, either by\nrejecting them after generation, or by truncating the model's latent space.\nWhile effective, these methods are inefficient, as a large fraction of training\ntime and model capacity are dedicated towards samples that will ultimately go\nunused. In this work we propose a novel approach to improve sample quality:\naltering the training dataset via instance selection before model training has\ntaken place. By refining the empirical data distribution before training, we\nredirect model capacity towards high-density regions, which ultimately improves\nsample fidelity, lowers model capacity requirements, and significantly reduces\ntraining time. Code is available at\nhttps://github.com/uoguelph-mlrg/instance_selection_for_gans.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 06:33:51 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 04:43:07 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["DeVries", "Terrance", ""], ["Drozdzal", "Michal", ""], ["Taylor", "Graham W.", ""]]}, {"id": "2007.15270", "submitter": "Gopiram Roshan Lal", "authors": "G Roshan Lal and Sahin Cem Geyik and Krishnaram Kenthapadi", "title": "Fairness-Aware Online Personalization", "comments": "Accepted in RecSys 2020, FAccTRec Workshop: Responsible\n  Recommendation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision making in crucial applications such as lending, hiring, and college\nadmissions has witnessed increasing use of algorithmic models and techniques as\na result of a confluence of factors such as ubiquitous connectivity, ability to\ncollect, aggregate, and process large amounts of fine-grained data using cloud\ncomputing, and ease of access to applying sophisticated machine learning\nmodels. Quite often, such applications are powered by search and recommendation\nsystems, which in turn make use of personalized ranking algorithms. At the same\ntime, there is increasing awareness about the ethical and legal challenges\nposed by the use of such data-driven systems. Researchers and practitioners\nfrom different disciplines have recently highlighted the potential for such\nsystems to discriminate against certain population groups, due to biases in the\ndatasets utilized for learning their underlying recommendation models. We\npresent a study of fairness in online personalization settings involving the\nranking of individuals. Starting from a fair warm-start machine-learned model,\nwe first demonstrate that online personalization can cause the model to learn\nto act in an unfair manner if the user is biased in his/her responses. For this\npurpose, we construct a stylized model for generating training data with\npotentially biased features as well as potentially biased labels and quantify\nthe extent of bias that is learned by the model when the user responds in a\nbiased manner as in many real-world scenarios. We then formulate the problem of\nlearning personalized models under fairness constraints and present a\nregularization based approach for mitigating biases in machine learning. We\ndemonstrate the efficacy of our approach through extensive simulations with\ndifferent parameter settings. Code:\nhttps://github.com/groshanlal/Fairness-Aware-Online-Personalization\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 07:16:17 GMT"}, {"version": "v2", "created": "Sun, 6 Sep 2020 10:03:27 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Lal", "G Roshan", ""], ["Geyik", "Sahin Cem", ""], ["Kenthapadi", "Krishnaram", ""]]}, {"id": "2007.15310", "submitter": "Junyu Lin", "authors": "Junyu Lin, Lei Xu, Yingqi Liu, Xiangyu Zhang", "title": "Black-box Adversarial Sample Generation Based on Differential Evolution", "comments": "29 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are being used in various daily tasks such as\nobject detection, speech processing, and machine translation. However, it is\nknown that DNNs suffer from robustness problems -- perturbed inputs called\nadversarial samples leading to misbehaviors of DNNs. In this paper, we propose\na black-box technique called Black-box Momentum Iterative Fast Gradient Sign\nMethod (BMI-FGSM) to test the robustness of DNN models. The technique does not\nrequire any knowledge of the structure or weights of the target DNN. Compared\nto existing white-box testing techniques that require accessing model internal\ninformation such as gradients, our technique approximates gradients through\nDifferential Evolution and uses approximated gradients to construct adversarial\nsamples. Experimental results show that our technique can achieve 100% success\nin generating adversarial samples to trigger misclassification, and over 95%\nsuccess in generating samples to trigger misclassification to a specific target\noutput label. It also demonstrates better perturbation distance and better\ntransferability. Compared to the state-of-the-art black-box technique, our\ntechnique is more efficient. Furthermore, we conduct testing on the commercial\nAliyun API and successfully trigger its misbehavior within a limited number of\nqueries, demonstrating the feasibility of real-world black-box attack.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 08:43:45 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Lin", "Junyu", ""], ["Xu", "Lei", ""], ["Liu", "Yingqi", ""], ["Zhang", "Xiangyu", ""]]}, {"id": "2007.15326", "submitter": "Harrison Wilde", "authors": "Harrison Wilde, Lucia Lushi Chen, Austin Nguyen, Zoe Kimpel, Joshua\n  Sidgwick, Adolfo De Unanue, Davide Veronese, Bilal Mateen, Rayid Ghani,\n  Sebastian Vollmer", "title": "A Recommendation and Risk Classification System for Connecting Rough\n  Sleepers to Essential Outreach Services", "comments": "10 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rough sleeping is a chronic problem faced by some of the most disadvantaged\npeople in modern society. This paper describes work carried out in partnership\nwith Homeless Link, a UK-based charity, in developing a data-driven approach to\nassess the quality of incoming alerts from members of the public aimed at\nconnecting people sleeping rough on the streets with outreach service\nproviders. Alerts are prioritised based on the predicted likelihood of\nsuccessfully connecting with the rough sleeper, helping to address capacity\nlimitations and to quickly, effectively, and equitably process all of the\nalerts that they receive. Initial evaluation concludes that our approach\nincreases the rate at which rough sleepers are found following a referral by at\nleast 15\\% based on labelled data, implying a greater overall increase when the\nalerts with unknown outcomes are considered, and suggesting the benefit in a\ntrial taking place over a longer period to assess the models in practice. The\ndiscussion and modelling process is done with careful considerations of ethics,\ntransparency and explainability due to the sensitive nature of the data in this\ncontext and the vulnerability of the people that are affected.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 09:14:46 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Wilde", "Harrison", ""], ["Chen", "Lucia Lushi", ""], ["Nguyen", "Austin", ""], ["Kimpel", "Zoe", ""], ["Sidgwick", "Joshua", ""], ["De Unanue", "Adolfo", ""], ["Veronese", "Davide", ""], ["Mateen", "Bilal", ""], ["Ghani", "Rayid", ""], ["Vollmer", "Sebastian", ""]]}, {"id": "2007.15331", "submitter": "Arthur Macherey", "authors": "Marie Billaud-Friess and Arthur Macherey and Anthony Nouy and\n  Cl\\'ementine Prieur", "title": "A PAC algorithm in relative precision for bandit problem with costly\n  sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of maximizing an expectation function over a\nfinite set, or finite-arm bandit problem. We first propose a naive stochastic\nbandit algorithm for obtaining a probably approximately correct (PAC) solution\nto this discrete optimization problem in relative precision, that is a solution\nwhich solves the optimization problem up to a relative error smaller than a\nprescribed tolerance, with high probability. We also propose an adaptive\nstochastic bandit algorithm which provides a PAC-solution with the same\nguarantees. The adaptive algorithm outperforms the mean complexity of the naive\nalgorithm in terms of number of generated samples and is particularly well\nsuited for applications with high sampling cost.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 09:22:25 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Billaud-Friess", "Marie", ""], ["Macherey", "Arthur", ""], ["Nouy", "Anthony", ""], ["Prieur", "Cl\u00e9mentine", ""]]}, {"id": "2007.15353", "submitter": "Xin Yuan", "authors": "Xin Yuan, Pedro Savarese, Michael Maire", "title": "Growing Efficient Deep Networks by Structured Continuous Sparsification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an approach to training deep networks while dynamically adjusting\ntheir architecture, driven by a principled combination of accuracy and sparsity\nobjectives. Unlike conventional pruning approaches, our method adopts a gradual\ncontinuous relaxation of discrete network structure optimization and then\nsamples sparse subnetworks, enabling efficient deep networks to be trained in a\ngrowing and pruning manner. Extensive experiments across CIFAR-10, ImageNet,\nPASCAL VOC, and Penn Treebank, with convolutional models for image\nclassification and semantic segmentation, and recurrent models for language\nmodeling, show that our training scheme yields efficient networks that are\nsmaller and more accurate than those produced by competing pruning methods.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 10:03:47 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Yuan", "Xin", ""], ["Savarese", "Pedro", ""], ["Maire", "Michael", ""]]}, {"id": "2007.15359", "submitter": "Azusa Sawada", "authors": "Azusa Sawada, Eiji Kaneko, Kazutoshi Sagi", "title": "Trade-offs in Top-k Classification Accuracies on Losses for Deep\n  Learning", "comments": "Submitted to ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an experimental analysis about trade-offs in top-k\nclassification accuracies on losses for deep leaning and proposal of a novel\ntop-k loss. Commonly-used cross entropy (CE) is not guaranteed to optimize\ntop-k prediction without infinite training data and model complexities. The\nobjective is to clarify when CE sacrifices top-k accuracies to optimize top-1\nprediction, and to design loss that improve top-k accuracy under such\nconditions. Our novel loss is basically CE modified by grouping temporal top-k\nclasses as a single class. To obtain a robust decision boundary, we introduce\nan adaptive transition from normal CE to our loss, and thus call it top-k\ntransition loss. It is demonstrated that CE is not always the best choice to\nlearn top-k prediction in our experiments. First, we explore trade-offs between\ntop-1 and top-k (=2) accuracies on synthetic datasets, and find a failure of CE\nin optimizing top-k prediction when we have complex data distribution for a\ngiven model to represent optimal top-1 prediction. Second, we compare top-k\naccuracies on CIFAR-100 dataset targeting top-5 prediction in deep learning.\nWhile CE performs the best in top-1 accuracy, in top-5 accuracy our loss\nperforms better than CE except using one experimental setup. Moreover, our loss\nhas been found to provide better top-k accuracies compared to CE at k larger\nthan 10. As a result, a ResNet18 model trained with our loss reaches 99 %\naccuracy with k=25 candidates, which is a smaller candidate number than that of\nCE by 8.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 10:18:57 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Sawada", "Azusa", ""], ["Kaneko", "Eiji", ""], ["Sagi", "Kazutoshi", ""]]}, {"id": "2007.15378", "submitter": "Mahsa Forouzesh", "authors": "Mahsa Forouzesh, Farnood Salehi and Patrick Thiran", "title": "Generalization Comparison of Deep Neural Networks via Output Sensitivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although recent works have brought some insights into the performance\nimprovement of techniques used in state-of-the-art deep-learning models, more\nwork is needed to understand their generalization properties. We shed light on\nthis matter by linking the loss function to the output's sensitivity to its\ninput. We find a rather strong empirical relation between the output\nsensitivity and the variance in the bias-variance decomposition of the loss\nfunction, which hints on using sensitivity as a metric for comparing the\ngeneralization performance of networks, without requiring labeled data. We find\nthat sensitivity is decreased by applying popular methods which improve the\ngeneralization performance of the model, such as (1) using a deep network\nrather than a wide one, (2) adding convolutional layers to baseline classifiers\ninstead of adding fully-connected layers, (3) using batch normalization,\ndropout and max-pooling, and (4) applying parameter initialization techniques.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 11:08:42 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Forouzesh", "Mahsa", ""], ["Salehi", "Farnood", ""], ["Thiran", "Patrick", ""]]}, {"id": "2007.15386", "submitter": "Katharina Ott", "authors": "Katharina Ott, Prateek Katiyar, Philipp Hennig, Michael Tiemann", "title": "When are Neural ODE Solutions Proper ODEs?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key appeal of the recently proposed Neural Ordinary Differential\nEquation(ODE) framework is that it seems to provide a continuous-time extension\nof discrete residual neural networks. As we show herein, though, trained Neural\nODE models actually depend on the specific numerical method used during\ntraining. If the trained model is supposed to be a flow generated from an ODE,\nit should be possible to choose another numerical solver with equal or smaller\nnumerical error without loss of performance. We observe that if training relies\non a solver with overly coarse discretization, then testing with another solver\nof equal or smaller numerical error results in a sharp drop in accuracy. In\nsuch cases, the combination of vector field and numerical method cannot be\ninterpreted as a flow generated from an ODE, which arguably poses a fatal\nbreakdown of the Neural ODE concept. We observe, however, that there exists a\ncritical step size beyond which the training yields a valid ODE vector field.\nWe propose a method that monitors the behavior of the ODE solver during\ntraining to adapt its step size, aiming to ensure a valid ODE without\nunnecessarily increasing computational cost. We verify this adaption algorithm\non two common bench mark datasets as well as a synthetic dataset. Furthermore,\nwe introduce a novel synthetic dataset in which the underlying ODE directly\ngenerates a classification task.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 11:24:05 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Ott", "Katharina", ""], ["Katiyar", "Prateek", ""], ["Hennig", "Philipp", ""], ["Tiemann", "Michael", ""]]}, {"id": "2007.15397", "submitter": "Sebastian Pineda-Arango", "authors": "Sebastian Pineda-Arango, David Obando-Paniagua, Alperen Dedeoglu,\n  Philip Kurzend\\\"orfer, Friedemann Schestag and Randolf Scholz", "title": "Improving Sample Efficiency with Normalized RBF Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In deep learning models, learning more with less data is becoming more\nimportant. This paper explores how neural networks with normalized Radial Basis\nFunction (RBF) kernels can be trained to achieve better sample efficiency.\nMoreover, we show how this kind of output layer can find embedding spaces where\nthe classes are compact and well-separated. In order to achieve this, we\npropose a two-phase method to train those type of neural networks on\nclassification tasks. Experiments on CIFAR-10 and CIFAR-100 show that networks\nwith normalized kernels as output layer can achieve higher sample efficiency,\nhigh compactness and well-separability through the presented method in\ncomparison to networks with SoftMax output layer.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 11:40:29 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 09:25:55 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Pineda-Arango", "Sebastian", ""], ["Obando-Paniagua", "David", ""], ["Dedeoglu", "Alperen", ""], ["Kurzend\u00f6rfer", "Philip", ""], ["Schestag", "Friedemann", ""], ["Scholz", "Randolf", ""]]}, {"id": "2007.15404", "submitter": "Eslam A. Hussein", "authors": "Eslam A.Hussein, Mehrdad Ghaziasgar, Christopher Thron", "title": "Regional Rainfall Prediction Using Support Vector Machine Classification\n  of Large-Scale Precipitation Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rainfall prediction helps planners anticipate potential social and economic\nimpacts produced by too much or too little rain. This research investigates a\nclass-based approach to rainfall prediction from 1-30 days in advance. The\nstudy made regional predictions based on sequences of daily rainfall maps of\nthe continental US, with rainfall quantized at 3 levels: light or no rain;\nmoderate; and heavy rain. Three regions were selected, corresponding to three\nsquares from a $5\\times5$ grid covering the map area. Rainfall predictions up\nto 30 days ahead for these three regions were based on a support vector machine\n(SVM) applied to consecutive sequences of prior daily rainfall map images. The\nresults show that predictions for corner squares in the grid were less accurate\nthan predictions obtained by a simple untrained classifier. However, SVM\npredictions for a central region outperformed the other two regions, as well as\nthe untrained classifier. We conclude that there is some evidence that SVMs\napplied to large-scale precipitation maps can under some conditions give useful\ninformation for predicting regional rainfall, but care must be taken to avoid\npitfall\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 11:56:19 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Hussein", "Eslam A.", ""], ["Ghaziasgar", "Mehrdad", ""], ["Thron", "Christopher", ""]]}, {"id": "2007.15409", "submitter": "Amit Livne", "authors": "Amit Livne, Eliad Shem Tov, Adir Solomon, Achiya Elyasaf, Bracha\n  Shapira, and Lior Rokach", "title": "Evolving Context-Aware Recommender Systems With Users in Mind", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A context-aware recommender system (CARS) applies sensing and analysis of\nuser context to provide personalized services. The contextual information can\nbe driven from sensors in order to improve the accuracy of the recommendations.\nYet, generating accurate recommendations is not enough to constitute a useful\nsystem from the users' perspective, since certain contextual information may\ncause different issues, such as draining the user's battery, privacy issues,\nand more. Adding high-dimensional contextual information may increase both the\ndimensionality and sparsity of the model. Previous studies suggest reducing the\namount of contextual information by selecting the most suitable contextual\ninformation using a domain knowledge. Another solution is compressing it into a\ndenser latent space, thus disrupting the ability to explain the recommendation\nitem to the user, and damaging users' trust. In this paper we present an\napproach for selecting low-dimensional subsets of the contextual information\nand incorporating them explicitly within CARS. Specifically, we present a novel\nfeature-selection algorithm, based on genetic algorithms (GA), that outperforms\nSOTA dimensional-reduction CARS algorithms, improves the accuracy and the\nexplainability of the recommendations, and allows for controlling user aspects,\nsuch as privacy and battery consumption. Furthermore, we exploit the top\nsubsets that are generated along the evolutionary process, by learning multiple\ndeep context-aware models and applying a stacking technique on them, thus\nimproving the accuracy while remaining at the explicit space. We evaluated our\napproach on two high-dimensional context-aware datasets driven from\nsmartphones. An empirical analysis of our results validates that our proposed\napproach outperforms SOTA CARS models while improving transparency and\nexplainability to the user.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 12:03:22 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Livne", "Amit", ""], ["Tov", "Eliad Shem", ""], ["Solomon", "Adir", ""], ["Elyasaf", "Achiya", ""], ["Shapira", "Bracha", ""], ["Rokach", "Lior", ""]]}, {"id": "2007.15418", "submitter": "Huaqing Xiong", "authors": "Bowen Weng, Huaqing Xiong, Lin Zhao, Yingbin Liang, Wei Zhang", "title": "Momentum Q-learning with Finite-Sample Convergence Guarantee", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing studies indicate that momentum ideas in conventional optimization\ncan be used to improve the performance of Q-learning algorithms. However, the\nfinite-sample analysis for momentum-based Q-learning algorithms is only\navailable for the tabular case without function approximations. This paper\nanalyzes a class of momentum-based Q-learning algorithms with finite-sample\nguarantee. Specifically, we propose the MomentumQ algorithm, which integrates\nthe Nesterov's and Polyak's momentum schemes, and generalizes the existing\nmomentum-based Q-learning algorithms. For the infinite state-action space case,\nwe establish the convergence guarantee for MomentumQ with linear function\napproximations and Markovian sampling. In particular, we characterize the\nfinite-sample convergence rate which is provably faster than the vanilla\nQ-learning. This is the first finite-sample analysis for momentum-based\nQ-learning algorithms with function approximations. For the tabular case under\nsynchronous sampling, we also obtain a finite-sample convergence rate that is\nslightly better than the SpeedyQ \\citep{azar2011speedy} when choosing a special\nfamily of step sizes. Finally, we demonstrate through various experiments that\nthe proposed MomentumQ outperforms other momentum-based Q-learning algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 12:27:03 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Weng", "Bowen", ""], ["Xiong", "Huaqing", ""], ["Zhao", "Lin", ""], ["Liang", "Yingbin", ""], ["Zhang", "Wei", ""]]}, {"id": "2007.15421", "submitter": "Abhirup Datta", "authors": "Arkajyoti Saha, Sumanta Basu, Abhirup Datta", "title": "Random Forests for dependent data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random forest (RF) is one of the most popular methods for estimating\nregression functions. The local nature of the RF algorithm, based on intra-node\nmeans and variances, is ideal when errors are i.i.d. For dependent error\nprocesses like time series and spatial settings where data in all the nodes\nwill be correlated, operating locally ignores this dependence. Also, RF will\ninvolve resampling of correlated data, violating the principles of bootstrap.\nTheoretically, consistency of RF has been established for i.i.d. errors, but\nlittle is known about the case of dependent errors.\n  We propose RF-GLS, a novel extension of RF for dependent error processes in\nthe same way Generalized Least Squares (GLS) fundamentally extends Ordinary\nLeast Squares (OLS) for linear models under dependence. The key to this\nextension is the equivalent representation of the local decision-making in a\nregression tree as a global OLS optimization which is then replaced with a GLS\nloss to create a GLS-style regression tree. This also synergistically addresses\nthe resampling issue, as the use of GLS loss amounts to resampling uncorrelated\ncontrasts (pre-whitened data) instead of the correlated data. For spatial\nsettings, RF-GLS can be used in conjunction with Gaussian Process correlated\nerrors to generate kriging predictions at new locations. RF becomes a special\ncase of RF-GLS with an identity working covariance matrix.\n  We establish consistency of RF-GLS under beta- (absolutely regular) mixing\nerror processes and show that this general result subsumes important cases like\nautoregressive time series and spatial Matern Gaussian Processes. As a\nbyproduct, we also establish consistency of RF for beta-mixing processes, which\nto our knowledge, is the first such result for RF under dependence.\n  We empirically demonstrate the improvement achieved by RF-GLS over RF for\nboth estimation and prediction under dependence.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 12:36:09 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 15:10:51 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Saha", "Arkajyoti", ""], ["Basu", "Sumanta", ""], ["Datta", "Abhirup", ""]]}, {"id": "2007.15474", "submitter": "Hao Hao Tan", "authors": "Hao Hao Tan, Dorien Herremans", "title": "Music FaderNets: Controllable Music Generation Based On High-Level\n  Features via Low-Level Feature Modelling", "comments": null, "journal-ref": "Proc. of 21st International Society of Music Information Retrieval\n  Conference, ISMIR 2020", "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High-level musical qualities (such as emotion) are often abstract,\nsubjective, and hard to quantify. Given these difficulties, it is not easy to\nlearn good feature representations with supervised learning techniques, either\nbecause of the insufficiency of labels, or the subjectiveness (and hence large\nvariance) in human-annotated labels. In this paper, we present a framework that\ncan learn high-level feature representations with a limited amount of data, by\nfirst modelling their corresponding quantifiable low-level attributes. We refer\nto our proposed framework as Music FaderNets, which is inspired by the fact\nthat low-level attributes can be continuously manipulated by separate \"sliding\nfaders\" through feature disentanglement and latent regularization techniques.\nHigh-level features are then inferred from the low-level representations\nthrough semi-supervised clustering using Gaussian Mixture Variational\nAutoencoders (GM-VAEs). Using arousal as an example of a high-level feature, we\nshow that the \"faders\" of our model are disentangled and change linearly w.r.t.\nthe modelled low-level attributes of the generated output music. Furthermore,\nwe demonstrate that the model successfully learns the intrinsic relationship\nbetween arousal and its corresponding low-level attributes (rhythm and note\ndensity), with only 1% of the training set being labelled. Finally, using the\nlearnt high-level feature representations, we explore the application of our\nframework in style transfer tasks across different arousal states. The\neffectiveness of this approach is verified through a subjective listening test.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 16:01:45 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Tan", "Hao Hao", ""], ["Herremans", "Dorien", ""]]}, {"id": "2007.15477", "submitter": "Yusuke Tomita", "authors": "Yusuke Tomita, Kenta Shiina, Yutaka Okabe, Hwee Kuan Lee", "title": "Machine-Learning Study using Improved Correlation Configuration and\n  Application to Quantum Monte Carlo Simulation", "comments": "6 pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:2001.03989", "journal-ref": "Phys. Rev. E 102, 021302 (2020)", "doi": "10.1103/PhysRevE.102.021302", "report-no": null, "categories": "cond-mat.stat-mech stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use the Fortuin-Kasteleyn representation based improved estimator of the\ncorrelation configuration as an alternative to the ordinary correlation\nconfiguration in the machine-learning study of the phase classification of spin\nmodels. The phases of classical spin models are classified using the improved\nestimators, and the method is also applied to the quantum Monte Carlo\nsimulation using the loop algorithm. We analyze the\nBerezinskii-Kosterlitz-Thouless (BKT) transition of the spin 1/2 quantum XY\nmodel on the square lattice. We classify the BKT phase and the paramagnetic\nphase of the quantum XY model using the machine-learning approach. We show that\nthe classification of the quantum XY model can be performed by using the\ntraining data of the classical XY model.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 11:00:57 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Tomita", "Yusuke", ""], ["Shiina", "Kenta", ""], ["Okabe", "Yutaka", ""], ["Lee", "Hwee Kuan", ""]]}, {"id": "2007.15528", "submitter": "Zheng Li", "authors": "Zheng Li and Yang Zhang", "title": "Membership Leakage in Label-Only Exposures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) has been widely adopted in various privacy-critical\napplications, e.g., face recognition and medical image analysis. However,\nrecent research has shown that ML models are vulnerable to attacks against\ntheir training data. Membership inference is one major attack in this domain:\nGiven a data sample and model, an adversary aims to determine whether the\nsample is part of the model's training set. Existing membership inference\nattacks leverage the confidence scores returned by the model as their inputs\n(score-based attacks). However, these attacks can be easily mitigated if the\nmodel only exposes the predicted label, i.e., the final model decision.\n  In this paper, we propose decision-based membership inference attacks and\ndemonstrate that label-only exposures are also vulnerable to membership\nleakage. In particular, we develop two types of decision-based attacks, namely\ntransfer-attack and boundary-attack. Empirical evaluation shows that our\ndecision-based attacks can achieve remarkable performance, and even outperform\nthe previous score-based attacks. We further present new insights on the\nsuccess of membership inference based on quantitative and qualitative analysis,\ni.e., member samples of a model are more distant to the model's decision\nboundary than non-member samples. Finally, we evaluate multiple defense\nmechanisms against our decision-based attacks and show that our two types of\nattacks can bypass most of these defenses.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 15:27:55 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 17:50:29 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Li", "Zheng", ""], ["Zhang", "Yang", ""]]}, {"id": "2007.15531", "submitter": "Boris Oreshkin N", "authors": "Boris N. Oreshkin, Arezou Amini, Lucy Coyle, Mark J. Coates", "title": "FC-GAGA: Fully Connected Gated Graph Architecture for Spatio-Temporal\n  Traffic Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting of multivariate time-series is an important problem that has\napplications in traffic management, cellular network configuration, and\nquantitative finance. A special case of the problem arises when there is a\ngraph available that captures the relationships between the time-series. In\nthis paper we propose a novel learning architecture that achieves performance\ncompetitive with or better than the best existing algorithms, without requiring\nknowledge of the graph. The key element of our proposed architecture is the\nlearnable fully connected hard graph gating mechanism that enables the use of\nthe state-of-the-art and highly computationally efficient fully connected\ntime-series forecasting architecture in traffic forecasting applications.\nExperimental results for two public traffic network datasets illustrate the\nvalue of our approach, and ablation studies confirm the importance of each\nelement of the architecture. The code is available here:\nhttps://github.com/boreshkinai/fc-gaga.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 15:35:15 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 19:41:19 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Oreshkin", "Boris N.", ""], ["Amini", "Arezou", ""], ["Coyle", "Lucy", ""], ["Coates", "Mark J.", ""]]}, {"id": "2007.15535", "submitter": "Jonas Krampe", "authors": "Jonas Krampe, Efstathios Paparoditis, Carsten Trenkler", "title": "Structural Inference in Sparse High-Dimensional Vector Autoregressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider statistical inference for impulse responses in sparse, structural\nhigh-dimensional vector autoregressive (SVAR) systems. We introduce consistent\nestimators of impulse responses in the high-dimensional setting and suggest\nvalid inference procedures for the same parameters. Statistical inference in\nour setting is much more involved since standard procedures, like the\ndelta-method, do not apply. By using local projection equations, we first\nconstruct a de-sparsified version of regularized estimators of the moving\naverage parameters associated with the VAR system. We then obtain estimators of\nthe structural impulse responses by combining the aforementioned de-sparsified\nestimators with a non-regularized estimator of the contemporaneous impact\nmatrix, also taking into account the high-dimensionality of the system. We show\nthat the distribution of the derived estimators of structural impulse responses\nhas a Gaussian limit. We also present a valid bootstrap procedure to estimate\nthis distribution. Applications of the inference procedure in the construction\nof confidence intervals for impulse responses as well as in tests for forecast\nerror variance decomposition are presented. Our procedure is illustrated by\nmeans of simulations.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 15:43:15 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 15:00:17 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Krampe", "Jonas", ""], ["Paparoditis", "Efstathios", ""], ["Trenkler", "Carsten", ""]]}, {"id": "2007.15541", "submitter": "Fadhel Ayed", "authors": "Fadhel Ayed, Lorenzo Stella, Tim Januschowski, Jan Gasthaus", "title": "Anomaly Detection at Scale: The Case for Deep Distributional Time Series\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new methodology for detecting anomalies in time\nseries data, with a primary application to monitoring the health of (micro-)\nservices and cloud resources. The main novelty in our approach is that instead\nof modeling time series consisting of real values or vectors of real values, we\nmodel time series of probability distributions over real values (or vectors).\nThis extension to time series of probability distributions allows the technique\nto be applied to the common scenario where the data is generated by requests\ncoming in to a service, which is then aggregated at a fixed temporal frequency.\nOur method is amenable to streaming anomaly detection and scales to monitoring\nfor anomalies on millions of time series. We show the superior accuracy of our\nmethod on synthetic and public real-world data. On the Yahoo Webscope data set,\nwe outperform the state of the art in 3 out of 4 data sets and we show that we\noutperform popular open-source anomaly detection tools by up to 17% average\nimprovement for a real-world data set.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 15:48:55 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Ayed", "Fadhel", ""], ["Stella", "Lorenzo", ""], ["Januschowski", "Tim", ""], ["Gasthaus", "Jan", ""]]}, {"id": "2007.15543", "submitter": "Prasoon Goyal", "authors": "Prasoon Goyal, Scott Niekum, Raymond J. Mooney", "title": "PixL2R: Guiding Reinforcement Learning Using Natural Language by Mapping\n  Pixels to Rewards", "comments": "Conference on Robot Learning (CoRL), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL), particularly in sparse reward settings, often\nrequires prohibitively large numbers of interactions with the environment,\nthereby limiting its applicability to complex problems. To address this,\nseveral prior approaches have used natural language to guide the agent's\nexploration. However, these approaches typically operate on structured\nrepresentations of the environment, and/or assume some structure in the natural\nlanguage commands. In this work, we propose a model that directly maps pixels\nto rewards, given a free-form natural language description of the task, which\ncan then be used for policy learning. Our experiments on the Meta-World robot\nmanipulation domain show that language-based rewards significantly improves the\nsample efficiency of policy learning, both in sparse and dense reward settings.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 15:50:38 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 13:42:41 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Goyal", "Prasoon", ""], ["Niekum", "Scott", ""], ["Mooney", "Raymond J.", ""]]}, {"id": "2007.15553", "submitter": "Quang Pham", "authors": "Quang Pham, Doyen Sahoo, Chenghao Liu, Steven C.H Hoi", "title": "Bilevel Continual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning aims to learn continuously from a stream of tasks and data\nin an online-learning fashion, being capable of exploiting what was learned\npreviously to improve current and future tasks while still being able to\nperform well on the previous tasks. One common limitation of many existing\ncontinual learning methods is that they often train a model directly on all\navailable training data without validation due to the nature of continual\nlearning, thus suffering poor generalization at test time. In this work, we\npresent a novel framework of continual learning named \"Bilevel Continual\nLearning\" (BCL) by unifying a {\\it bilevel optimization} objective and a {\\it\ndual memory management} strategy comprising both episodic memory and\ngeneralization memory to achieve effective knowledge transfer to future tasks\nand alleviate catastrophic forgetting on old tasks simultaneously. Our\nextensive experiments on continual learning benchmarks demonstrate the efficacy\nof the proposed BCL compared to many state-of-the-art methods. Our\nimplementation is available at\nhttps://github.com/phquang/bilevel-continual-learning.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 16:00:23 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Pham", "Quang", ""], ["Sahoo", "Doyen", ""], ["Liu", "Chenghao", ""], ["Hoi", "Steven C. H", ""]]}, {"id": "2007.15567", "submitter": "Changjian Shui", "authors": "Changjian Shui, Qi Chen, Jun Wen, Fan Zhou, Christian Gagn\\'e, Boyu\n  Wang", "title": "Beyond $\\mathcal{H}$-Divergence: Domain Adaptation Theory With\n  Jensen-Shannon Divergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reveal the incoherence between the widely-adopted empirical domain\nadversarial training and its generally-assumed theoretical counterpart based on\n$\\mathcal{H}$-divergence. Concretely, we find that $\\mathcal{H}$-divergence is\nnot equivalent to Jensen-Shannon divergence, the optimization objective in\ndomain adversarial training. To this end, we establish a new theoretical\nframework by directly proving the upper and lower target risk bounds based on\njoint distributional Jensen-Shannon divergence. We further derive\nbi-directional upper bounds for marginal and conditional shifts. Our framework\nexhibits inherent flexibilities for different transfer learning problems, which\nis usable for various scenarios where $\\mathcal{H}$-divergence-based theory\nfails to adapt. From an algorithmic perspective, our theory enables a generic\nguideline unifying principles of semantic conditional matching, feature\nmarginal matching, and label marginal shift correction. We employ algorithms\nfor each principle and empirically validate the benefits of our framework on\nreal datasets.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 16:19:59 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Shui", "Changjian", ""], ["Chen", "Qi", ""], ["Wen", "Jun", ""], ["Zhou", "Fan", ""], ["Gagn\u00e9", "Christian", ""], ["Wang", "Boyu", ""]]}, {"id": "2007.15568", "submitter": "Aziz Kocanaogullari", "authors": "Aziz Kocanaogullari, Murat Akcakaya and Deniz Erdogmus", "title": "Stopping Criterion Design for Recursive Bayesian Classification:\n  Analysis and Decision Geometry", "comments": null, "journal-ref": null, "doi": "10.1109/TPAMI.2021.3075915", "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems that are based on recursive Bayesian updates for classification limit\nthe cost of evidence collection through certain stopping/termination criteria\nand accordingly enforce decision making. Conventionally, two termination\ncriteria based on pre-defined thresholds over (i) the maximum of the state\nposterior distribution; and (ii) the state posterior uncertainty are commonly\nused. In this paper, we propose a geometric interpretation over the state\nposterior progression and accordingly we provide a point-by-point analysis over\nthe disadvantages of using such conventional termination criteria. For example,\nthrough the proposed geometric interpretation we show that confidence\nthresholds defined over maximum of the state posteriors suffer from stiffness\nthat results in unnecessary evidence collection whereas uncertainty based\nthresholding methods are fragile to number of categories and terminate\nprematurely if some state candidates are already discovered to be unfavorable.\nMoreover, both types of termination methods neglect the evolution of posterior\nupdates. We then propose a new stopping/termination criterion with a\ngeometrical insight to overcome the limitations of these conventional methods\nand provide a comparison in terms of decision accuracy and speed. We validate\nour claims using simulations and using real experimental data obtained through\na brain computer interfaced typing system.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 16:21:10 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 23:23:22 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Kocanaogullari", "Aziz", ""], ["Akcakaya", "Murat", ""], ["Erdogmus", "Deniz", ""]]}, {"id": "2007.15580", "submitter": "Dongxiao Zhang", "authors": "Nanzhe Wang, Haibin Chang, and Dongxiao Zhang", "title": "Deep-Learning based Inverse Modeling Approaches: A Subsurface Flow\n  Example", "comments": "53 pages, 22 figures, 7 tables", "journal-ref": "Journal of Geophysical Research: Solid Earth, e2020JB020549, 2020", "doi": "10.1029/2020JB020549", "report-no": null, "categories": "eess.SP cs.LG math.OC physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep-learning has achieved good performance and shown great potential for\nsolving forward and inverse problems. In this work, two categories of\ninnovative deep-learning based inverse modeling methods are proposed and\ncompared. The first category is deep-learning surrogate-based inversion\nmethods, in which the Theory-guided Neural Network (TgNN) is constructed as a\ndeep-learning surrogate for problems with uncertain model parameters. By\nincorporating physical laws and other constraints, the TgNN surrogate can be\nconstructed with limited simulation runs and accelerate the inversion process\nsignificantly. Three TgNN surrogate-based inversion methods are proposed,\nincluding the gradient method, the iterative ensemble smoother (IES), and the\ntraining method. The second category is direct-deep-learning-inversion methods,\nin which TgNN constrained with geostatistical information, named TgNN-geo, is\nproposed for direct inverse modeling. In TgNN-geo, two neural networks are\nintroduced to approximate the respective random model parameters and the\nsolution. Since the prior geostatistical information can be incorporated, the\ndirect-inversion method based on TgNN-geo works well, even in cases with sparse\nspatial measurements or imprecise prior statistics. Although the proposed\ndeep-learning based inverse modeling methods are general in nature, and thus\napplicable to a wide variety of problems, they are tested with several\nsubsurface flow problems. It is found that satisfactory results are obtained\nwith a high efficiency. Moreover, both the advantages and disadvantages are\nfurther analyzed for the proposed two categories of deep-learning based\ninversion methods.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 15:31:07 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Wang", "Nanzhe", ""], ["Chang", "Haibin", ""], ["Zhang", "Dongxiao", ""]]}, {"id": "2007.15588", "submitter": "Markus Wulfmeier", "authors": "Markus Wulfmeier, Dushyant Rao, Roland Hafner, Thomas Lampe, Abbas\n  Abdolmaleki, Tim Hertweck, Michael Neunert, Dhruva Tirumala, Noah Siegel,\n  Nicolas Heess, Martin Riedmiller", "title": "Data-efficient Hindsight Off-policy Option Learning", "comments": "Published at ICML2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Hindsight Off-policy Options (HO2), a data-efficient option\nlearning algorithm. Given any trajectory, HO2 infers likely option choices and\nbackpropagates through the dynamic programming inference procedure to robustly\ntrain all policy components off-policy and end-to-end. The approach outperforms\nexisting option learning methods on common benchmarks. To better understand the\noption framework and disentangle benefits from both temporal and action\nabstraction, we evaluate ablations with flat policies and mixture policies with\ncomparable optimization. The results highlight the importance of both types of\nabstraction as well as off-policy training and trust-region constraints,\nparticularly in challenging, simulated 3D robot manipulation tasks from raw\npixel inputs. Finally, we intuitively adapt the inference step to investigate\nthe effect of increased temporal abstraction on training with pre-trained\noptions and from scratch.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 16:52:33 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 15:55:50 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Wulfmeier", "Markus", ""], ["Rao", "Dushyant", ""], ["Hafner", "Roland", ""], ["Lampe", "Thomas", ""], ["Abdolmaleki", "Abbas", ""], ["Hertweck", "Tim", ""], ["Neunert", "Michael", ""], ["Tirumala", "Dhruva", ""], ["Siegel", "Noah", ""], ["Heess", "Nicolas", ""], ["Riedmiller", "Martin", ""]]}, {"id": "2007.15598", "submitter": "Ning Xu", "authors": "Ning Xu, Timothy C.G. Fisher, Jian Hong", "title": "Rademacher upper bounds for cross-validation errors with an application\n  to the lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish a general upper bound for $K$-fold cross-validation ($K$-CV)\nerrors that can be adapted to many $K$-CV-based estimators and learning\nalgorithms. Based on Rademacher complexity of the model and the\nOrlicz-$\\Psi_{\\nu}$ norm of the error process, the CV error upper bound applies\nto both light-tail and heavy-tail error distributions. We also extend the CV\nerror upper bound to $\\beta$-mixing data using the technique of independent\nblocking. We provide a Python package (\\texttt{CVbound},\n\\url{https://github.com/isaac2math}) for computing the CV error upper bound in\n$K$-CV-based algorithms. Using the lasso as an example, we demonstrate in\nsimulations that the upper bounds are tight and stable across different\nparameter settings and random seeds. As well as accurately bounding the CV\nerrors for the lasso, the minimizer of the new upper bounds can be used as a\ncriterion for variable selection. Compared with the CV-error minimizer,\nsimulations show that tuning the lasso penalty parameter according to the\nminimizer of the upper bound yields a more sparse and more stable model that\nretains all of the relevant variables.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 17:13:03 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Xu", "Ning", ""], ["Fisher", "Timothy C. G.", ""], ["Hong", "Jian", ""]]}, {"id": "2007.15614", "submitter": "Ning Xu", "authors": "Ning Xu, Timothy C.G. Fisher, Jian Hong", "title": "Accuracy and stability of solar variable selection comparison under\n  complicated dependence structures", "comments": "Minor errors on data and table fixed; to focus on variable selection,\n  causal inference moved to arXiv:2007.15769", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we focus on the empirical variable-selection peformance of\nsubsample-ordered least angle regression (Solar) -- a novel ultrahigh\ndimensional redesign of lasso -- on the empirical data with complicated\ndependence structures and, hence, severe multicollinearity and grouping effect\nissues. Previous researches show that Solar largely alleviates several known\nhigh-dimensional issues with least-angle regression and $\\mathcal{L}_1$\nshrinkage. Also, With the same computation load, solar yields substantiali\nmprovements over two lasso solvers (least-angle regression for lasso and\ncoordinate-descent) in terms of the sparsity (37-64\\% reduction in the average\nnumber of selected variables), stability and accuracy of variable selection.\nSimulations also demonstrate that solar enhances the robustness of variable\nselection to different settings of the irrepresentable condition and to\nvariations in the dependence structures assumed in regression analysis. To\nconfirm that the improvements are also available for empirical researches, we\nchoose the prostate cancer data and the Sydney house price data and apply two\nlasso solvers, elastic net and Solar on them for comparison. The results shows\nthat (i) lasso is affected by the grouping effect and randomly drop variables\nwith high correlations, resulting unreliable and uninterpretable results; (ii)\nelastic net is more robust to grouping effect; however, it completely lose\nvariable-selection sparsity when the dependence structure of the data is\ncomplicated; (iii) solar demonstrates its superior robustness to complicated\ndependence structures and grouping effect, returning variable-selection results\nwith better stability and sparsity. The code can be found at\nhttps://github.com/isaac2math/solar_application\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 17:29:00 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 18:30:24 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Xu", "Ning", ""], ["Fisher", "Timothy C. G.", ""], ["Hong", "Jian", ""]]}, {"id": "2007.15618", "submitter": "Ankit Pensia", "authors": "Ilias Diakonikolas, Daniel M. Kane, Ankit Pensia", "title": "Outlier Robust Mean Estimation with Subgaussian Rates via Stability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of outlier robust high-dimensional mean estimation under\na finite covariance assumption, and more broadly under finite low-degree moment\nassumptions. We consider a standard stability condition from the recent robust\nstatistics literature and prove that, except with exponentially small failure\nprobability, there exists a large fraction of the inliers satisfying this\ncondition. As a corollary, it follows that a number of recently developed\nalgorithms for robust mean estimation, including iterative filtering and\nnon-convex gradient descent, give optimal error estimators with\n(near-)subgaussian rates. Previous analyses of these algorithms gave\nsignificantly suboptimal rates. As a corollary of our approach, we obtain the\nfirst computationally efficient algorithm with subgaussian rate for\noutlier-robust mean estimation in the strong contamination model under a finite\ncovariance assumption.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 17:33:03 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 15:58:25 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Pensia", "Ankit", ""]]}, {"id": "2007.15623", "submitter": "Stephan Wojtowytsch", "authors": "Weinan E and Stephan Wojtowytsch", "title": "On the Banach spaces associated with multi-layer ReLU networks: Function\n  representation, approximation theory and gradient descent dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop Banach spaces for ReLU neural networks of finite depth $L$ and\ninfinite width. The spaces contain all finite fully connected $L$-layer\nnetworks and their $L^2$-limiting objects under bounds on the natural\npath-norm. Under this norm, the unit ball in the space for $L$-layer networks\nhas low Rademacher complexity and thus favorable generalization properties.\nFunctions in these spaces can be approximated by multi-layer neural networks\nwith dimension-independent convergence rates.\n  The key to this work is a new way of representing functions in some form of\nexpectations, motivated by multi-layer neural networks. This representation\nallows us to define a new class of continuous models for machine learning. We\nshow that the gradient flow defined this way is the natural continuous analog\nof the gradient descent dynamics for the associated multi-layer neural\nnetworks. We show that the path-norm increases at most polynomially under this\ncontinuous gradient flow dynamics.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 17:47:05 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["E", "Weinan", ""], ["Wojtowytsch", "Stephan", ""]]}, {"id": "2007.15707", "submitter": "Ning Xu", "authors": "Ning Xu, Timothy C.G. Fisher, Jian Hong", "title": "Solar: a least-angle regression for stable variable selection in\n  high-dimensional spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm for variable selection in high-dimensional data,\ncalled subsample-ordered least-angle regression (solar). Solar relies on the\naverage $L_0$ solution path computed across subsamples and alleviates several\nknown high-dimensional issues with lasso and least-angle regression. We\nillustrate in simulations that, with the same computation load, solar yields\nsubstantial improvements over lasso in terms of the sparsity (37-64\\% reduction\nin the average number of selected variables), stability and accuracy of\nvariable selection. Moreover, solar supplemented with the hold-out average (an\nadaptation of classical post-OLS tests) successfully purges almost all of the\nredundant variables while retaining all of the informative variables. Using\nsimulations and real-world data, we also illustrate numerically that sparse\nsolar variable selection is robust to complicated dependence structures and\nharsh settings of the irrepresentable condition. Moreover, replacing lasso with\nsolar in an ensemble system (e.g., the bootstrap ensemble), significantly\nreduces the computation load (at least 96\\% fewer subsample repetitions) of the\nbootstrap ensemble and improves selection sparsity. We provide a Python\nparallel computing package for solar (solarpy) in the supplementary file and\nhttps://github.com/isaac2math/solar.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 19:45:59 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 09:49:08 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Xu", "Ning", ""], ["Fisher", "Timothy C. G.", ""], ["Hong", "Jian", ""]]}, {"id": "2007.15710", "submitter": "Mert Al", "authors": "Mert Al, Semih Yagli, Sun-Yuan Kung", "title": "Privacy Enhancing Machine Learning via Removal of Unwanted Dependencies", "comments": "15 pages, 5 figures, submitted to IEEE Transactions on Neural\n  Networks and Learning Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid rise of IoT and Big Data has facilitated copious data driven\napplications to enhance our quality of life. However, the omnipresent and\nall-encompassing nature of the data collection can generate privacy concerns.\nHence, there is a strong need to develop techniques that ensure the data serve\nonly the intended purposes, giving users control over the information they\nshare. To this end, this paper studies new variants of supervised and\nadversarial learning methods, which remove the sensitive information in the\ndata before they are sent out for a particular application. The explored\nmethods optimize privacy preserving feature mappings and predictive models\nsimultaneously in an end-to-end fashion. Additionally, the models are built\nwith an emphasis on placing little computational burden on the user side so\nthat the data can be desensitized on device in a cheap manner. Experimental\nresults on mobile sensing and face datasets demonstrate that our models can\nsuccessfully maintain the utility performances of predictive models while\ncausing sensitive predictions to perform poorly.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 19:55:10 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 05:52:14 GMT"}, {"version": "v3", "created": "Wed, 3 Feb 2021 03:32:11 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Al", "Mert", ""], ["Yagli", "Semih", ""], ["Kung", "Sun-Yuan", ""]]}, {"id": "2007.15745", "submitter": "Li Yang", "authors": "Li Yang, Abdallah Shami", "title": "On Hyperparameter Optimization of Machine Learning Algorithms: Theory\n  and Practice", "comments": "69 Pages, 10 tables, accepted in Neurocomputing, Elsevier. Github\n  link:\n  https://github.com/LiYangHart/Hyperparameter-Optimization-of-Machine-Learning-Algorithms", "journal-ref": null, "doi": "10.1016/j.neucom.2020.07.061", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning algorithms have been used widely in various applications and\nareas. To fit a machine learning model into different problems, its\nhyper-parameters must be tuned. Selecting the best hyper-parameter\nconfiguration for machine learning models has a direct impact on the model's\nperformance. It often requires deep knowledge of machine learning algorithms\nand appropriate hyper-parameter optimization techniques. Although several\nautomatic optimization techniques exist, they have different strengths and\ndrawbacks when applied to different types of problems. In this paper,\noptimizing the hyper-parameters of common machine learning models is studied.\nWe introduce several state-of-the-art optimization techniques and discuss how\nto apply them to machine learning algorithms. Many available libraries and\nframeworks developed for hyper-parameter optimization problems are provided,\nand some open challenges of hyper-parameter optimization research are also\ndiscussed in this paper. Moreover, experiments are conducted on benchmark\ndatasets to compare the performance of different optimization methods and\nprovide practical examples of hyper-parameter optimization. This survey paper\nwill help industrial users, data analysts, and researchers to better develop\nmachine learning models by identifying the proper hyper-parameter\nconfigurations effectively.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 21:11:01 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 15:42:07 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Yang", "Li", ""], ["Shami", "Abdallah", ""]]}, {"id": "2007.15766", "submitter": "Wicher Bergsma", "authors": "Wicher Bergsma and Haziq Jamil", "title": "Regression modelling with I-priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the I-prior methodology as a unifying framework for estimating a\nvariety of regression models, including varying coefficient, multilevel,\nlongitudinal models, and models with functional covariates and responses. It\ncan also be used for multi-class classification, with low or high dimensional\ncovariates.\n  The I-prior is generally defined as a maximum entropy prior. For a regression\nfunction, the I-prior is Gaussian with covariance kernel proportional to the\nFisher information on the regression function, which is estimated by its\nposterior distribution under the I-prior. The I-prior has the intuitively\nappealing property that the more information is available on a linear\nfunctional of the regression function, the larger the prior variance, and the\nsmaller the influence of the prior mean on the posterior distribution.\n  Advantages compared to competing methods, such as Gaussian process regression\nor Tikhonov regularization, are ease of estimation and model comparison. In\nparticular, we develop an EM algorithm with a simple E and M step for\nestimating hyperparameters, facilitating estimation for complex models. We also\npropose a novel parsimonious model formulation, requiring a single scale\nparameter for each (possibly multidimensional) covariate and no further\nparameters for interaction effects. This simplifies estimation because fewer\nhyperparameters need to be estimated, and also simplifies model comparison of\nmodels with the same covariates but different interaction effects; in this\ncase, the model with the highest estimated likelihood can be selected.\n  Using a number of widely analyzed real data sets we show that predictive\nperformance of our methodology is competitive. An R-package implementing the\nmethodology is available (Jamil, 2019).\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 22:52:22 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 19:39:01 GMT"}, {"version": "v3", "created": "Mon, 14 Sep 2020 11:54:56 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Bergsma", "Wicher", ""], ["Jamil", "Haziq", ""]]}, {"id": "2007.15769", "submitter": "Ning Xu", "authors": "Ning Xu, Timothy C.G. Fisher, Jian Hong", "title": "Instrument variable detection with graph learning : an application to\n  high dimensional GIS-census data for house pricing", "comments": "introduction rewritten; detailed graph learning and variable\n  selection procedure explained", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Endogeneity bias and instrument variable validation have always been\nimportant topics in statistics and econometrics. In the era of big data, such\nissues typically combine with dimensionality issues and, hence, require even\nmore attention. In this paper, we merge two well-known tools from machine\nlearning and biostatistics---variable selection algorithms and probablistic\ngraphs---to estimate house prices and the corresponding causal structure using\n2010 data on Sydney. The estimation uses a 200-gigabyte ultrahigh dimensional\ndatabase consisting of local school data, GIS information, census data, house\ncharacteristics and other socio-economic records. Using \"big data\", we show\nthat it is possible to perform a data-driven instrument selection efficiently\nand purge out the invalid instruments. Our approach improves the sparsity of\nvariable selection, stability and robustness in the presence of high\ndimensionality, complicated causal structures and the consequent\nmulticollinearity, and recovers a sparse and intuitive causal structure. The\napproach also reveals an efficiency and effectiveness in endogeneity detection,\ninstrument validation, weak instrument pruning and the selection of valid\ninstruments. From the perspective of machine learning, the estimation results\nboth align with and confirms the facts of Sydney house market, the classical\neconomic theories and the previous findings of simultaneous equations modeling.\nMoreover, the estimation results are consistent with and supported by classical\neconometric tools such as two-stage least square regression and different\ninstrument tests. All the code may be found at\n\\url{https://github.com/isaac2math/solar_graph_learning}.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 23:11:54 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 18:24:22 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Xu", "Ning", ""], ["Fisher", "Timothy C. G.", ""], ["Hong", "Jian", ""]]}, {"id": "2007.15776", "submitter": "Palina Salanevich", "authors": "Deanna Needell, Aaron A. Nelson, Rayan Saab, Palina Salanevich", "title": "Random Vector Functional Link Networks for Function Approximation on\n  Manifolds", "comments": "40 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The learning speed of feed-forward neural networks is notoriously slow and\nhas presented a bottleneck in deep learning applications for several decades.\nFor instance, gradient-based learning algorithms, which are used extensively to\ntrain neural networks, tend to work slowly when all of the network parameters\nmust be iteratively tuned. To counter this, both researchers and practitioners\nhave tried introducing randomness to reduce the learning requirement. Based on\nthe original construction of Igelnik and Pao, single layer neural-networks with\nrandom input-to-hidden layer weights and biases have seen success in practice,\nbut the necessary theoretical justification is lacking. In this paper, we begin\nto fill this theoretical gap. We provide a (corrected) rigorous proof that the\nIgelnik and Pao construction is a universal approximator for continuous\nfunctions on compact domains, with approximation error decaying asymptotically\nlike $O(1/\\sqrt{n})$ for the number $n$ of network nodes. We then extend this\nresult to the non-asymptotic setting, proving that one can achieve any desired\napproximation error with high probability provided $n$ is sufficiently large.\nWe further adapt this randomized neural network architecture to approximate\nfunctions on smooth, compact submanifolds of Euclidean space, providing\ntheoretical guarantees in both the asymptotic and non-asymptotic forms.\nFinally, we illustrate our results on manifolds with numerical experiments.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 23:50:44 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Needell", "Deanna", ""], ["Nelson", "Aaron A.", ""], ["Saab", "Rayan", ""], ["Salanevich", "Palina", ""]]}, {"id": "2007.15788", "submitter": "Botao Hao", "authors": "Botao Hao, Jie Zhou, Zheng Wen, Will Wei Sun", "title": "Low-rank Tensor Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, multi-dimensional online decision making has been playing a\ncrucial role in many practical applications such as online recommendation and\ndigital marketing. To solve it, we introduce stochastic low-rank tensor\nbandits, a class of bandits whose mean rewards can be represented as a low-rank\ntensor. We propose two learning algorithms, tensor epoch-greedy and tensor\nelimination, and develop finite-time regret bounds for them. We observe that\ntensor elimination has an optimal dependency on the time horizon, while tensor\nepoch-greedy has a sharper dependency on tensor dimensions. Numerical\nexperiments further back up these theoretical findings and show that our\nalgorithms outperform various state-of-the-art approaches that ignore the\ntensor low-rank structure.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 01:05:53 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Hao", "Botao", ""], ["Zhou", "Jie", ""], ["Wen", "Zheng", ""], ["Sun", "Will Wei", ""]]}, {"id": "2007.15801", "submitter": "Jaehoon Lee", "authors": "Jaehoon Lee, Samuel S. Schoenholz, Jeffrey Pennington, Ben Adlam,\n  Lechao Xiao, Roman Novak, Jascha Sohl-Dickstein", "title": "Finite Versus Infinite Neural Networks: an Empirical Study", "comments": "17+11 pages; v2 references added, minor improvements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform a careful, thorough, and large scale empirical study of the\ncorrespondence between wide neural networks and kernel methods. By doing so, we\nresolve a variety of open questions related to the study of infinitely wide\nneural networks. Our experimental results include: kernel methods outperform\nfully-connected finite-width networks, but underperform convolutional finite\nwidth networks; neural network Gaussian process (NNGP) kernels frequently\noutperform neural tangent (NT) kernels; centered and ensembled finite networks\nhave reduced posterior variance and behave more similarly to infinite networks;\nweight decay and the use of a large learning rate break the correspondence\nbetween finite and infinite networks; the NTK parameterization outperforms the\nstandard parameterization for finite width networks; diagonal regularization of\nkernels acts similarly to early stopping; floating point precision limits\nkernel performance beyond a critical dataset size; regularized ZCA whitening\nimproves accuracy; finite network performance depends non-monotonically on\nwidth in ways not captured by double descent phenomena; equivariance of CNNs is\nonly beneficial for narrow networks far from the kernel regime. Our experiments\nadditionally motivate an improved layer-wise scaling for weight decay which\nimproves generalization in finite-width networks. Finally, we develop improved\nbest practices for using NNGP and NT kernels for prediction, including a novel\nensembling technique. Using these best practices we achieve state-of-the-art\nresults on CIFAR-10 classification for kernels corresponding to each\narchitecture class we consider.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 01:57:47 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 06:25:57 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Lee", "Jaehoon", ""], ["Schoenholz", "Samuel S.", ""], ["Pennington", "Jeffrey", ""], ["Adlam", "Ben", ""], ["Xiao", "Lechao", ""], ["Novak", "Roman", ""], ["Sohl-Dickstein", "Jascha", ""]]}, {"id": "2007.15802", "submitter": "Ren Wang", "authors": "Ren Wang, Gaoyuan Zhang, Sijia Liu, Pin-Yu Chen, Jinjun Xiong, Meng\n  Wang", "title": "Practical Detection of Trojan Neural Networks: Data-Limited and\n  Data-Free Cases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the training data are maliciously tampered, the predictions of the\nacquired deep neural network (DNN) can be manipulated by an adversary known as\nthe Trojan attack (or poisoning backdoor attack). The lack of robustness of\nDNNs against Trojan attacks could significantly harm real-life machine learning\n(ML) systems in downstream applications, therefore posing widespread concern to\ntheir trustworthiness. In this paper, we study the problem of the Trojan\nnetwork (TrojanNet) detection in the data-scarce regime, where only the weights\nof a trained DNN are accessed by the detector. We first propose a data-limited\nTrojanNet detector (TND), when only a few data samples are available for\nTrojanNet detection. We show that an effective data-limited TND can be\nestablished by exploring connections between Trojan attack and\nprediction-evasion adversarial attacks including per-sample attack as well as\nall-sample universal attack. In addition, we propose a data-free TND, which can\ndetect a TrojanNet without accessing any data samples. We show that such a TND\ncan be built by leveraging the internal response of hidden neurons, which\nexhibits the Trojan behavior even at random noise inputs. The effectiveness of\nour proposals is evaluated by extensive experiments under different model\narchitectures and datasets including CIFAR-10, GTSRB, and ImageNet.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 02:00:38 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Wang", "Ren", ""], ["Zhang", "Gaoyuan", ""], ["Liu", "Sijia", ""], ["Chen", "Pin-Yu", ""], ["Xiong", "Jinjun", ""], ["Wang", "Meng", ""]]}, {"id": "2007.15816", "submitter": "Changlin Wan", "authors": "Changlin Wan, Wennan Chang, Tong Zhao, Sha Cao, Chi Zhang", "title": "Denoising individual bias for a fairer binary submatrix detection", "comments": "Accepted at CIKM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low rank representation of binary matrix is powerful in disentangling sparse\nindividual-attribute associations, and has received wide applications. Existing\nbinary matrix factorization (BMF) or co-clustering (CC) methods often assume\ni.i.d background noise. However, this assumption could be easily violated in\nreal data, where heterogeneous row- or column-wise probability of binary\nentries results in disparate element-wise background distribution, and\nparalyzes the rationality of existing methods. We propose a binary data\ndenoising framework, namely BIND, which optimizes the detection of true\npatterns by estimating the row- or column-wise mixture distribution of patterns\nand disparate background, and eliminating the binary attributes that are more\nlikely from the background. BIND is supported by thoroughly derived\nmathematical property of the row- and column-wise mixture distributions. Our\nexperiment on synthetic and real-world data demonstrated BIND effectively\nremoves background noise and drastically increases the fairness and accuracy of\nstate-of-the arts BMF and CC methods.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 02:52:25 GMT"}, {"version": "v2", "created": "Sun, 9 Aug 2020 15:39:37 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Wan", "Changlin", ""], ["Chang", "Wennan", ""], ["Zhao", "Tong", ""], ["Cao", "Sha", ""], ["Zhang", "Chi", ""]]}, {"id": "2007.15821", "submitter": "Changlin Wan", "authors": "Changlin Wan, Wennan Chang, Tong Zhao, Sha Cao, Chi Zhang", "title": "Geometric All-Way Boolean Tensor Decomposition", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boolean tensor has been broadly utilized in representing high dimensional\nlogical data collected on spatial, temporal and/or other relational domains.\nBoolean Tensor Decomposition (BTD) factorizes a binary tensor into the Boolean\nsum of multiple rank-1 tensors, which is an NP-hard problem. Existing BTD\nmethods have been limited by their high computational cost, in applications to\nlarge scale or higher order tensors. In this work, we presented a\ncomputationally efficient BTD algorithm, namely \\textit{Geometric Expansion for\nall-order Tensor Factorization} (GETF), that sequentially identifies the rank-1\nbasis components for a tensor from a geometric perspective. We conducted\nrigorous theoretical analysis on the validity as well as algorithemic\nefficiency of GETF in decomposing all-order tensor. Experiments on both\nsynthetic and real-world data demonstrated that GETF has significantly improved\nperformance in reconstruction accuracy, extraction of latent structures and it\nis an order of magnitude faster than other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 03:29:44 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 19:21:59 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Wan", "Changlin", ""], ["Chang", "Wennan", ""], ["Zhao", "Tong", ""], ["Cao", "Sha", ""], ["Zhang", "Chi", ""]]}, {"id": "2007.15835", "submitter": "Mukund Sudarshan", "authors": "Mukund Sudarshan, Wesley Tansey, Rajesh Ranganath", "title": "Deep Direct Likelihood Knockoffs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Predictive modeling often uses black box machine learning methods, such as\ndeep neural networks, to achieve state-of-the-art performance. In scientific\ndomains, the scientist often wishes to discover which features are actually\nimportant for making the predictions. These discoveries may lead to costly\nfollow-up experiments and as such it is important that the error rate on\ndiscoveries is not too high. Model-X knockoffs enable important features to be\ndiscovered with control of the FDR. However, knockoffs require rich generative\nmodels capable of accurately modeling the knockoff features while ensuring they\nobey the so-called \"swap\" property. We develop Deep Direct Likelihood Knockoffs\n(DDLK), which directly minimizes the KL divergence implied by the knockoff swap\nproperty. DDLK consists of two stages: it first maximizes the explicit\nlikelihood of the features, then minimizes the KL divergence between the joint\ndistribution of features and knockoffs and any swap between them. To ensure\nthat the generated knockoffs are valid under any possible swap, DDLK uses the\nGumbel-Softmax trick to optimize the knockoff generator under the worst-case\nswap. We find DDLK has higher power than baselines while controlling the false\ndiscovery rate on a variety of synthetic and real benchmarks including a task\ninvolving a large dataset from one of the epicenters of COVID-19.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 04:09:46 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Sudarshan", "Mukund", ""], ["Tansey", "Wesley", ""], ["Ranganath", "Rajesh", ""]]}, {"id": "2007.15836", "submitter": "Yaguan Qian", "authors": "Yaguan Qian and Ximin Zhang and Bin Wang and Wei Li and Zhaoquan Gu\n  and Haijiang Wang and Wassim Swaileh", "title": "TEAM: We Need More Powerful Adversarial Examples for DNNs", "comments": "14 pages,13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep neural networks (DNNs) have achieved success in many\napplication fields, it is still vulnerable to imperceptible adversarial\nexamples that can lead to misclassification of DNNs easily. To overcome this\nchallenge, many defensive methods are proposed. Indeed, a powerful adversarial\nexample is a key benchmark to measure these defensive mechanisms. In this\npaper, we propose a novel method (TEAM, Taylor Expansion-Based Adversarial\nMethods) to generate more powerful adversarial examples than previous methods.\nThe main idea is to craft adversarial examples by minimizing the confidence of\nthe ground-truth class under untargeted attacks or maximizing the confidence of\nthe target class under targeted attacks. Specifically, we define the new\nobjective functions that approximate DNNs by using the second-order Taylor\nexpansion within a tiny neighborhood of the input. Then the Lagrangian\nmultiplier method is used to obtain the optimize perturbations for these\nobjective functions. To decrease the amount of computation, we further\nintroduce the Gauss-Newton (GN) method to speed it up. Finally, the\nexperimental result shows that our method can reliably produce adversarial\nexamples with 100% attack success rate (ASR) while only by smaller\nperturbations. In addition, the adversarial example generated with our method\ncan defeat defensive distillation based on gradient masking.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 04:11:02 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 01:38:11 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Qian", "Yaguan", ""], ["Zhang", "Ximin", ""], ["Wang", "Bin", ""], ["Li", "Wei", ""], ["Gu", "Zhaoquan", ""], ["Wang", "Haijiang", ""], ["Swaileh", "Wassim", ""]]}, {"id": "2007.15839", "submitter": "Fred Zhang", "authors": "Samuel B. Hopkins, Jerry Li, Fred Zhang", "title": "Robust and Heavy-Tailed Mean Estimation Made Simple, via Regret\n  Minimization", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the mean of a distribution in high\ndimensions when either the samples are adversarially corrupted or the\ndistribution is heavy-tailed. Recent developments in robust statistics have\nestablished efficient and (near) optimal procedures for both settings. However,\nthe algorithms developed on each side tend to be sophisticated and do not\ndirectly transfer to the other, with many of them having ad-hoc or complicated\nanalyses.\n  In this paper, we provide a meta-problem and a duality theorem that lead to a\nnew unified view on robust and heavy-tailed mean estimation in high dimensions.\nWe show that the meta-problem can be solved either by a variant of the Filter\nalgorithm from the recent literature on robust estimation or by the quantum\nentropy scoring scheme (QUE), due to Dong, Hopkins and Li (NeurIPS '19). By\nleveraging our duality theorem, these results translate into simple and\nefficient algorithms for both robust and heavy-tailed settings. Furthermore,\nthe QUE-based procedure has run-time that matches the fastest known algorithms\non both fronts.\n  Our analysis of Filter is through the classic regret bound of the\nmultiplicative weights update method. This connection allows us to avoid the\ntechnical complications in previous works and improve upon the run-time\nanalysis of a gradient-descent-based algorithm for robust mean estimation by\nCheng, Diakonikolas, Ge and Soltanolkotabi (ICML '20).\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 04:18:32 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 00:27:26 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Hopkins", "Samuel B.", ""], ["Li", "Jerry", ""], ["Zhang", "Fred", ""]]}, {"id": "2007.15840", "submitter": "Zhao Zhang", "authors": "Zhao Zhang, Yan Zhang, Mingliang Xu, Li Zhang, Yi Yang, Shuicheng Yan", "title": "A Survey on Concept Factorization: From Shallow to Deep Representation\n  Learning", "comments": "Please cite this work as: Zhao Zhang, Yan Zhang, Mingliang Xu, Li\n  Zhang, Yi Yang and Shuicheng Yan, \"A Survey on Concept Factorization: From\n  Shallow to Deep Representation Learning,\" Information Processing and\n  Management (IPM), Jan 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality of learned features by representation learning determines the\nperformance of learning algorithms and the related application tasks (such as\nhigh-dimensional data clustering). As a relatively new paradigm for\nrepresentation learning, Concept Factorization (CF) has attracted a great deal\nof interests in the areas of machine learning and data mining for over a\ndecade. Lots of effective CF based methods have been proposed based on\ndifferent perspectives and properties, but note that it still remains not easy\nto grasp the essential connections and figure out the underlying explanatory\nfactors from exiting studies. In this paper, we therefore survey the recent\nadvances on CF methodologies and the potential benchmarks by categorizing and\nsummarizing the current methods. Specifically, we first re-view the root CF\nmethod, and then explore the advancement of CF-based representation learning\nranging from shallow to deep/multilayer cases. We also introduce the potential\napplication areas of CF-based methods. Finally, we point out some future\ndirections for studying the CF-based representation learning. Overall, this\nsurvey provides an insightful overview of both theoretical basis and current\ndevelopments in the field of CF, which can also help the interested researchers\nto understand the current trends of CF and find the most appropriate CF\ntechniques to deal with particular applications.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 04:19:14 GMT"}, {"version": "v2", "created": "Sun, 3 Jan 2021 01:56:01 GMT"}, {"version": "v3", "created": "Sun, 31 Jan 2021 08:45:58 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Zhang", "Zhao", ""], ["Zhang", "Yan", ""], ["Xu", "Mingliang", ""], ["Zhang", "Li", ""], ["Yang", "Yi", ""], ["Yan", "Shuicheng", ""]]}, {"id": "2007.15847", "submitter": "Syed Hasib Akhter Faruqui", "authors": "Syed Hasib Akhter Faruqui, Adel Alaeddini, Jing Wang, and Carlos A.\n  Jaramillo", "title": "A Functional Model for Structure Learning and Parameter Estimation in\n  Continuous Time Bayesian Network: An Application in Identifying Patterns of\n  Multiple Chronic Conditions", "comments": "Submitted to IEEE Access for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian networks are powerful statistical models to study the probabilistic\nrelationships among set random variables with major applications in disease\nmodeling and prediction. Here, we propose a continuous time Bayesian network\nwith conditional dependencies, represented as Poisson regression, to model the\nimpact of exogenous variables on the conditional dependencies of the network.\nWe also propose an adaptive regularization method with an intuitive early\nstopping feature based on density based clustering for efficient learning of\nthe structure and parameters of the proposed network. Using a dataset of\npatients with multiple chronic conditions extracted from electronic health\nrecords of the Department of Veterans Affairs we compare the performance of the\nproposed approach with some of the existing methods in the literature for both\nshort-term (one-year ahead) and long-term (multi-year ahead) predictions. The\nproposed approach provides a sparse intuitive representation of the complex\nfunctional relationships between multiple chronic conditions. It also provides\nthe capability of analyzing multiple disease trajectories over time given any\ncombination of prior conditions.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 05:02:34 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 21:03:44 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Faruqui", "Syed Hasib Akhter", ""], ["Alaeddini", "Adel", ""], ["Wang", "Jing", ""], ["Jaramillo", "Carlos A.", ""]]}, {"id": "2007.15857", "submitter": "Yichen Shen", "authors": "Yichen Shen, Zhilu Zhang, Mert R. Sabuncu, Lin Sun", "title": "Real-Time Uncertainty Estimation in Computer Vision via\n  Uncertainty-Aware Distribution Distillation", "comments": "Accepted at IEEE Winter Conference on Applications of Computer Vision\n  (WACV), 2021; Equal contribution: Yichen Shen, Zhilu Zhang", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calibrated estimates of uncertainty are critical for many real-world computer\nvision applications of deep learning. While there are several widely-used\nuncertainty estimation methods, dropout inference stands out for its simplicity\nand efficacy. This technique, however, requires multiple forward passes through\nthe network during inference and therefore can be too resource-intensive to be\ndeployed in real-time applications. We propose a simple, easy-to-optimize\ndistillation method for learning the conditional predictive distribution of a\npre-trained dropout model for fast, sample-free uncertainty estimation in\ncomputer vision tasks. We empirically test the effectiveness of the proposed\nmethod on both semantic segmentation and depth estimation tasks and demonstrate\nour method can significantly reduce the inference time, enabling real-time\nuncertainty quantification, while achieving improved quality of both the\nuncertainty estimates and predictive performance over the regular dropout\nmodel.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 05:40:39 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 03:52:54 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Shen", "Yichen", ""], ["Zhang", "Zhilu", ""], ["Sabuncu", "Mert R.", ""], ["Sun", "Lin", ""]]}, {"id": "2007.15859", "submitter": "Yongbin Gu", "authors": "Pengcheng Li, Yongbin Gu", "title": "Learning Forward Reuse Distance", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Caching techniques are widely used in the era of cloud computing from\napplications, such as Web caches to infrastructures, Memcached and memory\ncaches in computer architectures. Prediction of cached data can greatly help\nimprove cache management and performance. The recent advancement of deep\nlearning techniques enables the design of novel intelligent cache replacement\npolicies. In this work, we propose a learning-aided approach to predict future\ndata accesses. We find that a powerful LSTM-based recurrent neural network\nmodel can provide high prediction accuracy based on only a cache trace as\ninput. The high accuracy results from a carefully crafted locality-driven\nfeature design. Inspired by the high prediction accuracy, we propose a pseudo\nOPT policy and evaluate it upon 13 real-world storage workloads from Microsoft\nResearch. Results demonstrate that the new cache policy improves state-of-art\npractical policies by up to 19.2% and incurs only 2.3% higher miss ratio than\nOPT on average.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 05:57:50 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Li", "Pengcheng", ""], ["Gu", "Yongbin", ""]]}, {"id": "2007.15884", "submitter": "Johannes Schmidt-Hieber", "authors": "Johannes Schmidt-Hieber", "title": "The Kolmogorov-Arnold representation theorem revisited", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a longstanding debate whether the Kolmogorov-Arnold representation\ntheorem can explain the use of more than one hidden layer in neural networks.\nThe Kolmogorov-Arnold representation decomposes a multivariate function into an\ninterior and an outer function and therefore has indeed a similar structure as\na neural network with two hidden layers. But there are distinctive differences.\nOne of the main obstacles is that the outer function depends on the represented\nfunction and can be wildly varying even if the represented function is smooth.\nWe derive modifications of the Kolmogorov-Arnold representation that transfer\nsmoothness properties of the represented function to the outer function and can\nbe well approximated by ReLU networks. It appears that instead of two hidden\nlayers, a more natural interpretation of the Kolmogorov-Arnold representation\nis that of a deep neural network where most of the layers are required to\napproximate the interior function.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 07:41:09 GMT"}, {"version": "v2", "created": "Sat, 2 Jan 2021 16:42:55 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Schmidt-Hieber", "Johannes", ""]]}, {"id": "2007.15890", "submitter": "Taisuke Kobayashi", "authors": "Taisuke Kobayashi", "title": "Towards Deep Robot Learning with Optimizer applicable to Non-stationary\n  Problems", "comments": "6 pages, 4 figures", "journal-ref": "IEEE/SICE International Symposium on System Integration, 2021", "doi": "10.1109/IEEECONF49454.2021.9382621", "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new optimizer for deep learning, named d-AmsGrad. In\nthe real-world data, noise and outliers cannot be excluded from dataset to be\nused for learning robot skills. This problem is especially striking for robots\nthat learn by collecting data in real time, which cannot be sorted manually.\nSeveral noise-robust optimizers have therefore been developed to resolve this\nproblem, and one of them, named AmsGrad, which is a variant of Adam optimizer,\nhas a proof of its convergence. However, in practice, it does not improve\nlearning performance in robotics scenarios. This reason is hypothesized that\nmost of robot learning problems are non-stationary, but AmsGrad assumes the\nmaximum second momentum during learning to be stationarily given. In order to\nadapt to the non-stationary problems, an improved version, which slowly decays\nthe maximum second momentum, is proposed. The proposed optimizer has the same\ncapability of reaching the global optimum as baselines, and its performance\noutperformed that of the baselines in robotics problems.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 07:55:09 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Kobayashi", "Taisuke", ""]]}, {"id": "2007.15911", "submitter": "Aniek Markus", "authors": "Aniek F. Markus, Jan A. Kors, Peter R. Rijnbeek", "title": "The role of explainability in creating trustworthy artificial\n  intelligence for health care: a comprehensive survey of the terminology,\n  design choices, and evaluation strategies", "comments": null, "journal-ref": "Journal of Biomedical Informatics, 113 (2021), 103655", "doi": "10.1016/j.jbi.2020.103655", "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artificial intelligence (AI) has huge potential to improve the health and\nwell-being of people, but adoption in clinical practice is still limited. Lack\nof transparency is identified as one of the main barriers to implementation, as\nclinicians should be confident the AI system can be trusted. Explainable AI has\nthe potential to overcome this issue and can be a step towards trustworthy AI.\nIn this paper we review the recent literature to provide guidance to\nresearchers and practitioners on the design of explainable AI systems for the\nhealth-care domain and contribute to formalization of the field of explainable\nAI. We argue the reason to demand explainability determines what should be\nexplained as this determines the relative importance of the properties of\nexplainability (i.e. interpretability and fidelity). Based on this, we propose\na framework to guide the choice between classes of explainable AI methods\n(explainable modelling versus post-hoc explanation; model-based,\nattribution-based, or example-based explanations; global and local\nexplanations). Furthermore, we find that quantitative evaluation metrics, which\nare important for objective standardized evaluation, are still lacking for some\nproperties (e.g. clarity) and types of explanations (e.g. example-based\nmethods). We conclude that explainable modelling can contribute to trustworthy\nAI, but the benefits of explainability still need to be proven in practice and\ncomplementary measures might be needed to create trustworthy AI in health care\n(e.g. reporting data quality, performing extensive (external) validation, and\nregulation).\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 09:08:27 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 08:32:38 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Markus", "Aniek F.", ""], ["Kors", "Jan A.", ""], ["Rijnbeek", "Peter R.", ""]]}, {"id": "2007.15930", "submitter": "Ryan Martin", "authors": "Yue Yang and Ryan Martin", "title": "Variational approximations of empirical Bayes posteriors in\n  high-dimensional linear models", "comments": "30 pages, 1 figure, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high-dimensions, the prior tails can have a significant effect on both\nposterior computation and asymptotic concentration rates. To achieve optimal\nrates while keeping the posterior computations relatively simple, an empirical\nBayes approach has recently been proposed, featuring thin-tailed conjugate\npriors with data-driven centers. While conjugate priors ease some of the\ncomputational burden, Markov chain Monte Carlo methods are still needed, which\ncan be expensive when dimension is high. In this paper, we develop a\nvariational approximation to the empirical Bayes posterior that is fast to\ncompute and retains the optimal concentration rate properties of the original.\nIn simulations, our method is shown to have superior performance compared to\nexisting variational approximations in the literature across a wide range of\nhigh-dimensional settings.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 09:50:01 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Yang", "Yue", ""], ["Martin", "Ryan", ""]]}, {"id": "2007.15951", "submitter": "Brian Kenji Iwana", "authors": "Brian Kenji Iwana, Seiichi Uchida", "title": "An Empirical Survey of Data Augmentation for Time Series Classification\n  with Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent times, deep artificial neural networks have achieved many successes\nin pattern recognition. Part of this success can be attributed to the reliance\non big data to increase generalization. However, in the field of time series\nrecognition, many datasets are often very small. One method of addressing this\nproblem is through the use of data augmentation. In this paper, we survey data\naugmentation techniques for time series and their application to time series\nclassification with neural networks. We propose a taxonomy and outline the four\nfamilies in time series data augmentation, including transformation-based\nmethods, pattern mixing, generative models, and decomposition methods.\nFurthermore, we empirically evaluate 12 time series data augmentation methods\non 128 time series classification datasets with six different types of neural\nnetworks. Through the results, we are able to analyze the characteristics,\nadvantages and disadvantages, and recommendations of each data augmentation\nmethod. This survey aims to help in the selection of time series data\naugmentation for neural network applications.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 10:33:54 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 09:58:48 GMT"}, {"version": "v3", "created": "Mon, 24 May 2021 07:40:30 GMT"}, {"version": "v4", "created": "Fri, 2 Jul 2021 09:15:08 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Iwana", "Brian Kenji", ""], ["Uchida", "Seiichi", ""]]}, {"id": "2007.15973", "submitter": "Xiaoxian Guo", "authors": "Xiaoxian Guo and Xiantao Zhang and Xinliang Tian and Xin Li and Wenyue\n  Lu", "title": "Predicting heave and surge motions of a semi-submersible with neural\n  networks", "comments": "16 pages, 22 figures, submitted to Applied Ocean Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time motion prediction of a vessel or a floating platform can help to\nimprove the performance of motion compensation systems. It can also provide\nuseful early-warning information for offshore operations that are critical with\nregard to motion. In this study, a long short-term memory (LSTM) -based machine\nlearning model was developed to predict heave and surge motions of a\nsemi-submersible. The training and test data came from a model test carried out\nin the deep-water ocean basin, at Shanghai Jiao Tong University, China. The\nmotion and measured waves were fed into LSTM cells and then went through serval\nfully connected (FC) layers to obtain the prediction. With the help of measured\nwaves, the prediction extended 46.5 s into future with an average accuracy\nclose to 90%. Using a noise-extended dataset, the trained model effectively\nworked with a noise level up to 0.8. As a further step, the model could predict\nmotions only based on the motion itself. Based on sensitive studies on the\narchitectures of the model, guidelines for the construction of the machine\nlearning model are proposed. The proposed LSTM model shows a strong ability to\npredict vessel wave-excited motions.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 11:24:46 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Guo", "Xiaoxian", ""], ["Zhang", "Xiantao", ""], ["Tian", "Xinliang", ""], ["Li", "Xin", ""], ["Lu", "Wenyue", ""]]}, {"id": "2007.16000", "submitter": "Dom Huh", "authors": "Dom Huh", "title": "Hierarchical BiGraph Neural Network as Recommendation Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks emerge as a promising modeling method for applications\ndealing with datasets that are best represented in the graph domain. In\nspecific, developing recommendation systems often require addressing sparse\nstructured data which often lacks the feature richness in either the user\nand/or item side and requires processing within the correct context for optimal\nperformance. These datasets intuitively can be mapped to and represented as\nnetworks or graphs. In this paper, we propose the Hierarchical BiGraph Neural\nNetwork (HBGNN), a hierarchical approach of using GNNs as recommendation\nsystems and structuring the user-item features using a bigraph framework. Our\nexperimental results show competitive performance with current recommendation\nsystem methods and transferability.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 18:01:41 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Huh", "Dom", ""]]}, {"id": "2007.16010", "submitter": "Subhadip Maji", "authors": "Subhadip Maji, Arijit Ghosh Chowdhury, Raghav Bali and Vamsi M\n  Bhandaru", "title": "Exclusion and Inclusion -- A model agnostic approach to feature\n  importance in DNNs", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks in NLP have enabled systems to learn complex non-linear\nrelationships. One of the major bottlenecks towards being able to use DNNs for\nreal world applications is their characterization as black boxes. To solve this\nproblem, we introduce a model agnostic algorithm which calculates phrase-wise\nimportance of input features. We contend that our method is generalizable to a\ndiverse set of tasks, by carrying out experiments for both Regression and\nClassification. We also observe that our approach is robust to outliers,\nimplying that it only captures the essential aspects of the input.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 07:50:53 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Maji", "Subhadip", ""], ["Chowdhury", "Arijit Ghosh", ""], ["Bali", "Raghav", ""], ["Bhandaru", "Vamsi M", ""]]}, {"id": "2007.16012", "submitter": "Josh Payne", "authors": "Josh Payne, Mario Srouji, Dian Ang Yap, Vineet Kosaraju", "title": "BERT Learns (and Teaches) Chemistry", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern computational organic chemistry is becoming increasingly data-driven.\nThere remain a large number of important unsolved problems in this area such as\nproduct prediction given reactants, drug discovery, and metric-optimized\nmolecule synthesis, but efforts to solve these problems using machine learning\nhave also increased in recent years. In this work, we propose the use of\nattention to study functional groups and other property-impacting molecular\nsubstructures from a data-driven perspective, using a transformer-based model\n(BERT) on datasets of string representations of molecules and analyzing the\nbehavior of its attention heads. We then apply the representations of\nfunctional groups and atoms learned by the model to tackle problems of\ntoxicity, solubility, drug-likeness, and synthesis accessibility on smaller\ndatasets using the learned representations as features for graph convolution\nand attention models on the graph structure of molecules, as well as\nfine-tuning of BERT. Finally, we propose the use of attention visualization as\na helpful tool for chemistry practitioners and students to quickly identify\nimportant substructures in various chemical properties.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 00:23:07 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Payne", "Josh", ""], ["Srouji", "Mario", ""], ["Yap", "Dian Ang", ""], ["Kosaraju", "Vineet", ""]]}, {"id": "2007.16013", "submitter": "Denis Filimonov", "authors": "Denis Filimonov, Ravi Teja Gadde, Ariya Rastrow", "title": "Neural Composition: Learning to Generate from Multiple Models", "comments": "Self-Supervised Learning for Speech and Audio Processing Workshop @\n  NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decomposing models into multiple components is critically important in many\napplications such as language modeling (LM) as it enables adapting individual\ncomponents separately and biasing of some components to the user's personal\npreferences. Conventionally, contextual and personalized adaptation for\nlanguage models, are achieved through class-based factorization, which requires\nclass-annotated data, or through biasing to individual phrases which is limited\nin scale. In this paper, we propose a system that combines model-defined\ncomponents, by learning when to activate the generation process from each\nindividual component, and how to combine probability distributions from each\ncomponent, directly from unlabeled text data.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 22:58:53 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 23:41:47 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Filimonov", "Denis", ""], ["Gadde", "Ravi Teja", ""], ["Rastrow", "Ariya", ""]]}, {"id": "2007.16030", "submitter": "Farbod Taymouri", "authors": "Farbod Taymouri, Marcello La Rosa", "title": "Encoder-Decoder Generative Adversarial Nets for Suffix Generation and\n  Remaining Time Prediction of Business Process Models", "comments": "arXiv admin note: substantial text overlap with arXiv:2003.11268", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an encoder-decoder architecture grounded on Generative\nAdversarial Networks (GANs), that generates a sequence of activities and their\ntimestamps in an end-to-end way. GANs work well with differentiable data such\nas images. However, a suffix is a sequence of categorical items. To this end,\nwe use the Gumbel-Softmax distribution to get a differentiable continuous\napproximation. The training works by putting one neural network against the\nother in a two-player game (hence the \"adversarial\" nature), which leads to\ngenerating suffixes close to the ground truth. From the experimental evaluation\nit emerges that the approach is superior to the baselines in terms of the\naccuracy of the predicted suffixes and corresponding remaining times, despite\nusing a naive feature encoding and only engineering features based on control\nflow and events completion time.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 03:39:17 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 09:49:24 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Taymouri", "Farbod", ""], ["La Rosa", "Marcello", ""]]}, {"id": "2007.16041", "submitter": "Usman Mahmood", "authors": "Usman Mahmood, Md Mahfuzur Rahman, Alex Fedorov, Noah Lewis, Zening\n  Fu, Vince D. Calhoun, Sergey M. Plis", "title": "Whole MILC: generalizing learned dynamics across tasks, datasets, and\n  populations", "comments": "Accepted at MICCAI 2020. arXiv admin note: substantial text overlap\n  with arXiv:1912.03130", "journal-ref": null, "doi": "10.1007/978-3-030-59728-3_40", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Behavioral changes are the earliest signs of a mental disorder, but arguably,\nthe dynamics of brain function gets affected even earlier. Subsequently,\nspatio-temporal structure of disorder-specific dynamics is crucial for early\ndiagnosis and understanding the disorder mechanism. A common way of learning\ndiscriminatory features relies on training a classifier and evaluating feature\nimportance. Classical classifiers, based on handcrafted features are quite\npowerful, but suffer the curse of dimensionality when applied to large input\ndimensions of spatio-temporal data. Deep learning algorithms could handle the\nproblem and a model introspection could highlight discriminatory\nspatio-temporal regions but need way more samples to train. In this paper we\npresent a novel self supervised training schema which reinforces whole sequence\nmutual information local to context (whole MILC). We pre-train the whole MILC\nmodel on unlabeled and unrelated healthy control data. We test our model on\nthree different disorders (i) Schizophrenia (ii) Autism and (iii) Alzheimers\nand four different studies. Our algorithm outperforms existing self-supervised\npre-training methods and provides competitive classification results to\nclassical machine learning algorithms. Importantly, whole MILC enables\nattribution of subject diagnosis to specific spatio-temporal regions in the\nfMRI signal.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 21:07:30 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 20:12:38 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Mahmood", "Usman", ""], ["Rahman", "Md Mahfuzur", ""], ["Fedorov", "Alex", ""], ["Lewis", "Noah", ""], ["Fu", "Zening", ""], ["Calhoun", "Vince D.", ""], ["Plis", "Sergey M.", ""]]}, {"id": "2007.16054", "submitter": "Francesco Cricri", "authors": "Nannan Zou and Honglei Zhang and Francesco Cricri and Hamed R.\n  Tavakoli and Jani Lainema and Miska Hannuksela and Emre Aksu and Esa Rahtu", "title": "Learning to Learn to Compress", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an end-to-end meta-learned system for image\ncompression. Traditional machine learning based approaches to image compression\ntrain one or more neural network for generalization performance. However, at\ninference time, the encoder or the latent tensor output by the encoder can be\noptimized for each test image. This optimization can be regarded as a form of\nadaptation or benevolent overfitting to the input content. In order to reduce\nthe gap between training and inference conditions, we propose a new training\nparadigm for learned image compression, which is based on meta-learning. In a\nfirst phase, the neural networks are trained normally. In a second phase, the\nModel-Agnostic Meta-learning approach is adapted to the specific case of image\ncompression, where the inner-loop performs latent tensor overfitting, and the\nouter loop updates both encoder and decoder neural networks based on the\noverfitting performance. Furthermore, after meta-learning, we propose to\noverfit and cluster the bias terms of the decoder on training image patches, so\nthat at inference time the optimal content-specific bias terms can be selected\nat encoder-side. Finally, we propose a new probability model for lossless\ncompression, which combines concepts from both multi-scale and super-resolution\nprobability model approaches. We show the benefits of all our proposed ideas\nvia carefully designed experiments.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 13:13:53 GMT"}, {"version": "v2", "created": "Sat, 1 May 2021 16:18:46 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Zou", "Nannan", ""], ["Zhang", "Honglei", ""], ["Cricri", "Francesco", ""], ["Tavakoli", "Hamed R.", ""], ["Lainema", "Jani", ""], ["Hannuksela", "Miska", ""], ["Aksu", "Emre", ""], ["Rahtu", "Esa", ""]]}, {"id": "2007.16056", "submitter": "Effrosyni Simou", "authors": "Effrosyni Simou, Dorina Thanou and Pascal Frossard", "title": "node2coords: Graph Representation Learning with Wasserstein Barycenters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to perform network analysis tasks, representations that capture the\nmost relevant information in the graph structure are needed. However, existing\nmethods do not learn representations that can be interpreted in a\nstraightforward way and that are robust to perturbations to the graph\nstructure. In this work, we address these two limitations by proposing\nnode2coords, a representation learning algorithm for graphs, which learns\nsimultaneously a low-dimensional space and coordinates for the nodes in that\nspace. The patterns that span the low dimensional space reveal the graph's most\nimportant structural information. The coordinates of the nodes reveal the\nproximity of their local structure to the graph structural patterns. In order\nto measure this proximity by taking into account the underlying graph, we\npropose to use Wasserstein distances. We introduce an autoencoder that employs\na linear layer in the encoder and a novel Wasserstein barycentric layer at the\ndecoder. Node connectivity descriptors, that capture the local structure of the\nnodes, are passed through the encoder to learn the small set of graph\nstructural patterns. In the decoder, the node connectivity descriptors are\nreconstructed as Wasserstein barycenters of the graph structural patterns. The\noptimal weights for the barycenter representation of a node's connectivity\ndescriptor correspond to the coordinates of that node in the low-dimensional\nspace. Experimental results demonstrate that the representations learned with\nnode2coords are interpretable, lead to node embeddings that are stable to\nperturbations of the graph structure and achieve competitive or superior\nresults compared to state-of-the-art methods in node classification.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 13:14:25 GMT"}, {"version": "v2", "created": "Sun, 3 Jan 2021 18:17:47 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Simou", "Effrosyni", ""], ["Thanou", "Dorina", ""], ["Frossard", "Pascal", ""]]}, {"id": "2007.16061", "submitter": "Xiaowen Dong", "authors": "Xiaowen Dong, Dorina Thanou, Laura Toni, Michael Bronstein, Pascal\n  Frossard", "title": "Graph signal processing for machine learning: A review and new\n  perspectives", "comments": null, "journal-ref": null, "doi": "10.1109/MSP.2020.3014591", "report-no": null, "categories": "cs.LG cs.SI eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effective representation, processing, analysis, and visualization of\nlarge-scale structured data, especially those related to complex domains such\nas networks and graphs, are one of the key questions in modern machine\nlearning. Graph signal processing (GSP), a vibrant branch of signal processing\nmodels and algorithms that aims at handling data supported on graphs, opens new\npaths of research to address this challenge. In this article, we review a few\nimportant contributions made by GSP concepts and tools, such as graph filters\nand transforms, to the development of novel machine learning algorithms. In\nparticular, our discussion focuses on the following three aspects: exploiting\ndata structure and relational priors, improving data and computational\nefficiency, and enhancing model interpretability. Furthermore, we provide new\nperspectives on future development of GSP techniques that may serve as a bridge\nbetween applied mathematics and signal processing on one side, and machine\nlearning and network science on the other. Cross-fertilization across these\ndifferent disciplines may help unlock the numerous challenges of complex data\nanalysis in the modern age.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 13:21:33 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Dong", "Xiaowen", ""], ["Thanou", "Dorina", ""], ["Toni", "Laura", ""], ["Bronstein", "Michael", ""], ["Frossard", "Pascal", ""]]}, {"id": "2007.16103", "submitter": "Yinghuan Shi", "authors": "Yinghuan Shi and Wanqi Yang and Kim-Han Thung and Hao Wang and Yang\n  Gao and Yang Pan and Li Zhang and Dinggang Shen", "title": "Learning-based Computer-aided Prescription Model for Parkinson's\n  Disease: A Data-driven Perspective", "comments": "IEEE JBHI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a novel problem: \"automatic prescription\nrecommendation for PD patients.\" To realize this goal, we first build a dataset\nby collecting 1) symptoms of PD patients, and 2) their prescription drug\nprovided by neurologists. Then, we build a novel computer-aided prescription\nmodel by learning the relation between observed symptoms and prescription drug.\nFinally, for the new coming patients, we could recommend (predict) suitable\nprescription drug on their observed symptoms by our prescription model. From\nthe methodology part, our proposed model, namely Prescription viA Learning\nlAtent Symptoms (PALAS), could recommend prescription using the multi-modality\nrepresentation of the data. In PALAS, a latent symptom space is learned to\nbetter model the relationship between symptoms and prescription drug, as there\nis a large semantic gap between them. Moreover, we present an efficient\nalternating optimization method for PALAS. We evaluated our method using the\ndata collected from 136 PD patients at Nanjing Brain Hospital, which can be\nregarded as a large dataset in PD research community. The experimental results\ndemonstrate the effectiveness and clinical potential of our method in this\nrecommendation task, if compared with other competing methods.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 14:34:35 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Shi", "Yinghuan", ""], ["Yang", "Wanqi", ""], ["Thung", "Kim-Han", ""], ["Wang", "Hao", ""], ["Gao", "Yang", ""], ["Pan", "Yang", ""], ["Zhang", "Li", ""], ["Shen", "Dinggang", ""]]}, {"id": "2007.16104", "submitter": "Hubert Banville", "authors": "Hubert Banville, Omar Chehab, Aapo Hyv\\\"arinen, Denis-Alexander\n  Engemann, Alexandre Gramfort", "title": "Uncovering the structure of clinical EEG signals with self-supervised\n  learning", "comments": "32 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective. Supervised learning paradigms are often limited by the amount of\nlabeled data that is available. This phenomenon is particularly problematic in\nclinically-relevant data, such as electroencephalography (EEG), where labeling\ncan be costly in terms of specialized expertise and human processing time.\nConsequently, deep learning architectures designed to learn on EEG data have\nyielded relatively shallow models and performances at best similar to those of\ntraditional feature-based approaches. However, in most situations, unlabeled\ndata is available in abundance. By extracting information from this unlabeled\ndata, it might be possible to reach competitive performance with deep neural\nnetworks despite limited access to labels. Approach. We investigated\nself-supervised learning (SSL), a promising technique for discovering structure\nin unlabeled data, to learn representations of EEG signals. Specifically, we\nexplored two tasks based on temporal context prediction as well as contrastive\npredictive coding on two clinically-relevant problems: EEG-based sleep staging\nand pathology detection. We conducted experiments on two large public datasets\nwith thousands of recordings and performed baseline comparisons with purely\nsupervised and hand-engineered approaches. Main results. Linear classifiers\ntrained on SSL-learned features consistently outperformed purely supervised\ndeep neural networks in low-labeled data regimes while reaching competitive\nperformance when all labels were available. Additionally, the embeddings\nlearned with each method revealed clear latent structures related to\nphysiological and clinical phenomena, such as age effects. Significance. We\ndemonstrate the benefit of self-supervised learning approaches on EEG data. Our\nresults suggest that SSL may pave the way to a wider use of deep learning\nmodels on EEG data.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 14:34:47 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Banville", "Hubert", ""], ["Chehab", "Omar", ""], ["Hyv\u00e4rinen", "Aapo", ""], ["Engemann", "Denis-Alexander", ""], ["Gramfort", "Alexandre", ""]]}, {"id": "2007.16109", "submitter": "Parijat Dube", "authors": "Samuel Ackerman, Parijat Dube, Eitan Farchi", "title": "Sequential Drift Detection in Deep Learning Classifiers", "comments": "11 pages + appendix, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We utilize neural network embeddings to detect data drift by formulating the\ndrift detection within an appropriate sequential decision framework. This\nenables control of the false alarm rate although the statistical tests are\nrepeatedly applied. Since change detection algorithms naturally face a tradeoff\nbetween avoiding false alarms and quick correct detection, we introduce a loss\nfunction which evaluates an algorithm's ability to balance these two concerns,\nand we use it in a series of experiments.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 14:46:21 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Ackerman", "Samuel", ""], ["Dube", "Parijat", ""], ["Farchi", "Eitan", ""]]}, {"id": "2007.16112", "submitter": "Zhiwu Huang", "authors": "Yan Wu, Aoming Liu, Zhiwu Huang, Siwei Zhang, Luc Van Gool", "title": "Neural Architecture Search as Sparse Supernet", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at enlarging the problem of Neural Architecture Search (NAS)\nfrom Single-Path and Multi-Path Search to automated Mixed-Path Search. In\nparticular, we model the NAS problem as a sparse supernet using a new\ncontinuous architecture representation with a mixture of sparsity constraints.\nThe sparse supernet enables us to automatically achieve sparsely-mixed paths\nupon a compact set of nodes. To optimize the proposed sparse supernet, we\nexploit a hierarchical accelerated proximal gradient algorithm within a\nbi-level optimization framework. Extensive experiments on Convolutional Neural\nNetwork and Recurrent Neural Network search demonstrate that the proposed\nmethod is capable of searching for compact, general and powerful neural\narchitectures.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 14:51:52 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 16:35:16 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Wu", "Yan", ""], ["Liu", "Aoming", ""], ["Huang", "Zhiwu", ""], ["Zhang", "Siwei", ""], ["Van Gool", "Luc", ""]]}, {"id": "2007.16149", "submitter": "Vasco Lopes Ferrinho", "authors": "Vasco Lopes and Lu\\'is A. Alexandre", "title": "HMCNAS: Neural Architecture Search using Hidden Markov Chains and\n  Bayesian Optimization", "comments": "9 pages, 1 figure, 2 tables, neural architecture search, macro-search", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search has achieved state-of-the-art performance in a\nvariety of tasks, out-performing human-designed networks. However, many\nassumptions, that require human definition, related with the problems being\nsolved or the models generated are still needed: final model architectures,\nnumber of layers to be sampled, forced operations, small search spaces, which\nultimately contributes to having models with higher performances at the cost of\ninducing bias into the system. In this paper, we propose HMCNAS, which is\ncomposed of two novel components: i) a method that leverages information about\nhuman-designed models to autonomously generate a complex search space, and ii)\nan Evolutionary Algorithm with Bayesian Optimization that is capable of\ngenerating competitive CNNs from scratch, without relying on human-defined\nparameters or small search spaces. The experimental results show that the\nproposed approach results in competitive architectures obtained in a very short\ntime. HMCNAS provides a step towards generalizing NAS, by providing a way to\ncreate competitive models, without requiring any human knowledge about the\nspecific task.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 16:04:08 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Lopes", "Vasco", ""], ["Alexandre", "Lu\u00eds A.", ""]]}, {"id": "2007.16170", "submitter": "Philippe Esling", "authors": "Philippe Esling, Ninon Devis, Adrien Bitton, Antoine Caillon, Axel\n  Chemla--Romeu-Santos, Constance Douwes", "title": "Diet deep generative audio models with structured lottery", "comments": "8 pages, 5 figures. Proceedings of the 23rd International Conference\n  on Digital Audio Effects (DAFx-20), Vienna, Austria, September 8-12, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MM cs.SD eess.AS stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning models have provided extremely successful solutions in most\naudio application fields. However, the high accuracy of these models comes at\nthe expense of a tremendous computation cost. This aspect is almost always\noverlooked in evaluating the quality of proposed models. However, models should\nnot be evaluated without taking into account their complexity. This aspect is\nespecially critical in audio applications, which heavily relies on specialized\nembedded hardware with real-time constraints. In this paper, we build on recent\nobservations that deep models are highly overparameterized, by studying the\nlottery ticket hypothesis on deep generative audio models. This hypothesis\nstates that extremely efficient small sub-networks exist in deep models and\nwould provide higher accuracy than larger models if trained in isolation.\nHowever, lottery tickets are found by relying on unstructured masking, which\nmeans that resulting models do not provide any gain in either disk size or\ninference time. Instead, we develop here a method aimed at performing\nstructured trimming. We show that this requires to rely on global selection and\nintroduce a specific criterion based on mutual information. First, we confirm\nthe surprising result that smaller models provide higher accuracy than their\nlarge counterparts. We further show that we can remove up to 95% of the model\nweights without significant degradation in accuracy. Hence, we can obtain very\nlight models for generative audio across popular methods such as Wavenet, SING\nor DDSP, that are up to 100 times smaller with commensurate accuracy. We study\nthe theoretical bounds for embedding these models on Raspberry Pi and Arduino,\nand show that we can obtain generative models on CPU with equivalent quality as\nlarge GPU models. Finally, we discuss the possibility of implementing deep\ngenerative audio models on embedded platforms.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 16:43:10 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Esling", "Philippe", ""], ["Devis", "Ninon", ""], ["Bitton", "Adrien", ""], ["Caillon", "Antoine", ""], ["Chemla--Romeu-Santos", "Axel", ""], ["Douwes", "Constance", ""]]}, {"id": "2007.16173", "submitter": "Taher Hekmatfar", "authors": "Taher Hekmatfar, Saman Haratizadeh, Sama Goliaei", "title": "Embedding Ranking-Oriented Recommender System Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based recommender systems (GRSs) analyze the structural information in\nthe graphical representation of data to make better recommendations, especially\nwhen the direct user-item relation data is sparse. Ranking-oriented GRSs that\nform a major class of recommendation systems, mostly use the graphical\nrepresentation of preference (or rank) data for measuring node similarities,\nfrom which they can infer a recommendation list using a neighborhood-based\nmechanism. In this paper, we propose PGRec, a novel graph-based\nranking-oriented recommendation framework. PGRec models the preferences of the\nusers over items, by a novel graph structure called PrefGraph. This graph is\nthen exploited by an improved embedding approach, taking advantage of both\nfactorization and deep learning methods, to extract vectors representing users,\nitems, and preferences. The resulting embedding are then used for predicting\nusers' unknown pairwise preferences from which the final recommendation lists\nare inferred. We have evaluated the performance of the proposed method against\nthe state of the art model-based and neighborhood-based recommendation methods,\nand our experiments show that PGRec outperforms the baseline algorithms up to\n3.2% in terms of NDCG@10 in different MovieLens datasets.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 16:56:54 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Hekmatfar", "Taher", ""], ["Haratizadeh", "Saman", ""], ["Goliaei", "Sama", ""]]}, {"id": "2007.16187", "submitter": "Philippe Esling", "authors": "Philippe Esling, Theis Bazin, Adrien Bitton, Tristan Carsault, Ninon\n  Devis", "title": "Ultra-light deep MIR by trimming lottery tickets", "comments": "8 pages, 2 figures. 21st International Society for Music Information\n  Retrieval Conference 11-15 October 2020, Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.MM cs.SD eess.AS stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Current state-of-the-art results in Music Information Retrieval are largely\ndominated by deep learning approaches. These provide unprecedented accuracy\nacross all tasks. However, the consistently overlooked downside of these models\nis their stunningly massive complexity, which seems concomitantly crucial to\ntheir success. In this paper, we address this issue by proposing a model\npruning method based on the lottery ticket hypothesis. We modify the original\napproach to allow for explicitly removing parameters, through structured\ntrimming of entire units, instead of simply masking individual weights. This\nleads to models which are effectively lighter in terms of size, memory and\nnumber of operations. We show that our proposal can remove up to 90% of the\nmodel parameters without loss of accuracy, leading to ultra-light deep MIR\nmodels. We confirm the surprising result that, at smaller compression ratios\n(removing up to 85% of a network), lighter models consistently outperform their\nheavier counterparts. We exhibit these results on a large array of MIR tasks\nincluding audio classification, pitch recognition, chord extraction, drum\ntranscription and onset estimation. The resulting ultra-light deep learning\nmodels for MIR can run on CPU, and can even fit on embedded devices with\nminimal degradation of accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 17:30:28 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Esling", "Philippe", ""], ["Bazin", "Theis", ""], ["Bitton", "Adrien", ""], ["Carsault", "Tristan", ""], ["Devis", "Ninon", ""]]}, {"id": "2007.16204", "submitter": "Brian Kim", "authors": "Brian Kim and Yalin E. Sagduyu and Tugba Erpek and Kemal Davaslioglu\n  and Sennur Ulukus", "title": "Adversarial Attacks with Multiple Antennas Against Deep Learning-Based\n  Modulation Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a wireless communication system, where a transmitter sends\nsignals to a receiver with different modulation types while the receiver\nclassifies the modulation types of the received signals using its deep\nlearning-based classifier. Concurrently, an adversary transmits adversarial\nperturbations using its multiple antennas to fool the classifier into\nmisclassifying the received signals. From the adversarial machine learning\nperspective, we show how to utilize multiple antennas at the adversary to\nimprove the adversarial (evasion) attack performance. Two main points are\nconsidered while exploiting the multiple antennas at the adversary, namely the\npower allocation among antennas and the utilization of channel diversity.\nFirst, we show that multiple independent adversaries, each with a single\nantenna cannot improve the attack performance compared to a single adversary\nwith multiple antennas using the same total power. Then, we consider various\nways to allocate power among multiple antennas at a single adversary such as\nallocating power to only one antenna, and proportional or inversely\nproportional to the channel gain. By utilizing channel diversity, we introduce\nan attack to transmit the adversarial perturbation through the channel with the\nlargest channel gain at the symbol level. We show that this attack reduces the\nclassifier accuracy significantly compared to other attacks under different\nchannel conditions in terms of channel variance and channel correlation across\nantennas. Also, we show that the attack success improves significantly as the\nnumber of antennas increases at the adversary that can better utilize channel\ndiversity to craft adversarial attacks.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 17:56:50 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Kim", "Brian", ""], ["Sagduyu", "Yalin E.", ""], ["Erpek", "Tugba", ""], ["Davaslioglu", "Kemal", ""], ["Ulukus", "Sennur", ""]]}, {"id": "2007.16208", "submitter": "Flora D. Salim", "authors": "Kyle K. Qin, Flora D. Salim, Yongli Ren, Wei Shao, Mark Heimann, Danai\n  Koutra", "title": "G-CREWE: Graph CompREssion With Embedding for Network Alignment", "comments": "10 pages, accepted at the 29th ACM International Conference\n  onInformation and Knowledge Management (CIKM 20)", "journal-ref": null, "doi": "10.1145/3340531.3411924", "report-no": null, "categories": "cs.SI cs.DB cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network alignment is useful for multiple applications that require\nincreasingly large graphs to be processed. Existing research approaches this as\nan optimization problem or computes the similarity based on node\nrepresentations. However, the process of aligning every pair of nodes between\nrelatively large networks is time-consuming and resource-intensive. In this\npaper, we propose a framework, called G-CREWE (Graph CompREssion With\nEmbedding) to solve the network alignment problem. G-CREWE uses node embeddings\nto align the networks on two levels of resolution, a fine resolution given by\nthe original network and a coarse resolution given by a compressed version, to\nachieve an efficient and effective network alignment. The framework first\nextracts node features and learns the node embedding via a Graph Convolutional\nNetwork (GCN). Then, node embedding helps to guide the process of graph\ncompression and finally improve the alignment performance. As part of G-CREWE,\nwe also propose a new compression mechanism called MERGE (Minimum dEgRee\nneiGhbors comprEssion) to reduce the size of the input networks while\npreserving the consistency in their topological structure. Experiments on all\nreal networks show that our method is more than twice as fast as the most\ncompetitive existing methods while maintaining high accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 05:30:21 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Qin", "Kyle K.", ""], ["Salim", "Flora D.", ""], ["Ren", "Yongli", ""], ["Shao", "Wei", ""], ["Heimann", "Mark", ""], ["Koutra", "Danai", ""]]}]