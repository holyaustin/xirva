[{"id": "1306.0040", "submitter": "James Scott", "authors": "James G. Scott and Liang Sun", "title": "Expectation-maximization for logistic regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a family of expectation-maximization (EM) algorithms for binary\nand negative-binomial logistic regression, drawing a sharp connection with the\nvariational-Bayes algorithm of Jaakkola and Jordan (2000). Indeed, our results\nallow a version of this variational-Bayes approach to be re-interpreted as a\ntrue EM algorithm. We study several interesting features of the algorithm, and\nof this previously unrecognized connection with variational Bayes. We also\ngeneralize the approach to sparsity-promoting priors, and to an online method\nwhose convergence properties are easily established. This latter method\ncompares favorably with stochastic-gradient descent in situations with marked\ncollinearity.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2013 21:57:12 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Scott", "James G.", ""], ["Sun", "Liang", ""]]}, {"id": "1306.0160", "submitter": "Praneeth Netrapalli", "authors": "Praneeth Netrapalli and Prateek Jain and Sujay Sanghavi", "title": "Phase Retrieval using Alternating Minimization", "comments": "Accepted for publication in IEEE Transactions on Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phase retrieval problems involve solving linear equations, but with missing\nsign (or phase, for complex numbers) information. More than four decades after\nit was first proposed, the seminal error reduction algorithm of (Gerchberg and\nSaxton 1972) and (Fienup 1982) is still the popular choice for solving many\nvariants of this problem. The algorithm is based on alternating minimization;\ni.e. it alternates between estimating the missing phase information, and the\ncandidate solution. Despite its wide usage in practice, no global convergence\nguarantees for this algorithm are known. In this paper, we show that a\n(resampling) variant of this approach converges geometrically to the solution\nof one such problem -- finding a vector $\\mathbf{x}$ from\n$\\mathbf{y},\\mathbf{A}$, where $\\mathbf{y} =\n\\left|\\mathbf{A}^{\\top}\\mathbf{x}\\right|$ and $|\\mathbf{z}|$ denotes a vector\nof element-wise magnitudes of $\\mathbf{z}$ -- under the assumption that\n$\\mathbf{A}$ is Gaussian.\n  Empirically, we demonstrate that alternating minimization performs similar to\nrecently proposed convex techniques for this problem (which are based on\n\"lifting\" to a convex matrix problem) in sample complexity and robustness to\nnoise. However, it is much more efficient and can scale to large problems.\nAnalytically, for a resampling version of alternating minimization, we show\ngeometric convergence to the solution, and sample complexity that is off by log\nfactors from obvious lower bounds. We also establish close to optimal scaling\nfor the case when the unknown vector is sparse. Our work represents the first\ntheoretical guarantee for alternating minimization (albeit with resampling) for\nany variant of phase retrieval problems in the non-convex setting.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2013 00:45:12 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2015 11:45:50 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Netrapalli", "Praneeth", ""], ["Jain", "Prateek", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1306.0186", "submitter": "Iain Murray", "authors": "Benigno Uria, Iain Murray, Hugo Larochelle", "title": "RNADE: The real-valued neural autoregressive density-estimator", "comments": "12 pages, 3 figures, 3 tables, 2 algorithms. Merges the published\n  paper and supplementary material into one document", "journal-ref": "Advances in Neural Information Processing Systems 26:2175-2183,\n  2013", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce RNADE, a new model for joint density estimation of real-valued\nvectors. Our model calculates the density of a datapoint as the product of\none-dimensional conditionals modeled using mixture density networks with shared\nparameters. RNADE learns a distributed representation of the data, while having\na tractable expression for the calculation of densities. A tractable likelihood\nallows direct comparison with other methods and training by standard\ngradient-based optimizers. We compare the performance of RNADE on several\ndatasets of heterogeneous and perceptual data, finding it outperforms mixture\nmodels in all but one case.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2013 09:37:53 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2014 11:14:27 GMT"}], "update_date": "2014-01-10", "authors_parsed": [["Uria", "Benigno", ""], ["Murray", "Iain", ""], ["Larochelle", "Hugo", ""]]}, {"id": "1306.0202", "submitter": "Gabriel Kronberger", "authors": "Gabriel Kronberger", "title": "Declarative Modeling and Bayesian Inference of Dark Matter Halos", "comments": "Presented at the Workshop \"Intelligent Information Processing\",\n  EUROCAST2013. To appear in selected papers of Computer Aided Systems Theory -\n  EUROCAST 2013; Volumes Editors: Roberto Moreno-D\\'iaz, Franz R. Pichler,\n  Alexis Quesada-Arencibia; LNCS Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic programming allows specification of probabilistic models in a\ndeclarative manner. Recently, several new software systems and languages for\nprobabilistic programming have been developed on the basis of newly developed\nand improved methods for approximate inference in probabilistic models. In this\ncontribution a probabilistic model for an idealized dark matter localization\nproblem is described. We first derive the probabilistic model for the inference\nof dark matter locations and masses, and then show how this model can be\nimplemented using BUGS and Infer.NET, two software systems for probabilistic\nprogramming. Finally, the different capabilities of both systems are discussed.\nThe presented dark matter model includes mainly non-conjugate factors, thus, it\nis difficult to implement this model with Infer.NET.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2013 12:32:11 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Kronberger", "Gabriel", ""]]}, {"id": "1306.0239", "submitter": "Yichuan Tang", "authors": "Yichuan Tang", "title": "Deep Learning using Linear Support Vector Machines", "comments": "Contribution to the ICML 2013 Challenges in Representation Learning\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, fully-connected and convolutional neural networks have been trained\nto achieve state-of-the-art performance on a wide variety of tasks such as\nspeech recognition, image classification, natural language processing, and\nbioinformatics. For classification tasks, most of these \"deep learning\" models\nemploy the softmax activation function for prediction and minimize\ncross-entropy loss. In this paper, we demonstrate a small but consistent\nadvantage of replacing the softmax layer with a linear support vector machine.\nLearning minimizes a margin-based loss instead of the cross-entropy loss. While\nthere have been various combinations of neural nets and SVMs in prior art, our\nresults using L2-SVMs show that by simply replacing softmax with linear SVMs\ngives significant gains on popular deep learning datasets MNIST, CIFAR-10, and\nthe ICML 2013 Representation Learning Workshop's face expression recognition\nchallenge.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2013 18:46:58 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2013 21:30:59 GMT"}, {"version": "v3", "created": "Mon, 23 Dec 2013 21:16:45 GMT"}, {"version": "v4", "created": "Sat, 21 Feb 2015 16:58:39 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Tang", "Yichuan", ""]]}, {"id": "1306.0308", "submitter": "Philipp Hennig PhD", "authors": "Philipp Hennig and S{\\o}ren Hauberg", "title": "Probabilistic Solutions to Differential Equations and their Application\n  to Riemannian Statistics", "comments": "11 page (9 page conference paper, plus supplements)", "journal-ref": "Proceedings of the 17th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2014, Reykjavik, Iceland. Journal of\n  Machine Learning Research: W&CP volume 33", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a probabilistic numerical method for the solution of both boundary\nand initial value problems that returns a joint Gaussian process posterior over\nthe solution. Such methods have concrete value in the statistics on Riemannian\nmanifolds, where non-analytic ordinary differential equations are involved in\nvirtually all computations. The probabilistic formulation permits marginalising\nthe uncertainty of the numerical solution such that statistics are less\nsensitive to inaccuracies. This leads to new Riemannian algorithms for mean\nvalue computations and principal geodesic analysis. Marginalisation also means\nresults can be less precise than point estimates, enabling a noticeable\nspeed-up over the state of the art. Our approach is an argument for a wider\npoint that uncertainty caused by numerical calculations should be tracked\nthroughout the pipeline of machine learning algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2013 06:56:47 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2014 12:51:32 GMT"}], "update_date": "2014-02-13", "authors_parsed": [["Hennig", "Philipp", ""], ["Hauberg", "S\u00f8ren", ""]]}, {"id": "1306.0393", "submitter": "Yuyi Wang", "authors": "Yuyi Wang, Jan Ramon and Zheng-Chu Guo", "title": "Learning from networked examples in a k-partite graph", "comments": "a special case", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning algorithms are based on the assumption that training\nexamples are drawn independently. However, this assumption does not hold\nanymore when learning from a networked sample where two or more training\nexamples may share common features. We propose an efficient weighting method\nfor learning from networked examples and show the sample error bound which is\nbetter than previous work.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2013 13:10:35 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2013 15:18:16 GMT"}, {"version": "v3", "created": "Sat, 18 Feb 2017 00:34:19 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Wang", "Yuyi", ""], ["Ramon", "Jan", ""], ["Guo", "Zheng-Chu", ""]]}, {"id": "1306.0404", "submitter": "Jun He", "authors": "Jun He, Dejiao Zhang, Laura Balzano, Tao Tao", "title": "Iterative Grassmannian Optimization for Robust Image Alignment", "comments": "Preprint submitted to the special issue of the Image and Vision\n  Computing Journal on the theme \"The Best of Face and Gesture 2013\"", "journal-ref": "Image and Vision Computing, 32(10), 800-813, 2014", "doi": "10.1016/j.imavis.2014.02.015", "report-no": null, "categories": "cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust high-dimensional data processing has witnessed an exciting development\nin recent years, as theoretical results have shown that it is possible using\nconvex programming to optimize data fit to a low-rank component plus a sparse\noutlier component. This problem is also known as Robust PCA, and it has found\napplication in many areas of computer vision. In image and video processing and\nface recognition, the opportunity to process massive image databases is\nemerging as people upload photo and video data online in unprecedented volumes.\nHowever, data quality and consistency is not controlled in any way, and the\nmassiveness of the data poses a serious computational challenge. In this paper\nwe present t-GRASTA, or \"Transformed GRASTA (Grassmannian Robust Adaptive\nSubspace Tracking Algorithm)\". t-GRASTA iteratively performs incremental\ngradient descent constrained to the Grassmann manifold of subspaces in order to\nsimultaneously estimate a decomposition of a collection of images into a\nlow-rank subspace, a sparse part of occlusions and foreground objects, and a\ntransformation such as rotation or translation of the image. We show that\nt-GRASTA is 4 $\\times$ faster than state-of-the-art algorithms, has half the\nmemory requirement, and can achieve alignment for face images as well as\njittered camera surveillance images.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2013 13:49:14 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2013 14:08:47 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["He", "Jun", ""], ["Zhang", "Dejiao", ""], ["Balzano", "Laura", ""], ["Tao", "Tao", ""]]}, {"id": "1306.0407", "submitter": "Igor Braga", "authors": "Vladimir Vapnik, Igor Braga, and Rauf Izmailov", "title": "Constructive Setting of the Density Ratio Estimation Problem and its\n  Rigorous Solution", "comments": "Added funding information", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general constructive setting of the density ratio estimation\nproblem as a solution of a (multidimensional) integral equation. In this\nequation, not only its right hand side is known approximately, but also the\nintegral operator is defined approximately. We show that this ill-posed problem\nhas a rigorous solution and obtain the solution in a closed form. The key\nelement of this solution is the novel V-matrix, which captures the geometry of\nthe observed samples. We compare our method with three well-known previously\nproposed ones. Our experimental results demonstrate the good potential of the\nnew approach.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2013 13:54:34 GMT"}, {"version": "v2", "created": "Sat, 15 Jun 2013 02:01:22 GMT"}], "update_date": "2013-06-18", "authors_parsed": [["Vapnik", "Vladimir", ""], ["Braga", "Igor", ""], ["Izmailov", "Rauf", ""]]}, {"id": "1306.0543", "submitter": "Misha Denil", "authors": "Misha Denil, Babak Shakibi, Laurent Dinh, Marc'Aurelio Ranzato, Nando\n  de Freitas", "title": "Predicting Parameters in Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that there is significant redundancy in the parameterization\nof several deep learning models. Given only a few weight values for each\nfeature it is possible to accurately predict the remaining values. Moreover, we\nshow that not only can the parameter values be predicted, but many of them need\nnot be learned at all. We train several different architectures by learning\nonly a small number of weights and predicting the rest. In the best case we are\nable to predict more than 95% of the weights of a network without any drop in\naccuracy.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2013 19:16:26 GMT"}, {"version": "v2", "created": "Mon, 27 Oct 2014 11:49:08 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Denil", "Misha", ""], ["Shakibi", "Babak", ""], ["Dinh", "Laurent", ""], ["Ranzato", "Marc'Aurelio", ""], ["de Freitas", "Nando", ""]]}, {"id": "1306.0604", "submitter": "Yingyu Liang", "authors": "Maria Florina Balcan, Steven Ehrlich, Yingyu Liang", "title": "Distributed k-Means and k-Median Clustering on General Topologies", "comments": "Corrected Theorem 4 in the appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides new algorithms for distributed clustering for two popular\ncenter-based objectives, k-median and k-means. These algorithms have provable\nguarantees and improve communication complexity over existing approaches.\nFollowing a classic approach in clustering by \\cite{har2004coresets}, we reduce\nthe problem of finding a clustering with low cost to the problem of finding a\ncoreset of small size. We provide a distributed method for constructing a\nglobal coreset which improves over the previous methods by reducing the\ncommunication complexity, and which works over general communication\ntopologies. Experimental results on large scale data sets show that this\napproach outperforms other coreset-based distributed clustering algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2013 21:49:19 GMT"}, {"version": "v2", "created": "Sun, 30 Jun 2013 02:26:33 GMT"}, {"version": "v3", "created": "Wed, 30 Oct 2013 19:20:30 GMT"}, {"version": "v4", "created": "Sat, 25 Jan 2020 23:23:11 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Balcan", "Maria Florina", ""], ["Ehrlich", "Steven", ""], ["Liang", "Yingyu", ""]]}, {"id": "1306.0618", "submitter": "Adam Kapelner", "authors": "Adam Kapelner and Justin Bleich", "title": "Prediction with Missing Data via Bayesian Additive Regression Trees", "comments": "18 pages, 3 figures, 2 tables, 1 algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for incorporating missing data in non-parametric\nstatistical learning without the need for imputation. We focus on a tree-based\nmethod, Bayesian Additive Regression Trees (BART), enhanced with \"Missingness\nIncorporated in Attributes,\" an approach recently proposed incorporating\nmissingness into decision trees (Twala, 2008). This procedure takes advantage\nof the partitioning mechanisms found in tree-based models. Simulations on\ngenerated models and real data indicate that our proposed method can forecast\nwell on complicated missing-at-random and not-missing-at-random models as well\nas models where missingness itself influences the response. Our procedure has\nhigher predictive performance and is more stable than competitors in many\ncases. We also illustrate BART's abilities to incorporate missingness into\nuncertainty intervals and to detect the influence of missingness on the model\nfit.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2013 22:57:20 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2013 01:28:09 GMT"}, {"version": "v3", "created": "Wed, 12 Feb 2014 22:01:18 GMT"}], "update_date": "2014-02-14", "authors_parsed": [["Kapelner", "Adam", ""], ["Bleich", "Justin", ""]]}, {"id": "1306.0626", "submitter": "Prateek Jain", "authors": "Prateek Jain and Inderjit S. Dhillon", "title": "Provable Inductive Matrix Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a movie recommendation system where apart from the ratings\ninformation, side information such as user's age or movie's genre is also\navailable. Unlike standard matrix completion, in this setting one should be\nable to predict inductively on new users/movies. In this paper, we study the\nproblem of inductive matrix completion in the exact recovery setting. That is,\nwe assume that the ratings matrix is generated by applying feature vectors to a\nlow-rank matrix and the goal is to recover back the underlying matrix.\nFurthermore, we generalize the problem to that of low-rank matrix estimation\nusing rank-1 measurements. We study this generic problem and provide conditions\nthat the set of measurements should satisfy so that the alternating\nminimization method (which otherwise is a non-convex method with no convergence\nguarantees) is able to recover back the {\\em exact} underlying low-rank matrix.\n  In addition to inductive matrix completion, we show that two other low-rank\nestimation problems can be studied in our framework: a) general low-rank matrix\nsensing using rank-1 measurements, and b) multi-label regression with missing\nlabels. For both the problems, we provide novel and interesting bounds on the\nnumber of measurements required by alternating minimization to provably\nconverges to the {\\em exact} low-rank matrix. In particular, our analysis for\nthe general low rank matrix sensing problem significantly improves the required\nstorage and computational cost than that required by the RIP-based matrix\nsensing methods \\cite{RechtFP2007}. Finally, we provide empirical validation of\nour approach and demonstrate that alternating minimization is able to recover\nthe true matrix for the above mentioned problems using a small number of\nmeasurements.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2013 00:38:17 GMT"}], "update_date": "2013-06-05", "authors_parsed": [["Jain", "Prateek", ""], ["Dhillon", "Inderjit S.", ""]]}, {"id": "1306.0686", "submitter": "Pooria Joulani", "authors": "Pooria Joulani, Andr\\'as Gy\\\"orgy, Csaba Szepesv\\'ari", "title": "Online Learning under Delayed Feedback", "comments": "Extended version of a paper accepted to ICML-2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online learning with delayed feedback has received increasing attention\nrecently due to its several applications in distributed, web-based learning\nproblems. In this paper we provide a systematic study of the topic, and analyze\nthe effect of delay on the regret of online learning algorithms. Somewhat\nsurprisingly, it turns out that delay increases the regret in a multiplicative\nway in adversarial problems, and in an additive way in stochastic problems. We\ngive meta-algorithms that transform, in a black-box fashion, algorithms\ndeveloped for the non-delayed case into ones that can handle the presence of\ndelays in the feedback loop. Modifications of the well-known UCB algorithm are\nalso developed for the bandit problem with delayed feedback, with the advantage\nover the meta-algorithms that they can be implemented with lower complexity.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2013 07:39:21 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2013 01:01:04 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Joulani", "Pooria", ""], ["Gy\u00f6rgy", "Andr\u00e1s", ""], ["Szepesv\u00e1ri", "Csaba", ""]]}, {"id": "1306.0733", "submitter": "Durk Kingma", "authors": "Diederik P Kingma", "title": "Fast Gradient-Based Inference with Continuous Latent Variable Models in\n  Auxiliary Form", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a technique for increasing the efficiency of gradient-based\ninference and learning in Bayesian networks with multiple layers of continuous\nlatent vari- ables. We show that, in many cases, it is possible to express such\nmodels in an auxiliary form, where continuous latent variables are\nconditionally deterministic given their parents and a set of independent\nauxiliary variables. Variables of mod- els in this auxiliary form have much\nlarger Markov blankets, leading to significant speedups in gradient-based\ninference, e.g. rapid mixing Hybrid Monte Carlo and efficient gradient-based\noptimization. The relative efficiency is confirmed in ex- periments.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2013 11:28:32 GMT"}], "update_date": "2013-06-05", "authors_parsed": [["Kingma", "Diederik P", ""]]}, {"id": "1306.0735", "submitter": "Christopher Nemeth", "authors": "Christopher Nemeth, Paul Fearnhead, Lyudmila Mihaylova", "title": "Particle approximations of the score and observed information matrix for\n  parameter estimation in state space models with linear computational cost", "comments": "Accepted to Journal of Computational and Graphical Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Poyiadjis et al. (2011) show how particle methods can be used to estimate\nboth the score and the observed information matrix for state space models.\nThese methods either suffer from a computational cost that is quadratic in the\nnumber of particles, or produce estimates whose variance increases\nquadratically with the amount of data. This paper introduces an alternative\napproach for estimating these terms at a computational cost that is linear in\nthe number of particles. The method is derived using a combination of kernel\ndensity estimation, to avoid the particle degeneracy that causes the\nquadratically increasing variance, and Rao-Blackwellisation. Crucially, we show\nthe method is robust to the choice of bandwidth within the kernel density\nestimation, as it has good asymptotic properties regardless of this choice. Our\nestimates of the score and observed information matrix can be used within both\nonline and batch procedures for estimating parameters for state space models.\nEmpirical results show improved parameter estimates compared to existing\nmethods at a significantly reduced computational cost. Supplementary materials\nincluding code are available.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2013 11:37:05 GMT"}, {"version": "v2", "created": "Tue, 3 Feb 2015 14:30:52 GMT"}, {"version": "v3", "created": "Fri, 4 Sep 2015 16:09:23 GMT"}], "update_date": "2015-09-07", "authors_parsed": [["Nemeth", "Christopher", ""], ["Fearnhead", "Paul", ""], ["Mihaylova", "Lyudmila", ""]]}, {"id": "1306.0811", "submitter": "Giovanni Zappella", "authors": "Nicol\\`o Cesa-Bianchi, Claudio Gentile and Giovanni Zappella", "title": "A Gang of Bandits", "comments": "NIPS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-armed bandit problems are receiving a great deal of attention because\nthey adequately formalize the exploration-exploitation trade-offs arising in\nseveral industrially relevant applications, such as online advertisement and,\nmore generally, recommendation systems. In many cases, however, these\napplications have a strong social component, whose integration in the bandit\nalgorithm could lead to a dramatic performance increase. For instance, we may\nwant to serve content to a group of users by taking advantage of an underlying\nnetwork of social relationships among them. In this paper, we introduce novel\nalgorithmic approaches to the solution of such networked bandit problems. More\nspecifically, we design and analyze a global strategy which allocates a bandit\nalgorithm to each network node (user) and allows it to \"share\" signals\n(contexts and payoffs) with the neghboring nodes. We then derive two more\nscalable variants of this strategy based on different ways of clustering the\ngraph nodes. We experimentally compare the algorithm and its variants to\nstate-of-the-art methods for contextual bandits that do not use the relational\ninformation. Our experiments, carried out on synthetic and real-world datasets,\nshow a marked increase in prediction performance obtained by exploiting the\nnetwork structure.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2013 14:24:31 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2013 16:32:25 GMT"}, {"version": "v3", "created": "Mon, 4 Nov 2013 10:07:42 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Cesa-Bianchi", "Nicol\u00f2", ""], ["Gentile", "Claudio", ""], ["Zappella", "Giovanni", ""]]}, {"id": "1306.0842", "submitter": "Krikamol Muandet", "authors": "Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, Arthur\n  Gretton, Bernhard Sch\\\"olkopf", "title": "Kernel Mean Estimation and Stein's Effect", "comments": "first draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mean function in reproducing kernel Hilbert space, or a kernel mean, is an\nimportant part of many applications ranging from kernel principal component\nanalysis to Hilbert-space embedding of distributions. Given finite samples, an\nempirical average is the standard estimate for the true kernel mean. We show\nthat this estimator can be improved via a well-known phenomenon in statistics\ncalled Stein's phenomenon. After consideration, our theoretical analysis\nreveals the existence of a wide class of estimators that are better than the\nstandard. Focusing on a subset of this class, we propose efficient shrinkage\nestimators for the kernel mean. Empirical evaluations on several benchmark\napplications clearly demonstrate that the proposed estimators outperform the\nstandard kernel mean estimator.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2013 16:09:20 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2013 17:18:25 GMT"}], "update_date": "2013-06-07", "authors_parsed": [["Muandet", "Krikamol", ""], ["Fukumizu", "Kenji", ""], ["Sriperumbudur", "Bharath", ""], ["Gretton", "Arthur", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1306.0886", "submitter": "Felix X. Yu", "authors": "Felix X. Yu, Dong Liu, Sanjiv Kumar, Tony Jebara, Shih-Fu Chang", "title": "$\\propto$SVM for learning with label proportions", "comments": "Appears in Proceedings of the 30th International Conference on\n  Machine Learning (ICML 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning with label proportions in which the training\ndata is provided in groups and only the proportion of each class in each group\nis known. We propose a new method called proportion-SVM, or $\\propto$SVM, which\nexplicitly models the latent unknown instance labels together with the known\ngroup label proportions in a large-margin framework. Unlike the existing works,\nour approach avoids making restrictive assumptions about the data. The\n$\\propto$SVM model leads to a non-convex integer programming problem. In order\nto solve it efficiently, we propose two algorithms: one based on simple\nalternating optimization and the other based on a convex relaxation. Extensive\nexperiments on standard datasets show that $\\propto$SVM outperforms the\nstate-of-the-art, especially for larger group sizes.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2013 19:35:31 GMT"}], "update_date": "2013-06-05", "authors_parsed": [["Yu", "Felix X.", ""], ["Liu", "Dong", ""], ["Kumar", "Sanjiv", ""], ["Jebara", "Tony", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1306.0895", "submitter": "Marco Cuturi", "authors": "Marco Cuturi", "title": "Sinkhorn Distances: Lightspeed Computation of Optimal Transportation\n  Distances", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 26, pages\n  2292--2300, 2013", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal transportation distances are a fundamental family of parameterized\ndistances for histograms. Despite their appealing theoretical properties,\nexcellent performance in retrieval tasks and intuitive formulation, their\ncomputation involves the resolution of a linear program whose cost is\nprohibitive whenever the histograms' dimension exceeds a few hundreds. We\npropose in this work a new family of optimal transportation distances that look\nat transportation problems from a maximum-entropy perspective. We smooth the\nclassical optimal transportation problem with an entropic regularization term,\nand show that the resulting optimum is also a distance which can be computed\nthrough Sinkhorn-Knopp's matrix scaling algorithm at a speed that is several\norders of magnitude faster than that of transportation solvers. We also report\nimproved performance over classical optimal transportation distances on the\nMNIST benchmark problem.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2013 14:45:10 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Cuturi", "Marco", ""]]}, {"id": "1306.0940", "submitter": "Ian Osband", "authors": "Ian Osband, Daniel Russo, Benjamin Van Roy", "title": "(More) Efficient Reinforcement Learning via Posterior Sampling", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most provably-efficient learning algorithms introduce optimism about\npoorly-understood states and actions to encourage exploration. We study an\nalternative approach for efficient exploration, posterior sampling for\nreinforcement learning (PSRL). This algorithm proceeds in repeated episodes of\nknown duration. At the start of each episode, PSRL updates a prior distribution\nover Markov decision processes and takes one sample from this posterior. PSRL\nthen follows the policy that is optimal for this sample during the episode. The\nalgorithm is conceptually simple, computationally efficient and allows an agent\nto encode prior knowledge in a natural way. We establish an $\\tilde{O}(\\tau S\n\\sqrt{AT})$ bound on the expected regret, where $T$ is time, $\\tau$ is the\nepisode length and $S$ and $A$ are the cardinalities of the state and action\nspaces. This bound is one of the first for an algorithm not based on optimism,\nand close to the state of the art for any reinforcement learning algorithm. We\nshow through simulation that PSRL significantly outperforms existing algorithms\nwith similar regret bounds.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2013 23:00:56 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2013 05:14:31 GMT"}, {"version": "v3", "created": "Thu, 26 Sep 2013 00:38:51 GMT"}, {"version": "v4", "created": "Wed, 13 Nov 2013 19:31:26 GMT"}, {"version": "v5", "created": "Thu, 26 Dec 2013 09:20:29 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Osband", "Ian", ""], ["Russo", "Daniel", ""], ["Van Roy", "Benjamin", ""]]}, {"id": "1306.0963", "submitter": "Been Kim", "authors": "Been Kim, Caleb M. Chacha, Julie Shah", "title": "Inferring Robot Task Plans from Human Team Meetings: A Generative\n  Modeling Approach with Logic-Based Prior", "comments": "Appears in Proceedings of the Twenty-Seventh AAAI Conference on\n  Artificial Intelligence (AAAI-13)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to reduce the burden of programming and deploying autonomous systems\nto work in concert with people in time-critical domains, such as military field\noperations and disaster response. Deployment plans for these operations are\nfrequently negotiated on-the-fly by teams of human planners. A human operator\nthen translates the agreed upon plan into machine instructions for the robots.\nWe present an algorithm that reduces this translation burden by inferring the\nfinal plan from a processed form of the human team's planning conversation. Our\napproach combines probabilistic generative modeling with logical plan\nvalidation used to compute a highly structured prior over possible plans. This\nhybrid approach enables us to overcome the challenge of performing inference\nover the large solution space with only a small amount of noisy data from the\nteam planning session. We validate the algorithm through human subject\nexperimentation and show we are able to infer a human team's final plan with\n83% accuracy on average. We also describe a robot demonstration in which two\npeople plan and execute a first-response collaborative task with a PR2 robot.\nTo the best of our knowledge, this is the first work that integrates a logical\nplanning technique within a generative model to perform plan inference.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2013 02:17:11 GMT"}], "update_date": "2013-06-06", "authors_parsed": [["Kim", "Been", ""], ["Chacha", "Caleb M.", ""], ["Shah", "Julie", ""]]}, {"id": "1306.1043", "submitter": "Jonas Peters", "authors": "Jonas Peters and Peter B\\\"uhlmann", "title": "Structural Intervention Distance (SID) for Evaluating Causal Graphs", "comments": null, "journal-ref": "Neural Computation 27:771-799, 2015", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference relies on the structure of a graph, often a directed acyclic\ngraph (DAG). Different graphs may result in different causal inference\nstatements and different intervention distributions. To quantify such\ndifferences, we propose a (pre-) distance between DAGs, the structural\nintervention distance (SID). The SID is based on a graphical criterion only and\nquantifies the closeness between two DAGs in terms of their corresponding\ncausal inference statements. It is therefore well-suited for evaluating graphs\nthat are used for computing interventions. Instead of DAGs it is also possible\nto compare CPDAGs, completed partially directed acyclic graphs that represent\nMarkov equivalence classes. Since it differs significantly from the popular\nStructural Hamming Distance (SHD), the SID constitutes a valuable additional\nmeasure. We discuss properties of this distance and provide an efficient\nimplementation with software code available on the first author's homepage (an\nR package is under construction).\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2013 10:15:46 GMT"}, {"version": "v2", "created": "Mon, 7 Apr 2014 16:37:32 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Peters", "Jonas", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "1306.1052", "submitter": "Aleksandr Aravkin", "authors": "Mohammad Emtiyaz Khan, Aleksandr Y. Aravkin, Michael P. Friedlander,\n  Matthias Seeger", "title": "Fast Dual Variational Inference for Non-Conjugate LGMs", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Gaussian models (LGMs) are widely used in statistics and machine\nlearning. Bayesian inference in non-conjugate LGMs is difficult due to\nintractable integrals involving the Gaussian prior and non-conjugate\nlikelihoods. Algorithms based on variational Gaussian (VG) approximations are\nwidely employed since they strike a favorable balance between accuracy,\ngenerality, speed, and ease of use. However, the structure of the optimization\nproblems associated with these approximations remains poorly understood, and\nstandard solvers take too long to converge. We derive a novel dual variational\ninference approach that exploits the convexity property of the VG\napproximations. We obtain an algorithm that solves a convex optimization\nproblem, reduces the number of variational parameters, and converges much\nfaster than previous methods. Using real-world data, we demonstrate these\nadvantages on a variety of LGMs, including Gaussian process classification, and\nlatent Gaussian Markov random fields.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2013 10:45:59 GMT"}], "update_date": "2013-06-06", "authors_parsed": [["Khan", "Mohammad Emtiyaz", ""], ["Aravkin", "Aleksandr Y.", ""], ["Friedlander", "Michael P.", ""], ["Seeger", "Matthias", ""]]}, {"id": "1306.1066", "submitter": "Benjamin Rubinstein", "authors": "Christos Dimitrakakis and Blaine Nelson and and Zuhe Zhang and\n  Aikaterini Mitrokotsa and Benjamin Rubinstein", "title": "Bayesian Differential Privacy through Posterior Sampling", "comments": "38 pages; An earlier version of this article was published in ALT\n  2014. This version has corrections and additional results", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy formalises privacy-preserving mechanisms that provide\naccess to a database. We pose the question of whether Bayesian inference itself\ncan be used directly to provide private access to data, with no modification.\nThe answer is affirmative: under certain conditions on the prior, sampling from\nthe posterior distribution can be used to achieve a desired level of privacy\nand utility. To do so, we generalise differential privacy to arbitrary dataset\nmetrics, outcome spaces and distribution families. This allows us to also deal\nwith non-i.i.d or non-tabular datasets. We prove bounds on the sensitivity of\nthe posterior to the data, which gives a measure of robustness. We also show\nhow to use posterior sampling to provide differentially private responses to\nqueries, within a decision-theoretic framework. Finally, we provide bounds on\nthe utility and on the distinguishability of datasets. The latter are\ncomplemented by a novel use of Le Cam's method to obtain lower bounds. All our\ngeneral results hold for arbitrary database metrics, including those for the\ncommon definition of differential privacy. For specific choices of the metric,\nwe give a number of examples satisfying our assumptions.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2013 11:38:46 GMT"}, {"version": "v2", "created": "Sat, 1 Feb 2014 13:40:36 GMT"}, {"version": "v3", "created": "Sun, 30 Mar 2014 15:31:32 GMT"}, {"version": "v4", "created": "Sun, 12 Jul 2015 03:44:30 GMT"}, {"version": "v5", "created": "Fri, 23 Dec 2016 12:28:36 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Dimitrakakis", "Christos", ""], ["Nelson", "Blaine", ""], ["Zhang", "and Zuhe", ""], ["Mitrokotsa", "Aikaterini", ""], ["Rubinstein", "Benjamin", ""]]}, {"id": "1306.1154", "submitter": "Anru Zhang", "authors": "T. Tony Cai and Anru Zhang", "title": "Sparse Representation of a Polytope and Recovery of Sparse Signals and\n  Low-rank Matrices", "comments": "to appear in IEEE Transactions on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper considers compressed sensing and affine rank minimization in both\nnoiseless and noisy cases and establishes sharp restricted isometry conditions\nfor sparse signal and low-rank matrix recovery. The analysis relies on a key\ntechnical tool which represents points in a polytope by convex combinations of\nsparse vectors. The technique is elementary while leads to sharp results.\n  It is shown that for any given constant $t\\ge {4/3}$, in compressed sensing\n$\\delta_{tk}^A < \\sqrt{(t-1)/t}$ guarantees the exact recovery of all $k$\nsparse signals in the noiseless case through the constrained $\\ell_1$\nminimization, and similarly in affine rank minimization\n$\\delta_{tr}^\\mathcal{M}< \\sqrt{(t-1)/t}$ ensures the exact reconstruction of\nall matrices with rank at most $r$ in the noiseless case via the constrained\nnuclear norm minimization. Moreover, for any $\\epsilon>0$,\n$\\delta_{tk}^A<\\sqrt{\\frac{t-1}{t}}+\\epsilon$ is not sufficient to guarantee\nthe exact recovery of all $k$-sparse signals for large $k$. Similar result also\nholds for matrix recovery. In addition, the conditions $\\delta_{tk}^A <\n\\sqrt{(t-1)/t}$ and $\\delta_{tr}^\\mathcal{M}< \\sqrt{(t-1)/t}$ are also shown to\nbe sufficient respectively for stable recovery of approximately sparse signals\nand low-rank matrices in the noisy case.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2013 15:50:28 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2013 03:45:04 GMT"}], "update_date": "2013-10-23", "authors_parsed": [["Cai", "T. Tony", ""], ["Zhang", "Anru", ""]]}, {"id": "1306.1185", "submitter": "Thomas Laurent", "authors": "Xavier Bresson, Thomas Laurent, David Uminsky and James H. von Brecht", "title": "Multiclass Total Variation Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ideas from the image processing literature have recently motivated a new set\nof clustering algorithms that rely on the concept of total variation. While\nthese algorithms perform well for bi-partitioning tasks, their recursive\nextensions yield unimpressive results for multiclass clustering tasks. This\npaper presents a general framework for multiclass total variation clustering\nthat does not rely on recursion. The results greatly outperform previous total\nvariation algorithms and compare well with state-of-the-art NMF approaches.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2013 17:42:57 GMT"}], "update_date": "2013-06-06", "authors_parsed": [["Bresson", "Xavier", ""], ["Laurent", "Thomas", ""], ["Uminsky", "David", ""], ["von Brecht", "James H.", ""]]}, {"id": "1306.1298", "submitter": "Allon G. Percus", "authors": "Cristina Garcia-Cardona, Arjuna Flenner, Allon G. Percus", "title": "Multiclass Semi-Supervised Learning on Graphs using Ginzburg-Landau\n  Functional Minimization", "comments": "16 pages, to appear in Springer's Lecture Notes in Computer Science\n  volume \"Pattern Recognition Applications and Methods 2013\", part of series on\n  Advances in Intelligent and Soft Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST physics.data-an stat.TH", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  We present a graph-based variational algorithm for classification of\nhigh-dimensional data, generalizing the binary diffuse interface model to the\ncase of multiple classes. Motivated by total variation techniques, the method\ninvolves minimizing an energy functional made up of three terms. The first two\nterms promote a stepwise continuous classification function with sharp\ntransitions between classes, while preserving symmetry among the class labels.\nThe third term is a data fidelity term, allowing us to incorporate prior\ninformation into the model in a semi-supervised framework. The performance of\nthe algorithm on synthetic data, as well as on the COIL and MNIST benchmark\ndatasets, is competitive with state-of-the-art graph-based multiclass\nsegmentation methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2013 05:32:00 GMT"}], "update_date": "2013-06-07", "authors_parsed": [["Garcia-Cardona", "Cristina", ""], ["Flenner", "Arjuna", ""], ["Percus", "Allon G.", ""]]}, {"id": "1306.1323", "submitter": "E.N.Sathishkumar", "authors": "T. Chandrasekhar, K. Thangavel, E.N. Sathishkumar", "title": "Verdict Accuracy of Quick Reduct Algorithm using Clustering and\n  Classification Techniques for Gene Expression Data", "comments": "7 pages, 3 figures", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 9,\n  Issue 1, No 1, January 2012, page no. 357-363", "doi": null, "report-no": null, "categories": "cs.LG cs.CE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most gene expression data, the number of training samples is very small\ncompared to the large number of genes involved in the experiments. However,\namong the large amount of genes, only a small fraction is effective for\nperforming a certain task. Furthermore, a small subset of genes is desirable in\ndeveloping gene expression based diagnostic tools for delivering reliable and\nunderstandable results. With the gene selection results, the cost of biological\nexperiment and decision can be greatly reduced by analyzing only the marker\ngenes. An important application of gene expression data in functional genomics\nis to classify samples according to their gene expression profiles. Feature\nselection (FS) is a process which attempts to select more informative features.\nIt is one of the important steps in knowledge discovery. Conventional\nsupervised FS methods evaluate various feature subsets using an evaluation\nfunction or metric to select only those features which are related to the\ndecision classes of the data under consideration. This paper studies a feature\nselection method based on rough set theory. Further K-Means, Fuzzy C-Means\n(FCM) algorithm have implemented for the reduced feature set without\nconsidering class labels. Then the obtained results are compared with the\noriginal class labels. Back Propagation Network (BPN) has also been used for\nclassification. Then the performance of K-Means, FCM, and BPN are analyzed\nthrough the confusion matrix. It is found that the BPN is performing well\ncomparatively.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2013 07:26:06 GMT"}], "update_date": "2013-06-07", "authors_parsed": [["Chandrasekhar", "T.", ""], ["Thangavel", "K.", ""], ["Sathishkumar", "E. N.", ""]]}, {"id": "1306.1350", "submitter": "Tuomo Sipola", "authors": "Tuomo Sipola, Fengyu Cong, Tapani Ristaniemi, Vinoo Alluri, Petri\n  Toiviainen, Elvira Brattico, Asoke K. Nandi", "title": "Diffusion map for clustering fMRI spatial maps extracted by independent\n  component analysis", "comments": "6 pages. 8 figures. Copyright (c) 2013 IEEE. Published at 2013 IEEE\n  International Workshop on Machine Learning for Signal Processing", "journal-ref": null, "doi": "10.1109/MLSP.2013.6661923", "report-no": null, "categories": "cs.CE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional magnetic resonance imaging (fMRI) produces data about activity\ninside the brain, from which spatial maps can be extracted by independent\ncomponent analysis (ICA). In datasets, there are n spatial maps that contain p\nvoxels. The number of voxels is very high compared to the number of analyzed\nspatial maps. Clustering of the spatial maps is usually based on correlation\nmatrices. This usually works well, although such a similarity matrix inherently\ncan explain only a certain amount of the total variance contained in the\nhigh-dimensional data where n is relatively small but p is large. For\nhigh-dimensional space, it is reasonable to perform dimensionality reduction\nbefore clustering. In this research, we used the recently developed diffusion\nmap for dimensionality reduction in conjunction with spectral clustering. This\nresearch revealed that the diffusion map based clustering worked as well as the\nmore traditional methods, and produced more compact clusters when needed.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2013 09:29:25 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2013 06:44:37 GMT"}, {"version": "v3", "created": "Sun, 14 Jul 2013 16:03:54 GMT"}, {"version": "v4", "created": "Fri, 27 Sep 2013 08:58:30 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Sipola", "Tuomo", ""], ["Cong", "Fengyu", ""], ["Ristaniemi", "Tapani", ""], ["Alluri", "Vinoo", ""], ["Toiviainen", "Petri", ""], ["Brattico", "Elvira", ""], ["Nandi", "Asoke K.", ""]]}, {"id": "1306.1433", "submitter": "Spencer Greenberg", "authors": "Spencer Greenberg, Mehryar Mohri", "title": "Tight Lower Bound on the Probability of a Binomial Exceeding its\n  Expectation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the proof of a tight lower bound on the probability that a binomial\nrandom variable exceeds its expected value. The inequality plays an important\nrole in a variety of contexts, including the analysis of relative deviation\nbounds in learning theory and generalization bounds for unbounded loss\nfunctions.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2013 15:15:07 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2013 16:40:39 GMT"}, {"version": "v3", "created": "Mon, 11 Nov 2013 04:43:10 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Greenberg", "Spencer", ""], ["Mohri", "Mehryar", ""]]}, {"id": "1306.1587", "submitter": "Hau-tieng Wu", "authors": "Amit Singer and Hau-tieng Wu", "title": "Spectral Convergence of the connection Laplacian from random samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral methods that are based on eigenvectors and eigenvalues of discrete\ngraph Laplacians, such as Diffusion Maps and Laplacian Eigenmaps are often used\nfor manifold learning and non-linear dimensionality reduction. It was\npreviously shown by Belkin and Niyogi \\cite{belkin_niyogi:2007} that the\neigenvectors and eigenvalues of the graph Laplacian converge to the\neigenfunctions and eigenvalues of the Laplace-Beltrami operator of the manifold\nin the limit of infinitely many data points sampled independently from the\nuniform distribution over the manifold. Recently, we introduced Vector\nDiffusion Maps and showed that the connection Laplacian of the tangent bundle\nof the manifold can be approximated from random samples. In this paper, we\npresent a unified framework for approximating other connection Laplacians over\nthe manifold by considering its principle bundle structure. We prove that the\neigenvectors and eigenvalues of these Laplacians converge in the limit of\ninfinitely many independent random samples. We generalize the spectral\nconvergence results to the case where the data points are sampled from a\nnon-uniform distribution, and for manifolds with and without boundary.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 02:06:31 GMT"}, {"version": "v2", "created": "Mon, 15 Sep 2014 22:38:55 GMT"}, {"version": "v3", "created": "Sun, 31 May 2015 17:58:26 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Singer", "Amit", ""], ["Wu", "Hau-tieng", ""]]}, {"id": "1306.1716", "submitter": "Alexander Petukhov", "authors": "Alexander Petukhov and Inna Kozlov", "title": "Fast greedy algorithm for subspace clustering from corrupted and\n  incomplete data", "comments": "arXiv admin note: substantial text overlap with arXiv:1304.4282", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the Fast Greedy Sparse Subspace Clustering (FGSSC) algorithm\nproviding an efficient method for clustering data belonging to a few\nlow-dimensional linear or affine subspaces. The main difference of our\nalgorithm from predecessors is its ability to work with noisy data having a\nhigh rate of erasures (missed entries with the known coordinates) and errors\n(corrupted entries with unknown coordinates). We discuss here how to implement\nthe fast version of the greedy algorithm with the maximum efficiency whose\ngreedy strategy is incorporated into iterations of the basic algorithm.\n  We provide numerical evidences that, in the subspace clustering capability,\nthe fast greedy algorithm outperforms not only the existing state-of-the art\nSSC algorithm taken by the authors as a basic algorithm but also the recent\nGSSC algorithm. At the same time, its computational cost is only slightly\nhigher than the cost of SSC.\n  The numerical evidence of the algorithm significant advantage is presented\nfor a few synthetic models as well as for the Extended Yale B dataset of facial\nimages. In particular, the face recognition misclassification rate turned out\nto be 6-20 times lower than for the SSC algorithm. We provide also the\nnumerical evidence that the FGSSC algorithm is able to perform clustering of\ncorrupted data efficiently even when the sum of subspace dimensions\nsignificantly exceeds the dimension of the ambient space.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 13:14:50 GMT"}], "update_date": "2013-06-10", "authors_parsed": [["Petukhov", "Alexander", ""], ["Kozlov", "Inna", ""]]}, {"id": "1306.1812", "submitter": "John Snyder", "authors": "John C. Snyder, Matthias Rupp, Katja Hansen, Leo Blooston,\n  Klaus-Robert M\\\"uller, Kieron Burke", "title": "Orbital-free Bond Breaking via Machine Learning", "comments": null, "journal-ref": null, "doi": "10.1063/1.4834075", "report-no": null, "categories": "physics.chem-ph cond-mat.mtrl-sci stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is used to approximate the kinetic energy of one dimensional\ndiatomics as a functional of the electron density. The functional can\naccurately dissociate a diatomic, and can be systematically improved with\ntraining. Highly accurate self-consistent densities and molecular forces are\nfound, indicating the possibility for ab-initio molecular dynamics simulations.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 19:13:27 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Snyder", "John C.", ""], ["Rupp", "Matthias", ""], ["Hansen", "Katja", ""], ["Blooston", "Leo", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Burke", "Kieron", ""]]}, {"id": "1306.1840", "submitter": "Paul Mineiro", "authors": "Paul Mineiro, Nikos Karampatziakis", "title": "Loss-Proportional Subsampling for Subsequent ERM", "comments": "Appears in the proceedings of the 30th International Conference on\n  Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a sampling scheme suitable for reducing a data set prior to\nselecting a hypothesis with minimum empirical risk. The sampling only considers\na subset of the ultimate (unknown) hypothesis set, but can nonetheless\nguarantee that the final excess risk will compare favorably with utilizing the\nentire original data set. We demonstrate the practical benefits of our approach\non a large dataset which we subsample and subsequently fit with boosted trees.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 20:12:17 GMT"}, {"version": "v2", "created": "Sun, 23 Jun 2013 05:32:31 GMT"}], "update_date": "2013-06-25", "authors_parsed": [["Mineiro", "Paul", ""], ["Karampatziakis", "Nikos", ""]]}, {"id": "1306.1851", "submitter": "Marcel  Nassar", "authors": "Marcel Nassar, Philip Schniter, and Brian L. Evans", "title": "A Factor Graph Approach to Joint OFDM Channel Estimation and Decoding in\n  Impulsive Noise Environments", "comments": "13 pages, 9 figures, submitted to IEEE Transactions on Signal\n  Processing", "journal-ref": null, "doi": "10.1109/TSP.2013.2295063", "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel receiver for orthogonal frequency division multiplexing\n(OFDM) transmissions in impulsive noise environments. Impulsive noise arises in\nmany modern wireless and wireline communication systems, such as Wi-Fi and\npowerline communications, due to uncoordinated interference that is much\nstronger than thermal noise. We first show that the bit-error-rate optimal\nreceiver jointly estimates the propagation channel coefficients, the noise\nimpulses, the finite-alphabet symbols, and the unknown bits. We then propose a\nnear-optimal yet computationally tractable approach to this joint estimation\nproblem using loopy belief propagation. In particular, we merge the recently\nproposed \"generalized approximate message passing\" (GAMP) algorithm with the\nforward-backward algorithm and soft-input soft-output decoding using a \"turbo\"\napproach. Numerical results indicate that the proposed receiver drastically\noutperforms existing receivers under impulsive noise and comes within 1 dB of\nthe matched-filter bound. Meanwhile, with N tones, the proposed\nfactor-graph-based receiver has only O(N log N) complexity, and it can be\nparallelized.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 21:57:30 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Nassar", "Marcel", ""], ["Schniter", "Philip", ""], ["Evans", "Brian L.", ""]]}, {"id": "1306.1913", "submitter": "Zoltan Szabo", "authors": "Andras Lorincz, Laszlo Jeni, Zoltan Szabo, Jeffrey Cohn, Takeo Kanade", "title": "Emotional Expression Classification using Time-Series Kernels", "comments": "IEEE International Workshop on Analysis and Modeling of Faces and\n  Gestures, Portland, Oregon, 28 June 2013 (accepted)", "journal-ref": null, "doi": "10.1109/CVPRW.2013.131", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of facial expressions, as spatio-temporal processes, can take\nadvantage of kernel methods if one considers facial landmark positions and\ntheir motion in 3D space. We applied support vector classification with kernels\nderived from dynamic time-warping similarity measures. We achieved over 99%\naccuracy - measured by area under ROC curve - using only the 'motion pattern'\nof the PCA compressed representation of the marker point vector, the so-called\nshape parameters. Beyond the classification of full motion patterns, several\nexpressions were recognized with over 90% accuracy in as few as 5-6 frames from\ntheir onset, about 200 milliseconds.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2013 12:57:39 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Lorincz", "Andras", ""], ["Jeni", "Laszlo", ""], ["Szabo", "Zoltan", ""], ["Cohn", "Jeffrey", ""], ["Kanade", "Takeo", ""]]}, {"id": "1306.2035", "submitter": "Martin Azizyan", "authors": "Martin Azizyan, Aarti Singh, Larry Wasserman", "title": "Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean\n  Separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While several papers have investigated computationally and statistically\nefficient methods for learning Gaussian mixtures, precise minimax bounds for\ntheir statistical performance as well as fundamental limits in high-dimensional\nsettings are not well-understood. In this paper, we provide precise information\ntheoretic bounds on the clustering accuracy and sample complexity of learning a\nmixture of two isotropic Gaussians in high dimensions under small mean\nseparation. If there is a sparse subset of relevant dimensions that determine\nthe mean separation, then the sample complexity only depends on the number of\nrelevant dimensions and mean separation, and can be achieved by a simple\ncomputationally efficient procedure. Our results provide the first step of a\ntheoretical basis for recent methods that combine feature selection and\nclustering.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2013 16:28:56 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Azizyan", "Martin", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1306.2084", "submitter": "Maximilian Nickel", "authors": "Maximilian Nickel, Volker Tresp", "title": "Logistic Tensor Factorization for Multi-Relational Data", "comments": "Accepted at ICML 2013 Workshop \"Structured Learning: Inferring Graphs\n  from Structured and Unstructured Inputs\" (SLG 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor factorizations have become increasingly popular approaches for various\nlearning tasks on structured data. In this work, we extend the RESCAL tensor\nfactorization, which has shown state-of-the-art results for multi-relational\nlearning, to account for the binary nature of adjacency tensors. We study the\nimprovements that can be gained via this approach on various benchmark datasets\nand show that the logistic extension can improve the prediction results\nsignificantly.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2013 01:45:49 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Nickel", "Maximilian", ""], ["Tresp", "Volker", ""]]}, {"id": "1306.2119", "submitter": "Francis Bach", "authors": "Francis Bach (INRIA Paris - Rocquencourt, LIENS), Eric Moulines (LTCI)", "title": "Non-strongly-convex smooth stochastic approximation with convergence\n  rate O(1/n)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the stochastic approximation problem where a convex function has\nto be minimized, given only the knowledge of unbiased estimates of its\ngradients at certain points, a framework which includes machine learning\nmethods based on the minimization of the empirical risk. We focus on problems\nwithout strong convexity, for which all previously known algorithms achieve a\nconvergence rate for function values of O(1/n^{1/2}). We consider and analyze\ntwo algorithms that achieve a rate of O(1/n) for classical supervised learning\nproblems. For least-squares regression, we show that averaged stochastic\ngradient descent with constant step-size achieves the desired rate. For\nlogistic regression, this is achieved by a simple novel stochastic gradient\nalgorithm that (a) constructs successive local quadratic approximations of the\nloss functions, while (b) preserving the same running time complexity as\nstochastic gradient descent. For these algorithms, we provide a non-asymptotic\nanalysis of the generalization error (in expectation, and also in high\nprobability for least-squares), and run extensive experiments on standard\nmachine learning benchmarks showing that they often outperform existing\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2013 07:31:10 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Bach", "Francis", "", "INRIA Paris - Rocquencourt, LIENS"], ["Moulines", "Eric", "", "LTCI"]]}, {"id": "1306.2194", "submitter": "Michael Chichignoud", "authors": "Michael Chichignoud and S\\'ebastien Loustau", "title": "Adaptive Noisy Clustering", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of adaptive noisy clustering is investigated. Given a set of\nnoisy observations $Z_i=X_i+\\epsilon_i$, $i=1,...,n$, the goal is to design\nclusters associated with the law of $X_i$'s, with unknown density $f$ with\nrespect to the Lebesgue measure. Since we observe a corrupted sample, a direct\napproach as the popular {\\it $k$-means} is not suitable in this case. In this\npaper, we propose a noisy $k$-means minimization, which is based on the\n$k$-means loss function and a deconvolution estimator of the density $f$. In\nparticular, this approach suffers from the dependence on a bandwidth involved\nin the deconvolution kernel. Fast rates of convergence for the excess risk are\nproposed for a particular choice of the bandwidth, which depends on the\nsmoothness of the density $f$.\n  Then, we turn out into the main issue of the paper: the data-driven choice of\nthe bandwidth. We state an adaptive upper bound for a new selection rule,\ncalled ERC (Empirical Risk Comparison). This selection rule is based on the\nLepski's principle, where empirical risks associated with different bandwidths\nare compared. Finally, we illustrate that this adaptive rule can be used in\nmany statistical problems of $M$-estimation where the empirical risk depends on\na nuisance parameter.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2013 13:15:25 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Chichignoud", "Michael", ""], ["Loustau", "S\u00e9bastien", ""]]}, {"id": "1306.2281", "submitter": "Dino Sejdinovic", "authors": "Dino Sejdinovic, Arthur Gretton and Wicher Bergsma", "title": "A Kernel Test for Three-Variable Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce kernel nonparametric tests for Lancaster three-variable\ninteraction and for total independence, using embeddings of signed measures\ninto a reproducing kernel Hilbert space. The resulting test statistics are\nstraightforward to compute, and are used in powerful interaction tests, which\nare consistent against all alternatives for a large family of reproducing\nkernels. We show the Lancaster test to be sensitive to cases where two\nindependent causes individually have weak influence on a third dependent\nvariable, but their combined effect has a strong influence. This makes the\nLancaster test especially suited to finding structure in directed graphical\nmodels, where it outperforms competing nonparametric tests in detecting such\nV-structures.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2013 18:54:47 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Sejdinovic", "Dino", ""], ["Gretton", "Arthur", ""], ["Bergsma", "Wicher", ""]]}, {"id": "1306.2298", "submitter": "Sadegh Motallebi", "authors": "Sadegh Motallebi, Sadegh Aliakbary, Jafar Habibi", "title": "Generative Model Selection Using a Scalable and Size-Independent Complex\n  Network Classifier", "comments": null, "journal-ref": "Chaos 23, 043127 (2013);", "doi": "10.1063/1.4840235", "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real networks exhibit nontrivial topological features such as heavy-tailed\ndegree distribution, high clustering, and small-worldness. Researchers have\ndeveloped several generative models for synthesizing artificial networks that\nare structurally similar to real networks. An important research problem is to\nidentify the generative model that best fits to a target network. In this\npaper, we investigate this problem and our goal is to select the model that is\nable to generate graphs similar to a given network instance. By the means of\ngenerating synthetic networks with seven outstanding generative models, we have\nutilized machine learning methods to develop a decision tree for model\nselection. Our proposed method, which is named \"Generative Model Selection for\nComplex Networks\" (GMSCN), outperforms existing methods with respect to\naccuracy, scalability and size-independence.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2013 19:42:10 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2013 09:46:04 GMT"}, {"version": "v3", "created": "Sat, 1 Feb 2014 10:42:30 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Motallebi", "Sadegh", ""], ["Aliakbary", "Sadegh", ""], ["Habibi", "Jafar", ""]]}, {"id": "1306.2533", "submitter": "Praneeth Vepakomma Praneeth Vepakomma", "authors": "Praneeth Vepakomma and Ahmed Elgammal", "title": "DISCOMAX: A Proximity-Preserving Distance Correlation Maximization\n  Algorithm", "comments": "Withdrawing as an updated and enhanced version of this paper is on\n  arxiv under my name as well titled Supervised Dimensionality Reduction via\n  Distance Correlation Maximization. See arXiv:1601.00236. That makes this\n  version pointless", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a regression setting we propose algorithms that reduce the dimensionality\nof the features while simultaneously maximizing a statistical measure of\ndependence known as distance correlation between the low-dimensional features\nand a response variable. This helps in solving the prediction problem with a\nlow-dimensional set of features. Our setting is different from subset-selection\nalgorithms where the problem is to choose the best subset of features for\nregression. Instead, we attempt to generate a new set of low-dimensional\nfeatures as in a feature-learning setting. We attempt to keep our proposed\napproach as model-free and our algorithm does not assume the application of any\nspecific regression model in conjunction with the low-dimensional features that\nit learns. The algorithm is iterative and is fomulated as a combination of the\nmajorization-minimization and concave-convex optimization procedures. We also\npresent spectral radius based convergence results for the proposed iterations.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2013 14:13:46 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2013 04:12:48 GMT"}, {"version": "v3", "created": "Fri, 17 Feb 2017 13:37:25 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Vepakomma", "Praneeth", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1306.2547", "submitter": "Aryeh Kontorovich", "authors": "Lee-Ad Gottlieb and Aryeh Kontorovich and Robert Krauthgamer", "title": "Efficient Classification for Metric Data", "comments": "This is the full version of an extended abstract that appeared in\n  Proceedings of the 23rd COLT, 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in large-margin classification of data residing in general\nmetric spaces (rather than Hilbert spaces) enable classification under various\nnatural metrics, such as string edit and earthmover distance. A general\nframework developed for this purpose by von Luxburg and Bousquet [JMLR, 2004]\nleft open the questions of computational efficiency and of providing direct\nbounds on generalization error.\n  We design a new algorithm for classification in general metric spaces, whose\nruntime and accuracy depend on the doubling dimension of the data points, and\ncan thus achieve superior classification performance in many common scenarios.\nThe algorithmic core of our approach is an approximate (rather than exact)\nsolution to the classical problems of Lipschitz extension and of Nearest\nNeighbor Search. The algorithm's generalization performance is guaranteed via\nthe fat-shattering dimension of Lipschitz classifiers, and we present\nexperimental evidence of its superiority to some common kernel methods. As a\nby-product, we offer a new perspective on the nearest neighbor classifier,\nwhich yields significantly sharper risk asymptotics than the classic analysis\nof Cover and Hart [IEEE Trans. Info. Theory, 1967].\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2013 15:00:35 GMT"}, {"version": "v2", "created": "Tue, 8 Jul 2014 19:56:43 GMT"}, {"version": "v3", "created": "Thu, 10 Jul 2014 21:33:44 GMT"}], "update_date": "2014-07-14", "authors_parsed": [["Gottlieb", "Lee-Ad", ""], ["Kontorovich", "Aryeh", ""], ["Krauthgamer", "Robert", ""]]}, {"id": "1306.2557", "submitter": "L.A. Prashanth", "authors": "L.A. Prashanth, Nathaniel Korda and R\\'emi Munos", "title": "Concentration bounds for temporal difference learning with linear\n  function approximation: The case of batch data and uniform sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a stochastic approximation (SA) based method with randomization of\nsamples for policy evaluation using the least squares temporal difference\n(LSTD) algorithm. Our proposed scheme is equivalent to running regular temporal\ndifference learning with linear function approximation, albeit with samples\npicked uniformly from a given dataset. Our method results in an $O(d)$\nimprovement in complexity in comparison to LSTD, where $d$ is the dimension of\nthe data. We provide non-asymptotic bounds for our proposed method, both in\nhigh probability and in expectation, under the assumption that the matrix\nunderlying the LSTD solution is positive definite. The latter assumption can be\neasily satisfied for the pathwise LSTD variant proposed in [23]. Moreover, we\nalso establish that using our method in place of LSTD does not impact the rate\nof convergence of the approximate value function to the true value function.\nThese rate results coupled with the low computational complexity of our method\nmake it attractive for implementation in big data settings, where $d$ is large.\nA similar low-complexity alternative for least squares regression is well-known\nas the stochastic gradient descent (SGD) algorithm. We provide finite-time\nbounds for SGD. We demonstrate the practicality of our method as an efficient\nalternative for pathwise LSTD empirically by combining it with the least\nsquares policy iteration (LSPI) algorithm in a traffic signal control\napplication. We also conduct another set of experiments that combines the SA\nbased low-complexity variant for least squares regression with the LinUCB\nalgorithm for contextual bandits, using the large scale news recommendation\ndataset from Yahoo.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2013 15:42:00 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2014 00:01:53 GMT"}, {"version": "v3", "created": "Mon, 16 Jun 2014 13:39:28 GMT"}, {"version": "v4", "created": "Wed, 18 Jun 2014 17:13:38 GMT"}, {"version": "v5", "created": "Tue, 28 Nov 2017 14:16:23 GMT"}, {"version": "v6", "created": "Fri, 24 Jan 2020 16:44:09 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Prashanth", "L. A.", ""], ["Korda", "Nathaniel", ""], ["Munos", "R\u00e9mi", ""]]}, {"id": "1306.2665", "submitter": "Weiyu Xu", "authors": "Myung Cho and Weiyu Xu", "title": "Precisely Verifying the Null Space Conditions in Compressed Sensing: A\n  Sandwiching Algorithm", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG cs.SY math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose new efficient algorithms to verify the null space\ncondition in compressed sensing (CS). Given an $(n-m) \\times n$ ($m>0$) CS\nmatrix $A$ and a positive $k$, we are interested in computing $\\displaystyle\n\\alpha_k = \\max_{\\{z: Az=0,z\\neq 0\\}}\\max_{\\{K: |K|\\leq k\\}}$ ${\\|z_K\n\\|_{1}}{\\|z\\|_{1}}$, where $K$ represents subsets of $\\{1,2,...,n\\}$, and $|K|$\nis the cardinality of $K$. In particular, we are interested in finding the\nmaximum $k$ such that $\\alpha_k < {1}{2}$. However, computing $\\alpha_k$ is\nknown to be extremely challenging. In this paper, we first propose a series of\nnew polynomial-time algorithms to compute upper bounds on $\\alpha_k$. Based on\nthese new polynomial-time algorithms, we further design a new sandwiching\nalgorithm, to compute the \\emph{exact} $\\alpha_k$ with greatly reduced\ncomplexity. When needed, this new sandwiching algorithm also achieves a smooth\ntradeoff between computational complexity and result accuracy. Empirical\nresults show the performance improvements of our algorithm over existing known\nmethods; and our algorithm outputs precise values of $\\alpha_k$, with much\nlower complexity than exhaustive search.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2013 21:57:47 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2013 22:26:55 GMT"}, {"version": "v3", "created": "Sat, 10 Aug 2013 01:14:46 GMT"}], "update_date": "2013-08-13", "authors_parsed": [["Cho", "Myung", ""], ["Xu", "Weiyu", ""]]}, {"id": "1306.2685", "submitter": "Alfredo Kalaitzis", "authors": "Alfredo Kalaitzis and Ricardo Silva", "title": "Flexible sampling of discrete data correlations without the marginal\n  distributions", "comments": "An overhauled version of the experimental section moved to the main\n  paper. Old experimental section moved to supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the joint dependence of discrete variables is a fundamental problem\nin machine learning, with many applications including prediction, clustering\nand dimensionality reduction. More recently, the framework of copula modeling\nhas gained popularity due to its modular parametrization of joint\ndistributions. Among other properties, copulas provide a recipe for combining\nflexible models for univariate marginal distributions with parametric families\nsuitable for potentially high dimensional dependence structures. More\nradically, the extended rank likelihood approach of Hoff (2007) bypasses\nlearning marginal models completely when such information is ancillary to the\nlearning task at hand as in, e.g., standard dimensionality reduction problems\nor copula parameter estimation. The main idea is to represent data by their\nobservable rank statistics, ignoring any other information from the marginals.\nInference is typically done in a Bayesian framework with Gaussian copulas, and\nit is complicated by the fact this implies sampling within a space where the\nnumber of constraints increases quadratically with the number of data points.\nThe result is slow mixing when using off-the-shelf Gibbs sampling. We present\nan efficient algorithm based on recent advances on constrained Hamiltonian\nMarkov chain Monte Carlo that is simple to implement and does not require\npaying for a quadratic cost in sample size.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2013 01:13:46 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2013 18:23:45 GMT"}, {"version": "v3", "created": "Thu, 14 Nov 2013 15:31:46 GMT"}], "update_date": "2013-11-15", "authors_parsed": [["Kalaitzis", "Alfredo", ""], ["Silva", "Ricardo", ""]]}, {"id": "1306.2733", "submitter": "Xuhui Fan", "authors": "Xuhui Fan, Longbing Cao, Richard Yi Da Xu", "title": "Copula Mixed-Membership Stochastic Blockmodel for Intra-Subgroup\n  Correlations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \\emph{Mixed-Membership Stochastic Blockmodel (MMSB)} is a popular\nframework for modeling social network relationships. It can fully exploit each\nindividual node's participation (or membership) in a social structure. Despite\nits powerful representations, this model makes an assumption that the\ndistributions of relational membership indicators between two nodes are\nindependent. Under many social network settings, however, it is possible that\ncertain known subgroups of people may have high or low correlations in terms of\ntheir membership categories towards each other, and such prior information\nshould be incorporated into the model. To this end, we introduce a \\emph{Copula\nMixed-Membership Stochastic Blockmodel (cMMSB)} where an individual Copula\nfunction is employed to jointly model the membership pairs of those nodes\nwithin the subgroup of interest. The model enables the use of various Copula\nfunctions to suit the scenario, while maintaining the membership's marginal\ndistribution, as needed, for modeling membership indicators with other nodes\noutside of the subgroup of interest. We describe the proposed model and its\ninference algorithm in detail for both the finite and infinite cases. In the\nexperiment section, we compare our algorithms with other popular models in\nterms of link prediction, using both synthetic and real world data.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2013 07:42:15 GMT"}, {"version": "v2", "created": "Sun, 6 Oct 2013 05:51:41 GMT"}], "update_date": "2013-10-08", "authors_parsed": [["Fan", "Xuhui", ""], ["Cao", "Longbing", ""], ["Da Xu", "Richard Yi", ""]]}, {"id": "1306.2759", "submitter": "Jingjing Xie", "authors": "Jingjing Xie, Bing Xu, Zhang Chuang", "title": "Horizontal and Vertical Ensemble with Deep Representation for\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning, especially which by using deep learning, has been\nwidely applied in classification. However, how to use limited size of labeled\ndata to achieve good classification performance with deep neural network, and\nhow can the learned features further improve classification remain indefinite.\nIn this paper, we propose Horizontal Voting Vertical Voting and Horizontal\nStacked Ensemble methods to improve the classification performance of deep\nneural networks. In the ICML 2013 Black Box Challenge, via using these methods\nindependently, Bing Xu achieved 3rd in public leaderboard, and 7th in private\nleaderboard; Jingjing Xie achieved 4th in public leaderboard, and 5th in\nprivate leaderboard.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2013 08:57:35 GMT"}], "update_date": "2013-06-13", "authors_parsed": [["Xie", "Jingjing", ""], ["Xu", "Bing", ""], ["Chuang", "Zhang", ""]]}, {"id": "1306.2801", "submitter": "KyungHyun Cho", "authors": "Kyunghyun Cho", "title": "Understanding Dropout: Training Multi-Layer Perceptrons with Auxiliary\n  Independent Stochastic Neurons", "comments": "ICONIP 2013: Special Session in Deep Learning (v4)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a simple, general method of adding auxiliary stochastic\nneurons to a multi-layer perceptron is proposed. It is shown that the proposed\nmethod is a generalization of recently successful methods of dropout (Hinton et\nal., 2012), explicit noise injection (Vincent et al., 2010; Bishop, 1995) and\nsemantic hashing (Salakhutdinov & Hinton, 2009). Under the proposed framework,\nan extension of dropout which allows using separate dropping probabilities for\ndifferent hidden neurons, or layers, is found to be available. The use of\ndifferent dropping probabilities for hidden layers separately is empirically\ninvestigated.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2013 12:38:40 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2013 15:09:29 GMT"}, {"version": "v3", "created": "Mon, 22 Jul 2013 11:42:18 GMT"}, {"version": "v4", "created": "Sun, 18 Aug 2013 21:39:12 GMT"}], "update_date": "2013-08-20", "authors_parsed": [["Cho", "Kyunghyun", ""]]}, {"id": "1306.2861", "submitter": "Roger Frigola", "authors": "Roger Frigola, Fredrik Lindsten, Thomas B. Sch\\\"on, Carl E. Rasmussen", "title": "Bayesian Inference and Learning in Gaussian Process State-Space Models\n  with Particle MCMC", "comments": null, "journal-ref": "Published in NIPS 2013, Advances in Neural Information Processing\n  Systems 26, pp. 3156--3164", "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SY", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  State-space models are successfully used in many areas of science,\nengineering and economics to model time series and dynamical systems. We\npresent a fully Bayesian approach to inference \\emph{and learning} (i.e. state\nestimation and system identification) in nonlinear nonparametric state-space\nmodels. We place a Gaussian process prior over the state transition dynamics,\nresulting in a flexible model able to capture complex dynamical phenomena. To\nenable efficient inference, we marginalize over the transition dynamics\nfunction and infer directly the joint smoothing distribution using specially\ntailored Particle Markov Chain Monte Carlo samplers. Once a sample from the\nsmoothing distribution is computed, the state transition predictive\ndistribution can be formulated analytically. Our approach preserves the full\nnonparametric expressivity of the model and can make use of sparse Gaussian\nprocesses to greatly reduce computational complexity.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2013 15:20:28 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2013 16:10:24 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Frigola", "Roger", ""], ["Lindsten", "Fredrik", ""], ["Sch\u00f6n", "Thomas B.", ""], ["Rasmussen", "Carl E.", ""]]}, {"id": "1306.2906", "submitter": "Yasmine Kawthar Zergat", "authors": "Kawthar Yasmine Zergat, Abderrahmane Amrouche", "title": "Robust Support Vector Machines for Speaker Verification Task", "comments": "5 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important step in speaker verification is extracting features that best\ncharacterize the speaker voice. This paper investigates a front-end processing\nthat aims at improving the performance of speaker verification based on the\nSVMs classifier, in text independent mode. This approach combines features\nbased on conventional Mel-cepstral Coefficients (MFCCs) and Line Spectral\nFrequencies (LSFs) to constitute robust multivariate feature vectors. To reduce\nthe high dimensionality required for training these feature vectors, we use a\ndimension reduction method called principal component analysis (PCA). In order\nto evaluate the robustness of these systems, different noisy environments have\nbeen used. The obtained results using TIMIT database showed that, using the\nparadigm that combines these spectral cues leads to a significant improvement\nin verification accuracy, especially with PCA reduction for low signal-to-noise\nratio noisy environment.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2013 17:32:02 GMT"}], "update_date": "2013-06-13", "authors_parsed": [["Zergat", "Kawthar Yasmine", ""], ["Amrouche", "Abderrahmane", ""]]}, {"id": "1306.2979", "submitter": "Srinadh Bhojanapalli", "authors": "Yudong Chen, Srinadh Bhojanapalli, Sujay Sanghavi, Rachel Ward", "title": "Completing Any Low-rank Matrix, Provably", "comments": "Added a new necessary condition(Theorem 6) and a result on completion\n  of row coherent matrices(Corollary 4). Partial results appeared in the\n  International Conference on Machine Learning 2014, under the title 'Coherent\n  Matrix Completion'. (34 pages, 4 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion, i.e., the exact and provable recovery of a low-rank matrix\nfrom a small subset of its elements, is currently only known to be possible if\nthe matrix satisfies a restrictive structural constraint---known as {\\em\nincoherence}---on its row and column spaces. In these cases, the subset of\nelements is sampled uniformly at random.\n  In this paper, we show that {\\em any} rank-$ r $ $ n$-by-$ n $ matrix can be\nexactly recovered from as few as $O(nr \\log^2 n)$ randomly chosen elements,\nprovided this random choice is made according to a {\\em specific biased\ndistribution}: the probability of any element being sampled should be\nproportional to the sum of the leverage scores of the corresponding row, and\ncolumn. Perhaps equally important, we show that this specific form of sampling\nis nearly necessary, in a natural precise sense; this implies that other\nperhaps more intuitive sampling schemes fail.\n  We further establish three ways to use the above result for the setting when\nleverage scores are not known \\textit{a priori}: (a) a sampling strategy for\nthe case when only one of the row or column spaces are incoherent, (b) a\ntwo-phase sampling procedure for general matrices that first samples to\nestimate leverage scores followed by sampling for exact recovery, and (c) an\nanalysis showing the advantages of weighted nuclear/trace-norm minimization\nover the vanilla un-weighted formulation for the case of non-uniform sampling.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2013 21:26:00 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2013 22:04:45 GMT"}, {"version": "v3", "created": "Fri, 11 Jul 2014 13:09:18 GMT"}, {"version": "v4", "created": "Mon, 21 Jul 2014 09:48:19 GMT"}], "update_date": "2014-07-22", "authors_parsed": [["Chen", "Yudong", ""], ["Bhojanapalli", "Srinadh", ""], ["Sanghavi", "Sujay", ""], ["Ward", "Rachel", ""]]}, {"id": "1306.2999", "submitter": "Xuhui Fan", "authors": "Xuhui Fan, Longbing Cao, Richard Yi Da Xu", "title": "Dynamic Infinite Mixed-Membership Stochastic Blockmodel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directional and pairwise measurements are often used to model\ninter-relationships in a social network setting. The Mixed-Membership\nStochastic Blockmodel (MMSB) was a seminal work in this area, and many of its\ncapabilities were extended since then. In this paper, we propose the\n\\emph{Dynamic Infinite Mixed-Membership stochastic blockModel (DIM3)}, a\ngeneralised framework that extends the existing work to a potentially infinite\nnumber of communities and mixture memberships for each of the network's nodes.\nThis model is in a dynamic setting, where additional model parameters are\nintroduced to reflect the degree of persistence between one's memberships at\nconsecutive times. Accordingly, two effective posterior sampling strategies and\ntheir results are presented using both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 00:42:19 GMT"}], "update_date": "2013-06-14", "authors_parsed": [["Fan", "Xuhui", ""], ["Cao", "Longbing", ""], ["Da Xu", "Richard Yi", ""]]}, {"id": "1306.3002", "submitter": "Xuhui Fan", "authors": "Xuhui Fan, Longbing Cao", "title": "A Convergence Theorem for the Graph Shift-type Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Shift (GS) algorithms are recently focused as a promising approach for\ndiscovering dense subgraphs in noisy data. However, there are no theoretical\nfoundations for proving the convergence of the GS Algorithm. In this paper, we\npropose a generic theoretical framework consisting of three key GS components:\nsimplex of generated sequence set, monotonic and continuous objective function\nand closed mapping. We prove that GS algorithms with such components can be\ntransformed to fit the Zangwill's convergence theorem, and the sequence set\ngenerated by the GS procedures always terminates at a local maximum, or at\nworst, contains a subsequence which converges to a local maximum of the\nsimilarity measure function. The framework is verified by expanding it to other\nGS-type algorithms and experimental results.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 01:00:21 GMT"}], "update_date": "2013-06-14", "authors_parsed": [["Fan", "Xuhui", ""], ["Cao", "Longbing", ""]]}, {"id": "1306.3003", "submitter": "Xuhui Fan", "authors": "Xuhui Fan, Yiling Zeng, Longbing Cao", "title": "Non-parametric Power-law Data Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has always been a great challenge for clustering algorithms to\nautomatically determine the cluster numbers according to the distribution of\ndatasets. Several approaches have been proposed to address this issue,\nincluding the recent promising work which incorporate Bayesian Nonparametrics\ninto the $k$-means clustering procedure. This approach shows simplicity in\nimplementation and solidity in theory, while it also provides a feasible way to\ninference in large scale datasets. However, several problems remains unsolved\nin this pioneering work, including the power-law data applicability, mechanism\nto merge centers to avoid the over-fitting problem, clustering order problem,\ne.t.c.. To address these issues, the Pitman-Yor Process based k-means (namely\n\\emph{pyp-means}) is proposed in this paper. Taking advantage of the Pitman-Yor\nProcess, \\emph{pyp-means} treats clusters differently by dynamically and\nadaptively changing the threshold to guarantee the generation of power-law\nclustering results. Also, one center agglomeration procedure is integrated into\nthe implementation to be able to merge small but close clusters and then\nadaptively determine the cluster number. With more discussion on the clustering\norder, the convergence proof, complexity analysis and extension to spectral\nclustering, our approach is compared with traditional clustering algorithm and\nvariational inference methods. The advantages and properties of pyp-means are\nvalidated by experiments on both synthetic datasets and real world datasets.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 01:20:50 GMT"}], "update_date": "2013-06-14", "authors_parsed": [["Fan", "Xuhui", ""], ["Zeng", "Yiling", ""], ["Cao", "Longbing", ""]]}, {"id": "1306.3058", "submitter": "Sebastien Paris", "authors": "S\\'ebastien Paris and Yann Doh and Herv\\'e Glotin and Xanadu Halkias\n  and Joseph Razik", "title": "Physeter catodon localization by sparse coding", "comments": "6 pages, 6 figures, workshop ICML4B in ICML 2013 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a spermwhale' localization architecture using jointly a\nbag-of-features (BoF) approach and machine learning framework. BoF methods are\nknown, especially in computer vision, to produce from a collection of local\nfeatures a global representation invariant to principal signal transformations.\nOur idea is to regress supervisely from these local features two rough\nestimates of the distance and azimuth thanks to some datasets where both\nacoustic events and ground-truth position are now available. Furthermore, these\nestimates can feed a particle filter system in order to obtain a precise\nspermwhale' position even in mono-hydrophone configuration. Anti-collision\nsystem and whale watching are considered applications of this work.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 09:05:08 GMT"}], "update_date": "2013-06-14", "authors_parsed": [["Paris", "S\u00e9bastien", ""], ["Doh", "Yann", ""], ["Glotin", "Herv\u00e9", ""], ["Halkias", "Xanadu", ""], ["Razik", "Joseph", ""]]}, {"id": "1306.3161", "submitter": "Maksim Lapin", "authors": "Maksim Lapin, Matthias Hein, Bernt Schiele", "title": "Learning Using Privileged Information: SVM+ and Weighted SVM", "comments": "18 pages, 8 figures; integrated reviewer comments, improved\n  typesetting", "journal-ref": "Neural Networks 53C (2014), pp. 95-108", "doi": "10.1016/j.neunet.2014.02.002", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior knowledge can be used to improve predictive performance of learning\nalgorithms or reduce the amount of data required for training. The same goal is\npursued within the learning using privileged information paradigm which was\nrecently introduced by Vapnik et al. and is aimed at utilizing additional\ninformation available only at training time -- a framework implemented by SVM+.\nWe relate the privileged information to importance weighting and show that the\nprior knowledge expressible with privileged features can also be encoded by\nweights associated with every training example. We show that a weighted SVM can\nalways replicate an SVM+ solution, while the converse is not true and we\nconstruct a counterexample highlighting the limitations of SVM+. Finally, we\ntouch on the problem of choosing weights for weighted SVMs when privileged\nfeatures are not available.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 16:36:07 GMT"}, {"version": "v2", "created": "Sun, 2 Mar 2014 13:57:55 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Lapin", "Maksim", ""], ["Hein", "Matthias", ""], ["Schiele", "Bernt", ""]]}, {"id": "1306.3162", "submitter": "Kishore Konda", "authors": "Kishore Reddy Konda, Roland Memisevic, Vincent Michalski", "title": "Learning to encode motion using spatio-temporal synchrony", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of learning to extract motion from videos. To this end,\nwe show that the detection of spatial transformations can be viewed as the\ndetection of synchrony between the image sequence and a sequence of features\nundergoing the motion we wish to detect. We show that learning about synchrony\nis possible using very fast, local learning rules, by introducing\nmultiplicative \"gating\" interactions between hidden units across frames. This\nmakes it possible to achieve competitive performance in a wide variety of\nmotion estimation tasks, using a small fraction of the time required to learn\nfeatures, and to outperform hand-crafted spatio-temporal features by a large\nmargin. We also show how learning about synchrony can be viewed as performing\ngreedy parameter estimation in the well-known motion energy model.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 16:46:03 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2013 20:14:24 GMT"}, {"version": "v3", "created": "Mon, 10 Feb 2014 11:19:23 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Konda", "Kishore Reddy", ""], ["Memisevic", "Roland", ""], ["Michalski", "Vincent", ""]]}, {"id": "1306.3203", "submitter": "Huahua Wang", "authors": "Huahua Wang and Arindam Banerjee", "title": "Bregman Alternating Direction Method of Multipliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mirror descent algorithm (MDA) generalizes gradient descent by using a\nBregman divergence to replace squared Euclidean distance. In this paper, we\nsimilarly generalize the alternating direction method of multipliers (ADMM) to\nBregman ADMM (BADMM), which allows the choice of different Bregman divergences\nto exploit the structure of problems. BADMM provides a unified framework for\nADMM and its variants, including generalized ADMM, inexact ADMM and Bethe ADMM.\n  We establish the global convergence and the $O(1/T)$ iteration complexity for\nBADMM. In some cases, BADMM can be faster than ADMM by a factor of\n$O(n/\\log(n))$. In solving the linear program of mass transportation problem,\nBADMM leads to massive parallelism and can easily run on GPU. BADMM is several\ntimes faster than highly optimized commercial software Gurobi.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 19:22:16 GMT"}, {"version": "v2", "created": "Tue, 1 Jul 2014 05:57:36 GMT"}, {"version": "v3", "created": "Tue, 8 Jul 2014 03:55:36 GMT"}], "update_date": "2014-07-09", "authors_parsed": [["Wang", "Huahua", ""], ["Banerjee", "Arindam", ""]]}, {"id": "1306.3212", "submitter": "Cho-Jui Hsieh Cho-Jui Hsieh", "authors": "Cho-Jui Hsieh, Matyas A. Sustik, Inderjit S. Dhillon and Pradeep\n  Ravikumar", "title": "Sparse Inverse Covariance Matrix Estimation Using Quadratic\n  Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The L1-regularized Gaussian maximum likelihood estimator (MLE) has been shown\nto have strong statistical guarantees in recovering a sparse inverse covariance\nmatrix, or alternatively the underlying graph structure of a Gaussian Markov\nRandom Field, from very limited samples. We propose a novel algorithm for\nsolving the resulting optimization problem which is a regularized\nlog-determinant program. In contrast to recent state-of-the-art methods that\nlargely use first order gradient information, our algorithm is based on\nNewton's method and employs a quadratic approximation, but with some\nmodifications that leverage the structure of the sparse Gaussian MLE problem.\nWe show that our method is superlinearly convergent, and present experimental\nresults using synthetic and real-world application data that demonstrate the\nconsiderable improvements in performance of our method when compared to other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 19:51:59 GMT"}], "update_date": "2013-06-14", "authors_parsed": [["Hsieh", "Cho-Jui", ""], ["Sustik", "Matyas A.", ""], ["Dhillon", "Inderjit S.", ""], ["Ravikumar", "Pradeep", ""]]}, {"id": "1306.3331", "submitter": "Muhammad Salman Asif", "authors": "M. Salman Asif and Justin Romberg", "title": "Sparse Recovery of Streaming Signals Using L1-Homotopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing methods for sparse signal recovery assume a static\nsystem: the unknown signal is a finite-length vector for which a fixed set of\nlinear measurements and a sparse representation basis are available and an\nL1-norm minimization program is solved for the reconstruction. However, the\nsame representation and reconstruction framework is not readily applicable in a\nstreaming system: the unknown signal changes over time, and it is measured and\nreconstructed sequentially over small time intervals.\n  In this paper, we discuss two such streaming systems and a homotopy-based\nalgorithm for quickly solving the associated L1-norm minimization programs: 1)\nRecovery of a smooth, time-varying signal for which, instead of using block\ntransforms, we use lapped orthogonal transforms for sparse representation. 2)\nRecovery of a sparse, time-varying signal that follows a linear dynamic model.\nFor both the systems, we iteratively process measurements over a sliding\ninterval and estimate sparse coefficients by solving a weighted L1-norm\nminimization program. Instead of solving a new L1 program from scratch at every\niteration, we use an available signal estimate as a starting point in a\nhomotopy formulation. Starting with a warm-start vector, our homotopy algorithm\nupdates the solution in a small number of computationally inexpensive steps as\nthe system changes. The homotopy algorithm presented in this paper is highly\nversatile as it can update the solution for the L1 problem in a number of\ndynamical settings. We demonstrate with numerical experiments that our proposed\nstreaming recovery framework outperforms the methods that represent and\nreconstruct a signal as independent, disjoint blocks, in terms of quality of\nreconstruction, and that our proposed homotopy-based updating scheme\noutperforms current state-of-the-art solvers in terms of the computation time\nand complexity.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2013 08:35:44 GMT"}], "update_date": "2013-06-17", "authors_parsed": [["Asif", "M. Salman", ""], ["Romberg", "Justin", ""]]}, {"id": "1306.3343", "submitter": "Zheng Pan", "authors": "Zheng Pan, Changshui Zhang", "title": "Relaxed Sparse Eigenvalue Conditions for Sparse Estimation via\n  Non-convex Regularized Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-convex regularizers usually improve the performance of sparse estimation\nin practice. To prove this fact, we study the conditions of sparse estimations\nfor the sharp concave regularizers which are a general family of non-convex\nregularizers including many existing regularizers. For the global solutions of\nthe regularized regression, our sparse eigenvalue based conditions are weaker\nthan that of L1-regularization for parameter estimation and sparseness\nestimation. For the approximate global and approximate stationary (AGAS)\nsolutions, almost the same conditions are also enough. We show that the desired\nAGAS solutions can be obtained by coordinate descent (CD) based methods.\nFinally, we perform some experiments to show the performance of CD methods on\ngiving AGAS solutions and the degree of weakness of the estimation conditions\nrequired by the sharp concave regularizers.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2013 09:10:00 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2013 06:25:21 GMT"}, {"version": "v3", "created": "Wed, 12 Feb 2014 09:27:57 GMT"}], "update_date": "2014-02-13", "authors_parsed": [["Pan", "Zheng", ""], ["Zhang", "Changshui", ""]]}, {"id": "1306.3392", "submitter": "Yue Wang", "authors": "Li Chen, Peter L. Choyke, Niya Wang, Robert Clarke, Zaver M.\n  Bhujwalla, Elizabeth M. C. Hillman, Yue Wang", "title": "Unsupervised deconvolution of dynamic imaging reveals intratumor\n  vascular heterogeneity", "comments": "Content: main manuscript, 31 pages", "journal-ref": null, "doi": "10.1371/journal.pone.0112143", "report-no": null, "categories": "q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intratumor heterogeneity is often manifested by vascular compartments with\ndistinct pharmacokinetics that cannot be resolved directly by in vivo dynamic\nimaging. We developed tissue-specific compartment modeling (TSCM), an\nunsupervised computational method of deconvolving dynamic imaging series from\nheterogeneous tumors that can improve vascular phenotyping in many biological\ncontexts. Applying TSCM to dynamic contrast-enhanced MRI of breast cancers\nrevealed characteristic intratumor vascular heterogeneity and therapeutic\nresponses that were otherwise undetectable.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2013 13:32:07 GMT"}, {"version": "v2", "created": "Wed, 3 Sep 2014 15:35:06 GMT"}, {"version": "v3", "created": "Thu, 4 Sep 2014 18:46:29 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Chen", "Li", ""], ["Choyke", "Peter L.", ""], ["Wang", "Niya", ""], ["Clarke", "Robert", ""], ["Bhujwalla", "Zaver M.", ""], ["Hillman", "Elizabeth M. C.", ""], ["Wang", "Yue", ""]]}, {"id": "1306.3409", "submitter": "Thomas B\\\"uhler", "authors": "Thomas B\\\"uhler, Syama Sundar Rangapuram, Simon Setzer, Matthias Hein", "title": "Constrained fractional set programs and their application in local\n  clustering and community detection", "comments": "Long version of paper accepted at ICML 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The (constrained) minimization of a ratio of set functions is a problem\nfrequently occurring in clustering and community detection. As these\noptimization problems are typically NP-hard, one uses convex or spectral\nrelaxations in practice. While these relaxations can be solved globally\noptimally, they are often too loose and thus lead to results far away from the\noptimum. In this paper we show that every constrained minimization problem of a\nratio of non-negative set functions allows a tight relaxation into an\nunconstrained continuous optimization problem. This result leads to a flexible\nframework for solving constrained problems in network analysis. While a\nglobally optimal solution for the resulting non-convex problem cannot be\nguaranteed, we outperform the loose convex or spectral relaxations by a large\nmargin on constrained local clustering problems.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2013 14:20:29 GMT"}], "update_date": "2013-06-17", "authors_parsed": [["B\u00fchler", "Thomas", ""], ["Rangapuram", "Syama Sundar", ""], ["Setzer", "Simon", ""], ["Hein", "Matthias", ""]]}, {"id": "1306.3474", "submitter": "Yijun Wang", "authors": "Yijun Wang", "title": "Classifying Single-Trial EEG during Motor Imagery with a Small Training\n  Set", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Before the operation of a motor imagery based brain-computer interface (BCI)\nadopting machine learning techniques, a cumbersome training procedure is\nunavoidable. The development of a practical BCI posed the challenge of\nclassifying single-trial EEG with a small training set. In this letter, we\naddressed this problem by employing a series of signal processing and machine\nlearning approaches to alleviate overfitting and obtained test accuracy similar\nto training accuracy on the datasets from BCI Competition III and our own\nexperiments.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2013 18:24:19 GMT"}], "update_date": "2013-06-17", "authors_parsed": [["Wang", "Yijun", ""]]}, {"id": "1306.3476", "submitter": "James Bergstra", "authors": "James Bergstra and David D. Cox", "title": "Hyperparameter Optimization and Boosting for Classifying Facial\n  Expressions: How good can a \"Null\" Model be?", "comments": "Presented at the Workshop on Representation and Learning, ICML 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the goals of the ICML workshop on representation and learning is to\nestablish benchmark scores for a new data set of labeled facial expressions.\nThis paper presents the performance of a \"Null\" model consisting of\nconvolutions with random weights, PCA, pooling, normalization, and a linear\nreadout. Our approach focused on hyperparameter optimization rather than novel\nmodel components. On the Facial Expression Recognition Challenge held by the\nKaggle website, our hyperparameter optimization approach achieved a score of\n60% accuracy on the test data. This paper also introduces a new ensemble\nconstruction variant that combines hyperparameter optimization with the\nconstruction of ensembles. This algorithm constructed an ensemble of four\nmodels that scored 65.5% accuracy. These scores rank 12th and 5th respectively\namong the 56 challenge participants. It is worth noting that our approach was\ndeveloped prior to the release of the data set, and applied without\nmodification; our strong competition performance suggests that the TPE\nhyperparameter optimization algorithm and domain expertise encoded in our Null\nmodel can generalize to new image classification data sets.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2013 18:28:52 GMT"}], "update_date": "2013-06-17", "authors_parsed": [["Bergstra", "James", ""], ["Cox", "David D.", ""]]}, {"id": "1306.3494", "submitter": "Jelena Bradic", "authors": "Jelena Bradic", "title": "Randomized maximum-contrast selection: subagging for large-scale\n  regression", "comments": null, "journal-ref": "Electron. J. Statist. Volume 10, Number 1 (2016), 121-170", "doi": "10.1214/15-EJS1085", "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a very general method for sparse and large-scale variable\nselection. The large-scale regression settings is such that both the number of\nparameters and the number of samples are extremely large. The proposed method\nis based on careful combination of penalized estimators, each applied to a\nrandom projection of the sample space into a low-dimensional space. In one\nspecial case that we study in detail, the random projections are divided into\nnon-overlapping blocks; each consisting of only a small portion of the original\ndata. Within each block we select the projection yielding the smallest\nout-of-sample error. Our random ensemble estimator then aggregates the results\naccording to new maximal-contrast voting scheme to determine the final selected\nset. Our theoretical results illuminate the effect on performance of increasing\nthe number of non-overlapping blocks. Moreover, we demonstrate that statistical\noptimality is retained along with the computational speedup. The proposed\nmethod achieves minimax rates for approximate recovery over all estimators\nusing the full set of samples. Furthermore, our theoretical results allow the\nnumber of subsamples to grow with the subsample size and do not require\nirrepresentable condition. The estimator is also compared empirically with\nseveral other popular high-dimensional estimators via an extensive simulation\nstudy, which reveals its excellent finite-sample performance.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2013 19:39:54 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2013 07:01:09 GMT"}, {"version": "v3", "created": "Fri, 19 Sep 2014 00:43:48 GMT"}, {"version": "v4", "created": "Wed, 1 Jul 2015 15:25:51 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Bradic", "Jelena", ""]]}, {"id": "1306.3530", "submitter": "Y. Kenan Yilmaz Dr.", "authors": "Y. Kenan Yilmaz", "title": "Generalized Beta Divergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper generalizes beta divergence beyond its classical form associated\nwith power variance functions of Tweedie models. Generalized form is\nrepresented by a compact definite integral as a function of variance function\nof the exponential dispersion model. This compact integral form simplifies\nderivations of many properties such as scaling, translation and expectation of\nthe beta divergence. Further, we show that beta divergence and (half of) the\nstatistical deviance are equivalent measures.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2013 23:41:58 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2013 11:50:23 GMT"}], "update_date": "2013-06-19", "authors_parsed": [["Yilmaz", "Y. Kenan", ""]]}, {"id": "1306.3558", "submitter": "Fabrizio Angiulli", "authors": "Fabrizio Angiulli and Fabio Fassetti and Luigi Palopoli and Giuseppe\n  Manco", "title": "Outlying Property Detection with Numerical Attributes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outlying property detection problem is the problem of discovering the\nproperties distinguishing a given object, known in advance to be an outlier in\na database, from the other database objects. In this paper, we analyze the\nproblem within a context where numerical attributes are taken into account,\nwhich represents a relevant case left open in the literature. We introduce a\nmeasure to quantify the degree the outlierness of an object, which is\nassociated with the relative likelihood of the value, compared to the to the\nrelative likelihood of other objects in the database. As a major contribution,\nwe present an efficient algorithm to compute the outlierness relative to\nsignificant subsets of the data. The latter subsets are characterized in a\n\"rule-based\" fashion, and hence the basis for the underlying explanation of the\noutlierness.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2013 08:52:46 GMT"}], "update_date": "2013-06-18", "authors_parsed": [["Angiulli", "Fabrizio", ""], ["Fassetti", "Fabio", ""], ["Palopoli", "Luigi", ""], ["Manco", "Giuseppe", ""]]}, {"id": "1306.3574", "submitter": "Garvesh Raskutti", "authors": "Garvesh Raskutti and Martin J. Wainwright and Bin Yu", "title": "Early stopping and non-parametric regression: An optimal data-dependent\n  stopping rule", "comments": "29 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The strategy of early stopping is a regularization technique based on\nchoosing a stopping time for an iterative algorithm. Focusing on non-parametric\nregression in a reproducing kernel Hilbert space, we analyze the early stopping\nstrategy for a form of gradient-descent applied to the least-squares loss\nfunction. We propose a data-dependent stopping rule that does not involve\nhold-out or cross-validation data, and we prove upper bounds on the squared\nerror of the resulting function estimate, measured in either the $L^2(P)$ and\n$L^2(P_n)$ norm. These upper bounds lead to minimax-optimal rates for various\nkernel classes, including Sobolev smoothness classes and other forms of\nreproducing kernel Hilbert spaces. We show through simulation that our stopping\nrule compares favorably to two other stopping rules, one based on hold-out data\nand the other based on Stein's unbiased risk estimate. We also establish a\ntight connection between our early stopping strategy and the solution path of a\nkernel ridge regression estimator.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2013 12:52:38 GMT"}], "update_date": "2013-06-18", "authors_parsed": [["Raskutti", "Garvesh", ""], ["Wainwright", "Martin J.", ""], ["Yu", "Bin", ""]]}, {"id": "1306.3627", "submitter": "Pablo Andrade", "authors": "Pablo de Morais Andrade, Julio Michael Stern, Carlos Alberto de\n  Bragan\\c{c}a Pereira", "title": "Bayesian test of significance for conditional independence: The\n  multinomial model", "comments": "24 pages, 33 figures", "journal-ref": null, "doi": "10.3390/e16031376", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional independence tests (CI tests) have received special attention\nlately in Machine Learning and Computational Intelligence related literature as\nan important indicator of the relationship among the variables used by their\nmodels. In the field of Probabilistic Graphical Models (PGM)--which includes\nBayesian Networks (BN) models--CI tests are especially important for the task\nof learning the PGM structure from data. In this paper, we propose the Full\nBayesian Significance Test (FBST) for tests of conditional independence for\ndiscrete datasets. FBST is a powerful Bayesian test for precise hypothesis, as\nan alternative to frequentist's significance tests (characterized by the\ncalculation of the \\emph{p-value}).\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2013 05:21:19 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Andrade", "Pablo de Morais", ""], ["Stern", "Julio Michael", ""], ["Pereira", "Carlos Alberto de Bragan\u00e7a", ""]]}, {"id": "1306.3690", "submitter": "Robert Krauthgamer", "authors": "Robert Krauthgamer, Boaz Nadler, Dan Vilenchik", "title": "Do semidefinite relaxations solve sparse PCA up to the information\n  limit?", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1310 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 3, 1300-1322", "doi": "10.1214/15-AOS1310", "report-no": "IMS-AOS-AOS1310", "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the leading principal components of data, assuming they are\nsparse, is a central task in modern high-dimensional statistics. Many\nalgorithms were developed for this sparse PCA problem, from simple diagonal\nthresholding to sophisticated semidefinite programming (SDP) methods. A key\ntheoretical question is under what conditions can such algorithms recover the\nsparse principal components? We study this question for a single-spike model\nwith an $\\ell_0$-sparse eigenvector, in the asymptotic regime as dimension $p$\nand sample size $n$ both tend to infinity. Amini and Wainwright [Ann. Statist.\n37 (2009) 2877-2921] proved that for sparsity levels $k\\geq\\Omega(n/\\log p)$,\nno algorithm, efficient or not, can reliably recover the sparse eigenvector. In\ncontrast, for $k\\leq O(\\sqrt{n/\\log p})$, diagonal thresholding is consistent.\nIt was further conjectured that an SDP approach may close this gap between\ncomputational and information limits. We prove that when\n$k\\geq\\Omega(\\sqrt{n})$, the proposed SDP approach, at least in its standard\nusage, cannot recover the sparse spike. In fact, we conjecture that in the\nsingle-spike model, no computationally-efficient algorithm can recover a spike\nof $\\ell_0$-sparsity $k\\geq\\Omega(\\sqrt{n})$. Finally, we present empirical\nresults suggesting that up to sparsity levels $k=O(\\sqrt{n})$, recovery is\npossible by a simple covariance thresholding algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2013 17:40:09 GMT"}, {"version": "v2", "created": "Sun, 21 Sep 2014 13:02:37 GMT"}, {"version": "v3", "created": "Mon, 12 Jan 2015 18:50:07 GMT"}, {"version": "v4", "created": "Wed, 3 Jun 2015 08:30:11 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Krauthgamer", "Robert", ""], ["Nadler", "Boaz", ""], ["Vilenchik", "Dan", ""]]}, {"id": "1306.3706", "submitter": "William Fithian", "authors": "William Fithian, Trevor Hastie", "title": "Local case-control sampling: Efficient subsampling in imbalanced data\n  sets", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1220 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 5, 1693-1724", "doi": "10.1214/14-AOS1220", "report-no": "IMS-AOS-AOS1220", "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For classification problems with significant class imbalance, subsampling can\nreduce computational costs at the price of inflated variance in estimating\nmodel parameters. We propose a method for subsampling efficiently for logistic\nregression by adjusting the class balance locally in feature space via an\naccept-reject scheme. Our method generalizes standard case-control sampling,\nusing a pilot estimate to preferentially select examples whose responses are\nconditionally rare given their features. The biased subsampling is corrected by\na post-hoc analytic adjustment to the parameters. The method is simple and\nrequires one parallelizable scan over the full data set. Standard case-control\nsampling is inconsistent under model misspecification for the population\nrisk-minimizing coefficients $\\theta^*$. By contrast, our estimator is\nconsistent for $\\theta^*$ provided that the pilot estimate is. Moreover, under\ncorrect specification and with a consistent, independent pilot estimate, our\nestimator has exactly twice the asymptotic variance of the full-sample MLE -\neven if the selected subsample comprises a miniscule fraction of the full data\nset, as happens when the original data are severely imbalanced. The factor of\ntwo improves to $1+\\frac{1}{c}$ if we multiply the baseline acceptance\nprobabilities by $c>1$ (and weight points with acceptance probability greater\nthan 1), taking roughly $\\frac{1+c}{2}$ times as many data points into the\nsubsample. Experiments on simulated and real data show that our method can\nsubstantially outperform standard case-control subsampling.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2013 21:18:12 GMT"}, {"version": "v2", "created": "Tue, 23 Sep 2014 08:53:42 GMT"}], "update_date": "2014-09-24", "authors_parsed": [["Fithian", "William", ""], ["Hastie", "Trevor", ""]]}, {"id": "1306.3729", "submitter": "Arun Tejasvi Chaganty", "authors": "Arun Tejasvi Chaganty and Percy Liang", "title": "Spectral Experts for Estimating Mixtures of Linear Regressions", "comments": "Accepted at ICML 2013. Includes supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminative latent-variable models are typically learned using EM or\ngradient-based optimization, which suffer from local optima. In this paper, we\ndevelop a new computationally efficient and provably consistent estimator for a\nmixture of linear regressions, a simple instance of a discriminative\nlatent-variable model. Our approach relies on a low-rank linear regression to\nrecover a symmetric tensor, which can be factorized into the parameters using a\ntensor power method. We prove rates of convergence for our estimator and\nprovide an empirical evaluation illustrating its strengths relative to local\noptimization (EM).\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2013 03:02:05 GMT"}], "update_date": "2013-06-18", "authors_parsed": [["Chaganty", "Arun Tejasvi", ""], ["Liang", "Percy", ""]]}, {"id": "1306.3809", "submitter": "Mihailo Stojnic", "authors": "Mihailo Stojnic", "title": "Spherical perceptron as a storage memory with limited errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math-ph math.MP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been known for a long time that the classical spherical perceptrons\ncan be used as storage memories. Seminal work of Gardner, \\cite{Gar88}, started\nan analytical study of perceptrons storage abilities. Many of the Gardner's\npredictions obtained through statistical mechanics tools have been rigorously\njustified. Among the most important ones are of course the storage capacities.\nThe first rigorous confirmations were obtained in \\cite{SchTir02,SchTir03} for\nthe storage capacity of the so-called positive spherical perceptron. These were\nlater reestablished in \\cite{TalBook} and a bit more recently in\n\\cite{StojnicGardGen13}. In this paper we consider a variant of the spherical\nperceptron that operates as a storage memory but allows for a certain fraction\nof errors. In Gardner's original work the statistical mechanics predictions in\nthis directions were presented sa well. Here, through a mathematically rigorous\nanalysis, we confirm that the Gardner's predictions in this direction are in\nfact provable upper bounds on the true values of the storage capacity.\nMoreover, we then present a mechanism that can be used to lower these bounds.\nNumerical results that we present indicate that the Garnder's storage capacity\npredictions may, in a fairly wide range of parameters, be not that far away\nfrom the true values.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2013 10:59:50 GMT"}], "update_date": "2013-06-18", "authors_parsed": [["Stojnic", "Mihailo", ""]]}, {"id": "1306.3862", "submitter": "Pierre Alquier", "authors": "Pierre Alquier", "title": "Bayesian methods for low-rank matrix estimation: short survey and\n  theoretical study", "comments": "Corrected version of a paper published in the proceedings of ALT 2013", "journal-ref": null, "doi": "10.1007/978-3-642-40935-6_22", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of low-rank matrix estimation recently received a lot of\nattention due to challenging applications. A lot of work has been done on\nrank-penalized methods and convex relaxation, both on the theoretical and\napplied sides. However, only a few papers considered Bayesian estimation. In\nthis paper, we review the different type of priors considered on matrices to\nfavour low-rank. We also prove that the obtained Bayesian estimators, under\nsuitable assumptions, enjoys the same optimality properties as the ones based\non penalization.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2013 13:57:05 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 12:49:54 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Alquier", "Pierre", ""]]}, {"id": "1306.3905", "submitter": "Julien Audiffren", "authors": "Julien Audiffren (LIF), Hachem Kadri (LIF)", "title": "Stability of Multi-Task Kernel Regression Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the stability properties of nonlinear multi-task regression in\nreproducing Hilbert spaces with operator-valued kernels. Such kernels, a.k.a.\nmulti-task kernels, are appropriate for learning prob- lems with nonscalar\noutputs like multi-task learning and structured out- put prediction. We show\nthat multi-task kernel regression algorithms are uniformly stable in the\ngeneral case of infinite-dimensional output spaces. We then derive under mild\nassumption on the kernel generaliza- tion bounds of such algorithms, and we\nshow their consistency even with non Hilbert-Schmidt operator-valued kernels .\nWe demonstrate how to apply the results to various multi-task kernel regression\nmethods such as vector-valued SVR and functional ridge regression.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2013 15:44:30 GMT"}], "update_date": "2013-06-18", "authors_parsed": [["Audiffren", "Julien", "", "LIF"], ["Kadri", "Hachem", "", "LIF"]]}, {"id": "1306.3917", "submitter": "Matthew Malloy", "authors": "Kevin Jamieson, Matthew Malloy, Robert Nowak, Sebastien Bubeck", "title": "On Finding the Largest Mean Among Many", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling from distributions to find the one with the largest mean arises in a\nbroad range of applications, and it can be mathematically modeled as a\nmulti-armed bandit problem in which each distribution is associated with an\narm. This paper studies the sample complexity of identifying the best arm\n(largest mean) in a multi-armed bandit problem. Motivated by large-scale\napplications, we are especially interested in identifying situations where the\ntotal number of samples that are necessary and sufficient to find the best arm\nscale linearly with the number of arms. We present a single-parameter\nmulti-armed bandit model that spans the range from linear to superlinear sample\ncomplexity. We also give a new algorithm for best arm identification, called\nPRISM, with linear sample complexity for a wide range of mean distributions.\nThe algorithm, like most exploration procedures for multi-armed bandits, is\nadaptive in the sense that the next arms to sample are selected based on\nprevious samples. We compare the sample complexity of adaptive procedures with\nsimpler non-adaptive procedures using new lower bounds. For many problem\ninstances, the increased sample complexity required by non-adaptive procedures\nis a polynomial factor of the number of arms.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2013 16:24:13 GMT"}], "update_date": "2013-06-18", "authors_parsed": [["Jamieson", "Kevin", ""], ["Malloy", "Matthew", ""], ["Nowak", "Robert", ""], ["Bubeck", "Sebastien", ""]]}, {"id": "1306.4032", "submitter": "Anne-Marie Lyne", "authors": "Anne-Marie Lyne, Mark Girolami, Yves Atchad\\'e, Heiko Strathmann,\n  Daniel Simpson", "title": "On Russian Roulette Estimates for Bayesian Inference with\n  Doubly-Intractable Likelihoods", "comments": "Published at http://dx.doi.org/10.1214/15-STS523 in the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2015, Vol. 30, No. 4, 443-467", "doi": "10.1214/15-STS523", "report-no": "IMS-STS-STS523", "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large number of statistical models are \"doubly-intractable\": the likelihood\nnormalising term, which is a function of the model parameters, is intractable,\nas well as the marginal likelihood (model evidence). This means that standard\ninference techniques to sample from the posterior, such as Markov chain Monte\nCarlo (MCMC), cannot be used. Examples include, but are not confined to,\nmassive Gaussian Markov random fields, autologistic models and Exponential\nrandom graph models. A number of approximate schemes based on MCMC techniques,\nApproximate Bayesian computation (ABC) or analytic approximations to the\nposterior have been suggested, and these are reviewed here. Exact MCMC schemes,\nwhich can be applied to a subset of doubly-intractable distributions, have also\nbeen developed and are described in this paper. As yet, no general method\nexists which can be applied to all classes of models with doubly-intractable\nposteriors. In addition, taking inspiration from the Physics literature, we\nstudy an alternative method based on representing the intractable likelihood as\nan infinite series. Unbiased estimates of the likelihood can then be obtained\nby finite time stochastic truncation of the series via Russian Roulette\nsampling, although the estimates are not necessarily positive. Results from the\nQuantum Chromodynamics literature are exploited to allow the use of possibly\nnegative estimates in a pseudo-marginal MCMC scheme such that expectations with\nrespect to the posterior distribution are preserved. The methodology is\nreviewed on well-known examples such as the parameters in Ising models, the\nposterior for Fisher-Bingham distributions on the $d$-Sphere and a large-scale\nGaussian Markov Random Field model describing the Ozone Column data. This leads\nto a critical assessment of the strengths and weaknesses of the methodology\nwith pointers to ongoing research.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2013 22:04:05 GMT"}, {"version": "v2", "created": "Tue, 8 Jul 2014 11:07:23 GMT"}, {"version": "v3", "created": "Wed, 4 Feb 2015 18:29:05 GMT"}, {"version": "v4", "created": "Thu, 10 Dec 2015 10:47:34 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Lyne", "Anne-Marie", ""], ["Girolami", "Mark", ""], ["Atchad\u00e9", "Yves", ""], ["Strathmann", "Heiko", ""], ["Simpson", "Daniel", ""]]}, {"id": "1306.4103", "submitter": "Ilya Soloveychik", "authors": "Ilya Soloveychik, Ami Wiesel", "title": "Group Symmetry and non-Gaussian Covariance Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider robust covariance estimation with group symmetry constraints.\nNon-Gaussian covariance estimation, e.g., Tyler scatter estimator and\nMultivariate Generalized Gaussian distribution methods, usually involve\nnon-convex minimization problems. Recently, it was shown that the underlying\nprinciple behind their success is an extended form of convexity over the\ngeodesics in the manifold of positive definite matrices. A modern approach to\nimprove estimation accuracy is to exploit prior knowledge via additional\nconstraints, e.g., restricting the attention to specific classes of covariances\nwhich adhere to prior symmetry structures. In this paper, we prove that such\ngroup symmetry constraints are also geodesically convex and can therefore be\nincorporated into various non-Gaussian covariance estimators. Practical\nexamples of such sets include: circulant, persymmetric and complex/quaternion\nproper structures. We provide a simple numerical technique for finding maximum\nlikelihood estimates under such constraints, and demonstrate their performance\nadvantage using synthetic experiments.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2013 08:40:40 GMT"}], "update_date": "2013-06-19", "authors_parsed": [["Soloveychik", "Ilya", ""], ["Wiesel", "Ami", ""]]}, {"id": "1306.4152", "submitter": "Maumita Bhattacharya", "authors": "Maumita Bhattacharya", "title": "Bioclimating Modelling: A Machine Learning Perspective", "comments": "8 pages, In the Proceedings of the 2012 International Joint\n  Conferences on Computer, Information, and Systems Sciences, and Engineering\n  (CISSE 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning (ML) approaches are widely used to generate bioclimatic\nmodels for prediction of geographic range of organism as a function of climate.\nApplications such as prediction of range shift in organism, range of invasive\nspecies influenced by climate change are important parameters in understanding\nthe impact of climate change. However, success of machine learning-based\napproaches depends on a number of factors. While it can be safely said that no\nparticular ML technique can be effective in all applications and success of a\ntechnique is predominantly dependent on the application or the type of the\nproblem, it is useful to understand their behaviour to ensure informed choice\nof techniques. This paper presents a comprehensive review of machine\nlearning-based bioclimatic model generation and analyses the factors\ninfluencing success of such models. Considering the wide use of statistical\ntechniques, in our discussion we also include conventional statistical\ntechniques used in bioclimatic modelling.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2013 11:42:03 GMT"}], "update_date": "2013-06-19", "authors_parsed": [["Bhattacharya", "Maumita", ""]]}, {"id": "1306.4375", "submitter": "Mihailo Stojnic", "authors": "Mihailo Stojnic", "title": "Discrete perceptrons", "comments": "arXiv admin note: substantial text overlap with arXiv:1306.3809,\n  arXiv:1306.3980, arXiv:1306.3979", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math-ph math.MP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perceptrons have been known for a long time as a promising tool within the\nneural networks theory. The analytical treatment for a special class of\nperceptrons started in seminal work of Gardner \\cite{Gar88}. Techniques\ninitially employed to characterize perceptrons relied on a statistical\nmechanics approach. Many of such predictions obtained in \\cite{Gar88} (and in a\nfollow-up \\cite{GarDer88}) were later on established rigorously as mathematical\nfacts (see, e.g.\n\\cite{SchTir02,SchTir03,TalBook,StojnicGardGen13,StojnicGardSphNeg13,StojnicGardSphErr13}).\nThese typically related to spherical perceptrons. A lot of work has been done\nrelated to various other types of perceptrons. Among the most challenging ones\nare what we will refer to as the discrete perceptrons. An introductory\nstatistical mechanics treatment of such perceptrons was given in\n\\cite{GutSte90}. Relying on results of \\cite{Gar88}, \\cite{GutSte90}\ncharacterized many of the features of several types of discrete perceptrons. We\nin this paper, consider a similar subclass of discrete perceptrons and provide\na mathematically rigorous set of results related to their performance. As it\nwill turn out, many of the statistical mechanics predictions obtained for\ndiscrete predictions will in fact appear as mathematically provable bounds.\nThis will in a way emulate a similar type of behavior we observed in\n\\cite{StojnicGardGen13,StojnicGardSphNeg13,StojnicGardSphErr13} when studying\nspherical perceptrons.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2013 11:05:26 GMT"}], "update_date": "2013-06-20", "authors_parsed": [["Stojnic", "Mihailo", ""]]}, {"id": "1306.4391", "submitter": "Akshay Soni", "authors": "Akshay Soni and Jarvis Haupt", "title": "On the Fundamental Limits of Recovering Tree Sparse Vectors from Noisy\n  Linear Measurements", "comments": "33 pages, 5 figures, IEEE Transactions on Information Theory\n  (accepted for publication)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent breakthrough results in compressive sensing (CS) have established that\nmany high dimensional signals can be accurately recovered from a relatively\nsmall number of non-adaptive linear observations, provided that the signals\npossess a sparse representation in some basis. Subsequent efforts have shown\nthat the performance of CS can be improved by exploiting additional structure\nin the locations of the nonzero signal coefficients during inference, or by\nutilizing some form of data-dependent adaptive measurement focusing during the\nsensing process. To our knowledge, our own previous work was the first to\nestablish the potential benefits that can be achieved when fusing the notions\nof adaptive sensing and structured sparsity -- that work examined the task of\nsupport recovery from noisy linear measurements, and established that an\nadaptive sensing strategy specifically tailored to signals that are tree-sparse\ncan significantly outperform adaptive and non-adaptive sensing strategies that\nare agnostic to the underlying structure. In this work we establish fundamental\nperformance limits for the task of support recovery of tree-sparse signals from\nnoisy measurements, in settings where measurements may be obtained either\nnon-adaptively (using a randomized Gaussian measurement strategy motivated by\ninitial CS investigations) or by any adaptive sensing strategy. Our main\nresults here imply that the adaptive tree sensing procedure analyzed in our\nprevious work is nearly optimal, in the sense that no other sensing and\nestimation strategy can perform fundamentally better for identifying the\nsupport of tree-sparse signals.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2013 23:27:17 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2013 02:58:26 GMT"}], "update_date": "2013-10-17", "authors_parsed": [["Soni", "Akshay", ""], ["Haupt", "Jarvis", ""]]}, {"id": "1306.4410", "submitter": "Junhui Wang", "authors": "Junhui Wang", "title": "Joint estimation of sparse multivariate regression and conditional\n  graphical models", "comments": "29 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate regression model is a natural generalization of the classical\nunivari- ate regression model for fitting multiple responses. In this paper, we\npropose a high- dimensional multivariate conditional regression model for\nconstructing sparse estimates of the multivariate regression coefficient matrix\nthat accounts for the dependency struc- ture among the multiple responses. The\nproposed method decomposes the multivariate regression problem into a series of\npenalized conditional log-likelihood of each response conditioned on the\ncovariates and other responses. It allows simultaneous estimation of the sparse\nregression coefficient matrix and the sparse inverse covariance matrix. The\nasymptotic selection consistency and normality are established for the\ndiverging dimension of the covariates and number of responses. The\neffectiveness of the pro- posed method is also demonstrated in a variety of\nsimulated examples as well as an application to the Glioblastoma multiforme\ncancer data.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2013 01:56:29 GMT"}], "update_date": "2016-11-26", "authors_parsed": [["Wang", "Junhui", ""]]}, {"id": "1306.4447", "submitter": "Antonio Villani", "authors": "Giuseppe Ateniese, Giovanni Felici, Luigi V. Mancini, Angelo\n  Spognardi, Antonio Villani, Domenico Vitali", "title": "Hacking Smart Machines with Smarter Ones: How to Extract Meaningful Data\n  from Machine Learning Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) algorithms are used to train computers to perform a\nvariety of complex tasks and improve with experience. Computers learn how to\nrecognize patterns, make unintended decisions, or react to a dynamic\nenvironment. Certain trained machines may be more effective than others because\nthey are based on more suitable ML algorithms or because they were trained\nthrough superior training sets. Although ML algorithms are known and publicly\nreleased, training sets may not be reasonably ascertainable and, indeed, may be\nguarded as trade secrets. While much research has been performed about the\nprivacy of the elements of training sets, in this paper we focus our attention\non ML classifiers and on the statistical information that can be unconsciously\nor maliciously revealed from them. We show that it is possible to infer\nunexpected but useful information from ML classifiers. In particular, we build\na novel meta-classifier and train it to hack other classifiers, obtaining\nmeaningful information about their training sets. This kind of information\nleakage can be exploited, for example, by a vendor to build more effective\nclassifiers or to simply acquire trade secrets from a competitor's apparatus,\npotentially violating its intellectual property rights.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2013 07:51:49 GMT"}], "update_date": "2013-06-20", "authors_parsed": [["Ateniese", "Giuseppe", ""], ["Felici", "Giovanni", ""], ["Mancini", "Luigi V.", ""], ["Spognardi", "Angelo", ""], ["Villani", "Antonio", ""], ["Vitali", "Domenico", ""]]}, {"id": "1306.4650", "submitter": "Julien Mairal", "authors": "Julien Mairal (INRIA Grenoble Rh\\^one-Alpes / LJK Laboratoire Jean\n  Kuntzmann)", "title": "Stochastic Majorization-Minimization Algorithms for Large-Scale\n  Optimization", "comments": "accepted for publication for Neural Information Processing Systems\n  (NIPS) 2013. This is the 9-pages version followed by 16 pages of appendices.\n  The title has changed compared to the first technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Majorization-minimization algorithms consist of iteratively minimizing a\nmajorizing surrogate of an objective function. Because of its simplicity and\nits wide applicability, this principle has been very popular in statistics and\nin signal processing. In this paper, we intend to make this principle scalable.\nWe introduce a stochastic majorization-minimization scheme which is able to\ndeal with large-scale or possibly infinite data sets. When applied to convex\noptimization problems under suitable assumptions, we show that it achieves an\nexpected convergence rate of $O(1/\\sqrt{n})$ after $n$ iterations, and of\n$O(1/n)$ for strongly convex functions. Equally important, our scheme almost\nsurely converges to stationary points for a large class of non-convex problems.\nWe develop several efficient algorithms based on our framework. First, we\npropose a new stochastic proximal gradient method, which experimentally matches\nstate-of-the-art solvers for large-scale $\\ell_1$-logistic regression. Second,\nwe develop an online DC programming algorithm for non-convex sparse estimation.\nFinally, we demonstrate the effectiveness of our approach for solving\nlarge-scale structured matrix factorization problems.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2013 19:21:48 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2013 12:29:41 GMT"}], "update_date": "2013-09-11", "authors_parsed": [["Mairal", "Julien", "", "INRIA Grenoble Rh\u00f4ne-Alpes / LJK Laboratoire Jean\n  Kuntzmann"]]}, {"id": "1306.4943", "submitter": "Gordon Belot", "authors": "Gordon Belot", "title": "Failure of Calibration is Typical", "comments": "Forthcoming in Statistics and Probability Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Schervish (1985b) showed that every forecasting system is noncalibrated for\nuncountably many data sequences that it might see. This result is strengthened\nhere: from a topological point of view, failure of calibration is typical and\ncalibration rare. Meanwhile, Bayesian forecasters are certain that they are\ncalibrated---this invites worries about the connection between Bayesianism and\nrationality.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2013 17:45:09 GMT"}], "update_date": "2013-06-21", "authors_parsed": [["Belot", "Gordon", ""]]}, {"id": "1306.4960", "submitter": "Zhaoran Wang", "authors": "Zhaoran Wang, Han Liu, Tong Zhang", "title": "Optimal computational and statistical rates of convergence for sparse\n  nonconvex learning problems", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1238 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 6, 2164-2201", "doi": "10.1214/14-AOS1238", "report-no": "IMS-AOS-AOS1238", "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide theoretical analysis of the statistical and computational\nproperties of penalized $M$-estimators that can be formulated as the solution\nto a possibly nonconvex optimization problem. Many important estimators fall in\nthis category, including least squares regression with nonconvex\nregularization, generalized linear models with nonconvex regularization and\nsparse elliptical random design regression. For these problems, it is\nintractable to calculate the global solution due to the nonconvex formulation.\nIn this paper, we propose an approximate regularization path-following method\nfor solving a variety of learning problems with nonconvex objective functions.\nUnder a unified analytic framework, we simultaneously provide explicit\nstatistical and computational rates of convergence for any local solution\nattained by the algorithm. Computationally, our algorithm attains a global\ngeometric rate of convergence for calculating the full regularization path,\nwhich is optimal among all first-order algorithms. Unlike most existing methods\nthat only attain geometric rates of convergence for one single regularization\nparameter, our algorithm calculates the full regularization path with the same\niteration complexity. In particular, we provide a refined iteration complexity\nbound to sharply characterize the performance of each stage along the\nregularization path. Statistically, we provide sharp sample complexity analysis\nfor all the approximate local solutions along the regularization path. In\nparticular, our analysis improves upon existing results by providing a more\nrefined sample complexity bound as well as an exact support recovery result for\nthe final estimator. These results show that the final estimator attains an\noracle statistical property due to the usage of nonconvex penalty.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2013 19:04:28 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2013 18:17:33 GMT"}, {"version": "v3", "created": "Fri, 27 Dec 2013 09:06:47 GMT"}, {"version": "v4", "created": "Tue, 4 Nov 2014 10:38:36 GMT"}, {"version": "v5", "created": "Tue, 27 Jan 2015 03:35:49 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Wang", "Zhaoran", ""], ["Liu", "Han", ""], ["Zhang", "Tong", ""]]}, {"id": "1306.5056", "submitter": "Tyler Sanderson", "authors": "Tyler Sanderson and Clayton Scott", "title": "Class Proportion Estimation with Application to Multiclass Anomaly\n  Rejection", "comments": "Accepted to AISTATS 2014. 15 pages. 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses two classification problems that fall under the heading\nof domain adaptation, wherein the distributions of training and testing\nexamples differ. The first problem studied is that of class proportion\nestimation, which is the problem of estimating the class proportions in an\nunlabeled testing data set given labeled examples of each class. Compared to\nprevious work on this problem, our approach has the novel feature that it does\nnot require labeled training data from one of the classes. This property allows\nus to address the second domain adaptation problem, namely, multiclass anomaly\nrejection. Here, the goal is to design a classifier that has the option of\nassigning a \"reject\" label, indicating that the instance did not arise from a\nclass present in the training data. We establish consistent learning strategies\nfor both of these domain adaptation problems, which to our knowledge are the\nfirst of their kind. We also implement the class proportion estimation\ntechnique and demonstrate its performance on several benchmark data sets.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2013 06:25:54 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2013 03:36:55 GMT"}, {"version": "v3", "created": "Sat, 22 Feb 2014 08:58:10 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Sanderson", "Tyler", ""], ["Scott", "Clayton", ""]]}, {"id": "1306.5310", "submitter": "C\\'edric Richard", "authors": "Wei Gao and Jie Chen and C\\'edric Richard and Jianguo Huang", "title": "Online dictionary learning for kernel LMS. Analysis and forward-backward\n  splitting algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive filtering algorithms operating in reproducing kernel Hilbert spaces\nhave demonstrated superiority over their linear counterpart for nonlinear\nsystem identification. Unfortunately, an undesirable characteristic of these\nmethods is that the order of the filters grows linearly with the number of\ninput data. This dramatically increases the computational burden and memory\nrequirement. A variety of strategies based on dictionary learning have been\nproposed to overcome this severe drawback. Few, if any, of these works analyze\nthe problem of updating the dictionary in a time-varying environment. In this\npaper, we present an analytical study of the convergence behavior of the\nGaussian least-mean-square algorithm in the case where the statistics of the\ndictionary elements only partially match the statistics of the input data. This\nallows us to emphasize the need for updating the dictionary in an online way,\nby discarding the obsolete elements and adding appropriate ones. We introduce a\nkernel least-mean-square algorithm with L1-norm regularization to automatically\nperform this task. The stability in the mean of this method is analyzed, and\nits performance is tested with experiments.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2013 11:11:20 GMT"}], "update_date": "2013-06-25", "authors_parsed": [["Gao", "Wei", ""], ["Chen", "Jie", ""], ["Richard", "C\u00e9dric", ""], ["Huang", "Jianguo", ""]]}, {"id": "1306.5362", "submitter": "Michael Mahoney", "authors": "Ping Ma and Michael W. Mahoney and Bin Yu", "title": "A Statistical Perspective on Algorithmic Leveraging", "comments": "44 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One popular method for dealing with large-scale data sets is sampling. For\nexample, by using the empirical statistical leverage scores as an importance\nsampling distribution, the method of algorithmic leveraging samples and\nrescales rows/columns of data matrices to reduce the data size before\nperforming computations on the subproblem. This method has been successful in\nimproving computational efficiency of algorithms for matrix problems such as\nleast-squares approximation, least absolute deviations approximation, and\nlow-rank matrix approximation. Existing work has focused on algorithmic issues\nsuch as worst-case running times and numerical issues associated with providing\nhigh-quality implementations, but none of it addresses statistical aspects of\nthis method.\n  In this paper, we provide a simple yet effective framework to evaluate the\nstatistical properties of algorithmic leveraging in the context of estimating\nparameters in a linear regression model with a fixed number of predictors. We\nshow that from the statistical perspective of bias and variance, neither\nleverage-based sampling nor uniform sampling dominates the other. This result\nis particularly striking, given the well-known result that, from the\nalgorithmic perspective of worst-case analysis, leverage-based sampling\nprovides uniformly superior worst-case algorithmic results, when compared with\nuniform sampling. Based on these theoretical results, we propose and analyze\ntwo new leveraging algorithms. A detailed empirical evaluation of existing\nleverage-based methods as well as these two new methods is carried out on both\nsynthetic and real data sets. The empirical results indicate that our theory is\na good predictor of practical performance of existing and new leverage-based\nalgorithms and that the new algorithms achieve improved performance.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2013 00:31:15 GMT"}], "update_date": "2013-06-25", "authors_parsed": [["Ma", "Ping", ""], ["Mahoney", "Michael W.", ""], ["Yu", "Bin", ""]]}, {"id": "1306.5368", "submitter": "Sanjeena Subedi", "authors": "Sanjeena Subedi and Paul D. McNicholas", "title": "A Variational Approximations-DIC Rubric for Parameter Estimation and\n  Mixture Model Selection Within a Family Setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture model-based clustering has become an increasingly popular data\nanalysis technique since its introduction over fifty years ago, and is now\ncommonly utilized within a family setting. Families of mixture models arise\nwhen the component parameters, usually the component covariance (or scale)\nmatrices, are decomposed and a number of constraints are imposed. Within the\nfamily setting, model selection involves choosing the member of the family,\ni.e., the appropriate covariance structure, in addition to the number of\nmixture components. To date, the Bayesian information criterion (BIC) has\nproved most effective for model selection, and the expectation-maximization\n(EM) algorithm is usually used for parameter estimation. In fact, this EM-BIC\nrubric has virtually monopolized the literature on families of mixture models.\nDeviating from this rubric, variational Bayes approximations are developed for\nparameter estimation and the deviance information criterion for model\nselection. The variational Bayes approach provides an alternate framework for\nparameter estimation by constructing a tight lower bound on the complex\nmarginal likelihood and maximizing this lower bound by minimizing the\nassociated Kullback-Leibler divergence. This approach is taken on the most\ncommonly used family of Gaussian mixture models, and real and simulated data\nare used to compare the new approach to the EM-BIC rubric.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2013 02:30:16 GMT"}, {"version": "v2", "created": "Mon, 27 Jun 2016 21:56:32 GMT"}, {"version": "v3", "created": "Wed, 6 Nov 2019 20:16:30 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Subedi", "Sanjeena", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1306.5532", "submitter": "St\\'ephane Mallat", "authors": "St\\'ephane Mallat and Ir\\`ene Waldspurger", "title": "Deep Learning by Scattering", "comments": "10 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce general scattering transforms as mathematical models of deep\nneural networks with l2 pooling. Scattering networks iteratively apply complex\nvalued unitary operators, and the pooling is performed by a complex modulus. An\nexpected scattering defines a contractive representation of a high-dimensional\nprobability distribution, which preserves its mean-square norm. We show that\nunsupervised learning can be casted as an optimization of the space contraction\nto preserve the volume occupied by unlabeled examples, at each layer of the\nnetwork. Supervised learning and classification are performed with an averaged\nscattering, which provides scattering estimations for multiple classes.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2013 07:52:45 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2015 17:26:01 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Mallat", "St\u00e9phane", ""], ["Waldspurger", "Ir\u00e8ne", ""]]}, {"id": "1306.5550", "submitter": "Cristopher Moore", "authors": "Florent Krzakala, Cristopher Moore, Elchanan Mossel, Joe Neeman, Allan\n  Sly, Lenka Zdeborov\\'a and Pan Zhang", "title": "Spectral redemption: clustering sparse networks", "comments": "11 pages, 6 figures. Clarified to what extent our claims are\n  rigorous, and to what extent they are conjectures; also added an\n  interpretation of the eigenvectors of the 2n-dimensional version of the\n  non-backtracking matrix", "journal-ref": "Proceedings of the National Academy of Sciences 110, no. 52\n  (2013): 20935-20940", "doi": "10.1073/pnas.1312486110", "report-no": null, "categories": "cs.SI cond-mat.stat-mech physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral algorithms are classic approaches to clustering and community\ndetection in networks. However, for sparse networks the standard versions of\nthese algorithms are suboptimal, in some cases completely failing to detect\ncommunities even when other algorithms such as belief propagation can do so.\nHere we introduce a new class of spectral algorithms based on a\nnon-backtracking walk on the directed edges of the graph. The spectrum of this\noperator is much better-behaved than that of the adjacency matrix or other\ncommonly used matrices, maintaining a strong separation between the bulk\neigenvalues and the eigenvalues relevant to community structure even in the\nsparse case. We show that our algorithm is optimal for graphs generated by the\nstochastic block model, detecting communities all the way down to the\ntheoretical limit. We also show the spectrum of the non-backtracking operator\nfor some real-world networks, illustrating its advantages over traditional\nspectral clustering.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2013 09:37:04 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2013 16:04:36 GMT"}], "update_date": "2014-01-20", "authors_parsed": [["Krzakala", "Florent", ""], ["Moore", "Cristopher", ""], ["Mossel", "Elchanan", ""], ["Neeman", "Joe", ""], ["Sly", "Allan", ""], ["Zdeborov\u00e1", "Lenka", ""], ["Zhang", "Pan", ""]]}, {"id": "1306.5554", "submitter": "Brian McWilliams", "authors": "Brian McWilliams, David Balduzzi and Joachim M. Buhmann", "title": "Correlated random features for fast semi-supervised learning", "comments": "15 pages, 3 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Correlated Nystrom Views (XNV), a fast semi-supervised\nalgorithm for regression and classification. The algorithm draws on two main\nideas. First, it generates two views consisting of computationally inexpensive\nrandom features. Second, XNV applies multiview regression using Canonical\nCorrelation Analysis (CCA) on unlabeled data to bias the regression towards\nuseful features. It has been shown that, if the views contains accurate\nestimators, CCA regression can substantially reduce variance with a minimal\nincrease in bias. Random views are justified by recent theoretical and\nempirical work showing that regression with random features closely\napproximates kernel regression, implying that random views can be expected to\ncontain accurate estimators. We show that XNV consistently outperforms a\nstate-of-the-art algorithm for semi-supervised learning: substantially\nimproving predictive performance and reducing the variability of performance on\na wide variety of real-world datasets, whilst also reducing runtime by orders\nof magnitude.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2013 09:49:08 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2013 11:28:33 GMT"}], "update_date": "2013-11-06", "authors_parsed": [["McWilliams", "Brian", ""], ["Balduzzi", "David", ""], ["Buhmann", "Joachim M.", ""]]}, {"id": "1306.5793", "submitter": "Michael Kallitsis", "authors": "Michael Kallitsis, Stilian Stoev, George Michailidis", "title": "A State-Space Approach for Optimal Traffic Monitoring via Network Flow\n  Sampling", "comments": "preliminary work, short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.NI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The robustness and integrity of IP networks require efficient tools for\ntraffic monitoring and analysis, which scale well with traffic volume and\nnetwork size. We address the problem of optimal large-scale flow monitoring of\ncomputer networks under resource constraints. We propose a stochastic\noptimization framework where traffic measurements are done by exploiting the\nspatial (across network links) and temporal relationship of traffic flows.\nSpecifically, given the network topology, the state-space characterization of\nnetwork flows and sampling constraints at each monitoring station, we seek an\noptimal packet sampling strategy that yields the best traffic volume estimation\nfor all flows of the network. The optimal sampling design is the result of a\nconcave minimization problem; then, Kalman filtering is employed to yield a\nsequence of traffic estimates for each network flow. We evaluate our algorithm\nusing real-world Internet2 data.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2013 21:58:30 GMT"}], "update_date": "2013-06-26", "authors_parsed": [["Kallitsis", "Michael", ""], ["Stoev", "Stilian", ""], ["Michailidis", "George", ""]]}, {"id": "1306.5824", "submitter": "Paul McNicholas", "authors": "Ryan P. Browne, Sanjeena Subedi and Paul McNicholas", "title": "Constrained Optimization for a Subset of the Gaussian Parsimonious\n  Clustering Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expectation-maximization (EM) algorithm is an iterative method for\nfinding maximum likelihood estimates when data are incomplete or are treated as\nbeing incomplete. The EM algorithm and its variants are commonly used for\nparameter estimation in applications of mixture models for clustering and\nclassification. This despite the fact that even the Gaussian mixture model\nlikelihood surface contains many local maxima and is singularity riddled.\nPrevious work has focused on circumventing this problem by constraining the\nsmallest eigenvalue of the component covariance matrices. In this paper, we\nconsider constraining the smallest eigenvalue, the largest eigenvalue, and both\nthe smallest and largest within the family setting. Specifically, a subset of\nthe GPCM family is considered for model-based clustering, where we use a\nre-parameterized version of the famous eigenvalue decomposition of the\ncomponent covariance matrices. Our approach is illustrated using various\nexperiments with simulated and real data.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2013 01:27:09 GMT"}], "update_date": "2013-06-26", "authors_parsed": [["Browne", "Ryan P.", ""], ["Subedi", "Sanjeena", ""], ["McNicholas", "Paul", ""]]}, {"id": "1306.5825", "submitter": "Ying Xiao", "authors": "Navin Goyal, Santosh Vempala and Ying Xiao", "title": "Fourier PCA and Robust Tensor Decomposition", "comments": "Extensively revised; details added; minor errors corrected;\n  exposition improved", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fourier PCA is Principal Component Analysis of a matrix obtained from higher\norder derivatives of the logarithm of the Fourier transform of a\ndistribution.We make this method algorithmic by developing a tensor\ndecomposition method for a pair of tensors sharing the same vectors in rank-$1$\ndecompositions. Our main application is the first provably polynomial-time\nalgorithm for underdetermined ICA, i.e., learning an $n \\times m$ matrix $A$\nfrom observations $y=Ax$ where $x$ is drawn from an unknown product\ndistribution with arbitrary non-Gaussian components. The number of component\ndistributions $m$ can be arbitrarily higher than the dimension $n$ and the\ncolumns of $A$ only need to satisfy a natural and efficiently verifiable\nnondegeneracy condition. As a second application, we give an alternative\nalgorithm for learning mixtures of spherical Gaussians with linearly\nindependent means. These results also hold in the presence of Gaussian noise.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2013 01:44:46 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2013 05:58:50 GMT"}, {"version": "v3", "created": "Fri, 15 Nov 2013 00:58:34 GMT"}, {"version": "v4", "created": "Fri, 30 May 2014 04:39:23 GMT"}, {"version": "v5", "created": "Fri, 27 Jun 2014 20:37:17 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Goyal", "Navin", ""], ["Vempala", "Santosh", ""], ["Xiao", "Ying", ""]]}, {"id": "1306.5860", "submitter": "Berk Ustun Berk Ustun", "authors": "Berk Ustun, Stefano Traca, Cynthia Rudin", "title": "Supersparse Linear Integer Models for Predictive Scoring Systems", "comments": "Short version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Supersparse Linear Integer Models (SLIM) as a tool to create\nscoring systems for binary classification. We derive theoretical bounds on the\ntrue risk of SLIM scoring systems, and present experimental results to show\nthat SLIM scoring systems are accurate, sparse, and interpretable\nclassification models.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2013 07:02:20 GMT"}], "update_date": "2013-06-26", "authors_parsed": [["Ustun", "Berk", ""], ["Traca", "Stefano", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1306.5918", "submitter": "Zhaosong Lu", "authors": "Zhaosong Lu and Lin Xiao", "title": "A Randomized Nonmonotone Block Proximal Gradient Method for a Class of\n  Structured Nonlinear Programming", "comments": "The previous title was \"Randomized Block Coordinate Non-Monotone\n  Gradient Method for a Class of Nonlinear Programming\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a randomized nonmonotone block proximal gradient (RNBPG) method\nfor minimizing the sum of a smooth (possibly nonconvex) function and a\nblock-separable (possibly nonconvex nonsmooth) function. At each iteration,\nthis method randomly picks a block according to any prescribed probability\ndistribution and solves typically several associated proximal subproblems that\nusually have a closed-form solution, until a certain progress on objective\nvalue is achieved. In contrast to the usual randomized block coordinate descent\nmethod [23,20], our method has a nonmonotone flavor and uses variable stepsizes\nthat can partially utilize the local curvature information of the smooth\ncomponent of objective function. We show that any accumulation point of the\nsolution sequence of the method is a stationary point of the problem {\\it\nalmost surely} and the method is capable of finding an approximate stationary\npoint with high probability. We also establish a sublinear rate of convergence\nfor the method in terms of the minimal expected squared norm of certain\nproximal gradients over the iterations. When the problem under consideration is\nconvex, we show that the expected objective values generated by RNBPG converge\nto the optimal value of the problem. Under some assumptions, we further\nestablish a sublinear and linear rate of convergence on the expected objective\nvalues generated by a monotone version of RNBPG. Finally, we conduct some\npreliminary experiments to test the performance of RNBPG on the\n$\\ell_1$-regularized least-squares problem and a dual SVM problem in machine\nlearning. The computational results demonstrate that our method substantially\noutperforms the randomized block coordinate {\\it descent} method with fixed or\nvariable stepsizes.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2013 11:11:42 GMT"}, {"version": "v2", "created": "Sat, 21 Mar 2015 01:11:56 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Lu", "Zhaosong", ""], ["Xiao", "Lin", ""]]}, {"id": "1306.5993", "submitter": "Adam Sykulski Dr", "authors": "Adam M. Sykulski, Sofia C. Olhede, Jonathan M. Lilly, Jeffrey J. Early", "title": "Frequency-Domain Stochastic Modeling of Stationary Bivariate or\n  Complex-Valued Signals", "comments": "To appear in IEEE Transactions on Signal Processing", "journal-ref": "IEEE Transactions on Signal Processing, 2017", "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are three equivalent ways of representing two jointly observed\nreal-valued signals: as a bivariate vector signal, as a single complex-valued\nsignal, or as two analytic signals known as the rotary components. Each\nrepresentation has unique advantages depending on the system of interest and\nthe application goals. In this paper we provide a joint framework for all three\nrepresentations in the context of frequency-domain stochastic modeling. This\nframework allows us to extend many established statistical procedures for\nbivariate vector time series to complex-valued and rotary representations.\nThese include procedures for parametrically modeling signal coherence,\nestimating model parameters using the Whittle likelihood, performing\nsemi-parametric modeling, and choosing between classes of nested models using\nmodel choice. We also provide a new method of testing for impropriety in\ncomplex-valued signals, which tests for noncircular or anisotropic second-order\nstatistical structure when the signal is represented in the complex plane.\nFinally, we demonstrate the usefulness of our methodology in capturing the\nanisotropic structure of signals observed from fluid dynamic simulations of\nturbulence.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2013 15:15:06 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2015 06:59:25 GMT"}, {"version": "v3", "created": "Sun, 22 May 2016 00:48:38 GMT"}, {"version": "v4", "created": "Wed, 15 Mar 2017 13:04:31 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Sykulski", "Adam M.", ""], ["Olhede", "Sofia C.", ""], ["Lilly", "Jonathan M.", ""], ["Early", "Jeffrey J.", ""]]}, {"id": "1306.6042", "submitter": "N. Raj Rao", "authors": "Raj Rao Nadakuditi", "title": "OptShrink: An algorithm for improved low-rank signal matrix denoising by\n  optimal, data-driven singular value shrinkage", "comments": "Published version. The algorithm can be downloaded from\n  http://www.eecs.umich.edu/~rajnrao/optshrink", "journal-ref": "IEEE Transactions on Information Theory, vol. 60, no. 6, pp. 1-17,\n  May 2014", "doi": "10.1109/TIT.2014.2311661", "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The truncated singular value decomposition (SVD) of the measurement matrix is\nthe optimal solution to the_representation_ problem of how to best approximate\na noisy measurement matrix using a low-rank matrix. Here, we consider the\n(unobservable)_denoising_ problem of how to best approximate a low-rank signal\nmatrix buried in noise by optimal (re)weighting of the singular vectors of the\nmeasurement matrix. We exploit recent results from random matrix theory to\nexactly characterize the large matrix limit of the optimal weighting\ncoefficients and show that they can be computed directly from data for a large\nclass of noise models that includes the i.i.d. Gaussian noise case.\n  Our analysis brings into sharp focus the shrinkage-and-thresholding form of\nthe optimal weights, the non-convex nature of the associated shrinkage function\n(on the singular values) and explains why matrix regularization via singular\nvalue thresholding with convex penalty functions (such as the nuclear norm)\nwill always be suboptimal. We validate our theoretical predictions with\nnumerical simulations, develop an implementable algorithm (OptShrink) that\nrealizes the predicted performance gains and show how our methods can be used\nto improve estimation in the setting where the measured matrix has missing\nentries.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2013 17:39:48 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2013 00:34:14 GMT"}, {"version": "v3", "created": "Mon, 3 Feb 2014 17:04:45 GMT"}, {"version": "v4", "created": "Fri, 18 Apr 2014 14:58:17 GMT"}], "update_date": "2014-04-21", "authors_parsed": [["Nadakuditi", "Raj Rao", ""]]}, {"id": "1306.6111", "submitter": "David Darmon", "authors": "David Darmon, Jared Sylvester, Michelle Girvan, William Rand", "title": "Understanding the Predictive Power of Computational Mechanics and Echo\n  State Networks in Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a large amount of interest in understanding users of social media in\norder to predict their behavior in this space. Despite this interest, user\npredictability in social media is not well-understood. To examine this\nquestion, we consider a network of fifteen thousand users on Twitter over a\nseven week period. We apply two contrasting modeling paradigms: computational\nmechanics and echo state networks. Both methods attempt to model the behavior\nof users on the basis of their past behavior. We demonstrate that the behavior\nof users on Twitter can be well-modeled as processes with self-feedback. We\nfind that the two modeling approaches perform very similarly for most users,\nbut that they differ in performance on a small subset of the users. By\nexploring the properties of these performance-differentiated users, we\nhighlight the challenges faced in applying predictive models to dynamic social\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2013 00:58:39 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2013 20:13:27 GMT"}], "update_date": "2013-08-27", "authors_parsed": [["Darmon", "David", ""], ["Sylvester", "Jared", ""], ["Girvan", "Michelle", ""], ["Rand", "William", ""]]}, {"id": "1306.6189", "submitter": "Aviv Tamar", "authors": "Aviv Tamar, Huan Xu, Shie Mannor", "title": "Scaling Up Robust MDPs by Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider large-scale Markov decision processes (MDPs) with parameter\nuncertainty, under the robust MDP paradigm. Previous studies showed that robust\nMDPs, based on a minimax approach to handle uncertainty, can be solved using\ndynamic programming for small to medium sized problems. However, due to the\n\"curse of dimensionality\", MDPs that model real-life problems are typically\nprohibitively large for such approaches. In this work we employ a reinforcement\nlearning approach to tackle this planning problem: we develop a robust\napproximate dynamic programming method based on a projected fixed point\nequation to approximately solve large scale robust MDPs. We show that the\nproposed method provably succeeds under certain technical conditions, and\ndemonstrate its effectiveness through simulation of an option pricing problem.\nTo the best of our knowledge, this is the first attempt to scale up the robust\nMDPs paradigm.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2013 09:52:51 GMT"}], "update_date": "2013-06-27", "authors_parsed": [["Tamar", "Aviv", ""], ["Xu", "Huan", ""], ["Mannor", "Shie", ""]]}, {"id": "1306.6239", "submitter": "Matthew Malloy", "authors": "Matthew L. Malloy and Robert D. Nowak", "title": "Near-Optimal Adaptive Compressed Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a simple adaptive sensing and group testing algorithm for\nsparse signal recovery. The algorithm, termed Compressive Adaptive Sense and\nSearch (CASS), is shown to be near-optimal in that it succeeds at the lowest\npossible signal-to-noise-ratio (SNR) levels, improving on previous work in\nadaptive compressed sensing. Like traditional compressed sensing based on\nrandom non-adaptive design matrices, the CASS algorithm requires only k log n\nmeasurements to recover a k-sparse signal of dimension n. However, CASS\nsucceeds at SNR levels that are a factor log n less than required by standard\ncompressed sensing. From the point of view of constructing and implementing the\nsensing operation as well as computing the reconstruction, the proposed\nalgorithm is substantially less computationally intensive than standard\ncompressed sensing. CASS is also demonstrated to perform considerably better in\npractice through simulation. To the best of our knowledge, this is the first\ndemonstration of an adaptive compressed sensing algorithm with near-optimal\ntheoretical guarantees and excellent practical performance. This paper also\nshows that methods like compressed sensing, group testing, and pooling have an\nadvantage beyond simply reducing the number of measurements or tests --\nadaptive versions of such methods can also improve detection and estimation\nperformance when compared to non-adaptive direct (uncompressed) sensing.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2013 13:39:01 GMT"}, {"version": "v2", "created": "Tue, 29 Apr 2014 14:52:52 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Malloy", "Matthew L.", ""], ["Nowak", "Robert D.", ""]]}, {"id": "1306.6482", "submitter": "Shun Kataoka", "authors": "Shun Kataoka, Muneki Yasuda, Cyril Furtlehner and Kazuyuki Tanaka", "title": "Traffic data reconstruction based on Markov random field modeling", "comments": "12 pages, 4 figures", "journal-ref": "Inverse Problems 30 (2014) 025003", "doi": "10.1088/0266-5611/30/2/025003", "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the traffic data reconstruction problem. Suppose we have the\ntraffic data of an entire city that are incomplete because some road data are\nunobserved. The problem is to reconstruct the unobserved parts of the data. In\nthis paper, we propose a new method to reconstruct incomplete traffic data\ncollected from various traffic sensors. Our approach is based on Markov random\nfield modeling of road traffic. The reconstruction is achieved by using\nmean-field method and a machine learning method. We numerically verify the\nperformance of our method using realistic simulated traffic data for the real\nroad network of Sendai, Japan.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2013 12:43:09 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["Kataoka", "Shun", ""], ["Yasuda", "Muneki", ""], ["Furtlehner", "Cyril", ""], ["Tanaka", "Kazuyuki", ""]]}, {"id": "1306.6557", "submitter": "Mladen Kolar", "authors": "Mladen Kolar, Han Liu", "title": "Optimal Feature Selection in High-Dimensional Discriminant Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the high-dimensional discriminant analysis problem. For this\nproblem, different methods have been proposed and justified by establishing\nexact convergence rates for the classification risk, as well as the l2\nconvergence results to the discriminative rule. However, sharp theoretical\nanalysis for the variable selection performance of these procedures have not\nbeen established, even though model interpretation is of fundamental importance\nin scientific data analysis. This paper bridges the gap by providing sharp\nsufficient conditions for consistent variable selection using the sparse\ndiscriminant analysis (Mai et al., 2012). Through careful analysis, we\nestablish rates of convergence that are significantly faster than the best\nknown results and admit an optimal scaling of the sample size n, dimensionality\np, and sparsity level s in the high-dimensional setting. Sufficient conditions\nare complemented by the necessary information theoretic limits on the variable\nselection problem in the context of high-dimensional discriminant analysis.\nExploiting a numerical equivalence result, our method also establish the\noptimal results for the ROAD estimator (Fan et al., 2012) and the sparse\noptimal scaling estimator (Clemmensen et al., 2011). Furthermore, we analyze an\nexhaustive search procedure, whose performance serves as a benchmark, and show\nthat it is variable selection consistent under weaker conditions. Extensive\nsimulations demonstrating the sharpness of the bounds are also provided.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2013 16:05:30 GMT"}], "update_date": "2013-06-28", "authors_parsed": [["Kolar", "Mladen", ""], ["Liu", "Han", ""]]}, {"id": "1306.6677", "submitter": "Berk Ustun Berk Ustun", "authors": "Berk Ustun, Stefano Trac\\`a, Cynthia Rudin", "title": "Supersparse Linear Integer Models for Interpretable Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scoring systems are classification models that only require users to add,\nsubtract and multiply a few meaningful numbers to make a prediction. These\nmodels are often used because they are practical and interpretable. In this\npaper, we introduce an off-the-shelf tool to create scoring systems that both\naccurate and interpretable, known as a Supersparse Linear Integer Model (SLIM).\nSLIM is a discrete optimization problem that minimizes the 0-1 loss to\nencourage a high level of accuracy, regularizes the L0-norm to encourage a high\nlevel of sparsity, and constrains coefficients to a set of interpretable\nvalues. We illustrate the practical and interpretable nature of SLIM scoring\nsystems through applications in medicine and criminology, and show that they\nare are accurate and sparse in comparison to state-of-the-art classification\nmodels using numerical experiments.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2013 22:42:01 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2013 05:08:21 GMT"}, {"version": "v3", "created": "Tue, 23 Jul 2013 03:02:27 GMT"}, {"version": "v4", "created": "Mon, 18 Nov 2013 04:11:29 GMT"}, {"version": "v5", "created": "Fri, 13 Dec 2013 00:38:21 GMT"}, {"version": "v6", "created": "Fri, 11 Apr 2014 03:16:54 GMT"}], "update_date": "2014-04-14", "authors_parsed": [["Ustun", "Berk", ""], ["Trac\u00e0", "Stefano", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1306.6709", "submitter": "Aur\\'elien Bellet", "authors": "Aur\\'elien Bellet, Amaury Habrard and Marc Sebban", "title": "A Survey on Metric Learning for Feature Vectors and Structured Data", "comments": "Technical report, 59 pages. Changes in v2: fixed typos and improved\n  presentation. Changes in v3: fixed typos. Changes in v4: fixed typos and new\n  methods", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for appropriate ways to measure the distance or similarity between\ndata is ubiquitous in machine learning, pattern recognition and data mining,\nbut handcrafting such good metrics for specific problems is generally\ndifficult. This has led to the emergence of metric learning, which aims at\nautomatically learning a metric from data and has attracted a lot of interest\nin machine learning and related fields for the past ten years. This survey\npaper proposes a systematic review of the metric learning literature,\nhighlighting the pros and cons of each approach. We pay particular attention to\nMahalanobis distance metric learning, a well-studied and successful framework,\nbut additionally present a wide range of methods that have recently emerged as\npowerful alternatives, including nonlinear metric learning, similarity learning\nand local metric learning. Recent trends and extensions, such as\nsemi-supervised metric learning, metric learning for histogram data and the\nderivation of generalization guarantees, are also covered. Finally, this survey\naddresses metric learning for structured data, in particular edit distance\nlearning, and attempts to give an overview of the remaining challenges in\nmetric learning for the years to come.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2013 03:56:15 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2013 04:48:05 GMT"}, {"version": "v3", "created": "Mon, 19 Aug 2013 21:28:07 GMT"}, {"version": "v4", "created": "Wed, 12 Feb 2014 07:45:11 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Bellet", "Aur\u00e9lien", ""], ["Habrard", "Amaury", ""], ["Sebban", "Marc", ""]]}, {"id": "1306.6843", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "Error AMP Chain Graphs", "comments": "In Proceedings of the 12th Scandinavian Conference on Artificial\n  Intelligence (SCAI 2013), to appear. Changes from v1 to v2: Minor correction\n  in Theorem 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Any regular Gaussian probability distribution that can be represented by an\nAMP chain graph (CG) can be expressed as a system of linear equations with\ncorrelated errors whose structure depends on the CG. However, the CG represents\nthe errors implicitly, as no nodes in the CG correspond to the errors. We\npropose in this paper to add some deterministic nodes to the CG in order to\nrepresent the errors explicitly. We call the result an EAMP CG. We will show\nthat, as desired, every AMP CG is Markov equivalent to its corresponding EAMP\nCG under marginalization of the error nodes. We will also show that every EAMP\nCG under marginalization of the error nodes is Markov equivalent to some LWF CG\nunder marginalization of the error nodes, and that the latter is Markov\nequivalent to some directed and acyclic graph (DAG) under marginalization of\nthe error nodes and conditioning on some selection nodes. This is important\nbecause it implies that the independence model represented by an AMP CG can be\naccounted for by some data generating process that is partially observed and\nhas selection bias. Finally, we will show that EAMP CGs are closed under\nmarginalization. This is a desirable feature because it guarantees parsimonious\nmodels under marginalization.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2013 13:55:04 GMT"}, {"version": "v2", "created": "Sun, 29 Sep 2013 21:37:21 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}]