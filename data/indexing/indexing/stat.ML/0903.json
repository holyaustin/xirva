[{"id": "0903.0649", "submitter": "John Lafferty", "authors": "Han Liu, John Lafferty and Larry Wasserman", "title": "The Nonparanormal: Semiparametric Estimation of High Dimensional\n  Undirected Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent methods for estimating sparse undirected graphs for real-valued data\nin high dimensional problems rely heavily on the assumption of normality. We\nshow how to use a semiparametric Gaussian copula--or \"nonparanormal\"--for high\ndimensional inference. Just as additive models extend linear models by\nreplacing linear functions with a set of one-dimensional smooth functions, the\nnonparanormal extends the normal by transforming the variables by smooth\nfunctions. We derive a method for estimating the nonparanormal, study the\nmethod's theoretical properties, and show that it works well in many examples.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2009 22:55:18 GMT"}], "update_date": "2009-03-05", "authors_parsed": [["Liu", "Han", ""], ["Lafferty", "John", ""], ["Wasserman", "Larry", ""]]}, {"id": "0903.1468", "submitter": "Massimiliano Pontil", "authors": "Karim Lounici, Massimiliano Pontil, Alexandre B. Tsybakov, Sara van de\n  Geer", "title": "Taking Advantage of Sparsity in Multi-Task Learning", "comments": null, "journal-ref": "10 pages, 1 figure, Proc. Computational Learning Theory Conference\n  (COLT 2009)", "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating multiple linear regression equations for\nthe purpose of both prediction and variable selection. Following recent work on\nmulti-task learning Argyriou et al. [2008], we assume that the regression\nvectors share the same sparsity pattern. This means that the set of relevant\npredictor variables is the same across the different equations. This assumption\nleads us to consider the Group Lasso as a candidate estimation method. We show\nthat this estimator enjoys nice sparsity oracle inequalities and variable\nselection properties. The results hold under a certain restricted eigenvalue\ncondition and a coherence condition on the design matrix, which naturally\nextend recent work in Bickel et al. [2007], Lounici [2008]. In particular, in\nthe multi-task learning scenario, in which the number of tasks can grow, we are\nable to remove completely the effect of the number of predictor variables in\nthe bounds. Finally, we show how our results can be extended to more general\nnoise distributions, of which we only require the variance to be finite.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2009 00:54:46 GMT"}], "update_date": "2012-08-21", "authors_parsed": [["Lounici", "Karim", ""], ["Pontil", "Massimiliano", ""], ["Tsybakov", "Alexandre B.", ""], ["van de Geer", "Sara", ""]]}, {"id": "0903.2003", "submitter": "Miika Ahdesm\\\"{a}ki", "authors": "Miika Ahdesm\\\"aki, Korbinian Strimmer", "title": "Feature selection in omics prediction problems using cat scores and\n  false nondiscovery rate control", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS277 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2010, Vol. 4, No. 1, 503-519", "doi": "10.1214/09-AOAS277", "report-no": "IMS-AOAS-AOAS277", "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the problem of feature selection in linear discriminant analysis\n(LDA), that is, when features are correlated. First, we introduce a pooled\ncentroids formulation of the multiclass LDA predictor function, in which the\nrelative weights of Mahalanobis-transformed predictors are given by\ncorrelation-adjusted $t$-scores (cat scores). Second, for feature selection we\npropose thresholding cat scores by controlling false nondiscovery rates (FNDR).\nThird, training of the classifier is based on James--Stein shrinkage estimates\nof correlations and variances, where regularization parameters are chosen\nanalytically without resampling. Overall, this results in an effective and\ncomputationally inexpensive framework for high-dimensional prediction with\nnatural feature selection. The proposed shrinkage discriminant procedures are\nimplemented in the R package ``sda'' available from the R repository CRAN.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2009 16:44:44 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2009 07:30:41 GMT"}, {"version": "v3", "created": "Fri, 31 Jul 2009 10:03:42 GMT"}, {"version": "v4", "created": "Fri, 8 Oct 2010 06:45:48 GMT"}], "update_date": "2010-10-11", "authors_parsed": [["Ahdesm\u00e4ki", "Miika", ""], ["Strimmer", "Korbinian", ""]]}, {"id": "0903.2515", "submitter": "Shuheng Zhou", "authors": "Shuheng Zhou, Sara van de Geer, Peter B\\\"uhlmann", "title": "Adaptive Lasso for High Dimensional Regression and Gaussian Graphical\n  Modeling", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the two-stage adaptive Lasso procedure (Zou, 2006) is consistent\nfor high-dimensional model selection in linear and Gaussian graphical models.\nOur conditions for consistency cover more general situations than those\naccomplished in previous work: we prove that restricted eigenvalue conditions\n(Bickel et al., 2008) are also sufficient for sparse structure estimation.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2009 23:17:49 GMT"}], "update_date": "2009-03-17", "authors_parsed": [["Zhou", "Shuheng", ""], ["van de Geer", "Sara", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "0903.4817", "submitter": "Martin Jaggi", "authors": "Bernd G\\\"artner, Martin Jaggi and Cl\\'ement Maria", "title": "An Exponential Lower Bound on the Complexity of Regularization Paths", "comments": "Journal version, 28 Pages, 5 Figures", "journal-ref": "Journal of Computational Geometry (JoCG) 3(1), 168-195, 2012", "doi": null, "report-no": null, "categories": "cs.LG cs.CG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a variety of regularized optimization problems in machine learning,\nalgorithms computing the entire solution path have been developed recently.\nMost of these methods are quadratic programs that are parameterized by a single\nparameter, as for example the Support Vector Machine (SVM). Solution path\nalgorithms do not only compute the solution for one particular value of the\nregularization parameter but the entire path of solutions, making the selection\nof an optimal parameter much easier.\n  It has been assumed that these piecewise linear solution paths have only\nlinear complexity, i.e. linearly many bends. We prove that for the support\nvector machine this complexity can be exponential in the number of training\npoints in the worst case. More strongly, we construct a single instance of n\ninput points in d dimensions for an SVM such that at least \\Theta(2^{n/2}) =\n\\Theta(2^d) many distinct subsets of support vectors occur as the\nregularization parameter changes.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2009 17:23:31 GMT"}, {"version": "v2", "created": "Thu, 4 Nov 2010 14:16:36 GMT"}, {"version": "v3", "created": "Thu, 25 Oct 2012 23:47:12 GMT"}], "update_date": "2012-10-31", "authors_parsed": [["G\u00e4rtner", "Bernd", ""], ["Jaggi", "Martin", ""], ["Maria", "Cl\u00e9ment", ""]]}, {"id": "0903.5255", "submitter": "Jianqing Fan", "authors": "Jianqing Fan, Rui Song", "title": "Sure independence screening in generalized linear models with\n  NP-dimensionality", "comments": "Published in at http://dx.doi.org/10.1214/10-AOS798 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2010, Vol. 38, No. 6, 3567-3604", "doi": "10.1214/10-AOS798", "report-no": "IMS-AOS-AOS798", "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrahigh-dimensional variable selection plays an increasingly important role\nin contemporary scientific discoveries and statistical research. Among others,\nFan and Lv [J. R. Stat. Soc. Ser. B Stat. Methodol. 70 (2008) 849-911] propose\nan independent screening framework by ranking the marginal correlations. They\nshowed that the correlation ranking procedure possesses a sure independence\nscreening property within the context of the linear model with Gaussian\ncovariates and responses. In this paper, we propose a more general version of\nthe independent learning with ranking the maximum marginal likelihood estimates\nor the maximum marginal likelihood itself in generalized linear models. We show\nthat the proposed methods, with Fan and Lv [J. R. Stat. Soc. Ser. B Stat.\nMethodol. 70 (2008) 849-911] as a very special case, also possess the sure\nscreening property with vanishing false selection rate. The conditions under\nwhich the independence learning possesses a sure screening is surprisingly\nsimple. This justifies the applicability of such a simple method in a wide\nspectrum. We quantify explicitly the extent to which the dimensionality can be\nreduced by independence screening, which depends on the interactions of the\ncovariance matrix of covariates and true parameters. Simulation studies are\nused to illustrate the utility of the proposed approaches. In addition, we\nestablish an exponential inequality for the quasi-maximum likelihood estimator\nwhich is useful for high-dimensional statistical learning.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2009 15:58:51 GMT"}, {"version": "v2", "created": "Sat, 4 Apr 2009 22:26:42 GMT"}, {"version": "v3", "created": "Thu, 8 Oct 2009 04:12:40 GMT"}, {"version": "v4", "created": "Mon, 18 Jan 2010 17:53:14 GMT"}, {"version": "v5", "created": "Tue, 13 Nov 2012 09:49:29 GMT"}], "update_date": "2012-11-14", "authors_parsed": [["Fan", "Jianqing", ""], ["Song", "Rui", ""]]}, {"id": "0903.5328", "submitter": "Alexander Rakhlin", "authors": "Jacob Abernethy, Alekh Agarwal, Peter L. Bartlett, Alexander Rakhlin", "title": "A Stochastic View of Optimal Regret through Minimax Duality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the regret of optimal strategies for online convex optimization\ngames. Using von Neumann's minimax theorem, we show that the optimal regret in\nthis adversarial setting is closely related to the behavior of the empirical\nminimization algorithm in a stochastic process setting: it is equal to the\nmaximum, over joint distributions of the adversary's action sequence, of the\ndifference between a sum of minimal expected losses and the minimal empirical\nloss. We show that the optimal regret has a natural geometric interpretation,\nsince it can be viewed as the gap in Jensen's inequality for a concave\nfunctional--the minimizer over the player's actions of expected loss--defined\non a set of probability distributions. We use this expression to obtain upper\nand lower bounds on the regret of an optimal strategy for a variety of online\nlearning problems. Our method provides upper bounds without the need to\nconstruct a learning algorithm; the lower bounds provide explicit optimal\nstrategies for the adversary.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2009 22:08:02 GMT"}], "update_date": "2009-04-01", "authors_parsed": [["Abernethy", "Jacob", ""], ["Agarwal", "Alekh", ""], ["Bartlett", "Peter L.", ""], ["Rakhlin", "Alexander", ""]]}]