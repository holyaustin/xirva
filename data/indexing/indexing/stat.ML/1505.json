[{"id": "1505.00274", "submitter": "Miao Liu", "authors": "Miao Liu, Christopher Amato, Xuejun Liao, Lawrence Carin, Jonathan P.\n  How", "title": "Stick-Breaking Policy Learning in Dec-POMDPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expectation maximization (EM) has recently been shown to be an efficient\nalgorithm for learning finite-state controllers (FSCs) in large decentralized\nPOMDPs (Dec-POMDPs). However, current methods use fixed-size FSCs and often\nconverge to maxima that are far from optimal. This paper considers a\nvariable-size FSC to represent the local policy of each agent. These\nvariable-size FSCs are constructed using a stick-breaking prior, leading to a\nnew framework called \\emph{decentralized stick-breaking policy representation}\n(Dec-SBPR). This approach learns the controller parameters with a variational\nBayesian algorithm without having to assume that the Dec-POMDP model is\navailable. The performance of Dec-SBPR is demonstrated on several benchmark\nproblems, showing that the algorithm scales to large problems while\noutperforming other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 20:29:27 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 20:48:32 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Liu", "Miao", ""], ["Amato", "Christopher", ""], ["Liao", "Xuejun", ""], ["Carin", "Lawrence", ""], ["How", "Jonathan P.", ""]]}, {"id": "1505.00294", "submitter": "Nirav Bhatt", "authors": "Nirav Bhatt and Arun Ayyar", "title": "Monotonous (Semi-)Nonnegative Matrix Factorization", "comments": null, "journal-ref": null, "doi": "10.1145/2732587.2732600", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF) factorizes a non-negative matrix into\nproduct of two non-negative matrices, namely a signal matrix and a mixing\nmatrix. NMF suffers from the scale and ordering ambiguities. Often, the source\nsignals can be monotonous in nature. For example, in source separation problem,\nthe source signals can be monotonously increasing or decreasing while the\nmixing matrix can have nonnegative entries. NMF methods may not be effective\nfor such cases as it suffers from the ordering ambiguity. This paper proposes\nan approach to incorporate notion of monotonicity in NMF, labeled as monotonous\nNMF. An algorithm based on alternating least-squares is proposed for recovering\nmonotonous signals from a data matrix. Further, the assumption on mixing matrix\nis relaxed to extend monotonous NMF for data matrix with real numbers as\nentries. The approach is illustrated using synthetic noisy data. The results\nobtained by monotonous NMF are compared with standard NMF algorithms in the\nliterature, and it is shown that monotonous NMF estimates source signals well\nin comparison to standard NMF algorithms when the underlying sources signals\nare monotonous.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 23:58:17 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Bhatt", "Nirav", ""], ["Ayyar", "Arun", ""]]}, {"id": "1505.00398", "submitter": "Ruoxi Wang", "authors": "Ruoxi Wang, Yingzhou Li, Michael W. Mahoney, Eric Darve", "title": "Block Basis Factorization for Scalable Kernel Matrix Evaluation", "comments": "16 pages, 5 figures", "journal-ref": "SIAM Journal on Matrix Analysis and Applications, 2019, Vol. 40,\n  No. 4 : pp. 1497-1526", "doi": "10.1137/18M1212586", "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel methods are widespread in machine learning; however, they are limited\nby the quadratic complexity of the construction, application, and storage of\nkernel matrices. Low-rank matrix approximation algorithms are widely used to\naddress this problem and reduce the arithmetic and storage cost. However, we\nobserved that for some datasets with wide intra-class variability, the optimal\nkernel parameter for smaller classes yields a matrix that is less well\napproximated by low-rank methods. In this paper, we propose an efficient\nstructured low-rank approximation method -- the Block Basis Factorization (BBF)\n-- and its fast construction algorithm to approximate radial basis function\n(RBF) kernel matrices. Our approach has linear memory cost and floating-point\noperations for many machine learning kernels. BBF works for a wide range of\nkernel bandwidth parameters and extends the domain of applicability of low-rank\napproximation methods significantly. Our empirical results demonstrate the\nstability and superiority over the state-of-art kernel approximation\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 07:05:45 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 21:48:23 GMT"}, {"version": "v3", "created": "Tue, 16 Apr 2019 07:06:20 GMT"}, {"version": "v4", "created": "Tue, 4 May 2021 06:02:31 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Wang", "Ruoxi", ""], ["Li", "Yingzhou", ""], ["Mahoney", "Michael W.", ""], ["Darve", "Eric", ""]]}, {"id": "1505.00401", "submitter": "David Powers", "authors": "David M. W. Powers", "title": "Visualization of Tradeoff in Evaluation: from Precision-Recall & PN to\n  LIFT, ROC & BIRD", "comments": "23 pages, 12 equations, 2 figures, 2 tables, 1 sidebar", "journal-ref": null, "doi": null, "report-no": "KIT-14-002", "categories": "cs.LG cs.AI cs.IR stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation often aims to reduce the correctness or error characteristics of a\nsystem down to a single number, but that always involves trade-offs. Another\nway of dealing with this is to quote two numbers, such as Recall and Precision,\nor Sensitivity and Specificity. But it can also be useful to see more than\nthis, and a graphical approach can explore sensitivity to cost, prevalence,\nbias, noise, parameters and hyper-parameters.\n  Moreover, most techniques are implicitly based on two balanced classes, and\nour ability to visualize graphically is intrinsically two dimensional, but we\noften want to visualize in a multiclass context. We review the dichotomous\napproaches relating to Precision, Recall, and ROC as well as the related LIFT\nchart, exploring how they handle unbalanced and multiclass data, and deriving\nnew probabilistic and information theoretic variants of LIFT that help deal\nwith the issues associated with the handling of multiple and unbalanced\nclasses.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 07:27:27 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 04:02:22 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Powers", "David M. W.", ""]]}, {"id": "1505.00428", "submitter": "Nilesh Tripuraneni", "authors": "Nilesh Tripuraneni, Shane Gu, Hong Ge, Zoubin Ghahramani", "title": "A Linear-Time Particle Gibbs Sampler for Infinite Hidden Markov Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infinite Hidden Markov Models (iHMM's) are an attractive, nonparametric\ngeneralization of the classical Hidden Markov Model which can automatically\ninfer the number of hidden states in the system. However, due to the\ninfinite-dimensional nature of transition dynamics performing inference in the\niHMM is difficult. In this paper, we present an infinite-state Particle Gibbs\n(PG) algorithm to resample state trajectories for the iHMM. The proposed\nalgorithm uses an efficient proposal optimized for iHMMs and leverages ancestor\nsampling to suppress degeneracy of the standard PG algorithm. Our algorithm\ndemonstrates significant convergence improvements on synthetic and real world\ndata sets. Additionally, the infinite-state PG algorithm has linear-time\ncomplexity in the number of states in the sampler, while competing methods\nscale quadratically.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 13:18:23 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2015 15:15:51 GMT"}], "update_date": "2015-06-10", "authors_parsed": [["Tripuraneni", "Nilesh", ""], ["Gu", "Shane", ""], ["Ge", "Hong", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1505.00477", "submitter": "Rocco Langone", "authors": "Rocco Langone, Raghvendra Mall, Carlos Alzate, Johan A. K. Suykens", "title": "Kernel Spectral Clustering and applications", "comments": "chapter contribution to the book \"Unsupervised Learning Algorithms\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this chapter we review the main literature related to kernel spectral\nclustering (KSC), an approach to clustering cast within a kernel-based\noptimization setting. KSC represents a least-squares support vector machine\nbased formulation of spectral clustering described by a weighted kernel PCA\nobjective. Just as in the classifier case, the binary clustering model is\nexpressed by a hyperplane in a high dimensional space induced by a kernel. In\naddition, the multi-way clustering can be obtained by combining a set of binary\ndecision functions via an Error Correcting Output Codes (ECOC) encoding scheme.\nBecause of its model-based nature, the KSC method encompasses three main steps:\ntraining, validation, testing. In the validation stage model selection is\nperformed to obtain tuning parameters, like the number of clusters present in\nthe data. This is a major advantage compared to classical spectral clustering\nwhere the determination of the clustering parameters is unclear and relies on\nheuristics. Once a KSC model is trained on a small subset of the entire data,\nit is able to generalize well to unseen test points. Beyond the basic\nformulation, sparse KSC algorithms based on the Incomplete Cholesky\nDecomposition (ICD) and $L_0$, $L_1, L_0 + L_1$, Group Lasso regularization are\nreviewed. In that respect, we show how it is possible to handle large scale\ndata. Also, two possible ways to perform hierarchical clustering and a soft\nclustering method are presented. Finally, real-world applications such as image\nsegmentation, power load time-series clustering, document clustering and big\ndata learning are considered.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 21:07:09 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Langone", "Rocco", ""], ["Mall", "Raghvendra", ""], ["Alzate", "Carlos", ""], ["Suykens", "Johan A. K.", ""]]}, {"id": "1505.00482", "submitter": "Larry Wasserman", "authors": "Martin Azizyan, Yen-Chi Chen, Aarti Singh and Larry Wasserman", "title": "Risk Bounds For Mode Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Density mode clustering is a nonparametric clustering method. The clusters\nare the basins of attraction of the modes of a density estimator. We study the\nrisk of mode-based clustering. We show that the clustering risk over the\ncluster cores --- the regions where the density is high --- is very small even\nin high dimensions. And under a low noise condition, the overall cluster risk\nis small even beyond the cores, in high dimensions.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 21:46:42 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Azizyan", "Martin", ""], ["Chen", "Yen-Chi", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1505.00526", "submitter": "Tianbao Yang", "authors": "Tianbao Yang, Lijun Zhang, Rong Jin, Shenghuo Zhu", "title": "An Explicit Sampling Dependent Spectral Error Bound for Column Subset\n  Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of column subset selection. We present\na novel analysis of the spectral norm reconstruction for a simple randomized\nalgorithm and establish a new bound that depends explicitly on the sampling\nprobabilities. The sampling dependent error bound (i) allows us to better\nunderstand the tradeoff in the reconstruction error due to sampling\nprobabilities, (ii) exhibits more insights than existing error bounds that\nexploit specific probability distributions, and (iii) implies better sampling\ndistributions. In particular, we show that a sampling distribution with\nprobabilities proportional to the square root of the statistical leverage\nscores is always better than uniform sampling and is better than leverage-based\nsampling when the statistical leverage scores are very nonuniform. And by\nsolving a constrained optimization problem related to the error bound with an\nefficient bisection search we are able to achieve better performance than using\neither the leverage-based distribution or that proportional to the square root\nof the statistical leverage scores. Numerical simulations demonstrate the\nbenefits of the new sampling distributions for low-rank matrix approximation\nand least square approximation compared to state-of-the art algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 05:49:04 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Yang", "Tianbao", ""], ["Zhang", "Lijun", ""], ["Jin", "Rong", ""], ["Zhu", "Shenghuo", ""]]}, {"id": "1505.00553", "submitter": "Naumaan Nayyar", "authors": "Naumaan Nayyar, Dileep Kalathil and Rahul Jain", "title": "On Regret-Optimal Learning in Decentralized Multi-player Multi-armed\n  Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning in single-player and multiplayer\nmultiarmed bandit models. Bandit problems are classes of online learning\nproblems that capture exploration versus exploitation tradeoffs. In a\nmultiarmed bandit model, players can pick among many arms, and each play of an\narm generates an i.i.d. reward from an unknown distribution. The objective is\nto design a policy that maximizes the expected reward over a time horizon for a\nsingle player setting and the sum of expected rewards for the multiplayer\nsetting. In the multiplayer setting, arms may give different rewards to\ndifferent players. There is no separate channel for coordination among the\nplayers. Any attempt at communication is costly and adds to regret. We propose\ntwo decentralizable policies, $\\tt E^3$ ($\\tt E$-$\\tt cubed$) and $\\tt\nE^3$-$\\tt TS$, that can be used in both single player and multiplayer settings.\nThese policies are shown to yield expected regret that grows at most as\nO($\\log^{1+\\epsilon} T$). It is well known that $\\log T$ is the lower bound on\nthe rate of growth of regret even in a centralized case. The proposed\nalgorithms improve on prior work where regret grew at O($\\log^2 T$). More\nfundamentally, these policies address the question of additional cost incurred\nin decentralized online learning, suggesting that there is at most an\n$\\epsilon$-factor cost in terms of order of regret. This solves a problem of\nrelevance in many domains and had been open for a while.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 08:22:04 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 20:23:38 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Nayyar", "Naumaan", ""], ["Kalathil", "Dileep", ""], ["Jain", "Rahul", ""]]}, {"id": "1505.00824", "submitter": "Eva Dyer", "authors": "Eva L. Dyer, Tom A. Goldstein, Raajen Patel, Konrad P. Kording, and\n  Richard G. Baraniuk", "title": "Self-Expressive Decompositions for Matrix Approximation and Clustering", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-aware methods for dimensionality reduction and matrix decomposition aim\nto find low-dimensional structure in a collection of data. Classical approaches\ndiscover such structure by learning a basis that can efficiently express the\ncollection. Recently, \"self expression\", the idea of using a small subset of\ndata vectors to represent the full collection, has been developed as an\nalternative to learning. Here, we introduce a scalable method for computing\nsparse SElf-Expressive Decompositions (SEED). SEED is a greedy method that\nconstructs a basis by sequentially selecting incoherent vectors from the\ndataset. After forming a basis from a subset of vectors in the dataset, SEED\nthen computes a sparse representation of the dataset with respect to this\nbasis. We develop sufficient conditions under which SEED exactly represents low\nrank matrices and vectors sampled from a unions of independent subspaces. We\nshow how SEED can be used in applications ranging from matrix approximation and\ndenoising to clustering, and apply it to numerous real-world datasets. Our\nresults demonstrate that SEED is an attractive low-complexity alternative to\nother sparse matrix factorization approaches such as sparse PCA and\nself-expressive methods for clustering.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 21:56:54 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Dyer", "Eva L.", ""], ["Goldstein", "Tom A.", ""], ["Patel", "Raajen", ""], ["Kording", "Konrad P.", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1505.00853", "submitter": "Bing Xu", "authors": "Bing Xu, Naiyan Wang, Tianqi Chen, Mu Li", "title": "Empirical Evaluation of Rectified Activations in Convolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the performance of different types of rectified\nactivation functions in convolutional neural network: standard rectified linear\nunit (ReLU), leaky rectified linear unit (Leaky ReLU), parametric rectified\nlinear unit (PReLU) and a new randomized leaky rectified linear units (RReLU).\nWe evaluate these activation function on standard image classification task.\nOur experiments suggest that incorporating a non-zero slope for negative part\nin rectified activation units could consistently improve the results. Thus our\nfindings are negative on the common belief that sparsity is the key of good\nperformance in ReLU. Moreover, on small scale dataset, using deterministic\nnegative slope or learning it are both prone to overfitting. They are not as\neffective as using their randomized counterpart. By using RReLU, we achieved\n75.68\\% accuracy on CIFAR-100 test set without multiple test or ensemble.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 01:16:39 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2015 06:58:14 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Xu", "Bing", ""], ["Wang", "Naiyan", ""], ["Chen", "Tianqi", ""], ["Li", "Mu", ""]]}, {"id": "1505.00864", "submitter": "Shihao Yang", "authors": "Shihao Yang, Mauricio Santillana, and S. C. Kou", "title": "Accurate estimation of influenza epidemics using Google search data via\n  ARGO", "comments": "23 pages, 2 figures, Proceedings of the National Academy of Sciences\n  (2015)", "journal-ref": null, "doi": "10.1073/pnas.1515373112", "report-no": null, "categories": "stat.AP cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate real-time tracking of influenza outbreaks helps public health\nofficials make timely and meaningful decisions that could save lives. We\npropose an influenza tracking model, ARGO (AutoRegression with GOogle search\ndata), that uses publicly available online search data. In addition to having a\nrigorous statistical foundation, ARGO outperforms all previously available\nGoogle-search-based tracking models, including the latest version of Google Flu\nTrends, even though it uses only low-quality search data as input from publicly\navailable Google Trends and Google Correlate websites. ARGO not only\nincorporates the seasonality in influenza epidemics but also captures changes\nin people's online search behavior over time. ARGO is also flexible,\nself-correcting, robust, and scalable, making it a potentially powerful tool\nthat can be used for real-time tracking of other social events at multiple\ntemporal and spatial resolutions.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 02:10:18 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2015 19:33:43 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Yang", "Shihao", ""], ["Santillana", "Mauricio", ""], ["Kou", "S. C.", ""]]}, {"id": "1505.00869", "submitter": "Chen Xu", "authors": "Chen Xu, Yongquan Zhang, Runze Li", "title": "On the Feasibility of Distributed Kernel Regression for Big Data", "comments": "22 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern scientific research, massive datasets with huge numbers of\nobservations are frequently encountered. To facilitate the computational\nprocess, a divide-and-conquer scheme is often used for the analysis of big\ndata. In such a strategy, a full dataset is first split into several manageable\nsegments; the final output is then averaged from the individual outputs of the\nsegments. Despite its popularity in practice, it remains largely unknown that\nwhether such a distributive strategy provides valid theoretical inferences to\nthe original data. In this paper, we address this fundamental issue for the\ndistributed kernel regression (DKR), where the algorithmic feasibility is\nmeasured by the generalization performance of the resulting estimator. To\njustify DKR, a uniform convergence rate is needed for bounding the\ngeneralization error over the individual outputs, which brings new and\nchallenging issues in the big data setup. Under mild conditions, we show that,\nwith a proper number of segments, DKR leads to an estimator that is\ngeneralization consistent to the unknown regression function. The obtained\nresults justify the method of DKR and shed light on the feasibility of using\nother distributed algorithms for processing big data. The promising preference\nof the method is supported by both simulation and real data examples.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 02:48:12 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Xu", "Chen", ""], ["Zhang", "Yongquan", ""], ["Li", "Runze", ""]]}, {"id": "1505.00991", "submitter": "Yair Goldberg", "authors": "Yael Travis-Lumer and Yair Goldberg", "title": "Kernel Machines for Current Status Data", "comments": "37 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In survival analysis, estimating the failure time distribution is an\nimportant and difficult task, since usually the data is subject to censoring.\nSpecifically, in this paper we consider current status data, a type of data\nwhere all of the observations are censored. The format of the data is such that\nthe failure time is restricted to knowledge of whether or not the failure time\nexceeds a random monitoring time. We propose a flexible kernel machine approach\nfor estimation of the failure time expectation as a function of the covariates,\nwith current status data. In order to obtain the kernel machine decision\nfunction, we minimize a regularized version of the empirical risk with respect\nto a new loss function. Using finite sample bounds and novel oracle\ninequalities, we prove that the obtained estimator converges to the true\nconditional expectation for a large family of probability measures. Finally, we\npresent a simulation study and an analysis of real-world data that compares the\nperformance of the proposed approach to existing methods. We show empirically\nthat our approach is comparable to current state of the art, and in some cases\nis even better.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 12:53:50 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 06:29:41 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Travis-Lumer", "Yael", ""], ["Goldberg", "Yair", ""]]}, {"id": "1505.01147", "submitter": "Franz J. Kir\\'aly", "authors": "Duncan A.J. Blythe and Franz J. Kir\\'aly", "title": "Prediction and Quantification of Individual Athletic Performance", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0157257", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide scientific foundations for athletic performance prediction on an\nindividual level, exposing the phenomenology of individual athletic running\nperformance in the form of a low-rank model dominated by an individual power\nlaw. We present, evaluate, and compare a selection of methods for prediction of\nindividual running performance, including our own, \\emph{local matrix\ncompletion} (LMC), which we show to perform best. We also show that many\ndocumented phenomena in quantitative sports science, such as the form of\nscoring tables, the success of existing prediction methods including Riegel's\nformula, the Purdy points scheme, the power law for world records performances\nand the broken power law for world record speeds may be explained on the basis\nof our findings in a unified way.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 19:59:29 GMT"}, {"version": "v2", "created": "Wed, 13 May 2015 19:07:32 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Blythe", "Duncan A. J.", ""], ["Kir\u00e1ly", "Franz J.", ""]]}, {"id": "1505.01164", "submitter": "Emily Fox", "authors": "You Ren, Emily B. Fox, and Andrew Bruce", "title": "Achieving a Hyperlocal Housing Price Index: Overcoming Data Sparsity by\n  Bayesian Dynamical Modeling of Multiple Data Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how housing values evolve over time is important to policy\nmakers, consumers and real estate professionals. Existing methods for\nconstructing housing indices are computed at a coarse spatial granularity, such\nas metropolitan regions, which can mask or distort price dynamics apparent in\nlocal markets, such as neighborhoods and census tracts. A challenge in moving\nto estimates at, for example, the census tract level is the sparsity of\nspatiotemporally localized house sales observations. Our work aims at\naddressing this challenge by leveraging observations from multiple census\ntracts discovered to have correlated valuation dynamics. Our proposed Bayesian\nnonparametric approach builds on the framework of latent factor models to\nenable a flexible, data-driven method for inferring the clustering of\ncorrelated census tracts. We explore methods for scalability and\nparallelizability of computations, yielding a housing valuation index at the\nlevel of census tract rather than zip code, and on a monthly basis rather than\nquarterly. Our analysis is provided on a large Seattle metropolitan housing\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 20:01:08 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Ren", "You", ""], ["Fox", "Emily B.", ""], ["Bruce", "Andrew", ""]]}, {"id": "1505.01206", "submitter": "Changshuai Wei", "authors": "Changshuai Wei, Daniel J. Schaid, Qing Lu", "title": "Trees Assembling Mann Whitney Approach for Detecting Genome-wide Joint\n  Association among Low Marginal Effect loci", "comments": null, "journal-ref": "Genet Epidemiol. 2013 Jan;37(1):84-91", "doi": "10.1002/gepi.21693", "report-no": null, "categories": "q-bio.QM stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common complex diseases are likely influenced by the interplay of hundreds,\nor even thousands, of genetic variants. Converging evidence shows that genetic\nvariants with low marginal effects (LME) play an important role in disease\ndevelopment. Despite their potential significance, discovering LME genetic\nvariants and assessing their joint association on high dimensional data (e.g.,\ngenome wide association studies) remain a great challenge. To facilitate joint\nassociation analysis among a large ensemble of LME genetic variants, we\nproposed a computationally efficient and powerful approach, which we call Trees\nAssembling Mann whitney (TAMW). Through simulation studies and an empirical\ndata application, we found that TAMW outperformed multifactor dimensionality\nreduction (MDR) and the likelihood ratio based Mann whitney approach (LRMW)\nwhen the underlying complex disease involves multiple LME loci and their\ninteractions. For instance, in a simulation with 20 interacting LME loci, TAMW\nattained a higher power (power=0.931) than both MDR (power=0.599) and LRMW\n(power=0.704). In an empirical study of 29 known Crohn's disease (CD) loci,\nTAMW also identified a stronger joint association with CD than those detected\nby MDR and LRMW. Finally, we applied TAMW to Wellcome Trust CD GWAS to conduct\na genome wide analysis. The analysis of 459K single nucleotide polymorphisms\nwas completed in 40 hours using parallel computing, and revealed a joint\nassociation predisposing to CD (p-value=2.763e-19). Further analysis of the\nnewly discovered association suggested that 13 genes, such as ATG16L1 and\nLACC1, may play an important role in CD pathophysiological and etiological\nprocesses.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 22:14:28 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Wei", "Changshuai", ""], ["Schaid", "Daniel J.", ""], ["Lu", "Qing", ""]]}, {"id": "1505.01300", "submitter": "Dominique Gay", "authors": "Dominique Gay, Romain Guigour\\`es, Marc Boull\\'e, Fabrice Cl\\'erot", "title": "Cats & Co: Categorical Time Series Coclustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We suggest a novel method of clustering and exploratory analysis of temporal\nevent sequences data (also known as categorical time series) based on\nthree-dimensional data grid models. A data set of temporal event sequences can\nbe represented as a data set of three-dimensional points, each point is defined\nby three variables: a sequence identifier, a time value and an event value.\nInstantiating data grid models to the 3D-points turns the problem into\n3D-coclustering.\n  The sequences are partitioned into clusters, the time variable is discretized\ninto intervals and the events are partitioned into clusters. The cross-product\nof the univariate partitions forms a multivariate partition of the\nrepresentation space, i.e., a grid of cells and it also represents a\nnonparametric estimator of the joint distribution of the sequences, time and\nevents dimensions. Thus, the sequences are grouped together because they have\nsimilar joint distribution of time and events, i.e., similar distribution of\nevents along the time dimension. The best data grid is computed using a\nparameter-free Bayesian model selection approach. We also suggest several\ncriteria for exploiting the resulting grid through agglomerative hierarchies,\nfor interpreting the clusters of sequences and characterizing their components\nthrough insightful visualizations. Extensive experiments on both synthetic and\nreal-world data sets demonstrate that data grid models are efficient, effective\nand discover meaningful underlying patterns of categorical time series data.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 09:50:12 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Gay", "Dominique", ""], ["Guigour\u00e8s", "Romain", ""], ["Boull\u00e9", "Marc", ""], ["Cl\u00e9rot", "Fabrice", ""]]}, {"id": "1505.01371", "submitter": "Shaobo Lin", "authors": "Shaobo Lin, Yao Wang and Lin Xu", "title": "Re-scale boosting for regression and classification", "comments": "13 pages; 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosting is a learning scheme that combines weak prediction rules to produce\na strong composite estimator, with the underlying intuition that one can obtain\naccurate prediction rules by combining \"rough\" ones. Although boosting is\nproved to be consistent and overfitting-resistant, its numerical convergence\nrate is relatively slow. The aim of this paper is to develop a new boosting\nstrategy, called the re-scale boosting (RBoosting), to accelerate the numerical\nconvergence rate and, consequently, improve the learning performance of\nboosting. Our studies show that RBoosting possesses the almost optimal\nnumerical convergence rate in the sense that, up to a logarithmic factor, it\ncan reach the minimax nonlinear approximation rate. We then use RBoosting to\ntackle both the classification and regression problems, and deduce a tight\ngeneralization error estimate. The theoretical and experimental results show\nthat RBoosting outperforms boosting in terms of generalization.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 14:05:08 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Lin", "Shaobo", ""], ["Wang", "Yao", ""], ["Xu", "Lin", ""]]}, {"id": "1505.01462", "submitter": "Nihar Shah", "authors": "Nihar B. Shah, Sivaraman Balakrishnan, Joseph Bradley, Abhay Parekh,\n  Kannan Ramchandran, Martin J. Wainwright", "title": "Estimation from Pairwise Comparisons: Sharp Minimax Bounds with Topology\n  Dependence", "comments": "39 pages, 5 figures. Significant extension of arXiv:1406.6618", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data in the form of pairwise comparisons arises in many domains, including\npreference elicitation, sporting competitions, and peer grading among others.\nWe consider parametric ordinal models for such pairwise comparison data\ninvolving a latent vector $w^* \\in \\mathbb{R}^d$ that represents the\n\"qualities\" of the $d$ items being compared; this class of models includes the\ntwo most widely used parametric models--the Bradley-Terry-Luce (BTL) and the\nThurstone models. Working within a standard minimax framework, we provide tight\nupper and lower bounds on the optimal error in estimating the quality score\nvector $w^*$ under this class of models. The bounds depend on the topology of\nthe comparison graph induced by the subset of pairs being compared via its\nLaplacian spectrum. Thus, in settings where the subset of pairs may be chosen,\nour results provide principled guidelines for making this choice. Finally, we\ncompare these error rates to those under cardinal measurement models and show\nthat the error rates in the ordinal and cardinal settings have identical\nscalings apart from constant pre-factors.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 19:04:09 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Shah", "Nihar B.", ""], ["Balakrishnan", "Sivaraman", ""], ["Bradley", "Joseph", ""], ["Parekh", "Abhay", ""], ["Ramchandran", "Kannan", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1505.01539", "submitter": "Luis Ortiz", "authors": "Luis E. Ortiz", "title": "Graphical Potential Games", "comments": "15 pages, To appear at The 26th International Conference on Game\n  Theory, part of the Stony Brook Game Theory Summer Festival 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Potential games, originally introduced in the early 1990's by Lloyd Shapley,\nthe 2012 Nobel Laureate in Economics, and his colleague Dov Monderer, are a\nvery important class of models in game theory. They have special properties\nsuch as the existence of Nash equilibria in pure strategies. This note\nintroduces graphical versions of potential games. Special cases of graphical\npotential games have already found applicability in many areas of science and\nengineering beyond economics, including artificial intelligence, computer\nvision, and machine learning. They have been effectively applied to the study\nand solution of important real-world problems such as routing and congestion in\nnetworks, distributed resource allocation (e.g., public goods), and\nrelaxation-labeling for image segmentation. Implicit use of graphical potential\ngames goes back at least 40 years. Several classes of games considered standard\nin the literature, including coordination games, local interaction games,\nlattice games, congestion games, and party-affiliation games, are instances of\ngraphical potential games. This note provides several characterizations of\ngraphical potential games by leveraging well-known results from the literature\non probabilistic graphical models. A major contribution of the work presented\nhere that particularly distinguishes it from previous work is establishing that\nthe convergence of certain type of game-playing rules implies that the\nagents/players must be embedded in some graphical potential game.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 23:27:55 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Ortiz", "Luis E.", ""]]}, {"id": "1505.01582", "submitter": "Debarghya Ghoshdastidar", "authors": "Debarghya Ghoshdastidar, Ambedkar Dukkipati", "title": "Consistency of Spectral Hypergraph Partitioning under Planted Partition\n  Model", "comments": "35 pages, 2 figures, 1 table", "journal-ref": "Ann. Statist. Volume 45, Number 1 (2017), 289-315", "doi": "10.1214/16-AOS1453", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypergraph partitioning lies at the heart of a number of problems in machine\nlearning and network sciences. Many algorithms for hypergraph partitioning have\nbeen proposed that extend standard approaches for graph partitioning to the\ncase of hypergraphs. However, theoretical aspects of such methods have seldom\nreceived attention in the literature as compared to the extensive studies on\nthe guarantees of graph partitioning. For instance, consistency results of\nspectral graph partitioning under the stochastic block model are well known. In\nthis paper, we present a planted partition model for sparse random non-uniform\nhypergraphs that generalizes the stochastic block model. We derive an error\nbound for a spectral hypergraph partitioning algorithm under this model using\nmatrix concentration inequalities. To the best of our knowledge, this is the\nfirst consistency result related to partitioning non-uniform hypergraphs.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 04:30:16 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2016 09:22:11 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Ghoshdastidar", "Debarghya", ""], ["Dukkipati", "Ambedkar", ""]]}, {"id": "1505.01627", "submitter": "Javier Gonz\\'alez", "authors": "Javier Gonz\\'alez, Joseph Longworth, David C. James and Neil D.\n  Lawrence", "title": "Bayesian Optimization for Synthetic Gene Design", "comments": "6 pages, 3 figures. NIPS 2014, Workshop in Bayesian Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of synthetic gene design using Bayesian optimization.\nThe main issue when designing a gene is that the design space is defined in\nterms of long strings of characters of different lengths, which renders the\noptimization intractable. We propose a three-step approach to deal with this\nissue. First, we use a Gaussian process model to emulate the behavior of the\ncell. As inputs of the model, we use a set of biologically meaningful gene\nfeatures, which allows us to define optimal gene designs rules. Based on the\nmodel outputs we define a multi-task acquisition function to optimize\nsimultaneously severals aspects of interest. Finally, we define an evaluation\nfunction, which allow us to rank sets of candidate gene sequences that are\ncoherent with the optimal design strategy. We illustrate the performance of\nthis approach in a real gene design experiment with mammalian cells.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 08:50:18 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Gonz\u00e1lez", "Javier", ""], ["Longworth", "Joseph", ""], ["James", "David C.", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1505.01802", "submitter": "Nagarajan Natarajan", "authors": "Nagarajan Natarajan, Oluwasanmi Koyejo, Pradeep Ravikumar, Inderjit S.\n  Dhillon", "title": "Optimal Decision-Theoretic Classification Using Non-Decomposable\n  Performance Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a general theoretical analysis of expected out-of-sample utility,\nalso referred to as decision-theoretic classification, for non-decomposable\nbinary classification metrics such as F-measure and Jaccard coefficient. Our\nkey result is that the expected out-of-sample utility for many performance\nmetrics is provably optimized by a classifier which is equivalent to a signed\nthresholding of the conditional probability of the positive class. Our analysis\nbridges a gap in the literature on binary classification, revealed in light of\nrecent results for non-decomposable metrics in population utility maximization\nstyle classification. Our results identify checkable properties of a\nperformance metric which are sufficient to guarantee a probability ranking\nprinciple. We propose consistent estimators for optimal expected out-of-sample\nclassification. As a consequence of the probability ranking principle,\ncomputational requirements can be reduced from exponential to cubic complexity\nin the general case, and further reduced to quadratic complexity in special\ncases. We provide empirical results on simulated and benchmark datasets\nevaluating the performance of the proposed algorithms for decision-theoretic\nclassification and comparing them to baseline and state-of-the-art methods in\npopulation utility maximization for non-decomposable metrics.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 18:19:24 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Natarajan", "Nagarajan", ""], ["Koyejo", "Oluwasanmi", ""], ["Ravikumar", "Pradeep", ""], ["Dhillon", "Inderjit S.", ""]]}, {"id": "1505.01866", "submitter": "K. V. Rashmi", "authors": "K. V. Rashmi and Ran Gilad-Bachrach", "title": "DART: Dropouts meet Multiple Additive Regression Trees", "comments": "AIStats 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple Additive Regression Trees (MART), an ensemble model of boosted\nregression trees, is known to deliver high prediction accuracy for diverse\ntasks, and it is widely used in practice. However, it suffers an issue which we\ncall over-specialization, wherein trees added at later iterations tend to\nimpact the prediction of only a few instances, and make negligible contribution\ntowards the remaining instances. This negatively affects the performance of the\nmodel on unseen data, and also makes the model over-sensitive to the\ncontributions of the few, initially added tress. We show that the commonly used\ntool to address this issue, that of shrinkage, alleviates the problem only to a\ncertain extent and the fundamental issue of over-specialization still remains.\nIn this work, we explore a different approach to address the problem that of\nemploying dropouts, a tool that has been recently proposed in the context of\nlearning deep neural networks. We propose a novel way of employing dropouts in\nMART, resulting in the DART algorithm. We evaluate DART on ranking, regression\nand classification tasks, using large scale, publicly available datasets, and\nshow that DART outperforms MART in each of the tasks, with a significant\nmargin. We also show that DART overcomes the issue of over-specialization to a\nconsiderable extent.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 20:38:48 GMT"}], "update_date": "2015-05-11", "authors_parsed": [["Rashmi", "K. V.", ""], ["Gilad-Bachrach", "Ran", ""]]}, {"id": "1505.01918", "submitter": "Michael Katehakis", "authors": "Wesley Cowan and Michael N. Katehakis", "title": "An Asymptotically Optimal Policy for Uniform Bandits of Unknown Support", "comments": "arXiv admin note: text overlap with arXiv:1504.05823", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem of a controller sampling sequentially from a finite\nnumber of $N \\geq 2$ populations, specified by random variables $X^i_k$, $ i =\n1,\\ldots , N,$ and $k = 1, 2, \\ldots$; where $X^i_k$ denotes the outcome from\npopulation $i$ the $k^{th}$ time it is sampled. It is assumed that for each\nfixed $i$, $\\{ X^i_k \\}_{k \\geq 1}$ is a sequence of i.i.d. uniform random\nvariables over some interval $[a_i, b_i]$, with the support (i.e., $a_i, b_i$)\nunknown to the controller. The objective is to have a policy $\\pi$ for\ndeciding, based on available data, from which of the $N$ populations to sample\nfrom at any time $n=1,2,\\ldots$ so as to maximize the expected sum of outcomes\nof $n$ samples or equivalently to minimize the regret due to lack on\ninformation of the parameters $\\{ a_i \\}$ and $\\{ b_i \\}$. In this paper, we\npresent a simple inflated sample mean (ISM) type policy that is asymptotically\noptimal in the sense of its regret achieving the asymptotic lower bound of\nBurnetas and Katehakis (1996). Additionally, finite horizon regret bounds are\ngiven.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 03:15:49 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 16:46:34 GMT"}, {"version": "v3", "created": "Thu, 24 Sep 2015 12:07:17 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Cowan", "Wesley", ""], ["Katehakis", "Michael N.", ""]]}, {"id": "1505.02065", "submitter": "Yannis Papanikolaou", "authors": "Yannis Papanikolaou, James R. Foulds, Timothy N. Rubin, Grigorios\n  Tsoumakas", "title": "Dense Distributions from Sparse Samples: Improved Gibbs Sampling\n  Parameter Estimators for LDA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel approach for estimating Latent Dirichlet Allocation\n(LDA) parameters from collapsed Gibbs samples (CGS), by leveraging the full\nconditional distributions over the latent variable assignments to efficiently\naverage over multiple samples, for little more computational cost than drawing\na single additional collapsed Gibbs sample. Our approach can be understood as\nadapting the soft clustering methodology of Collapsed Variational Bayes (CVB0)\nto CGS parameter estimation, in order to get the best of both techniques. Our\nestimators can straightforwardly be applied to the output of any existing\nimplementation of CGS, including modern accelerated variants. We perform\nextensive empirical comparisons of our estimators with those of standard\ncollapsed inference algorithms on real-world data for both unsupervised LDA and\nPrior-LDA, a supervised variant of LDA for multi-label classification. Our\nresults show a consistent advantage of our approach over traditional CGS under\nall experimental conditions, and over CVB0 inference in the majority of\nconditions. More broadly, our results highlight the importance of averaging\nover multiple samples in LDA parameter estimation, and the use of efficient\ncomputational techniques to do so.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 15:32:43 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2016 14:33:42 GMT"}, {"version": "v3", "created": "Mon, 17 Oct 2016 18:21:48 GMT"}, {"version": "v4", "created": "Tue, 18 Oct 2016 10:52:15 GMT"}, {"version": "v5", "created": "Fri, 28 Oct 2016 07:43:21 GMT"}, {"version": "v6", "created": "Tue, 11 Apr 2017 14:42:42 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Papanikolaou", "Yannis", ""], ["Foulds", "James R.", ""], ["Rubin", "Timothy N.", ""], ["Tsoumakas", "Grigorios", ""]]}, {"id": "1505.02206", "submitter": "Dinesh Jayaraman", "authors": "Dinesh Jayaraman and Kristen Grauman", "title": "Learning image representations tied to ego-motion", "comments": "Supplementary material appended at end. In ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how images of objects and scenes behave in response to specific\nego-motions is a crucial aspect of proper visual development, yet existing\nvisual learning methods are conspicuously disconnected from the physical source\nof their images. We propose to exploit proprioceptive motor signals to provide\nunsupervised regularization in convolutional neural networks to learn visual\nrepresentations from egocentric video. Specifically, we enforce that our\nlearned features exhibit equivariance i.e. they respond predictably to\ntransformations associated with distinct ego-motions. With three datasets, we\nshow that our unsupervised feature learning approach significantly outperforms\nprevious approaches on visual recognition and next-best-view prediction tasks.\nIn the most challenging test, we show that features learned from video captured\non an autonomous driving platform improve large-scale scene recognition in\nstatic images from a disjoint domain.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 23:15:00 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2016 19:30:18 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Jayaraman", "Dinesh", ""], ["Grauman", "Kristen", ""]]}, {"id": "1505.02212", "submitter": "Yakir Reshef", "authors": "Yakir A. Reshef, David N. Reshef, Pardis C. Sabeti, Michael M.\n  Mitzenmacher", "title": "Equitability, interval estimation, and statistical power", "comments": "Yakir A. Reshef and David N. Reshef are co-first authors, Pardis C.\n  Sabeti and Michael M. Mitzenmacher are co-last authors. This paper, together\n  with arXiv:1505.02212, subsumes arXiv:1408.4908", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG q-bio.QM stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For analysis of a high-dimensional dataset, a common approach is to test a\nnull hypothesis of statistical independence on all variable pairs using a\nnon-parametric measure of dependence. However, because this approach attempts\nto identify any non-trivial relationship no matter how weak, it often\nidentifies too many relationships to be useful. What is needed is a way of\nidentifying a smaller set of relationships that merit detailed further\nanalysis.\n  Here we formally present and characterize equitability, a property of\nmeasures of dependence that aims to overcome this challenge. Notionally, an\nequitable statistic is a statistic that, given some measure of noise, assigns\nsimilar scores to equally noisy relationships of different types [Reshef et al.\n2011]. We begin by formalizing this idea via a new object called the\ninterpretable interval, which functions as an interval estimate of the amount\nof noise in a relationship of unknown type. We define an equitable statistic as\none with small interpretable intervals.\n  We then draw on the equivalence of interval estimation and hypothesis testing\nto show that under moderate assumptions an equitable statistic is one that\nyields well powered tests for distinguishing not only between trivial and\nnon-trivial relationships of all kinds but also between non-trivial\nrelationships of different strengths. This means that equitability allows us to\nspecify a threshold relationship strength $x_0$ and to search for relationships\nof all kinds with strength greater than $x_0$. Thus, equitability can be\nthought of as a strengthening of power against independence that enables\nfruitful analysis of data sets with a small number of strong, interesting\nrelationships and a large number of weaker ones. We conclude with a\ndemonstration of how our two equivalent characterizations of equitability can\nbe used to evaluate the equitability of a statistic in practice.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2015 00:31:23 GMT"}, {"version": "v2", "created": "Tue, 12 May 2015 20:05:17 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Reshef", "Yakir A.", ""], ["Reshef", "David N.", ""], ["Sabeti", "Pardis C.", ""], ["Mitzenmacher", "Michael M.", ""]]}, {"id": "1505.02213", "submitter": "Yakir Reshef", "authors": "Yakir A. Reshef, David N. Reshef, Hilary K. Finucane, Pardis C.\n  Sabeti, Michael M. Mitzenmacher", "title": "Measuring dependence powerfully and equitably", "comments": "Yakir A. Reshef and David N. Reshef are co-first authors, Pardis C.\n  Sabeti and Michael M. Mitzenmacher are co-last authors. This paper, together\n  with arXiv:1505.02212, subsumes arXiv:1408.4908. v3 includes new analyses and\n  exposition", "journal-ref": "J.Mach.Learn.Res. 17 (2016), 1-63", "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.LG math.IT q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a high-dimensional data set we often wish to find the strongest\nrelationships within it. A common strategy is to evaluate a measure of\ndependence on every variable pair and retain the highest-scoring pairs for\nfollow-up. This strategy works well if the statistic used is equitable [Reshef\net al. 2015a], i.e., if, for some measure of noise, it assigns similar scores\nto equally noisy relationships regardless of relationship type (e.g., linear,\nexponential, periodic).\n  In this paper, we introduce and characterize a population measure of\ndependence called MIC*. We show three ways that MIC* can be viewed: as the\npopulation value of MIC, a highly equitable statistic from [Reshef et al.\n2011], as a canonical \"smoothing\" of mutual information, and as the supremum of\nan infinite sequence defined in terms of optimal one-dimensional partitions of\nthe marginals of the joint distribution. Based on this theory, we introduce an\nefficient approach for computing MIC* from the density of a pair of random\nvariables, and we define a new consistent estimator MICe for MIC* that is\nefficiently computable. In contrast, there is no known polynomial-time\nalgorithm for computing the original equitable statistic MIC. We show through\nsimulations that MICe has better bias-variance properties than MIC. We then\nintroduce and prove the consistency of a second statistic, TICe, that is a\ntrivial side-product of the computation of MICe and whose goal is powerful\nindependence testing rather than equitability.\n  We show in simulations that MICe and TICe have good equitability and power\nagainst independence respectively. The analyses here complement a more in-depth\nempirical evaluation of several leading measures of dependence [Reshef et al.\n2015b] that shows state-of-the-art performance for MICe and TICe.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2015 00:31:54 GMT"}, {"version": "v2", "created": "Tue, 12 May 2015 19:51:46 GMT"}, {"version": "v3", "created": "Wed, 6 Jul 2016 18:42:36 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Reshef", "Yakir A.", ""], ["Reshef", "David N.", ""], ["Finucane", "Hilary K.", ""], ["Sabeti", "Pardis C.", ""], ["Mitzenmacher", "Michael M.", ""]]}, {"id": "1505.02214", "submitter": "Yakir Reshef", "authors": "David N. Reshef, Yakir A. Reshef, Pardis C. Sabeti, Michael M.\n  Mitzenmacher", "title": "An Empirical Study of Leading Measures of Dependence", "comments": "David N. Reshef and Yakir A. Reshef are co-first authors, Pardis C.\n  Sabeti and Michael M. Mitzenmacher are co-last authors", "journal-ref": "Ann.Appl.Stat. 12 (2018) 123-155", "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.LG math.IT q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In exploratory data analysis, we are often interested in identifying\npromising pairwise associations for further analysis while filtering out\nweaker, less interesting ones. This can be accomplished by computing a measure\nof dependence on all variable pairs and examining the highest-scoring pairs,\nprovided the measure of dependence used assigns similar scores to equally noisy\nrelationships of different types. This property, called equitability, is\nformalized in Reshef et al. [2015b]. In addition to equitability, measures of\ndependence can also be assessed by the power of their corresponding\nindependence tests as well as their runtime.\n  Here we present extensive empirical evaluation of the equitability, power\nagainst independence, and runtime of several leading measures of dependence.\nThese include two statistics introduced in Reshef et al. [2015a]: MICe, which\nhas equitability as its primary goal, and TICe, which has power against\nindependence as its goal. Regarding equitability, our analysis finds that MICe\nis the most equitable method on functional relationships in most of the\nsettings we considered, although mutual information estimation proves the most\nequitable at large sample sizes in some specific settings. Regarding power\nagainst independence, we find that TICe, along with Heller and Gorfine's S^DDP,\nis the state of the art on the relationships we tested. Our analyses also show\na trade-off between power against independence and equitability consistent with\nthe theory in Reshef et al. [2015b]. In terms of runtime, MICe and TICe are\nsignificantly faster than many other measures of dependence tested, and\ncomputing either one makes computing the other trivial. This suggests that a\nfast and useful strategy for achieving a combination of power against\nindependence and equitability may be to filter relationships by TICe and then\nto examine the MICe of only the significant ones.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2015 00:32:20 GMT"}, {"version": "v2", "created": "Tue, 12 May 2015 19:54:34 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Reshef", "David N.", ""], ["Reshef", "Yakir A.", ""], ["Sabeti", "Pardis C.", ""], ["Mitzenmacher", "Michael M.", ""]]}, {"id": "1505.02250", "submitter": "Mert Pilanci", "authors": "Mert Pilanci, Martin J. Wainwright", "title": "Newton Sketch: A Linear-time Optimization Algorithm with\n  Linear-Quadratic Convergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a randomized second-order method for optimization known as the\nNewton Sketch: it is based on performing an approximate Newton step using a\nrandomly projected or sub-sampled Hessian. For self-concordant functions, we\nprove that the algorithm has super-linear convergence with exponentially high\nprobability, with convergence and complexity guarantees that are independent of\ncondition numbers and related problem-dependent quantities. Given a suitable\ninitialization, similar guarantees also hold for strongly convex and smooth\nobjectives without self-concordance. When implemented using randomized\nprojections based on a sub-sampled Hadamard basis, the algorithm typically has\nsubstantially lower complexity than Newton's method. We also describe\nextensions of our methods to programs involving convex constraints that are\nequipped with self-concordant barriers. We discuss and illustrate applications\nto linear programs, quadratic programs with convex constraints, logistic\nregression and other generalized linear models, as well as semidefinite\nprograms.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2015 09:36:26 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Pilanci", "Mert", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1505.02288", "submitter": "Alessio Benavoli", "authors": "Alessio Benavoli and Giorgio Corani and Francesca Mangili", "title": "Should we really use post-hoc tests based on mean-ranks?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST physics.data-an q-bio.QM stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical comparison of multiple algorithms over multiple data sets is\nfundamental in machine learning. This is typically carried out by the Friedman\ntest. When the Friedman test rejects the null hypothesis, multiple comparisons\nare carried out to establish which are the significant differences among\nalgorithms. The multiple comparisons are usually performed using the mean-ranks\ntest. The aim of this technical note is to discuss the inconsistencies of the\nmean-ranks post-hoc test with the goal of discouraging its use in machine\nlearning as well as in medicine, psychology, etc.. We show that the outcome of\nthe mean-ranks test depends on the pool of algorithms originally included in\nthe experiment. In other words, the outcome of the comparison between\nalgorithms A and B depends also on the performance of the other algorithms\nincluded in the original experiment. This can lead to paradoxical situations.\nFor instance the difference between A and B could be declared significant if\nthe pool comprises algorithms C, D, E and not significant if the pool comprises\nalgorithms F, G, H. To overcome these issues, we suggest instead to perform the\nmultiple comparison using a test whose outcome only depends on the two\nalgorithms being compared, such as the sign-test or the Wilcoxon signed-rank\ntest.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2015 15:54:56 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Benavoli", "Alessio", ""], ["Corani", "Giorgio", ""], ["Mangili", "Francesca", ""]]}, {"id": "1505.02294", "submitter": "Farideh Fazayeli", "authors": "Arindam Banerjee, Sheng Chen, Farideh Fazayeli, Vidyashankar Sivakumar", "title": "Estimation with Norm Regularization", "comments": "Fixed technical issues. Generalized some results", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of non-asymptotic estimation error and structured statistical\nrecovery based on norm regularized regression, such as Lasso, needs to consider\nfour aspects: the norm, the loss function, the design matrix, and the noise\nmodel. This paper presents generalizations of such estimation error analysis on\nall four aspects compared to the existing literature. We characterize the\nrestricted error set where the estimation error vector lies, establish\nrelations between error sets for the constrained and regularized problems, and\npresent an estimation error bound applicable to any norm. Precise\ncharacterizations of the bound is presented for isotropic as well as\nanisotropic subGaussian design matrices, subGaussian noise models, and convex\nloss functions, including least squares and generalized linear models. Generic\nchaining and associated results play an important role in the analysis. A key\nresult from the analysis is that the sample complexity of all such estimators\ndepends on the Gaussian width of a spherical cap corresponding to the\nrestricted error set. Further, once the number of samples $n$ crosses the\nrequired sample complexity, the estimation error decreases as\n$\\frac{c}{\\sqrt{n}}$, where $c$ depends on the Gaussian width of the unit norm\nball.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2015 17:25:14 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 16:24:01 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2015 20:47:14 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Banerjee", "Arindam", ""], ["Chen", "Sheng", ""], ["Fazayeli", "Farideh", ""], ["Sivakumar", "Vidyashankar", ""]]}, {"id": "1505.02324", "submitter": "Md Abul Hasnat", "authors": "Md. Abul Hasnat, Julien Velcin, St\\'ephane Bonnevay and Julien Jacques", "title": "Simultaneous Clustering and Model Selection for Multinomial\n  Distribution: A Comparative Study", "comments": "Accepted in the International Symposium on Intelligent Data Analysis\n  (IDA 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study different discrete data clustering methods, which use\nthe Model-Based Clustering (MBC) framework with the Multinomial distribution.\nOur study comprises several relevant issues, such as initialization, model\nestimation and model selection. Additionally, we propose a novel MBC method by\nefficiently combining the partitional and hierarchical clustering techniques.\nWe conduct experiments on both synthetic and real data and evaluate the methods\nusing accuracy, stability and computation time. Our study identifies\nappropriate strategies to be used for discrete data analysis with the MBC\nmethods. Moreover, our proposed method is very competitive w.r.t. clustering\naccuracy and better w.r.t. stability and computation time.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2015 22:29:40 GMT"}, {"version": "v2", "created": "Sun, 6 Sep 2015 09:03:48 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Hasnat", "Md. Abul", ""], ["Velcin", "Julien", ""], ["Bonnevay", "St\u00e9phane", ""], ["Jacques", "Julien", ""]]}, {"id": "1505.02343", "submitter": "Qibin Zhao Dr", "authors": "Qibin Zhao, Liqing Zhang, Andrzej Cichocki", "title": "Bayesian Sparse Tucker Models for Dimension Reduction and Tensor\n  Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tucker decomposition is the cornerstone of modern machine learning on\ntensorial data analysis, which have attracted considerable attention for\nmultiway feature extraction, compressive sensing, and tensor completion. The\nmost challenging problem is related to determination of model complexity (i.e.,\nmultilinear rank), especially when noise and missing data are present. In\naddition, existing methods cannot take into account uncertainty information of\nlatent factors, resulting in low generalization performance. To address these\nissues, we present a class of probabilistic generative Tucker models for tensor\ndecomposition and completion with structural sparsity over multilinear latent\nspace. To exploit structural sparse modeling, we introduce two group sparsity\ninducing priors by hierarchial representation of Laplace and Student-t\ndistributions, which facilitates fully posterior inference. For model learning,\nwe derived variational Bayesian inferences over all model (hyper)parameters,\nand developed efficient and scalable algorithms based on multilinear\noperations. Our methods can automatically adapt model complexity and infer an\noptimal multilinear rank by the principle of maximum lower bound of model\nevidence. Experimental results and comparisons on synthetic, chemometrics and\nneuroimaging data demonstrate remarkable performance of our models for\nrecovering ground-truth of multilinear rank and missing entries.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2015 05:17:34 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Zhao", "Qibin", ""], ["Zhang", "Liqing", ""], ["Cichocki", "Andrzej", ""]]}, {"id": "1505.02417", "submitter": "Dustin Tran", "authors": "Panos Toulis, Dustin Tran, Edoardo M. Airoldi", "title": "Towards stability and optimality in stochastic gradient descent", "comments": "Appears in Artificial Intelligence and Statistics, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterative procedures for parameter estimation based on stochastic gradient\ndescent allow the estimation to scale to massive data sets. However, in both\ntheory and practice, they suffer from numerical instability. Moreover, they are\nstatistically inefficient as estimators of the true parameter value. To address\nthese two issues, we propose a new iterative procedure termed averaged implicit\nSGD (AI-SGD). For statistical efficiency, AI-SGD employs averaging of the\niterates, which achieves the optimal Cram\\'{e}r-Rao bound under strong\nconvexity, i.e., it is an optimal unbiased estimator of the true parameter\nvalue. For numerical stability, AI-SGD employs an implicit update at each\niteration, which is related to proximal operators in optimization. In practice,\nAI-SGD achieves competitive performance with other state-of-the-art procedures.\nFurthermore, it is more stable than averaging procedures that do not employ\nproximal updates, and is simple to implement as it requires fewer tunable\nhyperparameters than procedures that do employ proximal updates.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2015 18:10:07 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2015 03:01:53 GMT"}, {"version": "v3", "created": "Fri, 3 Jun 2016 23:11:21 GMT"}, {"version": "v4", "created": "Tue, 7 Jun 2016 04:02:43 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Toulis", "Panos", ""], ["Tran", "Dustin", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "1505.02434", "submitter": "Zhenwen Dai", "authors": "Zhenwen Dai and James Hensman and Neil Lawrence", "title": "Spike and Slab Gaussian Process Latent Variable Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gaussian process latent variable model (GP-LVM) is a popular approach to\nnon-linear probabilistic dimensionality reduction. One design choice for the\nmodel is the number of latent variables. We present a spike and slab prior for\nthe GP-LVM and propose an efficient variational inference procedure that gives\na lower bound of the log marginal likelihood. The new model provides a more\nprincipled approach for selecting latent dimensions than the standard way of\nthresholding the length-scale parameters. The effectiveness of our approach is\ndemonstrated through experiments on real and simulated data. Further, we extend\nmulti-view Gaussian processes that rely on sharing latent dimensions (known as\nmanifold relevance determination) with spike and slab priors. This allows a\nmore principled approach for selecting a subset of the latent space for each\nview of data. The extended model outperforms the previous state-of-the-art when\napplied to a cross-modal multimedia retrieval task.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2015 21:00:21 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Dai", "Zhenwen", ""], ["Hensman", "James", ""], ["Lawrence", "Neil", ""]]}, {"id": "1505.02462", "submitter": "Taichi Kiwaki Mr", "authors": "Taichi Kiwaki", "title": "Soft-Deep Boltzmann Machines", "comments": "Major revision after bug fixes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a layered Boltzmann machine (BM) that can better exploit the\nadvantages of a distributed representation. It is widely believed that deep BMs\n(DBMs) have far greater representational power than its shallow counterpart,\nrestricted Boltzmann machines (RBMs). However, this expectation on the\nsupremacy of DBMs over RBMs has not ever been validated in a theoretical\nfashion. In this paper, we provide both theoretical and empirical evidences\nthat the representational power of DBMs can be actually rather limited in\ntaking advantages of distributed representations. We propose an approximate\nmeasure for the representational power of a BM regarding to the efficiency of a\ndistributed representation. With this measure, we show a surprising fact that\nDBMs can make inefficient use of distributed representations. Based on these\nobservations, we propose an alternative BM architecture, which we dub soft-deep\nBMs (sDBMs). We show that sDBMs can more efficiently exploit the distributed\nrepresentations in terms of the measure. Experiments demonstrate that sDBMs\noutperform several state-of-the-art models, including DBMs, in generative tasks\non binarized MNIST and Caltech-101 silhouettes.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 00:54:43 GMT"}, {"version": "v2", "created": "Tue, 12 May 2015 18:58:39 GMT"}, {"version": "v3", "created": "Sun, 21 Jun 2015 01:41:46 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Kiwaki", "Taichi", ""]]}, {"id": "1505.02475", "submitter": "Bala Rajaratnam", "authors": "Alfred O. Hero and Bala Rajaratnam", "title": "Foundational principles for large scale inference: Illustrations through\n  correlation mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When can reliable inference be drawn in the \"Big Data\" context? This paper\npresents a framework for answering this fundamental question in the context of\ncorrelation mining, with implications for general large scale inference. In\nlarge scale data applications like genomics, connectomics, and eco-informatics\nthe dataset is often variable-rich but sample-starved: a regime where the\nnumber $n$ of acquired samples (statistical replicates) is far fewer than the\nnumber $p$ of observed variables (genes, neurons, voxels, or chemical\nconstituents). Much of recent work has focused on understanding the\ncomputational complexity of proposed methods for \"Big Data.\" Sample complexity\nhowever has received relatively less attention, especially in the setting when\nthe sample size $n$ is fixed, and the dimension $p$ grows without bound. To\naddress this gap, we develop a unified statistical framework that explicitly\nquantifies the sample complexity of various inferential tasks. Sampling regimes\ncan be divided into several categories: 1) the classical asymptotic regime\nwhere the variable dimension is fixed and the sample size goes to infinity; 2)\nthe mixed asymptotic regime where both variable dimension and sample size go to\ninfinity at comparable rates; 3) the purely high dimensional asymptotic regime\nwhere the variable dimension goes to infinity and the sample size is fixed.\nEach regime has its niche but only the latter regime applies to exa-scale data\ndimension. We illustrate this high dimensional framework for the problem of\ncorrelation mining, where it is the matrix of pairwise and partial correlations\namong the variables that are of interest. We demonstrate various regimes of\ncorrelation mining based on the unifying perspective of high dimensional\nlearning rates and sample complexity for different structured covariance models\nand different inference tasks.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 03:35:33 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 18:46:41 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Hero", "Alfred O.", ""], ["Rajaratnam", "Bala", ""]]}, {"id": "1505.02729", "submitter": "Nakul Verma", "authors": "Nakul Verma and Kristin Branson", "title": "Sample complexity of learning Mahalanobis distance metrics", "comments": "26 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric learning seeks a transformation of the feature space that enhances\nprediction quality for the given task at hand. In this work we provide\nPAC-style sample complexity rates for supervised metric learning. We give\nmatching lower- and upper-bounds showing that the sample complexity scales with\nthe representation dimension when no assumptions are made about the underlying\ndata distribution. However, by leveraging the structure of the data\ndistribution, we show that one can achieve rates that are fine-tuned to a\nspecific notion of intrinsic complexity for a given dataset. Our analysis\nreveals that augmenting the metric learning optimization criterion with a\nsimple norm-based regularization can help adapt to a dataset's intrinsic\ncomplexity, yielding better generalization. Experiments on benchmark datasets\nvalidate our analysis and show that regularizing the metric can help discern\nthe signal even when the data contains high amounts of noise.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 18:55:42 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Verma", "Nakul", ""], ["Branson", "Kristin", ""]]}, {"id": "1505.02827", "submitter": "R\\'emi Bardenet", "authors": "R\\'emi Bardenet, Arnaud Doucet, Chris Holmes", "title": "On Markov chain Monte Carlo methods for tall data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo methods are often deemed too computationally\nintensive to be of any practical use for big data applications, and in\nparticular for inference on datasets containing a large number $n$ of\nindividual data points, also known as tall datasets. In scenarios where data\nare assumed independent, various approaches to scale up the Metropolis-Hastings\nalgorithm in a Bayesian inference context have been recently proposed in\nmachine learning and computational statistics. These approaches can be grouped\ninto two categories: divide-and-conquer approaches and, subsampling-based\nalgorithms. The aims of this article are as follows. First, we present a\ncomprehensive review of the existing literature, commenting on the underlying\nassumptions and theoretical guarantees of each method. Second, by leveraging\nour understanding of these limitations, we propose an original\nsubsampling-based approach which samples from a distribution provably close to\nthe posterior distribution of interest, yet can require less than $O(n)$ data\npoint likelihood evaluations at each iteration for certain statistical models\nin favourable scenarios. Finally, we have only been able so far to propose\nsubsampling-based methods which display good performance in scenarios where the\nBernstein-von Mises approximation of the target posterior distribution is\nexcellent. It remains an open challenge to develop such methods in scenarios\nwhere the Bernstein-von Mises approximation is poor.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 22:51:02 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Bardenet", "R\u00e9mi", ""], ["Doucet", "Arnaud", ""], ["Holmes", "Chris", ""]]}, {"id": "1505.02865", "submitter": "Michael Katehakis", "authors": "Wesley Cowan and Michael N. Katehakis", "title": "Asymptotic Behavior of Minimal-Exploration Allocation Policies: Almost\n  Sure, Arbitrarily Slow Growing Regret", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to provide further understanding into the\nstructure of the sequential allocation (\"stochastic multi-armed bandit\", or\nMAB) problem by establishing probability one finite horizon bounds and\nconvergence rates for the sample (or \"pseudo\") regret associated with two\nsimple classes of allocation policies $\\pi$.\n  For any slowly increasing function $g$, subject to mild regularity\nconstraints, we construct two policies (the $g$-Forcing, and the $g$-Inflated\nSample Mean) that achieve a measure of regret of order $ O(g(n))$ almost surely\nas $n \\to \\infty$, bound from above and below. Additionally, almost sure upper\nand lower bounds on the remainder term are established. In the constructions\nherein, the function $g$ effectively controls the \"exploration\" of the\nclassical \"exploration/exploitation\" tradeoff.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 03:35:47 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2015 16:35:04 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Cowan", "Wesley", ""], ["Katehakis", "Michael N.", ""]]}, {"id": "1505.02867", "submitter": "Charles Mathy", "authors": "Charles Mathy, Nate Derbinsky, Jos\\'e Bento, Jonathan Rosenthal and\n  Jonathan Yedidia", "title": "The Boundary Forest Algorithm for Online Supervised and Unsupervised\n  Learning", "comments": "7 pages, 4 figs, 1 page supp. info", "journal-ref": "Proc. of the 29th AAAI Conference on Artificial Intelligence\n  (AAAI), 2864-2870. Austin, TX, USA. (2015)", "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new instance-based learning algorithm called the Boundary\nForest (BF) algorithm, that can be used for supervised and unsupervised\nlearning. The algorithm builds a forest of trees whose nodes store previously\nseen examples. It can be shown data points one at a time and updates itself\nincrementally, hence it is naturally online. Few instance-based algorithms have\nthis property while being simultaneously fast, which the BF is. This is crucial\nfor applications where one needs to respond to input data in real time. The\nnumber of children of each node is not set beforehand but obtained from the\ntraining procedure, which makes the algorithm very flexible with regards to\nwhat data manifolds it can learn. We test its generalization performance and\nspeed on a range of benchmark datasets and detail in which settings it\noutperforms the state of the art. Empirically we find that training time scales\nas O(DNlog(N)) and testing as O(Dlog(N)), where D is the dimensionality and N\nthe amount of data,\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 03:45:11 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Mathy", "Charles", ""], ["Derbinsky", "Nate", ""], ["Bento", "Jos\u00e9", ""], ["Rosenthal", "Jonathan", ""], ["Yedidia", "Jonathan", ""]]}, {"id": "1505.02870", "submitter": "Eliot Brenner", "authors": "Eliot Brenner, David Sontag", "title": "Incorporating Type II Error Probabilities from Independence Tests into\n  Score-Based Learning of Bayesian Network Structure", "comments": "118 pages, 13 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a new consistent scoring function for structure learning of Bayesian\nnetworks. In contrast to traditional approaches to score-based structure\nlearning, such as BDeu or MDL, the complexity penalty that we propose is\ndata-dependent and is given by the probability that a conditional independence\ntest correctly shows that an edge cannot exist. What really distinguishes this\nnew scoring function from earlier work is that it has the property of becoming\ncomputationally easier to maximize as the amount of data increases. We prove a\npolynomial sample complexity result, showing that maximizing this score is\nguaranteed to correctly learn a structure with no false edges and a\ndistribution close to the generating distribution, whenever there exists a\nBayesian network which is a perfect map for the data generating distribution.\nAlthough the new score can be used with any search algorithm, in our related\nUAI 2013 paper [BS13], we have given empirical results showing that it is\nparticularly effective when used together with a linear programming relaxation\napproach to Bayesian network structure learning. The present paper contains all\ndetails of the proofs of the finite-sample complexity results in [BS13] as well\nas detailed explanation of the computation of the certain error probabilities\ncalled beta-values, whose precomputation and tabulation is necessary for the\nimplementation of the algorithm in [BS13].\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 04:31:08 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Brenner", "Eliot", ""], ["Sontag", "David", ""]]}, {"id": "1505.02910", "submitter": "Ilya Tolstikhin", "authors": "Ilya Tolstikhin, Nikita Zhivotovskiy, Gilles Blanchard", "title": "Permutational Rademacher Complexity: a New Complexity Measure for\n  Transductive Learning", "comments": "Corrected error in Inequality (1)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transductive learning considers situations when a learner observes $m$\nlabelled training points and $u$ unlabelled test points with the final goal of\ngiving correct answers for the test points. This paper introduces a new\ncomplexity measure for transductive learning called Permutational Rademacher\nComplexity (PRC) and studies its properties. A novel symmetrization inequality\nis proved, which shows that PRC provides a tighter control over expected\nsuprema of empirical processes compared to what happens in the standard i.i.d.\nsetting. A number of comparison results are also provided, which show the\nrelation between PRC and other popular complexity measures used in statistical\nlearning theory, including Rademacher complexity and Transductive Rademacher\nComplexity (TRC). We argue that PRC is a more suitable complexity measure for\ntransductive learning. Finally, these results are combined with a standard\nconcentration argument to provide novel data-dependent risk bounds for\ntransductive learning.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 08:49:54 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2016 11:56:33 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Tolstikhin", "Ilya", ""], ["Zhivotovskiy", "Nikita", ""], ["Blanchard", "Gilles", ""]]}, {"id": "1505.03001", "submitter": "Ofer Shwartz", "authors": "Ofer Shwartz and Boaz Nadler", "title": "Detecting the large entries of a sparse covariance matrix in\n  sub-quadratic time", "comments": null, "journal-ref": null, "doi": "10.1093/imaiai/iaw004", "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The covariance matrix of a $p$-dimensional random variable is a fundamental\nquantity in data analysis. Given $n$ i.i.d. observations, it is typically\nestimated by the sample covariance matrix, at a computational cost of\n$O(np^{2})$ operations. When $n,p$ are large, this computation may be\nprohibitively slow. Moreover, in several contemporary applications, the\npopulation matrix is approximately sparse, and only its few large entries are\nof interest. This raises the following question, at the focus of our work:\nAssuming approximate sparsity of the covariance matrix, can its large entries\nbe detected much faster, say in sub-quadratic time, without explicitly\ncomputing all its $p^{2}$ entries? In this paper, we present and theoretically\nanalyze two randomized algorithms that detect the large entries of an\napproximately sparse sample covariance matrix using only $O(np\\text{ poly log }\np)$ operations. Furthermore, assuming sparsity of the population matrix, we\nderive sufficient conditions on the underlying random variable and on the\nnumber of samples $n$, for the sample covariance matrix to satisfy our\napproximate sparsity requirements. Finally, we illustrate the performance of\nour algorithms via several simulations.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 13:30:06 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2015 08:58:27 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Shwartz", "Ofer", ""], ["Nadler", "Boaz", ""]]}, {"id": "1505.03036", "submitter": "Bernhard Sch\\\"olkopf", "authors": "Bernhard Sch\\\"olkopf, David W. Hogg, Dun Wang, Daniel Foreman-Mackey,\n  Dominik Janzing, Carl-Johann Simon-Gabriel, Jonas Peters", "title": "Removing systematic errors for exoplanet search via latent causes", "comments": "Extended version of a paper appearing in the Proceedings of the 32nd\n  International Conference on Machine Learning, Lille, France, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML astro-ph.EP astro-ph.IM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method for removing the effect of confounders in order to\nreconstruct a latent quantity of interest. The method, referred to as\nhalf-sibling regression, is inspired by recent work in causal inference using\nadditive noise models. We provide a theoretical justification and illustrate\nthe potential of the method in a challenging astronomy application.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 14:49:08 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Sch\u00f6lkopf", "Bernhard", ""], ["Hogg", "David W.", ""], ["Wang", "Dun", ""], ["Foreman-Mackey", "Daniel", ""], ["Janzing", "Dominik", ""], ["Simon-Gabriel", "Carl-Johann", ""], ["Peters", "Jonas", ""]]}, {"id": "1505.03257", "submitter": "Xinyang Yi", "authors": "Xinyang Yi, Zhaoran Wang, Constantine Caramanis and Han Liu", "title": "Optimal linear estimation under unknown nonlinear transform", "comments": "25 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear regression studies the problem of estimating a model parameter\n$\\beta^* \\in \\mathbb{R}^p$, from $n$ observations\n$\\{(y_i,\\mathbf{x}_i)\\}_{i=1}^n$ from linear model $y_i = \\langle\n\\mathbf{x}_i,\\beta^* \\rangle + \\epsilon_i$. We consider a significant\ngeneralization in which the relationship between $\\langle \\mathbf{x}_i,\\beta^*\n\\rangle$ and $y_i$ is noisy, quantized to a single bit, potentially nonlinear,\nnoninvertible, as well as unknown. This model is known as the single-index\nmodel in statistics, and, among other things, it represents a significant\ngeneralization of one-bit compressed sensing. We propose a novel spectral-based\nestimation procedure and show that we can recover $\\beta^*$ in settings (i.e.,\nclasses of link function $f$) where previous algorithms fail. In general, our\nalgorithm requires only very mild restrictions on the (unknown) functional\nrelationship between $y_i$ and $\\langle \\mathbf{x}_i,\\beta^* \\rangle$. We also\nconsider the high dimensional setting where $\\beta^*$ is sparse ,and introduce\na two-stage nonconvex framework that addresses estimation challenges in high\ndimensional regimes where $p \\gg n$. For a broad class of link functions\nbetween $\\langle \\mathbf{x}_i,\\beta^* \\rangle$ and $y_i$, we establish minimax\nlower bounds that demonstrate the optimality of our estimators in both the\nclassical and high dimensional regimes.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 06:50:37 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Yi", "Xinyang", ""], ["Wang", "Zhaoran", ""], ["Caramanis", "Constantine", ""], ["Liu", "Han", ""]]}, {"id": "1505.03410", "submitter": "Joseph  Salmon", "authors": "Olivier Fercoq, Alexandre Gramfort, Joseph Salmon", "title": "Mind the duality gap: safer rules for the Lasso", "comments": "erratum to ICML 2015, \"The authors would like to thanks Jalal Fadili\n  and Jingwei Liang for helping clarifying some misleading statements on the\n  equicorrelation set\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Screening rules allow to early discard irrelevant variables from the\noptimization in Lasso problems, or its derivatives, making solvers faster. In\nthis paper, we propose new versions of the so-called $\\textit{safe rules}$ for\nthe Lasso. Based on duality gap considerations, our new rules create safe test\nregions whose diameters converge to zero, provided that one relies on a\nconverging solver. This property helps screening out more variables, for a\nwider range of regularization parameter values. In addition to faster\nconvergence, we prove that we correctly identify the active sets (supports) of\nthe solutions in finite time. While our proposed strategy can cope with any\nsolver, its performance is demonstrated using a coordinate descent algorithm\nparticularly adapted to machine learning use cases. Significant computing time\nreductions are obtained with respect to previous safe rules.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 14:50:34 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2015 14:52:12 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2015 21:12:34 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Fercoq", "Olivier", ""], ["Gramfort", "Alexandre", ""], ["Salmon", "Joseph", ""]]}, {"id": "1505.03442", "submitter": "Xingye Qiao", "authors": "Xingye Qiao", "title": "Noncrossing Ordinal Classification", "comments": "32 pages, 9 figures. Accepted for Publication in Statistics and Its\n  Interface", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordinal data are often seen in real applications. Regular multicategory\nclassification methods are not designed for this data type and a more proper\ntreatment is needed. We consider a framework of ordinal classification which\npools the results from binary classifiers together. An inherent difficulty of\nthis framework is that the class prediction can be ambiguous due to boundary\ncrossing. To fix this issue, we propose a noncrossing ordinal classification\nmethod which materializes the framework by imposing noncrossing constraints. An\nasymptotic study of the proposed method is conducted. We show by simulated and\ndata examples that the proposed method can improve the classification\nperformance for ordinal data without the ambiguity caused by boundary\ncrossings.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 16:05:23 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2015 14:59:36 GMT"}, {"version": "v3", "created": "Mon, 21 Dec 2015 20:37:37 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Qiao", "Xingye", ""]]}, {"id": "1505.03481", "submitter": "Hansi Jiang", "authors": "Hansi Jiang and Carl Meyer", "title": "Relations Between Adjacency and Modularity Graph Partitioning", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the exact linear relation between the leading eigenvector of\nthe unnormalized modularity matrix and the eigenvectors of the adjacency matrix\nis developed. Based on this analysis a method to approximate the leading\neigenvector of the modularity matrix is given, and the relative error of the\napproximation is derived. A complete proof of the equivalence between\nnormalized modularity clustering and normalized adjacency clustering is also\ngiven. Some applications and experiments are given to illustrate and\ncorroborate the points that are made in the theoretical development.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2015 23:13:54 GMT"}, {"version": "v2", "created": "Sat, 30 Mar 2019 05:57:31 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Jiang", "Hansi", ""], ["Meyer", "Carl", ""]]}, {"id": "1505.03511", "submitter": "Kristofer Bouchard", "authors": "Kristofer E. Bouchard", "title": "Bootstrapped Adaptive Threshold Selection for Statistical Model\n  Selection and Estimation", "comments": "10 Pages, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central goal of neuroscience is to understand how activity in the nervous\nsystem is related to features of the external world, or to features of the\nnervous system itself. A common approach is to model neural responses as a\nweighted combination of external features, or vice versa. The structure of the\nmodel weights can provide insight into neural representations. Often, neural\ninput-output relationships are sparse, with only a few inputs contributing to\nthe output. In part to account for such sparsity, structured regularizers are\nincorporated into model fitting optimization. However, by imposing priors,\nstructured regularizers can make it difficult to interpret learned model\nparameters. Here, we investigate a simple, minimally structured model\nestimation method for accurate, unbiased estimation of sparse models based on\nBootstrapped Adaptive Threshold Selection followed by ordinary least-squares\nrefitting (BoATS). Through extensive numerical investigations, we show that\nthis method often performs favorably compared to L1 and L2 regularizers. In\nparticular, for a variety of model distributions and noise levels, BoATS more\naccurately recovers the parameters of sparse models, leading to more\nparsimonious explanations of outputs. Finally, we apply this method to the task\nof decoding human speech production from ECoG recordings.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 19:54:53 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Bouchard", "Kristofer E.", ""]]}, {"id": "1505.03772", "submitter": "Chao Gao", "authors": "Chao Gao, Zongming Ma, Anderson Y. Zhang, Harrison H. Zhou", "title": "Achieving Optimal Misclassification Proportion in Stochastic Block Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SI stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection is a fundamental statistical problem in network data\nanalysis. Many algorithms have been proposed to tackle this problem. Most of\nthese algorithms are not guaranteed to achieve the statistical optimality of\nthe problem, while procedures that achieve information theoretic limits for\ngeneral parameter spaces are not computationally tractable. In this paper, we\npresent a computationally feasible two-stage method that achieves optimal\nstatistical performance in misclassification proportion for stochastic block\nmodel under weak regularity conditions. Our two-stage procedure consists of a\ngeneric refinement step that can take a wide range of weakly consistent\ncommunity detection procedures as initializer, to which the refinement stage\napplies and outputs a community assignment achieving optimal misclassification\nproportion with high probability. The practical effectiveness of the new\nalgorithm is demonstrated by competitive numerical results.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 15:59:00 GMT"}, {"version": "v2", "created": "Tue, 19 May 2015 00:28:26 GMT"}, {"version": "v3", "created": "Thu, 21 May 2015 19:18:20 GMT"}, {"version": "v4", "created": "Fri, 29 May 2015 21:03:56 GMT"}, {"version": "v5", "created": "Sat, 3 Oct 2015 03:52:33 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Gao", "Chao", ""], ["Ma", "Zongming", ""], ["Zhang", "Anderson Y.", ""], ["Zhou", "Harrison H.", ""]]}, {"id": "1505.03898", "submitter": "Ming Yan", "authors": "Xiaolin Huang and Lei Shi and Ming Yan and Johan A.K. Suykens", "title": "Pinball Loss Minimization for One-bit Compressive Sensing: Convex Models\n  and Algorithms", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The one-bit quantization is implemented by one single comparator that\noperates at low power and a high rate. Hence one-bit compressive sensing\n(1bit-CS) becomes attractive in signal processing. When measurements are\ncorrupted by noise during signal acquisition and transmission, 1bit-CS is\nusually modeled as minimizing a loss function with a sparsity constraint. The\none-sided $\\ell_1$ loss and the linear loss are two popular loss functions for\n1bit-CS. To improve the decoding performance on noisy data, we consider the\npinball loss, which provides a bridge between the one-sided $\\ell_1$ loss and\nthe linear loss. Using the pinball loss, two convex models, an elastic-net\npinball model and its modification with the $\\ell_1$-norm constraint, are\nproposed. To efficiently solve them, the corresponding dual coordinate ascent\nalgorithms are designed and their convergence is proved. The numerical\nexperiments confirm the effectiveness of the proposed algorithms and the\nperformance of the pinball loss minimization for 1bit-CS.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 21:51:40 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2018 15:10:14 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Huang", "Xiaolin", ""], ["Shi", "Lei", ""], ["Yan", "Ming", ""], ["Suykens", "Johan A. K.", ""]]}, {"id": "1505.03906", "submitter": "Daniel Roy", "authors": "Gintare Karolina Dziugaite and Daniel M. Roy and Zoubin Ghahramani", "title": "Training generative neural networks via Maximum Mean Discrepancy\n  optimization", "comments": "10 pages, to appear in Uncertainty in Artificial Intelligence (UAI)\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider training a deep neural network to generate samples from an\nunknown distribution given i.i.d. data. We frame learning as an optimization\nminimizing a two-sample test statistic---informally speaking, a good generator\nnetwork produces samples that cause a two-sample test to fail to reject the\nnull hypothesis. As our two-sample test statistic, we use an unbiased estimate\nof the maximum mean discrepancy, which is the centerpiece of the nonparametric\nkernel two-sample test proposed by Gretton et al. (2012). We compare to the\nadversarial nets framework introduced by Goodfellow et al. (2014), in which\nlearning is a two-player game between a generator network and an adversarial\ndiscriminator network, both trained to outwit the other. From this perspective,\nthe MMD statistic plays the role of the discriminator. In addition to empirical\ncomparisons, we prove bounds on the generalization error incurred by optimizing\nthe empirical MMD.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 22:18:42 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Dziugaite", "Gintare Karolina", ""], ["Roy", "Daniel M.", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1505.04085", "submitter": "Parikshit Shah", "authors": "Parikshit Shah, Nikhil Rao, Gongguo Tang", "title": "Optimal Low-Rank Tensor Recovery from Separable Measurements: Four\n  Contractions Suffice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensors play a central role in many modern machine learning and signal\nprocessing applications. In such applications, the target tensor is usually of\nlow rank, i.e., can be expressed as a sum of a small number of rank one\ntensors. This motivates us to consider the problem of low rank tensor recovery\nfrom a class of linear measurements called separable measurements. As specific\nexamples, we focus on two distinct types of separable measurement mechanisms\n(a) Random projections, where each measurement corresponds to an inner product\nof the tensor with a suitable random tensor, and (b) the completion problem\nwhere measurements constitute revelation of a random set of entries. We present\na computationally efficient algorithm, with rigorous and order-optimal sample\ncomplexity results (upto logarithmic factors) for tensor recovery. Our method\nis based on reduction to matrix completion sub-problems and adaptation of\nLeurgans' method for tensor decomposition. We extend the methodology and sample\ncomplexity results to higher order tensors, and experimentally validate our\ntheoretical results.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 15:01:08 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Shah", "Parikshit", ""], ["Rao", "Nikhil", ""], ["Tang", "Gongguo", ""]]}, {"id": "1505.04097", "submitter": "Charmgil Hong", "authors": "Charmgil Hong, Milos Hauskrecht", "title": "MCODE: Multivariate Conditional Outlier Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outlier detection aims to identify unusual data instances that deviate from\nexpected patterns. The outlier detection is particularly challenging when\noutliers are context dependent and when they are defined by unusual\ncombinations of multiple outcome variable values. In this paper, we develop and\nstudy a new conditional outlier detection approach for multivariate outcome\nspaces that works by (1) transforming the conditional detection to the outlier\ndetection problem in a new (unconditional) space and (2) defining outlier\nscores by analyzing the data in the new space. Our approach relies on the\nclassifier chain decomposition of the multi-dimensional classification problem\nthat lets us transform the output space into a probability vector, one\nprobability for each dimension of the output space. Outlier scores applied to\nthese transformed vectors are then used to detect the outliers. Experiments on\nmultiple multi-dimensional classification problems with the different outlier\ninjection rates show that our methodology is robust and able to successfully\nidentify outliers when outliers are either sparse (manifested in one or very\nfew dimensions) or dense (affecting multiple dimensions).\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 15:37:51 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Hong", "Charmgil", ""], ["Hauskrecht", "Milos", ""]]}, {"id": "1505.04137", "submitter": "Harish Ramaswamy", "authors": "Harish G. Ramaswamy and Ambuj Tewari and Shivani Agarwal", "title": "Consistent Algorithms for Multiclass Classification with a Reject Option", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of $n$-class classification ($n\\geq 2$), where the\nclassifier can choose to abstain from making predictions at a given cost, say,\na factor $\\alpha$ of the cost of misclassification. Designing consistent\nalgorithms for such $n$-class classification problems with a `reject option' is\nthe main goal of this paper, thereby extending and generalizing previously\nknown results for $n=2$. We show that the Crammer-Singer surrogate and the one\nvs all hinge loss, albeit with a different predictor than the standard argmax,\nyield consistent algorithms for this problem when $\\alpha=\\frac{1}{2}$. More\ninterestingly, we design a new convex surrogate that is also consistent for\nthis problem when $\\alpha=\\frac{1}{2}$ and operates on a much lower dimensional\nspace ($\\log(n)$ as opposed to $n$). We also generalize all three surrogates to\nbe consistent for any $\\alpha\\in[0, \\frac{1}{2}]$.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 17:49:47 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Ramaswamy", "Harish G.", ""], ["Tewari", "Ambuj", ""], ["Agarwal", "Shivani", ""]]}, {"id": "1505.04214", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas, Aarti Singh", "title": "Algorithmic Connections Between Active Learning and Stochastic Convex\n  Optimization", "comments": "15 pages, published in proceedings of Algorithmic Learning Theory,\n  Springer Berlin Heidelberg, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interesting theoretical associations have been established by recent papers\nbetween the fields of active learning and stochastic convex optimization due to\nthe common role of feedback in sequential querying mechanisms. In this paper,\nwe continue this thread in two parts by exploiting these relations for the\nfirst time to yield novel algorithms in both fields, further motivating the\nstudy of their intersection. First, inspired by a recent optimization algorithm\nthat was adaptive to unknown uniform convexity parameters, we present a new\nactive learning algorithm for one-dimensional thresholds that can yield minimax\nrates by adapting to unknown noise parameters. Next, we show that one can\nperform $d$-dimensional stochastic minimization of smooth uniformly convex\nfunctions when only granted oracle access to noisy gradient signs along any\ncoordinate instead of real-valued gradients, by using a simple randomized\ncoordinate descent procedure where each line search can be solved by\n$1$-dimensional active learning, provably achieving the same error convergence\nrate as having the entire real-valued gradient. Combining these two parts\nyields an algorithm that solves stochastic convex optimization of uniformly\nconvex and smooth functions using only noisy gradient signs by repeatedly\nperforming active learning, achieves optimal rates and is adaptive to all\nunknown convexity and smoothness parameters.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 22:38:28 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Singh", "Aarti", ""]]}, {"id": "1505.04215", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas, Barnabas Poczos, Aarti Singh, Larry Wasserman", "title": "An Analysis of Active Learning With Uniform Feature Noise", "comments": "24 pages, 2 figures, published in the proceedings of the 17th\n  International Conference on Artificial Intelligence and Statistics (AISTATS),\n  2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In active learning, the user sequentially chooses values for feature $X$ and\nan oracle returns the corresponding label $Y$. In this paper, we consider the\neffect of feature noise in active learning, which could arise either because\n$X$ itself is being measured, or it is corrupted in transmission to the oracle,\nor the oracle returns the label of a noisy version of the query point. In\nstatistics, feature noise is known as \"errors in variables\" and has been\nstudied extensively in non-active settings. However, the effect of feature\nnoise in active learning has not been studied before. We consider the\nwell-known Berkson errors-in-variables model with additive uniform noise of\nwidth $\\sigma$.\n  Our simple but revealing setting is that of one-dimensional binary\nclassification setting where the goal is to learn a threshold (point where the\nprobability of a $+$ label crosses half). We deal with regression functions\nthat are antisymmetric in a region of size $\\sigma$ around the threshold and\nalso satisfy Tsybakov's margin condition around the threshold. We prove minimax\nlower and upper bounds which demonstrate that when $\\sigma$ is smaller than the\nminimiax active/passive noiseless error derived in \\cite{CN07}, then noise has\nno effect on the rates and one achieves the same noiseless rates. For larger\n$\\sigma$, the \\textit{unflattening} of the regression function on convolution\nwith uniform noise, along with its local antisymmetry around the threshold,\ntogether yield a behaviour where noise \\textit{appears} to be beneficial. Our\nkey result is that active learning can buy significant improvement over a\npassive strategy even in the presence of feature noise.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 22:54:01 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Poczos", "Barnabas", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1505.04243", "submitter": "Paul Grigas", "authors": "Robert M. Freund, Paul Grigas, Rahul Mazumder", "title": "A New Perspective on Boosting in Linear Regression via Subgradient\n  Optimization and Relatives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyze boosting algorithms in linear regression from a new\nperspective: that of modern first-order methods in convex optimization. We show\nthat classic boosting algorithms in linear regression, namely the incremental\nforward stagewise algorithm (FS$_\\varepsilon$) and least squares boosting\n(LS-Boost($\\varepsilon$)), can be viewed as subgradient descent to minimize the\nloss function defined as the maximum absolute correlation between the features\nand residuals. We also propose a modification of FS$_\\varepsilon$ that yields\nan algorithm for the Lasso, and that may be easily extended to an algorithm\nthat computes the Lasso path for different values of the regularization\nparameter. Furthermore, we show that these new algorithms for the Lasso may\nalso be interpreted as the same master algorithm (subgradient descent), applied\nto a regularized version of the maximum absolute correlation loss function. We\nderive novel, comprehensive computational guarantees for several boosting\nalgorithms in linear regression (including LS-Boost($\\varepsilon$) and\nFS$_\\varepsilon$) by using techniques of modern first-order methods in convex\noptimization. Our computational guarantees inform us about the statistical\nproperties of boosting algorithms. In particular they provide, for the first\ntime, a precise theoretical description of the amount of data-fidelity and\nregularization imparted by running a boosting algorithm with a prespecified\nlearning rate for a fixed but arbitrary number of iterations, for any dataset.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2015 04:23:08 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Freund", "Robert M.", ""], ["Grigas", "Paul", ""], ["Mazumder", "Rahul", ""]]}, {"id": "1505.04252", "submitter": "Shiqian Ma", "authors": "Tianyi Lin, Shiqian Ma, Shuzhong Zhang", "title": "Global Convergence of Unmodified 3-Block ADMM for a Class of Convex\n  Minimization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The alternating direction method of multipliers (ADMM) has been successfully\napplied to solve structured convex optimization problems due to its superior\npractical performance. The convergence properties of the 2-block ADMM have been\nstudied extensively in the literature. Specifically, it has been proven that\nthe 2-block ADMM globally converges for any penalty parameter $\\gamma>0$. In\nthis sense, the 2-block ADMM allows the parameter to be free, i.e., there is no\nneed to restrict the value for the parameter when implementing this algorithm\nin order to ensure convergence. However, for the 3-block ADMM, Chen \\etal\n\\cite{Chen-admm-failure-2013} recently constructed a counter-example showing\nthat it can diverge if no further condition is imposed. The existing results on\nstudying further sufficient conditions on guaranteeing the convergence of the\n3-block ADMM usually require $\\gamma$ to be smaller than a certain bound, which\nis usually either difficult to compute or too small to make it a practical\nalgorithm. In this paper, we show that the 3-block ADMM still globally\nconverges with any penalty parameter $\\gamma>0$ if the third function $f_3$ in\nthe objective is smooth and strongly convex, and its condition number is in\n$[1,1.0798)$, besides some other mild conditions. This requirement covers an\nimportant class of problems to be called regularized least squares\ndecomposition (RLSD) in this paper.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2015 08:00:54 GMT"}, {"version": "v2", "created": "Tue, 19 May 2015 03:28:26 GMT"}, {"version": "v3", "created": "Wed, 27 May 2015 07:22:52 GMT"}, {"version": "v4", "created": "Wed, 17 Jan 2018 22:44:55 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Lin", "Tianyi", ""], ["Ma", "Shiqian", ""], ["Zhang", "Shuzhong", ""]]}, {"id": "1505.04343", "submitter": "Yining Wang", "authors": "Yining Wang, Aarti Singh", "title": "Provably Correct Algorithms for Matrix Column Subset Selection with\n  Selectively Sampled Data", "comments": "42 pages. Accepted to Journal of Machine Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of matrix column subset selection, which selects a\nsubset of columns from an input matrix such that the input can be well\napproximated by the span of the selected columns. Column subset selection has\nbeen applied to numerous real-world data applications such as population\ngenetics summarization, electronic circuits testing and recommendation systems.\nIn many applications the complete data matrix is unavailable and one needs to\nselect representative columns by inspecting only a small portion of the input\nmatrix. In this paper we propose the first provably correct column subset\nselection algorithms for partially observed data matrices. Our proposed\nalgorithms exhibit different merits and limitations in terms of statistical\naccuracy, computational efficiency, sample complexity and sampling schemes,\nwhich provides a nice exploration of the tradeoff between these desired\nproperties for column subset selection. The proposed methods employ the idea of\nfeedback driven sampling and are inspired by several sampling schemes\npreviously introduced for low-rank matrix approximation tasks (Drineas et al.,\n2008; Frieze et al., 2004; Deshpande and Vempala, 2006; Krishnamurthy and\nSingh, 2014). Our analysis shows that, under the assumption that the input data\nmatrix has incoherent rows but possibly coherent columns, all algorithms\nprovably converge to the best low-rank approximation of the original data as\nnumber of selected columns increases. Furthermore, two of the proposed\nalgorithms enjoy a relative error bound, which is preferred for column subset\nselection and matrix approximation purposes. We also demonstrate through both\ntheoretical and empirical analysis the power of feedback driven sampling\ncompared to uniform random sampling on input matrices with highly correlated\ncolumns.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2015 01:36:27 GMT"}, {"version": "v2", "created": "Wed, 28 Sep 2016 21:06:40 GMT"}, {"version": "v3", "created": "Thu, 25 Jan 2018 00:13:23 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Wang", "Yining", ""], ["Singh", "Aarti", ""]]}, {"id": "1505.04363", "submitter": "Siqi Wu", "authors": "Siqi Wu and Bin Yu", "title": "Local identifiability of $l_1$-minimization dictionary learning: a\n  sufficient and almost necessary condition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the theoretical properties of learning a dictionary from $N$ signals\n$\\mathbf x_i\\in \\mathbb R^K$ for $i=1,...,N$ via $l_1$-minimization. We assume\nthat $\\mathbf x_i$'s are $i.i.d.$ random linear combinations of the $K$ columns\nfrom a complete (i.e., square and invertible) reference dictionary $\\mathbf D_0\n\\in \\mathbb R^{K\\times K}$. Here, the random linear coefficients are generated\nfrom either the $s$-sparse Gaussian model or the Bernoulli-Gaussian model.\nFirst, for the population case, we establish a sufficient and almost necessary\ncondition for the reference dictionary $\\mathbf D_0$ to be locally\nidentifiable, i.e., a local minimum of the expected $l_1$-norm objective\nfunction. Our condition covers both sparse and dense cases of the random linear\ncoefficients and significantly improves the sufficient condition by Gribonval\nand Schnass (2010). In addition, we show that for a complete $\\mu$-coherent\nreference dictionary, i.e., a dictionary with absolute pairwise column\ninner-product at most $\\mu\\in[0,1)$, local identifiability holds even when the\nrandom linear coefficient vector has up to $O(\\mu^{-2})$ nonzeros on average.\nMoreover, our local identifiability results also translate to the finite sample\ncase with high probability provided that the number of signals $N$ scales as\n$O(K\\log K)$.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2015 07:05:50 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 05:29:40 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Wu", "Siqi", ""], ["Yu", "Bin", ""]]}, {"id": "1505.04406", "submitter": "Stephen Bach", "authors": "Stephen H. Bach, Matthias Broecheler, Bert Huang, Lise Getoor", "title": "Hinge-Loss Markov Random Fields and Probabilistic Soft Logic", "comments": null, "journal-ref": "Journal of Machine Learning Research (JMLR), 18(109):1-67, 2017", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental challenge in developing high-impact machine learning\ntechnologies is balancing the need to model rich, structured domains with the\nability to scale to big data. Many important problem areas are both richly\nstructured and large scale, from social and biological networks, to knowledge\ngraphs and the Web, to images, video, and natural language. In this paper, we\nintroduce two new formalisms for modeling structured data, and show that they\ncan both capture rich structure and scale to big data. The first, hinge-loss\nMarkov random fields (HL-MRFs), is a new kind of probabilistic graphical model\nthat generalizes different approaches to convex inference. We unite three\napproaches from the randomized algorithms, probabilistic graphical models, and\nfuzzy logic communities, showing that all three lead to the same inference\nobjective. We then define HL-MRFs by generalizing this unified objective. The\nsecond new formalism, probabilistic soft logic (PSL), is a probabilistic\nprogramming language that makes HL-MRFs easy to define using a syntax based on\nfirst-order logic. We introduce an algorithm for inferring most-probable\nvariable assignments (MAP inference) that is much more scalable than\ngeneral-purpose convex optimization methods, because it uses message passing to\ntake advantage of sparse dependency structures. We then show how to learn the\nparameters of HL-MRFs. The learned HL-MRFs are as accurate as analogous\ndiscrete models, but much more scalable. Together, these algorithms enable\nHL-MRFs and PSL to model rich, structured data at scales not previously\npossible.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2015 15:19:09 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2015 04:32:24 GMT"}, {"version": "v3", "created": "Fri, 17 Nov 2017 00:10:03 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Bach", "Stephen H.", ""], ["Broecheler", "Matthias", ""], ["Huang", "Bert", ""], ["Getoor", "Lise", ""]]}, {"id": "1505.04413", "submitter": "Taco Cohen", "authors": "Taco S. Cohen, Max Welling", "title": "Harmonic Exponential Families on Manifolds", "comments": "fixed typo", "journal-ref": "Proceedings of the International Conference on Machine Learning,\n  2015", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a range of fields including the geosciences, molecular biology, robotics\nand computer vision, one encounters problems that involve random variables on\nmanifolds. Currently, there is a lack of flexible probabilistic models on\nmanifolds that are fast and easy to train. We define an extremely flexible\nclass of exponential family distributions on manifolds such as the torus,\nsphere, and rotation groups, and show that for these distributions the gradient\nof the log-likelihood can be computed efficiently using a non-commutative\ngeneralization of the Fast Fourier Transform (FFT). We discuss applications to\nBayesian camera motion estimation (where harmonic exponential families serve as\nconjugate priors), and modelling of the spatial distribution of earthquakes on\nthe surface of the earth. Our experimental results show that harmonic densities\nyield a significantly higher likelihood than the best competing method, while\nbeing orders of magnitude faster to train.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2015 16:27:14 GMT"}, {"version": "v2", "created": "Wed, 20 May 2015 20:08:41 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Cohen", "Taco S.", ""], ["Welling", "Max", ""]]}, {"id": "1505.04627", "submitter": "Michal Valko", "authors": "Alexandra Carpentier, Michal Valko", "title": "Simple regret for infinitely many armed bandits", "comments": "in 32th International Conference on Machine Learning (ICML 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a stochastic bandit problem with infinitely many arms. In this\nsetting, the learner has no chance of trying all the arms even once and has to\ndedicate its limited number of samples only to a certain number of arms. All\nprevious algorithms for this setting were designed for minimizing the\ncumulative regret of the learner. In this paper, we propose an algorithm aiming\nat minimizing the simple regret. As in the cumulative regret setting of\ninfinitely many armed bandits, the rate of the simple regret will depend on a\nparameter $\\beta$ characterizing the distribution of the near-optimal arms. We\nprove that depending on $\\beta$, our algorithm is minimax optimal either up to\na multiplicative constant or up to a $\\log(n)$ factor. We also provide\nextensions to several important cases: when $\\beta$ is unknown, in a natural\nsetting where the near-optimal arms have a small variance, and in the case of\nunknown time horizon.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 13:16:42 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Carpentier", "Alexandra", ""], ["Valko", "Michal", ""]]}, {"id": "1505.04630", "submitter": "Zhiyuan Tang", "authors": "Zhiyuan Tang, Dong Wang and Zhiyong Zhang", "title": "Recurrent Neural Network Training with Dark Knowledge Transfer", "comments": "ICASSP 2016", "journal-ref": null, "doi": "10.1109/ICASSP.2016.7472809", "report-no": null, "categories": "stat.ML cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs), particularly long short-term memory (LSTM),\nhave gained much attention in automatic speech recognition (ASR). Although some\nsuccessful stories have been reported, training RNNs remains highly\nchallenging, especially with limited training data. Recent research found that\na well-trained model can be used as a teacher to train other child models, by\nusing the predictions generated by the teacher model as supervision. This\nknowledge transfer learning has been employed to train simple neural nets with\na complex one, so that the final performance can reach a level that is\ninfeasible to obtain by regular training. In this paper, we employ the\nknowledge transfer learning approach to train RNNs (precisely LSTM) using a\ndeep neural network (DNN) model as the teacher. This is different from most of\nthe existing research on knowledge transfer learning, since the teacher (DNN)\nis assumed to be weaker than the child (RNN); however, our experiments on an\nASR task showed that it works fairly well: without applying any tricks on the\nlearning scheme, this approach can train RNNs successfully even with limited\ntraining data.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 13:26:02 GMT"}, {"version": "v2", "created": "Wed, 20 May 2015 05:58:15 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2016 03:31:51 GMT"}, {"version": "v4", "created": "Sat, 12 Mar 2016 07:51:36 GMT"}, {"version": "v5", "created": "Sun, 8 May 2016 12:40:35 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Tang", "Zhiyuan", ""], ["Wang", "Dong", ""], ["Zhang", "Zhiyong", ""]]}, {"id": "1505.04650", "submitter": "Mariano Tepper", "authors": "Mariano Tepper and Guillermo Sapiro", "title": "Compressed Nonnegative Matrix Factorization is Fast and Accurate", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2516971", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF) has an established reputation as a\nuseful data analysis technique in numerous applications. However, its usage in\npractical situations is undergoing challenges in recent years. The fundamental\nfactor to this is the increasingly growing size of the datasets available and\nneeded in the information sciences. To address this, in this work we propose to\nuse structured random compression, that is, random projections that exploit the\ndata structure, for two NMF variants: classical and separable. In separable NMF\n(SNMF) the left factors are a subset of the columns of the input matrix. We\npresent suitable formulations for each problem, dealing with different\nrepresentative algorithms within each one. We show that the resulting\ncompressed techniques are faster than their uncompressed variants, vastly\nreduce memory demands, and do not encompass any significant deterioration in\nperformance. The proposed structured random projections for SNMF allow to deal\nwith arbitrarily shaped large matrices, beyond the standard limit of\ntall-and-skinny matrices, granting access to very efficient computations in\nthis general setting. We accompany the algorithmic presentation with\ntheoretical foundations and numerous and diverse examples, showing the\nsuitability of the proposed approaches.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 14:12:22 GMT"}, {"version": "v2", "created": "Sun, 6 Sep 2015 20:22:36 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Tepper", "Mariano", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1505.04732", "submitter": "Luca Martino", "authors": "L. Martino, V. Elvira, D. Luengo, J. Corander", "title": "Layered Adaptive Importance Sampling", "comments": "Related Matlab codes: an iterative version at\n  http://www.lucamartino.altervista.org/CODE_LAIS_v03.zip and a non-iterative\n  version at http://www.lucamartino.altervista.org/LAIS_non_iterative_code.zip,\n  Statistics and Computing, 2016", "journal-ref": null, "doi": "10.1007/s11222-016-9642-5", "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo methods represent the \"de facto\" standard for approximating\ncomplicated integrals involving multidimensional target distributions. In order\nto generate random realizations from the target distribution, Monte Carlo\ntechniques use simpler proposal probability densities to draw candidate\nsamples. The performance of any such method is strictly related to the\nspecification of the proposal distribution, such that unfortunate choices\neasily wreak havoc on the resulting estimators. In this work, we introduce a\nlayered (i.e., hierarchical) procedure to generate samples employed within a\nMonte Carlo scheme. This approach ensures that an appropriate equivalent\nproposal density is always obtained automatically (thus eliminating the risk of\na catastrophic performance), although at the expense of a moderate increase in\nthe complexity. Furthermore, we provide a general unified importance sampling\n(IS) framework, where multiple proposal densities are employed and several IS\nschemes are introduced by applying the so-called deterministic mixture\napproach. Finally, given these schemes, we also propose a novel class of\nadaptive importance samplers using a population of proposals, where the\nadaptation is driven by independent parallel or interacting Markov Chain Monte\nCarlo (MCMC) chains. The resulting algorithms efficiently combine the benefits\nof both IS and MCMC methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 17:40:51 GMT"}, {"version": "v2", "created": "Sat, 30 May 2015 18:34:26 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2016 23:55:32 GMT"}, {"version": "v4", "created": "Sun, 27 Nov 2016 15:48:13 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Martino", "L.", ""], ["Elvira", "V.", ""], ["Luengo", "D.", ""], ["Corander", "J.", ""]]}, {"id": "1505.04778", "submitter": "Soledad Villar", "authors": "Takayuki Iguchi, Dustin G. Mixon, Jesse Peterson, Soledad Villar", "title": "On the tightness of an SDP relaxation of k-means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS cs.LG math.IT math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Recently, Awasthi et al. introduced an SDP relaxation of the $k$-means\nproblem in $\\mathbb R^m$. In this work, we consider a random model for the data\npoints in which $k$ balls of unit radius are deterministically distributed\nthroughout $\\mathbb R^m$, and then in each ball, $n$ points are drawn according\nto a common rotationally invariant probability distribution. For any fixed ball\nconfiguration and probability distribution, we prove that the SDP relaxation of\nthe $k$-means problem exactly recovers these planted clusters with probability\n$1-e^{-\\Omega(n)}$ provided the distance between any two of the ball centers is\n$>2+\\epsilon$, where $\\epsilon$ is an explicit function of the configuration of\nthe ball centers, and can be arbitrarily small when $m$ is large.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 19:50:00 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Iguchi", "Takayuki", ""], ["Mixon", "Dustin G.", ""], ["Peterson", "Jesse", ""], ["Villar", "Soledad", ""]]}, {"id": "1505.04780", "submitter": "Quanquan Gu", "authors": "Huan Gui and Quanquan Gu", "title": "Towards Faster Rates and Oracle Property for Low-Rank Matrix Estimation", "comments": "29 pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified framework for low-rank matrix estimation with nonconvex\npenalties. We first prove that the proposed estimator attains a faster\nstatistical rate than the traditional low-rank matrix estimator with nuclear\nnorm penalty. Moreover, we rigorously show that under a certain condition on\nthe magnitude of the nonzero singular values, the proposed estimator enjoys\noracle property (i.e., exactly recovers the true rank of the matrix), besides\nattaining a faster rate. As far as we know, this is the first work that\nestablishes the theory of low-rank matrix estimation with nonconvex penalties,\nconfirming the advantages of nonconvex penalties for matrix completion.\nNumerical experiments on both synthetic and real world datasets corroborate our\ntheory.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 19:56:25 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2015 19:47:35 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Gui", "Huan", ""], ["Gu", "Quanquan", ""]]}, {"id": "1505.04824", "submitter": "Hamid Reza Feyzmahdavian", "authors": "Hamid Reza Feyzmahdavian, Arda Aytekin, Mikael Johansson", "title": "An Asynchronous Mini-Batch Algorithm for Regularized Stochastic\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mini-batch optimization has proven to be a powerful paradigm for large-scale\nlearning. However, the state of the art parallel mini-batch algorithms assume\nsynchronous operation or cyclic update orders. When worker nodes are\nheterogeneous (due to different computational capabilities or different\ncommunication delays), synchronous and cyclic operations are inefficient since\nthey will leave workers idle waiting for the slower nodes to complete their\ncomputations. In this paper, we propose an asynchronous mini-batch algorithm\nfor regularized stochastic optimization problems with smooth loss functions\nthat eliminates idle waiting and allows workers to run at their maximal update\nrates. We show that by suitably choosing the step-size values, the algorithm\nachieves a rate of the order $O(1/\\sqrt{T})$ for general convex regularization\nfunctions, and the rate $O(1/T)$ for strongly convex regularization functions,\nwhere $T$ is the number of iterations. In both cases, the impact of asynchrony\non the convergence rate of our algorithm is asymptotically negligible, and a\nnear-linear speedup in the number of workers can be expected. Theoretical\nresults are confirmed in real implementations on a distributed computing\ninfrastructure.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 21:19:24 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Feyzmahdavian", "Hamid Reza", ""], ["Aytekin", "Arda", ""], ["Johansson", "Mikael", ""]]}, {"id": "1505.04935", "submitter": "Alina S\\^irbu", "authors": "Alina S\\^irbu and Ozalp Babaoglu", "title": "Towards Data-Driven Autonomics in Data Centers", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continued reliance on human operators for managing data centers is a major\nimpediment for them from ever reaching extreme dimensions. Large computer\nsystems in general, and data centers in particular, will ultimately be managed\nusing predictive computational and executable models obtained through\ndata-science tools, and at that point, the intervention of humans will be\nlimited to setting high-level goals and policies rather than performing\nlow-level operations. Data-driven autonomics, where management and control are\nbased on holistic predictive models that are built and updated using generated\ndata, opens one possible path towards limiting the role of operators in data\ncenters. In this paper, we present a data-science study of a public Google\ndataset collected in a 12K-node cluster with the goal of building and\nevaluating a predictive model for node failures. We use BigQuery, the big data\nSQL platform from the Google Cloud suite, to process massive amounts of data\nand generate a rich feature set characterizing machine state over time. We\ndescribe how an ensemble classifier can be built out of many Random Forest\nclassifiers each trained on these features, to predict if machines will fail in\na future 24-hour window. Our evaluation reveals that if we limit false positive\nrates to 5%, we can achieve true positive rates between 27% and 88% with\nprecision varying between 50% and 72%. We discuss the practicality of including\nour predictive model as the central component of a data-driven autonomic\nmanager and operating it on-line with live data streams (rather than off-line\non data logs). All of the scripts used for BigQuery and classification analyses\nare publicly available from the authors' website.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 09:58:05 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2015 13:45:52 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["S\u00eerbu", "Alina", ""], ["Babaoglu", "Ozalp", ""]]}, {"id": "1505.04966", "submitter": "Alhussein Fawzi", "authors": "Alhussein Fawzi, Mathieu Sinn, Pascal Frossard", "title": "Multi-task additive models with shared transfer functions based on\n  dictionary learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive models form a widely popular class of regression models which\nrepresent the relation between covariates and response variables as the sum of\nlow-dimensional transfer functions. Besides flexibility and accuracy, a key\nbenefit of these models is their interpretability: the transfer functions\nprovide visual means for inspecting the models and identifying domain-specific\nrelations between inputs and outputs. However, in large-scale problems\ninvolving the prediction of many related tasks, learning independently additive\nmodels results in a loss of model interpretability, and can cause overfitting\nwhen training data is scarce. We introduce a novel multi-task learning approach\nwhich provides a corpus of accurate and interpretable additive models for a\nlarge number of related forecasting tasks. Our key idea is to share transfer\nfunctions across models in order to reduce the model complexity and ease the\nexploration of the corpus. We establish a connection with sparse dictionary\nlearning and propose a new efficient fitting algorithm which alternates between\nsparse coding and transfer function updates. The former step is solved via an\nextension of Orthogonal Matching Pursuit, whose properties are analyzed using a\nnovel recovery condition which extends existing results in the literature. The\nlatter step is addressed using a traditional dictionary update rule.\nExperiments on real-world data demonstrate that our approach compares favorably\nto baseline methods while yielding an interpretable corpus of models, revealing\nstructure among the individual tasks and being more robust when training data\nis scarce. Our framework therefore extends the well-known benefits of additive\nmodels to common regression settings possibly involving thousands of tasks.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 12:36:22 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Fawzi", "Alhussein", ""], ["Sinn", "Mathieu", ""], ["Frossard", "Pascal", ""]]}, {"id": "1505.04984", "submitter": "Jonathan Huggins", "authors": "Jonathan H. Huggins and Joshua B. Tenenbaum", "title": "Risk and Regret of Hierarchical Bayesian Learners", "comments": "In Proceedings of the 32nd International Conference on Machine\n  Learning (ICML 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common statistical practice has shown that the full power of Bayesian methods\nis not realized until hierarchical priors are used, as these allow for greater\n\"robustness\" and the ability to \"share statistical strength.\" Yet it is an\nongoing challenge to provide a learning-theoretically sound formalism of such\nnotions that: offers practical guidance concerning when and how best to utilize\nhierarchical models; provides insights into what makes for a good hierarchical\nprior; and, when the form of the prior has been chosen, can guide the choice of\nhyperparameter settings. We present a set of analytical tools for understanding\nhierarchical priors in both the online and batch learning settings. We provide\nregret bounds under log-loss, which show how certain hierarchical models\ncompare, in retrospect, to the best single model in the model class. We also\nshow how to convert a Bayesian log-loss regret bound into a Bayesian risk bound\nfor any bounded loss, a result which may be of independent interest. Risk and\nregret bounds for Student's $t$ and hierarchical Gaussian priors allow us to\nformalize the concepts of \"robustness\" and \"sharing statistical strength.\"\nPriors for feature selection are investigated as well. Our results suggest that\nthe learning-theoretic benefits of using hierarchical priors can often come at\nlittle cost on practical problems.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 13:12:41 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Huggins", "Jonathan H.", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1505.05004", "submitter": "Maxime Gasse", "authors": "Maxime Gasse (DM2L), Alex Aussem (DM2L), Haytham Elghazel (DM2L)", "title": "An Experimental Comparison of Hybrid Algorithms for Bayesian Network\n  Structure Learning", "comments": "arXiv admin note: text overlap with arXiv:1101.5184 by other authors.\n  Lecture notes in computer science, springer, 2012, Machine Learning and\n  Knowledge Discovery in Databases, 7523, pp.58-73", "journal-ref": null, "doi": "10.1007/978-3-642-33460-3_9", "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel hybrid algorithm for Bayesian network structure learning,\ncalled Hybrid HPC (H2PC). It first reconstructs the skeleton of a Bayesian\nnetwork and then performs a Bayesian-scoring greedy hill-climbing search to\norient the edges. It is based on a subroutine called HPC, that combines ideas\nfrom incremental and divide-and-conquer constraint-based methods to learn the\nparents and children of a target variable. We conduct an experimental\ncomparison of H2PC against Max-Min Hill-Climbing (MMHC), which is currently the\nmost powerful state-of-the-art algorithm for Bayesian network structure\nlearning, on several benchmarks with various data sizes. Our extensive\nexperiments show that H2PC outperforms MMHC both in terms of goodness of fit to\nnew data and in terms of the quality of the network structure itself, which is\ncloser to the true dependence structure of the data. The source code (in R) of\nH2PC as well as all data sets used for the empirical tests are publicly\navailable.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 14:15:10 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2015 10:19:46 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Gasse", "Maxime", "", "DM2L"], ["Aussem", "Alex", "", "DM2L"], ["Elghazel", "Haytham", "", "DM2L"]]}, {"id": "1505.05007", "submitter": "Paul Blomstedt PhD", "authors": "Paul Blomstedt, Ritabrata Dutta, Sohan Seth, Alvis Brazma and Samuel\n  Kaski", "title": "Modelling-based experiment retrieval: A case study with gene expression\n  clustering", "comments": "Updated figures. The final version of this article will appear in\n  Bioinformatics (https://bioinformatics.oxfordjournals.org/)", "journal-ref": null, "doi": "10.1093/bioinformatics/btv762", "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Public and private repositories of experimental data are growing\nto sizes that require dedicated methods for finding relevant data. To improve\non the state of the art of keyword searches from annotations, methods for\ncontent-based retrieval have been proposed. In the context of gene expression\nexperiments, most methods retrieve gene expression profiles, requiring each\nexperiment to be expressed as a single profile, typically of case vs. control.\nA more general, recently suggested alternative is to retrieve experiments whose\nmodels are good for modelling the query dataset. However, for very noisy and\nhigh-dimensional query data, this retrieval criterion turns out to be very\nnoisy as well.\n  Results: We propose doing retrieval using a denoised model of the query\ndataset, instead of the original noisy dataset itself. To this end, we\nintroduce a general probabilistic framework, where each experiment is modelled\nseparately and the retrieval is done by finding related models. For retrieval\nof gene expression experiments, we use a probabilistic model called product\npartition model, which induces a clustering of genes that show similar\nexpression patterns across a number of samples. The suggested metric for\nretrieval using clusterings is the normalized information distance. Empirical\nresults finally suggest that inference for the full probabilistic model can be\napproximated with good performance using computationally faster heuristic\nclustering approaches (e.g. $k$-means). The method is highly scalable and\nstraightforward to apply to construct a general-purpose gene expression\nexperiment retrieval method.\n  Availability: The method can be implemented using standard clustering\nalgorithms and normalized information distance, available in many statistical\nsoftware packages.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 14:21:34 GMT"}, {"version": "v2", "created": "Tue, 26 May 2015 11:53:47 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2015 09:12:58 GMT"}, {"version": "v4", "created": "Mon, 4 Jan 2016 15:08:26 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Blomstedt", "Paul", ""], ["Dutta", "Ritabrata", ""], ["Seth", "Sohan", ""], ["Brazma", "Alvis", ""], ["Kaski", "Samuel", ""]]}, {"id": "1505.05114", "submitter": "Yuxin Chen", "authors": "Yuxin Chen, Emmanuel J. Candes", "title": "Solving Random Quadratic Systems of Equations Is Nearly as Easy as\n  Solving Linear Systems", "comments": "accepted to Communications on Pure and Applied Mathematics (CPAM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.NA math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the fundamental problem of solving quadratic systems of equations\nin $n$ variables, where $y_i = |\\langle \\boldsymbol{a}_i, \\boldsymbol{x}\n\\rangle|^2$, $i = 1, \\ldots, m$ and $\\boldsymbol{x} \\in \\mathbb{R}^n$ is\nunknown. We propose a novel method, which starting with an initial guess\ncomputed by means of a spectral method, proceeds by minimizing a nonconvex\nfunctional as in the Wirtinger flow approach. There are several key\ndistinguishing features, most notably, a distinct objective functional and\nnovel update rules, which operate in an adaptive fashion and drop terms bearing\ntoo much influence on the search direction. These careful selection rules\nprovide a tighter initial guess, better descent directions, and thus enhanced\npractical performance. On the theoretical side, we prove that for certain\nunstructured models of quadratic systems, our algorithms return the correct\nsolution in linear time, i.e. in time proportional to reading the data\n$\\{\\boldsymbol{a}_i\\}$ and $\\{y_i\\}$ as soon as the ratio $m/n$ between the\nnumber of equations and unknowns exceeds a fixed numerical constant. We extend\nthe theory to deal with noisy systems in which we only have $y_i \\approx\n|\\langle \\boldsymbol{a}_i, \\boldsymbol{x} \\rangle|^2$ and prove that our\nalgorithms achieve a statistical accuracy, which is nearly un-improvable. We\ncomplement our theoretical study with numerical examples showing that solving\nrandom quadratic systems is both computationally and statistically not much\nharder than solving linear systems of the same size---hence the title of this\npaper. For instance, we demonstrate empirically that the computational cost of\nour algorithm is about four times that of solving a least-squares problem of\nthe same size.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 18:37:07 GMT"}, {"version": "v2", "created": "Tue, 22 Mar 2016 17:05:16 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Chen", "Yuxin", ""], ["Candes", "Emmanuel J.", ""]]}, {"id": "1505.05117", "submitter": "Wesley Tansey", "authors": "Wesley Tansey, Oscar Hernan Madrid Padilla, Arun Sai Suggala, Pradeep\n  Ravikumar", "title": "Vector-Space Markov Random Fields via Exponential Families", "comments": "See https://github.com/tansey/vsmrfs for code", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Vector-Space Markov Random Fields (VS-MRFs), a novel class of\nundirected graphical models where each variable can belong to an arbitrary\nvector space. VS-MRFs generalize a recent line of work on scalar-valued,\nuni-parameter exponential family and mixed graphical models, thereby greatly\nbroadening the class of exponential families available (e.g., allowing\nmultinomial and Dirichlet distributions). Specifically, VS-MRFs are the joint\ngraphical model distributions where the node-conditional distributions belong\nto generic exponential families with general vector space domains. We also\npresent a sparsistent $M$-estimator for learning our class of MRFs that\nrecovers the correct set of edges with high probability. We validate our\napproach via a set of synthetic data experiments as well as a real-world case\nstudy of over four million foods from the popular diet tracking app\nMyFitnessPal. Our results demonstrate that our algorithm performs well\nempirically and that VS-MRFs are capable of capturing and highlighting\ninteresting structure in complex, real-world data. All code for our algorithm\nis open source and publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 18:46:47 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Tansey", "Wesley", ""], ["Padilla", "Oscar Hernan Madrid", ""], ["Suggala", "Arun Sai", ""], ["Ravikumar", "Pradeep", ""]]}, {"id": "1505.05208", "submitter": "Raajen Patel", "authors": "Raajen Patel, Thomas A. Goldstein, Eva L. Dyer, Azalia Mirhoseini, and\n  Richard G. Baraniuk", "title": "oASIS: Adaptive Column Sampling for Kernel Matrix Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel matrices (e.g. Gram or similarity matrices) are essential for many\nstate-of-the-art approaches to classification, clustering, and dimensionality\nreduction. For large datasets, the cost of forming and factoring such kernel\nmatrices becomes intractable. To address this challenge, we introduce a new\nadaptive sampling algorithm called Accelerated Sequential Incoherence Selection\n(oASIS) that samples columns without explicitly computing the entire kernel\nmatrix. We provide conditions under which oASIS is guaranteed to exactly\nrecover the kernel matrix with an optimal number of columns selected. Numerical\nexperiments on both synthetic and real-world datasets demonstrate that oASIS\nachieves performance comparable to state-of-the-art adaptive sampling methods\nat a fraction of the computational cost. The low runtime complexity of oASIS\nand its low memory footprint enable the solution of large problems that are\nsimply intractable using other adaptive methods.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 23:12:01 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Patel", "Raajen", ""], ["Goldstein", "Thomas A.", ""], ["Dyer", "Eva L.", ""], ["Mirhoseini", "Azalia", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1505.05216", "submitter": "Ali Heydari", "authors": "Ali Heydari", "title": "Convergence Analysis of Policy Iteration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive optimal control of nonlinear dynamic systems with deterministic and\nknown dynamics under a known undiscounted infinite-horizon cost function is\ninvestigated. Policy iteration scheme initiated using a stabilizing initial\ncontrol is analyzed in solving the problem. The convergence of the iterations\nand the optimality of the limit functions, which follows from the established\nuniqueness of the solution to the Bellman equation, are the main results of\nthis study. Furthermore, a theoretical comparison between the speed of\nconvergence of policy iteration versus value iteration is presented. Finally,\nthe convergence results are extended to the case of multi-step look-ahead\npolicy iteration.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 00:49:14 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Heydari", "Ali", ""]]}, {"id": "1505.05229", "submitter": "Tyler Massaro", "authors": "T. J. Massaro and H. Bozdogan", "title": "Variable subset selection via GA and information complexity in mixtures\n  of Poisson and negative binomial regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Count data, for example the number of observed cases of a disease in a city,\noften arise in the fields of healthcare analytics and epidemiology. In this\npaper, we consider performing regression on multivariate data in which our\noutcome is a count. Specifically, we derive log-likelihood functions for finite\nmixtures of regression models involving counts that come from a Poisson\ndistribution, as well as a negative binomial distribution when the counts are\nsignificantly overdispersed. Within our proposed modeling framework, we carry\nout optimal component selection using the information criteria scores AIC, BIC,\nCAIC, and ICOMP. We demonstrate applications of our approach on simulated data,\nas well as on a real data set of HIV cases in Tennessee counties from the year\n2010. Finally, using a genetic algorithm within our framework, we perform\nvariable subset selection to determine the covariates that are most responsible\nfor categorizing Tennessee counties. This leads to some interesting insights\ninto the traits of counties that have high HIV counts.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 02:36:13 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Massaro", "T. J.", ""], ["Bozdogan", "H.", ""]]}, {"id": "1505.05310", "submitter": "Ahmed Hefny", "authors": "Ahmed Hefny, Carlton Downey and Geoffrey Gordon", "title": "Supervised Learning for Dynamical System Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there has been substantial interest in spectral methods for learning\ndynamical systems. These methods are popular since they often offer a good\ntradeoff between computational and statistical efficiency. Unfortunately, they\ncan be difficult to use and extend in practice: e.g., they can make it\ndifficult to incorporate prior information such as sparsity or structure. To\naddress this problem, we present a new view of dynamical system learning: we\nshow how to learn dynamical systems by solving a sequence of ordinary\nsupervised learning problems, thereby allowing users to incorporate prior\nknowledge via standard techniques such as L1 regularization. Many existing\nspectral methods are special cases of this new framework, using linear\nregression as the supervised learner. We demonstrate the effectiveness of our\nframework by showing examples where nonlinear regression or lasso let us learn\nbetter state representations than plain linear regression does; the correctness\nof these instances follows directly from our general analysis.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 10:38:44 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2015 16:16:04 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Hefny", "Ahmed", ""], ["Downey", "Carlton", ""], ["Gordon", "Geoffrey", ""]]}, {"id": "1505.05424", "submitter": "Charles Blundell", "authors": "Charles Blundell and Julien Cornebise and Koray Kavukcuoglu and Daan\n  Wierstra", "title": "Weight Uncertainty in Neural Networks", "comments": "In Proceedings of the 32nd International Conference on Machine\n  Learning (ICML 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new, efficient, principled and backpropagation-compatible\nalgorithm for learning a probability distribution on the weights of a neural\nnetwork, called Bayes by Backprop. It regularises the weights by minimising a\ncompression cost, known as the variational free energy or the expected lower\nbound on the marginal likelihood. We show that this principled kind of\nregularisation yields comparable performance to dropout on MNIST\nclassification. We then demonstrate how the learnt uncertainty in the weights\ncan be used to improve generalisation in non-linear regression problems, and\nhow this weight uncertainty can be used to drive the exploration-exploitation\ntrade-off in reinforcement learning.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 15:39:48 GMT"}, {"version": "v2", "created": "Thu, 21 May 2015 14:07:23 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Blundell", "Charles", ""], ["Cornebise", "Julien", ""], ["Kavukcuoglu", "Koray", ""], ["Wierstra", "Daan", ""]]}, {"id": "1505.05461", "submitter": "Karl Rohe", "authors": "Karl Rohe", "title": "Network driven sampling; a critical threshold for design effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web crawling, snowball sampling, and respondent-driven sampling (RDS) are\nthree types of network sampling techniques used to contact individuals in\nhard-to-reach populations. This paper studies these procedures as a Markov\nprocess on the social network that is indexed by a tree. Each node in this tree\ncorresponds to an observation and each edge in the tree corresponds to a\nreferral. Indexing with a tree (instead of a chain) allows for the sampled\nunits to refer multiple future units into the sample. In survey sampling, the\ndesign effect characterizes the additional variance induced by a novel sampling\nstrategy. If the design effect is some value $DE$, then constructing an\nestimator from the novel design makes the variance of the estimator $DE$ times\ngreater than it would be under a simple random sample with the same sample size\n$n$. Under certain assumptions on the referral tree, the design effect of\nnetwork sampling has a critical threshold that is a function of the referral\nrate $m$ and the clustering structure in the social network, represented by the\nsecond eigenvalue of the Markov transition matrix, $\\lambda_2$. If $m <\n1/\\lambda_2^2$, then the design effect is finite (i.e. the standard estimator\nis $\\sqrt{n}$-consistent). However, if $m > 1/\\lambda_2^2$, then the design\neffect grows with $n$ (i.e. the standard estimator is no longer\n$\\sqrt{n}$-consistent). Past this critical threshold, the standard error of the\nestimator converges at the slower rate of $n^{\\log_m \\lambda_2}$. The Markov\nmodel allows for nodes to be resampled; computational results show that the\nfindings hold in without-replacement sampling. To estimate confidence intervals\nthat adapt to the correct level of uncertainty, a novel resampling procedure is\nproposed. Computational experiments compare this procedure to previous\ntechniques.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 17:36:03 GMT"}, {"version": "v2", "created": "Tue, 26 May 2015 13:01:27 GMT"}, {"version": "v3", "created": "Fri, 28 Aug 2015 22:43:28 GMT"}, {"version": "v4", "created": "Tue, 8 Sep 2015 21:53:47 GMT"}, {"version": "v5", "created": "Thu, 1 Jun 2017 11:27:37 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Rohe", "Karl", ""]]}, {"id": "1505.05561", "submitter": "Devansh Arpit", "authors": "Devansh Arpit, Yingbo Zhou, Hung Ngo, Venu Govindaraju", "title": "Why Regularized Auto-Encoders learn Sparse Representation?", "comments": "8 pages of content, 1 page of reference, 4 pages of supplementary.\n  ICML 2016; bug fix in lemma 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the authors of Batch Normalization (BN) identify and address an\nimportant problem involved in training deep networks-- \\textit{Internal\nCovariate Shift}-- the current solution has certain drawbacks. For instance, BN\ndepends on batch statistics for layerwise input normalization during training\nwhich makes the estimates of mean and standard deviation of input\n(distribution) to hidden layers inaccurate due to shifting parameter values\n(especially during initial training epochs). Another fundamental problem with\nBN is that it cannot be used with batch-size $ 1 $ during training. We address\nthese drawbacks of BN by proposing a non-adaptive normalization technique for\nremoving covariate shift, that we call \\textit{Normalization Propagation}. Our\napproach does not depend on batch statistics, but rather uses a\ndata-independent parametric estimate of mean and standard-deviation in every\nlayer thus being computationally faster compared with BN. We exploit the\nobservation that the pre-activation before Rectified Linear Units follow\nGaussian distribution in deep networks, and that once the first and second\norder statistics of any given dataset are normalized, we can forward propagate\nthis normalization without the need for recalculating the approximate\nstatistics for hidden layers.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 00:10:46 GMT"}, {"version": "v2", "created": "Fri, 29 May 2015 19:22:37 GMT"}, {"version": "v3", "created": "Wed, 2 Mar 2016 15:29:29 GMT"}, {"version": "v4", "created": "Mon, 23 May 2016 23:04:21 GMT"}, {"version": "v5", "created": "Fri, 17 Jun 2016 23:01:20 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Arpit", "Devansh", ""], ["Zhou", "Yingbo", ""], ["Ngo", "Hung", ""], ["Govindaraju", "Venu", ""]]}, {"id": "1505.05572", "submitter": "Colin LaMont", "authors": "Paul A. Wiggins, Colin H. LaMont", "title": "The development of an information criterion for Change-Point Analysis", "comments": "10 pages + supplement. 5 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change-point analysis is a flexible and computationally tractable tool for\nthe analysis of times series data from systems that transition between discrete\nstates and whose observables are corrupted by noise. The change-point algorithm\nis used to identify the time indices (change points) at which the system\ntransitions between these discrete states. We present a unified\ninformation-based approach to testing for the existence of change points. This\nnew approach reconciles two previously disparate approaches to Change-Point\nAnalysis (frequentist and information-based) for testing transitions between\nstates. The resulting method is statistically principled, parameter and prior\nfree and widely applicable to a wide range of change-point problems.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 01:17:27 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Wiggins", "Paul A.", ""], ["LaMont", "Colin H.", ""]]}, {"id": "1505.05629", "submitter": "Stefano Trac\\`a", "authors": "Stefano Trac\\`a, Cynthia Rudin, and Weiyu Yan", "title": "Regulating Greed Over Time in Multi-Armed Bandits", "comments": "Published in JMLR (Journal of Machine Learning Research). The paper\n  contains algorithms, theorems, proofs, and experimental results with\n  synthetic and real data. url: http://jmlr.org/papers/v22/17-720.html", "journal-ref": "Journal of Machine Learning Research, volume 22, number 3, pages\n  1-99, year 2021", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In retail, there are predictable yet dramatic time-dependent patterns in\ncustomer behavior, such as periodic changes in the number of visitors, or\nincreases in customers just before major holidays. The current paradigm of\nmulti-armed bandit analysis does not take these known patterns into account.\nThis means that for applications in retail, where prices are fixed for periods\nof time, current bandit algorithms will not suffice. This work provides a\nremedy that takes the time-dependent patterns into account, and we show how\nthis remedy is implemented for the UCB, $\\varepsilon$-greedy, and UCB-L\nalgorithms, and also through a new policy called the variable arm pool\nalgorithm. In the corrected methods, exploitation (greed) is regulated over\ntime, so that more exploitation occurs during higher reward periods, and more\nexploration occurs in periods of low reward. In order to understand why regret\nis reduced with the corrected methods, we present a set of bounds that provide\ninsight into why we would want to exploit during periods of high reward, and\ndiscuss the impact on regret. Our proposed methods perform well in experiments,\nand were inspired by a high-scoring entry in the Exploration and Exploitation 3\ncontest using data from Yahoo$!$ Front Page. That entry heavily used\ntime-series methods to regulate greed over time, which was substantially more\neffective than other contextual bandit methods.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 07:34:56 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2016 03:50:58 GMT"}, {"version": "v3", "created": "Mon, 4 Dec 2017 22:18:33 GMT"}, {"version": "v4", "created": "Sat, 13 Feb 2021 05:55:13 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Trac\u00e0", "Stefano", ""], ["Rudin", "Cynthia", ""], ["Yan", "Weiyu", ""]]}, {"id": "1505.05663", "submitter": "Thibaut Horel", "authors": "Jean Pouget-Abadie, Thibaut Horel", "title": "Inferring Graphs from Cascades: A Sparse Recovery Framework", "comments": "Full version of the ICML paper with the same title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Network Inference problem, one seeks to recover the edges of an\nunknown graph from the observations of cascades propagating over this graph. In\nthis paper, we approach this problem from the sparse recovery perspective. We\nintroduce a general model of cascades, including the voter model and the\nindependent cascade model, for which we provide the first algorithm which\nrecovers the graph's edges with high probability and $O(s\\log m)$ measurements\nwhere $s$ is the maximum degree of the graph and $m$ is the number of nodes.\nFurthermore, we show that our algorithm also recovers the edge weights (the\nparameters of the diffusion process) and is robust in the context of\napproximate sparsity. Finally we prove an almost matching lower bound of\n$\\Omega(s\\log\\frac{m}{s})$ and validate our approach empirically on synthetic\ngraphs.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 10:04:42 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Pouget-Abadie", "Jean", ""], ["Horel", "Thibaut", ""]]}, {"id": "1505.05668", "submitter": "Daniele Durante", "authors": "Daniele Durante and David B. Dunson", "title": "Locally Adaptive Dynamic Networks", "comments": null, "journal-ref": "Annals of Applied Statistics (2016). 10, 2203-2232", "doi": "10.1214/16-AOAS971", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our focus is on realistically modeling and forecasting dynamic networks of\nface-to-face contacts among individuals. Important aspects of such data that\nlead to problems with current methods include the tendency of the contacts to\nmove between periods of slow and rapid changes, and the dynamic heterogeneity\nin the actors' connectivity behaviors. Motivated by this application, we\ndevelop a novel method for Locally Adaptive DYnamic (LADY) network inference.\nThe proposed model relies on a dynamic latent space representation in which\neach actor's position evolves in time via stochastic differential equations.\nUsing a state space representation for these stochastic processes and\nP\\'olya-gamma data augmentation, we develop an efficient MCMC algorithm for\nposterior inference along with tractable procedures for online updating and\nforecasting of future networks. We evaluate performance in simulation studies,\nand consider an application to face-to-face contacts among individuals in a\nprimary school.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 10:27:53 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2016 09:22:49 GMT"}, {"version": "v3", "created": "Thu, 18 Aug 2016 14:16:21 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Durante", "Daniele", ""], ["Dunson", "David B.", ""]]}, {"id": "1505.05770", "submitter": "Danilo Jimenez Rezende", "authors": "Danilo Jimenez Rezende and Shakir Mohamed", "title": "Variational Inference with Normalizing Flows", "comments": "Proceedings of the 32nd International Conference on Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The choice of approximate posterior distribution is one of the core problems\nin variational inference. Most applications of variational inference employ\nsimple families of posterior approximations in order to allow for efficient\ninference, focusing on mean-field or other simple structured approximations.\nThis restriction has a significant impact on the quality of inferences made\nusing variational methods. We introduce a new approach for specifying flexible,\narbitrarily complex and scalable approximate posterior distributions. Our\napproximations are distributions constructed through a normalizing flow,\nwhereby a simple initial density is transformed into a more complex one by\napplying a sequence of invertible transformations until a desired level of\ncomplexity is attained. We use this view of normalizing flows to develop\ncategories of finite and infinitesimal flows and provide a unified view of\napproaches for constructing rich posterior approximations. We demonstrate that\nthe theoretical advantages of having posteriors that better match the true\nposterior, combined with the scalability of amortized variational approaches,\nprovides a clear improvement in performance and applicability of variational\ninference.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 15:36:37 GMT"}, {"version": "v2", "created": "Fri, 22 May 2015 09:13:28 GMT"}, {"version": "v3", "created": "Tue, 26 May 2015 15:46:33 GMT"}, {"version": "v4", "created": "Mon, 22 Jun 2015 18:36:32 GMT"}, {"version": "v5", "created": "Mon, 13 Jun 2016 08:46:44 GMT"}, {"version": "v6", "created": "Tue, 14 Jun 2016 09:01:36 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Rezende", "Danilo Jimenez", ""], ["Mohamed", "Shakir", ""]]}, {"id": "1505.05901", "submitter": "Mostafa Rahmani", "authors": "Mostafa Rahmani and George Atia", "title": "Randomized Robust Subspace Recovery for High Dimensional Data Matrices", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing ( Volume: 65, Issue: 6,\n  March15, 15 2017 )", "doi": "10.1109/TSP.2016.2645515", "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores and analyzes two randomized designs for robust Principal\nComponent Analysis (PCA) employing low-dimensional data sketching. In one\ndesign, a data sketch is constructed using random column sampling followed by\nlow dimensional embedding, while in the other, sketching is based on random\ncolumn and row sampling. Both designs are shown to bring about substantial\nsavings in complexity and memory requirements for robust subspace learning over\nconventional approaches that use the full scale data. A characterization of the\nsample and computational complexity of both designs is derived in the context\nof two distinct outlier models, namely, sparse and independent outlier models.\nThe proposed randomized approach can provably recover the correct subspace with\ncomputational and sample complexity that are almost independent of the size of\nthe data. The results of the mathematical analysis are confirmed through\nnumerical simulations using both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 21:04:33 GMT"}, {"version": "v2", "created": "Fri, 8 Apr 2016 22:25:06 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Rahmani", "Mostafa", ""], ["Atia", "George", ""]]}, {"id": "1505.06249", "submitter": "Alexandre Drouin", "authors": "Alexandre Drouin, S\\'ebastien Gigu\\`ere, Maxime D\\'eraspe,\n  Fran\\c{c}ois Laviolette, Mario Marchand, Jacques Corbeil", "title": "Greedy Biomarker Discovery in the Genome with Applications to\n  Antimicrobial Resistance", "comments": "Peer-reviewed and accepted for an oral presentation in the Greed is\n  Great workshop at the International Conference on Machine Learning, Lille,\n  France, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Set Covering Machine (SCM) is a greedy learning algorithm that produces\nsparse classifiers. We extend the SCM for datasets that contain a huge number\nof features. The whole genetic material of living organisms is an example of\nsuch a case, where the number of feature exceeds 10^7. Three human pathogens\nwere used to evaluate the performance of the SCM at predicting antimicrobial\nresistance. Our results show that the SCM compares favorably in terms of\nsparsity and accuracy against L1 and L2 regularized Support Vector Machines and\nCART decision trees. Moreover, the SCM was the only algorithm that could\nconsider the full feature space. For all other algorithms, the latter had to be\nfiltered as a preprocessing step.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 23:29:40 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Drouin", "Alexandre", ""], ["Gigu\u00e8re", "S\u00e9bastien", ""], ["D\u00e9raspe", "Maxime", ""], ["Laviolette", "Fran\u00e7ois", ""], ["Marchand", "Mario", ""], ["Corbeil", "Jacques", ""]]}, {"id": "1505.06279", "submitter": "Bernardino Romera-Paredes", "authors": "Andreas Maurer, Massimiliano Pontil, Bernardino Romera-Paredes", "title": "The Benefit of Multitask Representation Learning", "comments": "To appear in Journal of Machine Learning Research (JMLR). 31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a general method to learn data representations from multiple\ntasks. We provide a justification for this method in both settings of multitask\nlearning and learning-to-learn. The method is illustrated in detail in the\nspecial case of linear feature learning. Conditions on the theoretical\nadvantage offered by multitask representation learning over independent task\nlearning are established. In particular, focusing on the important example of\nhalf-space learning, we derive the regime in which multitask representation\nlearning is beneficial over independent task learning, as a function of the\nsample size, the number of tasks and the intrinsic data dimensionality. Other\npotential applications of our results include multitask feature learning in\nreproducing kernel Hilbert spaces and multilayer, deep networks.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2015 06:37:17 GMT"}, {"version": "v2", "created": "Fri, 25 Mar 2016 11:03:42 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Maurer", "Andreas", ""], ["Pontil", "Massimiliano", ""], ["Romera-Paredes", "Bernardino", ""]]}, {"id": "1505.06292", "submitter": "Or Zuk", "authors": "Avishai Wagner and Or Zuk", "title": "Low-Rank Matrix Recovery from Row-and-Column Affine Measurements", "comments": "ICML 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and study a row-and-column affine measurement scheme for low-rank\nmatrix recovery. Each measurement is a linear combination of elements in one\nrow or one column of a matrix $X$. This setting arises naturally in\napplications from different domains. However, current algorithms developed for\nstandard matrix recovery problems do not perform well in our case, hence the\nneed for developing new algorithms and theory for our problem. We propose a\nsimple algorithm for the problem based on Singular Value Decomposition ($SVD$)\nand least-squares ($LS$), which we term \\alg. We prove that (a simplified\nversion of) our algorithm can recover $X$ exactly with the minimum possible\nnumber of measurements in the noiseless case. In the general noisy case, we\nprove performance guarantees on the reconstruction accuracy under the Frobenius\nnorm. In simulations, our row-and-column design and \\alg algorithm show\nimproved speed, and comparable and in some cases better accuracy compared to\nstandard measurements designs and algorithms. Our theoretical and experimental\nresults suggest that the proposed row-and-column affine measurements scheme,\ntogether with our recovery algorithm, may provide a powerful framework for\naffine matrix reconstruction.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2015 08:45:20 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Wagner", "Avishai", ""], ["Zuk", "Or", ""]]}, {"id": "1505.06443", "submitter": "Timos Papadopoulos", "authors": "Timos Papadopoulos, Stephen Roberts and Kathy Willis", "title": "Detecting bird sound in unknown acoustic background using crowdsourced\n  training data", "comments": "Submitted to 'Big Data Sciences for Bioacoustic Environmental\n  Survey', 10th Advanced Multimodal Information Retrieval int'l summer school,\n  Ermites 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biodiversity monitoring using audio recordings is achievable at a truly\nglobal scale via large-scale deployment of inexpensive, unattended recording\nstations or by large-scale crowdsourcing using recording and species\nrecognition on mobile devices. The ability, however, to reliably identify\nvocalising animal species is limited by the fact that acoustic signatures of\ninterest in such recordings are typically embedded in a diverse and complex\nacoustic background. To avoid the problems associated with modelling such\nbackgrounds, we build generative models of bird sounds and use the concept of\nnovelty detection to screen recordings to detect sections of data which are\nlikely bird vocalisations. We present detection results against various\nacoustic environments and different signal-to-noise ratios. We discuss the\nissues related to selecting the cost function and setting detection thresholds\nin such algorithms. Our methods are designed to be scalable and automatically\napplicable to arbitrary selections of species depending on the specific\ngeographic region and time period of deployment.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2015 14:58:41 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Papadopoulos", "Timos", ""], ["Roberts", "Stephen", ""], ["Willis", "Kathy", ""]]}, {"id": "1505.06475", "submitter": "Wesley Tansey", "authors": "Wesley Tansey and James G. Scott", "title": "A Fast and Flexible Algorithm for the Graph-Fused Lasso", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm for solving the graph-fused lasso (GFL), a method\nfor parameter estimation that operates under the assumption that the signal\ntends to be locally constant over a predefined graph structure. Our key insight\nis to decompose the graph into a set of trails which can then each be solved\nefficiently using techniques for the ordinary (1D) fused lasso. We leverage\nthese trails in a proximal algorithm that alternates between closed form primal\nupdates and fast dual trail updates. The resulting techinque is both faster\nthan previous GFL methods and more flexible in the choice of loss function and\ngraph structure. Furthermore, we present two algorithms for constructing trail\nsets and show empirically that they offer a tradeoff between preprocessing time\nand convergence rate.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2015 20:18:00 GMT"}, {"version": "v2", "created": "Tue, 26 May 2015 23:04:05 GMT"}, {"version": "v3", "created": "Mon, 1 Jun 2015 05:40:58 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Tansey", "Wesley", ""], ["Scott", "James G.", ""]]}, {"id": "1505.06478", "submitter": "Syama Sundar Rangapuram", "authors": "Syama Sundar Rangapuram, Pramod Kaushik Mudrakarta and Matthias Hein", "title": "Tight Continuous Relaxation of the Balanced $k$-Cut Problem", "comments": "Long version of paper accepted at NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral Clustering as a relaxation of the normalized/ratio cut has become\none of the standard graph-based clustering methods. Existing methods for the\ncomputation of multiple clusters, corresponding to a balanced $k$-cut of the\ngraph, are either based on greedy techniques or heuristics which have weak\nconnection to the original motivation of minimizing the normalized cut. In this\npaper we propose a new tight continuous relaxation for any balanced $k$-cut\nproblem and show that a related recently proposed relaxation is in most cases\nloose leading to poor performance in practice. For the optimization of our\ntight continuous relaxation we propose a new algorithm for the difficult\nsum-of-ratios minimization problem which achieves monotonic descent. Extensive\ncomparisons show that our method outperforms all existing approaches for ratio\ncut and other balanced $k$-cut criteria.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2015 20:37:10 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Rangapuram", "Syama Sundar", ""], ["Mudrakarta", "Pramod Kaushik", ""], ["Hein", "Matthias", ""]]}, {"id": "1505.06485", "submitter": "Syama Sundar Rangapuram", "authors": "Syama Sundar Rangapuram and Matthias Hein", "title": "Constrained 1-Spectral Clustering", "comments": "Long version of paper accepted at AISTATS 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important form of prior information in clustering comes in form of\ncannot-link and must-link constraints. We present a generalization of the\npopular spectral clustering technique which integrates such constraints.\nMotivated by the recently proposed $1$-spectral clustering for the\nunconstrained problem, our method is based on a tight relaxation of the\nconstrained normalized cut into a continuous optimization problem. Opposite to\nall other methods which have been suggested for constrained spectral\nclustering, we can always guarantee to satisfy all constraints. Moreover, our\nsoft formulation allows to optimize a trade-off between normalized cut and the\nnumber of violated constraints. An efficient implementation is provided which\nscales to large datasets. We outperform consistently all other proposed methods\nin the experiments.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2015 21:25:44 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Rangapuram", "Syama Sundar", ""], ["Hein", "Matthias", ""]]}, {"id": "1505.06538", "submitter": "J Massey Cashore", "authors": "J. Massey Cashore, Xiaoting Zhao, Alexander A. Alemi, Yujia Liu, Peter\n  I. Frazier", "title": "Clustering via Content-Augmented Stochastic Blockmodels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of the data being created on the web contains interactions between users\nand items. Stochastic blockmodels, and other methods for community detection\nand clustering of bipartite graphs, can infer latent user communities and\nlatent item clusters from this interaction data. These methods, however,\ntypically ignore the items' contents and the information they provide about\nitem clusters, despite the tendency of items in the same latent cluster to\nshare commonalities in content. We introduce content-augmented stochastic\nblockmodels (CASB), which use item content together with user-item interaction\ndata to enhance the user communities and item clusters learned. Comparisons to\nseveral state-of-the-art benchmark methods, on datasets arising from scientists\ninteracting with scientific articles, show that content-augmented stochastic\nblockmodels provide highly accurate clusters with respect to metrics\nrepresentative of the underlying community structure.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 04:19:12 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Cashore", "J. Massey", ""], ["Zhao", "Xiaoting", ""], ["Alemi", "Alexander A.", ""], ["Liu", "Yujia", ""], ["Frazier", "Peter I.", ""]]}, {"id": "1505.06614", "submitter": "Renato De Leone", "authors": "Renato De Leone, Valentina Minnetti", "title": "Electre Tri-Machine Learning Approach to the Record Linkage Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short paper, the Electre Tri-Machine Learning Method, generally used\nto solve ordinal classification problems, is proposed for solving the Record\nLinkage problem. Preliminary experimental results show that, using the Electre\nTri method, high accuracy can be achieved and more than 99% of the matches and\nnonmatches were correctly identified by the procedure.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 13:02:32 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["De Leone", "Renato", ""], ["Minnetti", "Valentina", ""]]}, {"id": "1505.06659", "submitter": "Garvesh Raskutti", "authors": "Garvesh Raskutti and Michael Mahoney", "title": "Statistical and Algorithmic Perspectives on Randomized Sketching for\n  Ordinary Least-Squares -- ICML", "comments": "9 pages, Proceedings of the 32 nd International Conference on Machine\n  Learning, Lille, France, 2015. JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider statistical and algorithmic aspects of solving large-scale\nleast-squares (LS) problems using randomized sketching algorithms. Prior\nresults show that, from an \\emph{algorithmic perspective}, when using sketching\nmatrices constructed from random projections and leverage-score sampling, if\nthe number of samples $r$ much smaller than the original sample size $n$, then\nthe worst-case (WC) error is the same as solving the original problem, up to a\nvery small relative error. From a \\emph{statistical perspective}, one typically\nconsiders the mean-squared error performance of randomized sketching\nalgorithms, when data are generated according to a statistical linear model. In\nthis paper, we provide a rigorous comparison of both perspectives leading to\ninsights on how they differ. To do this, we first develop a framework for\nassessing, in a unified manner, algorithmic and statistical aspects of\nrandomized sketching methods. We then consider the statistical prediction\nefficiency (PE) and the statistical residual efficiency (RE) of the sketched LS\nestimator; and we use our framework to provide upper bounds for several types\nof random projection and random sampling algorithms. Among other results, we\nshow that the RE can be upper bounded when $r$ is much smaller than $n$, while\nthe PE typically requires the number of samples $r$ to be substantially larger.\nLower bounds developed in subsequent work show that our upper bounds on PE can\nnot be improved.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 15:10:53 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Raskutti", "Garvesh", ""], ["Mahoney", "Michael", ""]]}, {"id": "1505.06723", "submitter": "John Paisley", "authors": "San Gultekin, Aonan Zhang and John Paisley", "title": "Stochastic Annealing for Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We empirically evaluate a stochastic annealing strategy for Bayesian\nposterior optimization with variational inference. Variational inference is a\ndeterministic approach to approximate posterior inference in Bayesian models in\nwhich a typically non-convex objective function is locally optimized over the\nparameters of the approximating distribution. We investigate an annealing\nmethod for optimizing this objective with the aim of finding a better local\noptimal solution and compare with deterministic annealing methods and no\nannealing. We show that stochastic annealing can provide clear improvement on\nthe GMM and HMM, while performance on LDA tends to favor deterministic\nannealing methods.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 19:44:36 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Gultekin", "San", ""], ["Zhang", "Aonan", ""], ["Paisley", "John", ""]]}, {"id": "1505.06770", "submitter": "Yang Cao", "authors": "Yang Cao, Andrew Thompson, Meng Wang, Yao Xie", "title": "Sketching for Sequential Change-Point Detection", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study sequential change-point detection procedures based on linear\nsketches of high-dimensional signal vectors using generalized likelihood ratio\n(GLR) statistics. The GLR statistics allow for an unknown post-change mean that\nrepresents an anomaly or novelty. We consider both fixed and time-varying\nprojections, derive theoretical approximations to two fundamental performance\nmetrics: the average run length (ARL) and the expected detection delay (EDD);\nthese approximations are shown to be highly accurate by numerical simulations.\nWe further characterize the relative performance measure of the sketching\nprocedure compared to that without sketching and show that there can be little\nperformance loss when the signal strength is sufficiently large, and enough\nnumber of sketches are used. Finally, we demonstrate the good performance of\nsketching procedures using simulation and real-data examples on solar flare\ndetection and failure detection in power networks.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 22:19:07 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2016 18:58:56 GMT"}, {"version": "v3", "created": "Tue, 26 Apr 2016 15:56:42 GMT"}, {"version": "v4", "created": "Wed, 7 Sep 2016 17:31:08 GMT"}, {"version": "v5", "created": "Thu, 20 Jul 2017 03:25:01 GMT"}, {"version": "v6", "created": "Mon, 30 Apr 2018 04:34:03 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Cao", "Yang", ""], ["Thompson", "Andrew", ""], ["Wang", "Meng", ""], ["Xie", "Yao", ""]]}, {"id": "1505.06807", "submitter": "Ameet Talwalkar", "authors": "Xiangrui Meng, Joseph Bradley, Burak Yavuz, Evan Sparks, Shivaram\n  Venkataraman, Davies Liu, Jeremy Freeman, DB Tsai, Manish Amde, Sean Owen,\n  Doris Xin, Reynold Xin, Michael J. Franklin, Reza Zadeh, Matei Zaharia, Ameet\n  Talwalkar", "title": "MLlib: Machine Learning in Apache Spark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.MS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Apache Spark is a popular open-source platform for large-scale data\nprocessing that is well-suited for iterative machine learning tasks. In this\npaper we present MLlib, Spark's open-source distributed machine learning\nlibrary. MLlib provides efficient functionality for a wide range of learning\nsettings and includes several underlying statistical, optimization, and linear\nalgebra primitives. Shipped with Spark, MLlib supports several languages and\nprovides a high-level API that leverages Spark's rich ecosystem to simplify the\ndevelopment of end-to-end machine learning pipelines. MLlib has experienced a\nrapid growth due to its vibrant open-source community of over 140 contributors,\nand includes extensive documentation to support further growth and to let users\nquickly get up to speed.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 05:12:23 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Meng", "Xiangrui", ""], ["Bradley", "Joseph", ""], ["Yavuz", "Burak", ""], ["Sparks", "Evan", ""], ["Venkataraman", "Shivaram", ""], ["Liu", "Davies", ""], ["Freeman", "Jeremy", ""], ["Tsai", "DB", ""], ["Amde", "Manish", ""], ["Owen", "Sean", ""], ["Xin", "Doris", ""], ["Xin", "Reynold", ""], ["Franklin", "Michael J.", ""], ["Zadeh", "Reza", ""], ["Zaharia", "Matei", ""], ["Talwalkar", "Ameet", ""]]}, {"id": "1505.06812", "submitter": "Purushottam Kar", "authors": "Harikrishna Narasimhan and Purushottam Kar and Prateek Jain", "title": "Optimizing Non-decomposable Performance Measures: A Tale of Two Classes", "comments": "To appear in proceedings of the 32nd International Conference on\n  Machine Learning (ICML 2015)", "journal-ref": "Journal of Machine Learning Research, W&CP 37 (2015)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern classification problems frequently present mild to severe label\nimbalance as well as specific requirements on classification characteristics,\nand require optimizing performance measures that are non-decomposable over the\ndataset, such as F-measure. Such measures have spurred much interest and pose\nspecific challenges to learning algorithms since their non-additive nature\nprecludes a direct application of well-studied large scale optimization methods\nsuch as stochastic gradient descent.\n  In this paper we reveal that for two large families of performance measures\nthat can be expressed as functions of true positive/negative rates, it is\nindeed possible to implement point stochastic updates. The families we consider\nare concave and pseudo-linear functions of TPR, TNR which cover several\npopularly used performance measures such as F-measure, G-mean and H-mean.\n  Our core contribution is an adaptive linearization scheme for these families,\nusing which we develop optimization techniques that enable truly point-based\nstochastic updates. For concave performance measures we propose SPADE, a\nstochastic primal dual solver; for pseudo-linear measures we propose STAMP, a\nstochastic alternate maximization procedure. Both methods have crisp\nconvergence guarantees, demonstrate significant speedups over existing methods\n- often by an order of magnitude or more, and give similar or more accurate\npredictions on test data.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 05:59:33 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Narasimhan", "Harikrishna", ""], ["Kar", "Purushottam", ""], ["Jain", "Prateek", ""]]}, {"id": "1505.06813", "submitter": "Purushottam Kar", "authors": "Purushottam Kar and Harikrishna Narasimhan and Prateek Jain", "title": "Surrogate Functions for Maximizing Precision at the Top", "comments": "To appear in the the proceedings of the 32nd International Conference\n  on Machine Learning (ICML 2015)", "journal-ref": "Journal of Machine Learning Research, W&CP 37 (2015)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of maximizing precision at the top of a ranked list, often dubbed\nPrecision@k (prec@k), finds relevance in myriad learning applications such as\nranking, multi-label classification, and learning with severe label imbalance.\nHowever, despite its popularity, there exist significant gaps in our\nunderstanding of this problem and its associated performance measure.\n  The most notable of these is the lack of a convex upper bounding surrogate\nfor prec@k. We also lack scalable perceptron and stochastic gradient descent\nalgorithms for optimizing this performance measure. In this paper we make key\ncontributions in these directions. At the heart of our results is a family of\ntruly upper bounding surrogates for prec@k. These surrogates are motivated in a\nprincipled manner and enjoy attractive properties such as consistency to prec@k\nunder various natural margin/noise conditions.\n  These surrogates are then used to design a class of novel perceptron\nalgorithms for optimizing prec@k with provable mistake bounds. We also devise\nscalable stochastic gradient descent style methods for this problem with\nprovable convergence bounds. Our proofs rely on novel uniform convergence\nbounds which require an in-depth analysis of the structural properties of\nprec@k and its surrogates. We conclude with experimental results comparing our\nalgorithms with state-of-the-art cutting plane and stochastic gradient\nalgorithms for maximizing prec@k.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 06:01:24 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Kar", "Purushottam", ""], ["Narasimhan", "Harikrishna", ""], ["Jain", "Prateek", ""]]}, {"id": "1505.06814", "submitter": "Francesco  Palmieri A. N.", "authors": "Francesco A. N. Palmieri and Amedeo Buonanno", "title": "Discrete Independent Component Analysis (DICA) with Belief Propagation", "comments": "Sumbitted for publication (May 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply belief propagation to a Bayesian bipartite graph composed of\ndiscrete independent hidden variables and discrete visible variables. The\nnetwork is the Discrete counterpart of Independent Component Analysis (DICA)\nand it is manipulated in a factor graph form for inference and learning. A full\nset of simulations is reported for character images from the MNIST dataset. The\nresults show that the factorial code implemented by the sources contributes to\nbuild a good generative model for the data that can be used in various\ninference modes.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 06:02:05 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Palmieri", "Francesco A. N.", ""], ["Buonanno", "Amedeo", ""]]}, {"id": "1505.06915", "submitter": "Jean-Philippe Vert", "authors": "K\\'evin Vervier (CBIO), Pierre Mah\\'e, Maud Tournoud, Jean-Baptiste\n  Veyrieras, Jean-Philippe Vert (CBIO)", "title": "Large-scale Machine Learning for Metagenomics Sequence Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CE cs.LG q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metagenomics characterizes the taxonomic diversity of microbial communities\nby sequencing DNA directly from an environmental sample. One of the main\nchallenges in metagenomics data analysis is the binning step, where each\nsequenced read is assigned to a taxonomic clade. Due to the large volume of\nmetagenomics datasets, binning methods need fast and accurate algorithms that\ncan operate with reasonable computing requirements. While standard\nalignment-based methods provide state-of-the-art performance, compositional\napproaches that assign a taxonomic class to a DNA read based on the k-mers it\ncontains have the potential to provide faster solutions. In this work, we\ninvestigate the potential of modern, large-scale machine learning\nimplementations for taxonomic affectation of next-generation sequencing reads\nbased on their k-mers profile. We show that machine learning-based\ncompositional approaches benefit from increasing the number of fragments\nsampled from reference genome to tune their parameters, up to a coverage of\nabout 10, and from increasing the k-mer size to about 12. Tuning these models\ninvolves training a machine learning model on about 10 8 samples in 10 7\ndimensions, which is out of reach of standard soft-wares but can be done\nefficiently with modern implementations for large-scale machine learning. The\nresulting models are competitive in terms of accuracy with well-established\nalignment tools for problems involving a small to moderate number of candidate\nspecies, and for reasonable amounts of sequencing errors. We show, however,\nthat compositional approaches are still limited in their ability to deal with\nproblems involving a greater number of species, and more sensitive to\nsequencing errors. We finally confirm that compositional approach achieve\nfaster prediction times, with a gain of 3 to 15 times with respect to the\nBWA-MEM short read mapper, depending on the number of candidate species and the\nlevel of sequencing noise.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 12:02:04 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Vervier", "K\u00e9vin", "", "CBIO"], ["Mah\u00e9", "Pierre", "", "CBIO"], ["Tournoud", "Maud", "", "CBIO"], ["Veyrieras", "Jean-Baptiste", "", "CBIO"], ["Vert", "Jean-Philippe", "", "CBIO"]]}, {"id": "1505.06957", "submitter": "Nicolas Gillis", "authors": "Gabriella Casalino, Nicolas Gillis", "title": "Sequential Dimensionality Reduction for Extracting Localized Features", "comments": "24 pages, 12 figures. New numerical experiments on synthetic data\n  sets, discussion about the convergence", "journal-ref": "Pattern Recoginition 63, pp. 15-29, 2017", "doi": "10.1016/j.patcog.2016.09.006", "report-no": null, "categories": "cs.CV cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear dimensionality reduction techniques are powerful tools for image\nanalysis as they allow the identification of important features in a data set.\nIn particular, nonnegative matrix factorization (NMF) has become very popular\nas it is able to extract sparse, localized and easily interpretable features by\nimposing an additive combination of nonnegative basis elements. Nonnegative\nmatrix underapproximation (NMU) is a closely related technique that has the\nadvantage to identify features sequentially. In this paper, we propose a\nvariant of NMU that is particularly well suited for image analysis as it\nincorporates the spatial information, that is, it takes into account the fact\nthat neighboring pixels are more likely to be contained in the same features,\nand favors the extraction of localized features by looking for sparse basis\nelements. We show that our new approach competes favorably with comparable\nstate-of-the-art techniques on synthetic, facial and hyperspectral image data\nsets.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 14:06:16 GMT"}, {"version": "v2", "created": "Tue, 5 Jul 2016 06:44:58 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Casalino", "Gabriella", ""], ["Gillis", "Nicolas", ""]]}, {"id": "1505.06999", "submitter": "Luis Ortiz", "authors": "Joshua Belanich and Luis E. Ortiz", "title": "Some Open Problems in Optimal AdaBoost and Decision Stumps", "comments": "4 pages, rejected from COLT15 Open Problems May 19, 2015 (submitted\n  April 21, 2015; original 3 pages in COLT-conference format)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The significance of the study of the theoretical and practical properties of\nAdaBoost is unquestionable, given its simplicity, wide practical use, and\neffectiveness on real-world datasets. Here we present a few open problems\nregarding the behavior of \"Optimal AdaBoost,\" a term coined by Rudin,\nDaubechies, and Schapire in 2004 to label the simple version of the standard\nAdaBoost algorithm in which the weak learner that AdaBoost uses always outputs\nthe weak classifier with lowest weighted error among the respective hypothesis\nclass of weak classifiers implicit in the weak learner. We concentrate on the\nstandard, \"vanilla\" version of Optimal AdaBoost for binary classification that\nresults from using an exponential-loss upper bound on the misclassification\ntraining error. We present two types of open problems. One deals with general\nweak hypotheses. The other deals with the particular case of decision stumps,\nas often and commonly used in practice. Answers to the open problems can have\nimmediate significant impact to (1) cementing previously established results on\nasymptotic convergence properties of Optimal AdaBoost, for finite datasets,\nwhich in turn can be the start to any convergence-rate analysis; (2)\nunderstanding the weak-hypotheses class of effective decision stumps generated\nfrom data, which we have empirically observed to be significantly smaller than\nthe typically obtained class, as well as the effect on the weak learner's\nrunning time and previously established improved bounds on the generalization\nperformance of Optimal AdaBoost classifiers; and (3) shedding some light on the\n\"self control\" that AdaBoost tends to exhibit in practice.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 15:18:33 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Belanich", "Joshua", ""], ["Ortiz", "Luis E.", ""]]}, {"id": "1505.07008", "submitter": "Tianwen Wei", "authors": "Tianwen Wei", "title": "An Overview of the Asymptotic Performance of the Family of the FastICA\n  Algorithms", "comments": "To appear in the 12th International Conference on Latent Variable\n  Analysis and Source Separation (LVA/ICA 2015), Liberec, Czech Republic", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This contribution summarizes the results on the asymptotic performance of\nseveral variants of the FastICA algorithm. A number of new closed-form\nexpressions are presented.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 15:26:43 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Wei", "Tianwen", ""]]}, {"id": "1505.07067", "submitter": "Pedro Alejandro Ortega", "authors": "Pedro A. Ortega and Koby Crammer and Daniel D. Lee", "title": "Belief Flows of Robust Online Learning", "comments": "Appears in Workshop on Information Theory and Applications (ITA),\n  February 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new probabilistic model for online learning which\ndynamically incorporates information from stochastic gradients of an arbitrary\nloss function. Similar to probabilistic filtering, the model maintains a\nGaussian belief over the optimal weight parameters. Unlike traditional Bayesian\nupdates, the model incorporates a small number of gradient evaluations at\nlocations chosen using Thompson sampling, making it computationally tractable.\nThe belief is then transformed via a linear flow field which optimally updates\nthe belief distribution using rules derived from information theoretic\nprinciples. Several versions of the algorithm are shown using different\nconstraints on the flow field and compared with conventional online learning\nalgorithms. Results are given for several classification tasks including\nlogistic regression and multilayer neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 17:57:32 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Ortega", "Pedro A.", ""], ["Crammer", "Koby", ""], ["Lee", "Daniel D.", ""]]}, {"id": "1505.07343", "submitter": "Marco Congedo", "authors": "Marco Congedo (GIPSA-VIBS), Bijan Afsari (JHU), Alexandre Barachant\n  (GIPSA-VIBS), Maher Moakher (LAMSIN)", "title": "Approximate Joint Diagonalization and Geometric Mean of Symmetric\n  Positive Definite Matrices", "comments": null, "journal-ref": "PLoS ONE, Public Library of Science, 2015, 10 (4), pp.e0121423", "doi": "10.1371/journal.pone.0121423", "report-no": null, "categories": "math.DG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the connection between two problems that have arisen independently\nin the signal processing and related fields: the estimation of the geometric\nmean of a set of symmetric positive definite (SPD) matrices and their\napproximate joint diagonalization (AJD). Today there is a considerable interest\nin estimating the geometric mean of a SPD matrix set in the manifold of SPD\nmatrices endowed with the Fisher information metric. The resulting mean has\nseveral important invariance properties and has proven very useful in diverse\nengineering applications such as biomedical and image data processing. While\nfor two SPD matrices the mean has an algebraic closed form solution, for a set\nof more than two SPD matrices it can only be estimated by iterative algorithms.\nHowever, none of the existing iterative algorithms feature at the same time\nfast convergence, low computational complexity per iteration and guarantee of\nconvergence. For this reason, recently other definitions of geometric mean\nbased on symmetric divergence measures, such as the Bhattacharyya divergence,\nhave been considered. The resulting means, although possibly useful in\npractice, do not satisfy all desirable invariance properties. In this paper we\nconsider geometric means of co-variance matrices estimated on high-dimensional\ntime-series, assuming that the data is generated according to an instantaneous\nmixing model, which is very common in signal processing. We show that in these\ncircumstances we can approximate the Fisher information geometric mean by\nemploying an efficient AJD algorithm. Our approximation is in general much\ncloser to the Fisher information geometric mean as compared to its competitors\nand verifies many invariance properties. Furthermore, convergence is\nguaranteed, the computational complexity is low and the convergence rate is\nquadratic. The accuracy of this new geometric mean approximation is\ndemonstrated by means of simulations.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 12:25:29 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Congedo", "Marco", "", "GIPSA-VIBS"], ["Afsari", "Bijan", "", "JHU"], ["Barachant", "Alexandre", "", "GIPSA-VIBS"], ["Moakher", "Maher", "", "LAMSIN"]]}, {"id": "1505.07414", "submitter": "Lingzhou Xue", "authors": "Jianqing Fan, Lingzhou Xue and Jiawei Yao", "title": "Sufficient Forecasting Using Factor Models", "comments": "37 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider forecasting a single time series when there is a large number of\npredictors and a possible nonlinear effect. The dimensionality was first\nreduced via a high-dimensional (approximate) factor model implemented by the\nprincipal component analysis. Using the extracted factors, we develop a novel\nforecasting method called the sufficient forecasting, which provides a set of\nsufficient predictive indices, inferred from high-dimensional predictors, to\ndeliver additional predictive power. The projected principal component analysis\nwill be employed to enhance the accuracy of inferred factors when a\nsemi-parametric (approximate) factor model is assumed. Our method is also\napplicable to cross-sectional sufficient regression using extracted factors.\nThe connection between the sufficient forecasting and the deep learning\narchitecture is explicitly stated. The sufficient forecasting correctly\nestimates projection indices of the underlying factors even in the presence of\na nonparametric forecasting function. The proposed method extends the\nsufficient dimension reduction to high-dimensional regimes by condensing the\ncross-sectional information through factor models. We derive asymptotic\nproperties for the estimate of the central subspace spanned by these projection\ndirections as well as the estimates of the sufficient predictive indices. We\nfurther show that the natural method of running multiple regression of target\non estimated factors yields a linear estimate that actually falls into this\ncentral subspace. Our method and theory allow the number of predictors to be\nlarger than the number of observations. We finally demonstrate that the\nsufficient forecasting improves upon the linear forecasting in both simulation\nstudies and an empirical study of forecasting macroeconomic variables.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 17:46:37 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2015 23:25:53 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Fan", "Jianqing", ""], ["Xue", "Lingzhou", ""], ["Yao", "Jiawei", ""]]}, {"id": "1505.07519", "submitter": "Oliver Serang", "authors": "Julianus Pfeuffer and Oliver Serang", "title": "A Bounded $p$-norm Approximation of Max-Convolution for Sub-Quadratic\n  Bayesian Inference on Additive Factors", "comments": null, "journal-ref": "Journal of Machine Learning Research 17 (2016) 1-39", "doi": null, "report-no": null, "categories": "stat.CO cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max-convolution is an important problem closely resembling standard\nconvolution; as such, max-convolution occurs frequently across many fields.\nHere we extend the method with fastest known worst-case runtime, which can be\napplied to nonnegative vectors by numerically approximating the Chebyshev norm\n$\\| \\cdot \\|_\\infty$, and use this approach to derive two numerically stable\nmethods based on the idea of computing $p$-norms via fast convolution: The\nfirst method proposed, with runtime in $O( k \\log(k) \\log(\\log(k)) )$ (which is\nless than $18 k \\log(k)$ for any vectors that can be practically realized),\nuses the $p$-norm as a direct approximation of the Chebyshev norm. The second\napproach proposed, with runtime in $O( k \\log(k) )$ (although in practice both\nperform similarly), uses a novel null space projection method, which extracts\ninformation from a sequence of $p$-norms to estimate the maximum value in the\nvector (this is equivalent to querying a small number of moments from a\ndistribution of bounded support in order to estimate the maximum). The $p$-norm\napproaches are compared to one another and are shown to compute an\napproximation of the Viterbi path in a hidden Markov model where the transition\nmatrix is a Toeplitz matrix; the runtime of approximating the Viterbi path is\nthus reduced from $O( n k^2 )$ steps to $O( n $k \\log(k))$ steps in practice,\nand is demonstrated by inferring the U.S. unemployment rate from the S&P 500\nstock index.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 01:03:29 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2015 00:18:21 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Pfeuffer", "Julianus", ""], ["Serang", "Oliver", ""]]}, {"id": "1505.07649", "submitter": "Lucas Theis", "authors": "Lucas Theis and Matthew D. Hoffman", "title": "A trust-region method for stochastic variational inference with\n  applications to streaming data", "comments": "in Proceedings of the 32nd International Conference on Machine\n  Learning, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic variational inference allows for fast posterior inference in\ncomplex Bayesian models. However, the algorithm is prone to local optima which\ncan make the quality of the posterior approximation sensitive to the choice of\nhyperparameters and initialization. We address this problem by replacing the\nnatural gradient step of stochastic varitional inference with a trust-region\nupdate. We show that this leads to generally better results and reduced\nsensitivity to hyperparameters. We also describe a new strategy for variational\ninference on streaming data and show that here our trust-region method is\ncrucial for getting good performance.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 11:25:55 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Theis", "Lucas", ""], ["Hoffman", "Matthew D.", ""]]}, {"id": "1505.07752", "submitter": "Chitta Ranjan", "authors": "Chitta Ranjan, Kamran Paynabar, Jonathan E. Helm and Julian Pan", "title": "The Impact of Estimation: A New Method for Clustering and Trajectory\n  Estimation in Patient Flow Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to accurately forecast and control inpatient census, and thereby\nworkloads, is a critical and longstanding problem in hospital management.\nMajority of current literature focuses on optimal scheduling of inpatients, but\nlargely ignores the process of accurate estimation of the trajectory of\npatients throughout the treatment and recovery process. The result is that\ncurrent scheduling models are optimizing based on inaccurate input data. We\ndeveloped a Clustering and Scheduling Integrated (CSI) approach to capture\npatient flows through a network of hospital services. CSI functions by\nclustering patients into groups based on similarity of trajectory using a novel\nSemi-Markov model (SMM)-based clustering scheme proposed in this paper, as\nopposed to clustering by admit type or condition as in previous literature. The\nmethodology is validated by simulation and then applied to real patient data\nfrom a partner hospital where we see it outperforms current methods. Further,\nwe demonstrate that extant optimization methods achieve significantly better\nresults on key hospital performance measures under CSI, compared with\ntraditional estimation approaches, increasing elective admissions by 97% and\nutilization by 22% compared to 30% and 8% using traditional estimation\ntechniques. From a theoretical standpoint, the SMM-clustering is a novel\napproach applicable to any temporal-spatial stochastic data that is prevalent\nin many industries and application areas.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 16:47:57 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2015 14:13:03 GMT"}, {"version": "v3", "created": "Fri, 14 Aug 2015 19:16:52 GMT"}, {"version": "v4", "created": "Tue, 12 Jul 2016 16:36:24 GMT"}, {"version": "v5", "created": "Wed, 19 Oct 2016 03:59:24 GMT"}, {"version": "v6", "created": "Wed, 11 Jan 2017 01:17:26 GMT"}, {"version": "v7", "created": "Mon, 30 Jan 2017 02:26:23 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Ranjan", "Chitta", ""], ["Paynabar", "Kamran", ""], ["Helm", "Jonathan E.", ""], ["Pan", "Julian", ""]]}, {"id": "1505.07765", "submitter": "Theofanis Karaletsos", "authors": "Theofanis Karaletsos, Gunnar R\\\"atsch", "title": "Automatic Relevance Determination For Deep Generative Models", "comments": "equations 8-12 updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recurring problem when building probabilistic latent variable models is\nregularization and model selection, for instance, the choice of the\ndimensionality of the latent space. In the context of belief networks with\nlatent variables, this problem has been adressed with Automatic Relevance\nDetermination (ARD) employing Monte Carlo inference. We present a variational\ninference approach to ARD for Deep Generative Models using doubly stochastic\nvariational inference to provide fast and scalable learning. We show empirical\nresults on a standard dataset illustrating the effects of contracting the\nlatent space automatically. We show that the resulting latent representations\nare significantly more compact without loss of expressive power of the learned\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 17:33:43 GMT"}, {"version": "v2", "created": "Sat, 30 May 2015 06:29:17 GMT"}, {"version": "v3", "created": "Wed, 26 Aug 2015 19:56:04 GMT"}], "update_date": "2015-08-27", "authors_parsed": [["Karaletsos", "Theofanis", ""], ["R\u00e4tsch", "Gunnar", ""]]}, {"id": "1505.07818", "submitter": "Pascal Germain", "authors": "Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo\n  Larochelle, Fran\\c{c}ois Laviolette, Mario Marchand, Victor Lempitsky", "title": "Domain-Adversarial Training of Neural Networks", "comments": "Published in JMLR: http://jmlr.org/papers/v17/15-239.html", "journal-ref": "Journal of Machine Learning Research 2016, vol. 17, p. 1-35", "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new representation learning approach for domain adaptation, in\nwhich data at training and test time come from similar but different\ndistributions. Our approach is directly inspired by the theory on domain\nadaptation suggesting that, for effective domain transfer to be achieved,\npredictions must be made based on features that cannot discriminate between the\ntraining (source) and test (target) domains. The approach implements this idea\nin the context of neural network architectures that are trained on labeled data\nfrom the source domain and unlabeled data from the target domain (no labeled\ntarget-domain data is necessary). As the training progresses, the approach\npromotes the emergence of features that are (i) discriminative for the main\nlearning task on the source domain and (ii) indiscriminate with respect to the\nshift between the domains. We show that this adaptation behaviour can be\nachieved in almost any feed-forward model by augmenting it with few standard\nlayers and a new gradient reversal layer. The resulting augmented architecture\ncan be trained using standard backpropagation and stochastic gradient descent,\nand can thus be implemented with little effort using any of the deep learning\npackages. We demonstrate the success of our approach for two distinct\nclassification problems (document sentiment analysis and image classification),\nwhere state-of-the-art domain adaptation performance on standard benchmarks is\nachieved. We also validate the approach for descriptor learning task in the\ncontext of person re-identification application.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 19:34:53 GMT"}, {"version": "v2", "created": "Fri, 29 May 2015 13:32:12 GMT"}, {"version": "v3", "created": "Fri, 27 Nov 2015 16:57:53 GMT"}, {"version": "v4", "created": "Thu, 26 May 2016 19:56:08 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Ganin", "Yaroslav", ""], ["Ustinova", "Evgeniya", ""], ["Ajakan", "Hana", ""], ["Germain", "Pascal", ""], ["Larochelle", "Hugo", ""], ["Laviolette", "Fran\u00e7ois", ""], ["Marchand", "Mario", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1505.07925", "submitter": "Yun Yang", "authors": "Yun Yang, Martin J. Wainwright, Michael I. Jordan", "title": "On the Computational Complexity of High-Dimensional Bayesian Variable\n  Selection", "comments": "42 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational complexity of Markov chain Monte Carlo (MCMC)\nmethods for high-dimensional Bayesian linear regression under sparsity\nconstraints. We first show that a Bayesian approach can achieve\nvariable-selection consistency under relatively mild conditions on the design\nmatrix. We then demonstrate that the statistical criterion of posterior\nconcentration need not imply the computational desideratum of rapid mixing of\nthe MCMC algorithm. By introducing a truncated sparsity prior for variable\nselection, we provide a set of conditions that guarantee both\nvariable-selection consistency and rapid mixing of a particular\nMetropolis-Hastings algorithm. The mixing time is linear in the number of\ncovariates up to a logarithmic factor. Our proof controls the spectral gap of\nthe Markov chain by constructing a canonical path ensemble that is inspired by\nthe steps taken by greedy algorithms for variable selection.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 05:33:22 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Yang", "Yun", ""], ["Wainwright", "Martin J.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1505.08052", "submitter": "Javier Gonz\\'alez", "authors": "Javier Gonz\\'alez, Zhenwen Dai, Philipp Hennig, Neil D. Lawrence", "title": "Batch Bayesian Optimization via Local Penalization", "comments": "11 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popularity of Bayesian optimization methods for efficient exploration of\nparameter spaces has lead to a series of papers applying Gaussian processes as\nsurrogates in the optimization of functions. However, most proposed approaches\nonly allow the exploration of the parameter space to occur sequentially. Often,\nit is desirable to simultaneously propose batches of parameter values to\nexplore. This is particularly the case when large parallel processing\nfacilities are available. These facilities could be computational or physical\nfacets of the process being optimized. E.g. in biological experiments many\nexperimental set ups allow several samples to be simultaneously processed.\nBatch methods, however, require modeling of the interaction between the\nevaluations in the batch, which can be expensive in complex scenarios. We\ninvestigate a simple heuristic based on an estimate of the Lipschitz constant\nthat captures the most important aspect of this interaction (i.e. local\nrepulsion) at negligible computational overhead. The resulting algorithm\ncompares well, in running time, with much more elaborate alternatives. The\napproach assumes that the function of interest, $f$, is a Lipschitz continuous\nfunction. A wrap-loop around the acquisition function is used to collect\nbatches of points of certain size minimizing the non-parallelizable\ncomputational effort. The speed-up of our method with respect to previous\napproaches is significant in a set of computationally expensive experiments.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 14:11:09 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2015 08:20:12 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2015 20:02:21 GMT"}, {"version": "v4", "created": "Thu, 15 Oct 2015 00:16:57 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Gonz\u00e1lez", "Javier", ""], ["Dai", "Zhenwen", ""], ["Hennig", "Philipp", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1505.08098", "submitter": "Gianluigi Ciocca", "authors": "Simone Bianco, Gianluigi Ciocca, Claudio Cusano", "title": "CURL: Co-trained Unsupervised Representation Learning for Image\n  Classification", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a strategy for semi-supervised image classification\nthat leverages unsupervised representation learning and co-training. The\nstrategy, that is called CURL from Co-trained Unsupervised Representation\nLearning, iteratively builds two classifiers on two different views of the\ndata. The two views correspond to different representations learned from both\nlabeled and unlabeled data and differ in the fusion scheme used to combine the\nimage features. To assess the performance of our proposal, we conducted several\nexperiments on widely used data sets for scene and object recognition. We\nconsidered three scenarios (inductive, transductive and self-taught learning)\nthat differ in the strategy followed to exploit the unlabeled data. As image\nfeatures we considered a combination of GIST, PHOG, and LBP as well as features\nextracted from a Convolutional Neural Network. Moreover, two embodiments of\nCURL are investigated: one using Ensemble Projection as unsupervised\nrepresentation learning coupled with Logistic Regression, and one based on\nLapSVM. The results show that CURL clearly outperforms other supervised and\nsemi-supervised learning methods in the state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 15:57:40 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2015 12:21:20 GMT"}], "update_date": "2015-09-14", "authors_parsed": [["Bianco", "Simone", ""], ["Ciocca", "Gianluigi", ""], ["Cusano", "Claudio", ""]]}]