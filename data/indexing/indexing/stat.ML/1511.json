[{"id": "1511.00041", "submitter": "Murat Kocaoglu", "authors": "Karthikeyan Shanmugam, Murat Kocaoglu, Alexandros G. Dimakis, Sriram\n  Vishwanath", "title": "Learning Causal Graphs with Small Interventions", "comments": "Accepted to NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning causal networks with interventions, when\neach intervention is limited in size under Pearl's Structural Equation Model\nwith independent errors (SEM-IE). The objective is to minimize the number of\nexperiments to discover the causal directions of all the edges in a causal\ngraph. Previous work has focused on the use of separating systems for complete\ngraphs for this task. We prove that any deterministic adaptive algorithm needs\nto be a separating system in order to learn complete graphs in the worst case.\nIn addition, we present a novel separating system construction, whose size is\nclose to optimal and is arguably simpler than previous work in combinatorics.\nWe also develop a novel information theoretic lower bound on the number of\ninterventions that applies in full generality, including for randomized\nadaptive learning algorithms.\n  For general chordal graphs, we derive worst case lower bounds on the number\nof interventions. Building on observations about induced trees, we give a new\ndeterministic adaptive algorithm to learn directions on any chordal skeleton\ncompletely. In the worst case, our achievable scheme is an\n$\\alpha$-approximation algorithm where $\\alpha$ is the independence number of\nthe graph. We also show that there exist graph classes for which the sufficient\nnumber of experiments is close to the lower bound. In the other extreme, there\nare graph classes for which the required number of experiments is\nmultiplicatively $\\alpha$ away from our lower bound.\n  In simulations, our algorithm almost always performs very close to the lower\nbound, while the approach based on separating systems for complete graphs is\nsignificantly worse for random chordal graphs.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 22:24:13 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Shanmugam", "Karthikeyan", ""], ["Kocaoglu", "Murat", ""], ["Dimakis", "Alexandros G.", ""], ["Vishwanath", "Sriram", ""]]}, {"id": "1511.00054", "submitter": "David Moore", "authors": "David A. Moore and Stuart J. Russell", "title": "Gaussian Process Random Fields", "comments": "Advances in Neural Information Processing Systems (NIPS), 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes have been successful in both supervised and unsupervised\nmachine learning tasks, but their computational complexity has constrained\npractical applications. We introduce a new approximation for large-scale\nGaussian processes, the Gaussian Process Random Field (GPRF), in which local\nGPs are coupled via pairwise potentials. The GPRF likelihood is a simple,\ntractable, and parallelizeable approximation to the full GP marginal\nlikelihood, enabling latent variable modeling and hyperparameter selection on\nlarge datasets. We demonstrate its effectiveness on synthetic spatial data as\nwell as a real-world application to seismic event location.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2015 01:02:14 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Moore", "David A.", ""], ["Russell", "Stuart J.", ""]]}, {"id": "1511.00146", "submitter": "Mohammad Emtiyaz Khan", "authors": "Mohammad Emtiyaz Khan, Reza Babanezhad, Wu Lin, Mark Schmidt, Masashi\n  Sugiyama", "title": "Faster Stochastic Variational Inference using Proximal-Gradient Methods\n  with General Divergence Functions", "comments": "Published in UAI 2016. We have made the following change in this\n  revision: instead of expressing convergence rate results in terms of the\n  iterate difference, we state them in terms of the iterate distance divided by\n  the step-size (a measure of first-order optimality). We also removed some\n  claims about the performance with a fixed step size", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recent works have explored stochastic gradient methods for\nvariational inference that exploit the geometry of the variational-parameter\nspace. However, the theoretical properties of these methods are not\nwell-understood and these methods typically only apply to\nconditionally-conjugate models. We present a new stochastic method for\nvariational inference which exploits the geometry of the variational-parameter\nspace and also yields simple closed-form updates even for non-conjugate models.\nWe also give a convergence-rate analysis of our method and many other previous\nmethods which exploit the geometry of the space. Our analysis generalizes\nexisting convergence results for stochastic mirror-descent on non-convex\nobjectives by using a more general class of divergence functions. Beyond giving\na theoretical justification for a variety of recent methods, our experiments\nshow that new algorithms derived in this framework lead to state of the art\nresults on a variety of problems. Further, due to its generality, we expect\nthat our theoretical analysis could also apply to other applications.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2015 15:56:32 GMT"}, {"version": "v2", "created": "Sun, 12 Jun 2016 23:47:06 GMT"}, {"version": "v3", "created": "Fri, 12 Aug 2016 00:47:22 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Khan", "Mohammad Emtiyaz", ""], ["Babanezhad", "Reza", ""], ["Lin", "Wu", ""], ["Schmidt", "Mark", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1511.00152", "submitter": "Farhad Pourkamali-Anaraki", "authors": "Farhad Pourkamali-Anaraki and Stephen Becker", "title": "Preconditioned Data Sparsification for Big Data with Applications to PCA\n  and K-means", "comments": "28 pages, 10 figures", "journal-ref": null, "doi": "10.1109/TIT.2017.2672725", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze a compression scheme for large data sets that randomly keeps a\nsmall percentage of the components of each data sample. The benefit is that the\noutput is a sparse matrix and therefore subsequent processing, such as PCA or\nK-means, is significantly faster, especially in a distributed-data setting.\nFurthermore, the sampling is single-pass and applicable to streaming data. The\nsampling mechanism is a variant of previous methods proposed in the literature\ncombined with a randomized preconditioning to smooth the data. We provide\nguarantees for PCA in terms of the covariance matrix, and guarantees for\nK-means in terms of the error in the center estimators at a given step. We\npresent numerical evidence to show both that our bounds are nearly tight and\nthat our algorithms provide a real benefit when applied to standard test data\nsets, as well as providing certain benefits over related sampling approaches.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2015 17:20:00 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2015 22:40:13 GMT"}, {"version": "v3", "created": "Tue, 20 Sep 2016 00:35:14 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Pourkamali-Anaraki", "Farhad", ""], ["Becker", "Stephen", ""]]}, {"id": "1511.00158", "submitter": "Raymundo Navarrete", "authors": "Raymundo Navarrete and Divakar Viswanath", "title": "Prediction of Dynamical time Series Using Kernel Based Regression and\n  Smooth Splines", "comments": "minor changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction of dynamical time series with additive noise using support vector\nmachines or kernel based regression has been proved to be consistent for\ncertain classes of discrete dynamical systems. Consistency implies that these\nmethods are effective at computing the expected value of a point at a future\ntime given the present coordinates. However, the present coordinates themselves\nare noisy, and therefore, these methods are not necessarily effective at\nremoving noise. In this article, we consider denoising and prediction as\nseparate problems for flows, as opposed to discrete time dynamical systems, and\nshow that the use of smooth splines is more effective at removing noise.\nCombination of smooth splines and kernel based regression yields predictors\nthat are more accurate on benchmarks typically by a factor of 2 or more. We\nprove that kernel based regression in combination with smooth splines converges\nto the exact predictor for time series extracted from any compact invariant set\nof any sufficiently smooth flow. As a consequence of convergence, one can find\nexamples where the combination of kernel based regression with smooth splines\nis superior by even a factor of $100$. The predictors that we compute operate\non delay coordinate data and not the full state vector, which is typically not\nobservable.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2015 18:00:39 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 19:11:14 GMT"}, {"version": "v3", "created": "Wed, 20 Jun 2018 14:03:25 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Navarrete", "Raymundo", ""], ["Viswanath", "Divakar", ""]]}, {"id": "1511.00352", "submitter": "Abhinav Maurya", "authors": "Abhinav Maurya", "title": "Spatial Semantic Scan: Jointly Detecting Subtle Events and their Spatial\n  Footprint", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many methods have been proposed for detecting emerging events in text streams\nusing topic modeling. However, these methods have shortcomings that make them\nunsuitable for rapid detection of locally emerging events on massive text\nstreams. We describe Spatially Compact Semantic Scan (SCSS) that has been\ndeveloped specifically to overcome the shortcomings of current methods in\ndetecting new spatially compact events in text streams. SCSS employs\nalternating optimization between using semantic scan to estimate contrastive\nforeground topics in documents, and discovering spatial neighborhoods with high\noccurrence of documents containing the foreground topics. We evaluate our\nmethod on Emergency Department chief complaints dataset (ED dataset) to verify\nthe effectiveness of our method in detecting real-world disease outbreaks from\nfree-text ED chief complaint data.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 01:45:41 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2016 03:01:41 GMT"}, {"version": "v3", "created": "Sat, 28 May 2016 18:59:48 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Maurya", "Abhinav", ""]]}, {"id": "1511.00546", "submitter": "Lennart Gulikers", "authors": "Lennart Gulikers, Marc Lelarge, Laurent Massouli\\'e", "title": "An Impossibility Result for Reconstruction in a Degree-Corrected\n  Planted-Partition Model", "comments": "Appeared in Annals of Applied Probability", "journal-ref": "Annals of Applied Probability - Volume 28, Number 5 (2018),\n  3002-3027", "doi": null, "report-no": null, "categories": "math.PR cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Degree-Corrected Stochastic Block Model (DC-SBM): a random\ngraph on $n$ nodes, having i.i.d. weights $(\\phi_u)_{u=1}^n$ (possibly\nheavy-tailed), partitioned into $q \\geq 2$ asymptotically equal-sized clusters.\nThe model parameters are two constants $a,b > 0$ and the finite second moment\nof the weights $\\Phi^{(2)}$. Vertices $u$ and $v$ are connected by an edge with\nprobability $\\frac{\\phi_u \\phi_v}{n}a$ when they are in the same class and with\nprobability $\\frac{\\phi_u \\phi_v}{n}b$ otherwise.\n  We prove that it is information-theoretically impossible to estimate the\nclusters in a way positively correlated with the true community structure when\n$(a-b)^2 \\Phi^{(2)} \\leq q(a+b)$.\n  As by-products of our proof we obtain $(1)$ a precise coupling result for\nlocal neighbourhoods in DC-SBM's, that we use in a follow up paper [Gulikers et\nal., 2017] to establish a law of large numbers for local-functionals and $(2)$\nthat long-range interactions are weak in (power-law) DC-SBM's.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 15:30:40 GMT"}, {"version": "v2", "created": "Thu, 22 Sep 2016 16:33:29 GMT"}, {"version": "v3", "created": "Sat, 24 Nov 2018 21:46:20 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Gulikers", "Lennart", ""], ["Lelarge", "Marc", ""], ["Massouli\u00e9", "Laurent", ""]]}, {"id": "1511.00573", "submitter": "Tatsunori Hashimoto", "authors": "Tatsunori B. Hashimoto, Yi Sun, Tommi S. Jaakkola", "title": "From random walks to distances on unweighted graphs", "comments": "To appear in NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large unweighted directed graphs are commonly used to capture relations\nbetween entities. A fundamental problem in the analysis of such networks is to\nproperly define the similarity or dissimilarity between any two vertices.\nDespite the significance of this problem, statistical characterization of the\nproposed metrics has been limited. We introduce and develop a class of\ntechniques for analyzing random walks on graphs using stochastic calculus.\nUsing these techniques we generalize results on the degeneracy of hitting times\nand analyze a metric based on the Laplace transformed hitting time (LTHT). The\nmetric serves as a natural, provably well-behaved alternative to the expected\nhitting time. We establish a general correspondence between hitting times of\nthe Brownian motion and analogous hitting times on the graph. We show that the\nLTHT is consistent with respect to the underlying metric of a geometric graph,\npreserves clustering tendency, and remains robust against random addition of\nnon-geometric edges. Tests on simulated and real-world data show that the LTHT\nmatches theoretical predictions and outperforms alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 16:23:06 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Hashimoto", "Tatsunori B.", ""], ["Sun", "Yi", ""], ["Jaakkola", "Tommi S.", ""]]}, {"id": "1511.00830", "submitter": "Christos Louizos", "authors": "Christos Louizos, Kevin Swersky, Yujia Li, Max Welling and Richard\n  Zemel", "title": "The Variational Fair Autoencoder", "comments": "Fixed typo in eq. 3 and 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of learning representations that are invariant to\ncertain nuisance or sensitive factors of variation in the data while retaining\nas much of the remaining information as possible. Our model is based on a\nvariational autoencoding architecture with priors that encourage independence\nbetween sensitive and latent factors of variation. Any subsequent processing,\nsuch as classification, can then be performed on this purged latent\nrepresentation. To remove any remaining dependencies we incorporate an\nadditional penalty term based on the \"Maximum Mean Discrepancy\" (MMD) measure.\nWe discuss how these architectures can be efficiently trained on data and show\nin experiments that this method is more effective than previous work in\nremoving unwanted sources of variation while maintaining informative latent\nrepresentations.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 09:27:49 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2015 18:47:27 GMT"}, {"version": "v3", "created": "Thu, 12 Nov 2015 09:47:10 GMT"}, {"version": "v4", "created": "Tue, 5 Jan 2016 09:14:27 GMT"}, {"version": "v5", "created": "Thu, 4 Feb 2016 10:16:50 GMT"}, {"version": "v6", "created": "Thu, 10 Aug 2017 03:07:31 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Louizos", "Christos", ""], ["Swersky", "Kevin", ""], ["Li", "Yujia", ""], ["Welling", "Max", ""], ["Zemel", "Richard", ""]]}, {"id": "1511.00831", "submitter": "Yariv Aizenbud", "authors": "Yariv Aizenbud, Amit Bermanis, Amir Averbuch", "title": "PCA-Based Out-of-Sample Extension for Dimensionality Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality reduction methods are very common in the field of high\ndimensional data analysis. Typically, algorithms for dimensionality reduction\nare computationally expensive. Therefore, their applications for the analysis\nof massive amounts of data are impractical. For example, repeated computations\ndue to accumulated data are computationally prohibitive. In this paper, an\nout-of-sample extension scheme, which is used as a complementary method for\ndimensionality reduction, is presented. We describe an algorithm which performs\nan out-of-sample extension to newly-arrived data points. Unlike other extension\nalgorithms such as Nystr\\\"om algorithm, the proposed algorithm uses the\nintrinsic geometry of the data and properties for dimensionality reduction map.\nWe prove that the error of the proposed algorithm is bounded. Additionally to\nthe out-of-sample extension, the algorithm provides a degree of the abnormality\nof any newly-arrived data point.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 09:30:44 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Aizenbud", "Yariv", ""], ["Bermanis", "Amit", ""], ["Averbuch", "Amir", ""]]}, {"id": "1511.00871", "submitter": "Brijnesh Jain", "authors": "Brijnesh J. Jain", "title": "Properties of the Sample Mean in Graph Spaces and the\n  Majorize-Minimize-Mean Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most fundamental concepts in statistics is the concept of sample\nmean. Properties of the sample mean that are well-defined in Euclidean spaces\nbecome unwieldy or even unclear in graph spaces. Open problems related to the\nsample mean of graphs include: non-existence, non-uniqueness, statistical\ninconsistency, lack of convergence results of mean algorithms, non-existence of\nmidpoints, and disparity to midpoints. We present conditions to resolve all six\nproblems and propose a Majorize-Minimize-Mean (MMM) Algorithm. Experiments on\ngraph datasets representing images and molecules show that the MMM-Algorithm\nbest approximates a sample mean of graphs compared to six other mean\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 12:09:26 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Jain", "Brijnesh J.", ""]]}, {"id": "1511.01017", "submitter": "Ali Mousavi", "authors": "Ali Mousavi, Arian Maleki, Richard G. Baraniuk", "title": "Consistent Parameter Estimation for LASSO and Approximate Message\n  Passing", "comments": "arXiv admin note: text overlap with arXiv:1309.5979", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recovering a vector $\\beta_o \\in \\mathbb{R}^p$\nfrom $n$ random and noisy linear observations $y= X\\beta_o + w$, where $X$ is\nthe measurement matrix and $w$ is noise. The LASSO estimate is given by the\nsolution to the optimization problem $\\hat{\\beta}_{\\lambda} = \\arg \\min_{\\beta}\n\\frac{1}{2} \\|y-X\\beta\\|_2^2 + \\lambda \\| \\beta \\|_1$. Among the iterative\nalgorithms that have been proposed for solving this optimization problem,\napproximate message passing (AMP) has attracted attention for its fast\nconvergence. Despite significant progress in the theoretical analysis of the\nestimates of LASSO and AMP, little is known about their behavior as a function\nof the regularization parameter $\\lambda$, or the thereshold parameters\n$\\tau^t$. For instance the following basic questions have not yet been studied\nin the literature: (i) How does the size of the active set\n$\\|\\hat{\\beta}^\\lambda\\|_0/p$ behave as a function of $\\lambda$? (ii) How does\nthe mean square error $\\|\\hat{\\beta}_{\\lambda} - \\beta_o\\|_2^2/p$ behave as a\nfunction of $\\lambda$? (iii) How does $\\|\\beta^t - \\beta_o \\|_2^2/p$ behave as\na function of $\\tau^1, \\ldots, \\tau^{t-1}$? Answering these questions will help\nin addressing practical challenges regarding the optimal tuning of $\\lambda$ or\n$\\tau^1, \\tau^2, \\ldots$. This paper answers these questions in the asymptotic\nsetting and shows how these results can be employed in deriving simple and\ntheoretically optimal approaches for tuning the parameters $\\tau^1, \\ldots,\n\\tau^t$ for AMP or $\\lambda$ for LASSO. It also explores the connection between\nthe optimal tuning of the parameters of AMP and the optimal tuning of LASSO.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 18:05:21 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2015 16:20:58 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Mousavi", "Ali", ""], ["Maleki", "Arian", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1511.01032", "submitter": "Flavio Figueiredo", "authors": "Flavio Figueiredo, Bruno Ribeiro, Jussara Almeida, Christos Faloutsos", "title": "TribeFlow: Mining & Predicting User Trajectories", "comments": "To Appear at WWW 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.data-an physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Which song will Smith listen to next? Which restaurant will Alice go to\ntomorrow? Which product will John click next? These applications have in common\nthe prediction of user trajectories that are in a constant state of flux over a\nhidden network (e.g. website links, geographic location). What users are doing\nnow may be unrelated to what they will be doing in an hour from now. Mindful of\nthese challenges we propose TribeFlow, a method designed to cope with the\ncomplex challenges of learning personalized predictive models of\nnon-stationary, transient, and time-heterogeneous user trajectories. TribeFlow\nis a general method that can perform next product recommendation, next song\nrecommendation, next location prediction, and general arbitrary-length user\ntrajectory prediction without domain-specific knowledge. TribeFlow is more\naccurate and up to 413x faster than top competitors.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 18:57:39 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2016 15:31:02 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Figueiredo", "Flavio", ""], ["Ribeiro", "Bruno", ""], ["Almeida", "Jussara", ""], ["Faloutsos", "Christos", ""]]}, {"id": "1511.01169", "submitter": "Nitish Shirish Keskar", "authors": "Nitish Shirish Keskar and Albert S. Berahas", "title": "adaQN: An Adaptive Quasi-Newton Algorithm for Training RNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) are powerful models that achieve exceptional\nperformance on several pattern recognition problems. However, the training of\nRNNs is a computationally difficult task owing to the well-known\n\"vanishing/exploding\" gradient problem. Algorithms proposed for training RNNs\neither exploit no (or limited) curvature information and have cheap\nper-iteration complexity, or attempt to gain significant curvature information\nat the cost of increased per-iteration cost. The former set includes\ndiagonally-scaled first-order methods such as ADAGRAD and ADAM, while the\nlatter consists of second-order algorithms like Hessian-Free Newton and K-FAC.\nIn this paper, we present adaQN, a stochastic quasi-Newton algorithm for\ntraining RNNs. Our approach retains a low per-iteration cost while allowing for\nnon-diagonal scaling through a stochastic L-BFGS updating scheme. The method\nuses a novel L-BFGS scaling initialization scheme and is judicious in storing\nand retaining L-BFGS curvature pairs. We present numerical experiments on two\nlanguage modeling tasks and show that adaQN is competitive with popular RNN\ntraining algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 00:38:03 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2015 02:51:41 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2015 01:56:44 GMT"}, {"version": "v4", "created": "Thu, 7 Jan 2016 18:51:34 GMT"}, {"version": "v5", "created": "Tue, 23 Feb 2016 23:39:41 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Keskar", "Nitish Shirish", ""], ["Berahas", "Albert S.", ""]]}, {"id": "1511.01214", "submitter": "Giri Gopalan", "authors": "Giri Gopalan", "title": "Quantification of observed prior and likelihood information in\n  parametric Bayesian modeling", "comments": "Abbreviated and edited conference version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two data-dependent information metrics are developed to quantify the\ninformation of the prior and likelihood functions within a parametric Bayesian\nmodel, one of which is closely related to the reference priors from Berger,\nBernardo, and Sun, and information measure introduced by Lindley. A combination\nof theoretical, empirical, and computational support provides evidence that\nthese information-theoretic metrics may be useful diagnostic tools when\nperforming a Bayesian analysis.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 06:07:57 GMT"}, {"version": "v10", "created": "Fri, 24 Mar 2017 12:39:47 GMT"}, {"version": "v11", "created": "Mon, 27 Mar 2017 17:46:05 GMT"}, {"version": "v12", "created": "Sun, 18 Jun 2017 20:18:45 GMT"}, {"version": "v13", "created": "Thu, 7 Sep 2017 00:56:38 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2015 18:08:01 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2015 20:53:12 GMT"}, {"version": "v4", "created": "Sat, 12 Dec 2015 16:42:04 GMT"}, {"version": "v5", "created": "Fri, 18 Dec 2015 20:11:08 GMT"}, {"version": "v6", "created": "Thu, 7 Jan 2016 21:10:04 GMT"}, {"version": "v7", "created": "Mon, 7 Mar 2016 05:37:01 GMT"}, {"version": "v8", "created": "Sat, 25 Jun 2016 05:17:53 GMT"}, {"version": "v9", "created": "Mon, 12 Dec 2016 01:43:46 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Gopalan", "Giri", ""]]}, {"id": "1511.01280", "submitter": "Fabrice Rossi", "authors": "Arnaud De Myttenaere (SAMM, Viadeo), Boris Golden (Viadeo),\n  B\\'en\\'edicte Le Grand (CRI), Fabrice Rossi (SAMM)", "title": "Study of a bias in the offline evaluation of a recommendation algorithm", "comments": "arXiv admin note: substantial text overlap with arXiv:1407.0822", "journal-ref": "Petra Perner. 11th Industrial Conference on Data Mining, ICDM\n  2015, Jul 2015, Hamburg, Germany. Ibai Publishing, pp.57-70, 2015, Advances\n  in Data Mining", "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems have been integrated into the majority of large online\nsystems to filter and rank information according to user profiles. It thus\ninfluences the way users interact with the system and, as a consequence, bias\nthe evaluation of the performance of a recommendation algorithm computed using\nhistorical data (via offline evaluation). This paper describes this bias and\ndiscuss the relevance of a weighted offline evaluation to reduce this bias for\ndifferent classes of recommendation algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 10:46:58 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["De Myttenaere", "Arnaud", "", "SAMM, Viadeo"], ["Golden", "Boris", "", "Viadeo"], ["Grand", "B\u00e9n\u00e9dicte Le", "", "CRI"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1511.01281", "submitter": "Fabrice Rossi", "authors": "Mohamed Khalil El Mahrsi (LTCI, SAMM), Romain Guigour\\`es (SAMM),\n  Fabrice Rossi (SAMM), Marc Boull\\'e", "title": "Co-Clustering Network-Constrained Trajectory Data", "comments": null, "journal-ref": "Advances in Knowledge Discovery and Management, 615, Springer\n  International Publishing, pp.19-32, 2015, Studies in Computational\n  Intelligence, 978-3-319-23750-3", "doi": "10.1007/978-3-319-23751-0_2", "report-no": null, "categories": "stat.ML cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, clustering moving object trajectories kept gaining interest from\nboth the data mining and machine learning communities. This problem, however,\nwas studied mainly and extensively in the setting where moving objects can move\nfreely on the euclidean space. In this paper, we study the problem of\nclustering trajectories of vehicles whose movement is restricted by the\nunderlying road network. We model relations between these trajectories and road\nsegments as a bipartite graph and we try to cluster its vertices. We\ndemonstrate our approaches on synthetic data and show how it could be useful in\ninferring knowledge about the flow dynamics and the behavior of the drivers\nusing the road network.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 10:47:29 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Mahrsi", "Mohamed Khalil El", "", "LTCI, SAMM"], ["Guigour\u00e8s", "Romain", "", "SAMM"], ["Rossi", "Fabrice", "", "SAMM"], ["Boull\u00e9", "Marc", ""]]}, {"id": "1511.01284", "submitter": "Fabrice Rossi", "authors": "Bienvenue Kouway\\`e (SAMM), No\\\"el Fonton, Fabrice Rossi (SAMM)", "title": "Lasso based feature selection for malaria risk exposure prediction", "comments": "in Petra Perner. Machine Learning and Data Mining in Pattern\n  Recognition, Jul 2015, Hamburg, Germany. Ibai publishing, 2015, Machine\n  Learning and Data Mining in Pattern Recognition (proceedings of 11th\n  International Conference, MLDM 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In life sciences, the experts generally use empirical knowledge to recode\nvariables, choose interactions and perform selection by classical approach. The\naim of this work is to perform automatic learning algorithm for variables\nselection which can lead to know if experts can be help in they decision or\nsimply replaced by the machine and improve they knowledge and results. The\nLasso method can detect the optimal subset of variables for estimation and\nprediction under some conditions. In this paper, we propose a novel approach\nwhich uses automatically all variables available and all interactions. By a\ndouble cross-validation combine with Lasso, we select a best subset of\nvariables and with GLM through a simple cross-validation perform predictions.\nThe algorithm assures the stability and the the consistency of estimators.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 10:53:41 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Kouway\u00e8", "Bienvenue", "", "SAMM"], ["Fonton", "No\u00ebl", "", "SAMM"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1511.01289", "submitter": "Saiprasad Ravishankar", "authors": "Saiprasad Ravishankar and Yoram Bresler", "title": "Data-Driven Learning of a Union of Sparsifying Transforms Model for\n  Blind Compressed Sensing", "comments": "Appears in IEEE Transactions on Computational Imaging, 2016", "journal-ref": null, "doi": "10.1109/TCI.2016.2567299", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing is a powerful tool in applications such as magnetic\nresonance imaging (MRI). It enables accurate recovery of images from highly\nundersampled measurements by exploiting the sparsity of the images or image\npatches in a transform domain or dictionary. In this work, we focus on blind\ncompressed sensing (BCS), where the underlying sparse signal model is a priori\nunknown, and propose a framework to simultaneously reconstruct the underlying\nimage as well as the unknown model from highly undersampled measurements.\nSpecifically, our model is that the patches of the underlying image(s) are\napproximately sparse in a transform domain. We also extend this model to a\nunion of transforms model that better captures the diversity of features in\nnatural images. The proposed block coordinate descent type algorithms for blind\ncompressed sensing are highly efficient, and are guaranteed to converge to at\nleast the partial global and partial local minimizers of the highly non-convex\nBCS problems. Our numerical experiments show that the proposed framework\nusually leads to better quality of image reconstructions in MRI compared to\nseveral recent image reconstruction methods. Importantly, the learning of a\nunion of sparsifying transforms leads to better image reconstructions than a\nsingle adaptive transform.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 11:02:45 GMT"}, {"version": "v2", "created": "Sat, 1 Oct 2016 04:15:01 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Ravishankar", "Saiprasad", ""], ["Bresler", "Yoram", ""]]}, {"id": "1511.01304", "submitter": "Vladimir Temlyakov", "authors": "Vladimir Temlyakov", "title": "Dictionary descent in optimization", "comments": "arXiv admin note: text overlap with arXiv:1206.0392", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of convex optimization is studied. Usually in convex optimization\nthe minimization is over a d-dimensional domain. Very often the convergence\nrate of an optimization algorithm depends on the dimension d. The algorithms\nstudied in this paper utilize dictionaries instead of a canonical basis used in\nthe coordinate descent algorithms. We show how this approach allows us to\nreduce dimensionality of the problem. Also, we investigate which properties of\na dictionary are beneficial for the convergence rate of typical greedy-type\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 12:34:10 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Temlyakov", "Vladimir", ""]]}, {"id": "1511.01419", "submitter": "Ofer Meshi", "authors": "Ofer Meshi, Mehrdad Mahdavi, Adrian Weller and David Sontag", "title": "Train and Test Tightness of LP Relaxations in Structured Prediction", "comments": "To appear in ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured prediction is used in areas such as computer vision and natural\nlanguage processing to predict structured outputs such as segmentations or\nparse trees. In these settings, prediction is performed by MAP inference or,\nequivalently, by solving an integer linear program. Because of the complex\nscoring functions required to obtain accurate predictions, both learning and\ninference typically require the use of approximate solvers. We propose a\ntheoretical explanation to the striking observation that approximations based\non linear programming (LP) relaxations are often tight on real-world instances.\nIn particular, we show that learning with LP relaxed inference encourages\nintegrality of training instances, and that tightness generalizes from train to\ntest data.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 18:13:35 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2015 12:04:24 GMT"}, {"version": "v3", "created": "Wed, 27 Apr 2016 02:58:33 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Meshi", "Ofer", ""], ["Mahdavi", "Mehrdad", ""], ["Weller", "Adrian", ""], ["Sontag", "David", ""]]}, {"id": "1511.01443", "submitter": "Cheng Huang", "authors": "Cheng Huang and Xiaoming Huo", "title": "A Distributed One-Step Estimator", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed statistical inference has recently attracted enormous attention.\nMany existing work focuses on the averaging estimator. We propose a one-step\napproach to enhance a simple-averaging based distributed estimator. We derive\nthe corresponding asymptotic properties of the newly proposed estimator. We\nfind that the proposed one-step estimator enjoys the same asymptotic properties\nas the centralized estimator. The proposed one-step approach merely requires\none additional round of communication in relative to the averaging estimator;\nso the extra communication burden is insignificant. In finite sample cases,\nnumerical examples show that the proposed estimator outperforms the simple\naveraging estimator with a large margin in terms of the mean squared errors. A\npotential application of the one-step approach is that one can use multiple\nmachines to speed up large scale statistical inference with little compromise\nin the quality of estimators. The proposed method becomes more valuable when\ndata can only be available at distributed machines with limited communication\nbandwidth.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 19:18:41 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2015 20:04:10 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Huang", "Cheng", ""], ["Huo", "Xiaoming", ""]]}, {"id": "1511.01473", "submitter": "Alexander Wein", "authors": "Ankur Moitra and William Perry and Alexander S. Wein", "title": "How Robust are Reconstruction Thresholds for Community Detection?", "comments": "36 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic block model is one of the oldest and most ubiquitous models\nfor studying clustering and community detection. In an exciting sequence of\ndevelopments, motivated by deep but non-rigorous ideas from statistical\nphysics, Decelle et al. conjectured a sharp threshold for when community\ndetection is possible in the sparse regime. Mossel, Neeman and Sly and\nMassoulie proved the conjecture and gave matching algorithms and lower bounds.\n  Here we revisit the stochastic block model from the perspective of semirandom\nmodels where we allow an adversary to make `helpful' changes that strengthen\nties within each community and break ties between them. We show a surprising\nresult that these `helpful' changes can shift the information-theoretic\nthreshold, making the community detection problem strictly harder. We\ncomplement this by showing that an algorithm based on semidefinite programming\n(which was known to get close to the threshold) continues to work in the\nsemirandom model (even for partial recovery). This suggests that algorithms\nbased on semidefinite programming are robust in ways that any algorithm meeting\nthe information-theoretic threshold cannot be.\n  These results point to an interesting new direction: Can we find robust,\nsemirandom analogues to some of the classical, average-case thresholds in\nstatistics? We also explore this question in the broadcast tree model, and we\nshow that the viewpoint of semirandom models can help explain why some\nalgorithms are preferred to others in practice, in spite of the gaps in their\nstatistical performance on random models.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 20:50:21 GMT"}, {"version": "v2", "created": "Mon, 21 Mar 2016 21:28:21 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Moitra", "Ankur", ""], ["Perry", "William", ""], ["Wein", "Alexander S.", ""]]}, {"id": "1511.01543", "submitter": "Alessandro Chiuso", "authors": "A. Chiuso", "title": "Regularization and Bayesian Learning in Dynamical Systems: Past, Present\n  and Future", "comments": "Plenary Presentation at the IFAC SYSID 2015. Submitted to Annual\n  Reviews in Control", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization and Bayesian methods for system identification have been\nrepopularized in the recent years, and proved to be competitive w.r.t.\nclassical parametric approaches. In this paper we shall make an attempt to\nillustrate how the use of regularization in system identification has evolved\nover the years, starting from the early contributions both in the Automatic\nControl as well as Econometrics and Statistics literature. In particular we\nshall discuss some fundamental issues such as compound estimation problems and\nexchangeability which play and important role in regularization and Bayesian\napproaches, as also illustrated in early publications in Statistics. The\nhistorical and foundational issues will be given more emphasis (and space), at\nthe expense of the more recent developments which are only briefly discussed.\nThe main reason for such a choice is that, while the recent literature is\nreadily available, and surveys have already been published on the subject, in\nthe author's opinion a clear link with past work had not been completely\nclarified.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 22:50:41 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Chiuso", "A.", ""]]}, {"id": "1511.01644", "submitter": "Benjamin Letham", "authors": "Benjamin Letham, Cynthia Rudin, Tyler H. McCormick, David Madigan", "title": "Interpretable classifiers using rules and Bayesian analysis: Building a\n  better stroke prediction model", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS848 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 3, 1350-1371", "doi": "10.1214/15-AOAS848", "report-no": "IMS-AOAS-AOAS848", "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to produce predictive models that are not only accurate, but are also\ninterpretable to human experts. Our models are decision lists, which consist of\na series of if...then... statements (e.g., if high blood pressure, then stroke)\nthat discretize a high-dimensional, multivariate feature space into a series of\nsimple, readily interpretable decision statements. We introduce a generative\nmodel called Bayesian Rule Lists that yields a posterior distribution over\npossible decision lists. It employs a novel prior structure to encourage\nsparsity. Our experiments show that Bayesian Rule Lists has predictive accuracy\non par with the current top algorithms for prediction in machine learning. Our\nmethod is motivated by recent developments in personalized medicine, and can be\nused to produce highly accurate and interpretable medical scoring systems. We\ndemonstrate this by producing an alternative to the CHADS$_2$ score, actively\nused in clinical practice for estimating the risk of stroke in patients that\nhave atrial fibrillation. Our model is as interpretable as CHADS$_2$, but more\naccurate.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 08:01:05 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Letham", "Benjamin", ""], ["Rudin", "Cynthia", ""], ["McCormick", "Tyler H.", ""], ["Madigan", "David", ""]]}, {"id": "1511.01707", "submitter": "Johan Dahlin PhD", "authors": "Johan Dahlin and Thomas B. Sch\\\"on", "title": "Getting Started with Particle Metropolis-Hastings for Inference in\n  Nonlinear Dynamical Models", "comments": "41 pages, 7 figures. In press for Journal of Statistical Software.\n  Source code for R, Python and MATLAB available at:\n  https://github.com/compops/pmh-tutorial", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This tutorial provides a gentle introduction to the particle\nMetropolis-Hastings (PMH) algorithm for parameter inference in nonlinear\nstate-space models together with a software implementation in the statistical\nprogramming language R. We employ a step-by-step approach to develop an\nimplementation of the PMH algorithm (and the particle filter within) together\nwith the reader. This final implementation is also available as the package\npmhtutorial in the CRAN repository. Throughout the tutorial, we provide some\nintuition as to how the algorithm operates and discuss some solutions to\nproblems that might occur in practice. To illustrate the use of PMH, we\nconsider parameter inference in a linear Gaussian state-space model with\nsynthetic data and a nonlinear stochastic volatility model with real-world\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 11:59:36 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2015 19:58:56 GMT"}, {"version": "v3", "created": "Fri, 11 Dec 2015 07:40:04 GMT"}, {"version": "v4", "created": "Thu, 31 Mar 2016 06:53:18 GMT"}, {"version": "v5", "created": "Thu, 6 Jul 2017 11:49:01 GMT"}, {"version": "v6", "created": "Sun, 27 Aug 2017 23:14:39 GMT"}, {"version": "v7", "created": "Fri, 20 Oct 2017 00:24:07 GMT"}, {"version": "v8", "created": "Tue, 12 Mar 2019 13:09:43 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Dahlin", "Johan", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1511.01776", "submitter": "Meisam Razaviyayn", "authors": "Meisam Razaviyayn, Hung-Wei Tseng, Zhi-Quan Luo", "title": "Computational Intractability of Dictionary Learning for Sparse\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the dictionary learning problem for sparse\nrepresentation. We first show that this problem is NP-hard by polynomial time\nreduction of the densest cut problem. Then, using successive convex\napproximation strategies, we propose efficient dictionary learning schemes to\nsolve several practical formulations of this problem to stationary points.\nUnlike many existing algorithms in the literature, such as K-SVD, our proposed\ndictionary learning scheme is theoretically guaranteed to converge to the set\nof stationary points under certain mild assumptions. For the image denoising\napplication, the performance and the efficiency of the proposed dictionary\nlearning scheme are comparable to that of K-SVD algorithm in simulation.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 15:19:42 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Razaviyayn", "Meisam", ""], ["Tseng", "Hung-Wei", ""], ["Luo", "Zhi-Quan", ""]]}, {"id": "1511.01844", "submitter": "Lucas Theis", "authors": "Lucas Theis, A\\\"aron van den Oord, Matthias Bethge", "title": "A note on the evaluation of generative models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic generative models can be used for compression, denoising,\ninpainting, texture synthesis, semi-supervised learning, unsupervised feature\nlearning, and other tasks. Given this wide range of applications, it is not\nsurprising that a lot of heterogeneity exists in the way these models are\nformulated, trained, and evaluated. As a consequence, direct comparison between\nmodels is often difficult. This article reviews mostly known but often\nunderappreciated properties relating to the evaluation and interpretation of\ngenerative models with a focus on image models. In particular, we show that\nthree of the currently most commonly used criteria---average log-likelihood,\nParzen window estimates, and visual fidelity of samples---are largely\nindependent of each other when the data is high-dimensional. Good performance\nwith respect to one criterion therefore need not imply good performance with\nrespect to the other criteria. Our results show that extrapolation from one\ncriterion to another is not warranted and generative models need to be\nevaluated directly with respect to the application(s) they were intended for.\nIn addition, we provide examples demonstrating that Parzen window estimates\nshould generally be avoided.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 18:22:44 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2016 22:06:30 GMT"}, {"version": "v3", "created": "Sun, 24 Apr 2016 20:03:35 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Theis", "Lucas", ""], ["Oord", "A\u00e4ron van den", ""], ["Bethge", "Matthias", ""]]}, {"id": "1511.01846", "submitter": "Vladimir Temlyakov", "authors": "Vladimir Temlyakov", "title": "Sparse approximation by greedy algorithms", "comments": "arXiv admin note: substantial text overlap with arXiv:1303.6811,\n  arXiv:1303.3595", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a survey on recent results in constructive sparse approximation. Three\ndirections are discussed here: (1) Lebesgue-type inequalities for greedy\nalgorithms with respect to a special class of dictionaries, (2) constructive\nsparse approximation with respect to the trigonometric system, (3) sparse\napproximation with respect to dictionaries with tensor product structure. In\nall three cases constructive ways are provided for sparse approximation. The\ntechnique used is based on fundamental results from the theory of greedy\napproximation. In particular, results in the direction (1) are based on deep\nmethods developed recently in compressed sensing. We present some of these\nresults with detailed proofs.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 18:29:05 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Temlyakov", "Vladimir", ""]]}, {"id": "1511.01853", "submitter": "Pan Li", "authors": "Pan Li, Baosen Zhang, Yang Weng, Ram Rajagopal", "title": "A Sparse Linear Model and Significance Test for Individual Consumption\n  Prediction", "comments": "36 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate prediction of user consumption is a key part not only in\nunderstanding consumer flexibility and behavior patterns, but in the design of\nrobust and efficient energy saving programs as well. Existing prediction\nmethods usually have high relative errors that can be larger than 30% and have\ndifficulties accounting for heterogeneity between individual users. In this\npaper, we propose a method to improve prediction accuracy of individual users\nby adaptively exploring sparsity in historical data and leveraging predictive\nrelationship between different users. Sparsity is captured by popular least\nabsolute shrinkage and selection estimator, while user selection is formulated\nas an optimal hypothesis testing problem and solved via a covariance test.\nUsing real world data from PG&E, we provide extensive simulation validation of\nthe proposed method against well-known techniques such as support vector\nmachine, principle component analysis combined with linear regression, and\nrandom forest. The results demonstrate that our proposed methods are\noperationally efficient because of linear nature, and achieve optimal\nprediction performance.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 19:09:25 GMT"}, {"version": "v2", "created": "Sat, 25 Jun 2016 05:33:08 GMT"}, {"version": "v3", "created": "Tue, 21 Feb 2017 08:12:51 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Li", "Pan", ""], ["Zhang", "Baosen", ""], ["Weng", "Yang", ""], ["Rajagopal", "Ram", ""]]}, {"id": "1511.01865", "submitter": "Seyed Mostafa Kia", "authors": "Nastaran Mohammadian Rad, Andrea Bizzego, Seyed Mostafa Kia, Giuseppe\n  Jurman, Paola Venuti, Cesare Furlanello", "title": "Convolutional Neural Network for Stereotypical Motor Movement Detection\n  in Autism", "comments": "Presented at 5th NIPS Workshop on Machine Learning and Interpretation\n  in Neuroimaging (MLINI), 2015, (http://arxiv.org/html/1605.04435), Report-no:\n  MLINI/2015/13", "journal-ref": null, "doi": null, "report-no": "MLINI/2015/13", "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autism Spectrum Disorders (ASDs) are often associated with specific atypical\npostural or motor behaviors, of which Stereotypical Motor Movements (SMMs) have\na specific visibility. While the identification and the quantification of SMM\npatterns remain complex, its automation would provide support to accurate\ntuning of the intervention in the therapy of autism. Therefore, it is essential\nto develop automatic SMM detection systems in a real world setting, taking care\nof strong inter-subject and intra-subject variability. Wireless accelerometer\nsensing technology can provide a valid infrastructure for real-time SMM\ndetection, however such variability remains a problem also for machine learning\nmethods, in particular whenever handcrafted features extracted from\naccelerometer signal are considered. Here, we propose to employ the deep\nlearning paradigm in order to learn discriminating features from multi-sensor\naccelerometer signals. Our results provide preliminary evidence that feature\nlearning and transfer learning embedded in the deep architecture achieve higher\naccurate SMM detectors in longitudinal scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 19:36:33 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2016 21:02:02 GMT"}, {"version": "v3", "created": "Tue, 7 Jun 2016 19:11:34 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Rad", "Nastaran Mohammadian", ""], ["Bizzego", "Andrea", ""], ["Kia", "Seyed Mostafa", ""], ["Jurman", "Giuseppe", ""], ["Venuti", "Paola", ""], ["Furlanello", "Cesare", ""]]}, {"id": "1511.01870", "submitter": "Andrew Wilson", "authors": "Andrew Gordon Wilson, Christoph Dann, Hannes Nickisch", "title": "Thoughts on Massively Scalable Gaussian Processes", "comments": "25 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework and early results for massively scalable Gaussian\nprocesses (MSGP), significantly extending the KISS-GP approach of Wilson and\nNickisch (2015). The MSGP framework enables the use of Gaussian processes (GPs)\non billions of datapoints, without requiring distributed inference, or severe\nassumptions. In particular, MSGP reduces the standard $O(n^3)$ complexity of GP\nlearning and inference to $O(n)$, and the standard $O(n^2)$ complexity per test\npoint prediction to $O(1)$. MSGP involves 1) decomposing covariance matrices as\nKronecker products of Toeplitz matrices approximated by circulant matrices.\nThis multi-level circulant approximation allows one to unify the orthogonal\ncomputational benefits of fast Kronecker and Toeplitz approaches, and is\nsignificantly faster than either approach in isolation; 2) local kernel\ninterpolation and inducing points to allow for arbitrarily located data inputs,\nand $O(1)$ test time predictions; 3) exploiting block-Toeplitz Toeplitz-block\nstructure (BTTB), which enables fast inference and learning when\nmultidimensional Kronecker structure is not present; and 4) projections of the\ninput space to flexibly model correlated inputs and high dimensional data. The\nability to handle many ($m \\approx n$) inducing points allows for near-exact\naccuracy and large scale kernel learning.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 19:51:31 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Wilson", "Andrew Gordon", ""], ["Dann", "Christoph", ""], ["Nickisch", "Hannes", ""]]}, {"id": "1511.01942", "submitter": "Mark Schmidt", "authors": "Reza Babanezhad, Mohamed Osama Ahmed, Alim Virani, Mark Schmidt, Jakub\n  Kone\\v{c}n\\'y, Scott Sallinen", "title": "Stop Wasting My Gradients: Practical SVRG", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and analyze several strategies for improving the performance of\nstochastic variance-reduced gradient (SVRG) methods. We first show that the\nconvergence rate of these methods can be preserved under a decreasing sequence\nof errors in the control variate, and use this to derive variants of SVRG that\nuse growing-batch strategies to reduce the number of gradient calculations\nrequired in the early iterations. We further (i) show how to exploit support\nvectors to reduce the number of gradient computations in the later iterations,\n(ii) prove that the commonly-used regularized SVRG iteration is justified and\nimproves the convergence rate, (iii) consider alternate mini-batch selection\nstrategies, and (iv) consider the generalization error of the method.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 22:45:12 GMT"}], "update_date": "2016-08-06", "authors_parsed": [["Babanezhad", "Reza", ""], ["Ahmed", "Mohamed Osama", ""], ["Virani", "Alim", ""], ["Schmidt", "Mark", ""], ["Kone\u010dn\u00fd", "Jakub", ""], ["Sallinen", "Scott", ""]]}, {"id": "1511.01957", "submitter": "Weijie Su", "authors": "Weijie Su, Malgorzata Bogdan, Emmanuel Candes", "title": "False Discoveries Occur Early on the Lasso Path", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In regression settings where explanatory variables have very low correlations\nand there are relatively few effects, each of large magnitude, we expect the\nLasso to find the important variables with few errors, if any. This paper shows\nthat in a regime of linear sparsity---meaning that the fraction of variables\nwith a non-vanishing effect tends to a constant, however small---this cannot\nreally be the case, even when the design variables are stochastically\nindependent. We demonstrate that true features and null features are always\ninterspersed on the Lasso path, and that this phenomenon occurs no matter how\nstrong the effect sizes are. We derive a sharp asymptotic trade-off between\nfalse and true positive rates or, equivalently, between measures of type I and\ntype II errors along the Lasso path. This trade-off states that if we ever want\nto achieve a type II error (false negative rate) under a critical value, then\nanywhere on the Lasso path the type I error (false positive rate) will need to\nexceed a given threshold so that we can never have both errors at a low level\nat the same time. Our analysis uses tools from approximate message passing\n(AMP) theory as well as novel elements to deal with a possibly adaptive\nselection of the Lasso regularizing parameter.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 23:51:51 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2015 04:06:10 GMT"}, {"version": "v3", "created": "Sun, 29 Nov 2015 07:08:38 GMT"}, {"version": "v4", "created": "Thu, 15 Sep 2016 02:45:47 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Su", "Weijie", ""], ["Bogdan", "Malgorzata", ""], ["Candes", "Emmanuel", ""]]}, {"id": "1511.01987", "submitter": "Kazuto Fukuchi", "authors": "Kazuto Fukuchi and Jun Sakuma", "title": "Neutralized Empirical Risk Minimization with Generalization Neutrality\n  Bound", "comments": "27 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, machine learning plays an important role in the lives and\nindividual activities of numerous people. Accordingly, it has become necessary\nto design machine learning algorithms to ensure that discrimination, biased\nviews, or unfair treatment do not result from decision making or predictions\nmade via machine learning. In this work, we introduce a novel empirical risk\nminimization (ERM) framework for supervised learning, neutralized ERM (NERM)\nthat ensures that any classifiers obtained can be guaranteed to be neutral with\nrespect to a viewpoint hypothesis. More specifically, given a viewpoint\nhypothesis, NERM works to find a target hypothesis that minimizes the empirical\nrisk while simultaneously identifying a target hypothesis that is neutral to\nthe viewpoint hypothesis. Within the NERM framework, we derive a theoretical\nbound on empirical and generalization neutrality risks. Furthermore, as a\nrealization of NERM with linear classification, we derive a max-margin\nalgorithm, neutral support vector machine (SVM). Experimental results show that\nour neutral SVM shows improved classification performance in real datasets\nwithout sacrificing the neutrality guarantee.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 05:11:21 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Fukuchi", "Kazuto", ""], ["Sakuma", "Jun", ""]]}, {"id": "1511.02025", "submitter": "Patrick Miller", "authors": "Patrick J. Miller, Gitta H. Lubke, Daniel B. McArtor, C. S. Bergeman", "title": "Finding structure in data using multivariate tree boosting", "comments": null, "journal-ref": null, "doi": "10.1037/met0000087", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technology and collaboration enable dramatic increases in the size of\npsychological and psychiatric data collections, but finding structure in these\nlarge data sets with many collected variables is challenging. Decision tree\nensembles like random forests (Strobl, Malley, and Tutz, 2009) are a useful\ntool for finding structure, but are difficult to interpret with multiple\noutcome variables which are often of interest in psychology. To find and\ninterpret structure in data sets with multiple outcomes and many predictors\n(possibly exceeding the sample size), we introduce a multivariate extension to\na decision tree ensemble method called Gradient Boosted Regression Trees\n(Friedman, 2001). Our method, multivariate tree boosting, can be used for\nidentifying important predictors, detecting predictors with non-linear effects\nand interactions without specification of such effects, and for identifying\npredictors that cause two or more outcome variables to covary without\nparametric assumptions. We provide the R package 'mvtboost' to estimate, tune,\nand interpret the resulting model, which extends the implementation of\nunivariate boosting in the R package 'gbm' (Ridgeway, 2013) to continuous,\nmultivariate outcomes. To illustrate the approach, we analyze predictors of\npsychological well-being (Ryff and Keyes, 1995). Simulations verify that our\napproach identifies predictors with non-linear effects and achieves high\nprediction accuracy, exceeding or matching the performance of (penalized)\nmultivariate multiple regression and multivariate decision trees over a wide\nrange of conditions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 10:30:22 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2016 15:04:42 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Miller", "Patrick J.", ""], ["Lubke", "Gitta H.", ""], ["McArtor", "Daniel B.", ""], ["Bergeman", "C. S.", ""]]}, {"id": "1511.02086", "submitter": "Gal Mishne", "authors": "Gal Mishne, Ronen Talmon, Ron Meir, Jackie Schiller, Uri Dubin and\n  Ronald R. Coifman", "title": "Hierarchical Coupled Geometry Analysis for Neuronal Structure and\n  Activity Pattern Discovery", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": "10.1109/JSTSP.2016.2602061", "report-no": null, "categories": "q-bio.QM q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the wake of recent advances in experimental methods in neuroscience, the\nability to record in-vivo neuronal activity from awake animals has become\nfeasible. The availability of such rich and detailed physiological measurements\ncalls for the development of advanced data analysis tools, as commonly used\ntechniques do not suffice to capture the spatio-temporal network complexity. In\nthis paper, we propose a new hierarchical coupled geometry analysis, which\nexploits the hidden connectivity structures between neurons and the dynamic\npatterns at multiple time-scales. Our approach gives rise to the joint\norganization of neurons and dynamic patterns in data-driven hierarchical data\nstructures. These structures provide local to global data representations, from\nlocal partitioning of the data in flexible trees through a new multiscale\nmetric to a global manifold embedding. The application of our techniques to\nin-vivo neuronal recordings demonstrate the capability of extracting neuronal\nactivity patterns and identifying temporal trends, associated with particular\nbehavioral events and manipulations introduced in the experiments.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 14:14:29 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Mishne", "Gal", ""], ["Talmon", "Ronen", ""], ["Meir", "Ron", ""], ["Schiller", "Jackie", ""], ["Dubin", "Uri", ""], ["Coifman", "Ronald R.", ""]]}, {"id": "1511.02124", "submitter": "Rahul Gopal Krishnan", "authors": "Rahul G. Krishnan, Simon Lacoste-Julien, David Sontag", "title": "Barrier Frank-Wolfe for Marginal Inference", "comments": "25 pages, 12 figures, To appear in Neural Information Processing\n  Systems (NIPS) 2015, Corrected reference and cleaned up bibliography", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a globally-convergent algorithm for optimizing the\ntree-reweighted (TRW) variational objective over the marginal polytope. The\nalgorithm is based on the conditional gradient method (Frank-Wolfe) and moves\npseudomarginals within the marginal polytope through repeated maximum a\nposteriori (MAP) calls. This modular structure enables us to leverage black-box\nMAP solvers (both exact and approximate) for variational inference, and obtains\nmore accurate results than tree-reweighted algorithms that optimize over the\nlocal consistency relaxation. Theoretically, we bound the sub-optimality for\nthe proposed algorithm despite the TRW objective having unbounded gradients at\nthe boundary of the marginal polytope. Empirically, we demonstrate the\nincreased quality of results found by tightening the relaxation over the\nmarginal polytope as well as the spanning tree polytope on synthetic and\nreal-world instances.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 15:48:53 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2015 18:57:33 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Krishnan", "Rahul G.", ""], ["Lacoste-Julien", "Simon", ""], ["Sontag", "David", ""]]}, {"id": "1511.02176", "submitter": "Francesco Orabona", "authors": "Francesco Orabona and David Pal", "title": "Optimal Non-Asymptotic Lower Bound on the Minimax Regret of Learning\n  with Expert Advice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove non-asymptotic lower bounds on the expectation of the maximum of $d$\nindependent Gaussian variables and the expectation of the maximum of $d$\nindependent symmetric random walks. Both lower bounds recover the optimal\nleading constant in the limit. A simple application of the lower bound for\nrandom walks is an (asymptotically optimal) non-asymptotic lower bound on the\nminimax regret of online learning with expert advice.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 18:01:38 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Orabona", "Francesco", ""], ["Pal", "David", ""]]}, {"id": "1511.02187", "submitter": "Ricardo Pio Monti", "authors": "Ricardo Pio Monti, Romy Lorenz, Robert Leech, Christoforos\n  Anagnostopoulos, Giovanni Montana", "title": "Streaming regularization parameter selection via stochastic gradient\n  descent", "comments": "Paper withdrawn as it is no longer up to date", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework to perform streaming covariance selection. Our\napproach employs regularization constraints where a time-varying sparsity\nparameter is iteratively estimated via stochastic gradient descent. This allows\nfor the regularization parameter to be efficiently learnt in an online manner.\nThe proposed framework is developed for linear regression models and extended\nto graphical models via neighbourhood selection. Under mild assumptions, we are\nable to obtain convergence results in a non-stochastic setting. The\ncapabilities of such an approach are demonstrated using both synthetic data as\nwell as neuroimaging data.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 18:38:17 GMT"}, {"version": "v2", "created": "Wed, 2 Mar 2016 12:19:11 GMT"}, {"version": "v3", "created": "Wed, 2 Nov 2016 12:59:05 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Monti", "Ricardo Pio", ""], ["Lorenz", "Romy", ""], ["Leech", "Robert", ""], ["Anagnostopoulos", "Christoforos", ""], ["Montana", "Giovanni", ""]]}, {"id": "1511.02199", "submitter": "Mingyuan Zhou", "authors": "Mingyuan Zhou, Yulai Cong, Bo Chen", "title": "The Poisson Gamma Belief Network", "comments": "Neural Information Processing Systems (NIPS2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To infer a multilayer representation of high-dimensional count vectors, we\npropose the Poisson gamma belief network (PGBN) that factorizes each of its\nlayers into the product of a connection weight matrix and the nonnegative real\nhidden units of the next layer. The PGBN's hidden layers are jointly trained\nwith an upward-downward Gibbs sampler, each iteration of which upward samples\nDirichlet distributed connection weight vectors starting from the first layer\n(bottom data layer), and then downward samples gamma distributed hidden units\nstarting from the top hidden layer. The gamma-negative binomial process\ncombined with a layer-wise training strategy allows the PGBN to infer the width\nof each layer given a fixed budget on the width of the first layer. The PGBN\nwith a single hidden layer reduces to Poisson factor analysis. Example results\non text analysis illustrate interesting relationships between the width of the\nfirst layer and the inferred network structure, and demonstrate that the PGBN,\nwhose hidden units are imposed with correlated gamma priors, can add more\nlayers to increase its performance gains over Poisson factor analysis, given\nthe same limit on the width of the first layer.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 19:16:50 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2015 15:39:50 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Zhou", "Mingyuan", ""], ["Cong", "Yulai", ""], ["Chen", "Bo", ""]]}, {"id": "1511.02204", "submitter": "Rahul Mazumder", "authors": "Robert M. Freund and Paul Grigas and Rahul Mazumder", "title": "An Extended Frank-Wolfe Method with \"In-Face\" Directions, and its\n  Application to Low-Rank Matrix Completion", "comments": "25 pages, 3 tables and 2 figues", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated principally by the low-rank matrix completion problem, we present\nan extension of the Frank-Wolfe method that is designed to induce near-optimal\nsolutions on low-dimensional faces of the feasible region. This is accomplished\nby a new approach to generating ``in-face\" directions at each iteration, as\nwell as through new choice rules for selecting between in-face and ``regular\"\nFrank-Wolfe steps. Our framework for generating in-face directions generalizes\nthe notion of away-steps introduced by Wolfe. In particular, the in-face\ndirections always keep the next iterate within the minimal face containing the\ncurrent iterate. We present computational guarantees for the new method that\ntrade off efficiency in computing near-optimal solutions with upper bounds on\nthe dimension of minimal faces of iterates. We apply the new method to the\nmatrix completion problem, where low-dimensional faces correspond to low-rank\nmatrices. We present computational results that demonstrate the effectiveness\nof our methodological approach at producing nearly-optimal solutions of very\nlow rank. On both artificial and real datasets, we demonstrate significant\nspeed-ups in computing very low-rank nearly-optimal solutions as compared to\neither the Frank-Wolfe method or its traditional away-step variant.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 19:31:52 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Freund", "Robert M.", ""], ["Grigas", "Paul", ""], ["Mazumder", "Rahul", ""]]}, {"id": "1511.02222", "submitter": "Andrew Wilson", "authors": "Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, Eric P. Xing", "title": "Deep Kernel Learning", "comments": "19 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce scalable deep kernels, which combine the structural properties\nof deep learning architectures with the non-parametric flexibility of kernel\nmethods. Specifically, we transform the inputs of a spectral mixture base\nkernel with a deep architecture, using local kernel interpolation, inducing\npoints, and structure exploiting (Kronecker and Toeplitz) algebra for a\nscalable kernel representation. These closed-form kernels can be used as\ndrop-in replacements for standard kernels, with benefits in expressive power\nand scalability. We jointly learn the properties of these kernels through the\nmarginal likelihood of a Gaussian process. Inference and learning cost $O(n)$\nfor $n$ training points, and predictions cost $O(1)$ per test point. On a large\nand diverse collection of applications, including a dataset with 2 million\nexamples, we show improved performance over scalable Gaussian processes with\nflexible kernel learning models, and stand-alone deep architectures.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 20:38:08 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Wilson", "Andrew Gordon", ""], ["Hu", "Zhiting", ""], ["Salakhutdinov", "Ruslan", ""], ["Xing", "Eric P.", ""]]}, {"id": "1511.02254", "submitter": "Eric Heim", "authors": "Eric Heim (1), Matthew Berger (2), Lee Seversky (2), Milos Hauskrecht\n  (1) ((1) University of Pittsburgh, (2) Air Force Research Laboratory,\n  Information Directorate)", "title": "Active Perceptual Similarity Modeling with Auxiliary Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a model of perceptual similarity from a collection of objects is a\nfundamental task in machine learning underlying numerous applications. A common\nway to learn such a model is from relative comparisons in the form of triplets:\nresponses to queries of the form \"Is object a more similar to b than it is to\nc?\". If no consideration is made in the determination of which queries to ask,\nexisting similarity learning methods can require a prohibitively large number\nof responses. In this work, we consider the problem of actively learning from\ntriplets -finding which queries are most useful for learning. Different from\nprevious active triplet learning approaches, we incorporate auxiliary\ninformation into our similarity model and introduce an active learning scheme\nto find queries that are informative for quickly learning both the relevant\naspects of auxiliary data and the directly-learned similarity components.\nCompared to prior approaches, we show that we can learn just as effectively\nwith much fewer queries. For evaluation, we introduce a new dataset of\nexhaustive triplet comparisons obtained from humans and demonstrate improved\nperformance for different types of auxiliary information.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 22:30:46 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Heim", "Eric", ""], ["Berger", "Matthew", ""], ["Seversky", "Lee", ""], ["Hauskrecht", "Milos", ""]]}, {"id": "1511.02258", "submitter": "Ze Jia Zhang", "authors": "Z. Zhang, K. Duraisamy, N. A. Gumerov", "title": "Efficient Multiscale Gaussian Process Regression using Hierarchical\n  Clustering", "comments": "22 pages, 9 figures. Preprint. Submitted to Machine Learning Mar.\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard Gaussian Process (GP) regression, a powerful machine learning tool,\nis computationally expensive when it is applied to large datasets, and\npotentially inaccurate when data points are sparsely distributed in a\nhigh-dimensional feature space. To address these challenges, a new multiscale,\nsparsified GP algorithm is formulated, with the goal of application to large\nscientific computing datasets. In this approach, the data is partitioned into\nclusters and the cluster centers are used to define a reduced training set,\nresulting in an improvement over standard GPs in terms of training and\nevaluation costs. Further, a hierarchical technique is used to adaptively map\nthe local covariance representation to the underlying sparsity of the feature\nspace, leading to improved prediction accuracy when the data distribution is\nhighly non-uniform. A theoretical investigation of the computational complexity\nof the algorithm is presented. The efficacy of this method is then demonstrated\non smooth and discontinuous analytical functions and on data from a direct\nnumerical simulation of turbulent combustion.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 23:18:13 GMT"}, {"version": "v2", "created": "Mon, 7 Mar 2016 04:20:37 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Zhang", "Z.", ""], ["Duraisamy", "K.", ""], ["Gumerov", "N. A.", ""]]}, {"id": "1511.02270", "submitter": "Matey Neykov", "authors": "Matey Neykov, Qian Lin, Jun S. Liu", "title": "Signed Support Recovery for Single Index Models in High-Dimensions", "comments": "38 pages, 7 figures; 1 table; data set analysis added; typos\n  corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the support recovery problem for single index models\n$Y=f(\\boldsymbol{X}^{\\intercal} \\boldsymbol{\\beta},\\varepsilon)$, where $f$ is\nan unknown link function, $\\boldsymbol{X}\\sim N_p(0,\\mathbb{I}_{p})$ and\n$\\boldsymbol{\\beta}$ is an $s$-sparse unit vector such that\n$\\boldsymbol{\\beta}_{i}\\in \\{\\pm\\frac{1}{\\sqrt{s}},0\\}$. In particular, we look\ninto the performance of two computationally inexpensive algorithms: (a) the\ndiagonal thresholding sliced inverse regression (DT-SIR) introduced by Lin et\nal. (2015); and (b) a semi-definite programming (SDP) approach inspired by\nAmini & Wainwright (2008). When $s=O(p^{1-\\delta})$ for some $\\delta>0$, we\ndemonstrate that both procedures can succeed in recovering the support of\n$\\boldsymbol{\\beta}$ as long as the rescaled sample size\n$\\kappa=\\frac{n}{s\\log(p-s)}$ is larger than a certain critical threshold. On\nthe other hand, when $\\kappa$ is smaller than a critical value, any algorithm\nfails to recover the support with probability at least $\\frac{1}{2}$\nasymptotically. In other words, we demonstrate that both DT-SIR and the SDP\napproach are optimal (up to a scalar) for recovering the support of\n$\\boldsymbol{\\beta}$ in terms of sample size. We provide extensive simulations,\nas well as a real dataset application to help verify our theoretical\nobservations.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2015 00:23:53 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2016 03:01:04 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Neykov", "Matey", ""], ["Lin", "Qian", ""], ["Liu", "Jun S.", ""]]}, {"id": "1511.02381", "submitter": "Shahab Asoodeh", "authors": "Shahab Asoodeh, Mario Diaz, Fady Alajaji, and Tam\\'as Linder", "title": "Information Extraction Under Privacy Constraints", "comments": "55 pages, 6 figures. Improved the organization and added detailed\n  literature review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A privacy-constrained information extraction problem is considered where for\na pair of correlated discrete random variables $(X,Y)$ governed by a given\njoint distribution, an agent observes $Y$ and wants to convey to a potentially\npublic user as much information about $Y$ as possible without compromising the\namount of information revealed about $X$. To this end, the so-called {\\em\nrate-privacy function} is introduced to quantify the maximal amount of\ninformation (measured in terms of mutual information) that can be extracted\nfrom $Y$ under a privacy constraint between $X$ and the extracted information,\nwhere privacy is measured using either mutual information or maximal\ncorrelation. Properties of the rate-privacy function are analyzed and\ninformation-theoretic and estimation-theoretic interpretations of it are\npresented for both the mutual information and maximal correlation privacy\nmeasures. It is also shown that the rate-privacy function admits a closed-form\nexpression for a large family of joint distributions of $(X,Y)$. Finally, the\nrate-privacy function under the mutual information privacy measure is\nconsidered for the case where $(X,Y)$ has a joint probability density function\nby studying the problem where the extracted information is a uniform\nquantization of $Y$ corrupted by additive Gaussian noise. The asymptotic\nbehavior of the rate-privacy function is studied as the quantization resolution\ngrows without bound and it is observed that not all of the properties of the\nrate-privacy function carry over from the discrete to the continuous case.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2015 17:27:44 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 01:46:42 GMT"}, {"version": "v3", "created": "Sun, 17 Jan 2016 20:39:48 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Asoodeh", "Shahab", ""], ["Diaz", "Mario", ""], ["Alajaji", "Fady", ""], ["Linder", "Tam\u00e1s", ""]]}, {"id": "1511.02386", "submitter": "Dustin Tran", "authors": "Rajesh Ranganath, Dustin Tran, David M. Blei", "title": "Hierarchical Variational Models", "comments": "Appears in International Conference on Machine Learning, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Black box variational inference allows researchers to easily prototype and\nevaluate an array of models. Recent advances allow such algorithms to scale to\nhigh dimensions. However, a central question remains: How to specify an\nexpressive variational distribution that maintains efficient computation? To\naddress this, we develop hierarchical variational models (HVMs). HVMs augment a\nvariational approximation with a prior on its parameters, which allows it to\ncapture complex structure for both discrete and continuous latent variables.\nThe algorithm we develop is black box, can be used for any HVM, and has the\nsame computational efficiency as the original approximation. We study HVMs on a\nvariety of deep discrete latent variable models. HVMs generalize other\nexpressive variational distributions and maintains higher fidelity to the\nposterior.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2015 19:01:48 GMT"}, {"version": "v2", "created": "Mon, 30 May 2016 21:16:38 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Ranganath", "Rajesh", ""], ["Tran", "Dustin", ""], ["Blei", "David M.", ""]]}, {"id": "1511.02476", "submitter": "Florent Krzakala", "authors": "Lenka Zdeborov\\'a, and Florent Krzakala", "title": "Statistical physics of inference: Thresholds and algorithms", "comments": "86 pages, 16 Figures. Review article based on HDR thesis of the first\n  author and lecture notes of the second", "journal-ref": "Advances in Physics Volume 65, 2016 - Issue 5", "doi": "10.1080/00018732.2016.1211393", "report-no": null, "categories": "cond-mat.stat-mech cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many questions of fundamental interest in todays science can be formulated as\ninference problems: Some partial, or noisy, observations are performed over a\nset of variables and the goal is to recover, or infer, the values of the\nvariables based on the indirect information contained in the measurements. For\nsuch problems, the central scientific questions are: Under what conditions is\nthe information contained in the measurements sufficient for a satisfactory\ninference to be possible? What are the most efficient algorithms for this task?\nA growing body of work has shown that often we can understand and locate these\nfundamental barriers by thinking of them as phase transitions in the sense of\nstatistical physics. Moreover, it turned out that we can use the gained\nphysical insight to develop new promising algorithms. Connection between\ninference and statistical physics is currently witnessing an impressive\nrenaissance and we review here the current state-of-the-art, with a pedagogical\nfocus on the Ising model which formulated as an inference problem we call the\nplanted spin glass. In terms of applications we review two classes of problems:\n(i) inference of clusters on graphs and networks, with community detection as a\nspecial case and (ii) estimating a signal from its noisy linear measurements,\nwith compressed sensing as a case of sparse estimation. Our goal is to provide\na pedagogical review for researchers in physics and other fields interested in\nthis fascinating topic.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2015 12:36:23 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2016 21:37:44 GMT"}, {"version": "v3", "created": "Tue, 8 Mar 2016 01:27:30 GMT"}, {"version": "v4", "created": "Thu, 28 Jul 2016 12:40:25 GMT"}, {"version": "v5", "created": "Mon, 22 Jan 2018 19:41:41 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Zdeborov\u00e1", "Lenka", ""], ["Krzakala", "Florent", ""]]}, {"id": "1511.02540", "submitter": "Pierre-Yves Mass\\'e", "authors": "Pierre-Yves Mass\\'e and Yann Ollivier", "title": "Speed learning on the fly", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The practical performance of online stochastic gradient descent algorithms is\nhighly dependent on the chosen step size, which must be tediously hand-tuned in\nmany applications. The same is true for more advanced variants of stochastic\ngradients, such as SAGA, SVRG, or AdaGrad. Here we propose to adapt the step\nsize by performing a gradient descent on the step size itself, viewing the\nwhole performance of the learning trajectory as a function of step size.\nImportantly, this adaptation can be computed online at little cost, without\nhaving to iterate backward passes over the full data.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2015 23:15:19 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Mass\u00e9", "Pierre-Yves", ""], ["Ollivier", "Yann", ""]]}, {"id": "1511.02543", "submitter": "Roger Grosse", "authors": "Roger B. Grosse, Zoubin Ghahramani, and Ryan P. Adams", "title": "Sandwiching the marginal likelihood using bidirectional Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing the marginal likelihood (ML) of a model requires marginalizing out\nall of the parameters and latent variables, a difficult high-dimensional\nsummation or integration problem. To make matters worse, it is often hard to\nmeasure the accuracy of one's ML estimates. We present bidirectional Monte\nCarlo, a technique for obtaining accurate log-ML estimates on data simulated\nfrom a model. This method obtains stochastic lower bounds on the log-ML using\nannealed importance sampling or sequential Monte Carlo, and obtains stochastic\nupper bounds by running these same algorithms in reverse starting from an exact\nposterior sample. The true value can be sandwiched between these two stochastic\nbounds with high probability. Using the ground truth log-ML estimates obtained\nfrom our method, we quantitatively evaluate a wide variety of existing ML\nestimators on several latent variable models: clustering, a low rank\napproximation, and a binary attributes model. These experiments yield insights\ninto how to accurately estimate marginal likelihoods.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2015 23:55:36 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Grosse", "Roger B.", ""], ["Ghahramani", "Zoubin", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1511.02619", "submitter": "Wei Ping", "authors": "Wei Ping, Qiang Liu, Alexander Ihler", "title": "Decomposition Bounds for Marginal MAP", "comments": "NIPS 2015 (full-length)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marginal MAP inference involves making MAP predictions in systems defined\nwith latent variables or missing information. It is significantly more\ndifficult than pure marginalization and MAP tasks, for which a large class of\nefficient and convergent variational algorithms, such as dual decomposition,\nexist. In this work, we generalize dual decomposition to a generic power sum\ninference task, which includes marginal MAP, along with pure marginalization\nand MAP, as special cases. Our method is based on a block coordinate descent\nalgorithm on a new convex decomposition bound, that is guaranteed to converge\nmonotonically, and can be parallelized efficiently. We demonstrate our approach\non marginal MAP queries defined on real-world problems from the UAI approximate\ninference challenge, showing that our framework is faster and more reliable\nthan previous methods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 10:21:39 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Ping", "Wei", ""], ["Liu", "Qiang", ""], ["Ihler", "Alexander", ""]]}, {"id": "1511.02722", "submitter": "Ricardo Silva", "authors": "Ricardo Silva and Shohei Shimizu", "title": "Learning Instrumental Variables with Non-Gaussianity Assumptions:\n  Theoretical Limitations and Practical Algorithms", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a causal effect from observational data is not straightforward, as\nthis is not possible without further assumptions. If hidden common causes\nbetween treatment $X$ and outcome $Y$ cannot be blocked by other measurements,\none possibility is to use an instrumental variable. In principle, it is\npossible under some assumptions to discover whether a variable is structurally\ninstrumental to a target causal effect $X \\rightarrow Y$, but current\nframeworks are somewhat lacking on how general these assumptions can be. A\ninstrumental variable discovery problem is challenging, as no variable can be\ntested as an instrument in isolation but only in groups, but different\nvariables might require different conditions to be considered an instrument.\nMoreover, identification constraints might be hard to detect statistically. In\nthis paper, we give a theoretical characterization of instrumental variable\ndiscovery, highlighting identifiability problems and solutions, the need for\nnon-Gaussianity assumptions, and how they fit within existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 15:40:50 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Silva", "Ricardo", ""], ["Shimizu", "Shohei", ""]]}, {"id": "1511.02729", "submitter": "Benjamin Guedj", "authors": "Benjamin Guedj and Sylvain Robbiano", "title": "PAC-Bayesian High Dimensional Bipartite Ranking", "comments": null, "journal-ref": "Journal of Statistical Planning and Inference (2018), vol. 196,\n  70--86", "doi": "10.1016/j.jspi.2017.10.010", "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper is devoted to the bipartite ranking problem, a classical\nstatistical learning task, in a high dimensional setting. We propose a scoring\nand ranking strategy based on the PAC-Bayesian approach. We consider nonlinear\nadditive scoring functions, and we derive non-asymptotic risk bounds under a\nsparsity assumption. In particular, oracle inequalities in probability holding\nunder a margin condition assess the performance of our procedure, and prove its\nminimax optimality. An MCMC-flavored algorithm is proposed to implement our\nmethod, along with its behavior on synthetic and real-life datasets.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 16:01:26 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 21:46:13 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Guedj", "Benjamin", ""], ["Robbiano", "Sylvain", ""]]}, {"id": "1511.02796", "submitter": "Ricardo Silva", "authors": "Ricardo Silva", "title": "Bayesian Inference in Cumulative Distribution Fields", "comments": "14 pages, 4 figures. Presented at the 12th Brazilian Meeting on\n  Bayesian Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One approach for constructing copula functions is by multiplication. Given\nthat products of cumulative distribution functions (CDFs) are also CDFs, an\nadjustment to this multiplication will result in a copula model, as discussed\nby Liebscher (J Mult Analysis, 2008). Parameterizing models via products of\nCDFs has some advantages, both from the copula perspective (e.g., it is\nwell-defined for any dimensionality) and from general multivariate analysis\n(e.g., it provides models where small dimensional marginal distributions can be\neasily read-off from the parameters). Independently, Huang and Frey (J Mach\nLearn Res, 2011) showed the connection between certain sparse graphical models\nand products of CDFs, as well as message-passing (dynamic programming) schemes\nfor computing the likelihood function of such models. Such schemes allows\nmodels to be estimated with likelihood-based methods. We discuss and\ndemonstrate MCMC approaches for estimating such models in a Bayesian context,\ntheir application in copula modeling, and how message-passing can be strongly\nsimplified. Importantly, our view of message-passing opens up possibilities to\nscaling up such methods, given that even dynamic programming is not a scalable\nsolution for calculating likelihood functions in many models.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 18:27:22 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Silva", "Ricardo", ""]]}, {"id": "1511.02821", "submitter": "Chao Chen", "authors": "Chao Chen, Alina Zare, and J. Tory Cobb", "title": "Partial Membership Latent Dirichlet Allocation", "comments": "cut to 6 pages, add sunset results", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models (e.g., pLSA, LDA, SLDA) have been widely used for segmenting\nimagery. These models are confined to crisp segmentation. Yet, there are many\nimages in which some regions cannot be assigned a crisp label (e.g., transition\nregions between a foggy sky and the ground or between sand and water at a\nbeach). In these cases, a visual word is best represented with partial\nmemberships across multiple topics. To address this, we present a partial\nmembership latent Dirichlet allocation (PM-LDA) model and associated parameter\nestimation algorithms. Experimental results on two natural image datasets and\none SONAR image dataset show that PM-LDA can produce both crisp and soft\nsemantic image segmentations; a capability existing methods do not have.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 20:04:56 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 03:59:15 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Chen", "Chao", ""], ["Zare", "Alina", ""], ["Cobb", "J. Tory", ""]]}, {"id": "1511.02825", "submitter": "Changzhe Jiao", "authors": "Changzhe Jiao, Alina Zare", "title": "Multiple Instance Dictionary Learning using Functions of Multiple\n  Instances", "comments": "Final submission to ICPR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multiple instance dictionary learning method using functions of multiple\ninstances (DL-FUMI) is proposed to address target detection and two-class\nclassification problems with inaccurate training labels. Given inaccurate\ntraining labels, DL-FUMI learns a set of target dictionary atoms that describe\nthe most distinctive and representative features of the true positive class as\nwell as a set of nontarget dictionary atoms that account for the shared\ninformation found in both the positive and negative instances. Experimental\nresults show that the estimated target dictionary atoms found by DL-FUMI are\nmore representative prototypes and identify better discriminative features of\nthe true positive class than existing methods in the literature. DL-FUMI is\nshown to have significantly better performance on several target detection and\nclassification problems as compared to other multiple instance learning (MIL)\ndictionary learning algorithms on a variety of MIL problems.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 20:12:19 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2016 21:31:50 GMT"}, {"version": "v3", "created": "Wed, 3 Aug 2016 21:03:51 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Jiao", "Changzhe", ""], ["Zare", "Alina", ""]]}, {"id": "1511.03144", "submitter": "Theodoros Tsiligkaridis", "authors": "Theodoros Tsiligkaridis", "title": "Asynchronous Decentralized 20 Questions for Adaptive Search", "comments": "19 pages, Submitted. arXiv admin note: substantial text overlap with\n  arXiv:1312.7847", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.IT cs.SY math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of adaptively searching for an unknown\ntarget using multiple agents connected through a time-varying network topology.\nAgents are equipped with sensors capable of fast information processing, and we\npropose a decentralized collaborative algorithm for controlling their search\ngiven noisy observations. Specifically, we propose decentralized extensions of\nthe adaptive query-based search strategy that combines elements from the 20\nquestions approach and social learning. Under standard assumptions on the\ntime-varying network dynamics, we prove convergence to correct consensus on the\nvalue of the parameter as the number of iterations go to infinity. The\nconvergence analysis takes a novel approach using martingale-based techniques\ncombined with spectral graph theory. Our results establish that stability and\nconsistency can be maintained even with one-way updating and randomized\npairwise averaging, thus providing a scalable low complexity method with\nperformance guarantees. We illustrate the effectiveness of our algorithm for\nrandom network topologies.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 15:35:12 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 15:27:05 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Tsiligkaridis", "Theodoros", ""]]}, {"id": "1511.03163", "submitter": "Vincenzo Lomonaco", "authors": "Davide Maltoni and Vincenzo Lomonaco", "title": "Semi-supervised Tuning from Temporal Coherence", "comments": "Under review as a conference paper at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works demonstrated the usefulness of temporal coherence to regularize\nsupervised training or to learn invariant features with deep architectures. In\nparticular, enforcing smooth output changes while presenting temporally-closed\nframes from video sequences, proved to be an effective strategy. In this paper\nwe prove the efficacy of temporal coherence for semi-supervised incremental\ntuning. We show that a deep architecture, just mildly trained in a supervised\nmanner, can progressively improve its classification accuracy, if exposed to\nvideo sequences of unlabeled data. The extent to which, in some cases, a\nsemi-supervised tuning allows to improve classification accuracy (approaching\nthe supervised one) is somewhat surprising. A number of control experiments\npointed out the fundamental role of temporal coherence.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 16:14:23 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2015 13:45:07 GMT"}, {"version": "v3", "created": "Mon, 4 Jan 2016 15:54:36 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Maltoni", "Davide", ""], ["Lomonaco", "Vincenzo", ""]]}, {"id": "1511.03198", "submitter": "Soheil Kolouri", "authors": "Soheil Kolouri, Yang Zou, and Gustavo K. Rohde", "title": "Sliced Wasserstein Kernels for Probability Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal transport distances, otherwise known as Wasserstein distances, have\nrecently drawn ample attention in computer vision and machine learning as a\npowerful discrepancy measure for probability distributions. The recent\ndevelopments on alternative formulations of the optimal transport have allowed\nfor faster solutions to the problem and has revamped its practical applications\nin machine learning. In this paper, we exploit the widely used kernel methods\nand provide a family of provably positive definite kernels based on the Sliced\nWasserstein distance and demonstrate the benefits of these kernels in a variety\nof learning tasks. Our work provides a new perspective on the application of\noptimal transport flavored distances through kernel methods in machine learning\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 17:41:48 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Kolouri", "Soheil", ""], ["Zou", "Yang", ""], ["Rohde", "Gustavo K.", ""]]}, {"id": "1511.03243", "submitter": "Yingzhen Li", "authors": "Jos\\'e Miguel Hern\\'andez-Lobato, Yingzhen Li, Mark Rowland, Daniel\n  Hern\\'andez-Lobato, Thang Bui and Richard E. Turner", "title": "Black-box $\\alpha$-divergence Minimization", "comments": "Accepted at ICML 2016. The first version (v1) was presented at NIPS\n  workshops on Advances in Approximate Bayesian Inference and Black Box\n  Learning and Inference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Black-box alpha (BB-$\\alpha$) is a new approximate inference method based on\nthe minimization of $\\alpha$-divergences. BB-$\\alpha$ scales to large datasets\nbecause it can be implemented using stochastic gradient descent. BB-$\\alpha$\ncan be applied to complex probabilistic models with little effort since it only\nrequires as input the likelihood function and its gradients. These gradients\ncan be easily obtained using automatic differentiation. By changing the\ndivergence parameter $\\alpha$, the method is able to interpolate between\nvariational Bayes (VB) ($\\alpha \\rightarrow 0$) and an algorithm similar to\nexpectation propagation (EP) ($\\alpha = 1$). Experiments on probit regression\nand neural network regression and classification problems show that BB-$\\alpha$\nwith non-standard settings of $\\alpha$, such as $\\alpha = 0.5$, usually\nproduces better predictions than with $\\alpha \\rightarrow 0$ (VB) or $\\alpha =\n1$ (EP).\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 20:02:48 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2016 23:56:55 GMT"}, {"version": "v3", "created": "Wed, 1 Jun 2016 19:05:03 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""], ["Li", "Yingzhen", ""], ["Rowland", "Mark", ""], ["Hern\u00e1ndez-Lobato", "Daniel", ""], ["Bui", "Thang", ""], ["Turner", "Richard E.", ""]]}, {"id": "1511.03249", "submitter": "Daniel Hern\\'andez-Lobato", "authors": "Daniel Hern\\'andez-Lobato, Jos\\'e Miguel Hern\\'andez-Lobato, Yingzhen\n  Li, Thang Bui, Richard E. Turner", "title": "Stochastic Expectation Propagation for Large Scale Gaussian Process\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for large scale Gaussian process classification has been recently\nproposed based on expectation propagation (EP). Such a method allows Gaussian\nprocess classifiers to be trained on very large datasets that were out of the\nreach of previous deployments of EP and has been shown to be competitive with\nrelated techniques based on stochastic variational inference. Nevertheless, the\nmemory resources required scale linearly with the dataset size, unlike in\nvariational methods. This is a severe limitation when the number of instances\nis very large. Here we show that this problem is avoided when stochastic EP is\nused to train the model.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 20:11:10 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2015 20:29:43 GMT"}, {"version": "v3", "created": "Fri, 4 Dec 2015 14:13:23 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Hern\u00e1ndez-Lobato", "Daniel", ""], ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""], ["Li", "Yingzhen", ""], ["Bui", "Thang", ""], ["Turner", "Richard E.", ""]]}, {"id": "1511.03260", "submitter": "Paul Mineiro", "authors": "Paul Mineiro and Nikos Karampatziakis", "title": "A Hierarchical Spectral Method for Extreme Classification", "comments": "Reference implementation available at\n  https://github.com/pmineiro/xlst", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme classification problems are multiclass and multilabel classification\nproblems where the number of outputs is so large that straightforward\nstrategies are neither statistically nor computationally viable. One strategy\nfor dealing with the computational burden is via a tree decomposition of the\noutput space. While this typically leads to training and inference that scales\nsublinearly with the number of outputs, it also results in reduced statistical\nperformance. In this work, we identify two shortcomings of tree decomposition\nmethods, and describe two heuristic mitigations. We compose these with an\neigenvalue technique for constructing the tree. The end result is a\ncomputationally efficient algorithm that provides good statistical performance\non several extreme data sets.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 20:52:52 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 21:40:58 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2016 17:26:46 GMT"}, {"version": "v4", "created": "Thu, 4 Feb 2016 01:07:10 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Mineiro", "Paul", ""], ["Karampatziakis", "Nikos", ""]]}, {"id": "1511.03299", "submitter": "Yoni Halpern", "authors": "Yoni Halpern and Steven Horng and David Sontag", "title": "Anchored Discrete Factor Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a semi-supervised learning algorithm for learning discrete factor\nanalysis models with arbitrary structure on the latent variables. Our algorithm\nassumes that every latent variable has an \"anchor\", an observed variable with\nonly that latent variable as its parent. Given such anchors, we show that it is\npossible to consistently recover moments of the latent variables and use these\nmoments to learn complete models. We also introduce a new technique for\nimproving the robustness of method-of-moment algorithms by optimizing over the\nmarginal polytope or its relaxations. We evaluate our algorithm using two\nreal-world tasks, tag prediction on questions from the Stack Overflow website\nand medical diagnosis in an emergency department.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 21:40:05 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Halpern", "Yoni", ""], ["Horng", "Steven", ""], ["Sontag", "David", ""]]}, {"id": "1511.03405", "submitter": "Thang Bui", "authors": "Thang D. Bui, Jos\\'e Miguel Hern\\'andez-Lobato, Yingzhen Li, Daniel\n  Hern\\'andez-Lobato, Richard E. Turner", "title": "Training Deep Gaussian Processes using Stochastic Expectation\n  Propagation and Probabilistic Backpropagation", "comments": "accepted to Workshop on Advances in Approximate Bayesian Inference,\n  NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Gaussian processes (DGPs) are multi-layer hierarchical generalisations\nof Gaussian processes (GPs) and are formally equivalent to neural networks with\nmultiple, infinitely wide hidden layers. DGPs are probabilistic and\nnon-parametric and as such are arguably more flexible, have a greater capacity\nto generalise, and provide better calibrated uncertainty estimates than\nalternative deep models. The focus of this paper is scalable approximate\nBayesian learning of these networks. The paper develops a novel and efficient\nextension of probabilistic backpropagation, a state-of-the-art method for\ntraining Bayesian neural networks, that can be used to train DGPs. The new\nmethod leverages a recently proposed method for scaling Expectation\nPropagation, called stochastic Expectation Propagation. The method is able to\nautomatically discover useful input warping, expansion or compression, and it\nis therefore is a flexible form of Bayesian kernel design. We demonstrate the\nsuccess of the new method for supervised learning on several real-world\ndatasets, showing that it typically outperforms GP regression and is never much\nworse.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 07:40:48 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Bui", "Thang D.", ""], ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""], ["Li", "Yingzhen", ""], ["Hern\u00e1ndez-Lobato", "Daniel", ""], ["Turner", "Richard E.", ""]]}, {"id": "1511.03463", "submitter": "Dimitris Kugiumtzis", "authors": "Elsa Siggiridou and Dimitris Kugiumtzis", "title": "Granger Causality in Multi-variate Time Series using a Time Ordered\n  Restricted Vector Autoregressive Model", "comments": "15 pages, 4 tables, 6 figures", "journal-ref": null, "doi": "10.1109/TSP.2015.2500893", "report-no": null, "categories": "stat.ME math.ST physics.data-an stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Granger causality has been used for the investigation of the inter-dependence\nstructure of the underlying systems of multi-variate time series. In\nparticular, the direct causal effects are commonly estimated by the conditional\nGranger causality index (CGCI). In the presence of many observed variables and\nrelatively short time series, CGCI may fail because it is based on vector\nautoregressive models (VAR) involving a large number of coefficients to be\nestimated. In this work, the VAR is restricted by a scheme that modifies the\nrecently developed method of backward-in-time selection (BTS) of the lagged\nvariables and the CGCI is combined with BTS. Further, the proposed approach is\ncompared favorably to other restricted VAR representations, such as the\ntop-down strategy, the bottom-up strategy, and the least absolute shrinkage and\nselection operator (LASSO), in terms of sensitivity and specificity of CGCI.\nThis is shown by using simulations of linear and nonlinear, low and\nhigh-dimensional systems and different time series lengths. For nonlinear\nsystems, CGCI from the restricted VAR representations are compared with\nanalogous nonlinear causality indices. Further, CGCI in conjunction with BTS\nand other restricted VAR representations is applied to multi-channel scalp\nelectroencephalogram (EEG) recordings of epileptic patients containing\nepileptiform discharges. CGCI on the restricted VAR, and BTS in particular,\ncould track the changes in brain connectivity before, during and after\nepileptiform discharges, which was not possible using the full VAR\nrepresentation.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 11:35:21 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Siggiridou", "Elsa", ""], ["Kugiumtzis", "Dimitris", ""]]}, {"id": "1511.03472", "submitter": "Michael Idowu", "authors": "Michael A. Idowu", "title": "Instantaneous Modelling and Reverse Engineering of DataConsistent Prime\n  Models in Seconds!", "comments": "Complex Adaptive Systems San Jose, CA November 2-4, 2015, 11 figures,\n  8 pages", "journal-ref": "Idowu, MA. Procedia Computer Science, Complex Adaptive Systems San\n  Jose, CA, 61, 373-380, 2015", "doi": "10.1016/j.procs.2015.09.163", "report-no": null, "categories": "q-bio.QM nlin.AO stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A theoretical framework that supports automated construction of dynamic prime\nmodels purely from experimental time series data has been invented and\ndeveloped, which can automatically generate (construct) data-driven models of\nany time series data in seconds. This has resulted in the formulation and\nformalisation of new reverse engineering and dynamic methods for automated\nsystems modelling of complex systems, including complex biological, financial,\ncontrol, and artificial neural network systems. The systems/model theory behind\nthe invention has been formalised as a new, effective and robust system\nidentification strategy complementary to process-based modelling. The proposed\ndynamic modelling and network inference solutions often involve tackling\nextremely difficult parameter estimation challenges, inferring unknown\nunderlying network structures, and unsupervised formulation and construction of\nsmart and intelligent ODE models of complex systems. In underdetermined\nconditions, i.e., cases of dealing with how best to instantaneously and rapidly\nconstruct data-consistent prime models of unknown (or well-studied) complex\nsystem from small-sized time series data, inference of unknown underlying\nnetwork of interaction is more challenging. This article reports a robust\nstep-by-step mathematical and computational analysis of the entire prime model\nconstruction process that determines a model from data in less than a minute.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 12:18:58 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Idowu", "Michael A.", ""]]}, {"id": "1511.03570", "submitter": "Guido F.  Montufar", "authors": "Guido Montufar, Jason Morton", "title": "Dimension of Marginals of Kronecker Product Models", "comments": "27 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.NE math.AG math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Kronecker product model is the set of visible marginal probability\ndistributions of an exponential family whose sufficient statistics matrix\nfactorizes as a Kronecker product of two matrices, one for the visible\nvariables and one for the hidden variables. We estimate the dimension of these\nmodels by the maximum rank of the Jacobian in the limit of large parameters.\nThe limit is described by the tropical morphism; a piecewise linear map with\npieces corresponding to slicings of the visible matrix by the normal fan of the\nhidden matrix. We obtain combinatorial conditions under which the model has the\nexpected dimension, equal to the minimum of the number of natural parameters\nand the dimension of the ambient probability simplex. Additionally, we prove\nthat the binary restricted Boltzmann machine always has the expected dimension.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 00:44:59 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Montufar", "Guido", ""], ["Morton", "Jason", ""]]}, {"id": "1511.03607", "submitter": "Ju Sun", "authors": "Ju Sun, Qing Qu, John Wright", "title": "Complete Dictionary Recovery over the Sphere I: Overview and the\n  Geometric Picture", "comments": "Accepted by IEEE Transaction on Information Theory; revised according\n  to the reviewers' comments", "journal-ref": "IEEE Trans. Information Theory, 63(2): 853 - 884 (2017)", "doi": "10.1109/TIT.2016.2632162", "report-no": null, "categories": "cs.IT cs.CV math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recovering a complete (i.e., square and\ninvertible) matrix $\\mathbf A_0$, from $\\mathbf Y \\in \\mathbb{R}^{n \\times p}$\nwith $\\mathbf Y = \\mathbf A_0 \\mathbf X_0$, provided $\\mathbf X_0$ is\nsufficiently sparse. This recovery problem is central to theoretical\nunderstanding of dictionary learning, which seeks a sparse representation for a\ncollection of input signals and finds numerous applications in modern signal\nprocessing and machine learning. We give the first efficient algorithm that\nprovably recovers $\\mathbf A_0$ when $\\mathbf X_0$ has $O(n)$ nonzeros per\ncolumn, under suitable probability model for $\\mathbf X_0$. In contrast, prior\nresults based on efficient algorithms either only guarantee recovery when\n$\\mathbf X_0$ has $O(\\sqrt{n})$ zeros per column, or require multiple rounds of\nSDP relaxation to work when $\\mathbf X_0$ has $O(n^{1-\\delta})$ nonzeros per\ncolumn (for any constant $\\delta \\in (0, 1)$). }\n  Our algorithmic pipeline centers around solving a certain nonconvex\noptimization problem with a spherical constraint. In this paper, we provide a\ngeometric characterization of the objective landscape. In particular, we show\nthat the problem is highly structured: with high probability, (1) there are no\n\"spurious\" local minimizers; and (2) around all saddle points the objective has\na negative directional curvature. This distinctive structure makes the problem\namenable to efficient optimization algorithms. In a companion paper\n(arXiv:1511.04777), we design a second-order trust-region algorithm over the\nsphere that provably converges to a local minimizer from arbitrary\ninitializations, despite the presence of saddle points.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 19:09:22 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2015 03:48:37 GMT"}, {"version": "v3", "created": "Thu, 1 Sep 2016 17:19:08 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Sun", "Ju", ""], ["Qu", "Qing", ""], ["Wright", "John", ""]]}, {"id": "1511.03643", "submitter": "David Lopez-Paz", "authors": "David Lopez-Paz, L\\'eon Bottou, Bernhard Sch\\\"olkopf, Vladimir Vapnik", "title": "Unifying distillation and privileged information", "comments": null, "journal-ref": "Proceedings of the International Conference on Learning\n  Representations (2016) 1-10", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distillation (Hinton et al., 2015) and privileged information (Vapnik &\nIzmailov, 2015) are two techniques that enable machines to learn from other\nmachines. This paper unifies these two techniques into generalized\ndistillation, a framework to learn from multiple machines and data\nrepresentations. We provide theoretical and causal insight about the inner\nworkings of generalized distillation, extend it to unsupervised, semisupervised\nand multitask learning scenarios, and illustrate its efficacy on a variety of\nnumerical simulations on both synthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 20:27:54 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 16:54:47 GMT"}, {"version": "v3", "created": "Fri, 26 Feb 2016 02:21:52 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Lopez-Paz", "David", ""], ["Bottou", "L\u00e9on", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Vapnik", "Vladimir", ""]]}, {"id": "1511.03688", "submitter": "David Degras", "authors": "Herv\\'e Cardot and David Degras", "title": "Online Principal Component Analysis in High Dimension: Which Algorithm\n  to Choose?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the current context of data explosion, online techniques that do not\nrequire storing all data in memory are indispensable to routinely perform tasks\nlike principal component analysis (PCA). Recursive algorithms that update the\nPCA with each new observation have been studied in various fields of research\nand found wide applications in industrial monitoring, computer vision,\nastronomy, and latent semantic indexing, among others. This work provides\nguidance for selecting an online PCA algorithm in practice. We present the main\napproaches to online PCA, namely, perturbation techniques, incremental methods,\nand stochastic optimization, and compare their statistical accuracy,\ncomputation time, and memory requirements using artificial and real data.\nExtensions to missing data and to functional data are discussed. All studied\nalgorithms are available in the R package onlinePCA on CRAN.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 21:25:26 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Cardot", "Herv\u00e9", ""], ["Degras", "David", ""]]}, {"id": "1511.03722", "submitter": "Nan Jiang", "authors": "Nan Jiang and Lihong Li", "title": "Doubly Robust Off-policy Value Evaluation for Reinforcement Learning", "comments": "14 pages; 4 figures; ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of off-policy value evaluation in reinforcement learning\n(RL), where one aims to estimate the value of a new policy based on data\ncollected by a different policy. This problem is often a critical step when\napplying RL in real-world problems. Despite its importance, existing general\nmethods either have uncontrolled bias or suffer high variance. In this work, we\nextend the doubly robust estimator for bandits to sequential decision-making\nproblems, which gets the best of both worlds: it is guaranteed to be unbiased\nand can have a much lower variance than the popular importance sampling\nestimators. We demonstrate the estimator's accuracy in several benchmark\nproblems, and illustrate its use as a subroutine in safe policy improvement. We\nalso provide theoretical results on the hardness of the problem, and show that\nour estimator can match the lower bound in certain scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 22:59:51 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2016 01:23:10 GMT"}, {"version": "v3", "created": "Thu, 26 May 2016 15:43:08 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Jiang", "Nan", ""], ["Li", "Lihong", ""]]}, {"id": "1511.03760", "submitter": "Yichen Chen", "authors": "Mengdi Wang, Yichen Chen, Jialin Liu, Yuantao Gu", "title": "Random Multi-Constraint Projection: Stochastic Gradient Methods for\n  Convex Optimization with Many Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider convex optimization problems subject to a large number of\nconstraints. We focus on stochastic problems in which the objective takes the\nform of expected values and the feasible set is the intersection of a large\nnumber of convex sets. We propose a class of algorithms that perform both\nstochastic gradient descent and random feasibility updates simultaneously. At\nevery iteration, the algorithms sample a number of projection points onto a\nrandomly selected small subsets of all constraints. Three feasibility update\nschemes are considered: averaging over random projected points, projecting onto\nthe most distant sample, projecting onto a special polyhedral set constructed\nbased on sample points. We prove the almost sure convergence of these\nalgorithms, and analyze the iterates' feasibility error and optimality error,\nrespectively. We provide new convergence rate benchmarks for stochastic\nfirst-order optimization with many constraints. The rate analysis and numerical\nexperiments reveal that the algorithm using the polyhedral-set projection\nscheme is the most efficient one within known algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 02:22:26 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Wang", "Mengdi", ""], ["Chen", "Yichen", ""], ["Liu", "Jialin", ""], ["Gu", "Yuantao", ""]]}, {"id": "1511.03796", "submitter": "Yuancheng Zhu", "authors": "Yuancheng Zhu, Zhe Liu and Siqi Sun", "title": "Learning Nonparametric Forest Graphical Models with Prior Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for incorporating prior information into nonparametric\nestimation of graphical models. To avoid distributional assumptions, we\nrestrict the graph to be a forest and build on the work of forest density\nestimation (FDE). We reformulate the FDE approach from a Bayesian perspective,\nand introduce prior distributions on the graphs. As two concrete examples, we\napply this framework to estimating scale-free graphs and learning multiple\ngraphs with similar structures. The resulting algorithms are equivalent to\nfinding a maximum spanning tree of a weighted graph with a penalty term on the\nconnectivity pattern of the graph. We solve the optimization problem via a\nminorize-maximization procedure with Kruskal's algorithm. Simulations show that\nthe proposed methods outperform competing parametric methods, and are robust to\nthe true data distribution. They also lead to improvement in predictive power\nand interpretability in two real data sets.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 06:36:53 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 03:18:09 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Zhu", "Yuancheng", ""], ["Liu", "Zhe", ""], ["Sun", "Siqi", ""]]}, {"id": "1511.03803", "submitter": "Weijie Su", "authors": "Cynthia Dwork and Weijie Su and Li Zhang", "title": "Private False Discovery Rate Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide the first differentially private algorithms for controlling the\nfalse discovery rate (FDR) in multiple hypothesis testing, with essentially no\nloss in power under certain conditions. Our general approach is to adapt a\nwell-known variant of the Benjamini-Hochberg procedure (BHq), making each step\ndifferentially private. This destroys the classical proof of FDR control. To\nprove FDR control of our method, (a) we develop a new proof of the original\n(non-private) BHq algorithm and its robust variants -- a proof requiring only\nthe assumption that the true null test statistics are independent, allowing for\narbitrary correlations between the true nulls and false nulls. This assumption\nis fairly weak compared to those previously shown in the vast literature on\nthis topic, and explains in part the empirical robustness of BHq. Then (b) we\nrelate the FDR control properties of the differentially private version to the\ncontrol properties of the non-private version. \\end{enumerate} We also present\na low-distortion \"one-shot\" differentially private primitive for \"top $k$\"\nproblems, e.g., \"Which are the $k$ most popular hobbies?\" (which we apply to:\n\"Which hypotheses have the $k$ most significant $p$-values?\"), and use it to\nget a faster privacy-preserving instantiation of our general approach at little\ncost in accuracy. The proof of privacy for the one-shot top~$k$ algorithm\nintroduces a new technique of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 07:31:55 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Dwork", "Cynthia", ""], ["Su", "Weijie", ""], ["Zhang", "Li", ""]]}, {"id": "1511.03947", "submitter": "Chris Glynn", "authors": "Chris Glynn, Surya T. Tokdar, David L. Banks, Brian Howard", "title": "Bayesian Analysis of Dynamic Linear Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In dynamic topic modeling, the proportional contribution of a topic to a\ndocument depends on the temporal dynamics of that topic's overall prevalence in\nthe corpus. We extend the Dynamic Topic Model of Blei and Lafferty (2006) by\nexplicitly modeling document level topic proportions with covariates and\ndynamic structure that includes polynomial trends and periodicity. A Markov\nChain Monte Carlo (MCMC) algorithm that utilizes Polya-Gamma data augmentation\nis developed for posterior inference. Conditional independencies in the model\nand sampling are made explicit, and our MCMC algorithm is parallelized where\npossible to allow for inference in large corpora. To address computational\nbottlenecks associated with Polya-Gamma sampling, we appeal to the Central\nLimit Theorem to develop a Gaussian approximation to the Polya-Gamma random\nvariable. This approximation is fast and reliable for parameter values relevant\nin the text mining domain. Our model and inference algorithm are validated with\nmultiple simulation examples, and we consider the application of modeling\ntrends in PubMed abstracts. We demonstrate that sharing information across\ndocuments is critical for accurately estimating document-specific topic\nproportions. We also show that explicitly modeling polynomial and periodic\nbehavior improves our ability to predict topic prevalence at future time\npoints.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 16:26:13 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Glynn", "Chris", ""], ["Tokdar", "Surya T.", ""], ["Banks", "David L.", ""], ["Howard", "Brian", ""]]}, {"id": "1511.03962", "submitter": "Yangfeng Ji", "authors": "Yangfeng Ji, Trevor Cohn, Lingpeng Kong, Chris Dyer, Jacob Eisenstein", "title": "Document Context Language Models", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text documents are structured on multiple levels of detail: individual words\nare related by syntax, but larger units of text are related by discourse\nstructure. Existing language models generally fail to account for discourse\nstructure, but it is crucial if we are to have language models that reward\ncoherence and generate coherent texts. We present and empirically evaluate a\nset of multi-level recurrent neural network language models, called\nDocument-Context Language Models (DCLM), which incorporate contextual\ninformation both within and beyond the sentence. In comparison with word-level\nrecurrent neural network language models, the DCLM models obtain slightly\nbetter predictive likelihoods, and considerably better assessments of document\ncoherence.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 16:53:50 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 19:40:50 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2015 03:26:47 GMT"}, {"version": "v4", "created": "Sun, 21 Feb 2016 23:46:44 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Ji", "Yangfeng", ""], ["Cohn", "Trevor", ""], ["Kong", "Lingpeng", ""], ["Dyer", "Chris", ""], ["Eisenstein", "Jacob", ""]]}, {"id": "1511.03990", "submitter": "Karthikeyan Natesan Ramamurthy", "authors": "Karthikeyan Natesan Ramamurthy, Aleksandr Y. Aravkin, Jayaraman J.\n  Thiagarajan", "title": "Automatic Inference of the Quantile Parameter", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised learning is an active research area, with numerous applications in\ndiverse fields such as data analytics, computer vision, speech and audio\nprocessing, and image understanding. In most cases, the loss functions used in\nmachine learning assume symmetric noise models, and seek to estimate the\nunknown function parameters. However, loss functions such as quantile and\nquantile Huber generalize the symmetric $\\ell_1$ and Huber losses to the\nasymmetric setting, for a fixed quantile parameter. In this paper, we propose\nto jointly infer the quantile parameter and the unknown function parameters,\nfor the asymmetric quantile Huber and quantile losses. We explore various\nproperties of the quantile Huber loss and implement a convexity certificate\nthat can be used to check convexity in the quantile parameter. When the loss if\nconvex with respect to the parameter of the function, we prove that it is\nbiconvex in both the function and the quantile parameters, and propose an\nalgorithm to jointly estimate these. Results with synthetic and real data\ndemonstrate that the proposed approach can automatically recover the quantile\nparameter corresponding to the noise and also provide an improved recovery of\nfunction parameters. To illustrate the potential of the framework, we extend\nthe gradient boosting machines with quantile losses to automatically estimate\nthe quantile parameter at each iteration.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 17:54:46 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Ramamurthy", "Karthikeyan Natesan", ""], ["Aravkin", "Aleksandr Y.", ""], ["Thiagarajan", "Jayaraman J.", ""]]}, {"id": "1511.04033", "submitter": "Emilie Devijver", "authors": "Emilie Devijver, M\\'elina Gallopin", "title": "Block-diagonal covariance selection for high-dimensional Gaussian\n  graphical models", "comments": "Accepted in JASA", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian graphical models are widely utilized to infer and visualize networks\nof dependencies between continuous variables. However, inferring the graph is\ndifficult when the sample size is small compared to the number of variables. To\nreduce the number of parameters to estimate in the model, we propose a\nnon-asymptotic model selection procedure supported by strong theoretical\nguarantees based on an oracle inequality and a minimax lower bound. The\ncovariance matrix of the model is approximated by a block-diagonal matrix. The\nstructure of this matrix is detected by thresholding the sample covariance\nmatrix, where the threshold is selected using the slope heuristic. Based on the\nblock-diagonal structure of the covariance matrix, the estimation problem is\ndivided into several independent problems: subsequently, the network of\ndependencies between variables is inferred using the graphical lasso algorithm\nin each block. The performance of the procedure is illustrated on simulated\ndata. An application to a real gene expression dataset with a limited sample\nsize is also presented: the dimension reduction allows attention to be\nobjectively focused on interactions among smaller subsets of genes, leading to\na more parsimonious and interpretable modular network.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 19:56:45 GMT"}, {"version": "v2", "created": "Fri, 8 Apr 2016 10:56:02 GMT"}, {"version": "v3", "created": "Thu, 29 Sep 2016 08:33:19 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Devijver", "Emilie", ""], ["Gallopin", "M\u00e9lina", ""]]}, {"id": "1511.04150", "submitter": "Danica J. Sutherland", "authors": "Junier B. Oliva, Danica J. Sutherland, Barnab\\'as P\\'oczos, Jeff\n  Schneider", "title": "Deep Mean Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of distributions and high-level features from deep architecture has\nbecome commonplace in modern computer vision. Both of these methodologies have\nseparately achieved a great deal of success in many computer vision tasks.\nHowever, there has been little work attempting to leverage the power of these\nto methodologies jointly. To this end, this paper presents the Deep Mean Maps\n(DMMs) framework, a novel family of methods to non-parametrically represent\ndistributions of features in convolutional neural network models.\n  DMMs are able to both classify images using the distribution of top-level\nfeatures, and to tune the top-level features for performing this task. We show\nhow to implement DMMs using a special mean map layer composed of typical CNN\noperations, making both forward and backward propagation simple.\n  We illustrate the efficacy of DMMs at analyzing distributional patterns in\nimage data in a synthetic data experiment. We also show that we extending\nexisting deep architectures with DMMs improves the performance of existing CNNs\non several challenging real-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 03:36:51 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 06:24:14 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Oliva", "Junier B.", ""], ["Sutherland", "Danica J.", ""], ["P\u00f3czos", "Barnab\u00e1s", ""], ["Schneider", "Jeff", ""]]}, {"id": "1511.04156", "submitter": "Josh Merel", "authors": "Josh Merel, David Carlson, Liam Paninski, John P. Cunningham", "title": "Neuroprosthetic decoder training as imitation learning", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pcbi.1004948", "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroprosthetic brain-computer interfaces function via an algorithm which\ndecodes neural activity of the user into movements of an end effector, such as\na cursor or robotic arm. In practice, the decoder is often learned by updating\nits parameters while the user performs a task. When the user's intention is not\ndirectly observable, recent methods have demonstrated value in training the\ndecoder against a surrogate for the user's intended movement. We describe how\ntraining a decoder in this way is a novel variant of an imitation learning\nproblem, where an oracle or expert is employed for supervised training in lieu\nof direct observations, which are not available. Specifically, we describe how\na generic imitation learning meta-algorithm, dataset aggregation (DAgger, [1]),\ncan be adapted to train a generic brain-computer interface. By deriving\nexisting learning algorithms for brain-computer interfaces in this framework,\nwe provide a novel analysis of regret (an important metric of learning\nefficacy) for brain-computer interfaces. This analysis allows us to\ncharacterize the space of algorithmic variants and bounds on their regret\nrates. Existing approaches for decoder learning have been performed in the\ncursor control setting, but the available design principles for these decoders\nare such that it has been impossible to scale them to naturalistic settings.\nLeveraging our findings, we then offer an algorithm that combines imitation\nlearning with optimal control, which should allow for training of arbitrary\neffectors for which optimal control can generate goal-oriented control. We\ndemonstrate this novel and general BCI algorithm with simulated neuroprosthetic\ncontrol of a 26 degree-of-freedom model of an arm, a sophisticated and\nrealistic end effector.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 04:21:33 GMT"}, {"version": "v2", "created": "Mon, 14 Mar 2016 16:39:03 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Merel", "Josh", ""], ["Carlson", "David", ""], ["Paninski", "Liam", ""], ["Cunningham", "John P.", ""]]}, {"id": "1511.04157", "submitter": "Kai Fan", "authors": "Kai Fan, Katherine Heller", "title": "$k$-means: Fighting against Degeneracy in Sequential Monte Carlo with an\n  Application to Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For regular particle filter algorithm or Sequential Monte Carlo (SMC)\nmethods, the initial weights are traditionally dependent on the proposed\ndistribution, the posterior distribution at the current timestamp in the\nsampled sequence, and the target is the posterior distribution of the previous\ntimestamp. This is technically correct, but leads to algorithms which usually\nhave practical issues with degeneracy, where all particles eventually collapse\nonto a single particle. In this paper, we propose and evaluate using $k$ means\nclustering to attack and even take advantage of this degeneracy. Specifically,\nwe propose a Stochastic SMC algorithm which initializes the set of $k$ means,\nproviding the initial centers chosen from the collapsed particles. To fight\nagainst degeneracy, we adjust the regular SMC weights, mediated by cluster\nproportions, and then correct them to retain the same expectation as before. We\nexperimentally demonstrate that our approach has better performance than\nvanilla algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 04:47:58 GMT"}], "update_date": "2015-11-16", "authors_parsed": [["Fan", "Kai", ""], ["Heller", "Katherine", ""]]}, {"id": "1511.04210", "submitter": "Ohad Shamir", "authors": "Itay Safran, Ohad Shamir", "title": "On the Quality of the Initial Basin in Overspecified Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning, in the form of artificial neural networks, has achieved\nremarkable practical success in recent years, for a variety of difficult\nmachine learning applications. However, a theoretical explanation for this\nremains a major open problem, since training neural networks involves\noptimizing a highly non-convex objective function, and is known to be\ncomputationally hard in the worst case. In this work, we study the\n\\emph{geometric} structure of the associated non-convex objective function, in\nthe context of ReLU networks and starting from a random initialization of the\nnetwork parameters. We identify some conditions under which it becomes more\nfavorable to optimization, in the sense of (i) High probability of initializing\nat a point from which there is a monotonically decreasing path to a global\nminimum; and (ii) High probability of initializing at a basin (suitably\ndefined) with a small minimal objective value. A common theme in our results is\nthat such properties are more likely to hold for larger (\"overspecified\")\nnetworks, which accords with some recent empirical and theoretical\nobservations.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 09:35:34 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2016 16:22:46 GMT"}, {"version": "v3", "created": "Tue, 14 Jun 2016 05:39:27 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Safran", "Itay", ""], ["Shamir", "Ohad", ""]]}, {"id": "1511.04211", "submitter": "Jan Hendrik Metzen", "authors": "Jan Hendrik Metzen", "title": "Active Contextual Entropy Search", "comments": "Corrected title of reference #19", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual policy search allows adapting robotic movement primitives to\ndifferent situations. For instance, a locomotion primitive might be adapted to\ndifferent terrain inclinations or desired walking speeds. Such an adaptation is\noften achievable by modifying a small number of hyperparameters. However,\nlearning, when performed on real robotic systems, is typically restricted to a\nsmall number of trials. Bayesian optimization has recently been proposed as a\nsample-efficient means for contextual policy search that is well suited under\nthese conditions. In this work, we extend entropy search, a variant of Bayesian\noptimization, such that it can be used for active contextual policy search\nwhere the agent selects those tasks during training in which it expects to\nlearn the most. Empirical results in simulation suggest that this allows\nlearning successful behavior with less trials.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 09:37:40 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2015 09:22:01 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Metzen", "Jan Hendrik", ""]]}, {"id": "1511.04383", "submitter": "Bopeng Li", "authors": "Bopeng Li, Sougata Chaudhuri, Ambuj Tewari", "title": "Handling Class Imbalance in Link Prediction using Learning to Rank\n  Techniques", "comments": "The paper has been withdrawn due to a baseline implementation error\n  in experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the link prediction problem in a partially observed network,\nwhere the objective is to make predictions in the unobserved portion of the\nnetwork. Many existing methods reduce link prediction to binary classification\nproblem. However, the dominance of absent links in real world networks makes\nmisclassification error a poor performance metric. Instead, researchers have\nargued for using ranking performance measures, like AUC, AP and NDCG, for\nevaluation. Our main contribution is to recast the link prediction problem as a\nlearning to rank problem and use effective learning to rank techniques directly\nduring training. This is in contrast to existing work that uses ranking\nmeasures only during evaluation. Our approach is able to deal with the class\nimbalance problem by using effective, scalable learning to rank techniques\nduring training. Furthermore, our approach allows us to combine network\ntopology and node features. As a demonstration of our general approach, we\ndevelop a link prediction method by optimizing the cross-entropy surrogate,\noriginally used in the popular ListNet ranking algorithm. We conduct extensive\nexperiments on publicly available co-authorship, citation and metabolic\nnetworks to demonstrate the merits of our method.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 18:06:15 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2016 02:40:57 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Li", "Bopeng", ""], ["Chaudhuri", "Sougata", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1511.04402", "submitter": "William Herlands", "authors": "William Herlands, Maria De-Arteaga, Daniel Neill, Artur Dubrawski", "title": "Lass-0: sparse non-convex regression by local search", "comments": "8 pages, 1 figure. NIPS 2015 Workshop of Optimization (OPT2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compute approximate solutions to L0 regularized linear regression using L1\nregularization, also known as the Lasso, as an initialization step. Our\nalgorithm, the Lass-0 (\"Lass-zero\"), uses a computationally efficient stepwise\nsearch to determine a locally optimal L0 solution given any L1 regularization\nsolution. We present theoretical results of consistency under orthogonality and\nappropriate handling of redundant features. Empirically, we use synthetic data\nto demonstrate that Lass-0 solutions are closer to the true sparse support than\nL1 regularization models. Additionally, in real-world data Lass-0 finds more\nparsimonious solutions than L1 regularization while maintaining similar\npredictive accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 19:07:50 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2016 16:24:23 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Herlands", "William", ""], ["De-Arteaga", "Maria", ""], ["Neill", "Daniel", ""], ["Dubrawski", "Artur", ""]]}, {"id": "1511.04408", "submitter": "William Herlands", "authors": "William Herlands, Andrew Wilson, Hannes Nickisch, Seth Flaxman, Daniel\n  Neill, Wilbert van Panhuis, Eric Xing", "title": "Scalable Gaussian Processes for Characterizing Multidimensional Change\n  Surfaces", "comments": "18 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a scalable Gaussian process model for identifying and\ncharacterizing smooth multidimensional changepoints, and automatically learning\nchanges in expressive covariance structure. We use Random Kitchen Sink features\nto flexibly define a change surface in combination with expressive spectral\nmixture kernels to capture the complex statistical structure. Finally, through\nthe use of novel methods for additive non-separable kernels, we can scale the\nmodel to large datasets. We demonstrate the model on numerical and real world\ndata, including a large spatio-temporal disease dataset where we identify\npreviously unknown heterogeneous changes in space and time.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 19:38:17 GMT"}], "update_date": "2015-11-16", "authors_parsed": [["Herlands", "William", ""], ["Wilson", "Andrew", ""], ["Nickisch", "Hannes", ""], ["Flaxman", "Seth", ""], ["Neill", "Daniel", ""], ["van Panhuis", "Wilbert", ""], ["Xing", "Eric", ""]]}, {"id": "1511.04412", "submitter": "Mazen Melibari", "authors": "Mazen Melibari, Pascal Poupart, Prashant Doshi and George Trimponias", "title": "Dynamic Sum Product Networks for Tractable Inference on Sequence Data\n  (Extended Version)", "comments": "Published in the Proceedings of the International Conference on\n  Probabilistic Graphical Models (PGM), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sum-Product Networks (SPN) have recently emerged as a new class of tractable\nprobabilistic graphical models. Unlike Bayesian networks and Markov networks\nwhere inference may be exponential in the size of the network, inference in\nSPNs is in time linear in the size of the network. Since SPNs represent\ndistributions over a fixed set of variables only, we propose dynamic sum\nproduct networks (DSPNs) as a generalization of SPNs for sequence data of\nvarying length. A DSPN consists of a template network that is repeated as many\ntimes as needed to model data sequences of any length. We present a local\nsearch technique to learn the structure of the template network. In contrast to\ndynamic Bayesian networks for which inference is generally exponential in the\nnumber of variables per time slice, DSPNs inherit the linear inference\ncomplexity of SPNs. We demonstrate the advantages of DSPNs over DBNs and other\nmodels on several datasets of sequence data.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 19:56:15 GMT"}, {"version": "v2", "created": "Sat, 16 Jul 2016 03:37:01 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Melibari", "Mazen", ""], ["Poupart", "Pascal", ""], ["Doshi", "Prashant", ""], ["Trimponias", "George", ""]]}, {"id": "1511.04508", "submitter": "Nicolas Papernot", "authors": "Nicolas Papernot and Patrick McDaniel and Xi Wu and Somesh Jha and\n  Ananthram Swami", "title": "Distillation as a Defense to Adversarial Perturbations against Deep\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning algorithms have been shown to perform extremely well on many\nclassical machine learning problems. However, recent studies have shown that\ndeep learning, like other machine learning techniques, is vulnerable to\nadversarial samples: inputs crafted to force a deep neural network (DNN) to\nprovide adversary-selected outputs. Such attacks can seriously undermine the\nsecurity of the system supported by the DNN, sometimes with devastating\nconsequences. For example, autonomous vehicles can be crashed, illicit or\nillegal content can bypass content filters, or biometric authentication systems\ncan be manipulated to allow improper access. In this work, we introduce a\ndefensive mechanism called defensive distillation to reduce the effectiveness\nof adversarial samples on DNNs. We analytically investigate the\ngeneralizability and robustness properties granted by the use of defensive\ndistillation when training DNNs. We also empirically study the effectiveness of\nour defense mechanisms on two DNNs placed in adversarial settings. The study\nshows that defensive distillation can reduce effectiveness of sample creation\nfrom 95% to less than 0.5% on a studied DNN. Such dramatic gains can be\nexplained by the fact that distillation leads gradients used in adversarial\nsample creation to be reduced by a factor of 10^30. We also find that\ndistillation increases the average minimum number of features that need to be\nmodified to create adversarial samples by about 800% on one of the DNNs we\ntested.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 04:51:04 GMT"}, {"version": "v2", "created": "Mon, 14 Mar 2016 13:08:09 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Papernot", "Nicolas", ""], ["McDaniel", "Patrick", ""], ["Wu", "Xi", ""], ["Jha", "Somesh", ""], ["Swami", "Ananthram", ""]]}, {"id": "1511.04514", "submitter": "Zhuoran Yang", "authors": "Zhuoran Yang, Zhaoran Wang, Han Liu, Yonina C. Eldar, Tong Zhang", "title": "Sparse Nonlinear Regression: Parameter Estimation and Asymptotic\n  Inference", "comments": "32 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study parameter estimation and asymptotic inference for sparse nonlinear\nregression. More specifically, we assume the data are given by $y = f( x^\\top\n\\beta^* ) + \\epsilon$, where $f$ is nonlinear. To recover $\\beta^*$, we propose\nan $\\ell_1$-regularized least-squares estimator. Unlike classical linear\nregression, the corresponding optimization problem is nonconvex because of the\nnonlinearity of $f$. In spite of the nonconvexity, we prove that under mild\nconditions, every stationary point of the objective enjoys an optimal\nstatistical rate of convergence. In addition, we provide an efficient algorithm\nthat provably converges to a stationary point. We also access the uncertainty\nof the obtained estimator. Specifically, based on any stationary point of the\nobjective, we construct valid hypothesis tests and confidence intervals for the\nlow dimensional components of the high-dimensional parameter $\\beta^*$.\nDetailed numerical results are provided to back up our theory.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 05:57:24 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Yang", "Zhuoran", ""], ["Wang", "Zhaoran", ""], ["Liu", "Han", ""], ["Eldar", "Yonina C.", ""], ["Zhang", "Tong", ""]]}, {"id": "1511.04581", "submitter": "Eugene Belilovsky", "authors": "Wacha Bounliphone, Eugene Belilovsky, Matthew B. Blaschko, Ioannis\n  Antonoglou, Arthur Gretton", "title": "A Test of Relative Similarity For Model Selection in Generative Models", "comments": "International Conference on Learning Representations 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic generative models provide a powerful framework for representing\ndata that avoids the expense of manual annotation typically needed by\ndiscriminative approaches. Model selection in this generative setting can be\nchallenging, however, particularly when likelihoods are not easily accessible.\nTo address this issue, we introduce a statistical test of relative similarity,\nwhich is used to determine which of two models generates samples that are\nsignificantly closer to a real-world reference dataset of interest. We use as\nour test statistic the difference in maximum mean discrepancies (MMDs) between\nthe reference dataset and each model dataset, and derive a powerful,\nlow-variance test based on the joint asymptotic distribution of the MMDs\nbetween each reference-model pair. In experiments on deep generative models,\nincluding the variational auto-encoder and generative moment matching network,\nthe tests provide a meaningful ranking of model performance as a function of\nparameter and training settings.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 17:18:47 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 11:12:05 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2016 15:35:53 GMT"}, {"version": "v4", "created": "Mon, 15 Feb 2016 15:12:44 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Bounliphone", "Wacha", ""], ["Belilovsky", "Eugene", ""], ["Blaschko", "Matthew B.", ""], ["Antonoglou", "Ioannis", ""], ["Gretton", "Arthur", ""]]}, {"id": "1511.04590", "submitter": "Li Yao", "authors": "Li Yao, Nicolas Ballas, Kyunghyun Cho, John R. Smith, Yoshua Bengio", "title": "Oracle performance for visual captioning", "comments": "BMVC2016 (Oral paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of associating images and videos with a natural language description\nhas attracted a great amount of attention recently. Rapid progress has been\nmade in terms of both developing novel algorithms and releasing new datasets.\nIndeed, the state-of-the-art results on some of the standard datasets have been\npushed into the regime where it has become more and more difficult to make\nsignificant improvements. Instead of proposing new models, this work\ninvestigates the possibility of empirically establishing performance upper\nbounds on various visual captioning datasets without extra data labelling\neffort or human evaluation. In particular, it is assumed that visual captioning\nis decomposed into two steps: from visual inputs to visual concepts, and from\nvisual concepts to natural language descriptions. One would be able to obtain\nan upper bound when assuming the first step is perfect and only requiring\ntraining a conditional language model for the second step. We demonstrate the\nconstruction of such bounds on MS-COCO, YouTube2Text and LSMDC (a combination\nof M-VAD and MPII-MD). Surprisingly, despite of the imperfect process we used\nfor visual concept extraction in the first step and the simplicity of the\nlanguage model for the second step, we show that current state-of-the-art\nmodels fall short when being compared with the learned upper bounds.\nFurthermore, with such a bound, we quantify several important factors\nconcerning image and video captioning: the number of visual concepts captured\nby different models, the trade-off between the amount of visual elements\ncaptured and their accuracy, and the intrinsic difficulty and blessing of\ndifferent datasets.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 18:02:39 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 04:20:08 GMT"}, {"version": "v3", "created": "Sun, 3 Jan 2016 04:55:57 GMT"}, {"version": "v4", "created": "Wed, 6 Jan 2016 23:38:25 GMT"}, {"version": "v5", "created": "Wed, 14 Sep 2016 16:55:29 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Yao", "Li", ""], ["Ballas", "Nicolas", ""], ["Cho", "Kyunghyun", ""], ["Smith", "John R.", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1511.04690", "submitter": "Weiyang Liu", "authors": "Weiyang Liu, Rongmei Lin, Meng Yang", "title": "Robust Elastic Net Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a robust elastic net (REN) model for high-dimensional sparse\nregression and give its performance guarantees (both the statistical error\nbound and the optimization bound). A simple idea of trimming the inner product\nis applied to the elastic net model. Specifically, we robustify the covariance\nmatrix by trimming the inner product based on the intuition that the trimmed\ninner product can not be significant affected by a bounded number of\narbitrarily corrupted points (outliers). The REN model can also derive two\ninteresting special cases: robust Lasso and robust soft thresholding.\nComprehensive experimental results show that the robustness of the proposed\nmodel consistently outperforms the original elastic net and matches the\nperformance guarantees nicely.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2015 12:17:00 GMT"}, {"version": "v2", "created": "Sun, 1 May 2016 18:03:05 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Liu", "Weiyang", ""], ["Lin", "Rongmei", ""], ["Yang", "Meng", ""]]}, {"id": "1511.04775", "submitter": "Cyril Stark", "authors": "Cyril Stark", "title": "Expressive recommender systems through normalized nonnegative models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce normalized nonnegative models (NNM) for explorative data\nanalysis. NNMs are partial convexifications of models from probability theory.\nWe demonstrate their value at the example of item recommendation. We show that\nNNM-based recommender systems satisfy three criteria that all recommender\nsystems should ideally satisfy: high predictive power, computational\ntractability, and expressive representations of users and items. Expressive\nuser and item representations are important in practice to succinctly summarize\nthe pool of customers and the pool of items. In NNMs, user representations are\nexpressive because each user's preference can be regarded as normalized mixture\nof preferences of stereotypical users. The interpretability of item and user\nrepresentations allow us to arrange properties of items (e.g., genres of movies\nor topics of documents) or users (e.g., personality traits) hierarchically.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2015 22:39:58 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Stark", "Cyril", ""]]}, {"id": "1511.04776", "submitter": "Marc Goessling", "authors": "Marc Goessling, Yali Amit", "title": "Mixtures of Sparse Autoregressive Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider high-dimensional distribution estimation through autoregressive\nnetworks. By combining the concepts of sparsity, mixtures and parameter sharing\nwe obtain a simple model which is fast to train and which achieves\nstate-of-the-art or better results on several standard benchmark datasets.\nSpecifically, we use an L1-penalty to regularize the conditional distributions\nand introduce a procedure for automatic parameter sharing between mixture\ncomponents. Moreover, we propose a simple distributed representation which\npermits exact likelihood evaluations since the latent variables are interleaved\nwith the observable variables and can be easily integrated out. Our model\nachieves excellent generalization performance and scales well to extremely high\ndimensions.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2015 22:54:02 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2015 04:21:25 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2016 05:01:11 GMT"}, {"version": "v4", "created": "Tue, 26 Apr 2016 23:12:32 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Goessling", "Marc", ""], ["Amit", "Yali", ""]]}, {"id": "1511.04777", "submitter": "Ju Sun", "authors": "Ju Sun, Qing Qu, John Wright", "title": "Complete Dictionary Recovery over the Sphere II: Recovery by Riemannian\n  Trust-region Method", "comments": "The second of two papers based on the report arXiv:1504.06785.\n  Accepted by IEEE Transaction on Information Theory; revised according to the\n  reviewers' comments", "journal-ref": "IEEE Trans. Information Theory, 63(2): 885 - 914 (2017)", "doi": "10.1109/TIT.2016.2632149", "report-no": null, "categories": "cs.IT cs.CV math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recovering a complete (i.e., square and\ninvertible) matrix $\\mathbf A_0$, from $\\mathbf Y \\in \\mathbb{R}^{n \\times p}$\nwith $\\mathbf Y = \\mathbf A_0 \\mathbf X_0$, provided $\\mathbf X_0$ is\nsufficiently sparse. This recovery problem is central to theoretical\nunderstanding of dictionary learning, which seeks a sparse representation for a\ncollection of input signals and finds numerous applications in modern signal\nprocessing and machine learning. We give the first efficient algorithm that\nprovably recovers $\\mathbf A_0$ when $\\mathbf X_0$ has $O(n)$ nonzeros per\ncolumn, under suitable probability model for $\\mathbf X_0$.\n  Our algorithmic pipeline centers around solving a certain nonconvex\noptimization problem with a spherical constraint, and hence is naturally\nphrased in the language of manifold optimization. In a companion paper\n(arXiv:1511.03607), we have showed that with high probability our nonconvex\nformulation has no \"spurious\" local minimizers and around any saddle point the\nobjective function has a negative directional curvature. In this paper, we take\nadvantage of the particular geometric structure, and describe a Riemannian\ntrust region algorithm that provably converges to a local minimizer with from\narbitrary initializations. Such minimizers give excellent approximations to\nrows of $\\mathbf X_0$. The rows are then recovered by linear programming\nrounding and deflation.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2015 23:00:29 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2015 03:57:40 GMT"}, {"version": "v3", "created": "Thu, 1 Sep 2016 17:27:12 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Sun", "Ju", ""], ["Qu", "Qing", ""], ["Wright", "John", ""]]}, {"id": "1511.04780", "submitter": "Sebastian Weichwald", "authors": "Sebastian Weichwald, Timm Meyer, Ozan \\\"Ozdenizci, Bernhard\n  Sch\\\"olkopf, Tonio Ball, Moritz Grosse-Wentrup", "title": "Causal interpretation rules for encoding and decoding models in\n  neuroimaging", "comments": "accepted manuscript", "journal-ref": "NeuroImage, 110:48-59, 2015", "doi": "10.1016/j.neuroimage.2015.01.036", "report-no": null, "categories": "stat.ML cs.LG q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal terminology is often introduced in the interpretation of encoding and\ndecoding models trained on neuroimaging data. In this article, we investigate\nwhich causal statements are warranted and which ones are not supported by\nempirical evidence. We argue that the distinction between encoding and decoding\nmodels is not sufficient for this purpose: relevant features in encoding and\ndecoding models carry a different meaning in stimulus- and in response-based\nexperimental paradigms. We show that only encoding models in the stimulus-based\nsetting support unambiguous causal interpretations. By combining encoding and\ndecoding models trained on the same data, however, we obtain insights into\ncausal relations beyond those that are implied by each individual model type.\nWe illustrate the empirical relevance of our theoretical findings on EEG data\nrecorded during a visuo-motor learning task.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2015 23:16:10 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Weichwald", "Sebastian", ""], ["Meyer", "Timm", ""], ["\u00d6zdenizci", "Ozan", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Ball", "Tonio", ""], ["Grosse-Wentrup", "Moritz", ""]]}, {"id": "1511.04817", "submitter": "Matt Wytock", "authors": "Matt Wytock and J. Zico Kolter", "title": "Probabilistic Segmentation via Total Variation Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a convex approach to probabilistic segmentation and modeling of\ntime series data. Our approach builds upon recent advances in multivariate\ntotal variation regularization, and seeks to learn a separate set of parameters\nfor the distribution over the observations at each time point, but with an\nadditional penalty that encourages the parameters to remain constant over time.\nWe propose efficient optimization methods for solving the resulting (large)\noptimization problems, and a two-stage procedure for estimating recurring\nclusters under such models, based upon kernel density estimation. Finally, we\nshow on a number of real-world segmentation tasks, the resulting methods often\nperform as well or better than existing latent variable models, while being\nsubstantially easier to train.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 04:11:00 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Wytock", "Matt", ""], ["Kolter", "J. Zico", ""]]}, {"id": "1511.04834", "submitter": "Arvind Neelakantan", "authors": "Arvind Neelakantan, Quoc V. Le, Ilya Sutskever", "title": "Neural Programmer: Inducing Latent Programs with Gradient Descent", "comments": "Accepted as a conference paper at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have achieved impressive supervised classification\nperformance in many tasks including image recognition, speech recognition, and\nsequence to sequence learning. However, this success has not been translated to\napplications like question answering that may involve complex arithmetic and\nlogic reasoning. A major limitation of these models is in their inability to\nlearn even simple arithmetic and logic operations. For example, it has been\nshown that neural networks fail to learn to add two binary numbers reliably. In\nthis work, we propose Neural Programmer, an end-to-end differentiable neural\nnetwork augmented with a small set of basic arithmetic and logic operations.\nNeural Programmer can call these augmented operations over several steps,\nthereby inducing compositional programs that are more complex than the built-in\noperations. The model learns from a weak supervision signal which is the result\nof execution of the correct program, hence it does not require expensive\nannotation of the correct program itself. The decisions of what operations to\ncall, and what data segments to apply to are inferred by Neural Programmer.\nSuch decisions, during training, are done in a differentiable fashion so that\nthe entire network can be trained jointly by gradient descent. We find that\ntraining the model is difficult, but it can be greatly improved by adding\nrandom noise to the gradient. On a fairly complex synthetic table-comprehension\ndataset, traditional recurrent networks and attentional models perform poorly\nwhile Neural Programmer typically obtains nearly perfect accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 06:03:58 GMT"}, {"version": "v2", "created": "Tue, 1 Mar 2016 07:00:28 GMT"}, {"version": "v3", "created": "Thu, 4 Aug 2016 18:23:03 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Neelakantan", "Arvind", ""], ["Le", "Quoc V.", ""], ["Sutskever", "Ilya", ""]]}, {"id": "1511.04839", "submitter": "Weiran Wang", "authors": "Tomer Michaeli, Weiran Wang, Karen Livescu", "title": "Nonparametric Canonical Correlation Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical correlation analysis (CCA) is a classical representation learning\ntechnique for finding correlated variables in multi-view data. Several\nnonlinear extensions of the original linear CCA have been proposed, including\nkernel and deep neural network methods. These approaches seek maximally\ncorrelated projections among families of functions, which the user specifies\n(by choosing a kernel or neural network structure), and are computationally\ndemanding. Interestingly, the theory of nonlinear CCA, without functional\nrestrictions, had been studied in the population setting by Lancaster already\nin the 1950s, but these results have not inspired practical algorithms. We\nrevisit Lancaster's theory to devise a practical algorithm for nonparametric\nCCA (NCCA). Specifically, we show that the solution can be expressed in terms\nof the singular value decomposition of a certain operator associated with the\njoint density of the views. Thus, by estimating the population density from\ndata, NCCA reduces to solving an eigenvalue system, superficially like kernel\nCCA but, importantly, without requiring the inversion of any kernel matrix. We\nalso derive a partially linear CCA (PLCCA) variant in which one of the views\nundergoes a linear projection while the other is nonparametric. Using a kernel\ndensity estimate based on a small number of nearest neighbors, our NCCA and\nPLCCA algorithms are memory-efficient, often run much faster, and perform\nbetter than kernel CCA and comparable to deep CCA.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 06:25:59 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2015 16:26:17 GMT"}, {"version": "v3", "created": "Sun, 10 Jan 2016 15:16:00 GMT"}, {"version": "v4", "created": "Sun, 7 Feb 2016 16:11:45 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Michaeli", "Tomer", ""], ["Wang", "Weiran", ""], ["Livescu", "Karen", ""]]}, {"id": "1511.04898", "submitter": "Bertrand Thirion", "authors": "Bertrand Thirion (PARIETAL), Andr\\'es Hoyos-Idrobo (NEUROSPIN,\n  PARIETAL), Jonas Kahn (LPP), Gael Varoquaux (NEUROSPIN, PARIETAL)", "title": "Fast clustering for scalable statistical analysis on structured images", "comments": "ICML Workshop on Statistics, Machine Learning and Neuroscience\n  (Stamlins 2015), Jul 2015, Lille, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of brain images as markers for diseases or behavioral differences is\nchallenged by the small effects size and the ensuing lack of power, an issue\nthat has incited researchers to rely more systematically on large cohorts.\nCoupled with resolution increases, this leads to very large datasets. A\nstriking example in the case of brain imaging is that of the Human Connectome\nProject: 20 Terabytes of data and growing. The resulting data deluge poses\nsevere challenges regarding the tractability of some processing steps\n(discriminant analysis, multivariate models) due to the memory demands posed by\nthese data. In this work, we revisit dimension reduction approaches, such as\nrandom projections, with the aim of replacing costly function evaluations by\ncheaper ones while decreasing the memory requirements. Specifically, we\ninvestigate the use of alternate schemes, based on fast clustering, that are\nwell suited for signals exhibiting a strong spatial structure, such as\nanatomical and functional brain images. Our contribution is twofold: i) we\npropose a linear-time clustering scheme that bypasses the percolation issues\ninherent in these algorithms and thus provides compressions nearly as good as\ntraditional quadratic-complexity variance-minimizing clustering schemes, ii) we\nshow that cluster-based compression can have the virtuous effect of removing\nhigh-frequency noise, actually improving subsequent estimations steps. As a\nconsequence, the proposed approach yields very accurate models on several\nlarge-scale problems yet with impressive gains in computational efficiency,\nmaking it possible to analyze large datasets.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 10:26:18 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Thirion", "Bertrand", "", "PARIETAL"], ["Hoyos-Idrobo", "Andr\u00e9s", "", "NEUROSPIN,\n  PARIETAL"], ["Kahn", "Jonas", "", "LPP"], ["Varoquaux", "Gael", "", "NEUROSPIN, PARIETAL"]]}, {"id": "1511.04970", "submitter": "Bruno Gon\\c{c}alves", "authors": "Bruno Gon\\c{c}alves and David S\\'anchez", "title": "Learning about Spanish dialects through Twitter", "comments": "16 pages, 5 figures, 1 table", "journal-ref": "RILI, XVI 2 (28), 65-75 (2016)", "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.CY physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper maps the large-scale variation of the Spanish language by\nemploying a corpus based on geographically tagged Twitter messages. Lexical\ndialects are extracted from an analysis of variants of tens of concepts. The\nresulting maps show linguistic variation on an unprecedented scale across the\nglobe. We discuss the properties of the main dialects within a machine learning\napproach and find that varieties spoken in urban areas have an international\ncharacter in contrast to country areas where dialects show a more regional\nuniformity.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 14:29:38 GMT"}, {"version": "v2", "created": "Mon, 6 Feb 2017 00:51:34 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Gon\u00e7alves", "Bruno", ""], ["S\u00e1nchez", "David", ""]]}, {"id": "1511.05042", "submitter": "Alexandre de Br\\'ebisson", "authors": "Alexandre de Br\\'ebisson and Pascal Vincent", "title": "An Exploration of Softmax Alternatives Belonging to the Spherical Loss\n  Family", "comments": "Published at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a multi-class classification problem, it is standard to model the output\nof a neural network as a categorical distribution conditioned on the inputs.\nThe output must therefore be positive and sum to one, which is traditionally\nenforced by a softmax. This probabilistic mapping allows to use the maximum\nlikelihood principle, which leads to the well-known log-softmax loss. However\nthe choice of the softmax function seems somehow arbitrary as there are many\nother possible normalizing functions. It is thus unclear why the log-softmax\nloss would perform better than other loss alternatives. In particular Vincent\net al. (2015) recently introduced a class of loss functions, called the\nspherical family, for which there exists an efficient algorithm to compute the\nupdates of the output weights irrespective of the output size. In this paper,\nwe explore several loss functions from this family as possible alternatives to\nthe traditional log-softmax. In particular, we focus our investigation on\nspherical bounds of the log-softmax loss and on two spherical log-likelihood\nlosses, namely the log-Spherical Softmax suggested by Vincent et al. (2015) and\nthe log-Taylor Softmax that we introduce. Although these alternatives do not\nyield as good results as the log-softmax loss on two language modeling tasks,\nthey surprisingly outperform it in our experiments on MNIST and CIFAR-10,\nsuggesting that they might be relevant in a broad range of applications.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 17:15:51 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2016 21:36:50 GMT"}, {"version": "v3", "created": "Sun, 28 Feb 2016 13:22:44 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["de Br\u00e9bisson", "Alexandre", ""], ["Vincent", "Pascal", ""]]}, {"id": "1511.05101", "submitter": "Ferenc Husz\\'ar", "authors": "Ferenc Husz\\'ar", "title": "How (not) to Train your Generative Model: Scheduled Sampling,\n  Likelihood, Adversary?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern applications and progress in deep learning research have created\nrenewed interest for generative models of text and of images. However, even\ntoday it is unclear what objective functions one should use to train and\nevaluate these models. In this paper we present two contributions.\n  Firstly, we present a critique of scheduled sampling, a state-of-the-art\ntraining method that contributed to the winning entry to the MSCOCO image\ncaptioning benchmark in 2015. Here we show that despite this impressive\nempirical performance, the objective function underlying scheduled sampling is\nimproper and leads to an inconsistent learning algorithm.\n  Secondly, we revisit the problems that scheduled sampling was meant to\naddress, and present an alternative interpretation. We argue that maximum\nlikelihood is an inappropriate training objective when the end-goal is to\ngenerate natural-looking samples. We go on to derive an ideal objective\nfunction to use in this situation instead. We introduce a generalisation of\nadversarial training, and show how such method can interpolate between maximum\nlikelihood training and our ideal training objective. To our knowledge this is\nthe first theoretical analysis that explains why adversarial training tends to\nproduce samples with higher perceived quality.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 19:43:19 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Husz\u00e1r", "Ferenc", ""]]}, {"id": "1511.05102", "submitter": "Denise Reeves PhD", "authors": "Denise M. Reeves", "title": "Resolving the Geometric Locus Dilemma for Support Vector Learning\n  Machines", "comments": "170 pages, 33 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capacity control, the bias/variance dilemma, and learning unknown functions\nfrom data, are all concerned with identifying effective and consistent fits of\nunknown geometric loci to random data points. A geometric locus is a curve or\nsurface formed by points, all of which possess some uniform property. A\ngeometric locus of an algebraic equation is the set of points whose coordinates\nare solutions of the equation. Any given curve or surface must pass through\neach point on a specified locus. This paper argues that it is impossible to fit\nrandom data points to algebraic equations of partially configured geometric\nloci that reference arbitrary Cartesian coordinate systems. It also argues that\nthe fundamental curve of a linear decision boundary is actually a principal\neigenaxis. It is shown that learning principal eigenaxes of linear decision\nboundaries involves finding a point of statistical equilibrium for which\neigenenergies of principal eigenaxis components are symmetrically balanced with\neach other. It is demonstrated that learning linear decision boundaries\ninvolves strong duality relationships between a statistical eigenlocus of\nprincipal eigenaxis components and its algebraic forms, in primal and dual,\ncorrelated Hilbert spaces. Locus equations are introduced and developed that\ndescribe principal eigen-coordinate systems for lines, planes, and hyperplanes.\nThese equations are used to introduce and develop primal and dual statistical\neigenlocus equations of principal eigenaxes of linear decision boundaries.\nImportant generalizations for linear decision boundaries are shown to be\nencoded within a dual statistical eigenlocus of principal eigenaxis components.\nPrincipal eigenaxes of linear decision boundaries are shown to encode Bayes'\nlikelihood ratio for common covariance data and a robust likelihood ratio for\nall other data.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 19:44:54 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Reeves", "Denise M.", ""]]}, {"id": "1511.05118", "submitter": "Gilles Puy", "authors": "Gilles Puy, Nicolas Tremblay, R\\'emi Gribonval, Pierre Vandergheynst", "title": "Random sampling of bandlimited signals on graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of sampling k-bandlimited signals on graphs. We propose\ntwo sampling strategies that consist in selecting a small subset of nodes at\nrandom. The first strategy is non-adaptive, i.e., independent of the graph\nstructure, and its performance depends on a parameter called the graph\ncoherence. On the contrary, the second strategy is adaptive but yields optimal\nresults. Indeed, no more than O(k log(k)) measurements are sufficient to ensure\nan accurate and stable recovery of all k-bandlimited signals. This second\nstrategy is based on a careful choice of the sampling distribution, which can\nbe estimated quickly. Then, we propose a computationally efficient decoder to\nreconstruct k-bandlimited signals from their samples. We prove that it yields\naccurate reconstructions and that it is also stable to noise. Finally, we\nconduct several experiments to test these techniques.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 20:38:37 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 12:36:39 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Puy", "Gilles", ""], ["Tremblay", "Nicolas", ""], ["Gribonval", "R\u00e9mi", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1511.05121", "submitter": "Rahul Gopal Krishnan", "authors": "Rahul G. Krishnan, Uri Shalit, David Sontag", "title": "Deep Kalman Filters", "comments": "17 pages, 14 figures: Fixed typo in Fig. 1(b) and added reference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kalman Filters are one of the most influential models of time-varying\nphenomena. They admit an intuitive probabilistic interpretation, have a simple\nfunctional form, and enjoy widespread adoption in a variety of disciplines.\nMotivated by recent variational methods for learning deep generative models, we\nintroduce a unified algorithm to efficiently learn a broad spectrum of Kalman\nfilters. Of particular interest is the use of temporal generative models for\ncounterfactual inference. We investigate the efficacy of such models for\ncounterfactual inference, and to that end we introduce the \"Healing MNIST\"\ndataset where long-term structure, noise and actions are applied to sequences\nof digits. We show the efficacy of our method for modeling this dataset. We\nfurther show how our model can be used for counterfactual inference for\npatients, based on electronic health record data of 8,000 patients over 4.5\nyears.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 20:46:38 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2015 20:47:00 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Krishnan", "Rahul G.", ""], ["Shalit", "Uri", ""], ["Sontag", "David", ""]]}, {"id": "1511.05174", "submitter": "Vishwanath Saragadam Raja Venkata", "authors": "Vishwanath Saragadam, Xin Li, Aswin Sankaranarayanan", "title": "Cross-scale predictive dictionaries", "comments": "12 pages", "journal-ref": null, "doi": "10.1109/TIP.2018.2869719", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representations using data dictionaries provide an efficient model\nparticularly for signals that do not enjoy alternate analytic sparsifying\ntransformations. However, solving inverse problems with sparsifying\ndictionaries can be computationally expensive, especially when the dictionary\nunder consideration has a large number of atoms. In this paper, we incorporate\nadditional structure on to dictionary-based sparse representations for visual\nsignals to enable speedups when solving sparse approximation problems. The\nspecific structure that we endow onto sparse models is that of a multi-scale\nmodeling where the sparse representation at each scale is constrained by the\nsparse representation at coarser scales. We show that this cross-scale\npredictive model delivers significant speedups, often in the range of\n10-60$\\times$, with little loss in accuracy for linear inverse problems\nassociated with images, videos, and light fields.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 21:07:38 GMT"}, {"version": "v2", "created": "Sat, 24 Dec 2016 21:09:48 GMT"}, {"version": "v3", "created": "Tue, 4 Sep 2018 03:25:13 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Saragadam", "Vishwanath", ""], ["Li", "Xin", ""], ["Sankaranarayanan", "Aswin", ""]]}, {"id": "1511.05190", "submitter": "Benjamin Nachman", "authors": "Luke de Oliveira, Michael Kagan, Lester Mackey, Benjamin Nachman, and\n  Ariel Schwartzman", "title": "Jet-Images -- Deep Learning Edition", "comments": "32 pages, 24 figures. Version that is published in JHEP", "journal-ref": "JHEP 07 (2016) 069", "doi": "10.1007/JHEP07(2016)069", "report-no": null, "categories": "hep-ph physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building on the notion of a particle physics detector as a camera and the\ncollimated streams of high energy particles, or jets, it measures as an image,\nwe investigate the potential of machine learning techniques based on deep\nlearning architectures to identify highly boosted W bosons. Modern deep\nlearning algorithms trained on jet images can out-perform standard\nphysically-motivated feature driven approaches to jet tagging. We develop\ntechniques for visualizing how these features are learned by the network and\nwhat additional information is used to improve performance. This interplay\nbetween physically-motivated feature driven tools and supervised learning\nalgorithms is general and can be used to significantly increase the sensitivity\nto discover new particles and new forces, and gain a deeper understanding of\nthe physics within jets.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 21:44:37 GMT"}, {"version": "v2", "created": "Mon, 4 Apr 2016 01:17:53 GMT"}, {"version": "v3", "created": "Sun, 22 Jan 2017 18:38:57 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["de Oliveira", "Luke", ""], ["Kagan", "Michael", ""], ["Mackey", "Lester", ""], ["Nachman", "Benjamin", ""], ["Schwartzman", "Ariel", ""]]}, {"id": "1511.05191", "submitter": "Mahdi Pakdaman Naeini", "authors": "Mahdi Pakdaman Naeini, Gregory F. Cooper", "title": "Binary Classifier Calibration using an Ensemble of Near Isotonic\n  Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning accurate probabilistic models from data is crucial in many practical\ntasks in data mining. In this paper we present a new non-parametric calibration\nmethod called \\textit{ensemble of near isotonic regression} (ENIR). The method\ncan be considered as an extension of BBQ, a recently proposed calibration\nmethod, as well as the commonly used calibration method based on isotonic\nregression. ENIR is designed to address the key limitation of isotonic\nregression which is the monotonicity assumption of the predictions. Similar to\nBBQ, the method post-processes the output of a binary classifier to obtain\ncalibrated probabilities. Thus it can be combined with many existing\nclassification models. We demonstrate the performance of ENIR on synthetic and\nreal datasets for the commonly used binary classification models. Experimental\nresults show that the method outperforms several common binary classifier\ncalibration methods. In particular on the real data, ENIR commonly performs\nstatistically significantly better than the other methods, and never worse. It\nis able to improve the calibration power of classifiers, while retaining their\ndiscrimination power. The method is also computationally tractable for large\nscale datasets, as it is $O(N \\log N)$ time, where $N$ is the number of\nsamples.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 21:46:40 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Naeini", "Mahdi Pakdaman", ""], ["Cooper", "Gregory F.", ""]]}, {"id": "1511.05202", "submitter": "Sean Welleck", "authors": "Sean J. Welleck", "title": "Efficient AUC Optimization for Information Ranking Applications", "comments": "12 pages", "journal-ref": "ECIR 2016, LNCS 9626, pp.159-170, 2016", "doi": "10.1007/978-3-319-30671-1_12", "report-no": null, "categories": "cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adequate evaluation of an information retrieval system to estimate future\nperformance is a crucial task. Area under the ROC curve (AUC) is widely used to\nevaluate the generalization of a retrieval system. However, the objective\nfunction optimized in many retrieval systems is the error rate and not the AUC\nvalue. This paper provides an efficient and effective non-linear approach to\noptimize AUC using additive regression trees, with a special emphasis on the\nuse of multi-class AUC (MAUC) because multiple relevance levels are widely used\nin many ranking applications. Compared to a conventional linear approach, the\nperformance of the non-linear approach is comparable on binary-relevance\nbenchmark datasets and is better on multi-relevance benchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 22:12:00 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2015 21:28:00 GMT"}, {"version": "v3", "created": "Sat, 23 Apr 2016 23:42:09 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Welleck", "Sean J.", ""]]}, {"id": "1511.05219", "submitter": "James Zou", "authors": "Daniel Russo and James Zou", "title": "How much does your data exploration overfit? Controlling bias via\n  information usage", "comments": "Accepted at IEEE Transactions on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern data is messy and high-dimensional, and it is often not clear a priori\nwhat are the right questions to ask. Instead, the analyst typically needs to\nuse the data to search for interesting analyses to perform and hypotheses to\ntest. This is an adaptive process, where the choice of analysis to be performed\nnext depends on the results of the previous analyses on the same data.\nUltimately, which results are reported can be heavily influenced by the data.\nIt is widely recognized that this process, even if well-intentioned, can lead\nto biases and false discoveries, contributing to the crisis of reproducibility\nin science. But while %the adaptive nature of exploration any data-exploration\nrenders standard statistical theory invalid, experience suggests that different\ntypes of exploratory analysis can lead to disparate levels of bias, and the\ndegree of bias also depends on the particulars of the data set. In this paper,\nwe propose a general information usage framework to quantify and provably bound\nthe bias and other error metrics of an arbitrary exploratory analysis. We prove\nthat our mutual information based bound is tight in natural settings, and then\nuse it to give rigorous insights into when commonly used procedures do or do\nnot lead to substantially biased estimation. Through the lens of information\nusage, we analyze the bias of specific exploration procedures such as\nfiltering, rank selection and clustering. Our general framework also naturally\nmotivates randomization techniques that provably reduces exploration bias while\npreserving the utility of the data analysis. We discuss the connections between\nour approach and related ideas from differential privacy and blinded data\nanalysis, and supplement our results with illustrative simulations.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 23:36:25 GMT"}, {"version": "v2", "created": "Thu, 6 Oct 2016 04:53:06 GMT"}, {"version": "v3", "created": "Tue, 8 Oct 2019 01:14:03 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Russo", "Daniel", ""], ["Zou", "James", ""]]}, {"id": "1511.05261", "submitter": "Zhao Kang", "authors": "Zhao Kang, Chong Peng, Qiang Cheng", "title": "Robust PCA via Nonconvex Rank Approximation", "comments": "IEEE International Conference on Data Mining", "journal-ref": null, "doi": "10.1109/ICDM.2015.15", "report-no": null, "categories": "cs.CV cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous applications in data mining and machine learning require recovering\na matrix of minimal rank. Robust principal component analysis (RPCA) is a\ngeneral framework for handling this kind of problems. Nuclear norm based convex\nsurrogate of the rank function in RPCA is widely investigated. Under certain\nassumptions, it can recover the underlying true low rank matrix with high\nprobability. However, those assumptions may not hold in real-world\napplications. Since the nuclear norm approximates the rank by adding all\nsingular values together, which is essentially a $\\ell_1$-norm of the singular\nvalues, the resulting approximation error is not trivial and thus the resulting\nmatrix estimator can be significantly biased. To seek a closer approximation\nand to alleviate the above-mentioned limitations of the nuclear norm, we\npropose a nonconvex rank approximation. This approximation to the matrix rank\nis tighter than the nuclear norm. To solve the associated nonconvex\nminimization problem, we develop an efficient augmented Lagrange multiplier\nbased optimization algorithm. Experimental results demonstrate that our method\noutperforms current state-of-the-art algorithms in both accuracy and\nefficiency.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 03:00:30 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Kang", "Zhao", ""], ["Peng", "Chong", ""], ["Cheng", "Qiang", ""]]}, {"id": "1511.05265", "submitter": "Siqi Sun", "authors": "Sheng Wang, Siqi Sun and Jinbo Xu", "title": "AUC-maximized Deep Convolutional Neural Fields for Sequence Labeling", "comments": "Under review as a conference paper at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (DCNN) has shown excellent performance in\na variety of machine learning tasks. This manuscript presents Deep\nConvolutional Neural Fields (DeepCNF), a combination of DCNN with Conditional\nRandom Field (CRF), for sequence labeling with highly imbalanced label\ndistribution. The widely-used training methods, such as maximum-likelihood and\nmaximum labelwise accuracy, do not work well on highly imbalanced data. To\nhandle this, we present a new training algorithm called maximum-AUC for\nDeepCNF. That is, we train DeepCNF by directly maximizing the empirical Area\nUnder the ROC Curve (AUC), which is an unbiased measurement for imbalanced\ndata. To fulfill this, we formulate AUC in a pairwise ranking framework,\napproximate it by a polynomial function and then apply a gradient-based\nprocedure to optimize it. We then test our AUC-maximized DeepCNF on three very\ndifferent protein sequence labeling tasks: solvent accessibility prediction,\n8-state secondary structure prediction, and disorder prediction. Our\nexperimental results confirm that maximum-AUC greatly outperforms the other two\ntraining methods on 8-state secondary structure prediction and disorder\nprediction since their label distributions are highly imbalanced and also have\nsimilar performance as the other two training methods on the solvent\naccessibility prediction problem which has three equally-distributed labels.\nFurthermore, our experimental results also show that our AUC-trained DeepCNF\nmodels greatly outperform existing popular predictors of these three tasks.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 03:21:43 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 22:45:31 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Wang", "Sheng", ""], ["Sun", "Siqi", ""], ["Xu", "Jinbo", ""]]}, {"id": "1511.05286", "submitter": "Oren Kraus", "authors": "Oren Z. Kraus, Lei Jimmy Ba, Brendan Frey", "title": "Classifying and Segmenting Microscopy Images Using Convolutional\n  Multiple Instance Learning", "comments": null, "journal-ref": "Bioinformatics (2016) 32 (12): i52-i59", "doi": "10.1093/bioinformatics/btw252", "report-no": null, "categories": "cs.CV q-bio.SC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) have achieved state of the art\nperformance on both classification and segmentation tasks. Applying CNNs to\nmicroscopy images is challenging due to the lack of datasets labeled at the\nsingle cell level. We extend the application of CNNs to microscopy image\nclassification and segmentation using multiple instance learning (MIL). We\npresent the adaptive Noisy-AND MIL pooling function, a new MIL operator that is\nrobust to outliers. Combining CNNs with MIL enables training CNNs using full\nresolution microscopy images with global labels. We base our approach on the\nsimilarity between the aggregation function used in MIL and pooling layers used\nin CNNs. We show that training MIL CNNs end-to-end outperforms several previous\nmethods on both mammalian and yeast microscopy images without requiring any\nsegmentation steps.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 06:55:58 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Kraus", "Oren Z.", ""], ["Ba", "Lei Jimmy", ""], ["Frey", "Brendan", ""]]}, {"id": "1511.05297", "submitter": "Vamsi Ithapu", "authors": "Vamsi K Ithapu, Sathya N Ravi, Vikas Singh", "title": "On the interplay of network structure and gradient convergence in deep\n  learning", "comments": "54th Allerton Conference on Communication, Control and Computing\n  2016; pgs 488-495", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The regularization and output consistency behavior of dropout and layer-wise\npretraining for learning deep networks have been fairly well studied. However,\nour understanding of how the asymptotic convergence of backpropagation in deep\narchitectures is related to the structural properties of the network and other\ndesign choices (like denoising and dropout rate) is less clear at this time. An\ninteresting question one may ask is whether the network architecture and input\ndata statistics may guide the choices of learning parameters and vice versa. In\nthis work, we explore the association between such structural, distributional\nand learnability aspects vis-\\`a-vis their interaction with parameter\nconvergence rates. We present a framework to address these questions based on\nconvergence of backpropagation for general nonconvex objectives using\nfirst-order information. This analysis suggests an interesting relationship\nbetween feature denoising and dropout. Building upon these results, we obtain a\nsetup that provides systematic guidance regarding the choice of learning\nparameters and network sizes that achieve a certain level of convergence (in\nthe optimization sense) often mediated by statistical attributes of the inputs.\nOur results are supported by a set of experimental evaluations as well as\nindependent empirical observations reported by other groups.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 07:31:56 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 21:49:44 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2016 21:47:36 GMT"}, {"version": "v4", "created": "Mon, 18 Jan 2016 20:17:03 GMT"}, {"version": "v5", "created": "Tue, 29 Mar 2016 23:16:43 GMT"}, {"version": "v6", "created": "Mon, 3 Oct 2016 16:21:39 GMT"}, {"version": "v7", "created": "Tue, 4 Oct 2016 20:56:42 GMT"}, {"version": "v8", "created": "Wed, 22 Feb 2017 17:28:01 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Ithapu", "Vamsi K", ""], ["Ravi", "Sathya N", ""], ["Singh", "Vikas", ""]]}, {"id": "1511.05309", "submitter": "Yehezkel Resheff", "authors": "Yehezkel S. Resheff, Daphna Weinshall", "title": "Optimized Linear Imputation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often in real-world datasets, especially in high dimensional data, some\nfeature values are missing. Since most data analysis and statistical methods do\nnot handle gracefully missing values, the first step in the analysis requires\nthe imputation of missing values. Indeed, there has been a long standing\ninterest in methods for the imputation of missing values as a pre-processing\nstep. One recent and effective approach, the IRMI stepwise regression\nimputation method, uses a linear regression model for each real-valued feature\non the basis of all other features in the dataset. However, the proposed\niterative formulation lacks convergence guarantee. Here we propose a closely\nrelated method, stated as a single optimization problem and a block\ncoordinate-descent solution which is guaranteed to converge to a local minimum.\nExperiments show results on both synthetic and benchmark datasets, which are\ncomparable to the results of the IRMI method whenever it converges. However,\nwhile in the set of experiments described here IRMI often does not converge,\nthe performance of our methods is shown to be markedly superior in comparison\nwith other methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 08:26:40 GMT"}, {"version": "v2", "created": "Mon, 25 Jul 2016 17:46:18 GMT"}, {"version": "v3", "created": "Wed, 7 Dec 2016 13:28:52 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Resheff", "Yehezkel S.", ""], ["Weinshall", "Daphna", ""]]}, {"id": "1511.05385", "submitter": "Ruth Misener", "authors": "Doniyor Ulmasov, Caroline Baroukh, Benoit Chachuat, Marc Peter\n  Deisenroth, Ruth Misener", "title": "Bayesian Optimization with Dimension Scheduling: Application to\n  Biological Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian Optimization (BO) is a data-efficient method for global black-box\noptimization of an expensive-to-evaluate fitness function. BO typically assumes\nthat computation cost of BO is cheap, but experiments are time consuming or\ncostly. In practice, this allows us to optimize ten or fewer critical\nparameters in up to 1,000 experiments. But experiments may be less expensive\nthan BO methods assume: In some simulation models, we may be able to conduct\nmultiple thousands of experiments in a few hours, and the computational burden\nof BO is no longer negligible compared to experimentation time. To address this\nchallenge we introduce a new Dimension Scheduling Algorithm (DSA), which\nreduces the computational burden of BO for many experiments. The key idea is\nthat DSA optimizes the fitness function only along a small set of dimensions at\neach iteration. This DSA strategy (1) reduces the necessary computation time,\n(2) finds good solutions faster than the traditional BO method, and (3) can be\nparallelized straightforwardly. We evaluate the DSA in the context of\noptimizing parameters of dynamic models of microalgae metabolism and show\nfaster convergence than traditional BO.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 13:08:10 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Ulmasov", "Doniyor", ""], ["Baroukh", "Caroline", ""], ["Chachuat", "Benoit", ""], ["Deisenroth", "Marc Peter", ""], ["Misener", "Ruth", ""]]}, {"id": "1511.05392", "submitter": "Eric Nalisnick", "authors": "Eric Nalisnick, Sachin Ravi", "title": "Learning the Dimensionality of Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method for learning word embeddings with data-dependent\ndimensionality. Our Stochastic Dimensionality Skip-Gram (SD-SG) and Stochastic\nDimensionality Continuous Bag-of-Words (SD-CBOW) are nonparametric analogs of\nMikolov et al.'s (2013) well-known 'word2vec' models. Vector dimensionality is\nmade dynamic by employing techniques used by Cote & Larochelle (2016) to define\nan RBM with an infinite number of hidden units. We show qualitatively and\nquantitatively that SD-SG and SD-CBOW are competitive with their\nfixed-dimension counterparts while providing a distribution over embedding\ndimensionalities, which offers a window into how semantics distribute across\ndimensions.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 13:28:55 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 04:43:11 GMT"}, {"version": "v3", "created": "Thu, 13 Apr 2017 17:44:37 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Nalisnick", "Eric", ""], ["Ravi", "Sachin", ""]]}, {"id": "1511.05424", "submitter": "Kristoffer Stensbo-Smidt", "authors": "Kristoffer Stensbo-Smidt, Fabian Gieseke, Christian Igel, Andrew Zirm\n  and Kim Steenstrup Pedersen", "title": "Sacrificing information for the greater good: how to select photometric\n  bands for optimal accuracy", "comments": "20 pages, 13 figures, 3 tables. v2: Significant changes to match\n  revised version, including new photo-z estimation experiment", "journal-ref": null, "doi": "10.1093/mnras/stw2476", "report-no": null, "categories": "astro-ph.IM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale surveys make huge amounts of photometric data available. Because\nof the sheer amount of objects, spectral data cannot be obtained for all of\nthem. Therefore it is important to devise techniques for reliably estimating\nphysical properties of objects from photometric information alone. These\nestimates are needed to automatically identify interesting objects worth a\nfollow-up investigation as well as to produce the required data for a\nstatistical analysis of the space covered by a survey. We argue that machine\nlearning techniques are suitable to compute these estimates accurately and\nefficiently. This study promotes a feature selection algorithm, which selects\nthe most informative magnitudes and colours for a given task of estimating\nphysical quantities from photometric data alone. Using k nearest neighbours\nregression, a well-known non-parametric machine learning method, we show that\nusing the found features significantly increases the accuracy of the\nestimations compared to using standard features and standard methods. We\nillustrate the usefulness of the approach by estimating specific star formation\nrates (sSFRs) and redshifts (photo-z's) using only the broad-band photometry\nfrom the Sloan Digital Sky Survey (SDSS). For estimating sSFRs, we demonstrate\nthat our method produces better estimates than traditional spectral energy\ndistribution (SED) fitting. For estimating photo-z's, we show that our method\nproduces more accurate photo-z's than the method employed by SDSS. The study\nhighlights the general importance of performing proper model selection to\nimprove the results of machine learning systems and how feature selection can\nprovide insights into the predictive relevance of particular input features.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 14:52:40 GMT"}, {"version": "v2", "created": "Wed, 6 Jul 2016 12:15:17 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Stensbo-Smidt", "Kristoffer", ""], ["Gieseke", "Fabian", ""], ["Igel", "Christian", ""], ["Zirm", "Andrew", ""], ["Pedersen", "Kim Steenstrup", ""]]}, {"id": "1511.05432", "submitter": "Uri Shaham", "authors": "Uri Shaham, Yutaro Yamada, and Sahand Negahban", "title": "Understanding Adversarial Training: Increasing Local Stability of Neural\n  Nets through Robust Optimization", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2018.04.027", "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework for increasing local stability of Artificial\nNeural Nets (ANNs) using Robust Optimization (RO). We achieve this through an\nalternating minimization-maximization procedure, in which the loss of the\nnetwork is minimized over perturbed examples that are generated at each\nparameter update. We show that adversarial training of ANNs is in fact\nrobustification of the network optimization, and that our proposed framework\ngeneralizes previous approaches for increasing local stability of ANNs.\nExperimental results reveal that our approach increases the robustness of the\nnetwork to existing adversarial examples, while making it harder to generate\nnew ones. Furthermore, our algorithm improves the accuracy of the network also\non the original test data.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 15:14:57 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2015 16:35:50 GMT"}, {"version": "v3", "created": "Sat, 16 Jan 2016 19:05:27 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Shaham", "Uri", ""], ["Yamada", "Yutaro", ""], ["Negahban", "Sahand", ""]]}, {"id": "1511.05440", "submitter": "Michael Mathieu", "authors": "Michael Mathieu, Camille Couprie and Yann LeCun", "title": "Deep multi-scale video prediction beyond mean square error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to predict future images from a video sequence involves the\nconstruction of an internal representation that models the image evolution\naccurately, and therefore, to some degree, its content and dynamics. This is\nwhy pixel-space video prediction may be viewed as a promising avenue for\nunsupervised feature learning. In addition, while optical flow has been a very\nstudied problem in computer vision for a long time, future frame prediction is\nrarely approached. Still, many vision applications could benefit from the\nknowledge of the next frames of videos, that does not require the complexity of\ntracking every pixel trajectories. In this work, we train a convolutional\nnetwork to generate future frames given an input sequence. To deal with the\ninherently blurry predictions obtained from the standard Mean Squared Error\n(MSE) loss function, we propose three different and complementary feature\nlearning strategies: a multi-scale architecture, an adversarial training\nmethod, and an image gradient difference loss function. We compare our\npredictions to different published results based on recurrent neural networks\non the UCF101 dataset\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 15:36:32 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 23:21:22 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2015 04:58:24 GMT"}, {"version": "v4", "created": "Thu, 7 Jan 2016 21:52:53 GMT"}, {"version": "v5", "created": "Fri, 15 Jan 2016 02:09:16 GMT"}, {"version": "v6", "created": "Fri, 26 Feb 2016 22:10:30 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Mathieu", "Michael", ""], ["Couprie", "Camille", ""], ["LeCun", "Yann", ""]]}, {"id": "1511.05464", "submitter": "Joseph  Salmon", "authors": "Igor Colin and Aur\\'elien Bellet and Joseph Salmon and St\\'ephan\n  Cl\\'emen\\c{c}on", "title": "Extending Gossip Algorithms to Distributed Estimation of U-Statistics", "comments": "to be presented at NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG cs.SY stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient and robust algorithms for decentralized estimation in networks are\nessential to many distributed systems. Whereas distributed estimation of sample\nmean statistics has been the subject of a good deal of attention, computation\nof $U$-statistics, relying on more expensive averaging over pairs of\nobservations, is a less investigated area. Yet, such data functionals are\nessential to describe global properties of a statistical population, with\nimportant examples including Area Under the Curve, empirical variance, Gini\nmean difference and within-cluster point scatter. This paper proposes new\nsynchronous and asynchronous randomized gossip algorithms which simultaneously\npropagate data across the network and maintain local estimates of the\n$U$-statistic of interest. We establish convergence rate bounds of $O(1/t)$ and\n$O(\\log t / t)$ for the synchronous and asynchronous cases respectively, where\n$t$ is the number of iterations, with explicit data and network dependent\nterms. Beyond favorable comparisons in terms of rate analysis, numerical\nexperiments provide empirical evidence the proposed algorithms surpasses the\npreviously introduced approach.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 16:49:52 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Colin", "Igor", ""], ["Bellet", "Aur\u00e9lien", ""], ["Salmon", "Joseph", ""], ["Cl\u00e9men\u00e7on", "St\u00e9phan", ""]]}, {"id": "1511.05467", "submitter": "Daniel Hern\\'andez-Lobato", "authors": "Daniel Hern\\'andez-Lobato, Jos\\'e Miguel Hern\\'andez-Lobato, Amar\n  Shah, Ryan P. Adams", "title": "Predictive Entropy Search for Multi-objective Bayesian Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present PESMO, a Bayesian method for identifying the Pareto set of\nmulti-objective optimization problems, when the functions are expensive to\nevaluate. The central idea of PESMO is to choose evaluation points so as to\nmaximally reduce the entropy of the posterior distribution over the Pareto set.\nCritically, the PESMO multi-objective acquisition function can be decomposed as\na sum of objective-specific acquisition functions, which enables the algorithm\nto be used in \\emph{decoupled} scenarios in which the objectives can be\nevaluated separately and perhaps with different costs. This decoupling\ncapability also makes it possible to identify difficult objectives that require\nmore evaluations. PESMO also offers gains in efficiency, as its cost scales\nlinearly with the number of objectives, in comparison to the exponential cost\nof other methods. We compare PESMO with other related methods for\nmulti-objective Bayesian optimization on synthetic and real-world problems. The\nresults show that PESMO produces better recommendations with a smaller number\nof evaluations of the objectives, and that a decoupled evaluation can lead to\nimprovements in performance, particularly when the number of objectives is\nlarge.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 16:59:33 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2015 19:27:30 GMT"}, {"version": "v3", "created": "Sun, 21 Feb 2016 16:25:27 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Hern\u00e1ndez-Lobato", "Daniel", ""], ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""], ["Shah", "Amar", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1511.05483", "submitter": "Johan Dahlin Mr.", "authors": "Johan Dahlin, Fredrik Lindsten, Joel Kronander and Thomas B. Sch\\\"on", "title": "Accelerating pseudo-marginal Metropolis-Hastings by correlating\n  auxiliary variables", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pseudo-marginal Metropolis-Hastings (pmMH) is a powerful method for Bayesian\ninference in models where the posterior distribution is analytical intractable\nor computationally costly to evaluate directly. It operates by introducing\nadditional auxiliary variables into the model and form an extended target\ndistribution, which then can be evaluated point-wise. In many cases, the\nstandard Metropolis-Hastings is then applied to sample from the extended target\nand the sought posterior can be obtained by marginalisation. However, in some\nimplementations this approach suffers from poor mixing as the auxiliary\nvariables are sampled from an independent proposal. We propose a modification\nto the pmMH algorithm in which a Crank-Nicolson (CN) proposal is used instead.\nThis results in that we introduce a positive correlation in the auxiliary\nvariables. We investigate how to tune the CN proposal and its impact on the\nmixing of the resulting pmMH sampler. The conclusion is that the proposed\nmodification can have a beneficial effect on both the mixing of the Markov\nchain and the computational cost for each iteration of the pmMH algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 17:35:57 GMT"}], "update_date": "2016-08-06", "authors_parsed": [["Dahlin", "Johan", ""], ["Lindsten", "Fredrik", ""], ["Kronander", "Joel", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1511.05493", "submitter": "Yujia Li", "authors": "Yujia Li, Daniel Tarlow, Marc Brockschmidt, Richard Zemel", "title": "Gated Graph Sequence Neural Networks", "comments": "Published as a conference paper in ICLR 2016. Fixed a typo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-structured data appears frequently in domains including chemistry,\nnatural language semantics, social networks, and knowledge bases. In this work,\nwe study feature learning techniques for graph-structured inputs. Our starting\npoint is previous work on Graph Neural Networks (Scarselli et al., 2009), which\nwe modify to use gated recurrent units and modern optimization techniques and\nthen extend to output sequences. The result is a flexible and broadly useful\nclass of neural network models that has favorable inductive biases relative to\npurely sequence-based models (e.g., LSTMs) when the problem is\ngraph-structured. We demonstrate the capabilities on some simple AI (bAbI) and\ngraph algorithm learning tasks. We then show it achieves state-of-the-art\nperformance on a problem from program verification, in which subgraphs need to\nbe matched to abstract data structures.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 18:10:12 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 22:03:02 GMT"}, {"version": "v3", "created": "Tue, 3 May 2016 21:55:01 GMT"}, {"version": "v4", "created": "Fri, 22 Sep 2017 21:36:00 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Li", "Yujia", ""], ["Tarlow", "Daniel", ""], ["Brockschmidt", "Marc", ""], ["Zemel", "Richard", ""]]}, {"id": "1511.05614", "submitter": "Ryan Dew", "authors": "Ryan Dew and Asim Ansari", "title": "Model-based Dashboards for Customer Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automating the customer analytics process is crucial for companies that\nmanage distinct customer bases. In such data-rich and dynamic environments,\nvisualization plays a key role in understanding events of interest. These ideas\nhave led to the popularity of analytics dashboards, yet academic research has\npaid scant attention to these managerial needs. We develop a probabilistic,\nnonparametric framework for understanding and predicting individual-level\nspending using Gaussian process priors over latent functions that describe\ncustomer spending along calendar time, interpurchase time, and customer\nlifetime dimensions. These curves form a dashboard that provides a visual\nmodel-based representation of purchasing dynamics that is easily\ncomprehensible. The model flexibly and automatically captures the form and\nduration of the impact of events that influence spend propensity, even when\nsuch events are unknown a-priori. We illustrate the use of our Gaussian Process\nPropensity Model (GPPM) on data from two popular mobile games. We show that the\nGPPM generalizes hazard and buy-till-you-die models by incorporating calendar\ntime dynamics while simultaneously accounting for recency and lifetime effects.\nIt therefore provides insights about spending propensity beyond those available\nfrom these models. Finally, we show that the GPPM outperforms these benchmarks\nboth in fitting and forecasting real and simulated spend data.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 23:19:00 GMT"}, {"version": "v2", "created": "Sat, 28 Nov 2015 22:23:37 GMT"}, {"version": "v3", "created": "Tue, 1 Mar 2016 21:27:58 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Dew", "Ryan", ""], ["Ansari", "Asim", ""]]}, {"id": "1511.05634", "submitter": "Carlo Baldassi", "authors": "Carlo Baldassi, Alessandro Ingrosso, Carlo Lucibello, Luca Saglietti\n  and Riccardo Zecchina", "title": "Local entropy as a measure for sampling solutions in Constraint\n  Satisfaction Problems", "comments": "46 pages (main text: 22), 7 figures. This is an author-created,\n  un-copyedited version of an article published in Journal of Statistical\n  Mechanics: Theory and Experiment. IOP Publishing Ltd is not responsible for\n  any errors or omissions in this version of the manuscript or any version\n  derived from it. The Version of Record is available online at\n  http://dx.doi.org/10.1088/1742-5468/2016/02/023301", "journal-ref": "J. Stat. Mech. 2016 (2) 023301", "doi": "10.1088/1742-5468/2016/02/023301", "report-no": null, "categories": "cond-mat.dis-nn stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel Entropy-driven Monte Carlo (EdMC) strategy to\nefficiently sample solutions of random Constraint Satisfaction Problems (CSPs).\nFirst, we extend a recent result that, using a large-deviation analysis, shows\nthat the geometry of the space of solutions of the Binary Perceptron Learning\nProblem (a prototypical CSP), contains regions of very high-density of\nsolutions. Despite being sub-dominant, these regions can be found by optimizing\na local entropy measure. Building on these results, we construct a fast solver\nthat relies exclusively on a local entropy estimate, and can be applied to\ngeneral CSPs. We describe its performance not only for the Perceptron Learning\nProblem but also for the random $K$-Satisfiabilty Problem (another prototypical\nCSP with a radically different structure), and show numerically that a simple\nzero-temperature Metropolis search in the smooth local entropy landscape can\nreach sub-dominant clusters of optimal solutions in a small number of steps,\nwhile standard Simulated Annealing either requires extremely long cooling\nprocedures or just fails. We also discuss how the EdMC can heuristically be\nmade even more efficient for the cases we studied.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 01:03:59 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2016 18:54:40 GMT"}], "update_date": "2016-02-26", "authors_parsed": [["Baldassi", "Carlo", ""], ["Ingrosso", "Alessandro", ""], ["Lucibello", "Carlo", ""], ["Saglietti", "Luca", ""], ["Zecchina", "Riccardo", ""]]}, {"id": "1511.05650", "submitter": "Seungjin Choi", "authors": "Juho Lee and Seungjin Choi", "title": "Tree-Guided MCMC Inference for Normalized Random Measure Mixture Models", "comments": "12 pages, 10 figures, NIPS-2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalized random measures (NRMs) provide a broad class of discrete random\nmeasures that are often used as priors for Bayesian nonparametric models.\nDirichlet process is a well-known example of NRMs. Most of posterior inference\nmethods for NRM mixture models rely on MCMC methods since they are easy to\nimplement and their convergence is well studied. However, MCMC often suffers\nfrom slow convergence when the acceptance rate is low. Tree-based inference is\nan alternative deterministic posterior inference method, where Bayesian\nhierarchical clustering (BHC) or incremental Bayesian hierarchical clustering\n(IBHC) have been developed for DP or NRM mixture (NRMM) models, respectively.\nAlthough IBHC is a promising method for posterior inference for NRMM models due\nto its efficiency and applicability to online inference, its convergence is not\nguaranteed since it uses heuristics that simply selects the best solution after\nmultiple trials are made. In this paper, we present a hybrid inference\nalgorithm for NRMM models, which combines the merits of both MCMC and IBHC.\nTrees built by IBHC outlines partitions of data, which guides\nMetropolis-Hastings procedure to employ appropriate proposals. Inheriting the\nnature of MCMC, our tree-guided MCMC (tgMCMC) is guaranteed to converge, and\nenjoys the fast convergence thanks to the effective proposals guided by trees.\nExperiments on both synthetic and real-world datasets demonstrate the benefit\nof our method.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 03:16:27 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Lee", "Juho", ""], ["Choi", "Seungjin", ""]]}, {"id": "1511.05660", "submitter": "Mehdi Korki", "authors": "H. Zayyani, M. Korki and F. Marvasti", "title": "Bayesian hypothesis testing for one bit compressed sensing with sensing\n  matrix perturbation", "comments": "2 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter proposes a low-computational Bayesian algorithm for noisy sparse\nrecovery in the context of one bit compressed sensing with sensing matrix\nperturbation. The proposed algorithm which is called BHT-MLE comprises a sparse\nsupport detector and an amplitude estimator. The support detector utilizes\nBayesian hypothesis test, while the amplitude estimator uses an ML estimator\nwhich is obtained by solving a convex optimization problem. Simulation results\nshow that BHT-MLE algorithm offers more reconstruction accuracy than that of an\nML estimator (MLE) at a low computational cost.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 05:28:26 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Zayyani", "H.", ""], ["Korki", "M.", ""], ["Marvasti", "F.", ""]]}, {"id": "1511.05680", "submitter": "Wuxuan Jiang", "authors": "Wuxuan Jiang, Cong Xie, Zhihua Zhang", "title": "Wishart Mechanism for Differentially Private Principal Components\n  Analysis", "comments": "A full version with technical proofs. Accepted to AAAI-16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new input perturbation mechanism for publishing a covariance\nmatrix to achieve $(\\epsilon,0)$-differential privacy. Our mechanism uses a\nWishart distribution to generate matrix noise. In particular, We apply this\nmechanism to principal component analysis. Our mechanism is able to keep the\npositive semi-definiteness of the published covariance matrix. Thus, our\napproach gives rise to a general publishing framework for input perturbation of\na symmetric positive semidefinite matrix. Moreover, compared with the classic\nLaplace mechanism, our method has better utility guarantee. To the best of our\nknowledge, Wishart mechanism is the best input perturbation approach for\n$(\\epsilon,0)$-differentially private PCA. We also compare our work with\nprevious exponential mechanism algorithms in the literature and provide near\noptimal bound while having more flexibility and less computational\nintractability.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 07:34:23 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 06:41:29 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Jiang", "Wuxuan", ""], ["Xie", "Cong", ""], ["Zhang", "Zhihua", ""]]}, {"id": "1511.05706", "submitter": "Pratik Jawanpuria", "authors": "Pratik Jawanpuria and Maksim Lapin and Matthias Hein and Bernt Schiele", "title": "Efficient Output Kernel Learning for Multiple Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paradigm of multi-task learning is that one can achieve better\ngeneralization by learning tasks jointly and thus exploiting the similarity\nbetween the tasks rather than learning them independently of each other. While\npreviously the relationship between tasks had to be user-defined in the form of\nan output kernel, recent approaches jointly learn the tasks and the output\nkernel. As the output kernel is a positive semidefinite matrix, the resulting\noptimization problems are not scalable in the number of tasks as an\neigendecomposition is required in each step. \\mbox{Using} the theory of\npositive semidefinite kernels we show in this paper that for a certain class of\nregularizers on the output kernel, the constraint of being positive\nsemidefinite can be dropped as it is automatically satisfied for the relaxed\nproblem. This leads to an unconstrained dual problem which can be solved\nefficiently. Experiments on several multi-task and multi-class data sets\nillustrate the efficacy of our approach in terms of computational efficiency as\nwell as generalization performance.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 09:37:54 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Jawanpuria", "Pratik", ""], ["Lapin", "Maksim", ""], ["Hein", "Matthias", ""], ["Schiele", "Bernt", ""]]}, {"id": "1511.05720", "submitter": "Jonathan Weed", "authors": "Jonathan Weed, Vianney Perchet, Philippe Rigollet", "title": "Online learning in repeated auctions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by online advertising auctions, we consider repeated Vickrey\nauctions where goods of unknown value are sold sequentially and bidders only\nlearn (potentially noisy) information about a good's value once it is\npurchased. We adopt an online learning approach with bandit feedback to model\nthis problem and derive bidding strategies for two models: stochastic and\nadversarial. In the stochastic model, the observed values of the goods are\nrandom variables centered around the true value of the good. In this case,\nlogarithmic regret is achievable when competing against well behaved\nadversaries. In the adversarial model, the goods need not be identical and we\nsimply compare our performance against that of the best fixed bid in hindsight.\nWe show that sublinear regret is also achievable in this case and prove\nmatching minimax lower bounds. To our knowledge, this is the first complete set\nof strategies for bidders participating in auctions of this type.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 10:17:33 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Weed", "Jonathan", ""], ["Perchet", "Vianney", ""], ["Rigollet", "Philippe", ""]]}, {"id": "1511.05741", "submitter": "Erwan Scornet", "authors": "G\\'erard Biau (LSTA), Erwan Scornet (LSTA)", "title": "A Random Forest Guided Tour", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The random forest algorithm, proposed by L. Breiman in 2001, has been\nextremely successful as a general-purpose classification and regression method.\nThe approach, which combines several randomized decision trees and aggregates\ntheir predictions by averaging, has shown excellent performance in settings\nwhere the number of variables is much larger than the number of observations.\nMoreover, it is versatile enough to be applied to large-scale problems, is\neasily adapted to various ad-hoc learning tasks, and returns measures of\nvariable importance. The present article reviews the most recent theoretical\nand methodological developments for random forests. Emphasis is placed on the\nmathematical forces driving the algorithm, with special attention given to the\nselection of parameters, the resampling mechanism, and variable importance\nmeasures. This review is intended to provide non-experts easy access to the\nmain ideas.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 11:34:43 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Biau", "G\u00e9rard", "", "LSTA"], ["Scornet", "Erwan", "", "LSTA"]]}, {"id": "1511.05835", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "Alternative Markov and Causal Properties for Acyclic Directed Mixed\n  Graphs", "comments": "Minor changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend Andersson-Madigan-Perlman chain graphs by (i) relaxing the\nsemidirected acyclity constraint so that only directed cycles are forbidden,\nand (ii) allowing up to two edges between any pair of nodes. We introduce\nglobal, and ordered local and pairwise Markov properties for the new models. We\nshow the equivalence of these properties for strictly positive probability\ndistributions. We also show that when the random variables are continuous, the\nnew models can be interpreted as systems of structural equations with\ncorrelated errors. This enables us to adapt Pearl's do-calculus to them.\nFinally, we describe an exact algorithm for learning the new models from\nobservational and interventional data via answer set programming.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 15:33:57 GMT"}, {"version": "v2", "created": "Sun, 3 Jan 2016 19:34:34 GMT"}, {"version": "v3", "created": "Thu, 11 Feb 2016 10:28:10 GMT"}, {"version": "v4", "created": "Fri, 19 Feb 2016 09:42:49 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}, {"id": "1511.05837", "submitter": "Stylianos Kampakis", "authors": "Stylianos Kampakis, William Thomas", "title": "Using Machine Learning to Predict the Outcome of English County twenty\n  over Cricket Matches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cricket betting is a multi-billion dollar market. Therefore, there is a\nstrong incentive for models that can predict the outcomes of games and beat the\nodds provided by bookers. The aim of this study was to investigate to what\ndegree it is possible to predict the outcome of cricket matches. The target\ncompetition was the English twenty over county cricket cup. The original\nfeatures alongside engineered features gave rise to more than 500 team and\nplayer statistics. The models were optimized firstly with team features only\nand then both team and player features. The performance of the models was\ntested over individual seasons from 2009 to 2014 having been trained over\nprevious season data in each case. The optimal model was a simple prediction\nmethod combined with complex hierarchical features and was shown to\nsignificantly outperform a gambling industry benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 15:39:18 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Kampakis", "Stylianos", ""], ["Thomas", "William", ""]]}, {"id": "1511.05864", "submitter": "Sangkyun Lee", "authors": "Sangkyun Lee, Damian Brzyski and Malgorzata Bogdan", "title": "Fast Saddle-Point Algorithm for Generalized Dantzig Selector and FDR\n  Control with the Ordered l1-Norm", "comments": "In AISTATS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a primal-dual proximal extragradient algorithm to\nsolve the generalized Dantzig selector (GDS) estimation problem, based on a new\nconvex-concave saddle-point (SP) reformulation. Our new formulation makes it\npossible to adopt recent developments in saddle-point optimization, to achieve\nthe optimal $O(1/k)$ rate of convergence. Compared to the optimal non-SP\nalgorithms, ours do not require specification of sensitive parameters that\naffect algorithm performance or solution quality. We also provide a new\nanalysis showing a possibility of local acceleration to achieve the rate of\n$O(1/k^2)$ in special cases even without strong convexity or strong smoothness.\nAs an application, we propose a GDS equipped with the ordered $\\ell_1$-norm,\nshowing its false discovery rate control properties in variable selection.\nAlgorithm performance is compared between ours and other alternatives,\nincluding the linearized ADMM, Nesterov's smoothing, Nemirovski's mirror-prox,\nand the accelerated hybrid proximal extragradient techniques.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 16:29:51 GMT"}, {"version": "v2", "created": "Tue, 19 Apr 2016 11:39:34 GMT"}, {"version": "v3", "created": "Thu, 2 Jun 2016 08:50:32 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Lee", "Sangkyun", ""], ["Brzyski", "Damian", ""], ["Bogdan", "Malgorzata", ""]]}, {"id": "1511.05897", "submitter": "Harrison Edwards", "authors": "Harrison Edwards, Amos Storkey", "title": "Censoring Representations with an Adversary", "comments": "Paper accepted to ICLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In practice, there are often explicit constraints on what representations or\ndecisions are acceptable in an application of machine learning. For example it\nmay be a legal requirement that a decision must not favour a particular group.\nAlternatively it can be that that representation of data must not have\nidentifying information. We address these two related issues by learning\nflexible representations that minimize the capability of an adversarial critic.\nThis adversary is trying to predict the relevant sensitive variable from the\nrepresentation, and so minimizing the performance of the adversary ensures\nthere is little or no information in the representation about the sensitive\nvariable. We demonstrate this adversarial approach on two problems: making\ndecisions free from discrimination and removing private information from\nimages. We formulate the adversarial model as a minimax problem, and optimize\nthat minimax objective using a stochastic gradient alternate min-max optimizer.\nWe demonstrate the ability to provide discriminant free representations for\nstandard test problems, and compare with previous state of the art methods for\nfairness, showing statistically significant improvement across most cases. The\nflexibility of this method is shown via a novel problem: removing annotations\nfrom images, from unaligned training examples of annotated and unannotated\nimages, and with no a priori knowledge of the form of annotation provided to\nthe model.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 18:06:24 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2016 15:53:45 GMT"}, {"version": "v3", "created": "Fri, 4 Mar 2016 11:01:34 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Edwards", "Harrison", ""], ["Storkey", "Amos", ""]]}, {"id": "1511.05932", "submitter": "Simon Lacoste-Julien", "authors": "Simon Lacoste-Julien and Martin Jaggi", "title": "On the Global Linear Convergence of Frank-Wolfe Optimization Variants", "comments": "Appears in: Advances in Neural Information Processing Systems 28\n  (NIPS 2015). 26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Frank-Wolfe (FW) optimization algorithm has lately re-gained popularity\nthanks in particular to its ability to nicely handle the structured constraints\nappearing in machine learning applications. However, its convergence rate is\nknown to be slow (sublinear) when the solution lies at the boundary. A simple\nless-known fix is to add the possibility to take 'away steps' during\noptimization, an operation that importantly does not require a feasibility\noracle. In this paper, we highlight and clarify several variants of the\nFrank-Wolfe optimization algorithm that have been successfully applied in\npractice: away-steps FW, pairwise FW, fully-corrective FW and Wolfe's minimum\nnorm point algorithm, and prove for the first time that they all enjoy global\nlinear convergence, under a weaker condition than strong convexity of the\nobjective. The constant in the convergence rate has an elegant interpretation\nas the product of the (classical) condition number of the function with a novel\ngeometric quantity that plays the role of a 'condition number' of the\nconstraint set. We provide pointers to where these algorithms have made a\ndifference in practice, in particular with the flow polytope, the marginal\npolytope and the base polytope for submodular optimization.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 20:24:43 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Lacoste-Julien", "Simon", ""], ["Jaggi", "Martin", ""]]}, {"id": "1511.05939", "submitter": "Oren Rippel", "authors": "Oren Rippel, Manohar Paluri, Piotr Dollar, Lubomir Bourdev", "title": "Metric Learning with Adaptive Density Discrimination", "comments": "ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance metric learning (DML) approaches learn a transformation to a\nrepresentation space where distance is in correspondence with a predefined\nnotion of similarity. While such models offer a number of compelling benefits,\nit has been difficult for these to compete with modern classification\nalgorithms in performance and even in feature extraction.\n  In this work, we propose a novel approach explicitly designed to address a\nnumber of subtle yet important issues which have stymied earlier DML\nalgorithms. It maintains an explicit model of the distributions of the\ndifferent classes in representation space. It then employs this knowledge to\nadaptively assess similarity, and achieve local discrimination by penalizing\nclass distribution overlap.\n  We demonstrate the effectiveness of this idea on several tasks. Our approach\nachieves state-of-the-art classification results on a number of fine-grained\nvisual recognition datasets, surpassing the standard softmax classifier and\noutperforming triplet loss by a relative margin of 30-40%. In terms of\ncomputational performance, it alleviates training inefficiencies in the\ntraditional triplet loss, reaching the same error in 5-30 times fewer\niterations. Beyond classification, we further validate the saliency of the\nlearnt representations via their attribute concentration and hierarchy recovery\nproperties, achieving 10-25% relative gains on the softmax classifier and\n25-50% on triplet loss in these tasks.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 20:41:05 GMT"}, {"version": "v2", "created": "Wed, 2 Mar 2016 04:52:08 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Rippel", "Oren", ""], ["Paluri", "Manohar", ""], ["Dollar", "Piotr", ""], ["Bourdev", "Lubomir", ""]]}, {"id": "1511.06014", "submitter": "Tor Lattimore", "authors": "Tor Lattimore", "title": "Regret Analysis of the Finite-Horizon Gittins Index Strategy for\n  Multi-Armed Bandits", "comments": "32 pages, to appear in COLT 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I analyse the frequentist regret of the famous Gittins index strategy for\nmulti-armed bandits with Gaussian noise and a finite horizon. Remarkably it\nturns out that this approach leads to finite-time regret guarantees comparable\nto those available for the popular UCB algorithm. Along the way I derive\nfinite-time bounds on the Gittins index that are asymptotically exact and may\nbe of independent interest. I also discuss some computational issues and\npresent experimental results suggesting that a particular version of the\nGittins index strategy is a modest improvement on existing algorithms with\nfinite-time regret guarantees such as UCB and Thompson sampling.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 22:52:26 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2016 01:31:35 GMT"}, {"version": "v3", "created": "Fri, 27 May 2016 22:07:00 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Lattimore", "Tor", ""]]}, {"id": "1511.06036", "submitter": "Masayuki Ohzeki", "authors": "Masayuki Ohzeki", "title": "Stochastic gradient method with accelerated stochastic dynamics", "comments": "12 pages, proceedings for International Meeting on High-Dimensional\n  Data Driven Science (HD3-2015)\n  (http://www.sparse-modeling.jp/HD3-2015/index_e.html)", "journal-ref": null, "doi": "10.1088/1742-6596/699/1/012019", "report-no": null, "categories": "stat.ML cond-mat.dis-nn cond-mat.stat-mech cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel technique to implement stochastic gradient\nmethods, which are beneficial for learning from large datasets, through\naccelerated stochastic dynamics. A stochastic gradient method is based on\nmini-batch learning for reducing the computational cost when the amount of data\nis large. The stochasticity of the gradient can be mitigated by the injection\nof Gaussian noise, which yields the stochastic Langevin gradient method; this\nmethod can be used for Bayesian posterior sampling. However, the performance of\nthe stochastic Langevin gradient method depends on the mixing rate of the\nstochastic dynamics. In this study, we propose violating the detailed balance\ncondition to enhance the mixing rate. Recent studies have revealed that\nviolating the detailed balance condition accelerates the convergence to a\nstationary state and reduces the correlation time between the samplings. We\nimplement this violation of the detailed balance condition in the stochastic\ngradient Langevin method and test our method for a simple model to demonstrate\nits performance.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 01:01:59 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Ohzeki", "Masayuki", ""]]}, {"id": "1511.06038", "submitter": "Yishu Miao", "authors": "Yishu Miao, Lei Yu and Phil Blunsom", "title": "Neural Variational Inference for Text Processing", "comments": "ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in neural variational inference have spawned a renaissance in\ndeep latent variable models. In this paper we introduce a generic variational\ninference framework for generative and conditional models of text. While\ntraditional variational methods derive an analytic approximation for the\nintractable distributions over latent variables, here we construct an inference\nnetwork conditioned on the discrete text input to provide the variational\ndistribution. We validate this framework on two very different text modelling\napplications, generative document modelling and supervised question answering.\nOur neural variational document model combines a continuous stochastic document\nrepresentation with a bag-of-words generative model and achieves the lowest\nreported perplexities on two standard test corpora. The neural answer selection\nmodel employs a stochastic representation layer within an attention mechanism\nto extract the semantics between a question and answer pair. On two question\nanswering benchmarks this model exceeds all previous published benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 01:23:28 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2015 14:35:48 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2016 19:49:17 GMT"}, {"version": "v4", "created": "Sat, 4 Jun 2016 06:41:58 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Miao", "Yishu", ""], ["Yu", "Lei", ""], ["Blunsom", "Phil", ""]]}, {"id": "1511.06051", "submitter": "Robert Nishihara", "authors": "Philipp Moritz, Robert Nishihara, Ion Stoica, Michael I. Jordan", "title": "SparkNet: Training Deep Networks in Spark", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep networks is a time-consuming process, with networks for object\nrecognition often requiring multiple days to train. For this reason, leveraging\nthe resources of a cluster to speed up training is an important area of work.\nHowever, widely-popular batch-processing computational frameworks like\nMapReduce and Spark were not designed to support the asynchronous and\ncommunication-intensive workloads of existing distributed deep learning\nsystems. We introduce SparkNet, a framework for training deep networks in\nSpark. Our implementation includes a convenient interface for reading data from\nSpark RDDs, a Scala interface to the Caffe deep learning framework, and a\nlightweight multi-dimensional tensor library. Using a simple parallelization\nscheme for stochastic gradient descent, SparkNet scales well with the cluster\nsize and tolerates very high-latency communication. Furthermore, it is easy to\ndeploy and use with no parameter tuning, and it is compatible with existing\nCaffe models. We quantify the dependence of the speedup obtained by SparkNet on\nthe number of machines, the communication frequency, and the cluster's\ncommunication overhead, and we benchmark our system's performance on the\nImageNet dataset.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 03:29:56 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2015 10:35:40 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2016 07:48:06 GMT"}, {"version": "v4", "created": "Sun, 28 Feb 2016 23:43:36 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Moritz", "Philipp", ""], ["Nishihara", "Robert", ""], ["Stoica", "Ion", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1511.06063", "submitter": "Nirav Bhatt", "authors": "P Satya Jayadev, Aravind Rajeswaran, Nirav P Bhatt, Ramkrishna\n  Pasumarthy", "title": "A Novel Approach for Phase Identification in Smart Grids Using Graph\n  Theory and Principal Component Analysis", "comments": "Accepted for the presentation at ACC 16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consumers with low demand, like households, are generally supplied\nsingle-phase power by connecting their service mains to one of the phases of a\ndistribution transformer. The distribution companies face the problem of\nkeeping a record of consumer connectivity to a phase due to uninformed changes\nthat happen. The exact phase connectivity information is important for the\nefficient operation and control of distribution system. We propose a new data\ndriven approach to the problem based on Principal Component Analysis (PCA) and\nits Graph Theoretic interpretations, using energy measurements in equally timed\nshort intervals, generated from smart meters. We propose an algorithm for\ninferring phase connectivity from noisy measurements. The algorithm is\ndemonstrated using simulated data for phase connectivities in distribution\nnetworks.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 05:39:16 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2016 14:31:29 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Jayadev", "P Satya", ""], ["Rajeswaran", "Aravind", ""], ["Bhatt", "Nirav P", ""], ["Pasumarthy", "Ramkrishna", ""]]}, {"id": "1511.06067", "submitter": "Cheng Tai", "authors": "Cheng Tai, Tong Xiao, Yi Zhang, Xiaogang Wang, Weinan E", "title": "Convolutional neural networks with low-rank regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large CNNs have delivered impressive performance in various computer vision\napplications. But the storage and computation requirements make it problematic\nfor deploying these models on mobile devices. Recently, tensor decompositions\nhave been used for speeding up CNNs. In this paper, we further develop the\ntensor decomposition technique. We propose a new algorithm for computing the\nlow-rank tensor decomposition for removing the redundancy in the convolution\nkernels. The algorithm finds the exact global optimizer of the decomposition\nand is more effective than iterative methods. Based on the decomposition, we\nfurther propose a new method for training low-rank constrained CNNs from\nscratch. Interestingly, while achieving a significant speedup, sometimes the\nlow-rank constrained CNNs delivers significantly better performance than their\nnon-constrained counterparts. On the CIFAR-10 dataset, the proposed low-rank\nNIN model achieves $91.31\\%$ accuracy (without data augmentation), which also\nimproves upon state-of-the-art result. We evaluated the proposed method on\nCIFAR-10 and ILSVRC12 datasets for a variety of modern CNNs, including AlexNet,\nNIN, VGG and GoogleNet with success. For example, the forward time of VGG-16 is\nreduced by half while the performance is still comparable. Empirical success\nsuggests that low-rank tensor decompositions can be a very useful tool for\nspeeding up large CNNs.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 06:13:55 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2015 23:46:17 GMT"}, {"version": "v3", "created": "Sun, 14 Feb 2016 03:46:09 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Tai", "Cheng", ""], ["Xiao", "Tong", ""], ["Zhang", "Yi", ""], ["Wang", "Xiaogang", ""], ["E", "Weinan", ""]]}, {"id": "1511.06068", "submitter": "Michael Cogswell", "authors": "Michael Cogswell, Faruk Ahmed, Ross Girshick, Larry Zitnick, Dhruv\n  Batra", "title": "Reducing Overfitting in Deep Networks by Decorrelating Representations", "comments": "12 pages, 5 figures, 5 tables, Accepted to ICLR 2016, (v4 adds\n  acknowledgements)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One major challenge in training Deep Neural Networks is preventing\noverfitting. Many techniques such as data augmentation and novel regularizers\nsuch as Dropout have been proposed to prevent overfitting without requiring a\nmassive amount of training data. In this work, we propose a new regularizer\ncalled DeCov which leads to significantly reduced overfitting (as indicated by\nthe difference between train and val performance), and better generalization.\nOur regularizer encourages diverse or non-redundant representations in Deep\nNeural Networks by minimizing the cross-covariance of hidden activations. This\nsimple intuition has been explored in a number of past works but surprisingly\nhas never been applied as a regularizer in supervised learning. Experiments\nacross a range of datasets and network architectures show that this loss always\nreduces overfitting while almost always maintaining or increasing\ngeneralization performance and often improving performance over Dropout.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 06:23:09 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2016 21:12:29 GMT"}, {"version": "v3", "created": "Mon, 29 Feb 2016 21:23:05 GMT"}, {"version": "v4", "created": "Fri, 10 Jun 2016 10:59:37 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Cogswell", "Michael", ""], ["Ahmed", "Faruk", ""], ["Girshick", "Ross", ""], ["Zitnick", "Larry", ""], ["Batra", "Dhruv", ""]]}, {"id": "1511.06114", "submitter": "Minh-Thang Luong", "authors": "Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, Lukasz\n  Kaiser", "title": "Multi-task Sequence to Sequence Learning", "comments": "10 pages, 4 figures, ICLR 2016 camera-ready, added parsing SOTA\n  results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence to sequence learning has recently emerged as a new paradigm in\nsupervised learning. To date, most of its applications focused on only one task\nand not much work explored this framework for multiple tasks. This paper\nexamines three multi-task learning (MTL) settings for sequence to sequence\nmodels: (a) the oneto-many setting - where the encoder is shared between\nseveral tasks such as machine translation and syntactic parsing, (b) the\nmany-to-one setting - useful when only the decoder can be shared, as in the\ncase of translation and image caption generation, and (c) the many-to-many\nsetting - where multiple encoders and decoders are shared, which is the case\nwith unsupervised objectives and translation. Our results show that training on\na small amount of parsing and image caption data can improve the translation\nquality between English and German by up to 1.5 BLEU points over strong\nsingle-task baselines on the WMT benchmarks. Furthermore, we have established a\nnew state-of-the-art result in constituent parsing with 93.0 F1. Lastly, we\nreveal interesting properties of the two unsupervised learning objectives,\nautoencoder and skip-thought, in the MTL context: autoencoder helps less in\nterms of perplexities but more on BLEU scores compared to skip-thought.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 10:24:14 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2016 06:46:29 GMT"}, {"version": "v3", "created": "Wed, 10 Feb 2016 08:10:59 GMT"}, {"version": "v4", "created": "Tue, 1 Mar 2016 10:55:58 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Luong", "Minh-Thang", ""], ["Le", "Quoc V.", ""], ["Sutskever", "Ilya", ""], ["Vinyals", "Oriol", ""], ["Kaiser", "Lukasz", ""]]}, {"id": "1511.06120", "submitter": "Emanuele Olivetti", "authors": "Emanuele Olivetti, Sandro Vega-Pons and Paolo Avesani", "title": "The Kernel Two-Sample Test for Brain Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In clinical and neuroscientific studies, systematic differences between two\npopulations of brain networks are investigated in order to characterize mental\ndiseases or processes. Those networks are usually represented as graphs built\nfrom neuroimaging data and studied by means of graph analysis methods. The\ntypical machine learning approach to study these brain graphs creates a\nclassifier and tests its ability to discriminate the two populations. In\ncontrast to this approach, in this work we propose to directly test whether two\npopulations of graphs are different or not, by using the kernel two-sample test\n(KTST), without creating the intermediate classifier. We claim that, in\ngeneral, the two approaches provides similar results and that the KTST requires\nmuch less computation. Additionally, in the regime of low sample size, we claim\nthat the KTST has lower frequency of Type II error than the classification\napproach. Besides providing algorithmic considerations to support these claims,\nwe show strong evidence through experiments and one simulation.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 11:01:54 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Olivetti", "Emanuele", ""], ["Vega-Pons", "Sandro", ""], ["Avesani", "Paolo", ""]]}, {"id": "1511.06198", "submitter": "Kai Zhang", "authors": "Kai Zhang", "title": "Spherical Cap Packing Asymptotics and Rank-Extreme Detection", "comments": "14 pages; 1 figure. Accepted Jan 31, 2017 by IEEE Transactions on\n  Information Theory", "journal-ref": null, "doi": "10.1109/TIT.2017.2700202", "report-no": null, "categories": "math.ST cs.IT math.IT physics.data-an stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the spherical cap packing problem with a probabilistic approach.\nSuch probabilistic considerations result in an asymptotic sharp universal\nuniform bound on the maximal inner product between any set of unit vectors and\na stochastically independent uniformly distributed unit vector. When the set of\nunit vectors are themselves independently uniformly distributed, we further\ndevelop the extreme value distribution limit of the maximal inner product,\nwhich characterizes its uncertainty around the bound.\n  As applications of the above asymptotic results, we derive (1) an asymptotic\nsharp universal uniform bound on the maximal spurious correlation, as well as\nits uniform convergence in distribution when the explanatory variables are\nindependently Gaussian distributed; and (2) an asymptotic sharp universal bound\non the maximum norm of a low-rank elliptically distributed vector, as well as\nrelated limiting distributions. With these results, we develop a fast detection\nmethod for a low-rank structure in high-dimensional Gaussian data without using\nthe spectrum information.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 15:08:10 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 23:52:16 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Zhang", "Kai", ""]]}, {"id": "1511.06201", "submitter": "Zhirong Wu", "authors": "Zhirong Wu, Dahua Lin, Xiaoou Tang", "title": "Adjustable Bounded Rectifiers: Towards Deep Binary Representations", "comments": "Under review as a conference paper at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary representation is desirable for its memory efficiency, computation\nspeed and robustness. In this paper, we propose adjustable bounded rectifiers\nto learn binary representations for deep neural networks. While hard\nconstraining representations across layers to be binary makes training\nunreasonably difficult, we softly encourage activations to diverge from real\nvalues to binary by approximating step functions. Our final representation is\ncompletely binary. We test our approach on MNIST, CIFAR10, and ILSVRC2012\ndataset, and systematically study the training dynamics of the binarization\nprocess. Our approach can binarize the last layer representation without loss\nof performance and binarize all the layers with reasonably small degradations.\nThe memory space that it saves may allow more sophisticated models to be\ndeployed, thus compensating the loss. To the best of our knowledge, this is the\nfirst work to report results on current deep network architectures using\ncomplete binary middle representations. Given the learned representations, we\nfind that the firing or inhibition of a binary neuron is usually associated\nwith a meaningful interpretation across different classes. This suggests that\nthe semantic structure of a neural network may be manifested through a guided\nbinarization process.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 15:14:02 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Wu", "Zhirong", ""], ["Lin", "Dahua", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1511.06208", "submitter": "Moshe Salhov", "authors": "Moshe Salhov and Amit Bermanis and Guy Wolf and Amir Averbuch", "title": "Diffusion Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion Maps framework is a kernel based method for manifold learning and\ndata analysis that defines diffusion similarities by imposing a Markovian\nprocess on the given dataset. Analysis by this process uncovers the intrinsic\ngeometric structures in the data. Recently, it was suggested to replace the\nstandard kernel by a measure-based kernel that incorporates information about\nthe density of the data. Thus, the manifold assumption is replaced by a more\ngeneral measure-based assumption.\n  The measure-based diffusion kernel incorporates two separate independent\nrepresentations. The first determines a measure that correlates with a density\nthat represents normal behaviors and patterns in the data. The second consists\nof the analyzed multidimensional data points.\n  In this paper, we present a representation framework for data analysis of\ndatasets that is based on a closed-form decomposition of the measure-based\nkernel. The proposed representation preserves pairwise diffusion distances that\ndoes not depend on the data size while being invariant to scale. For a\nstationary data, no out-of-sample extension is needed for embedding newly\narrived data points in the representation space. Several aspects of the\npresented methodology are demonstrated on analytically generated data.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 15:30:39 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Salhov", "Moshe", ""], ["Bermanis", "Amit", ""], ["Wolf", "Guy", ""], ["Averbuch", "Amir", ""]]}, {"id": "1511.06238", "submitter": "Miriam Cha", "authors": "Miriam Cha, Youngjune Gwon, H.T. Kung", "title": "Multimodal sparse representation learning and applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised methods have proven effective for discriminative tasks in a\nsingle-modality scenario. In this paper, we present a multimodal framework for\nlearning sparse representations that can capture semantic correlation between\nmodalities. The framework can model relationships at a higher level by forcing\nthe shared sparse representation. In particular, we propose the use of joint\ndictionary learning technique for sparse coding and formulate the joint\nrepresentation for concision, cross-modal representations (in case of a missing\nmodality), and union of the cross-modal representations. Given the accelerated\ngrowth of multimodal data posted on the Web such as YouTube, Wikipedia, and\nTwitter, learning good multimodal features is becoming increasingly important.\nWe show that the shared representations enabled by our framework substantially\nimprove the classification performance under both unimodal and multimodal\nsettings. We further show how deep architectures built on the proposed\nframework are effective for the case of highly nonlinear correlations between\nmodalities. The effectiveness of our approach is demonstrated experimentally in\nimage denoising, multimedia event detection and retrieval on the TRECVID\ndataset (audio-video), category classification on the Wikipedia dataset\n(image-text), and sentiment classification on PhotoTweet (image-text).\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 16:26:24 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2016 23:18:09 GMT"}, {"version": "v3", "created": "Wed, 2 Mar 2016 19:22:48 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Cha", "Miriam", ""], ["Gwon", "Youngjune", ""], ["Kung", "H. T.", ""]]}, {"id": "1511.06247", "submitter": "Armando Vieira", "authors": "Armando Vieira", "title": "Predicting online user behaviour using deep learning algorithms", "comments": "21 pages, 3 figures. arXiv admin note: text overlap with\n  arXiv:1412.6601, arXiv:1406.1231, arXiv:1508.03856 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a robust classifier to predict buying intentions based on user\nbehaviour within a large e-commerce website. In this work we compare\ntraditional machine learning techniques with the most advanced deep learning\napproaches. We show that both Deep Belief Networks and Stacked Denoising\nauto-Encoders achieved a substantial improvement by extracting features from\nhigh dimensional data during the pre-train phase. They prove also to be more\nconvenient to deal with severe class imbalance.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 16:47:00 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2015 18:53:45 GMT"}, {"version": "v3", "created": "Thu, 26 May 2016 11:53:55 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Vieira", "Armando", ""]]}, {"id": "1511.06251", "submitter": "Qianxiao Li", "authors": "Qianxiao Li, Cheng Tai, Weinan E", "title": "Stochastic modified equations and adaptive stochastic gradient\n  algorithms", "comments": "Major changes including a proof of the weak approximation, asymptotic\n  expansions and application-oriented adaptive algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop the method of stochastic modified equations (SME), in which\nstochastic gradient algorithms are approximated in the weak sense by\ncontinuous-time stochastic differential equations. We exploit the continuous\nformulation together with optimal control theory to derive novel adaptive\nhyper-parameter adjustment policies. Our algorithms have competitive\nperformance with the added benefit of being robust to varying models and\ndatasets. This provides a general methodology for the analysis and design of\nstochastic gradient algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 16:49:33 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 19:58:15 GMT"}, {"version": "v3", "created": "Tue, 20 Jun 2017 13:56:33 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Li", "Qianxiao", ""], ["Tai", "Cheng", ""], ["E", "Weinan", ""]]}, {"id": "1511.06285", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k, Emilia Rejmund, Krzysztof Marasek", "title": "Harvesting comparable corpora and mining them for equivalent bilingual\n  sentences using statistical classification and analogy- based heuristics", "comments": "Springer p. 433-441, 2015", "journal-ref": null, "doi": "10.1007/978-3-319-25252-0_46", "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel sentences are a relatively scarce but extremely useful resource for\nmany applications including cross-lingual retrieval and statistical machine\ntranslation. This research explores our new methodologies for mining such data\nfrom previously obtained comparable corpora. The task is highly practical since\nnon-parallel multilingual data exist in far greater quantities than parallel\ncorpora, but parallel sentences are a much more useful resource. Here we\npropose a web crawling method for building subject-aligned comparable corpora\nfrom e.g. Wikipedia dumps and Euronews web page. The improvements in machine\ntranslation are shown on Polish-English language pair for various text domains.\nWe also tested another method of building parallel corpora based on comparable\ncorpora data. It lets automatically broad existing corpus of sentences from\nsubject of corpora based on analogies between them.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 15:26:06 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""], ["Rejmund", "Emilia", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1511.06321", "submitter": "Yen-Chang Hsu", "authors": "Yen-Chang Hsu, Zsolt Kira", "title": "Neural network-based clustering using pairwise constraints", "comments": "ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a neural network-based end-to-end clustering framework.\nWe design a novel strategy to utilize the contrastive criteria for pushing\ndata-forming clusters directly from raw data, in addition to learning a feature\nembedding suitable for such clustering. The network is trained with weak\nlabels, specifically partial pairwise relationships between data instances. The\ncluster assignments and their probabilities are then obtained at the output\nlayer by feed-forwarding the data. The framework has the interesting\ncharacteristic that no cluster centers need to be explicitly specified, thus\nthe resulting cluster distribution is purely data-driven and no distance\nmetrics need to be predefined. The experiments show that the proposed approach\nbeats the conventional two-stage method (feature embedding with k-means) by a\nsignificant margin. It also compares favorably to the performance of the\nstandard cross entropy loss for classification. Robustness analysis also shows\nthat the method is largely insensitive to the number of clusters. Specifically,\nwe show that the number of dominant clusters is close to the true number of\nclusters even when a large k is used for clustering.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 19:36:38 GMT"}, {"version": "v2", "created": "Sat, 19 Dec 2015 17:24:34 GMT"}, {"version": "v3", "created": "Thu, 31 Dec 2015 18:53:46 GMT"}, {"version": "v4", "created": "Tue, 5 Jan 2016 23:46:17 GMT"}, {"version": "v5", "created": "Tue, 26 Apr 2016 15:59:39 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Hsu", "Yen-Chang", ""], ["Kira", "Zsolt", ""]]}, {"id": "1511.06340", "submitter": "Yanwei Fu", "authors": "Yanwei Fu and De-An Huang and Leonid Sigal", "title": "Robust Classification by Pre-conditioned LASSO and Transductive\n  Diffusion Component Analysis", "comments": "we will significantly change the content of this paper which makes it\n  another paper. In order not to misleading, we decided to withdraw it. The\n  updated version can not be shared currently, for some reason. We will update\n  it once it is OK to be shared", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern machine learning-based recognition approaches require large-scale\ndatasets with large number of labelled training images. However, such datasets\nare inherently difficult and costly to collect and annotate. Hence there is a\ngreat and growing interest in automatic dataset collection methods that can\nleverage the web. % which are collected % in a cheap, efficient and yet\nunreliable way. Collecting datasets in this way, however, requires robust and\nefficient ways for detecting and excluding outliers that are common and\nprevalent. % Outliers are thus a % prominent treat of using these dataset. So\nfar, there have been a limited effort in machine learning community to directly\ndetect outliers for robust classification. Inspired by the recent work on\nPre-conditioned LASSO, this paper formulates the outlier detection task using\nPre-conditioned LASSO and employs \\red{unsupervised} transductive diffusion\ncomponent analysis to both integrate the topological structure of the data\nmanifold, from labeled and unlabeled instances, and reduce the feature\ndimensionality. Synthetic experiments as well as results on two real-world\nclassification tasks show that our framework can robustly detect the outliers\nand improve classification.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 20:13:51 GMT"}, {"version": "v2", "created": "Wed, 25 Dec 2019 02:06:46 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Fu", "Yanwei", ""], ["Huang", "De-An", ""], ["Sigal", "Leonid", ""]]}, {"id": "1511.06350", "submitter": "David Belanger", "authors": "David Belanger, Andrew McCallum", "title": "Structured Prediction Energy Networks", "comments": "ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce structured prediction energy networks (SPENs), a flexible\nframework for structured prediction. A deep architecture is used to define an\nenergy function of candidate labels, and then predictions are produced by using\nback-propagation to iteratively optimize the energy with respect to the labels.\nThis deep architecture captures dependencies between labels that would lead to\nintractable graphical models, and performs structure learning by automatically\nlearning discriminative features of the structured output. One natural\napplication of our technique is multi-label classification, which traditionally\nhas required strict prior assumptions about the interactions between labels to\nensure tractable learning and prediction. We are able to apply SPENs to\nmulti-label problems with substantially larger label sets than previous\napplications of structured prediction, while modeling high-order interactions\nusing minimal structural assumptions. Overall, deep learning provides\nremarkable tools for learning features of the inputs to a prediction problem,\nand this work extends these techniques to learning features of structured\noutputs. Our experiments provide impressive performance on a variety of\nbenchmark multi-label classification tasks, demonstrate that our technique can\nbe used to provide interpretable structure learning, and illuminate fundamental\ntrade-offs between feed-forward and iterative structured prediction.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 20:39:59 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2016 16:28:36 GMT"}, {"version": "v3", "created": "Thu, 23 Jun 2016 20:21:11 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Belanger", "David", ""], ["McCallum", "Andrew", ""]]}, {"id": "1511.06382", "submitter": "R Devon Hjelm", "authors": "R Devon Hjelm and Kyunghyun Cho and Junyoung Chung and Russ\n  Salakhutdinov and Vince Calhoun and Nebojsa Jojic", "title": "Iterative Refinement of the Approximate Posterior for Directed Belief\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational methods that rely on a recognition network to approximate the\nposterior of directed graphical models offer better inference and learning than\nprevious methods. Recent advances that exploit the capacity and flexibility in\nthis approach have expanded what kinds of models can be trained. However, as a\nproposal for the posterior, the capacity of the recognition network is limited,\nwhich can constrain the representational power of the generative model and\nincrease the variance of Monte Carlo estimates. To address these issues, we\nintroduce an iterative refinement procedure for improving the approximate\nposterior of the recognition network and show that training with the refined\nposterior is competitive with state-of-the-art methods. The advantages of\nrefinement are further evident in an increased effective sample size, which\nimplies a lower variance of gradient estimates.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 21:11:12 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2015 21:40:50 GMT"}, {"version": "v3", "created": "Sun, 3 Jan 2016 05:05:30 GMT"}, {"version": "v4", "created": "Mon, 14 Mar 2016 16:56:38 GMT"}, {"version": "v5", "created": "Sat, 29 Oct 2016 05:10:31 GMT"}, {"version": "v6", "created": "Tue, 20 Feb 2018 16:02:50 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Hjelm", "R Devon", ""], ["Cho", "Kyunghyun", ""], ["Chung", "Junyoung", ""], ["Salakhutdinov", "Russ", ""], ["Calhoun", "Vince", ""], ["Jojic", "Nebojsa", ""]]}, {"id": "1511.06385", "submitter": "Chunchuan Lv Mr.", "authors": "Chunchuan Lyu, Kaizhu Huang, Hai-Ning Liang", "title": "A Unified Gradient Regularization Family for Adversarial Examples", "comments": "The paper has been presented at ICDM 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples are augmented data points generated by imperceptible\nperturbation of input samples. They have recently drawn much attention with the\nmachine learning and data mining community. Being difficult to distinguish from\nreal examples, such adversarial examples could change the prediction of many of\nthe best learning models including the state-of-the-art deep learning models.\nRecent attempts have been made to build robust models that take into account\nadversarial examples. However, these methods can either lead to performance\ndrops or lack mathematical motivations. In this paper, we propose a unified\nframework to build robust machine learning models against adversarial examples.\nMore specifically, using the unified framework, we develop a family of gradient\nregularization methods that effectively penalize the gradient of loss function\nw.r.t. inputs. Our proposed framework is appealing in that it offers a unified\nview to deal with adversarial examples. It incorporates another\nrecently-proposed perturbation based approach as a special case. In addition,\nwe present some visual effects that reveals semantic meaning in those\nperturbations, and thus support our regularization method and provide another\nexplanation for generalizability of adversarial examples. By applying this\ntechnique to Maxout networks, we conduct a series of experiments and achieve\nencouraging results on two benchmark datasets. In particular,we attain the best\naccuracy on MNIST data (without data augmentation) and competitive performance\non CIFAR-10 data.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 21:14:43 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Lyu", "Chunchuan", ""], ["Huang", "Kaizhu", ""], ["Liang", "Hai-Ning", ""]]}, {"id": "1511.06390", "submitter": "Jost Tobias Springenberg", "authors": "Jost Tobias Springenberg", "title": "Unsupervised and Semi-supervised Learning with Categorical Generative\n  Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a method for learning a discriminative classifier\nfrom unlabeled or partially labeled data. Our approach is based on an objective\nfunction that trades-off mutual information between observed examples and their\npredicted categorical class distribution, against robustness of the classifier\nto an adversarial generative model. The resulting algorithm can either be\ninterpreted as a natural generalization of the generative adversarial networks\n(GAN) framework or as an extension of the regularized information maximization\n(RIM) framework to robust classification against an optimal adversary. We\nempirically evaluate our method - which we dub categorical generative\nadversarial networks (or CatGAN) - on synthetic data as well as on challenging\nimage classification tasks, demonstrating the robustness of the learned\nclassifiers. We further qualitatively assess the fidelity of samples generated\nby the adversarial generator that is learned alongside the discriminative\nclassifier, and identify links between the CatGAN objective and discriminative\nclustering algorithms (such as RIM).\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 21:26:58 GMT"}, {"version": "v2", "created": "Sat, 30 Apr 2016 21:23:46 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Springenberg", "Jost Tobias", ""]]}, {"id": "1511.06391", "submitter": "Oriol Vinyals", "authors": "Oriol Vinyals, Samy Bengio, Manjunath Kudlur", "title": "Order Matters: Sequence to sequence for sets", "comments": "Accepted as a conference paper at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequences have become first class citizens in supervised learning thanks to\nthe resurgence of recurrent neural networks. Many complex tasks that require\nmapping from or to a sequence of observations can now be formulated with the\nsequence-to-sequence (seq2seq) framework which employs the chain rule to\nefficiently represent the joint probability of sequences. In many cases,\nhowever, variable sized inputs and/or outputs might not be naturally expressed\nas sequences. For instance, it is not clear how to input a set of numbers into\na model where the task is to sort them; similarly, we do not know how to\norganize outputs when they correspond to random variables and the task is to\nmodel their unknown joint probability. In this paper, we first show using\nvarious examples that the order in which we organize input and/or output data\nmatters significantly when learning an underlying model. We then discuss an\nextension of the seq2seq framework that goes beyond sequences and handles input\nsets in a principled way. In addition, we propose a loss which, by searching\nover possible orders during training, deals with the lack of structure of\noutput sets. We show empirical evidence of our claims regarding ordering, and\non the modifications to the seq2seq framework on benchmark language modeling\nand parsing tasks, as well as two artificial tasks -- sorting numbers and\nestimating the joint probability of unknown graphical models.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 21:31:26 GMT"}, {"version": "v2", "created": "Sat, 16 Jan 2016 16:50:35 GMT"}, {"version": "v3", "created": "Tue, 9 Feb 2016 17:03:38 GMT"}, {"version": "v4", "created": "Tue, 23 Feb 2016 22:25:12 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Vinyals", "Oriol", ""], ["Bengio", "Samy", ""], ["Kudlur", "Manjunath", ""]]}, {"id": "1511.06416", "submitter": "Daniel Seita", "authors": "Daniel Seita, Haoyu Chen, and John Canny", "title": "Fast Parallel SAME Gibbs Sampling on General Discrete Bayesian Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental task in machine learning and related fields is to perform\ninference on Bayesian networks. Since exact inference takes exponential time in\ngeneral, a variety of approximate methods are used. Gibbs sampling is one of\nthe most accurate approaches and provides unbiased samples from the posterior\nbut it has historically been too expensive for large models. In this paper, we\npresent an optimized, parallel Gibbs sampler augmented with state replication\n(SAME or State Augmented Marginal Estimation) to decrease convergence time. We\nfind that SAME can improve the quality of parameter estimates while\naccelerating convergence. Experiments on both synthetic and real data show that\nour Gibbs sampler is substantially faster than the state of the art sampler,\nJAGS, without sacrificing accuracy. Our ultimate objective is to introduce the\nGibbs sampler to researchers in many fields to expand their range of feasible\ninference problems.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 22:08:22 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Seita", "Daniel", ""], ["Chen", "Haoyu", ""], ["Canny", "John", ""]]}, {"id": "1511.06419", "submitter": "Maria De-Arteaga", "authors": "Maria De-Arteaga, Artur Dubrawski, Peter Huggins", "title": "Canonical Autocorrelation Analysis", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an extension of sparse Canonical Correlation Analysis (CCA)\ndesigned for finding multiple-to-multiple linear correlations within a single\nset of variables. Unlike CCA, which finds correlations between two sets of data\nwhere the rows are matched exactly but the columns represent separate sets of\nvariables, the method proposed here, Canonical Autocorrelation Analysis (CAA),\nfinds multivariate correlations within just one set of variables. This can be\nuseful when we look for hidden parsimonious structures in data, each involving\nonly a small subset of all features. In addition, the discovered correlations\nare highly interpretable as they are formed by pairs of sparse linear\ncombinations of the original features. We show how CAA can be of use as a tool\nfor anomaly detection when the expected structure of correlations is not\nfollowed by anomalous data. We illustrate the utility of CAA in two application\ndomains where single-class and unsupervised learning of correlation structures\nare particularly relevant: breast cancer diagnosis and radiation threat\ndetection. When applied to the Wisconsin Breast Cancer data, single-class CAA\nis competitive with supervised methods used in literature. On the radiation\nthreat detection task, unsupervised CAA performs significantly better than an\nunsupervised alternative prevalent in the domain, while providing valuable\nadditional insights for threat analysis.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 22:13:43 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["De-Arteaga", "Maria", ""], ["Dubrawski", "Artur", ""], ["Huggins", "Peter", ""]]}, {"id": "1511.06421", "submitter": "Jacob Gardner", "authors": "Jacob R. Gardner, Paul Upchurch, Matt J. Kusner, Yixuan Li, Kilian Q.\n  Weinberger, Kavita Bala, John E. Hopcroft", "title": "Deep Manifold Traversal: Changing Labels with Convolutional Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tasks in computer vision can be cast as a \"label changing\" problem,\nwhere the goal is to make a semantic change to the appearance of an image or\nsome subject in an image in order to alter the class membership. Although\nsuccessful task-specific methods have been developed for some label changing\napplications, to date no general purpose method exists. Motivated by this we\npropose deep manifold traversal, a method that addresses the problem in its\nmost general form: it first approximates the manifold of natural images then\nmorphs a test image along a traversal path away from a source class and towards\na target class while staying near the manifold throughout. The resulting\nalgorithm is surprisingly effective and versatile. It is completely data\ndriven, requiring only an example set of images from the desired source and\ntarget domains. We demonstrate deep manifold traversal on highly diverse label\nchanging tasks: changing an individual's appearance (age and hair color),\nchanging the season of an outdoor image, and transforming a city skyline\ntowards nighttime.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 22:17:20 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2016 01:37:02 GMT"}, {"version": "v3", "created": "Thu, 17 Mar 2016 17:57:55 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Gardner", "Jacob R.", ""], ["Upchurch", "Paul", ""], ["Kusner", "Matt J.", ""], ["Li", "Yixuan", ""], ["Weinberger", "Kilian Q.", ""], ["Bala", "Kavita", ""], ["Hopcroft", "John E.", ""]]}, {"id": "1511.06423", "submitter": "Ziyuan Lin", "authors": "Ziyuan Lin and Jaakko Peltonen", "title": "An Information Retrieval Approach to Finding Dependent Subspaces of\n  Multiple Views", "comments": "9 pages, 15 figures. Submitted for ICLR 2016; the authors contributed\n  equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding relationships between multiple views of data is essential both for\nexploratory analysis and as pre-processing for predictive tasks. A prominent\napproach is to apply variants of Canonical Correlation Analysis (CCA), a\nclassical method seeking correlated components between views. The basic CCA is\nrestricted to maximizing a simple dependency criterion, correlation, measured\ndirectly between data coordinates. We introduce a new method that finds\ndependent subspaces of views directly optimized for the data analysis task of\n\\textit{neighbor retrieval between multiple views}. We optimize mappings for\neach view such as linear transformations to maximize cross-view similarity\nbetween neighborhoods of data samples. The criterion arises directly from the\nwell-defined retrieval task, detects nonlinear and local similarities, is able\nto measure dependency of data relationships rather than only individual data\ncoordinates, and is related to well understood measures of information\nretrieval quality. In experiments we show the proposed method outperforms\nalternatives in preserving cross-view neighborhood similarities, and yields\ninsights into local dependencies between multiple views.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 22:20:34 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2016 23:09:07 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Lin", "Ziyuan", ""], ["Peltonen", "Jaakko", ""]]}, {"id": "1511.06429", "submitter": "Sebastian H\\\"ofer", "authors": "Rico Jonschkowski, Sebastian H\\\"ofer, Oliver Brock", "title": "Patterns for Learning with Side Information", "comments": "The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised, semi-supervised, and unsupervised learning estimate a function\ngiven input/output samples. Generalization of the learned function to unseen\ndata can be improved by incorporating side information into learning. Side\ninformation are data that are neither from the input space nor from the output\nspace of the function, but include useful information for learning it. In this\npaper we show that learning with side information subsumes a variety of related\napproaches, e.g. multi-task learning, multi-view learning and learning using\nprivileged information. Our main contributions are (i) a new perspective that\nconnects these previously isolated approaches, (ii) insights about how these\nmethods incorporate different types of prior knowledge, and hence implement\ndifferent patterns, (iii) facilitating the application of these methods in\nnovel tasks, as well as (iv) a systematic experimental evaluation of these\npatterns in two supervised learning tasks.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 22:39:35 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 06:35:18 GMT"}, {"version": "v3", "created": "Mon, 7 Dec 2015 12:38:26 GMT"}, {"version": "v4", "created": "Thu, 7 Jan 2016 16:45:52 GMT"}, {"version": "v5", "created": "Wed, 10 Feb 2016 11:57:18 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Jonschkowski", "Rico", ""], ["H\u00f6fer", "Sebastian", ""], ["Brock", "Oliver", ""]]}, {"id": "1511.06442", "submitter": "Henry Gouk", "authors": "Henry Gouk, Bernhard Pfahringer, Michael Cree", "title": "Fast Metric Learning For Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity metrics are a core component of many information retrieval and\nmachine learning systems. In this work we propose a method capable of learning\na similarity metric from data equipped with a binary relation. By considering\nonly the similarity constraints, and initially ignoring the features, we are\nable to learn target vectors for each instance using one of several\nappropriately designed loss functions. A regression model can then be\nconstructed that maps novel feature vectors to the same target vector space,\nresulting in a feature extractor that computes vectors for which a predefined\nmetric is a meaningful measure of similarity. We present results on both\nmulticlass and multi-label classification datasets that demonstrate\nconsiderably faster convergence, as well as higher accuracy on the majority of\nthe intrinsic evaluation tasks and all extrinsic evaluation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 23:10:00 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 06:05:30 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2015 15:27:11 GMT"}, {"version": "v4", "created": "Wed, 17 Feb 2016 02:11:00 GMT"}, {"version": "v5", "created": "Tue, 5 Apr 2016 07:29:48 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Gouk", "Henry", ""], ["Pfahringer", "Bernhard", ""], ["Cree", "Michael", ""]]}, {"id": "1511.06443", "submitter": "Daniel Roy", "authors": "Gintare Karolina Dziugaite and Daniel M. Roy", "title": "Neural Network Matrix Factorization", "comments": "Minor modifications to notation. Added additional experiments and\n  discussion. 7 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data often comes in the form of an array or matrix. Matrix factorization\ntechniques attempt to recover missing or corrupted entries by assuming that the\nmatrix can be written as the product of two low-rank matrices. In other words,\nmatrix factorization approximates the entries of the matrix by a simple, fixed\nfunction---namely, the inner product---acting on the latent feature vectors for\nthe corresponding row and column. Here we consider replacing the inner product\nby an arbitrary function that we learn from the data at the same time as we\nlearn the latent feature vectors. In particular, we replace the inner product\nby a multi-layer feed-forward neural network, and learn by alternating between\noptimizing the network for fixed latent features, and optimizing the latent\nfeatures for a fixed network. The resulting approach---which we call neural\nnetwork matrix factorization or NNMF, for short---dominates standard low-rank\ntechniques on a suite of benchmark but is dominated by some recent proposals\nthat take advantage of the graph features. Given the vast range of\narchitectures, activation functions, regularizers, and optimization techniques\nthat could be used within the NNMF framework, it seems likely the true\npotential of the approach has yet to be reached.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 23:13:29 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2015 04:29:39 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Dziugaite", "Gintare Karolina", ""], ["Roy", "Daniel M.", ""]]}, {"id": "1511.06455", "submitter": "Zhenwen Dai", "authors": "Zhenwen Dai, Andreas Damianou, Javier Gonz\\'alez, Neil Lawrence", "title": "Variational Auto-encoded Deep Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a scalable deep non-parametric generative model by augmenting deep\nGaussian processes with a recognition model. Inference is performed in a novel\nscalable variational framework where the variational posterior distributions\nare reparametrized through a multilayer perceptron. The key aspect of this\nreformulation is that it prevents the proliferation of variational parameters\nwhich otherwise grow linearly in proportion to the sample size. We derive a new\nformulation of the variational lower bound that allows us to distribute most of\nthe computation in a way that enables to handle datasets of the size of\nmainstream deep learning tasks. We show the efficacy of the method on a variety\nof challenges including deep unsupervised learning and deep Bayesian\noptimization.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 23:47:34 GMT"}, {"version": "v2", "created": "Mon, 29 Feb 2016 21:34:58 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Dai", "Zhenwen", ""], ["Damianou", "Andreas", ""], ["Gonz\u00e1lez", "Javier", ""], ["Lawrence", "Neil", ""]]}, {"id": "1511.06458", "submitter": "Nathan Wiebe", "authors": "Nathan Wiebe, Christopher Granade, Ashish Kapoor, Krysta M Svore", "title": "Bayesian inference via rejection filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a method for approximating Bayesian inference using rejection\nsampling. We not only make the process efficient, but also dramatically reduce\nthe memory required relative to conventional methods by combining rejection\nsampling with particle filtering. We also provide an approximate form of\nrejection sampling that makes rejection filtering tractable in cases where\nexact rejection sampling is not efficient. Finally, we present several\nnumerical examples of rejection filtering that show its ability to track time\ndependent parameters in online settings and also benchmark its performance on\nMNIST classification problems.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 00:08:07 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2015 02:01:59 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Wiebe", "Nathan", ""], ["Granade", "Christopher", ""], ["Kapoor", "Ashish", ""], ["Svore", "Krysta M", ""]]}, {"id": "1511.06462", "submitter": "Ilya Soloveychik", "authors": "Ilya Soloveychik and Ami Wiesel", "title": "Joint Inverse Covariances Estimation with Mutual Linear Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of joint estimation of structured inverse covariance\nmatrices. We perform the estimation using groups of measurements with different\ncovariances of the same unknown structure. Assuming the inverse covariances to\nspan a low dimensional linear subspace in the space of symmetric matrices, our\naim is to determine this structure. It is then utilized to improve the\nestimation of the inverse covariances. We propose a novel optimization\nalgorithm discovering and exploiting the underlying structure and provide its\nefficient implementation. Numerical simulations are presented to illustrate the\nperformance benefits of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 00:18:12 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Soloveychik", "Ilya", ""], ["Wiesel", "Ami", ""]]}, {"id": "1511.06464", "submitter": "Martin Arjovsky", "authors": "Martin Arjovsky, Amar Shah, Yoshua Bengio", "title": "Unitary Evolution Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) are notoriously difficult to train. When the\neigenvalues of the hidden to hidden weight matrix deviate from absolute value\n1, optimization becomes difficult due to the well studied issue of vanishing\nand exploding gradients, especially when trying to learn long-term\ndependencies. To circumvent this problem, we propose a new architecture that\nlearns a unitary weight matrix, with eigenvalues of absolute value exactly 1.\nThe challenge we address is that of parametrizing unitary matrices in a way\nthat does not require expensive computations (such as eigendecomposition) after\neach weight update. We construct an expressive unitary weight matrix by\ncomposing several structured matrices that act as building blocks with\nparameters to be learned. Optimization with this parameterization becomes\nfeasible only when considering hidden states in the complex domain. We\ndemonstrate the potential of this architecture by achieving state of the art\nresults in several hard tasks involving very long-term dependencies.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 00:37:33 GMT"}, {"version": "v2", "created": "Sat, 28 Nov 2015 18:42:08 GMT"}, {"version": "v3", "created": "Thu, 18 Feb 2016 00:52:28 GMT"}, {"version": "v4", "created": "Wed, 25 May 2016 23:34:38 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Arjovsky", "Martin", ""], ["Shah", "Amar", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1511.06481", "submitter": "Guillaume Alain", "authors": "Guillaume Alain, Alex Lamb, Chinnadhurai Sankar, Aaron Courville,\n  Yoshua Bengio", "title": "Variance Reduction in SGD by Distributed Importance Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are able to accelerate their learning by selecting training materials\nthat are the most informative and at the appropriate level of difficulty. We\npropose a framework for distributing deep learning in which one set of workers\nsearch for the most informative examples in parallel while a single worker\nupdates the model on examples selected by importance sampling. This leads the\nmodel to update using an unbiased estimate of the gradient which also has\nminimum variance when the sampling proposal is proportional to the L2-norm of\nthe gradient. We show experimentally that this method reduces gradient variance\neven in a context where the cost of synchronization across machines cannot be\nignored, and where the factors for importance sampling are not updated\ninstantly across the training set.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 03:09:43 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2015 23:26:44 GMT"}, {"version": "v3", "created": "Wed, 2 Dec 2015 14:45:25 GMT"}, {"version": "v4", "created": "Thu, 7 Jan 2016 20:43:32 GMT"}, {"version": "v5", "created": "Thu, 14 Jan 2016 04:45:44 GMT"}, {"version": "v6", "created": "Thu, 21 Jan 2016 04:33:21 GMT"}, {"version": "v7", "created": "Sat, 16 Apr 2016 19:40:08 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Alain", "Guillaume", ""], ["Lamb", "Alex", ""], ["Sankar", "Chinnadhurai", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1511.06499", "submitter": "Dustin Tran", "authors": "Dustin Tran, Rajesh Ranganath, David M. Blei", "title": "The Variational Gaussian Process", "comments": "Appears in International Conference on Learning Representations, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference is a powerful tool for approximate inference, and it\nhas been recently applied for representation learning with deep generative\nmodels. We develop the variational Gaussian process (VGP), a Bayesian\nnonparametric variational family, which adapts its shape to match complex\nposterior distributions. The VGP generates approximate posterior samples by\ngenerating latent inputs and warping them through random non-linear mappings;\nthe distribution over random mappings is learned during inference, enabling the\ntransformed outputs to adapt to varying complexity. We prove a universal\napproximation theorem for the VGP, demonstrating its representative power for\nlearning any model. For inference we present a variational objective inspired\nby auto-encoders and perform black box inference over a wide class of models.\nThe VGP achieves new state-of-the-art results for unsupervised learning,\ninferring models such as the deep latent Gaussian model and the recently\nproposed DRAW.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 06:01:23 GMT"}, {"version": "v2", "created": "Fri, 4 Mar 2016 20:56:01 GMT"}, {"version": "v3", "created": "Wed, 23 Mar 2016 23:11:38 GMT"}, {"version": "v4", "created": "Sun, 17 Apr 2016 22:14:13 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Tran", "Dustin", ""], ["Ranganath", "Rajesh", ""], ["Blei", "David M.", ""]]}, {"id": "1511.06644", "submitter": "C\\'esar Lincoln Cavalcante Mattos", "authors": "C\\'esar Lincoln C. Mattos, Zhenwen Dai, Andreas Damianou, Jeremy\n  Forth, Guilherme A. Barreto, Neil D. Lawrence", "title": "Recurrent Gaussian Processes", "comments": "Published as a conference paper at ICLR 2016. 12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define Recurrent Gaussian Processes (RGP) models, a general family of\nBayesian nonparametric models with recurrent GP priors which are able to learn\ndynamical patterns from sequential data. Similar to Recurrent Neural Networks\n(RNNs), RGPs can have different formulations for their internal states,\ndistinct inference methods and be extended with deep structures. In such\ncontext, we propose a novel deep RGP model whose autoregressive states are\nlatent, thereby performing representation and dynamical learning\nsimultaneously. To fully exploit the Bayesian nature of the RGP model we\ndevelop the Recurrent Variational Bayes (REVARB) framework, which enables\nefficient inference and strong regularization through coherent propagation of\nuncertainty across the RGP layers and states. We also introduce a RGP extension\nwhere variational parameters are greatly reduced by being reparametrized\nthrough RNN-based sequential recognition models. We apply our model to the\ntasks of nonlinear system identification and human motion modeling. The\npromising obtained results indicate that our RGP model maintains its highly\nflexibility while being able to avoid overfitting and being applicable even\nwhen larger datasets are not available.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 15:37:24 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 10:39:07 GMT"}, {"version": "v3", "created": "Wed, 13 Jan 2016 12:15:13 GMT"}, {"version": "v4", "created": "Wed, 20 Jan 2016 18:03:50 GMT"}, {"version": "v5", "created": "Tue, 9 Feb 2016 12:39:07 GMT"}, {"version": "v6", "created": "Wed, 24 Feb 2016 20:01:19 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Mattos", "C\u00e9sar Lincoln C.", ""], ["Dai", "Zhenwen", ""], ["Damianou", "Andreas", ""], ["Forth", "Jeremy", ""], ["Barreto", "Guilherme A.", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1511.06683", "submitter": "Maksim Lapin", "authors": "Maksim Lapin, Matthias Hein and Bernt Schiele", "title": "Top-k Multiclass SVM", "comments": "NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class ambiguity is typical in image classification problems with a large\nnumber of classes. When classes are difficult to discriminate, it makes sense\nto allow k guesses and evaluate classifiers based on the top-k error instead of\nthe standard zero-one loss. We propose top-k multiclass SVM as a direct method\nto optimize for top-k performance. Our generalization of the well-known\nmulticlass SVM is based on a tight convex upper bound of the top-k error. We\npropose a fast optimization scheme based on an efficient projection onto the\ntop-k simplex, which is of its own interest. Experiments on five datasets show\nconsistent improvements in top-k accuracy compared to various baselines.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 16:49:33 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Lapin", "Maksim", ""], ["Hein", "Matthias", ""], ["Schiele", "Bernt", ""]]}, {"id": "1511.06718", "submitter": "Cyril Stark", "authors": "Cyril Stark", "title": "Top-N recommendations from expressive recommender systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalized nonnegative models assign probability distributions to users and\nrandom variables to items; see [Stark, 2015]. Rating an item is regarded as\nsampling the random variable assigned to the item with respect to the\ndistribution assigned to the user who rates the item. Models of that kind are\nhighly expressive. For instance, using normalized nonnegative models we can\nunderstand users' preferences as mixtures of interpretable user stereotypes,\nand we can arrange properties of users and items in a hierarchical manner.\nThese features would not be useful if the predictive power of normalized\nnonnegative models was poor. Thus, we analyze here the performance of\nnormalized nonnegative models for top-N recommendation and observe that their\nperformance matches the performance of methods like PureSVD which was\nintroduced in [Cremonesi et al., 2010]. We conclude that normalized nonnegative\nmodels not only provide accurate recommendations but they also deliver (for\nfree) representations that are interpretable. We deepen the discussion of\nnormalized nonnegative models by providing further theoretical insights. In\nparticular, we introduce total variational distance as an operational\nsimilarity measure, we discover scenarios where normalized nonnegative models\nyield unique representations of users and items, we prove that the inference of\noptimal normalized nonnegative models is NP-hard and finally, we discuss the\nrelationship between normalized nonnegative models and nonnegative matrix\nfactorization.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 18:18:45 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Stark", "Cyril", ""]]}, {"id": "1511.06772", "submitter": "Jes\\'us Villalba", "authors": "Jes\\'us Villalba", "title": "PLDA with Two Sources of Inter-session Variability", "comments": "Technical Report, ViVoLab, I3A, University of Zaragoza, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In some speaker recognition scenarios we find conversations recorded\nsimultaneously over multiple channels. That is the case of the interviews in\nthe NIST SRE dataset. To take advantage of that, we propose a modification of\nthe PLDA model that considers two different inter-session variability terms.\nThe first term is tied between all the recordings belonging to the same\nconversation whereas the second is not. Thus, the former mainly intends to\ncapture the variability due to the phonetic content of the conversation while\nthe latter tries to capture the channel variability. In this document, we\nderive the equations for this model. This model was applied in the paper\n\"Handling Recordings Acquired Simultaneously over Multiple Channels with PLDA\"\npublished at Interspeech 2013.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 21:08:04 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Villalba", "Jes\u00fas", ""]]}, {"id": "1511.06807", "submitter": "Arvind Neelakantan", "authors": "Arvind Neelakantan, Luke Vilnis, Quoc V. Le, Ilya Sutskever, Lukasz\n  Kaiser, Karol Kurach, James Martens", "title": "Adding Gradient Noise Improves Learning for Very Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep feedforward and recurrent networks have achieved impressive results in\nmany perception and language processing applications. This success is partially\nattributed to architectural innovations such as convolutional and long\nshort-term memory networks. The main motivation for these architectural\ninnovations is that they capture better domain knowledge, and importantly are\neasier to optimize than more basic architectures. Recently, more complex\narchitectures such as Neural Turing Machines and Memory Networks have been\nproposed for tasks including question answering and general computation,\ncreating a new set of optimization challenges. In this paper, we discuss a\nlow-overhead and easy-to-implement technique of adding gradient noise which we\nfind to be surprisingly effective when training these very deep architectures.\nThe technique not only helps to avoid overfitting, but also can result in lower\ntraining loss. This method alone allows a fully-connected 20-layer deep network\nto be trained with standard gradient descent, even starting from a poor\ninitialization. We see consistent improvements for many complex models,\nincluding a 72% relative reduction in error rate over a carefully-tuned\nbaseline on a challenging question-answering task, and a doubling of the number\nof accurate binary multiplication models learned across 7,000 random restarts.\nWe encourage further application of this technique to additional complex modern\narchitectures.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 01:11:29 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Neelakantan", "Arvind", ""], ["Vilnis", "Luke", ""], ["Le", "Quoc V.", ""], ["Sutskever", "Ilya", ""], ["Kaiser", "Lukasz", ""], ["Kurach", "Karol", ""], ["Martens", "James", ""]]}, {"id": "1511.06821", "submitter": "Xin Lu Tan", "authors": "Xin Lu Tan, Andreas Buja, and Zongming Ma", "title": "Kernel Additive Principal Components", "comments": "54 pages including appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive principal components (APCs for short) are a nonlinear generalization\nof linear principal components. We focus on smallest APCs to describe additive\nnonlinear constraints that are approximately satisfied by the data. Thus APCs\nfit data with implicit equations that treat the variables symmetrically, as\nopposed to regression analyses which fit data with explicit equations that\ntreat the data asymmetrically by singling out a response variable. We propose a\nregularized data-analytic procedure for APC estimation using kernel methods. In\ncontrast to existing approaches to APCs that are based on regularization\nthrough subspace restriction, kernel methods achieve regularization through\nshrinkage and therefore grant distinctive flexibility in APC estimation by\nallowing the use of infinite-dimensional functions spaces for searching APC\ntransformation while retaining computational feasibility. To connect population\nAPCs and kernelized finite-sample APCs, we study kernelized population APCs and\ntheir associated eigenproblems, which eventually lead to the establishment of\nconsistency of the estimated APCs. Lastly, we discuss an iterative algorithm\nfor computing kernelized finite-sample APCs.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 03:12:04 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Tan", "Xin Lu", ""], ["Buja", "Andreas", ""], ["Ma", "Zongming", ""]]}, {"id": "1511.06890", "submitter": "Kian Hsiang Low", "authors": "Chun Kai Ling, Kian Hsiang Low, Patrick Jaillet", "title": "Gaussian Process Planning with Lipschitz Continuous Reward Functions:\n  Towards Unifying Bayesian Optimization, Active Learning, and Beyond", "comments": "30th AAAI Conference on Artificial Intelligence (AAAI 2016), Extended\n  version with proofs, 17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel nonmyopic adaptive Gaussian process planning\n(GPP) framework endowed with a general class of Lipschitz continuous reward\nfunctions that can unify some active learning/sensing and Bayesian optimization\ncriteria and offer practitioners some flexibility to specify their desired\nchoices for defining new tasks/problems. In particular, it utilizes a\nprincipled Bayesian sequential decision problem framework for jointly and\nnaturally optimizing the exploration-exploitation trade-off. In general, the\nresulting induced GPP policy cannot be derived exactly due to an uncountable\nset of candidate observations. A key contribution of our work here thus lies in\nexploiting the Lipschitz continuity of the reward functions to solve for a\nnonmyopic adaptive epsilon-optimal GPP (epsilon-GPP) policy. To plan in real\ntime, we further propose an asymptotically optimal, branch-and-bound anytime\nvariant of epsilon-GPP with performance guarantee. We empirically demonstrate\nthe effectiveness of our epsilon-GPP policy and its anytime variant in Bayesian\noptimization and an energy harvesting task.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 14:57:48 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Ling", "Chun Kai", ""], ["Low", "Kian Hsiang", ""], ["Jaillet", "Patrick", ""]]}, {"id": "1511.06891", "submitter": "Kian Hsiang Low", "authors": "Yehong Zhang, Trong Nghia Hoang, Kian Hsiang Low, Mohan Kankanhalli", "title": "Near-Optimal Active Learning of Multi-Output Gaussian Processes", "comments": "30th AAAI Conference on Artificial Intelligence (AAAI 2016), Extended\n  version with proofs, 13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of active learning of a multi-output\nGaussian process (MOGP) model representing multiple types of coexisting\ncorrelated environmental phenomena. In contrast to existing works, our active\nlearning problem involves selecting not just the most informative sampling\nlocations to be observed but also the types of measurements at each selected\nlocation for minimizing the predictive uncertainty (i.e., posterior joint\nentropy) of a target phenomenon of interest given a sampling budget.\nUnfortunately, such an entropy criterion scales poorly in the numbers of\ncandidate sampling locations and selected observations when optimized. To\nresolve this issue, we first exploit a structure common to sparse MOGP models\nfor deriving a novel active learning criterion. Then, we exploit a relaxed form\nof submodularity property of our new criterion for devising a polynomial-time\napproximation algorithm that guarantees a constant-factor approximation of that\nachieved by the optimal set of selected observations. Empirical evaluation on\nreal-world datasets shows that our proposed approach outperforms existing\nalgorithms for active learning of MOGP and single-output GP models.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 15:08:53 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 08:45:36 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Zhang", "Yehong", ""], ["Hoang", "Trong Nghia", ""], ["Low", "Kian Hsiang", ""], ["Kankanhalli", "Mohan", ""]]}, {"id": "1511.06909", "submitter": "Shihao Ji", "authors": "Shihao Ji, S. V. N. Vishwanathan, Nadathur Satish, Michael J. Anderson\n  and Pradeep Dubey", "title": "BlackOut: Speeding up Recurrent Neural Network Language Models With Very\n  Large Vocabularies", "comments": "Published as a conference paper at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose BlackOut, an approximation algorithm to efficiently train massive\nrecurrent neural network language models (RNNLMs) with million word\nvocabularies. BlackOut is motivated by using a discriminative loss, and we\ndescribe a new sampling strategy which significantly reduces computation while\nimproving stability, sample efficiency, and rate of convergence. One way to\nunderstand BlackOut is to view it as an extension of the DropOut strategy to\nthe output layer, wherein we use a discriminative training loss and a weighted\nsampling scheme. We also establish close connections between BlackOut,\nimportance sampling, and noise contrastive estimation (NCE). Our experiments,\non the recently released one billion word language modeling benchmark,\ndemonstrate scalability and accuracy of BlackOut; we outperform the\nstate-of-the art, and achieve the lowest perplexity scores on this dataset.\nMoreover, unlike other established methods which typically require GPUs or CPU\nclusters, we show that a carefully implemented version of BlackOut requires\nonly 1-10 days on a single machine to train a RNNLM with a million word\nvocabulary and billions of parameters on one billion words. Although we\ndescribe BlackOut in the context of RNNLM training, it can be used to any\nnetworks with large softmax output layers.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 17:49:30 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 07:09:16 GMT"}, {"version": "v3", "created": "Wed, 16 Dec 2015 06:08:54 GMT"}, {"version": "v4", "created": "Mon, 21 Dec 2015 04:40:55 GMT"}, {"version": "v5", "created": "Wed, 6 Jan 2016 21:57:56 GMT"}, {"version": "v6", "created": "Sun, 21 Feb 2016 16:40:26 GMT"}, {"version": "v7", "created": "Thu, 31 Mar 2016 17:37:25 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Ji", "Shihao", ""], ["Vishwanathan", "S. V. N.", ""], ["Satish", "Nadathur", ""], ["Anderson", "Michael J.", ""], ["Dubey", "Pradeep", ""]]}, {"id": "1511.07125", "submitter": "Patrick Gallagher", "authors": "Patrick W. Gallagher, Shuai Tang, Zhuowen Tu", "title": "What Happened to My Dog in That Network: Unraveling Top-down Generators\n  in Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Top-down information plays a central role in human perception, but plays\nrelatively little role in many current state-of-the-art deep networks, such as\nConvolutional Neural Networks (CNNs). This work seeks to explore a path by\nwhich top-down information can have a direct impact within current deep\nnetworks. We explore this path by learning and using \"generators\" corresponding\nto the network internal effects of three types of transformation (each a\nrestriction of a general affine transformation): rotation, scaling, and\ntranslation. We demonstrate how these learned generators can be used to\ntransfer top-down information to novel settings, as mediated by the \"feature\nflows\" that the transformations (and the associated generators) correspond to\ninside the network. Specifically, we explore three aspects: 1) using generators\nas part of a method for synthesizing transformed images --- given a previously\nunseen image, produce versions of that image corresponding to one or more\nspecified transformations, 2) \"zero-shot learning\" --- when provided with a\nfeature flow corresponding to the effect of a transformation of unknown amount,\nleverage learned generators as part of a method by which to perform an accurate\ncategorization of the amount of transformation, even for amounts never observed\nduring training, and 3) (inside-CNN) \"data augmentation\" --- improve the\nclassification performance of an existing network by using the learned\ngenerators to directly provide additional training \"inside the CNN\".\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 07:48:01 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Gallagher", "Patrick W.", ""], ["Tang", "Shuai", ""], ["Tu", "Zhuowen", ""]]}, {"id": "1511.07130", "submitter": "Amar Shah", "authors": "Amar Shah, Zoubin Ghahramani", "title": "Parallel Predictive Entropy Search for Batch Global Optimization of\n  Expensive Objective Functions", "comments": "12 pages in Neural Information Processing Systems 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop parallel predictive entropy search (PPES), a novel algorithm for\nBayesian optimization of expensive black-box objective functions. At each\niteration, PPES aims to select a batch of points which will maximize the\ninformation gain about the global maximizer of the objective. Well known\nstrategies exist for suggesting a single evaluation point based on previous\nobservations, while far fewer are known for selecting batches of points to\nevaluate in parallel. The few batch selection schemes that have been studied\nall resort to greedy methods to compute an optimal batch. To the best of our\nknowledge, PPES is the first non-greedy batch Bayesian optimization strategy.\nWe demonstrate the benefit of this approach in optimization performance on both\nsynthetic and real world applications, including problems in machine learning,\nrocket science and robotics.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 08:21:17 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Shah", "Amar", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1511.07211", "submitter": "Adish Singla", "authors": "Adish Singla, Sebastian Tschiatschek, Andreas Krause", "title": "Noisy Submodular Maximization via Adaptive Sampling with Applications to\n  Crowdsourced Image Collection Summarization", "comments": "Extended version of AAAI'16 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of maximizing an unknown submodular function that can\nonly be accessed via noisy evaluations. Our work is motivated by the task of\nsummarizing content, e.g., image collections, by leveraging users' feedback in\nform of clicks or ratings. For summarization tasks with the goal of maximizing\ncoverage and diversity, submodular set functions are a natural choice. When the\nunderlying submodular function is unknown, users' feedback can provide noisy\nevaluations of the function that we seek to maximize. We provide a generic\nalgorithm -- \\submM{} -- for maximizing an unknown submodular function under\ncardinality constraints. This algorithm makes use of a novel exploration module\n-- \\blbox{} -- that proposes good elements based on adaptively sampling noisy\nfunction evaluations. \\blbox{} is able to accommodate different kinds of\nobservation models such as value queries and pairwise comparisons. We provide\nPAC-style guarantees on the quality and sampling cost of the solution obtained\nby \\submM{}. We demonstrate the effectiveness of our approach in an\ninteractive, crowdsourced image collection summarization application.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 13:19:05 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2015 09:49:35 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Singla", "Adish", ""], ["Tschiatschek", "Sebastian", ""], ["Krause", "Andreas", ""]]}, {"id": "1511.07281", "submitter": "Andr\\'es Felipe L\\'opez-Lopera", "authors": "Andr\\'es F. L\\'opez-Lopera and Mauricio A. \\'Alvarez and \\'Avaro A.\n  Orozco", "title": "Sparse Linear Models applied to Power Quality Disturbance Classification", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-52277-7_63", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power quality (PQ) analysis describes the non-pure electric signals that are\nusually present in electric power systems. The automatic recognition of PQ\ndisturbances can be seen as a pattern recognition problem, in which different\ntypes of waveform distortion are differentiated based on their features.\nSimilar to other quasi-stationary signals, PQ disturbances can be decomposed\ninto time-frequency dependent components by using time-frequency or time-scale\ntransforms, also known as dictionaries. These dictionaries are used in the\nfeature extraction step in pattern recognition systems. Short-time Fourier,\nWavelets and Stockwell transforms are some of the most common dictionaries used\nin the PQ community, aiming to achieve a better signal representation. To the\nbest of our knowledge, previous works about PQ disturbance classification have\nbeen restricted to the use of one among several available dictionaries. Taking\nadvantage of the theory behind sparse linear models (SLM), we introduce a\nsparse method for PQ representation, starting from overcomplete dictionaries.\nIn particular, we apply Group Lasso. We employ different types of\ntime-frequency (or time-scale) dictionaries to characterize the PQ\ndisturbances, and evaluate their performance under different pattern\nrecognition algorithms. We show that the SLM reduce the PQ classification\ncomplexity promoting sparse basis selection, and improving the classification\naccuracy.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 15:44:16 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["L\u00f3pez-Lopera", "Andr\u00e9s F.", ""], ["\u00c1lvarez", "Mauricio A.", ""], ["Orozco", "\u00c1varo A.", ""]]}, {"id": "1511.07293", "submitter": "Zhaosong Lu", "authors": "Zhaosong Lu and Xiaorui Li", "title": "Sparse Recovery via Partial Regularization: Models, Theory and\n  Algorithms", "comments": "35 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IT cs.LG math.IT stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of sparse recovery, it is known that most of existing\nregularizers such as $\\ell_1$ suffer from some bias incurred by some leading\nentries (in magnitude) of the associated vector. To neutralize this bias, we\npropose a class of models with partial regularizers for recovering a sparse\nsolution of a linear system. We show that every local minimizer of these models\nis sufficiently sparse or the magnitude of all its nonzero entries is above a\nuniform constant depending only on the data of the linear system. Moreover, for\na class of partial regularizers, any global minimizer of these models is a\nsparsest solution to the linear system. We also establish some sufficient\nconditions for local or global recovery of the sparsest solution to the linear\nsystem, among which one of the conditions is weaker than the best known\nrestricted isometry property (RIP) condition for sparse recovery by $\\ell_1$.\nIn addition, a first-order feasible augmented Lagrangian (FAL) method is\nproposed for solving these models, in which each subproblem is solved by a\nnonmonotone proximal gradient (NPG) method. Despite the complication of the\npartial regularizers, we show that each proximal subproblem in NPG can be\nsolved as a certain number of one-dimensional optimization problems, which\nusually have a closed-form solution. We also show that any accumulation point\nof the sequence generated by FAL is a first-order stationary point of the\nmodels. Numerical results on compressed sensing and sparse logistic regression\ndemonstrate that the proposed models substantially outperform the widely used\nones in the literature in terms of solution quality.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 16:08:24 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Lu", "Zhaosong", ""], ["Li", "Xiaorui", ""]]}, {"id": "1511.07294", "submitter": "Zhanxing Zhu", "authors": "Zhanxing Zhu and Amos J. Storkey", "title": "Stochastic Parallel Block Coordinate Descent for Large-scale Saddle\n  Point Problems", "comments": "Accepted by AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider convex-concave saddle point problems with a separable structure\nand non-strongly convex functions. We propose an efficient stochastic block\ncoordinate descent method using adaptive primal-dual updates, which enables\nflexible parallel optimization for large-scale problems. Our method shares the\nefficiency and flexibility of block coordinate descent methods with the\nsimplicity of primal-dual methods and utilizing the structure of the separable\nconvex-concave saddle point problem. It is capable of solving a wide range of\nmachine learning applications, including robust principal component analysis,\nLasso, and feature selection by group Lasso, etc. Theoretically and\nempirically, we demonstrate significantly better performance than\nstate-of-the-art methods in all these applications.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 16:12:11 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Zhu", "Zhanxing", ""], ["Storkey", "Amos J.", ""]]}, {"id": "1511.07318", "submitter": "Jes\\'us Villalba Jesus A Villalba Lopez", "authors": "Jes\\'us Villalba", "title": "Bayesian SPLDA", "comments": "Technical Report, ViVoLab, I3A, University of Zaragoza, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this document we are going to derive the equations needed to implement a\nVariational Bayes estimation of the parameters of the simplified probabilistic\nlinear discriminant analysis (SPLDA) model. This can be used to adapt SPLDA\nfrom one database to another with few development data or to implement the\nfully Bayesian recipe. Our approach is similar to Bishop's VB PPCA.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 20:43:43 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Villalba", "Jes\u00fas", ""]]}, {"id": "1511.07334", "submitter": "Andr\\'es Felipe L\\'opez-Lopera", "authors": "Andr\\'es F. L\\'opez-Lopera and Mauricio A. \\'Alvarez", "title": "Switched latent force models for reverse-engineering transcriptional\n  regulation in gene expression data", "comments": null, "journal-ref": null, "doi": "10.1109/TCBB.2017.2764908", "report-no": null, "categories": "physics.bio-ph physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To survive environmental conditions, cells transcribe their response\nactivities into encoded mRNA sequences in order to produce certain amounts of\nprotein concentrations. The external conditions are mapped into the cell\nthrough the activation of special proteins called transcription factors (TFs).\nDue to the difficult task to measure experimentally TF behaviours, and the\nchallenges to capture their quick-time dynamics, different types of models\nbased on differential equations have been proposed. However, those approaches\nusually incur in costly procedures, and they present problems to describe\nsudden changes in TF regulators. In this paper, we present a switched dynamical\nlatent force model for reverse-engineering transcriptional regulation in gene\nexpression data which allows the exact inference over latent TF activities\ndriving some observed gene expressions through a linear differential equation.\nTo deal with discontinuities in the dynamics, we introduce an approach that\nswitches between different TF activities and different dynamical systems. This\ncreates a versatile representation of transcription networks that can capture\ndiscrete changes and non-linearities We evaluate our model on both simulated\ndata and real-data (e.g. microaerobic shift in E. coli, yeast respiration),\nconcluding that our framework allows for the fitting of the expression data\nwhile being able to infer continuous-time TF profiles.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 17:38:38 GMT"}, {"version": "v2", "created": "Wed, 25 Oct 2017 08:50:06 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["L\u00f3pez-Lopera", "Andr\u00e9s F.", ""], ["\u00c1lvarez", "Mauricio A.", ""]]}, {"id": "1511.07367", "submitter": "Evan Archer", "authors": "Evan Archer, Il Memming Park, Lars Buesing, John Cunningham, Liam\n  Paninski", "title": "Black box variational inference for state space models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent variable time-series models are among the most heavily used tools from\nmachine learning and applied statistics. These models have the advantage of\nlearning latent structure both from noisy observations and from the temporal\nordering in the data, where it is assumed that meaningful correlation structure\nexists across time. A few highly-structured models, such as the linear\ndynamical system with linear-Gaussian observations, have closed-form inference\nprocedures (e.g. the Kalman Filter), but this case is an exception to the\ngeneral rule that exact posterior inference in more complex generative models\nis intractable. Consequently, much work in time-series modeling focuses on\napproximate inference procedures for one particular class of models. Here, we\nextend recent developments in stochastic variational inference to develop a\n`black-box' approximate inference technique for latent variable models with\nlatent dynamical structure. We propose a structured Gaussian variational\napproximate posterior that carries the same intuition as the standard Kalman\nfilter-smoother but, importantly, permits us to use the same inference approach\nto approximate the posterior of much more general, nonlinear latent variable\ngenerative models. We show that our approach recovers accurate estimates in the\ncase of basic models with closed-form posteriors, and more interestingly\nperforms well in comparison to variational approaches that were designed in a\nbespoke fashion for specific non-conjugate models.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 19:08:08 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Archer", "Evan", ""], ["Park", "Il Memming", ""], ["Buesing", "Lars", ""], ["Cunningham", "John", ""], ["Paninski", "Liam", ""]]}, {"id": "1511.07421", "submitter": "Jes\\'us Villalba", "authors": "Jes\\'us Villalba", "title": "Unsupervised Adaptation of SPLDA", "comments": "Technical Report, ViVolab, I3A, University of Zaragoza, Spain. arXiv\n  admin note: text overlap with arXiv:1511.07318", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  State-of-the-art speaker recognition relays on models that need a large\namount of training data. This models are successful in tasks like NIST SRE\nbecause there is sufficient data available. However, in real applications, we\nusually do not have so much data and, in many cases, the speaker labels are\nunknown. We present a method to adapt a PLDA model from a domain with a large\namount of labeled data to another with unlabeled data. We describe a generative\nmodel that produces both sets of data where the unknown labels are modeled like\nlatent variables. We used variational Bayes to estimate the hidden variables.\nHere, we derive the equations for this model. This model has been used in the\npapers: \"UNSUPERVISED ADAPTATION OF PLDA BY USING VARIATIONAL BAYES METHODS\"\npublised at ICASSP 2014, \"Unsupervised Training of PLDA with Variational Bayes\"\npublished at Iberspeech 2014, and \"VARIATIONAL BAYESIAN PLDA FOR SPEAKER\nDIARIZATION IN THE MGB CHALLENGE\" published at ASRU 2015.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 21:25:59 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Villalba", "Jes\u00fas", ""]]}, {"id": "1511.07422", "submitter": "Jes\\'us Villalba", "authors": "Jes\\'us Villalba", "title": "Variational Bayes Factor Analysis for i-Vector Extraction", "comments": "Technical Report, ViVoLab, I3A, University of Zaragoza, Spain. arXiv\n  admin note: text overlap with arXiv:1511.07318", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this document we are going to derive the equations needed to implement a\nVariational Bayes i-vector extractor. This can be used to extract longer\ni-vectors reducing the risk of overfittig or to adapt an i-vector extractor\nfrom a database to another with scarce development data. This work is based on\nPatrick Kenny's joint factor analysis and Christopher Bishop's variational\nprincipal components.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 21:38:25 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Villalba", "Jes\u00fas", ""]]}, {"id": "1511.07428", "submitter": "Ananda Theertha Suresh", "authors": "Alon Orlitsky, Ananda Theertha Suresh, Yihong Wu", "title": "Estimating the number of unseen species: A bird in the hand is worth\n  $\\log n $ in the bush", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the number of unseen species is an important problem in many\nscientific endeavors. Its most popular formulation, introduced by Fisher, uses\n$n$ samples to predict the number $U$ of hitherto unseen species that would be\nobserved if $t\\cdot n$ new samples were collected. Of considerable interest is\nthe largest ratio $t$ between the number of new and existing samples for which\n$U$ can be accurately predicted.\n  In seminal works, Good and Toulmin constructed an intriguing estimator that\npredicts $U$ for all $t\\le 1$, thereby showing that the number of species can\nbe estimated for a population twice as large as that observed. Subsequently\nEfron and Thisted obtained a modified estimator that empirically predicts $U$\neven for some $t>1$, but without provable guarantees.\n  We derive a class of estimators that $\\textit{provably}$ predict $U$ not just\nfor constant $t>1$, but all the way up to $t$ proportional to $\\log n$. This\nshows that the number of species can be estimated for a population $\\log n$\ntimes larger than that observed, a factor that grows arbitrarily large as $n$\nincreases. We also show that this range is the best possible and that the\nestimators' mean-square error is optimal up to constants for any $t$. Our\napproach yields the first provable guarantee for the Efron-Thisted estimator\nand, in addition, a variant which achieves stronger theoretical and\nexperimental performance than existing methodologies on a variety of synthetic\nand real datasets.\n  The estimators we derive are simple linear estimators that are computable in\ntime proportional to $n$. The performance guarantees hold uniformly for all\ndistributions, and apply to all four standard sampling models commonly used\nacross various scientific disciplines: multinomial, Poisson, hypergeometric,\nand Bernoulli product.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 20:58:55 GMT"}, {"version": "v2", "created": "Mon, 29 Feb 2016 20:58:19 GMT"}, {"version": "v3", "created": "Thu, 3 Mar 2016 02:52:44 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Orlitsky", "Alon", ""], ["Suresh", "Ananda Theertha", ""], ["Wu", "Yihong", ""]]}, {"id": "1511.07528", "submitter": "Nicolas Papernot", "authors": "Nicolas Papernot and Patrick McDaniel and Somesh Jha and Matt\n  Fredrikson and Z. Berkay Celik and Ananthram Swami", "title": "The Limitations of Deep Learning in Adversarial Settings", "comments": "Accepted to the 1st IEEE European Symposium on Security & Privacy,\n  IEEE 2016. Saarbrucken, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning takes advantage of large datasets and computationally efficient\ntraining algorithms to outperform other approaches at various machine learning\ntasks. However, imperfections in the training phase of deep neural networks\nmake them vulnerable to adversarial samples: inputs crafted by adversaries with\nthe intent of causing deep neural networks to misclassify. In this work, we\nformalize the space of adversaries against deep neural networks (DNNs) and\nintroduce a novel class of algorithms to craft adversarial samples based on a\nprecise understanding of the mapping between inputs and outputs of DNNs. In an\napplication to computer vision, we show that our algorithms can reliably\nproduce samples correctly classified by human subjects but misclassified in\nspecific targets by a DNN with a 97% adversarial success rate while only\nmodifying on average 4.02% of the input features per sample. We then evaluate\nthe vulnerability of different sample classes to adversarial perturbations by\ndefining a hardness measure. Finally, we describe preliminary work outlining\ndefenses against adversarial samples by defining a predictive measure of\ndistance between a benign input and a target classification.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 01:07:08 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Papernot", "Nicolas", ""], ["McDaniel", "Patrick", ""], ["Jha", "Somesh", ""], ["Fredrikson", "Matt", ""], ["Celik", "Z. Berkay", ""], ["Swami", "Ananthram", ""]]}, {"id": "1511.07551", "submitter": "Yanshuai Cao", "authors": "Yanshuai Cao, David J. Fleet", "title": "Transductive Log Opinion Pool of Gaussian Process Experts", "comments": "Accepted at NIPS2015 Workshop on Nonparametric Methods for Large\n  Scale Representation Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework for analyzing transductive combination of Gaussian\nprocess (GP) experts, where independently trained GP experts are combined in a\nway that depends on test point location, in order to scale GPs to big data. The\nframework provides some theoretical justification for the generalized product\nof GP experts (gPoE-GP) which was previously shown to work well in practice but\nlacks theoretical basis. Based on the proposed framework, an improvement over\ngPoE-GP is introduced and empirically validated.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 03:08:59 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Cao", "Yanshuai", ""], ["Fleet", "David J.", ""]]}, {"id": "1511.07715", "submitter": "Dekang Zhu Mr.", "authors": "Dekang Zhu, Dan P. Guralnik, Xuezhi Wang, Xiang Li and Bill Moran", "title": "Statistical Properties of the Single Linkage Hierarchical Clustering\n  Estimator", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance-based hierarchical clustering (HC) methods are widely used in\nunsupervised data analysis but few authors take account of uncertainty in the\ndistance data. We incorporate a statistical model of the uncertainty through\ncorruption or noise in the pairwise distances and investigate the problem of\nestimating the HC as unknown parameters from measurements. Specifically, we\nfocus on single linkage hierarchical clustering (SLHC) and study its geometry.\nWe prove that under fairly reasonable conditions on the probability\ndistribution governing measurements, SLHC is equivalent to maximum partial\nprofile likelihood estimation (MPPLE) with some of the information contained in\nthe data ignored. At the same time, we show that direct evaluation of SLHC on\nmaximum likelihood estimation (MLE) of pairwise distances yields a consistent\nestimator. Consequently, a full MLE is expected to perform better than SLHC in\ngetting the correct HC results for the ground truth metric.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 14:15:11 GMT"}, {"version": "v2", "created": "Thu, 1 Sep 2016 00:55:30 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Zhu", "Dekang", ""], ["Guralnik", "Dan P.", ""], ["Wang", "Xuezhi", ""], ["Li", "Xiang", ""], ["Moran", "Bill", ""]]}, {"id": "1511.07827", "submitter": "Robert Leech", "authors": "Romy Lorenz, Ricardo P Monti, Ines R Violante, Aldo A Faisal,\n  Christoforos Anagnostopoulos, Robert Leech and Giovanni Montana", "title": "Stopping criteria for boosting automatic experimental design using\n  real-time fMRI with Bayesian optimization", "comments": "Oral presentation at MLINI 2015 - 5th NIPS Workshop on Machine\n  Learning and Interpretation in Neuroimaging: Beyond the Scanner", "journal-ref": null, "doi": null, "report-no": "MLINI/2015/15", "categories": "q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization has been proposed as a practical and efficient tool\nthrough which to tune parameters in many difficult settings. Recently, such\ntechniques have been combined with real-time fMRI to propose a novel framework\nwhich turns on its head the conventional functional neuroimaging approach. This\nclosed-loop method automatically designs the optimal experiment to evoke a\ndesired target brain pattern. One of the challenges associated with extending\nsuch methods to real-time brain imaging is the need for adequate stopping\ncriteria, an aspect of Bayesian optimization which has received limited\nattention. In light of high scanning costs and limited attentional capacities\nof subjects an accurate and reliable stopping criteria is essential. In order\nto address this issue we propose and empirically study the performance of two\nstopping criteria.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 18:28:54 GMT"}, {"version": "v2", "created": "Tue, 22 Mar 2016 16:54:36 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Lorenz", "Romy", ""], ["Monti", "Ricardo P", ""], ["Violante", "Ines R", ""], ["Faisal", "Aldo A", ""], ["Anagnostopoulos", "Christoforos", ""], ["Leech", "Robert", ""], ["Montana", "Giovanni", ""]]}, {"id": "1511.07837", "submitter": "Zhaosong Lu", "authors": "Zhaosong Lu and Xiaojun Chen", "title": "Generalized Conjugate Gradient Methods for $\\ell_1$ Regularized Convex\n  Quadratic Programming with Finite Convergence", "comments": "36 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conjugate gradient (CG) method is an efficient iterative method for\nsolving large-scale strongly convex quadratic programming (QP). In this paper\nwe propose some generalized CG (GCG) methods for solving the\n$\\ell_1$-regularized (possibly not strongly) convex QP that terminate at an\noptimal solution in a finite number of iterations. At each iteration, our\nmethods first identify a face of an orthant and then either perform an exact\nline search along the direction of the negative projected minimum-norm\nsubgradient of the objective function or execute a CG subroutine that conducts\na sequence of CG iterations until a CG iterate crosses the boundary of this\nface or an approximate minimizer of over this face or a subface is found. We\ndetermine which type of step should be taken by comparing the magnitude of some\ncomponents of the minimum-norm subgradient of the objective function to that of\nits rest components. Our analysis on finite convergence of these methods makes\nuse of an error bound result and some key properties of the aforementioned\nexact line search and the CG subroutine. We also show that the proposed methods\nare capable of finding an approximate solution of the problem by allowing some\ninexactness on the execution of the CG subroutine. The overall arithmetic\noperation cost of our GCG methods for finding an $\\epsilon$-optimal solution\ndepends on $\\epsilon$ in $O(\\log(1/\\epsilon))$, which is superior to the\naccelerated proximal gradient method [2,23] that depends on $\\epsilon$ in\n$O(1/\\sqrt{\\epsilon})$. In addition, our GCG methods can be extended\nstraightforwardly to solve box-constrained convex QP with finite convergence.\nNumerical results demonstrate that our methods are very favorable for solving\nill-conditioned problems.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 19:28:09 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2015 22:16:30 GMT"}, {"version": "v3", "created": "Fri, 12 Feb 2016 19:23:49 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Lu", "Zhaosong", ""], ["Chen", "Xiaojun", ""]]}, {"id": "1511.07896", "submitter": "Vishesh Karwa", "authors": "Vishesh Karwa and Dan Kifer and Aleksandra B. Slavkovi\\'c", "title": "Private Posterior distributions from Variational approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy preserving mechanisms such as differential privacy inject additional\nrandomness in the form of noise in the data, beyond the sampling mechanism.\nIgnoring this additional noise can lead to inaccurate and invalid inferences.\nIn this paper, we incorporate the privacy mechanism explicitly into the\nlikelihood function by treating the original data as missing, with an end goal\nof estimating posterior distributions over model parameters. This leads to a\nprincipled way of performing valid statistical inference using private data,\nhowever, the corresponding likelihoods are intractable. In this paper, we\nderive fast and accurate variational approximations to tackle such intractable\nlikelihoods that arise due to privacy. We focus on estimating posterior\ndistributions of parameters of the naive Bayes log-linear model, where the\nsufficient statistics of this model are shared using a differentially private\ninterface. Using a simulation study, we show that the posterior approximations\noutperform the naive method of ignoring the noise addition mechanism.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 21:49:02 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Karwa", "Vishesh", ""], ["Kifer", "Dan", ""], ["Slavkovi\u0107", "Aleksandra B.", ""]]}, {"id": "1511.07902", "submitter": "Bicheng Ying", "authors": "Bicheng Ying and Ali H. Sayed", "title": "Performance Limits of Stochastic Sub-Gradient Learning, Part I: Single\n  Agent Case", "comments": "Part II is available on http://arxiv.org/abs/1704.06025", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work and the supporting Part II, we examine the performance of\nstochastic sub-gradient learning strategies under weaker conditions than\nusually considered in the literature. The new conditions are shown to be\nautomatically satisfied by several important cases of interest including SVM,\nLASSO, and Total-Variation denoising formulations. In comparison, these\nproblems do not satisfy the traditional assumptions used in prior analyses and,\ntherefore, conclusions derived from these earlier treatments are not directly\napplicable to these problems. The results in this article establish that\nstochastic sub-gradient strategies can attain linear convergence rates, as\nopposed to sub-linear rates, to the steady-state regime. A realizable\nexponential-weighting procedure is employed to smooth the intermediate iterates\nand guarantee useful performance bounds in terms of convergence rate and\nexcessive risk performance. Part I of this work focuses on single-agent\nscenarios, which are common in stand-alone learning applications, while Part II\nextends the analysis to networked learners. The theoretical conclusions are\nillustrated by several examples and simulations, including comparisons with the\nFISTA procedure.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 22:31:19 GMT"}, {"version": "v2", "created": "Sun, 24 Apr 2016 21:04:14 GMT"}, {"version": "v3", "created": "Sat, 30 Jul 2016 22:17:34 GMT"}, {"version": "v4", "created": "Fri, 21 Apr 2017 17:55:25 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Ying", "Bicheng", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1511.07916", "submitter": "KyungHyun Cho", "authors": "Kyunghyun Cho", "title": "Natural Language Understanding with Distributed Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a lecture note for the course DS-GA 3001 <Natural Language\nUnderstanding with Distributed Representation> at the Center for Data Science ,\nNew York University in Fall, 2015. As the name of the course suggests, this\nlecture note introduces readers to a neural network based approach to natural\nlanguage understanding/processing. In order to make it as self-contained as\npossible, I spend much time on describing basics of machine learning and neural\nnetworks, only after which how they are used for natural languages is\nintroduced. On the language front, I almost solely focus on language modelling\nand machine translation, two of which I personally find most fascinating and\nmost fundamental to natural language understanding.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 23:23:13 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Cho", "Kyunghyun", ""]]}, {"id": "1511.07944", "submitter": "Dekang Zhu Mr.", "authors": "Dekang Zhu, Dan P. Guralnik, Xuezhi Wang, Xiang Li and Bill Moran", "title": "Maximum Likelihood Estimation for Single Linkage Hierarchical Clustering", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a statistical model for estimation of a dendrogram from single\nlinkage hierarchical clustering (SLHC) that takes account of uncertainty\nthrough noise or corruption in the measurements of separation of data. Our\nfocus is on just the estimation of the hierarchy of partitions afforded by the\ndendrogram, rather than the heights in the latter. The concept of estimating\nthis \"dendrogram structure'' is introduced, and an approximate maximum\nlikelihood estimator (MLE) for the dendrogram structure is described. These\nideas are illustrated by a simple Monte Carlo simulation that, at least for\nsmall data sets, suggests the method outperforms SLHC in the presence of noise.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 03:35:46 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Zhu", "Dekang", ""], ["Guralnik", "Dan P.", ""], ["Wang", "Xuezhi", ""], ["Li", "Xiang", ""], ["Moran", "Bill", ""]]}, {"id": "1511.08102", "submitter": "Matey Neykov", "authors": "Matey Neykov, Jun S. Liu, Tianxi Cai", "title": "L1-Regularized Least Squares for Support Recovery of High Dimensional\n  Single Index Models with Gaussian Designs", "comments": "36 pages; 6 figures; typos corrected; clearer notation introduced", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that for a certain class of single index models (SIMs) $Y =\nf(\\boldsymbol{X}_{p \\times 1}^\\intercal\\boldsymbol{\\beta}_0, \\varepsilon)$,\nsupport recovery is impossible when $\\boldsymbol{X} \\sim \\mathcal{N}(0,\n\\mathbb{I}_{p \\times p})$ and a model complexity adjusted sample size is below\na critical threshold. Recently, optimal algorithms based on Sliced Inverse\nRegression (SIR) were suggested. These algorithms work provably under the\nassumption that the design $\\boldsymbol{X}$ comes from an i.i.d. Gaussian\ndistribution. In the present paper we analyze algorithms based on covariance\nscreening and least squares with $L_1$ penalization (i.e. LASSO) and\ndemonstrate that they can also enjoy optimal (up to a scalar) rescaled sample\nsize in terms of support recovery, albeit under slightly different assumptions\non $f$ and $\\varepsilon$ compared to the SIR based algorithms. Furthermore, we\nshow more generally, that LASSO succeeds in recovering the signed support of\n$\\boldsymbol{\\beta}_0$ if $\\boldsymbol{X} \\sim \\mathcal{N}(0,\n\\boldsymbol{\\Sigma})$, and the covariance $\\boldsymbol{\\Sigma}$ satisfies the\nirrepresentable condition. Our work extends existing results on the support\nrecovery of LASSO for the linear model, to a more general class of SIMs.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 16:00:44 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2016 22:28:16 GMT"}, {"version": "v3", "created": "Thu, 23 Jun 2016 02:46:01 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Neykov", "Matey", ""], ["Liu", "Jun S.", ""], ["Cai", "Tianxi", ""]]}, {"id": "1511.08136", "submitter": "Yisen Wang", "authors": "Yisen Wang, Chaobing Song, Shu-Tao Xia", "title": "Unifying Decision Trees Split Criteria Using Tsallis Entropy", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The construction of efficient and effective decision trees remains a key\ntopic in machine learning because of their simplicity and flexibility. A lot of\nheuristic algorithms have been proposed to construct near-optimal decision\ntrees. ID3, C4.5 and CART are classical decision tree algorithms and the split\ncriteria they used are Shannon entropy, Gain Ratio and Gini index respectively.\nAll the split criteria seem to be independent, actually, they can be unified in\na Tsallis entropy framework. Tsallis entropy is a generalization of Shannon\nentropy and provides a new approach to enhance decision trees' performance with\nan adjustable parameter $q$. In this paper, a Tsallis Entropy Criterion (TEC)\nalgorithm is proposed to unify Shannon entropy, Gain Ratio and Gini index,\nwhich generalizes the split criteria of decision trees. More importantly, we\nreveal the relations between Tsallis entropy with different $q$ and other split\ncriteria. Experimental results on UCI data sets indicate that the TEC algorithm\nachieves statistically significant improvement over the classical algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 17:49:55 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2015 02:29:07 GMT"}, {"version": "v3", "created": "Sat, 5 Dec 2015 08:08:22 GMT"}, {"version": "v4", "created": "Mon, 18 Jan 2016 07:53:55 GMT"}, {"version": "v5", "created": "Tue, 23 Aug 2016 01:02:14 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Wang", "Yisen", ""], ["Song", "Chaobing", ""], ["Xia", "Shu-Tao", ""]]}, {"id": "1511.08327", "submitter": "Nathalie Villa-Vialaneix", "authors": "Robin Genuer (ISPED, SISTM), Jean-Michel Poggi (UPD5, LM-Orsay),\n  Christine Tuleau-Malot (JAD), Nathalie Villa-Vialaneix (MIAT INRA)", "title": "Random Forests for Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big Data is one of the major challenges of statistical science and has\nnumerous consequences from algorithmic and theoretical viewpoints. Big Data\nalways involve massive data but they also often include online data and data\nheterogeneity. Recently some statistical methods have been adapted to process\nBig Data, like linear regression models, clustering methods and bootstrapping\nschemes. Based on decision trees combined with aggregation and bootstrap ideas,\nrandom forests were introduced by Breiman in 2001. They are a powerful\nnonparametric statistical method allowing to consider in a single and versatile\nframework regression problems, as well as two-class and multi-class\nclassification problems. Focusing on classification problems, this paper\nproposes a selective review of available proposals that deal with scaling\nrandom forests to Big Data problems. These proposals rely on parallel\nenvironments or on online adaptations of random forests. We also describe how\nrelated quantities -- such as out-of-bag error and variable importance -- are\naddressed in these methods. Then, we formulate various remarks for random\nforests in the Big Data context. Finally, we experiment five variants on two\nmassive datasets (15 and 120 millions of observations), a simulated one as well\nas real world data. One variant relies on subsampling while three others are\nrelated to parallel implementations of random forests and involve either\nvarious adaptations of bootstrap to Big Data or to \"divide-and-conquer\"\napproaches. The fifth variant relates on online learning of random forests.\nThese numerical experiments lead to highlight the relative performance of the\ndifferent variants, as well as some of their limitations.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 09:04:47 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 14:51:57 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Genuer", "Robin", "", "ISPED, SISTM"], ["Poggi", "Jean-Michel", "", "UPD5, LM-Orsay"], ["Tuleau-Malot", "Christine", "", "JAD"], ["Villa-Vialaneix", "Nathalie", "", "MIAT INRA"]]}, {"id": "1511.08343", "submitter": "Jaesik Choi", "authors": "Yunseong Hwang, Anh Tong and Jaesik Choi", "title": "The Automatic Statistician: A Relational Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Processes (GPs) provide a general and analytically tractable way of\nmodeling complex time-varying, nonparametric functions. The Automatic Bayesian\nCovariance Discovery (ABCD) system constructs natural-language description of\ntime-series data by treating unknown time-series data nonparametrically using\nGP with a composite covariance kernel function. Unfortunately, learning a\ncomposite covariance kernel with a single time-series data set often results in\nless informative kernel that may not give qualitative, distinctive descriptions\nof data. We address this challenge by proposing two relational kernel learning\nmethods which can model multiple time-series data sets by finding common,\nshared causes of changes. We show that the relational kernel learning methods\nfind more accurate models for regression problems on several real-world data\nsets; US stock data, US house price index data and currency exchange rate data.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 10:26:51 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2016 03:08:12 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Hwang", "Yunseong", ""], ["Tong", "Anh", ""], ["Choi", "Jaesik", ""]]}, {"id": "1511.08400", "submitter": "David Krueger", "authors": "David Krueger, Roland Memisevic", "title": "Regularizing RNNs by Stabilizing Activations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We stabilize the activations of Recurrent Neural Networks (RNNs) by\npenalizing the squared distance between successive hidden states' norms.\n  This penalty term is an effective regularizer for RNNs including LSTMs and\nIRNNs, improving performance on character-level language modeling and phoneme\nrecognition, and outperforming weight noise and dropout.\n  We achieve competitive performance (18.6\\% PER) on the TIMIT phoneme\nrecognition task for RNNs evaluated without beam search or an RNN transducer.\n  With this penalty term, IRNN can achieve similar performance to LSTM on\nlanguage modeling, although adding the penalty term to the LSTM results in\nsuperior performance.\n  Our penalty term also prevents the exponential growth of IRNN's activations\noutside of their training horizon, allowing them to generalize to much longer\nsequences.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 14:35:27 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2015 04:52:03 GMT"}, {"version": "v3", "created": "Thu, 17 Dec 2015 02:09:00 GMT"}, {"version": "v4", "created": "Fri, 8 Jan 2016 00:58:39 GMT"}, {"version": "v5", "created": "Fri, 5 Feb 2016 04:58:47 GMT"}, {"version": "v6", "created": "Wed, 2 Mar 2016 20:42:08 GMT"}, {"version": "v7", "created": "Tue, 26 Apr 2016 05:21:11 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Krueger", "David", ""], ["Memisevic", "Roland", ""]]}, {"id": "1511.08405", "submitter": "Joon Kwon", "authors": "Joon Kwon and Vianney Perchet", "title": "Gains and Losses are Fundamentally Different in Regret Minimization: The\n  Sparse Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that, in the classical non-stochastic regret minimization\nproblem with $d$ decisions, gains and losses to be respectively maximized or\nminimized are fundamentally different. Indeed, by considering the additional\nsparsity assumption (at each stage, at most $s$ decisions incur a nonzero\noutcome), we derive optimal regret bounds of different orders. Specifically,\nwith gains, we obtain an optimal regret guarantee after $T$ stages of order\n$\\sqrt{T\\log s}$, so the classical dependency in the dimension is replaced by\nthe sparsity size. With losses, we provide matching upper and lower bounds of\norder $\\sqrt{Ts\\log(d)/d}$, which is decreasing in $d$. Eventually, we also\nstudy the bandit setting, and obtain an upper bound of order $\\sqrt{Ts\\log\n(d/s)}$ when outcomes are losses. This bound is proven to be optimal up to the\nlogarithmic factor $\\sqrt{\\log(d/s)}$.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 14:53:00 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Kwon", "Joon", ""], ["Perchet", "Vianney", ""]]}, {"id": "1511.08551", "submitter": "Xinyang Yi", "authors": "Xinyang Yi and Constantine Caramanis", "title": "Regularized EM Algorithms: A Unified Framework and Statistical\n  Guarantees", "comments": "53 pages, 3 figures. A shorter version appears in NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent variable models are a fundamental modeling tool in machine learning\napplications, but they present significant computational and analytical\nchallenges. The popular EM algorithm and its variants, is a much used\nalgorithmic tool; yet our rigorous understanding of its performance is highly\nincomplete. Recently, work in Balakrishnan et al. (2014) has demonstrated that\nfor an important class of problems, EM exhibits linear local convergence. In\nthe high-dimensional setting, however, the M-step may not be well defined. We\naddress precisely this setting through a unified treatment using\nregularization. While regularization for high-dimensional problems is by now\nwell understood, the iterative EM algorithm requires a careful balancing of\nmaking progress towards the solution while identifying the right structure\n(e.g., sparsity or low-rank). In particular, regularizing the M-step using the\nstate-of-the-art high-dimensional prescriptions (e.g., Wainwright (2014)) is\nnot guaranteed to provide this balance. Our algorithm and analysis are linked\nin a way that reveals the balance between optimization and statistical errors.\nWe specialize our general framework to sparse gaussian mixture models,\nhigh-dimensional mixed regression, and regression with missing variables,\nobtaining statistical guarantees for each of these examples.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 03:46:36 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2015 09:54:59 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Yi", "Xinyang", ""], ["Caramanis", "Constantine", ""]]}, {"id": "1511.08681", "submitter": "Christos Dimitrakakis", "authors": "Aristide Tossou, Christos Dimitrakakis", "title": "Algorithms for Differentially Private Multi-Armed Bandits", "comments": null, "journal-ref": "AAAI 2016, Feb 2016, Phoenix, Arizona, United States", "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present differentially private algorithms for the stochastic Multi-Armed\nBandit (MAB) problem. This is a problem for applications such as adaptive\nclinical trials, experiment design, and user-targeted advertising where private\ninformation is connected to individual rewards. Our major contribution is to\nshow that there exist $(\\epsilon, \\delta)$ differentially private variants of\nUpper Confidence Bound algorithms which have optimal regret, $O(\\epsilon^{-1} +\n\\log T)$. This is a significant improvement over previous results, which only\nachieve poly-log regret $O(\\epsilon^{-2} \\log^{2} T)$, because of our use of a\nnovel interval-based mechanism. We also substantially improve the bounds of\nprevious family of algorithms which use a continual release mechanism.\nExperiments clearly validate our theoretical bounds.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 14:16:00 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Tossou", "Aristide", ""], ["Dimitrakakis", "Christos", ""]]}, {"id": "1511.08768", "submitter": "Neeraja Sahasrabudhe", "authors": "Vivek S. Borkar, Vikranth R. Dwaracherla, Neeraja Sahasrabudhe", "title": "Gradient Estimation with Simultaneous Perturbation and Compressive\n  Sensing", "comments": "24 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at achieving a \"good\" estimator for the gradient of a\nfunction on a high-dimensional space. Often such functions are not sensitive in\nall coordinates and the gradient of the function is almost sparse. We propose a\nmethod for gradient estimation that combines ideas from Spall's Simultaneous\nPerturbation Stochastic Approximation with compressive sensing. The aim is to\nobtain \"good\" estimator without too many function evaluations. Application to\nestimating gradient outer product matrix as well as standard optimization\nproblems are illustrated via simulations.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 18:51:29 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 09:44:07 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Borkar", "Vivek S.", ""], ["Dwaracherla", "Vikranth R.", ""], ["Sahasrabudhe", "Neeraja", ""]]}, {"id": "1511.08895", "submitter": "Murat A. Erdogdu", "authors": "Murat A. Erdogdu", "title": "Newton-Stein Method: An optimization method for GLMs via Stein's Lemma", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of efficiently computing the maximum likelihood\nestimator in Generalized Linear Models (GLMs) when the number of observations\nis much larger than the number of coefficients ($n \\gg p \\gg 1$). In this\nregime, optimization algorithms can immensely benefit from approximate second\norder information. We propose an alternative way of constructing the curvature\ninformation by formulating it as an estimation problem and applying a\nStein-type lemma, which allows further improvements through sub-sampling and\neigenvalue thresholding. Our algorithm enjoys fast convergence rates,\nresembling that of second order methods, with modest per-iteration cost. We\nprovide its convergence analysis for the general case where the rows of the\ndesign matrix are samples from a sub-gaussian distribution. We show that the\nconvergence has two phases, a quadratic phase followed by a linear phase.\nFinally, we empirically demonstrate that our algorithm achieves the highest\nperformance compared to various algorithms on several datasets.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2015 13:19:18 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Erdogdu", "Murat A.", ""]]}, {"id": "1511.08963", "submitter": "Bryon Aragam", "authors": "Bryon Aragam, Arash A. Amini, Qing Zhou", "title": "Learning Directed Acyclic Graphs with Penalized Neighbourhood Regression", "comments": "54 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a family of regularized score-based estimators for learning the\nstructure of a directed acyclic graph (DAG) for a multivariate normal\ndistribution from high-dimensional data with $p\\gg n$. Our main results\nestablish support recovery guarantees and deviation bounds for a family of\npenalized least-squares estimators under concave regularization without\nassuming prior knowledge of a variable ordering. These results apply to a\nvariety of practical situations that allow for arbitrary nondegenerate\ncovariance structures as well as many popular regularizers including the MCP,\nSCAD, $\\ell_{0}$ and $\\ell_{1}$. The proof relies on interpreting a DAG as a\nrecursive linear structural equation model, which reduces the estimation\nproblem to a series of neighbourhood regressions. We provide a novel\nstatistical analysis of these neighbourhood problems, establishing uniform\ncontrol over the superexponential family of neighbourhoods associated with a\nGaussian distribution. We then apply these results to study the statistical\nproperties of score-based DAG estimators, learning causal DAGs, and inferring\nconditional independence relations via graphical models. Our results\nyield---for the first time---finite-sample guarantees for structure learning of\nGaussian DAGs in high-dimensions via score-based estimation.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2015 03:52:28 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2015 19:41:19 GMT"}, {"version": "v3", "created": "Mon, 2 Oct 2017 02:59:27 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Aragam", "Bryon", ""], ["Amini", "Arash A.", ""], ["Zhou", "Qing", ""]]}, {"id": "1511.09107", "submitter": "K. Ch. Chatzisavvas", "authors": "Panagiotis Stalidis, Maria Giatsoglou, Konstantinos Diamantaras,\n  George Sarigiannidis, Konstantinos Ch. Chatzisavvas", "title": "Machine Learning Sentiment Prediction based on Hybrid Document\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated sentiment analysis and opinion mining is a complex process\nconcerning the extraction of useful subjective information from text. The\nexplosion of user generated content on the Web, especially the fact that\nmillions of users, on a daily basis, express their opinions on products and\nservices to blogs, wikis, social networks, message boards, etc., render the\nreliable, automated export of sentiments and opinions from unstructured text\ncrucial for several commercial applications. In this paper, we present a novel\nhybrid vectorization approach for textual resources that combines a weighted\nvariant of the popular Word2Vec representation (based on Term Frequency-Inverse\nDocument Frequency) representation and with a Bag- of-Words representation and\na vector of lexicon-based sentiment values. The proposed text representation\napproach is assessed through the application of several machine learning\nclassification algorithms on a dataset that is used extensively in literature\nfor sentiment detection. The classification accuracy derived through the\nproposed hybrid vectorization approach is higher than when its individual\ncomponents are used for text represenation, and comparable with\nstate-of-the-art sentiment detection methodologies.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2015 22:41:43 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Stalidis", "Panagiotis", ""], ["Giatsoglou", "Maria", ""], ["Diamantaras", "Konstantinos", ""], ["Sarigiannidis", "George", ""], ["Chatzisavvas", "Konstantinos Ch.", ""]]}, {"id": "1511.09123", "submitter": "Ka-Chun Wong", "authors": "Ka-Chun Wong", "title": "A Short Survey on Data Clustering Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With rapidly increasing data, clustering algorithms are important tools for\ndata analytics in modern research. They have been successfully applied to a\nwide range of domains; for instance, bioinformatics, speech recognition, and\nfinancial analysis. Formally speaking, given a set of data instances, a\nclustering algorithm is expected to divide the set of data instances into the\nsubsets which maximize the intra-subset similarity and inter-subset\ndissimilarity, where a similarity measure is defined beforehand. In this work,\nthe state-of-the-arts clustering algorithms are reviewed from design concept to\nmethodology; Different clustering paradigms are discussed. Advanced clustering\nalgorithms are also discussed. After that, the existing clustering evaluation\nmetrics are reviewed. A summary with future insights is provided at the end.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 08:02:37 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Wong", "Ka-Chun", ""]]}, {"id": "1511.09153", "submitter": "Yangyang Xu", "authors": "Yangyang Xu, Ioannis Akrotirianakis, Amit Chakraborty", "title": "Alternating direction method of multipliers for regularized multiclass\n  support vector machines", "comments": "in Lecture Notes in Computer Science (LNCS) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The support vector machine (SVM) was originally designed for binary\nclassifications. A lot of effort has been put to generalize the binary SVM to\nmulticlass SVM (MSVM) which are more complex problems. Initially, MSVMs were\nsolved by considering their dual formulations which are quadratic programs and\ncan be solved by standard second-order methods. However, the duals of MSVMs\nwith regularizers are usually more difficult to formulate and computationally\nvery expensive to solve. This paper focuses on several regularized MSVMs and\nextends the alternating direction method of multiplier (ADMM) to these MSVMs.\nUsing a splitting technique, all considered MSVMs are written as two-block\nconvex programs, for which the ADMM has global convergence guarantees.\nNumerical experiments on synthetic and real data demonstrate the high\nefficiency and accuracy of our algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 04:47:50 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Xu", "Yangyang", ""], ["Akrotirianakis", "Ioannis", ""], ["Chakraborty", "Amit", ""]]}, {"id": "1511.09159", "submitter": "Yangyang Xu", "authors": "Yangyang Xu, Ioannis Akrotirianakis, Amit Chakraborty", "title": "Proximal gradient method for huberized support vector machine", "comments": "in Pattern analysis and application, 2015", "journal-ref": null, "doi": "10.1007/s10044-015-0485-z", "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Support Vector Machine (SVM) has been used in a wide variety of\nclassification problems. The original SVM uses the hinge loss function, which\nis non-differentiable and makes the problem difficult to solve in particular\nfor regularized SVMs, such as with $\\ell_1$-regularization. This paper\nconsiders the Huberized SVM (HSVM), which uses a differentiable approximation\nof the hinge loss function. We first explore the use of the Proximal Gradient\n(PG) method to solving binary-class HSVM (B-HSVM) and then generalize it to\nmulti-class HSVM (M-HSVM). Under strong convexity assumptions, we show that our\nalgorithm converges linearly. In addition, we give a finite convergence result\nabout the support of the solution, based on which we further accelerate the\nalgorithm by a two-stage method. We present extensive numerical experiments on\nboth synthetic and real datasets which demonstrate the superiority of our\nmethods over some state-of-the-art methods for both binary- and multi-class\nSVMs.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 05:02:02 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Xu", "Yangyang", ""], ["Akrotirianakis", "Ioannis", ""], ["Chakraborty", "Amit", ""]]}, {"id": "1511.09392", "submitter": "Krzysztof Wo{\\l}k", "authors": "Agnieszka Wo{\\l}k, Krzysztof Wo{\\l}k, Krzysztof Marasek", "title": "Enhancements in statistical spoken language translation by\n  de-normalization of ASR results", "comments": "International Academy Publishing. arXiv admin note: text overlap with\n  arXiv:1510.04500", "journal-ref": "Journal of Computers, 2016 VOL 11, ISSN: 1796-203X, p. 33-40, 2016", "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoken language translation (SLT) has become very important in an\nincreasingly globalized world. Machine translation (MT) for automatic speech\nrecognition (ASR) systems is a major challenge of great interest. This research\ninvestigates that automatic sentence segmentation of speech that is important\nfor enriching speech recognition output and for aiding downstream language\nprocessing. This article focuses on the automatic sentence segmentation of\nspeech and improving MT results. We explore the problem of identifying sentence\nboundaries in the transcriptions produced by automatic speech recognition\nsystems in the Polish language. We also experiment with reverse normalization\nof the recognized speech samples.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 15:34:21 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Wo\u0142k", "Agnieszka", ""], ["Wo\u0142k", "Krzysztof", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1511.09422", "submitter": "Jos\\'e Miguel Hern\\'andez-Lobato", "authors": "Jos\\'e Miguel Hern\\'andez-Lobato, Michael A. Gelbart, Ryan P. Adams,\n  Matthew W. Hoffman and Zoubin Ghahramani", "title": "A General Framework for Constrained Bayesian Optimization using\n  Information-based Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an information-theoretic framework for solving global black-box\noptimization problems that also have black-box constraints. Of particular\ninterest to us is to efficiently solve problems with decoupled constraints, in\nwhich subsets of the objective and constraint functions may be evaluated\nindependently. For example, when the objective is evaluated on a CPU and the\nconstraints are evaluated independently on a GPU. These problems require an\nacquisition function that can be separated into the contributions of the\nindividual function evaluations. We develop one such acquisition function and\ncall it Predictive Entropy Search with Constraints (PESC). PESC is an\napproximation to the expected information gain criterion and it compares\nfavorably to alternative approaches based on improvement in several synthetic\nand real-world problems. In addition to this, we consider problems with a mix\nof functions that are fast and slow to evaluate. These problems require\nbalancing the amount of time spent in the meta-computation of PESC and in the\nactual evaluation of the target objective. We take a bounded rationality\napproach and develop partial update for PESC which trades off accuracy against\nspeed. We then propose a method for adaptively switching between the partial\nand full updates for PESC. This allows us to interpolate between versions of\nPESC that are efficient in terms of function evaluations and those that are\nefficient in terms of wall-clock time. Overall, we demonstrate that PESC is an\neffective algorithm that provides a promising direction towards a unified\nsolution for constrained Bayesian optimization.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 18:36:45 GMT"}, {"version": "v2", "created": "Sun, 4 Sep 2016 20:01:28 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""], ["Gelbart", "Michael A.", ""], ["Adams", "Ryan P.", ""], ["Hoffman", "Matthew W.", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1511.09433", "submitter": "Joel Tropp", "authors": "Samet Oymak and Joel A. Tropp", "title": "Universality laws for randomized dimension reduction, with applications", "comments": "v2 and v3 with technical corrections. Code for reproducing figures\n  available at http://users.cms.caltech.edu/~jtropp/", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimension reduction is the process of embedding high-dimensional data into a\nlower dimensional space to facilitate its analysis. In the Euclidean setting,\none fundamental technique for dimension reduction is to apply a random linear\nmap to the data. This dimension reduction procedure succeeds when it preserves\ncertain geometric features of the set.\n  The question is how large the embedding dimension must be to ensure that\nrandomized dimension reduction succeeds with high probability.\n  This paper studies a natural family of randomized dimension reduction maps\nand a large class of data sets. It proves that there is a phase transition in\nthe success probability of the dimension reduction map as the embedding\ndimension increases. For a given data set, the location of the phase transition\nis the same for all maps in this family. Furthermore, each map has the same\nstability properties, as quantified through the restricted minimum singular\nvalue. These results can be viewed as new universality laws in high-dimensional\nstochastic geometry.\n  Universality laws for randomized dimension reduction have many applications\nin applied mathematics, signal processing, and statistics. They yield design\nprinciples for numerical linear algebra algorithms, for compressed sensing\nmeasurement ensembles, and for random linear codes. Furthermore, these results\nhave implications for the performance of statistical estimation methods under a\nlarge class of random experimental designs.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 19:14:23 GMT"}, {"version": "v2", "created": "Thu, 7 Sep 2017 18:22:19 GMT"}, {"version": "v3", "created": "Sun, 17 Sep 2017 00:45:33 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Oymak", "Samet", ""], ["Tropp", "Joel A.", ""]]}]