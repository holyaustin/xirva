[{"id": "0805.1390", "submitter": "Sanjoy Dasgupta", "authors": "Sanjoy Dasgupta and Yoav Freund", "title": "Random projection trees for vector quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple and computationally efficient scheme for tree-structured vector\nquantization is presented. Unlike previous methods, its quantization error\ndepends only on the intrinsic dimension of the data distribution, rather than\nthe apparent dimension of the space in which the data happen to lie.\n", "versions": [{"version": "v1", "created": "Fri, 9 May 2008 17:52:42 GMT"}], "update_date": "2008-05-12", "authors_parsed": [["Dasgupta", "Sanjoy", ""], ["Freund", "Yoav", ""]]}, {"id": "0805.2015", "submitter": "Christos Dimitrakakis", "authors": "Christos Dimitrakakis and Michail G. Lagoudakis", "title": "Algorithms and Bounds for Rollout Sampling Approximate Policy Iteration", "comments": "14 pages, presented at EWRL'08", "journal-ref": null, "doi": null, "report-no": "IAS-UVA-08-03", "categories": "stat.ML cs.LO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several approximate policy iteration schemes without value functions, which\nfocus on policy representation using classifiers and address policy learning as\na supervised learning problem, have been proposed recently. Finding good\npolicies with such methods requires not only an appropriate classifier, but\nalso reliable examples of best actions, covering the state space sufficiently.\nUp to this time, little work has been done on appropriate covering schemes and\non methods for reducing the sample complexity of such methods, especially in\ncontinuous state spaces. This paper focuses on the simplest possible covering\nscheme (a discretized grid over the state space) and performs a\nsample-complexity comparison between the simplest (and previously commonly\nused) rollout sampling allocation strategy, which allocates samples equally at\neach state under consideration, and an almost as simple method, which allocates\nsamples only as needed and requires significantly fewer samples.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2008 11:20:29 GMT"}, {"version": "v2", "created": "Fri, 18 Jul 2008 14:26:35 GMT"}], "update_date": "2009-12-30", "authors_parsed": [["Dimitrakakis", "Christos", ""], ["Lagoudakis", "Michail G.", ""]]}, {"id": "0805.2744", "submitter": "Fionn Murtagh", "authors": "Fionn Murtagh", "title": "Symmetry in Data Mining and Analysis: A Unifying View based on Hierarchy", "comments": "35 pages, 3 figures, 84 references", "journal-ref": "Proceedings of Steklov Institute of Mathematics, 265, 177-198,\n  2009", "doi": "10.1134/S0081543809020175", "report-no": null, "categories": "stat.ML math.GM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data analysis and data mining are concerned with unsupervised pattern finding\nand structure determination in data sets. The data sets themselves are\nexplicitly linked as a form of representation to an observational or otherwise\nempirical domain of interest. \"Structure\" has long been understood as symmetry\nwhich can take many forms with respect to any transformation, including point,\ntranslational, rotational, and many others. Beginning with the role of number\ntheory in expressing data, we show how we can naturally proceed to hierarchical\nstructures. We show how this both encapsulates traditional paradigms in data\nanalysis, and also opens up new perspectives towards issues that are on the\norder of the day, including data mining of massive, high dimensional,\nheterogeneous data sets. Linkages with other fields are also discussed\nincluding computational logic and symbolic dynamics. The structures in data\nsurveyed here are based on hierarchy, represented as p-adic numbers or an\nultrametric topology.\n", "versions": [{"version": "v1", "created": "Sun, 18 May 2008 18:47:56 GMT"}, {"version": "v2", "created": "Fri, 30 Jan 2009 00:29:10 GMT"}, {"version": "v3", "created": "Mon, 1 Jun 2009 12:39:11 GMT"}], "update_date": "2011-01-11", "authors_parsed": [["Murtagh", "Fionn", ""]]}, {"id": "0805.3005", "submitter": "Dapo Omidiran", "authors": "Dapo Omidiran, Martin J. Wainwright", "title": "High-dimensional subset recovery in noise: Sparsified measurements\n  without loss of statistical efficiency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the support of a vector $\\beta^* \\in\n\\mathbb{R}^{p}$ based on observations contaminated by noise. A significant body\nof work has studied behavior of $\\ell_1$-relaxations when applied to\nmeasurement matrices drawn from standard dense ensembles (e.g., Gaussian,\nBernoulli). In this paper, we analyze \\emph{sparsified} measurement ensembles,\nand consider the trade-off between measurement sparsity, as measured by the\nfraction $\\gamma$ of non-zero entries, and the statistical efficiency, as\nmeasured by the minimal number of observations $n$ required for exact support\nrecovery with probability converging to one. Our main result is to prove that\nit is possible to let $\\gamma \\to 0$ at some rate, yielding measurement\nmatrices with a vanishing fraction of non-zeros per row while retaining the\nsame statistical efficiency as dense ensembles. A variety of simulation results\nconfirm the sharpness of our theoretical predictions.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2008 05:22:12 GMT"}], "update_date": "2008-05-21", "authors_parsed": [["Omidiran", "Dapo", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "0805.3273", "submitter": "Pranab K. Sen", "authors": "Pranab K. Sen", "title": "Kendall's tau in high-dimensional genomic parsimony", "comments": "Published in at http://dx.doi.org/10.1214/074921708000000183 the IMS\n  Collections (http://www.imstat.org/publications/imscollections.htm) by the\n  Institute of Mathematical Statistics (http://www.imstat.org)", "journal-ref": "IMS Collections 2008, Vol. 3, 251-266", "doi": "10.1214/074921708000000183", "report-no": "IMS-COLL3-IMSCOLL318", "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional data models, often with low sample size, abound in many\ninterdisciplinary studies, genomics and large biological systems being most\nnoteworthy. The conventional assumption of multinormality or linearity of\nregression may not be plausible for such models which are likely to be\nstatistically complex due to a large number of parameters as well as various\nunderlying restraints. As such, parametric approaches may not be very\neffective. Anything beyond parametrics, albeit, having increased scope and\nrobustness perspectives, may generally be baffled by the low sample size and\nhence unable to give reasonable margins of errors. Kendall's tau statistic is\nexploited in this context with emphasis on dimensional rather than sample size\nasymptotics. The Chen--Stein theorem has been thoroughly appraised in this\nstudy. Applications of these findings in some microarray data models are\nillustrated.\n", "versions": [{"version": "v1", "created": "Wed, 21 May 2008 13:13:30 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Sen", "Pranab K.", ""]]}, {"id": "0805.3286", "submitter": "Jennifer Clarke", "authors": "Jennifer Clarke, David Seo", "title": "An ensemble approach to improved prediction from multitype data", "comments": "Published in at http://dx.doi.org/10.1214/074921708000000219 the IMS\n  Collections (http://www.imstat.org/publications/imscollections.htm) by the\n  Institute of Mathematical Statistics (http://www.imstat.org)", "journal-ref": "IMS Collections 2008, Vol. 3, 302-317", "doi": "10.1214/074921708000000219", "report-no": "IMS-COLL3-IMSCOLL321", "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed a strategy for the analysis of newly available binary data\nto improve outcome predictions based on existing data (binary or non-binary).\nOur strategy involves two modeling approaches for the newly available data, one\ncombining binary covariate selection via LASSO with logistic regression and one\nbased on logic trees. The results of these models are then compared to the\nresults of a model based on existing data with the objective of combining model\nresults to achieve the most accurate predictions. The combination of model\npredictions is aided by the use of support vector machines to identify\nsubspaces of the covariate space in which specific models lead to successful\npredictions. We demonstrate our approach in the analysis of single nucleotide\npolymorphism (SNP) data and traditional clinical risk factors for the\nprediction of coronary heart disease.\n", "versions": [{"version": "v1", "created": "Wed, 21 May 2008 14:09:21 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Clarke", "Jennifer", ""], ["Seo", "David", ""]]}]