[{"id": "1705.00002", "submitter": "Danil Kuzin", "authors": "Danil Kuzin, Olga Isupova, Lyudmila Mihaylova", "title": "Compressive Sensing Approaches for Autonomous Object Detection in Video\n  Sequences", "comments": "SDF 2015", "journal-ref": null, "doi": "10.1109/SDF.2015.7347706", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video analytics requires operating with large amounts of data. Compressive\nsensing allows to reduce the number of measurements required to represent the\nvideo using the prior knowledge of sparsity of the original signal, but it\nimposes certain conditions on the design matrix. The Bayesian compressive\nsensing approach relaxes the limitations of the conventional approach using the\nprobabilistic reasoning and allows to include different prior knowledge about\nthe signal structure. This paper presents two Bayesian compressive sensing\nmethods for autonomous object detection in a video sequence from a static\ncamera. Their performance is compared on the real datasets with the\nnon-Bayesian greedy algorithm. It is shown that the Bayesian methods can\nprovide the same accuracy as the greedy algorithm but much faster; or if the\ncomputational time is not critical they can provide more accurate results.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 20:24:33 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Kuzin", "Danil", ""], ["Isupova", "Olga", ""], ["Mihaylova", "Lyudmila", ""]]}, {"id": "1705.00003", "submitter": "Qiuping Xu", "authors": "Qiuping Xu and Vikas Sharma", "title": "Ensemble Sales Forecasting Study in Semiconductor Industry", "comments": "14 pages, Industrial Conference on Data Mining 2017 (ICDM 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sales forecasting plays a prominent role in business planning and business\nstrategy. The value and importance of advance information is a cornerstone of\nplanning activity, and a well-set forecast goal can guide sale-force more\nefficiently. In this paper CPU sales forecasting of Intel Corporation, a\nmultinational semiconductor industry, was considered. Past sale, future\nbooking, exchange rates, Gross domestic product (GDP) forecasting, seasonality\nand other indicators were innovatively incorporated into the quantitative\nmodeling. Benefit from the recent advances in computation power and software\ndevelopment, millions of models built upon multiple regressions, time series\nanalysis, random forest and boosting tree were executed in parallel. The models\nwith smaller validation errors were selected to form the ensemble model. To\nbetter capture the distinct characteristics, forecasting models were\nimplemented at lead time and lines of business level. The moving windows\nvalidation process automatically selected the models which closely represent\ncurrent market condition. The weekly cadence forecasting schema allowed the\nmodel to response effectively to market fluctuation. Generic variable\nimportance analysis was also developed to increase the model interpretability.\nRather than assuming fixed distribution, this non-parametric permutation\nvariable importance analysis provided a general framework across methods to\nevaluate the variable importance. This variable importance framework can\nfurther extend to classification problem by modifying the mean absolute\npercentage error(MAPE) into misclassify error. Please find the demo code at :\nhttps://github.com/qx0731/ensemble_forecast_methods\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 03:50:15 GMT"}, {"version": "v2", "created": "Tue, 2 May 2017 05:27:14 GMT"}, {"version": "v3", "created": "Tue, 16 May 2017 16:57:22 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Xu", "Qiuping", ""], ["Sharma", "Vikas", ""]]}, {"id": "1705.00092", "submitter": "Rory Donovan-Maiye", "authors": "Gregory R. Johnson, Rory M. Donovan-Maiye, Mary M. Maleckar", "title": "Generative Modeling with Conditional Autoencoders: Building an\n  Integrated Cell", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.CB q-bio.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a conditional generative model to learn variation in cell and\nnuclear morphology and the location of subcellular structures from microscopy\nimages. Our model generalizes to a wide range of subcellular localization and\nallows for a probabilistic interpretation of cell and nuclear morphology and\nstructure localization from fluorescence images. We demonstrate the\neffectiveness of our approach by producing photo-realistic cell images using\nour generative model. The conditional nature of the model provides the ability\nto predict the localization of unobserved structures given cell and nuclear\nmorphology.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 22:50:51 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Johnson", "Gregory R.", ""], ["Donovan-Maiye", "Rory M.", ""], ["Maleckar", "Mary M.", ""]]}, {"id": "1705.00105", "submitter": "Yury Maximov", "authors": "Sumit Sidana, Mikhail Trofimov, Oleg Horodnitskii, Charlotte Laclau,\n  Yury Maximov, Massih-Reza Amini", "title": "Representation Learning and Pairwise Ranking for Implicit Feedback in\n  Recommendation Systems", "comments": "12 pages, 4 figures, 5 tables, Modified version contains updated\n  results of all the models and rectifies some of the earlier issues", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel ranking framework for collaborative\nfiltering with the overall aim of learning user preferences over items by\nminimizing a pairwise ranking loss. We show the minimization problem involves\ndependent random variables and provide a theoretical analysis by proving the\nconsistency of the empirical risk minimization in the worst case where all\nusers choose a minimal number of positive and negative items. We further derive\na Neural-Network model that jointly learns a new representation of users and\nitems in an embedded space as well as the preference relation of users over the\npairs of items. The learning objective is based on three scenarios of ranking\nlosses that control the ability of the model to maintain the ordering over the\nitems induced from the users' preferences, as well as, the capacity of the\ndot-product defined in the learned embedded space to produce the ordering. The\nproposed model is by nature suitable for implicit feedback and involves the\nestimation of only very few parameters. Through extensive experiments on\nseveral real-world benchmarks on implicit data, we show the interest of\nlearning the preference and the embedding simultaneously when compared to\nlearning those separately. We also demonstrate that our approach is very\ncompetitive with the best state-of-the-art collaborative filtering techniques\nproposed for implicit feedback.\n", "versions": [{"version": "v1", "created": "Sat, 29 Apr 2017 01:03:40 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 09:47:02 GMT"}, {"version": "v3", "created": "Wed, 18 Apr 2018 14:35:37 GMT"}, {"version": "v4", "created": "Thu, 12 Jul 2018 09:31:35 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Sidana", "Sumit", ""], ["Trofimov", "Mikhail", ""], ["Horodnitskii", "Oleg", ""], ["Laclau", "Charlotte", ""], ["Maximov", "Yury", ""], ["Amini", "Massih-Reza", ""]]}, {"id": "1705.00219", "submitter": "Amit Dhurandhar", "authors": "Amit Dhurandhar, Steve Hanneke and Liu Yang", "title": "Learning with Changing Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the setting where features are added or change\ninterpretation over time, which has applications in multiple domains such as\nretail, manufacturing, finance. In particular, we propose an approach to\nprovably determine the time instant from which the new/changed features start\nbecoming relevant with respect to an output variable in an agnostic\n(supervised) learning setting. We also suggest an efficient version of our\napproach which has the same asymptotic performance. Moreover, our theory also\napplies when we have more than one such change point. Independent post analysis\nof a change point identified by our method for a large retailer revealed that\nit corresponded in time with certain unflattering news stories about a brand\nthat resulted in the change in customer behavior. We also applied our method to\ndata from an advanced manufacturing plant identifying the time instant from\nwhich downstream features became relevant. To the best of our knowledge this is\nthe first work that formally studies change point detection in a distribution\nindependent agnostic setting, where the change point is based on the changing\nrelationship between input and output.\n", "versions": [{"version": "v1", "created": "Sat, 29 Apr 2017 18:11:10 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Dhurandhar", "Amit", ""], ["Hanneke", "Steve", ""], ["Yang", "Liu", ""]]}, {"id": "1705.00334", "submitter": "Sibi Venkatesan", "authors": "Sibi Venkatesan, James K. Miller, Jeff Schneider and Artur Dubrawski", "title": "Scaling Active Search using Linear Similarity Functions", "comments": "To be published as conference paper at IJCAI 2017, 11 pages, 2\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active Search has become an increasingly useful tool in information retrieval\nproblems where the goal is to discover as many target elements as possible\nusing only limited label queries. With the advent of big data, there is a\ngrowing emphasis on the scalability of such techniques to handle very large and\nvery complex datasets.\n  In this paper, we consider the problem of Active Search where we are given a\nsimilarity function between data points. We look at an algorithm introduced by\nWang et al. [2013] for Active Search over graphs and propose crucial\nmodifications which allow it to scale significantly. Their approach selects\npoints by minimizing an energy function over the graph induced by the\nsimilarity function on the data. Our modifications require the similarity\nfunction to be a dot-product between feature vectors of data points, equivalent\nto having a linear kernel for the adjacency matrix. With this, we are able to\nscale tremendously: for $n$ data points, the original algorithm runs in\n$O(n^2)$ time per iteration while ours runs in only $O(nr + r^2)$ given\n$r$-dimensional features.\n  We also describe a simple alternate approach using a weighted-neighbor\npredictor which also scales well. In our experiments, we show that our method\nis competitive with existing semi-supervised approaches. We also briefly\ndiscuss conditions under which our algorithm performs well.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 15:49:01 GMT"}, {"version": "v2", "created": "Tue, 22 Aug 2017 00:08:00 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Venkatesan", "Sibi", ""], ["Miller", "James K.", ""], ["Schneider", "Jeff", ""], ["Dubrawski", "Artur", ""]]}, {"id": "1705.00375", "submitter": "Natali Ruchansky", "authors": "Natali Ruchansky and Mark Crovella and Evimaria Terzi", "title": "Targeted matrix completion", "comments": "Proceedings of the 2017 SIAM International Conference on Data Mining\n  (SDM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion is a problem that arises in many data-analysis settings\nwhere the input consists of a partially-observed matrix (e.g., recommender\nsystems, traffic matrix analysis etc.). Classical approaches to matrix\ncompletion assume that the input partially-observed matrix is low rank. The\nsuccess of these methods depends on the number of observed entries and the rank\nof the matrix; the larger the rank, the more entries need to be observed in\norder to accurately complete the matrix. In this paper, we deal with matrices\nthat are not necessarily low rank themselves, but rather they contain low-rank\nsubmatrices. We propose Targeted, which is a general framework for completing\nsuch matrices. In this framework, we first extract the low-rank submatrices and\nthen apply a matrix-completion algorithm to these low-rank submatrices as well\nas the remainder matrix separately. Although for the completion itself we use\nstate-of-the-art completion methods, our results demonstrate that Targeted\nachieves significantly smaller reconstruction errors than other classical\nmatrix-completion methods. One of the key technical contributions of the paper\nlies in the identification of the low-rank submatrices from the input\npartially-observed matrices.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 21:30:05 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Ruchansky", "Natali", ""], ["Crovella", "Mark", ""], ["Terzi", "Evimaria", ""]]}, {"id": "1705.00394", "submitter": "Zhenghang Cui", "authors": "Zhenghang Cui, Issei Sato and Masashi Sugiyama", "title": "Stochastic Divergence Minimization for Biterm Topic Model", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": "10.1587/transinf.2017EDP7310", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the emergence and the thriving development of social networks, a huge\nnumber of short texts are accumulated and need to be processed. Inferring\nlatent topics of collected short texts is useful for understanding its hidden\nstructure and predicting new contents. Unlike conventional topic models such as\nlatent Dirichlet allocation (LDA), a biterm topic model (BTM) was recently\nproposed for short texts to overcome the sparseness of document-level word\nco-occurrences by directly modeling the generation process of word pairs.\nStochastic inference algorithms based on collapsed Gibbs sampling (CGS) and\ncollapsed variational inference have been proposed for BTM. However, they\neither require large computational complexity, or rely on very crude\nestimation. In this work, we develop a stochastic divergence minimization\ninference algorithm for BTM to estimate latent topics more accurately in a\nscalable way. Experiments demonstrate the superiority of our proposed algorithm\ncompared with existing inference algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 01:05:09 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Cui", "Zhenghang", ""], ["Sato", "Issei", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1705.00395", "submitter": "Lingzhou Xue", "authors": "Wei Luo, Lingzhou Xue, Jiawei Yao and Xiufan Yu", "title": "Inverse Moment Methods for Sufficient Forecasting using High-Dimensional\n  Predictors", "comments": "21 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider forecasting a single time series using a large number of\npredictors in the presence of a possible nonlinear forecast function. Assuming\nthat the predictors affect the response through the latent factors, we propose\nto first conduct factor analysis and then apply sufficient dimension reduction\non the estimated factors, to derive the reduced data for subsequent\nforecasting. Using directional regression and the inverse third-moment method\nin the stage of sufficient dimension reduction, the proposed methods can\ncapture the non-monotone effect of factors on the response. We also allow a\ndiverging number of factors and only impose general regularity conditions on\nthe distribution of factors, avoiding the undesired time reversibility of the\nfactors by the latter. These make the proposed methods fundamentally more\napplicable than the sufficient forecasting method in Fan et al. (2017). The\nproposed methods are demonstrated in both simulation studies and an empirical\nstudy of forecasting monthly macroeconomic data from 1959 to 2016. Also, our\ntheory contributes to the literature of sufficient dimension reduction, as it\nincludes an invariance result, a path to perform sufficient dimension reduction\nunder the high-dimensional setting without assuming sparsity, and the\ncorresponding order-determination procedure.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 01:10:35 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 04:56:25 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Luo", "Wei", ""], ["Xue", "Lingzhou", ""], ["Yao", "Jiawei", ""], ["Yu", "Xiufan", ""]]}, {"id": "1705.00461", "submitter": "Chavent Marie", "authors": "Marie Chavent, Guy Chavent", "title": "Optimal Projected Variance Group-Sparse Block PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of defining a group sparse formulation for Principal\nComponents Analysis (PCA) - or its equivalent formulations as Low Rank\napproximation or Dictionary Learning problems - which achieves a compromise\nbetween maximizing the variance explained by the components and promoting\nsparsity of the loadings. So we propose first a new definition of the variance\nexplained by non necessarily orthogonal components, which is optimal in some\naspect and compatible with the principal components situation. Then we use a\nspecific regularization of this variance by the group-$\\ell_{1}$ norm to define\na Group Sparse Maximum Variance (GSMV) formulation of PCA. The GSMV formulation\nachieves our objective by construction, and has the nice property that the\ninner non smooth optimization problem can be solved analytically, thus reducing\nGSMV to the maximization of a smooth and convex function under unit norm and\northogonality constraints, which generalizes Journee et al. (2010) to group\nsparsity. Numerical comparison with deflation on synthetic data shows that GSMV\nproduces steadily slightly better and more robust results for the retrieval of\nhidden sparse structures, and is about three times faster on these examples.\nApplication to real data shows the interest of group sparsity for variables\nselection in PCA of mixed data (categorical/numerical) .\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 10:30:05 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 10:51:57 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Chavent", "Marie", ""], ["Chavent", "Guy", ""]]}, {"id": "1705.00470", "submitter": "Thomas Moerland", "authors": "Thomas M. Moerland, Joost Broekens and Catholijn M. Jonker", "title": "Learning Multimodal Transition Dynamics for Model-Based Reinforcement\n  Learning", "comments": "Scaling Up Reinforcement Learning (SURL) Workshop @ European Machine\n  Learning Conference (ECML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study how to learn stochastic, multimodal transition\ndynamics in reinforcement learning (RL) tasks. We focus on evaluating\ntransition function estimation, while we defer planning over this model to\nfuture work. Stochasticity is a fundamental property of many task environments.\nHowever, discriminative function approximators have difficulty estimating\nmultimodal stochasticity. In contrast, deep generative models do capture\ncomplex high-dimensional outcome distributions. First we discuss why, amongst\nsuch models, conditional variational inference (VI) is theoretically most\nappealing for model-based RL. Subsequently, we compare different VI models on\ntheir ability to learn complex stochasticity on simulated functions, as well as\non a typical RL gridworld with multimodal dynamics. Results show VI\nsuccessfully predicts multimodal outcomes, but also robustly ignores these for\ndeterministic parts of the transition dynamics. In summary, we show a robust\nmethod to learn multimodal transitions using function approximation, which is a\nkey preliminary for model-based RL in stochastic domains.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 11:06:04 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 13:50:49 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Moerland", "Thomas M.", ""], ["Broekens", "Joost", ""], ["Jonker", "Catholijn M.", ""]]}, {"id": "1705.00557", "submitter": "Yacine Jernite", "authors": "Yacine Jernite, Samuel R. Bowman and David Sontag", "title": "Discourse-Based Objectives for Fast Unsupervised Sentence Representation\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a novel objective function for the unsupervised training\nof neural network sentence encoders. It exploits signals from paragraph-level\ndiscourse coherence to train these models to understand text. Our objective is\npurely discriminative, allowing us to train models many times faster than was\npossible under prior methods, and it yields models which perform well in\nextrinsic evaluations.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 09:15:35 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Jernite", "Yacine", ""], ["Bowman", "Samuel R.", ""], ["Sontag", "David", ""]]}, {"id": "1705.00607", "submitter": "Cheng Zhang", "authors": "Cheng Zhang, Hedvig Kjellstrom, Stephan Mandt", "title": "Determinantal Point Processes for Mini-Batch Diversification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a mini-batch diversification scheme for stochastic gradient descent\n(SGD). While classical SGD relies on uniformly sampling data points to form a\nmini-batch, we propose a non-uniform sampling scheme based on the Determinantal\nPoint Process (DPP). The DPP relies on a similarity measure between data points\nand gives low probabilities to mini-batches which contain redundant data, and\nhigher probabilities to mini-batches with more diverse data. This\nsimultaneously balances the data and leads to stochastic gradients with lower\nvariance. We term this approach Diversified Mini-Batch SGD (DM-SGD). We show\nthat regular SGD and a biased version of stratified sampling emerge as special\ncases. Furthermore, DM-SGD generalizes stratified sampling to cases where no\ndiscrete features exist to bin the data into groups. We show experimentally\nthat our method results more interpretable and diverse features in unsupervised\nsetups, and in better classification accuracies in supervised setups.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 17:53:51 GMT"}, {"version": "v2", "created": "Wed, 23 Aug 2017 23:34:28 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Zhang", "Cheng", ""], ["Kjellstrom", "Hedvig", ""], ["Mandt", "Stephan", ""]]}, {"id": "1705.00674", "submitter": "Heather Patsolic", "authors": "Heather G. Patsolic, Youngser Park, Vince Lyzinski, Carey E. Priebe", "title": "Vertex Nomination Via Seeded Graph Matching", "comments": "19 pages, 14 (sub)figures, edits: removed investigation of the impact\n  of seeds and moved the material to a supplement that will be available on the\n  webpage indicated in the article, and did some word-smithing to make the\n  article cleaner", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider two networks on overlapping, non-identical vertex sets. Given\nvertices of interest in the first network, we seek to identify the\ncorresponding vertices, if any exist, in the second network. While in\nmoderately sized networks graph matching methods can be applied directly to\nrecover the missing correspondences, herein we present a principled methodology\nappropriate for situations in which the networks are too large for brute-force\ngraph matching. Our methodology identifies vertices in a local neighborhood of\nthe vertices of interest in the first network that have verifiable\ncorresponding vertices in the second network. Leveraging these known\ncorrespondences, referred to as seeds, we match the induced subgraphs in each\nnetwork generated by the neighborhoods of these verified seeds, and rank the\nvertices of the second network in terms of the most likely matches to the\noriginal vertices of interest. We demonstrate the applicability of our\nmethodology through simulations and real data examples.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 19:21:27 GMT"}, {"version": "v2", "created": "Sat, 22 Jul 2017 20:48:12 GMT"}, {"version": "v3", "created": "Sun, 6 Jan 2019 16:59:16 GMT"}, {"version": "v4", "created": "Fri, 2 Aug 2019 05:32:14 GMT"}, {"version": "v5", "created": "Tue, 5 Nov 2019 21:48:02 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Patsolic", "Heather G.", ""], ["Park", "Youngser", ""], ["Lyzinski", "Vince", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1705.00678", "submitter": "Zhao Kang", "authors": "Zhao Kang, Chong Peng, Qiang Cheng", "title": "Twin Learning for Similarity and Clustering: A Unified Kernel Approach", "comments": "Published in AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many similarity-based clustering methods work in two separate steps including\nsimilarity matrix computation and subsequent spectral clustering. However,\nsimilarity measurement is challenging because it is usually impacted by many\nfactors, e.g., the choice of similarity metric, neighborhood size, scale of\ndata, noise and outliers. Thus the learned similarity matrix is often not\nsuitable, let alone optimal, for the subsequent clustering. In addition,\nnonlinear similarity often exists in many real world data which, however, has\nnot been effectively considered by most existing methods. To tackle these two\nchallenges, we propose a model to simultaneously learn cluster indicator matrix\nand similarity information in kernel spaces in a principled way. We show\ntheoretical relationships to kernel k-means, k-means, and spectral clustering\nmethods. Then, to address the practical issue of how to select the most\nsuitable kernel for a particular clustering task, we further extend our model\nwith a multiple kernel learning ability. With this joint model, we can\nautomatically accomplish three subtasks of finding the best cluster indicator\nmatrix, the most accurate similarity relations and the optimal combination of\nmultiple kernels. By leveraging the interactions between these three subtasks\nin a joint framework, each subtask can be iteratively boosted by using the\nresults of the others towards an overall optimal solution. Extensive\nexperiments are performed to demonstrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 19:33:27 GMT"}, {"version": "v2", "created": "Wed, 3 May 2017 00:29:13 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Kang", "Zhao", ""], ["Peng", "Chong", ""], ["Cheng", "Qiang", ""]]}, {"id": "1705.00687", "submitter": "Junming Yin", "authors": "Junming Yin and Yaoliang Yu", "title": "Convex-constrained Sparse Additive Modeling and Its Extensions", "comments": "17 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse additive modeling is a class of effective methods for performing\nhigh-dimensional nonparametric regression. In this work we show how shape\nconstraints such as convexity/concavity and their extensions, can be integrated\ninto additive models. The proposed sparse difference of convex additive models\n(SDCAM) can estimate most continuous functions without any a priori smoothness\nassumption. Motivated by a characterization of difference of convex functions,\nour method incorporates a natural regularization functional to avoid\noverfitting and to reduce model complexity. Computationally, we develop an\nefficient backfitting algorithm with linear per-iteration complexity.\nExperiments on both synthetic and real data verify that our method is\ncompetitive against state-of-the-art sparse additive models, with improved\nperformance in most scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 19:56:03 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Yin", "Junming", ""], ["Yu", "Yaoliang", ""]]}, {"id": "1705.00722", "submitter": "John Paisley", "authors": "San Gultekin and John Paisley", "title": "Nonlinear Kalman Filtering with Divergence Minimization", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2752729", "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the nonlinear Kalman filtering problem using Kullback-Leibler\n(KL) and $\\alpha$-divergence measures as optimization criteria. Unlike linear\nKalman filters, nonlinear Kalman filters do not have closed form Gaussian\nposteriors because of a lack of conjugacy due to the nonlinearity in the\nlikelihood. In this paper we propose novel algorithms to optimize the forward\nand reverse forms of the KL divergence, as well as the alpha-divergence which\ncontains these two as limiting cases. Unlike previous approaches, our\nalgorithms do not make approximations to the divergences being optimized, but\nuse Monte Carlo integration techniques to derive unbiased algorithms for direct\noptimization. We assess performance on radar and sensor tracking, and options\npricing problems, showing general improvement over the UKF and EKF, as well as\ncompetitive performance with particle filtering.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 21:42:54 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Gultekin", "San", ""], ["Paisley", "John", ""]]}, {"id": "1705.00740", "submitter": "Cheng Li", "authors": "Bingyu Wang, Cheng Li, Virgil Pavlu, Javed Aslam", "title": "Regularizing Model Complexity and Label Structure for Multi-Label Text\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label text classification is a popular machine learning task where each\ndocument is assigned with multiple relevant labels. This task is challenging\ndue to high dimensional features and correlated labels. Multi-label text\nclassifiers need to be carefully regularized to prevent the severe over-fitting\nin the high dimensional space, and also need to take into account label\ndependencies in order to make accurate predictions under uncertainty. We\ndemonstrate significant and practical improvement by carefully regularizing the\nmodel complexity during training phase, and also regularizing the label search\nspace during prediction phase. Specifically, we regularize the classifier\ntraining using Elastic-net (L1+L2) penalty for reducing model complexity/size,\nand employ early stopping to prevent overfitting. At prediction time, we apply\nsupport inference to restrict the search space to label sets encountered in the\ntraining set, and F-optimizer GFM to make optimal predictions for the F1\nmetric. We show that although support inference only provides density\nestimations on existing label combinations, when combined with GFM predictor,\nthe algorithm can output unseen label combinations. Taken collectively, our\nexperiments show state of the art results on many benchmark datasets. Beyond\nperformance and practical contributions, we make some interesting observations.\nContrary to the prior belief, which deems support inference as purely an\napproximate inference procedure, we show that support inference acts as a\nstrong regularizer on the label prediction structure. It allows the classifier\nto take into account label dependencies during prediction even if the\nclassifiers had not modeled any label dependencies during training.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 23:30:13 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Wang", "Bingyu", ""], ["Li", "Cheng", ""], ["Pavlu", "Virgil", ""], ["Aslam", "Javed", ""]]}, {"id": "1705.00797", "submitter": "Konstantin Bauman", "authors": "Evgeny Bauman and Konstantin Bauman", "title": "One-Class Semi-Supervised Learning: Detecting Linearly Separable Class\n  by its Mean", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we presented a novel semi-supervised one-class classification\nalgorithm which assumes that class is linearly separable from other elements.\nWe proved theoretically that class is linearly separable if and only if it is\nmaximal by probability within the sets with the same mean. Furthermore, we\npresented an algorithm for identifying such linearly separable class utilizing\nlinear programming. We described three application cases including an\nassumption of linear separability, Gaussian distribution, and the case of\nlinear separability in transformed space of kernel functions. Finally, we\ndemonstrated the work of the proposed algorithm on the USPS dataset and\nanalyzed the relationship of the performance of the algorithm and the size of\nthe initially labeled sample.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 05:00:28 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Bauman", "Evgeny", ""], ["Bauman", "Konstantin", ""]]}, {"id": "1705.00850", "submitter": "Haiping Huang", "authors": "Haiping Huang, and Alireza Goudarzi", "title": "Random active path model of deep neural networks with diluted binary\n  synapses", "comments": "10 pages, 5 figures, with Supplemental Material (upon request)", "journal-ref": "Phys. Rev. E 98, 042311 (2018)", "doi": "10.1103/PhysRevE.98.042311", "report-no": null, "categories": "cs.LG cond-mat.stat-mech stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has become a powerful and popular tool for a variety of machine\nlearning tasks. However, it is challenging to understand the mechanism of deep\nlearning from a theoretical perspective. In this work, we propose a random\nactive path model to study collective properties of deep neural networks with\nbinary synapses, under the removal perturbation of connections between layers.\nIn the model, the path from input to output is randomly activated, and the\ncorresponding input unit constrains the weights along the path into the form of\na $p$-weight interaction glass model. A critical value of the perturbation is\nobserved to separate a spin glass regime from a paramagnetic regime, with the\ntransition being of the first order. The paramagnetic phase is conjectured to\nhave a poor generalization performance.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 08:16:12 GMT"}, {"version": "v2", "created": "Fri, 18 May 2018 00:49:27 GMT"}, {"version": "v3", "created": "Wed, 26 Sep 2018 13:09:33 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Huang", "Haiping", ""], ["Goudarzi", "Alireza", ""]]}, {"id": "1705.00885", "submitter": "Luca Pappalardo", "authors": "Luca Pappalardo and Paolo Cintia", "title": "Quantifying the relation between performance and success in soccer", "comments": null, "journal-ref": "Advances in Complex Systems, 2017", "doi": "10.1142/S021952591750014X", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of massive data about sports activities offers nowadays the\nopportunity to quantify the relation between performance and success. In this\nstudy, we analyze more than 6,000 games and 10 million events in six European\nleagues and investigate this relation in soccer competitions. We discover that\na team's position in a competition's final ranking is significantly related to\nits typical performance, as described by a set of technical features extracted\nfrom the soccer data. Moreover we find that, while victory and defeats can be\nexplained by the team's performance during a game, it is difficult to detect\ndraws by using a machine learning approach. We then simulate the outcomes of an\nentire season of each league only relying on technical data, i.e. excluding the\ngoals scored, exploiting a machine learning model trained on data from past\nseasons. The simulation produces a team ranking (the PC ranking) which is close\nto the actual ranking, suggesting that a complex systems' view on soccer has\nthe potential of revealing hidden patterns regarding the relation between\nperformance and success.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 10:05:59 GMT"}, {"version": "v2", "created": "Fri, 11 Aug 2017 20:14:50 GMT"}, {"version": "v3", "created": "Tue, 26 Sep 2017 11:07:06 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Pappalardo", "Luca", ""], ["Cintia", "Paolo", ""]]}, {"id": "1705.00891", "submitter": "Syed Ali Asad Rizvi", "authors": "Syed Ali Asad Rizvi, Stephen J. Roberts, Michael A. Osborne and Favour\n  Nyikosa", "title": "A Novel Approach to Forecasting Financial Volatility with Gaussian\n  Process Envelopes", "comments": "16 pages, 8 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CE q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we use Gaussian Process (GP) regression to propose a novel\napproach for predicting volatility of financial returns by forecasting the\nenvelopes of the time series. We provide a direct comparison of their\nperformance to traditional approaches such as GARCH. We compare the forecasting\npower of three approaches: GP regression on the absolute and squared returns;\nregression on the envelope of the returns and the absolute returns; and\nregression on the envelope of the negative and positive returns separately. We\nuse a maximum a posteriori estimate with a Gaussian prior to determine our\nhyperparameters. We also test the effect of hyperparameter updating at each\nforecasting step. We use our approaches to forecast out-of-sample volatility of\nfour currency pairs over a 2 year period, at half-hourly intervals. From three\nkernels, we select the kernel giving the best performance for our data. We use\ntwo published accuracy measures and four statistical loss functions to evaluate\nthe forecasting ability of GARCH vs GPs. In mean squared error the GP's perform\n20% better than a random walk model, and 50% better than GARCH for the same\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 10:30:13 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Rizvi", "Syed Ali Asad", ""], ["Roberts", "Stephen J.", ""], ["Osborne", "Michael A.", ""], ["Nyikosa", "Favour", ""]]}, {"id": "1705.00919", "submitter": "Soumitro Chakrabarty", "authors": "Soumitro Chakrabarty and Emanu\\\"el. A. P. Habets", "title": "Broadband DOA estimation using Convolutional neural networks trained\n  with noise signals", "comments": "Published in Proceedings of IEEE Workshop on Applications of Signal\n  Processing to Audio and Acoustics (WASPAA) 2017", "journal-ref": null, "doi": "10.1109/WASPAA.2017.8170010", "report-no": null, "categories": "cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A convolution neural network (CNN) based classification method for broadband\nDOA estimation is proposed, where the phase component of the short-time Fourier\ntransform coefficients of the received microphone signals are directly fed into\nthe CNN and the features required for DOA estimation are learnt during\ntraining. Since only the phase component of the input is used, the CNN can be\ntrained with synthesized noise signals, thereby making the preparation of the\ntraining data set easier compared to using speech signals. Through experimental\nevaluation, the ability of the proposed noise trained CNN framework to\ngeneralize to speech sources is demonstrated. In addition, the robustness of\nthe system to noise, small perturbations in microphone positions, as well as\nits ability to adapt to different acoustic conditions is investigated using\nexperiments with simulated and real data.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 11:31:43 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 12:57:36 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Chakrabarty", "Soumitro", ""], ["Habets", "Emanu\u00ebl. A. P.", ""]]}, {"id": "1705.00956", "submitter": "Gal Shulkind", "authors": "Gal Shulkind, Lior Horesh, Haim Avron", "title": "Experimental Design for Non-Parametric Correction of Misspecified\n  Dynamical Models", "comments": "A couple of (??) were corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a class of misspecified dynamical models where the governing term\nis only approximately known. Under the assumption that observations of the\nsystem's evolution are accessible for various initial conditions, our goal is\nto infer a non-parametric correction to the misspecified driving term such as\nto faithfully represent the system dynamics and devise system evolution\npredictions for unobserved initial conditions.\n  We model the unknown correction term as a Gaussian Process and analyze the\nproblem of efficient experimental design to find an optimal correction term\nunder constraints such as a limited experimental budget. We suggest a novel\nformulation for experimental design for this Gaussian Process and show that\napproximately optimal (up to a constant factor) designs may be efficiently\nderived by utilizing results from the literature on submodular optimization.\nOur numerical experiments exemplify the effectiveness of these techniques.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 13:23:53 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 14:24:30 GMT"}, {"version": "v3", "created": "Sun, 4 Jun 2017 19:33:02 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Shulkind", "Gal", ""], ["Horesh", "Lior", ""], ["Avron", "Haim", ""]]}, {"id": "1705.00995", "submitter": "Amir Karami", "authors": "Amir Karami and Aryya Gangopadhyay and Bin Zhou and Hadi Kharrazi", "title": "Fuzzy Approach Topic Discovery in Health and Medical Corpora", "comments": "12 Pages, International Journal of Fuzzy Systems, 2017", "journal-ref": null, "doi": "10.1007/s40815-017-0327-9", "report-no": null, "categories": "stat.ML cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of medical documents and electronic health records (EHRs) are in\ntext format that poses a challenge for data processing and finding relevant\ndocuments. Looking for ways to automatically retrieve the enormous amount of\nhealth and medical knowledge has always been an intriguing topic. Powerful\nmethods have been developed in recent years to make the text processing\nautomatic. One of the popular approaches to retrieve information based on\ndiscovering the themes in health & medical corpora is topic modeling, however,\nthis approach still needs new perspectives. In this research we describe fuzzy\nlatent semantic analysis (FLSA), a novel approach in topic modeling using fuzzy\nperspective. FLSA can handle health & medical corpora redundancy issue and\nprovides a new method to estimate the number of topics. The quantitative\nevaluations show that FLSA produces superior performance and features to latent\nDirichlet allocation (LDA), the most popular topic model.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 14:29:14 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 02:41:00 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Karami", "Amir", ""], ["Gangopadhyay", "Aryya", ""], ["Zhou", "Bin", ""], ["Kharrazi", "Hadi", ""]]}, {"id": "1705.01015", "submitter": "Jens Behrmann", "authors": "Jens Behrmann, Christian Etmann, Tobias Boskamp, Rita Casadonte,\n  J\\\"org Kriegsmann, Peter Maass", "title": "Deep Learning for Tumor Classification in Imaging Mass Spectrometry", "comments": "10 pages, 5 figures", "journal-ref": "Bioinformatics, 2018, Volume 34, Issue 7, Pages 1215-1223", "doi": "10.1093/bioinformatics/btx724", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Tumor classification using Imaging Mass Spectrometry (IMS) data\nhas a high potential for future applications in pathology. Due to the\ncomplexity and size of the data, automated feature extraction and\nclassification steps are required to fully process the data. Deep learning\noffers an approach to learn feature extraction and classification combined in a\nsingle model. Commonly these steps are handled separately in IMS data analysis,\nhence deep learning offers an alternative strategy worthwhile to explore.\nResults: Methodologically, we propose an adapted architecture based on deep\nconvolutional networks to handle the characteristics of mass spectrometry data,\nas well as a strategy to interpret the learned model in the spectral domain\nbased on a sensitivity analysis. The proposed methods are evaluated on two\nchallenging tumor classification tasks and compared to a baseline approach.\nCompetitiveness of the proposed methods are shown on both tasks by studying the\nperformance via cross-validation. Moreover, the learned models are analyzed by\nthe proposed sensitivity analysis revealing biologically plausible effects as\nwell as confounding factors of the considered task. Thus, this study may serve\nas a starting point for further development of deep learning approaches in IMS\nclassification tasks.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 15:15:19 GMT"}, {"version": "v2", "created": "Mon, 15 May 2017 14:34:05 GMT"}, {"version": "v3", "created": "Fri, 20 Oct 2017 10:03:22 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Behrmann", "Jens", ""], ["Etmann", "Christian", ""], ["Boskamp", "Tobias", ""], ["Casadonte", "Rita", ""], ["Kriegsmann", "J\u00f6rg", ""], ["Maass", "Peter", ""]]}, {"id": "1705.01024", "submitter": "Yinchu Zhu", "authors": "Yinchu Zhu and Jelena Bradic", "title": "A projection pursuit framework for testing general high-dimensional\n  hypothesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article develops a framework for testing general hypothesis in\nhigh-dimensional models where the number of variables may far exceed the number\nof observations. Existing literature has considered less than a handful of\nhypotheses, such as testing individual coordinates of the model parameter.\nHowever, the problem of testing general and complex hypotheses remains widely\nopen. We propose a new inference method developed around the hypothesis\nadaptive projection pursuit framework, which solves the testing problems in the\nmost general case. The proposed inference is centered around a new class of\nestimators defined as $l_1$ projection of the initial guess of the unknown onto\nthe space defined by the null. This projection automatically takes into account\nthe structure of the null hypothesis and allows us to study formal inference\nfor a number of long-standing problems. For example, we can directly conduct\ninference on the sparsity level of the model parameters and the minimum signal\nstrength. This is especially significant given the fact that the former is a\nfundamental condition underlying most of the theoretical development in\nhigh-dimensional statistics, while the latter is a key condition used to\nestablish variable selection properties. Moreover, the proposed method is\nasymptotically exact and has satisfactory power properties for testing very\ngeneral functionals of the high-dimensional parameters. The simulation studies\nlend further support to our theoretical claims and additionally show excellent\nfinite-sample size and power properties of the proposed test.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 15:30:54 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Zhu", "Yinchu", ""], ["Bradic", "Jelena", ""]]}, {"id": "1705.01143", "submitter": "Shih-Chieh Su", "authors": "Shih-Chieh Su", "title": "Summarized Network Behavior Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the entity-wise topical behavior from massive network logs.\nBoth the temporal and the spatial relationships of the behavior are explored\nwith the learning architectures combing the recurrent neural network (RNN) and\nthe convolutional neural network (CNN). To make the behavioral data appropriate\nfor the spatial learning in CNN, several reduction steps are taken to form the\ntopical metrics and place them homogeneously like pixels in the images. The\nexperimental result shows both the temporal- and the spatial- gains when\ncompared to a multilayer perceptron (MLP) network. A new learning framework\ncalled spatially connected convolutional networks (SCCN) is introduced to more\nefficiently predict the behavior.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 19:12:23 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Su", "Shih-Chieh", ""]]}, {"id": "1705.01166", "submitter": "Michael Abbott", "authors": "Henry H. Mattingly, Mark K. Transtrum, Michael C. Abbott, Benjamin B.\n  Machta", "title": "Maximizing the information learned from finite data selects a simple\n  model", "comments": "9 pages, 8 figures. v3 has improved discussion and adds an appendix\n  about MDL and Bayes factors, and matches version to appear in PNAS (modulo\n  comma placement). Title changed from \"Rational Ignorance: Simpler Models\n  Learn More Information from Finite Data\"", "journal-ref": "PNAS February 2018", "doi": "10.1073/pnas.1715306115", "report-no": null, "categories": "physics.data-an cond-mat.stat-mech cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use the language of uninformative Bayesian prior choice to study the\nselection of appropriately simple effective models. We advocate for the prior\nwhich maximizes the mutual information between parameters and predictions,\nlearning as much as possible from limited data. When many parameters are poorly\nconstrained by the available data, we find that this prior puts weight only on\nboundaries of the parameter manifold. Thus it selects a lower-dimensional\neffective theory in a principled way, ignoring irrelevant parameter directions.\nIn the limit where there is sufficient data to tightly constrain any number of\nparameters, this reduces to Jeffreys prior. But we argue that this limit is\npathological when applied to the hyper-ribbon parameter manifolds generic in\nscience, because it leads to dramatic dependence on effects invisible to\nexperiment.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 20:27:14 GMT"}, {"version": "v2", "created": "Fri, 1 Sep 2017 13:55:26 GMT"}, {"version": "v3", "created": "Wed, 14 Feb 2018 15:24:36 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Mattingly", "Henry H.", ""], ["Transtrum", "Mark K.", ""], ["Abbott", "Michael C.", ""], ["Machta", "Benjamin B.", ""]]}, {"id": "1705.01204", "submitter": "Marianna Pensky", "authors": "Marianna Pensky and Teng Zhang", "title": "Spectral clustering in the dynamic stochastic block model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper, we studied a Dynamic Stochastic Block Model (DSBM)\nunder the assumptions that the connection probabilities, as functions of time,\nare smooth and that at most $s$ nodes can switch their class memberships\nbetween two consecutive time points. We estimate the edge probability tensor by\na kernel-type procedure and extract the group memberships of the nodes by\nspectral clustering. The procedure is computationally viable, adaptive to the\nunknown smoothness of the functional connection probabilities, to the rate $s$\nof membership switching and to the unknown number of clusters. In addition, it\nis accompanied by non-asymptotic guarantees for the precision of estimation and\nclustering.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 23:55:26 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Pensky", "Marianna", ""], ["Zhang", "Teng", ""]]}, {"id": "1705.01305", "submitter": "Albert Thomas", "authors": "Stephan Cl\\'emen\\c{c}on (LTCI, TSI), Albert Thomas (LTCI)", "title": "Mass Volume Curves and Anomaly Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at formulating the issue of ranking multivariate unlabeled\nobservations depending on their degree of abnormality as an unsupervised\nstatistical learning task. In the 1-d situation, this problem is usually\ntackled by means of tail estimation techniques: univariate observations are\nviewed as all the more `abnormal' as they are located far in the tail(s) of the\nunderlying probability distribution. It would be desirable as well to dispose\nof a scalar valued `scoring' function allowing for comparing the degree of\nabnormality of multivariate observations. Here we formulate the issue of\nscoring anomalies as a M-estimation problem by means of a novel functional\nperformance criterion, referred to as the Mass Volume curve (MV curve in\nshort), whose optimal elements are strictly increasing transforms of the\ndensity almost everywhere on the support of the density. We first study the\nstatistical estimation of the MV curve of a given scoring function and we\nprovide a strategy to build confidence regions using a smoothed bootstrap\napproach. Optimization of this functional criterion over the set of piecewise\nconstant scoring functions is next tackled. This boils down to estimating a\nsequence of empirical minimum volume sets whose levels are chosen adaptively\nfrom the data, so as to adjust to the variations of the optimal MV curve, while\ncontroling the bias of its approximation by a stepwise curve. Generalization\nbounds are then established for the difference in sup norm between the MV curve\nof the empirical scoring function thus obtained and the optimal MV curve.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 08:44:32 GMT"}, {"version": "v2", "created": "Mon, 3 Sep 2018 09:02:10 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Cl\u00e9men\u00e7on", "Stephan", "", "LTCI, TSI"], ["Thomas", "Albert", "", "LTCI"]]}, {"id": "1705.01306", "submitter": "Daniel Fleischer", "authors": "Alon Rozental, Daniel Fleischer", "title": "Amobee at SemEval-2017 Task 4: Deep Learning System for Sentiment\n  Detection on Twitter", "comments": "6 pages, accepted to the 11th International Workshop on Semantic\n  Evaluation (SemEval-2017)", "journal-ref": null, "doi": "10.18653/v1/S17-2108", "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the Amobee sentiment analysis system, adapted to compete\nin SemEval 2017 task 4. The system consists of two parts: a supervised training\nof RNN models based on a Twitter sentiment treebank, and the use of feedforward\nNN, Naive Bayes and logistic regression classifiers to produce predictions for\nthe different sub-tasks. The algorithm reached the 3rd place on the 5-label\nclassification task (sub-task C).\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 08:50:56 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Rozental", "Alon", ""], ["Fleischer", "Daniel", ""]]}, {"id": "1705.01342", "submitter": "Abubakar Abid", "authors": "Abubakar Abid, Ada Poon, James Zou", "title": "Linear Regression with Shuffled Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is it possible to perform linear regression on datasets whose labels are\nshuffled with respect to the inputs? We explore this question by proposing\nseveral estimators that recover the weights of a noisy linear model from labels\nthat are shuffled by an unknown permutation. We show that the analog of the\nclassical least-squares estimator produces inconsistent estimates in this\nsetting, and introduce an estimator based on the self-moments of the input\nfeatures and labels. We study the regimes in which each estimator excels, and\ngeneralize the estimators to the setting where partial ordering information is\navailable in the form of experiments replicated independently. The result is a\nframework that enables robust inference, as we demonstrate by experiments on\nboth synthetic and standard datasets, where we are able to recover approximate\nweights using only shuffled labels. Our work demonstrates that linear\nregression in the absence of complete ordering information is possible and can\nbe of practical interest, particularly in experiments that characterize\npopulations of particles, such as flow cytometry.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 10:12:12 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 07:26:33 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Abid", "Abubakar", ""], ["Poon", "Ada", ""], ["Zou", "James", ""]]}, {"id": "1705.01485", "submitter": "Marco Todescato", "authors": "Marco Todescato, Andrea Carron, Ruggero Carli, Gianluigi Pillonetto,\n  Luca Schenato", "title": "Efficient Spatio-Temporal Gaussian Regression via Kalman Filtering", "comments": "26 pages, 12 figures. Submitted to IEEE Transactions on Pattern\n  Analysis and Machine Intelligence", "journal-ref": null, "doi": "10.1016/j.automatica.2020.109032", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study the non-parametric reconstruction of spatio-temporal\ndynamical Gaussian processes (GPs) via GP regression from sparse and noisy\ndata. GPs have been mainly applied to spatial regression where they represent\none of the most powerful estimation approaches also thanks to their universal\nrepresenting properties. Their extension to dynamical processes has been\ninstead elusive so far since classical implementations lead to unscalable\nalgorithms. We then propose a novel procedure to address this problem by\ncoupling GP regression and Kalman filtering. In particular, assuming space/time\nseparability of the covariance (kernel) of the process and rational time\nspectrum, we build a finite-dimensional discrete-time state-space process\nrepresentation amenable of Kalman filtering. With sampling over a finite set of\nfixed spatial locations, our major finding is that the Kalman filter state at\ninstant $t_k$ represents a sufficient statistic to compute the minimum variance\nestimate of the process at any $t \\geq t_k$ over the entire spatial domain.\nThis result can be interpreted as a novel Kalman representer theorem for\ndynamical GPs. We then extend the study to situations where the set of spatial\ninput locations can vary over time. The proposed algorithms are finally tested\non both synthetic and real field data, also providing comparisons with standard\nGP and truncated GP regression techniques.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 15:49:38 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Todescato", "Marco", ""], ["Carron", "Andrea", ""], ["Carli", "Ruggero", ""], ["Pillonetto", "Gianluigi", ""], ["Schenato", "Luca", ""]]}, {"id": "1705.01502", "submitter": "Ran Rubin", "authors": "Ran Rubin, L.F. Abbott and Haim Sompolinsky", "title": "Balanced Excitation and Inhibition are Required for High-Capacity,\n  Noise-Robust Neuronal Selectivity", "comments": "Article and supplementary information", "journal-ref": "Proceedings of the National Academy of Sciences of the United\n  States of America, 114(41), 2017", "doi": "10.1073/pnas.1705841114", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neurons and networks in the cerebral cortex must operate reliably despite\nmultiple sources of noise. To evaluate the impact of both input and output\nnoise, we determine the robustness of single-neuron stimulus selective\nresponses, as well as the robustness of attractor states of networks of neurons\nperforming memory tasks. We find that robustness to output noise requires\nsynaptic connections to be in a balanced regime in which excitation and\ninhibition are strong and largely cancel each other. We evaluate the conditions\nrequired for this regime to exist and determine the properties of networks\noperating within it. A plausible synaptic plasticity rule for learning that\nbalances weight configurations is presented. Our theory predicts an optimal\nratio of the number of excitatory and inhibitory synapses for maximizing the\nencoding capacity of balanced networks for a given statistics of afferent\nactivations. Previous work has shown that balanced networks amplify\nspatio-temporal variability and account for observed asynchronous irregular\nstates. Here we present a novel type of balanced network that amplifies small\nchanges in the impinging signals, and emerges automatically from learning to\nperform neuronal and network functions robustly.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 16:38:01 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Rubin", "Ran", ""], ["Abbott", "L. F.", ""], ["Sompolinsky", "Haim", ""]]}, {"id": "1705.01601", "submitter": "Marek Smieja", "authors": "Marek \\'Smieja and Bernhard C. Geiger", "title": "Semi-supervised cross-entropy clustering with information bottleneck\n  constraint", "comments": null, "journal-ref": "Information Sciences, vol. 421, Dec. 2017, pp. 254-271", "doi": "10.1016/j.ins.2017.07.016", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a semi-supervised clustering method, CEC-IB, that\nmodels data with a set of Gaussian distributions and that retrieves clusters\nbased on a partial labeling provided by the user (partition-level side\ninformation). By combining the ideas from cross-entropy clustering (CEC) with\nthose from the information bottleneck method (IB), our method trades between\nthree conflicting goals: the accuracy with which the data set is modeled, the\nsimplicity of the model, and the consistency of the clustering with side\ninformation. Experiments demonstrate that CEC-IB has a performance comparable\nto Gaussian mixture models (GMM) in a classical semi-supervised scenario, but\nis faster, more robust to noisy labels, automatically determines the optimal\nnumber of clusters, and performs well when not all classes are present in the\nside information. Moreover, in contrast to other semi-supervised models, it can\nbe successfully applied in discovering natural subgroups if the partition-level\nside information is derived from the top levels of a hierarchical clustering.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 20:09:43 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["\u015amieja", "Marek", ""], ["Geiger", "Bernhard C.", ""]]}, {"id": "1705.01667", "submitter": "Masahito Ohue", "authors": "Masahito Ohue, Takuro Yamazaki, Tomohiro Ban, and Yutaka Akiyama", "title": "Link Mining for Kernel-based Compound-Protein Interaction Predictions\n  Using a Chemogenomics Approach", "comments": null, "journal-ref": "In the Thirteenth International Conference on Intelligent\n  Computing (ICIC2017), Lecture Notes in Computer Science, 10362: 549-558, 2017", "doi": "10.1007/978-3-319-63312-1_48", "report-no": null, "categories": "q-bio.QM cs.CE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual screening (VS) is widely used during computational drug discovery to\nreduce costs. Chemogenomics-based virtual screening (CGBVS) can be used to\npredict new compound-protein interactions (CPIs) from known CPI network data\nusing several methods, including machine learning and data mining. Although\nCGBVS facilitates highly efficient and accurate CPI prediction, it has poor\nperformance for prediction of new compounds for which CPIs are unknown. The\npairwise kernel method (PKM) is a state-of-the-art CGBVS method and shows high\naccuracy for prediction of new compounds. In this study, on the basis of link\nmining, we improved the PKM by combining link indicator kernel (LIK) and\nchemical similarity and evaluated the accuracy of these methods. The proposed\nmethod obtained an average area under the precision-recall curve (AUPR) value\nof 0.562, which was higher than that achieved by the conventional Gaussian\ninteraction profile (GIP) method (0.425), and the calculation time was only\nincreased by a few percent.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 01:29:19 GMT"}, {"version": "v2", "created": "Fri, 30 Jun 2017 03:46:09 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ohue", "Masahito", ""], ["Yamazaki", "Takuro", ""], ["Ban", "Tomohiro", ""], ["Akiyama", "Yutaka", ""]]}, {"id": "1705.01708", "submitter": "Tomoya Sakai", "authors": "Tomoya Sakai, Gang Niu, Masashi Sugiyama", "title": "Semi-Supervised AUC Optimization based on Positive-Unlabeled Learning", "comments": "To appear in Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximizing the area under the receiver operating characteristic curve (AUC)\nis a standard approach to imbalanced classification. So far, various supervised\nAUC optimization methods have been developed and they are also extended to\nsemi-supervised scenarios to cope with small sample problems. However, existing\nsemi-supervised AUC optimization methods rely on strong distributional\nassumptions, which are rarely satisfied in real-world problems. In this paper,\nwe propose a novel semi-supervised AUC optimization method that does not\nrequire such restrictive assumptions. We first develop an AUC optimization\nmethod based only on positive and unlabeled data (PU-AUC) and then extend it to\nsemi-supervised learning by combining it with a supervised AUC optimization\nmethod. We theoretically prove that, without the restrictive distributional\nassumptions, unlabeled data contribute to improving the generalization\nperformance in PU and semi-supervised AUC optimization methods. Finally, we\ndemonstrate the practical usefulness of the proposed methods through\nexperiments.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 05:46:32 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 15:25:32 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Sakai", "Tomoya", ""], ["Niu", "Gang", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1705.01877", "submitter": "Marek Smieja", "authors": "Marek \\'Smieja and {\\L}ukasz Struski and Jacek Tabor", "title": "Semi-supervised model-based clustering with controlled clusters leakage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on finding clusters in partially categorized data\nsets. We propose a semi-supervised version of Gaussian mixture model, called\nC3L, which retrieves natural subgroups of given categories. In contrast to\nother semi-supervised models, C3L is parametrized by user-defined leakage\nlevel, which controls maximal inconsistency between initial categorization and\nresulting clustering. Our method can be implemented as a module in practical\nexpert systems to detect clusters, which combine expert knowledge with true\ndistribution of data. Moreover, it can be used for improving the results of\nless flexible clustering techniques, such as projection pursuit clustering. The\npaper presents extensive theoretical analysis of the model and fast algorithm\nfor its efficient optimization. Experimental results show that C3L finds high\nquality clustering model, which can be applied in discovering meaningful groups\nin partially classified data.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 15:13:28 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["\u015amieja", "Marek", ""], ["Struski", "\u0141ukasz", ""], ["Tabor", "Jacek", ""]]}, {"id": "1705.01936", "submitter": "Curtis Northcutt", "authors": "Curtis G. Northcutt, Tailin Wu, Isaac L. Chuang", "title": "Learning with Confident Examples: Rank Pruning for Robust Classification\n  with Noisy Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noisy PN learning is the problem of binary classification when training\nexamples may be mislabeled (flipped) uniformly with noise rate rho1 for\npositive examples and rho0 for negative examples. We propose Rank Pruning (RP)\nto solve noisy PN learning and the open problem of estimating the noise rates,\ni.e. the fraction of wrong positive and negative labels. Unlike prior\nsolutions, RP is time-efficient and general, requiring O(T) for any\nunrestricted choice of probabilistic classifier with T fitting time. We prove\nRP has consistent noise estimation and equivalent expected risk as learning\nwith uncorrupted labels in ideal conditions, and derive closed-form solutions\nwhen conditions are non-ideal. RP achieves state-of-the-art noise estimation\nand F1, error, and AUC-PR for both MNIST and CIFAR datasets, regardless of the\namount of noise and performs similarly impressively when a large portion of\ntraining examples are noise drawn from a third distribution. To highlight, RP\nwith a CNN classifier can predict if an MNIST digit is a \"one\"or \"not\" with\nonly 0.25% error, and 0.46 error across all digits, even when 50% of positive\nexamples are mislabeled and 50% of observed positive labels are mislabeled\nnegative examples.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 17:59:30 GMT"}, {"version": "v2", "created": "Thu, 11 May 2017 16:07:54 GMT"}, {"version": "v3", "created": "Wed, 9 Aug 2017 23:21:44 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Northcutt", "Curtis G.", ""], ["Wu", "Tailin", ""], ["Chuang", "Isaac L.", ""]]}, {"id": "1705.01968", "submitter": "Josua Krause", "authors": "Josua Krause, Aritra Dasgupta, Jordan Swartz, Yindalon\n  Aphinyanaphongs, Enrico Bertini", "title": "A Workflow for Visual Diagnostics of Binary Classifiers using\n  Instance-Level Explanations", "comments": "Published at IEEE Conference on Visual Analytics Science and\n  Technology (IEEE VAST 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-in-the-loop data analysis applications necessitate greater transparency\nin machine learning models for experts to understand and trust their decisions.\nTo this end, we propose a visual analytics workflow to help data scientists and\ndomain experts explore, diagnose, and understand the decisions made by a binary\nclassifier. The approach leverages \"instance-level explanations\", measures of\nlocal feature relevance that explain single instances, and uses them to build a\nset of visual representations that guide the users in their investigation. The\nworkflow is based on three main visual representations and steps: one based on\naggregate statistics to see how data distributes across correct / incorrect\ndecisions; one based on explanations to understand which features are used to\nmake these decisions; and one based on raw data, to derive insights on\npotential root causes for the observed patterns. The workflow is derived from a\nlong-term collaboration with a group of machine learning and healthcare\nprofessionals who used our method to make sense of machine learning models they\ndeveloped. The case study from this collaboration demonstrates that the\nproposed workflow helps experts derive useful knowledge about the model and the\nphenomena it describes, thus experts can generate useful hypotheses on how a\nmodel can be improved.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 18:24:38 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 17:34:37 GMT"}, {"version": "v3", "created": "Sun, 1 Oct 2017 22:24:17 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Krause", "Josua", ""], ["Dasgupta", "Aritra", ""], ["Swartz", "Jordan", ""], ["Aphinyanaphongs", "Yindalon", ""], ["Bertini", "Enrico", ""]]}, {"id": "1705.02033", "submitter": "Yu Chen", "authors": "Yu Chen and Mohammed J. Zaki", "title": "KATE: K-Competitive Autoencoder for Text", "comments": "10 pages, KDD'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoencoders have been successful in learning meaningful representations from\nimage datasets. However, their performance on text datasets has not been widely\nstudied. Traditional autoencoders tend to learn possibly trivial\nrepresentations of text documents due to their confounding properties such as\nhigh-dimensionality, sparsity and power-law word distributions. In this paper,\nwe propose a novel k-competitive autoencoder, called KATE, for text documents.\nDue to the competition between the neurons in the hidden layer, each neuron\nbecomes specialized in recognizing specific data patterns, and overall the\nmodel can learn meaningful representations of textual data. A comprehensive set\nof experiments show that KATE can learn better representations than traditional\nautoencoders including denoising, contractive, variational, and k-sparse\nautoencoders. Our model also outperforms deep generative models, probabilistic\ntopic models, and even word representation models (e.g., Word2Vec) in terms of\nseveral downstream tasks such as document classification, regression, and\nretrieval.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 22:04:17 GMT"}, {"version": "v2", "created": "Sun, 4 Jun 2017 16:50:03 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Chen", "Yu", ""], ["Zaki", "Mohammed J.", ""]]}, {"id": "1705.02047", "submitter": "Vatsal Shah", "authors": "Vatsal Shah, Nikhil Rao, Weicong Ding", "title": "Matrix Completion via Factorizing Polynomials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting unobserved entries of a partially observed matrix has found wide\napplicability in several areas, such as recommender systems, computational\nbiology, and computer vision. Many scalable methods with rigorous theoretical\nguarantees have been developed for algorithms where the matrix is factored into\nlow-rank components, and embeddings are learned for the row and column\nentities. While there has been recent research on incorporating explicit side\ninformation in the low-rank matrix factorization setting, often implicit\ninformation can be gleaned from the data, via higher-order interactions among\nentities. Such implicit information is especially useful in cases where the\ndata is very sparse, as is often the case in real-world datasets. In this\npaper, we design a method to learn embeddings in the context of recommendation\nsystems, using the observation that higher powers of a graph transition\nprobability matrix encode the probability that a random walker will hit that\nnode in a given number of steps. We develop a coordinate descent algorithm to\nsolve the resulting optimization, that makes explicit computation of the higher\norder powers of the matrix redundant, preserving sparsity and making\ncomputations efficient. Experiments on several datasets show that our method,\nthat can use higher order information, outperforms methods that only use\nexplicitly available side information, those that use only second-order\nimplicit information and in some cases, methods based on deep neural networks\nas well.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 23:47:12 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 20:28:36 GMT"}, {"version": "v3", "created": "Wed, 14 Feb 2018 00:38:17 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Shah", "Vatsal", ""], ["Rao", "Nikhil", ""], ["Ding", "Weicong", ""]]}, {"id": "1705.02193", "submitter": "Hakan Bilen", "authors": "James Thewlis, Hakan Bilen, Andrea Vedaldi", "title": "Unsupervised learning of object landmarks by factorized spatial\n  embeddings", "comments": "To be published in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning automatically the structure of object categories remains an\nimportant open problem in computer vision. In this paper, we propose a novel\nunsupervised approach that can discover and learn landmarks in object\ncategories, thus characterizing their structure. Our approach is based on\nfactorizing image deformations, as induced by a viewpoint change or an object\ndeformation, by learning a deep neural network that detects landmarks\nconsistently with such visual effects. Furthermore, we show that the learned\nlandmarks establish meaningful correspondences between different object\ninstances in a category without having to impose this requirement explicitly.\nWe assess the method qualitatively on a variety of object types, natural and\nman-made. We also show that our unsupervised landmarks are highly predictive of\nmanually-annotated landmarks in face benchmark datasets, and can be used to\nregress these with a high degree of accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 12:45:35 GMT"}, {"version": "v2", "created": "Sun, 6 Aug 2017 08:54:26 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Thewlis", "James", ""], ["Bilen", "Hakan", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1705.02212", "submitter": "Michel Besserve", "authors": "Michel Besserve, Naji Shajarisales, Bernhard Sch\\\"olkopf and Dominik\n  Janzing", "title": "Group invariance principles for causal generative models", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The postulate of independence of cause and mechanism (ICM) has recently led\nto several new causal discovery algorithms. The interpretation of independence\nand the way it is utilized, however, varies across these methods. Our aim in\nthis paper is to propose a group theoretic framework for ICM to unify and\ngeneralize these approaches. In our setting, the cause-mechanism relationship\nis assessed by comparing it against a null hypothesis through the application\nof random generic group transformations. We show that the group theoretic view\nprovides a very general tool to study the structure of data generating\nmechanisms with direct applications to machine learning.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 13:34:16 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Besserve", "Michel", ""], ["Shajarisales", "Naji", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Janzing", "Dominik", ""]]}, {"id": "1705.02224", "submitter": "Lovedeep Gondara", "authors": "Lovedeep Gondara", "title": "Detecting Adversarial Samples Using Density Ratio Estimates", "comments": "Updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models, especially based on deep architectures are used in\neveryday applications ranging from self driving cars to medical diagnostics. It\nhas been shown that such models are dangerously susceptible to adversarial\nsamples, indistinguishable from real samples to human eye, adversarial samples\nlead to incorrect classifications with high confidence. Impact of adversarial\nsamples is far-reaching and their efficient detection remains an open problem.\nWe propose to use direct density ratio estimation as an efficient model\nagnostic measure to detect adversarial samples. Our proposed method works\nequally well with single and multi-channel samples, and with different\nadversarial sample generation methods. We also propose a method to use density\nratio estimates for generating adversarial samples with an added constraint of\npreserving density ratio.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 15:28:59 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 15:23:57 GMT"}, {"version": "v3", "created": "Fri, 19 May 2017 21:22:00 GMT"}, {"version": "v4", "created": "Mon, 20 Nov 2017 16:17:18 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Gondara", "Lovedeep", ""]]}, {"id": "1705.02355", "submitter": "Michela Paganini", "authors": "Michela Paganini, Luke de Oliveira, Benjamin Nachman", "title": "Accelerating Science with Generative Adversarial Networks: An\n  Application to 3D Particle Showers in Multi-Layer Calorimeters", "comments": "6 pages, 3 figures; version accepted by Physical Review Letters (PRL)", "journal-ref": "Phys. Rev. Lett. 120, 042003 (2018)", "doi": "10.1103/PhysRevLett.120.042003", "report-no": null, "categories": "hep-ex hep-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physicists at the Large Hadron Collider (LHC) rely on detailed simulations of\nparticle collisions to build expectations of what experimental data may look\nlike under different theory modeling assumptions. Petabytes of simulated data\nare needed to develop analysis techniques, though they are expensive to\ngenerate using existing algorithms and computing resources. The modeling of\ndetectors and the precise description of particle cascades as they interact\nwith the material in the calorimeter are the most computationally demanding\nsteps in the simulation pipeline. We therefore introduce a deep neural\nnetwork-based generative model to enable high-fidelity, fast, electromagnetic\ncalorimeter simulation. There are still challenges for achieving precision\nacross the entire phase space, but our current solution can reproduce a variety\nof particle shower properties while achieving speed-up factors of up to\n100,000$\\times$. This opens the door to a new era of fast simulation that could\nsave significant computing time and disk space, while extending the reach of\nphysics searches and precision measurements at the LHC and beyond.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 18:20:47 GMT"}, {"version": "v2", "created": "Thu, 21 Dec 2017 22:27:48 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Paganini", "Michela", ""], ["de Oliveira", "Luke", ""], ["Nachman", "Benjamin", ""]]}, {"id": "1705.02372", "submitter": "Zongming Ma", "authors": "Zhuang Ma, Zongming Ma", "title": "Exploration of Large Networks with Covariates via Fast and Universal\n  Latent Space Model Fitting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent space models are effective tools for statistical modeling and\nexploration of network data. These models can effectively model real world\nnetwork characteristics such as degree heterogeneity, transitivity, homophily,\netc. Due to their close connection to generalized linear models, it is also\nnatural to incorporate covariate information in them. The current paper\npresents two universal fitting algorithms for networks with edge covariates:\none based on nuclear norm penalization and the other based on projected\ngradient descent. Both algorithms are motivated by maximizing likelihood for a\nspecial class of inner-product models while working simultaneously for a wide\nrange of different latent space models, such as distance models, which allow\nlatent vectors to affect edge formation in flexible ways. These fitting\nmethods, especially the one based on projected gradient descent, are fast and\nscalable to large networks. We obtain their rates of convergence for both\ninner-product models and beyond. The effectiveness of the modeling approach and\nfitting algorithms is demonstrated on five real world network datasets for\ndifferent statistical tasks, including community detection with and without\nedge covariates, and network assisted learning.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 19:24:52 GMT"}, {"version": "v2", "created": "Fri, 18 Aug 2017 15:33:40 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Ma", "Zhuang", ""], ["Ma", "Zongming", ""]]}, {"id": "1705.02394", "submitter": "Stefan Scherer", "authors": "Jonathan Chang, Stefan Scherer", "title": "Learning Representations of Emotional Speech with Deep Convolutional\n  Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically assessing emotional valence in human speech has historically\nbeen a difficult task for machine learning algorithms. The subtle changes in\nthe voice of the speaker that are indicative of positive or negative emotional\nstates are often \"overshadowed\" by voice characteristics relating to emotional\nintensity or emotional activation. In this work we explore a representation\nlearning approach that automatically derives discriminative representations of\nemotional speech. In particular, we investigate two machine learning strategies\nto improve classifier performance: (1) utilization of unlabeled data using a\ndeep convolutional generative adversarial network (DCGAN), and (2) multitask\nlearning. Within our extensive experiments we leverage a multitask annotated\nemotional corpus as well as a large unlabeled meeting corpus (around 100\nhours). Our speaker-independent classification experiments show that in\nparticular the use of unlabeled data in our investigations improves performance\nof the classifiers and both fully supervised baseline approaches are\noutperformed considerably. We improve the classification of emotional valence\non a discrete 5-point scale to 43.88% and on a 3-point scale to 49.80%, which\nis competitive to state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 18:28:25 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Chang", "Jonathan", ""], ["Scherer", "Stefan", ""]]}, {"id": "1705.02411", "submitter": "Anirudh Raju", "authors": "Ming Sun, Anirudh Raju, George Tucker, Sankaran Panchapagesan,\n  Gengshen Fu, Arindam Mandal, Spyros Matsoukas, Nikko Strom, Shiv Vitaladevuni", "title": "Max-Pooling Loss Training of Long Short-Term Memory Networks for\n  Small-Footprint Keyword Spotting", "comments": null, "journal-ref": "Spoken Language Technology Workshop (SLT), 2016 IEEE (pp.\n  474-480). IEEE", "doi": "10.1109/SLT.2016.7846306", "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a max-pooling based loss function for training Long Short-Term\nMemory (LSTM) networks for small-footprint keyword spotting (KWS), with low\nCPU, memory, and latency requirements. The max-pooling loss training can be\nfurther guided by initializing with a cross-entropy loss trained network. A\nposterior smoothing based evaluation approach is employed to measure keyword\nspotting performance. Our experimental results show that LSTM models trained\nusing cross-entropy loss or max-pooling loss outperform a cross-entropy loss\ntrained baseline feed-forward Deep Neural Network (DNN). In addition,\nmax-pooling loss trained LSTM with randomly initialized network performs better\ncompared to cross-entropy loss trained LSTM. Finally, the max-pooling loss\ntrained LSTM initialized with a cross-entropy pre-trained network shows the\nbest performance, which yields $67.6\\%$ relative reduction compared to baseline\nfeed-forward DNN in Area Under the Curve (AUC) measure.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 22:36:04 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Sun", "Ming", ""], ["Raju", "Anirudh", ""], ["Tucker", "George", ""], ["Panchapagesan", "Sankaran", ""], ["Fu", "Gengshen", ""], ["Mandal", "Arindam", ""], ["Matsoukas", "Spyros", ""], ["Strom", "Nikko", ""], ["Vitaladevuni", "Shiv", ""]]}, {"id": "1705.02414", "submitter": "Patrick Doetsch", "authors": "Patrick Doetsch, Pavel Golik, Hermann Ney", "title": "A comprehensive study of batch construction strategies for recurrent\n  neural networks in MXNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we compare different batch construction methods for mini-batch\ntraining of recurrent neural networks. While popular implementations like\nTensorFlow and MXNet suggest a bucketing approach to improve the\nparallelization capabilities of the recurrent training process, we propose a\nsimple ordering strategy that arranges the training sequences in a stochastic\nalternatingly sorted way. We compare our method to sequence bucketing as well\nas various other batch construction strategies on the CHiME-4 noisy speech\nrecognition corpus. The experiments show that our alternated sorting approach\nis able to compete both in training time and recognition performance while\nbeing conceptually simpler to implement.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 22:45:31 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Doetsch", "Patrick", ""], ["Golik", "Pavel", ""], ["Ney", "Hermann", ""]]}, {"id": "1705.02436", "submitter": "Artemy Kolchinsky", "authors": "Artemy Kolchinsky, Brendan D. Tracey, David H. Wolpert", "title": "Nonlinear Information Bottleneck", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information bottleneck (IB) is a technique for extracting information in one\nrandom variable $X$ that is relevant for predicting another random variable\n$Y$. IB works by encoding $X$ in a compressed \"bottleneck\" random variable $M$\nfrom which $Y$ can be accurately decoded. However, finding the optimal\nbottleneck variable involves a difficult optimization problem, which until\nrecently has been considered for only two limited cases: discrete $X$ and $Y$\nwith small state spaces, and continuous $X$ and $Y$ with a Gaussian joint\ndistribution (in which case optimal encoding and decoding maps are linear). We\npropose a method for performing IB on arbitrarily-distributed discrete and/or\ncontinuous $X$ and $Y$, while allowing for nonlinear encoding and decoding\nmaps. Our approach relies on a novel non-parametric upper bound for mutual\ninformation. We describe how to implement our method using neural networks. We\nthen show that it achieves better performance than the recently-proposed\n\"variational IB\" method on several real-world datasets.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 03:13:21 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 20:57:07 GMT"}, {"version": "v3", "created": "Fri, 16 Feb 2018 20:37:51 GMT"}, {"version": "v4", "created": "Fri, 2 Mar 2018 00:15:09 GMT"}, {"version": "v5", "created": "Wed, 4 Apr 2018 20:55:02 GMT"}, {"version": "v6", "created": "Fri, 20 Jul 2018 21:24:38 GMT"}, {"version": "v7", "created": "Tue, 4 Sep 2018 22:39:50 GMT"}, {"version": "v8", "created": "Thu, 17 Oct 2019 18:01:46 GMT"}, {"version": "v9", "created": "Sat, 30 Nov 2019 19:03:11 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Kolchinsky", "Artemy", ""], ["Tracey", "Brendan D.", ""], ["Wolpert", "David H.", ""]]}, {"id": "1705.02438", "submitter": "Zhimin Chen", "authors": "Zhimin Chen, Yuguang Tong", "title": "Face Super-Resolution Through Wasserstein GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have received a tremendous amount of\nattention in the past few years, and have inspired applications addressing a\nwide range of problems. Despite its great potential, GANs are difficult to\ntrain. Recently, a series of papers (Arjovsky & Bottou, 2017a; Arjovsky et al.\n2017b; and Gulrajani et al. 2017) proposed using Wasserstein distance as the\ntraining objective and promised easy, stable GAN training across architectures\nwith minimal hyperparameter tuning. In this paper, we compare the performance\nof Wasserstein distance with other training objectives on a variety of GAN\narchitectures in the context of single image super-resolution. Our results\nagree that Wasserstein GAN with gradient penalty (WGAN-GP) provides stable and\nconverging GAN training and that Wasserstein distance is an effective metric to\ngauge training progress.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 03:48:02 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Chen", "Zhimin", ""], ["Tong", "Yuguang", ""]]}, {"id": "1705.02441", "submitter": "Jelena Bradic", "authors": "Jelena Bradic, Yinchu Zhu", "title": "Comments on `High-dimensional simultaneous inference with the bootstrap'", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide comments on the article \"High-dimensional simultaneous inference\nwith the bootstrap\" by Ruben Dezeure, Peter Buhlmann and Cun-Hui Zhang.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 04:15:06 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Bradic", "Jelena", ""], ["Zhu", "Yinchu", ""]]}, {"id": "1705.02518", "submitter": "Subhabrata Mukherjee", "authors": "Subhabrata Mukherjee, Kashyap Popat, Gerhard Weikum", "title": "Exploring Latent Semantic Factors to Find Useful Product Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online reviews provided by consumers are a valuable asset for e-Commerce\nplatforms, influencing potential consumers in making purchasing decisions.\nHowever, these reviews are of varying quality, with the useful ones buried deep\nwithin a heap of non-informative reviews. In this work, we attempt to\nautomatically identify review quality in terms of its helpfulness to the end\nconsumers. In contrast to previous works in this domain exploiting a variety of\nsyntactic and community-level features, we delve deep into the semantics of\nreviews as to what makes them useful, providing interpretable explanation for\nthe same. We identify a set of consistency and semantic factors, all from the\ntext, ratings, and timestamps of user-generated reviews, making our approach\ngeneralizable across all communities and domains. We explore review semantics\nin terms of several latent factors like the expertise of its author, his\njudgment about the fine-grained facets of the underlying product, and his\nwriting style. These are cast into a Hidden Markov Model -- Latent Dirichlet\nAllocation (HMM-LDA) based model to jointly infer: (i) reviewer expertise, (ii)\nitem facets, and (iii) review helpfulness. Large-scale experiments on five\nreal-world datasets from Amazon show significant improvement over\nstate-of-the-art baselines in predicting and ranking useful reviews.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 19:21:48 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Mukherjee", "Subhabrata", ""], ["Popat", "Kashyap", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1705.02519", "submitter": "Subhabrata Mukherjee", "authors": "Subhabrata Mukherjee, Hemank Lamba, Gerhard Weikum", "title": "Item Recommendation with Evolving User Preferences and Experience", "comments": null, "journal-ref": null, "doi": "10.1109/ICDM.2015.111", "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current recommender systems exploit user and item similarities by\ncollaborative filtering. Some advanced methods also consider the temporal\nevolution of item ratings as a global background process. However, all prior\nmethods disregard the individual evolution of a user's experience level and how\nthis is expressed in the user's writing in a review community. In this paper,\nwe model the joint evolution of user experience, interest in specific item\nfacets, writing style, and rating behavior. This way we can generate individual\nrecommendations that take into account the user's maturity level (e.g.,\nrecommending art movies rather than blockbusters for a cinematography expert).\nAs only item ratings and review texts are observables, we capture the user's\nexperience and interests in a latent model learned from her reviews, vocabulary\nand writing style. We develop a generative HMM-LDA model to trace user\nevolution, where the Hidden Markov Model (HMM) traces her latent experience\nprogressing over time -- with solely user reviews and ratings as observables\nover time. The facets of a user's interest are drawn from a Latent Dirichlet\nAllocation (LDA) model derived from her reviews, as a function of her (again\nlatent) experience level. In experiments with five real-world datasets, we show\nthat our model improves the rating prediction over state-of-the-art baselines,\nby a substantial margin. We also show, in a use-case study, that our model\nperforms well in the assessment of user experience levels.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 19:22:41 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Mukherjee", "Subhabrata", ""], ["Lamba", "Hemank", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1705.02522", "submitter": "Subhabrata Mukherjee", "authors": "Subhabrata Mukherjee, Gerhard Weikum, Cristian Danescu-Niculescu-Mizil", "title": "People on Drugs: Credibility of User Statements in Health Communities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online health communities are a valuable source of information for patients\nand physicians. However, such user-generated resources are often plagued by\ninaccuracies and misinformation. In this work we propose a method for\nautomatically establishing the credibility of user-generated medical statements\nand the trustworthiness of their authors by exploiting linguistic cues and\ndistant supervision from expert sources. To this end we introduce a\nprobabilistic graphical model that jointly learns user trustworthiness,\nstatement credibility, and language objectivity. We apply this methodology to\nthe task of extracting rare or unknown side-effects of medical drugs --- this\nbeing one of the problems where large scale non-expert data has the potential\nto complement expert medical knowledge. We show that our method can reliably\nextract side-effects and filter out false statements, while identifying\ntrustworthy users that are likely to contribute valuable medical information.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 19:38:33 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Mukherjee", "Subhabrata", ""], ["Weikum", "Gerhard", ""], ["Danescu-Niculescu-Mizil", "Cristian", ""]]}, {"id": "1705.02553", "submitter": "Kamyar Azizzadenesheli Ph.D.", "authors": "Kamyar Azizzadenesheli, Alessandro Lazaric, Animashree Anandkumar", "title": "Experimental results : Reinforcement Learning of POMDPs using Spectral\n  Methods", "comments": "30th Conference on Neural Information Processing Systems (NIPS 2016),\n  Barcelona, Spain", "journal-ref": "NIPS-DeepRL-Workshop-2016Barcelona", "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new reinforcement learning algorithm for partially observable\nMarkov decision processes (POMDP) based on spectral decomposition methods.\nWhile spectral methods have been previously employed for consistent learning of\n(passive) latent variable models such as hidden Markov models, POMDPs are more\nchallenging since the learner interacts with the environment and possibly\nchanges the future observations in the process. We devise a learning algorithm\nrunning through epochs, in each epoch we employ spectral techniques to learn\nthe POMDP parameters from a trajectory generated by a fixed policy. At the end\nof the epoch, an optimization oracle returns the optimal memoryless planning\npolicy which maximizes the expected reward based on the estimated POMDP model.\nWe prove an order-optimal regret bound with respect to the optimal memoryless\npolicy and efficient scaling with respect to the dimensionality of observation\nand action spaces.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 02:49:10 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Azizzadenesheli", "Kamyar", ""], ["Lazaric", "Alessandro", ""], ["Anandkumar", "Animashree", ""]]}, {"id": "1705.02556", "submitter": "Ishan Jindal", "authors": "Ishan Jindal and Matthew Nokleby", "title": "Classification and Representation via Separable Subspaces: Performance\n  Limits and Algorithms", "comments": "This paper is submitted to IEEE JSTSP Special Issue on\n  Information-Theoretic Methods in Data Acquisition, Analysis, and Processing\n  2018", "journal-ref": "IEEE Journal of Selected Topics in Signal Processing ( Volume: 12\n  , Issue: 5 , Oct. 2018 )", "doi": "10.1109/JSTSP.2018.2838549", "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the classification performance of Kronecker-structured models in two\nasymptotic regimes and developed an algorithm for separable, fast and compact\nK-S dictionary learning for better classification and representation of\nmultidimensional signals by exploiting the structure in the signal. First, we\nstudy the classification performance in terms of diversity order and pairwise\ngeometry of the subspaces. We derive an exact expression for the diversity\norder as a function of the signal and subspace dimensions of a K-S model. Next,\nwe study the classification capacity, the maximum rate at which the number of\nclasses can grow as the signal dimension goes to infinity. Then we describe a\nfast algorithm for Kronecker-Structured Learning of Discriminative Dictionaries\n(K-SLD2). Finally, we evaluate the empirical classification performance of K-S\nmodels for the synthetic data, showing that they agree with the diversity order\nanalysis. We also evaluate the performance of K-SLD2 on synthetic and\nreal-world datasets showing that the K-SLD2 balances compact signal\nrepresentation and good classification performance.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 03:36:05 GMT"}, {"version": "v2", "created": "Fri, 29 Dec 2017 18:13:49 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Jindal", "Ishan", ""], ["Nokleby", "Matthew", ""]]}, {"id": "1705.02627", "submitter": "Mostafa Tavassolipour", "authors": "Mostafa Tavassolipour, Seyed Abolfazl Motahari, Mohammad-Taghi Manzuri\n  Shalmani", "title": "Learning of Gaussian Processes in Distributed and Communication Limited\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is of fundamental importance to find algorithms obtaining optimal\nperformance for learning of statistical models in distributed and communication\nlimited systems. Aiming at characterizing the optimal strategies, we consider\nlearning of Gaussian Processes (GPs) in distributed systems as a pivotal\nexample. We first address a very basic problem: how many bits are required to\nestimate the inner-products of Gaussian vectors across distributed machines?\nUsing information theoretic bounds, we obtain an optimal solution for the\nproblem which is based on vector quantization. Two suboptimal and more\npractical schemes are also presented as substitute for the vector quantization\nscheme. In particular, it is shown that the performance of one of the practical\nschemes which is called per-symbol quantization is very close to the optimal\none. Schemes provided for the inner-product calculations are incorporated into\nour proposed distributed learning methods for GPs. Experimental results show\nthat with spending few bits per symbol in our communication scheme, our\nproposed methods outperform previous zero rate distributed GP learning schemes\nsuch as Bayesian Committee Model (BCM) and Product of experts (PoE).\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 14:15:57 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Tavassolipour", "Mostafa", ""], ["Motahari", "Seyed Abolfazl", ""], ["Shalmani", "Mohammad-Taghi Manzuri", ""]]}, {"id": "1705.02643", "submitter": "Davide Bacciu", "authors": "Davide Bacciu and Francesco Crecchi and Davide Morelli", "title": "DropIn: Making Reservoir Computing Neural Networks Robust to Missing\n  Inputs by Dropout", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a novel, principled approach to train recurrent neural\nnetworks from the Reservoir Computing family that are robust to missing part of\nthe input features at prediction time. By building on the ensembling properties\nof Dropout regularization, we propose a methodology, named DropIn, which\nefficiently trains a neural model as a committee machine of subnetworks, each\ncapable of predicting with a subset of the original input features. We discuss\nthe application of the DropIn methodology in the context of Reservoir Computing\nmodels and targeting applications characterized by input sources that are\nunreliable or prone to be disconnected, such as in pervasive wireless sensor\nnetworks and ambient intelligence. We provide an experimental assessment using\nreal-world data from such application domains, showing how the Dropin\nmethodology allows to maintain predictive performances comparable to those of a\nmodel without missing features, even when 20\\%-50\\% of the inputs are not\navailable.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 16:03:06 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Bacciu", "Davide", ""], ["Crecchi", "Francesco", ""], ["Morelli", "Davide", ""]]}, {"id": "1705.02667", "submitter": "Subhabrata Mukherjee", "authors": "Subhabrata Mukherjee, Gerhard Weikum", "title": "People on Media: Jointly Identifying Credible News and Trustworthy\n  Citizen Journalists in Online Communities", "comments": null, "journal-ref": null, "doi": "10.1145/2806416.2806537", "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Media seems to have become more partisan, often providing a biased coverage\nof news catering to the interest of specific groups. It is therefore essential\nto identify credible information content that provides an objective narrative\nof an event. News communities such as digg, reddit, or newstrust offer\nrecommendations, reviews, quality ratings, and further insights on journalistic\nworks. However, there is a complex interaction between different factors in\nsuch online communities: fairness and style of reporting, language clarity and\nobjectivity, topical perspectives (like political viewpoint), expertise and\nbias of community members, and more. This paper presents a model to\nsystematically analyze the different interactions in a news community between\nusers, news, and sources. We develop a probabilistic graphical model that\nleverages this joint interaction to identify 1) highly credible news articles,\n2) trustworthy news sources, and 3) expert users who perform the role of\n\"citizen journalists\" in the community. Our method extends CRF models to\nincorporate real-valued ratings, as some communities have very fine-grained\nscales that cannot be easily discretized without losing information. To the\nbest of our knowledge, this paper is the first full-fledged analysis of\ncredibility, trust, and expertise in news communities.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 17:41:31 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 16:40:16 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Mukherjee", "Subhabrata", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1705.02668", "submitter": "Subhabrata Mukherjee", "authors": "Subhabrata Mukherjee, Sourav Dutta, Gerhard Weikum", "title": "Credible Review Detection with Limited Information using Consistency\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online reviews provide viewpoints on the strengths and shortcomings of\nproducts/services, influencing potential customers' purchasing decisions.\nHowever, the proliferation of non-credible reviews -- either fake (promoting/\ndemoting an item), incompetent (involving irrelevant aspects), or biased --\nentails the problem of identifying credible reviews. Prior works involve\nclassifiers harnessing rich information about items/users -- which might not be\nreadily available in several domains -- that provide only limited\ninterpretability as to why a review is deemed non-credible. This paper presents\na novel approach to address the above issues. We utilize latent topic models\nleveraging review texts, item ratings, and timestamps to derive consistency\nfeatures without relying on item/user histories, unavailable for \"long-tail\"\nitems/users. We develop models, for computing review credibility scores to\nprovide interpretable evidence for non-credible reviews, that are also\ntransferable to other domains -- addressing the scarcity of labeled data.\nExperiments on real-world datasets demonstrate improvements over\nstate-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 17:43:01 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Mukherjee", "Subhabrata", ""], ["Dutta", "Sourav", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1705.02669", "submitter": "Subhabrata Mukherjee", "authors": "Subhabrata Mukherjee, Stephan Guennemann, Gerhard Weikum", "title": "Item Recommendation with Continuous Experience Evolution of Users using\n  Brownian Motion", "comments": null, "journal-ref": null, "doi": "10.1145/2939672.2939780", "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online review communities are dynamic as users join and leave, adopt new\nvocabulary, and adapt to evolving trends. Recent work has shown that\nrecommender systems benefit from explicit consideration of user experience.\nHowever, prior work assumes a fixed number of discrete experience levels,\nwhereas in reality users gain experience and mature continuously over time.\nThis paper presents a new model that captures the continuous evolution of user\nexperience, and the resulting language model in reviews and other posts. Our\nmodel is unsupervised and combines principles of Geometric Brownian Motion,\nBrownian Motion, and Latent Dirichlet Allocation to trace a smooth temporal\nprogression of user experience and language model respectively. We develop\npractical algorithms for estimating the model parameters from data and for\ninference with our model (e.g., to recommend items). Extensive experiments with\nfive real-world datasets show that our model not only fits data better than\ndiscrete-model baselines, but also outperforms state-of-the-art methods for\npredicting item ratings.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 17:46:43 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 06:47:51 GMT"}, {"version": "v3", "created": "Wed, 9 Aug 2017 17:56:00 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Mukherjee", "Subhabrata", ""], ["Guennemann", "Stephan", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1705.02687", "submitter": "Seyed Sajjadi", "authors": "Seyed Sajjadi, Bruce Shapiro, Christopher McKinlay, Allen Sarkisyan,\n  Carol Shubin, Efunwande Osoba", "title": "Finding Bottlenecks: Predicting Student Attrition with Unsupervised\n  Classifier", "comments": "7 pages, 10 figures, Finding Bottlenecks: Predicting Student\n  Attrition with Unsupervised Classifiers, IEEE, IntelliSys 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CY cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With pressure to increase graduation rates and reduce time to degree in\nhigher education, it is important to identify at-risk students early. Automated\nearly warning systems are therefore highly desirable. In this paper, we use\nunsupervised clustering techniques to predict the graduation status of declared\nmajors in five departments at California State University Northridge (CSUN),\nbased on a minimal number of lower division courses in each major. In addition,\nwe use the detected clusters to identify hidden bottleneck courses.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 19:45:49 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Sajjadi", "Seyed", ""], ["Shapiro", "Bruce", ""], ["McKinlay", "Christopher", ""], ["Sarkisyan", "Allen", ""], ["Shubin", "Carol", ""], ["Osoba", "Efunwande", ""]]}, {"id": "1705.02689", "submitter": "Seyed Sajjadi", "authors": "Seyed A Sajjadi, Danial Moazen, Ani Nahapetian", "title": "AirDraw: Leveraging Smart Watch Motion Sensors for Mobile Human Computer\n  Interactions", "comments": "6 pages, AirDraw, Leveraging Smart Watch Motion Sensors for Mobile\n  Human Computer Interactions : IEEE, CCNC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wearable computing is one of the fastest growing technologies today. Smart\nwatches are poised to take over at least of half the wearable devices market in\nthe near future. Smart watch screen size, however, is a limiting factor for\ngrowth, as it restricts practical text input. On the other hand, wearable\ndevices have some features, such as consistent user interaction and hands-free,\nheads-up operations, which pave the way for gesture recognition methods of text\nentry. This paper proposes a new text input method for smart watches, which\nutilizes motion sensor data and machine learning approaches to detect letters\nwritten in the air by a user. This method is less computationally intensive and\nless expensive when compared to computer vision approaches. It is also not\naffected by lighting factors, which limit computer vision solutions. The\nAirDraw system prototype developed to test this approach is presented.\nAdditionally, experimental results close to 71% accuracy are presented.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 19:58:54 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Sajjadi", "Seyed A", ""], ["Moazen", "Danial", ""], ["Nahapetian", "Ani", ""]]}, {"id": "1705.02737", "submitter": "Lovedeep Gondara", "authors": "Lovedeep Gondara, Ke Wang", "title": "MIDA: Multiple Imputation using Denoising Autoencoders", "comments": "To appear in the proceedings of the 22nd Pacific-Asia Conference on\n  Knowledge Discovery and Data Mining (PAKDD 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing data is a significant problem impacting all domains. State-of-the-art\nframework for minimizing missing data bias is multiple imputation, for which\nthe choice of an imputation model remains nontrivial. We propose a multiple\nimputation model based on overcomplete deep denoising autoencoders. Our\nproposed model is capable of handling different data types, missingness\npatterns, missingness proportions and distributions. Evaluation on several real\nlife datasets show our proposed model significantly outperforms current\nstate-of-the-art methods under varying conditions while simultaneously\nimproving end of the line analytics.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 04:00:25 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 21:15:44 GMT"}, {"version": "v3", "created": "Sat, 17 Feb 2018 16:05:32 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Gondara", "Lovedeep", ""], ["Wang", "Ke", ""]]}, {"id": "1705.02891", "submitter": "Alessandro Barp", "authors": "Alessandro Barp, Francois-Xavier Briol, Anthony D. Kennedy, Mark\n  Girolami", "title": "Geometry and Dynamics for Markov Chain Monte Carlo", "comments": "Submitted to \"Annual Review of Statistics and Its Applications\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG hep-lat math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo methods have revolutionised mathematical computation\nand enabled statistical inference within many previously intractable models. In\nthis context, Hamiltonian dynamics have been proposed as an efficient way of\nbuilding chains which can explore probability densities efficiently. The method\nemerges from physics and geometry and these links have been extensively studied\nby a series of authors through the last thirty years. However, there is\ncurrently a gap between the intuitions and knowledge of users of the\nmethodology and our deep understanding of these theoretical foundations. The\naim of this review is to provide a comprehensive introduction to the geometric\ntools used in Hamiltonian Monte Carlo at a level accessible to statisticians,\nmachine learners and other users of the methodology with only a basic\nunderstanding of Monte Carlo methods. This will be complemented with some\ndiscussion of the most recent advances in the field which we believe will\nbecome increasingly relevant to applied scientists.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 14:19:53 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Barp", "Alessandro", ""], ["Briol", "Francois-Xavier", ""], ["Kennedy", "Anthony D.", ""], ["Girolami", "Mark", ""]]}, {"id": "1705.02894", "submitter": "Jong Chul Ye", "authors": "Jae Hyun Lim and Jong Chul Ye", "title": "Geometric GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Nets (GANs) represent an important milestone for\neffective generative models, which has inspired numerous variants seemingly\ndifferent from each other. One of the main contributions of this paper is to\nreveal a unified geometric structure in GAN and its variants. Specifically, we\nshow that the adversarial generative model training can be decomposed into\nthree geometric steps: separating hyperplane search, discriminator parameter\nupdate away from the separating hyperplane, and the generator update along the\nnormal vector direction of the separating hyperplane. This geometric intuition\nreveals the limitations of the existing approaches and leads us to propose a\nnew formulation called geometric GAN using SVM separating hyperplane that\nmaximizes the margin. Our theoretical analysis shows that the geometric GAN\nconverges to a Nash equilibrium between the discriminator and generator. In\naddition, extensive numerical results show that the superior performance of\ngeometric GAN.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 14:32:33 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 01:12:28 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Lim", "Jae Hyun", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1705.02908", "submitter": "Yangqiu Song", "authors": "Yangqiu Song and Dan Roth", "title": "Machine Learning with World Knowledge: The Position and Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has become pervasive in multiple domains, impacting a wide\nvariety of applications, such as knowledge discovery and data mining, natural\nlanguage processing, information retrieval, computer vision, social and health\ninformatics, ubiquitous computing, etc. Two essential problems of machine\nlearning are how to generate features and how to acquire labels for machines to\nlearn. Particularly, labeling large amount of data for each domain-specific\nproblem can be very time consuming and costly. It has become a key obstacle in\nmaking learning protocols realistic in applications. In this paper, we will\ndiscuss how to use the existing general-purpose world knowledge to enhance\nmachine learning processes, by enriching the features or reducing the labeling\nwork. We start from the comparison of world knowledge with domain-specific\nknowledge, and then introduce three key problems in using world knowledge in\nlearning processes, i.e., explicit and implicit feature representation,\ninference for knowledge linking and disambiguation, and learning with direct or\nindirect supervision. Finally we discuss the future directions of this research\ntopic.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 15:06:32 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Song", "Yangqiu", ""], ["Roth", "Dan", ""]]}, {"id": "1705.02928", "submitter": "Yuantao Gu", "authors": "Xiudong Wang and Yuantao Gu", "title": "Cross-label Suppression: A Discriminative and Fast Dictionary Learning\n  with Group Regularization", "comments": "36 pages, 12 figures, 11 tables", "journal-ref": null, "doi": "10.1109/TIP.2017.2703101", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses image classification through learning a compact and\ndiscriminative dictionary efficiently. Given a structured dictionary with each\natom (columns in the dictionary matrix) related to some label, we propose\ncross-label suppression constraint to enlarge the difference among\nrepresentations for different classes. Meanwhile, we introduce group\nregularization to enforce representations to preserve label properties of\noriginal samples, meaning the representations for the same class are encouraged\nto be similar. Upon the cross-label suppression, we don't resort to\nfrequently-used $\\ell_0$-norm or $\\ell_1$-norm for coding, and obtain\ncomputational efficiency without losing the discriminative power for\ncategorization. Moreover, two simple classification schemes are also developed\nto take full advantage of the learnt dictionary. Extensive experiments on six\ndata sets including face recognition, object categorization, scene\nclassification, texture recognition and sport action categorization are\nconducted, and the results show that the proposed approach can outperform lots\nof recently presented dictionary algorithms on both recognition accuracy and\ncomputational efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 15:49:43 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Wang", "Xiudong", ""], ["Gu", "Yuantao", ""]]}, {"id": "1705.02994", "submitter": "Andrea Montanari", "authors": "Hamid Javadi and Andrea Montanari", "title": "Non-negative Matrix Factorization via Archetypal Analysis", "comments": "39 pages; 11 pdf figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a collection of data points, non-negative matrix factorization (NMF)\nsuggests to express them as convex combinations of a small set of `archetypes'\nwith non-negative entries. This decomposition is unique only if the true\narchetypes are non-negative and sufficiently sparse (or the weights are\nsufficiently sparse), a regime that is captured by the separability condition\nand its generalizations.\n  In this paper, we study an approach to NMF that can be traced back to the\nwork of Cutler and Breiman (1994) and does not require the data to be\nseparable, while providing a generally unique decomposition. We optimize the\ntrade-off between two objectives: we minimize the distance of the data points\nfrom the convex envelope of the archetypes (which can be interpreted as an\nempirical risk), while minimizing the distance of the archetypes from the\nconvex envelope of the data (which can be interpreted as a data-dependent\nregularization). The archetypal analysis method of (Cutler, Breiman, 1994) is\nrecovered as the limiting case in which the last term is given infinite weight.\n  We introduce a `uniqueness condition' on the data which is necessary for\nexactly recovering the archetypes from noiseless data. We prove that, under\nuniqueness (plus additional regularity conditions on the geometry of the\narchetypes), our estimator is robust. While our approach requires solving a\nnon-convex optimization problem, we find that standard optimization methods\nsucceed in finding good solutions both for real and synthetic data.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 17:53:36 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Javadi", "Hamid", ""], ["Montanari", "Andrea", ""]]}, {"id": "1705.03290", "submitter": "Iiris Sundin", "authors": "Iiris Sundin, Tomi Peltola, Muntasir Mamun Majumder, Pedram Daee,\n  Marta Soare, Homayun Afrabandpey, Caroline Heckman, Samuel Kaski and Pekka\n  Marttinen", "title": "Improving drug sensitivity predictions in precision medicine through\n  active expert knowledge elicitation", "comments": "24 pages, 3 figures", "journal-ref": null, "doi": "10.1093/bioinformatics/bty257", "report-no": null, "categories": "cs.AI cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the efficacy of a drug for a given individual, using\nhigh-dimensional genomic measurements, is at the core of precision medicine.\nHowever, identifying features on which to base the predictions remains a\nchallenge, especially when the sample size is small. Incorporating expert\nknowledge offers a promising alternative to improve a prediction model, but\ncollecting such knowledge is laborious to the expert if the number of candidate\nfeatures is very large. We introduce a probabilistic model that can incorporate\nexpert feedback about the impact of genomic measurements on the sensitivity of\na cancer cell for a given drug. We also present two methods to intelligently\ncollect this feedback from the expert, using experimental design and\nmulti-armed bandit models. In a multiple myeloma blood cancer data set (n=51),\nexpert knowledge decreased the prediction error by 8%. Furthermore, the\nintelligent approaches can be used to reduce the workload of feedback\ncollection to less than 30% on average compared to a naive approach.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 12:04:33 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Sundin", "Iiris", ""], ["Peltola", "Tomi", ""], ["Majumder", "Muntasir Mamun", ""], ["Daee", "Pedram", ""], ["Soare", "Marta", ""], ["Afrabandpey", "Homayun", ""], ["Heckman", "Caroline", ""], ["Kaski", "Samuel", ""], ["Marttinen", "Pekka", ""]]}, {"id": "1705.03297", "submitter": "Youngser Park", "authors": "Carey E. Priebe, Youngser Park, Minh Tang, Avanti Athreya, Vince\n  Lyzinski, Joshua T. Vogelstein, Yichen Qin, Ben Cocanougher, Katharina\n  Eichler, Marta Zlatic, Albert Cardona", "title": "Semiparametric spectral modeling of the Drosophila connectome", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present semiparametric spectral modeling of the complete larval Drosophila\nmushroom body connectome. Motivated by a thorough exploratory data analysis of\nthe network via Gaussian mixture modeling (GMM) in the adjacency spectral\nembedding (ASE) representation space, we introduce the latent structure model\n(LSM) for network modeling and inference. LSM is a generalization of the\nstochastic block model (SBM) and a special case of the random dot product graph\n(RDPG) latent position model, and is amenable to semiparametric GMM in the ASE\nrepresentation space. The resulting connectome code derived via semiparametric\nGMM composed with ASE captures latent connectome structure and elucidates\nbiologically relevant neuronal properties.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 12:51:28 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Priebe", "Carey E.", ""], ["Park", "Youngser", ""], ["Tang", "Minh", ""], ["Athreya", "Avanti", ""], ["Lyzinski", "Vince", ""], ["Vogelstein", "Joshua T.", ""], ["Qin", "Yichen", ""], ["Cocanougher", "Ben", ""], ["Eichler", "Katharina", ""], ["Zlatic", "Marta", ""], ["Cardona", "Albert", ""]]}, {"id": "1705.03304", "submitter": "Syed Awais Wahab Shah", "authors": "Syed Awais Wahab Shah, Tamer Khattab, Muhammad Zeeshan Shakir, and\n  Mazen Omar Hasna", "title": "A Distributed Approach for Networked Flying Platform Association with\n  Small Cells in 5G+ Networks", "comments": "Submitted to IEEE GLOBECOM 2017, 7 pages and 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The densification of small-cell base stations in a 5G architecture is a\npromising approach to enhance the coverage area and facilitate the ever\nincreasing capacity demand of end users. However, the bottleneck is an\nintelligent management of a backhaul/fronthaul network for these small-cell\nbase stations. This involves efficient association and placement of the\nbackhaul hubs that connects these small-cells with the core network.\nTerrestrial hubs suffer from an inefficient non line of sight link limitations\nand unavailability of a proper infrastructure in an urban area. Seeing the\npopularity of flying platforms, we employ here an idea of using networked\nflying platform (NFP) such as unmanned aerial vehicles (UAVs), drones, unmanned\nballoons flying at different altitudes, as aerial backhaul hubs. The\nassociation problem of these NFP-hubs and small-cell base stations is\nformulated considering backhaul link and NFP related limitations such as\nmaximum number of supported links and bandwidth. Then, this paper presents an\nefficient and distributed solution of the designed problem, which performs a\ngreedy search in order to maximize the sum rate of the overall network. A\nfavorable performance is observed via a numerical comparison of our proposed\nmethod with optimal exhaustive search algorithm in terms of sum rate and\nrun-time speed.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 16:53:10 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Shah", "Syed Awais Wahab", ""], ["Khattab", "Tamer", ""], ["Shakir", "Muhammad Zeeshan", ""], ["Hasna", "Mazen Omar", ""]]}, {"id": "1705.03387", "submitter": "Hyeungill Lee", "authors": "Hyeungill Lee, Sungyeob Han, Jungwoo Lee", "title": "Generative Adversarial Trainer: Defense to Adversarial Perturbations\n  with GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel technique to make neural network robust to adversarial\nexamples using a generative adversarial network. We alternately train both\nclassifier and generator networks. The generator network generates an\nadversarial perturbation that can easily fool the classifier network by using a\ngradient of each image. Simultaneously, the classifier network is trained to\nclassify correctly both original and adversarial images generated by the\ngenerator. These procedures help the classifier network to become more robust\nto adversarial perturbations. Furthermore, our adversarial training framework\nefficiently reduces overfitting and outperforms other regularization methods\nsuch as Dropout. We applied our method to supervised learning for CIFAR\ndatasets, and experimantal results show that our method significantly lowers\nthe generalization error of the network. To the best of our knowledge, this is\nthe first method which uses GAN to improve supervised learning.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 15:30:58 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 21:44:32 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Lee", "Hyeungill", ""], ["Han", "Sungyeob", ""], ["Lee", "Jungwoo", ""]]}, {"id": "1705.03419", "submitter": "Ishan Jindal", "authors": "Ishan Jindal, Matthew Nokleby and Xuewen Chen", "title": "Learning Deep Networks from Noisy Labels with Dropout Regularization", "comments": "Published at 2016 IEEE 16th International Conference on Data Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large datasets often have unreliable labels-such as those obtained from\nAmazon's Mechanical Turk or social media platforms-and classifiers trained on\nmislabeled datasets often exhibit poor performance. We present a simple,\neffective technique for accounting for label noise when training deep neural\nnetworks. We augment a standard deep network with a softmax layer that models\nthe label noise statistics. Then, we train the deep network and noise model\njointly via end-to-end stochastic gradient descent on the (perhaps mislabeled)\ndataset. The augmented model is overdetermined, so in order to encourage the\nlearning of a non-trivial noise model, we apply dropout regularization to the\nweights of the noise model during training. Numerical experiments on noisy\nversions of the CIFAR-10 and MNIST datasets show that the proposed dropout\ntechnique outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 16:42:32 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Jindal", "Ishan", ""], ["Nokleby", "Matthew", ""], ["Chen", "Xuewen", ""]]}, {"id": "1705.03420", "submitter": "Mahdi Barzegar Khalilsarai", "authors": "Mahdi Barzegar Khalilsarai, Saeid Haghighatshoar, Giuseppe Caire,\n  Gerhard Wunder", "title": "Compressive Estimation of a Stochastic Process with Unknown\n  Autocorrelation Function", "comments": "6 pages, 4 figures. Accepted for presentation in ISIT 2017, Aachen,\n  Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the prediction of a circularly symmetric zero-mean\nstationary Gaussian process from a window of observations consisting of\nfinitely many samples. This is a prevalent problem in a wide range of\napplications in communication theory and signal processing. Due to\nstationarity, when the autocorrelation function or equivalently the power\nspectral density (PSD) of the process is available, the Minimum Mean Squared\nError (MMSE) predictor is readily obtained. In particular, it is given by a\nlinear operator that depends on autocorrelation of the process as well as the\nnoise power in the observed samples. The prediction becomes, however, quite\nchallenging when the PSD of the process is unknown. In this paper, we propose a\nblind predictor that does not require the a priori knowledge of the PSD of the\nprocess and compare its performance with that of an MMSE predictor that has a\nfull knowledge of the PSD. To design such a blind predictor, we use the random\nspectral representation of a stationary Gaussian process. We apply the\nwell-known atomic-norm minimization technique to the observed samples to obtain\na discrete quantization of the underlying random spectrum, which we use to\npredict the process. Our simulation results show that this estimator has a good\nperformance comparable with that of the MMSE estimator.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 16:44:26 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Khalilsarai", "Mahdi Barzegar", ""], ["Haghighatshoar", "Saeid", ""], ["Caire", "Giuseppe", ""], ["Wunder", "Gerhard", ""]]}, {"id": "1705.03439", "submitter": "Yixin Wang", "authors": "Yixin Wang, David M. Blei", "title": "Frequentist Consistency of Variational Bayes", "comments": null, "journal-ref": "Journal of the American Statistical Association 114.527 (2019):\n  1147-1161", "doi": "10.1080/01621459.2018.1473776", "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge for modern Bayesian statistics is how to perform scalable\ninference of posterior distributions. To address this challenge, variational\nBayes (VB) methods have emerged as a popular alternative to the classical\nMarkov chain Monte Carlo (MCMC) methods. VB methods tend to be faster while\nachieving comparable predictive performance. However, there are few theoretical\nresults around VB. In this paper, we establish frequentist consistency and\nasymptotic normality of VB methods. Specifically, we connect VB methods to\npoint estimates based on variational approximations, called frequentist\nvariational approximations, and we use the connection to prove a variational\nBernstein-von Mises theorem. The theorem leverages the theoretical\ncharacterizations of frequentist variational approximations to understand\nasymptotic properties of VB. In summary, we prove that (1) the VB posterior\nconverges to the Kullback-Leibler (KL) minimizer of a normal distribution,\ncentered at the truth and (2) the corresponding variational expectation of the\nparameter is consistent and asymptotically normal. As applications of the\ntheorem, we derive asymptotic properties of VB posteriors in Bayesian mixture\nmodels, Bayesian generalized linear mixed models, and Bayesian stochastic block\nmodels. We conduct a simulation study to illustrate these theoretical results.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 17:30:16 GMT"}, {"version": "v2", "created": "Thu, 24 May 2018 14:41:19 GMT"}, {"version": "v3", "created": "Thu, 8 Jul 2021 01:16:04 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Wang", "Yixin", ""], ["Blei", "David M.", ""]]}, {"id": "1705.03451", "submitter": "Rui L. Lopes", "authors": "Alipio Jorge, German Larrazabal, Pablo Guillen, Rui L. Lopes", "title": "Proceedings of the Workshop on Data Mining for Oil and Gas", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.16408.39681", "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of exploring and exploiting Oil and Gas (O&G) generates a lot of\ndata that can bring more efficiency to the industry. The opportunities for\nusing data mining techniques in the \"digital oil-field\" remain largely\nunexplored or uncharted. With the high rate of data expansion, companies are\nscrambling to develop ways to develop near-real-time predictive analytics, data\nmining and machine learning capabilities, and are expanding their data storage\ninfrastructure and resources. With these new goals, come the challenges of\nmanaging data growth, integrating intelligence tools, and analyzing the data to\nglean useful insights. Oil and Gas companies need data solutions to\neconomically extract value from very large volumes of a wide variety of data\ngenerated from exploration, well drilling and production devices and sensors.\n  Data mining for oil and gas industry throughout the lifecycle of the\nreservoir includes the following roles: locating hydrocarbons, managing\ngeological data, drilling and formation evaluation, well construction, well\ncompletion, and optimizing production through the life of the oil field. For\neach of these phases during the lifecycle of oil field, data mining play a\nsignificant role. Based on which phase were talking about, knowledge creation\nthrough scientific models, data analytics and machine learning, a effective,\nproductive, and on demand data insight is critical for decision making within\nthe organization.\n  The significant challenges posed by this complex and economically vital field\njustify a meeting of data scientists that are willing to share their experience\nand knowledge. Thus, the Worskhop on Data Mining for Oil and Gas (DM4OG) aims\nto provide a quality forum for researchers that work on the significant\nchallenges arising from the synergy between data science, machine learning, and\nthe modeling and optimization problems in the O&G industry.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 17:55:15 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 13:17:17 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Jorge", "Alipio", ""], ["Larrazabal", "German", ""], ["Guillen", "Pablo", ""], ["Lopes", "Rui L.", ""]]}, {"id": "1705.03508", "submitter": "Hamid Reza Hassanzadeh", "authors": "Hamid Reza Hassanzadeh, Ying Sha, May D. Wang", "title": "DeepDeath: Learning to Predict the Underlying Cause of Death with Big\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple cause-of-death data provides a valuable source of information that\ncan be used to enhance health standards by predicting health related\ntrajectories in societies with large populations. These data are often\navailable in large quantities across U.S. states and require Big Data\ntechniques to uncover complex hidden patterns. We design two different classes\nof models suitable for large-scale analysis of mortality data, a Hadoop-based\nensemble of random forests trained over N-grams, and the DeepDeath, a deep\nclassifier based on the recurrent neural network (RNN). We apply both classes\nto the mortality data provided by the National Center for Health Statistics and\nshow that while both perform significantly better than the random classifier,\nthe deep model that utilizes long short-term memory networks (LSTMs), surpasses\nthe N-gram based models and is capable of learning the temporal aspect of the\ndata without a need for building ad-hoc, expert-driven features.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 17:01:57 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Hassanzadeh", "Hamid Reza", ""], ["Sha", "Ying", ""], ["Wang", "May D.", ""]]}, {"id": "1705.03536", "submitter": "Jonathan Mei", "authors": "Jonathan Mei and Jos\\'e M.F. Moura", "title": "SILVar: Single Index Latent Variable Models", "comments": "Print ISSN: 1053-587X Online ISSN: 1941-0476, IEEE Transactions on\n  Signal Processing, online March 21, 2018", "journal-ref": null, "doi": "10.1109/TSP.2018.2818075", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A semi-parametric, non-linear regression model in the presence of latent\nvariables is introduced. These latent variables can correspond to unmodeled\nphenomena or unmeasured agents in a complex networked system. This new\nformulation allows joint estimation of certain non-linearities in the system,\nthe direct interactions between measured variables, and the effects of\nunmodeled elements on the observed system. The particular form of the model\nadopted is justified, and learning is posed as a regularized empirical risk\nminimization. This leads to classes of structured convex optimization problems\nwith a \"sparse plus low-rank\" flavor. Relations between the proposed model and\nseveral common model paradigms, such as those of Robust Principal Component\nAnalysis (PCA) and Vector Autoregression (VAR), are established. Particularly\nin the VAR setting, the low-rank contributions can come from broad trends\nexhibited in the time series. Details of the algorithm for learning the model\nare presented. Experiments demonstrate the performance of the model and the\nestimation algorithm on simulated and real data.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 20:46:19 GMT"}, {"version": "v2", "created": "Wed, 27 Dec 2017 19:30:51 GMT"}, {"version": "v3", "created": "Sat, 24 Mar 2018 01:15:31 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Mei", "Jonathan", ""], ["Moura", "Jos\u00e9 M. F.", ""]]}, {"id": "1705.03562", "submitter": "Steven Hansen", "authors": "Steven Stenberg Hansen", "title": "Deep Episodic Value Iteration for Model-based Meta-Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new deep meta reinforcement learner, which we call Deep Episodic\nValue Iteration (DEVI). DEVI uses a deep neural network to learn a similarity\nmetric for a non-parametric model-based reinforcement learning algorithm. Our\nmodel is trained end-to-end via back-propagation. Despite being trained using\nthe model-free Q-learning objective, we show that DEVI's model-based internal\nstructure provides `one-shot' transfer to changes in reward and transition\nstructure, even for tasks with very high-dimensional state spaces.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 22:59:18 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Hansen", "Steven Stenberg", ""]]}, {"id": "1705.03566", "submitter": "Mostafa Rahmani", "authors": "Mostafa Rahmani, George Atia", "title": "Spatial Random Sampling: A Structure-Preserving Data Sketching Tool", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2017.2723472", "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random column sampling is not guaranteed to yield data sketches that preserve\nthe underlying structures of the data and may not sample sufficiently from\nless-populated data clusters. Also, adaptive sampling can often provide\naccurate low rank approximations, yet may fall short of producing descriptive\ndata sketches, especially when the cluster centers are linearly dependent.\nMotivated by that, this paper introduces a novel randomized column sampling\ntool dubbed Spatial Random Sampling (SRS), in which data points are sampled\nbased on their proximity to randomly sampled points on the unit sphere. The\nmost compelling feature of SRS is that the corresponding probability of\nsampling from a given data cluster is proportional to the surface area the\ncluster occupies on the unit sphere, independently from the size of the cluster\npopulation. Although it is fully randomized, SRS is shown to provide\ndescriptive and balanced data representations. The proposed idea addresses a\npressing need in data science and holds potential to inspire many novel\napproaches for analysis of big data.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 23:31:15 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 23:19:02 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Rahmani", "Mostafa", ""], ["Atia", "George", ""]]}, {"id": "1705.03594", "submitter": "Bradley Price", "authors": "Bradley S. Price and Charles J. Geyer and Adam J. Rothman", "title": "Automatic Response Category Combination in Multinomial Logistic\n  Regression", "comments": "19 Pages, 10 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a penalized likelihood method that simultaneously fits the\nmultinomial logistic regression model and combines subsets of the response\ncategories. The penalty is non differentiable when pairs of columns in the\noptimization variable are equal. This encourages pairwise equality of these\ncolumns in the estimator, which corresponds to response category combination.\nWe use an alternating direction method of multipliers algorithm to compute the\nestimator and we discuss the algorithm's convergence. Prediction and model\nselection are also addressed.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 03:02:21 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Price", "Bradley S.", ""], ["Geyer", "Charles J.", ""], ["Rothman", "Adam J.", ""]]}, {"id": "1705.03669", "submitter": "Rui L. Lopes", "authors": "Rui L. Lopes, Al\\'ipio Jorge", "title": "Mind the Gap: A Well Log Data Analysis", "comments": "Part of DM4OG 2017 proceedings (arXiv:1705.03451)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main task in oil and gas exploration is to gain an understanding of the\ndistribution and nature of rocks and fluids in the subsurface. Well logs are\nrecords of petro-physical data acquired along a borehole, providing direct\ninformation about what is in the subsurface. The data collected by logging\nwells can have significant economic consequences, due to the costs inherent to\ndrilling wells, and the potential return of oil deposits. In this paper, we\ndescribe preliminary work aimed at building a general framework for well log\nprediction.\n  First, we perform a descriptive and exploratory analysis of the gaps in the\nneutron porosity logs of more than a thousand wells in the North Sea. Then, we\ngenerate artificial gaps in the neutron logs that reflect the statistics\ncollected before. Finally, we compare Artificial Neural Networks, Random\nForests, and three algorithms of Linear Regression in the prediction of missing\ngaps on a well-by-well basis.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 09:27:16 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Lopes", "Rui L.", ""], ["Jorge", "Al\u00edpio", ""]]}, {"id": "1705.03771", "submitter": "Feng Nan", "authors": "Feng Nan and Venkatesh Saligrama", "title": "Comments on the proof of adaptive submodular function minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We point out an issue with Theorem 5 appearing in \"Group-based active query\nselection for rapid diagnosis in time-critical situations\". Theorem 5 bounds\nthe expected number of queries for a greedy algorithm to identify the class of\nan item within a constant factor of optimal. The Theorem is based on\ncorrectness of a result on minimization of adaptive submodular functions. We\npresent an example that shows that a critical step in Theorem A.11 of \"Adaptive\nSubmodularity: Theory and Applications in Active Learning and Stochastic\nOptimization\" is incorrect.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 13:52:31 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Nan", "Feng", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1705.03821", "submitter": "Djallel Bouneffouf", "authors": "Djallel Bouneffouf, Irina Rish, Guillermo A. Cecchi, Raphael Feraud", "title": "Context Attentive Bandits: Contextual Bandit with Restricted Context", "comments": "IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a novel formulation of the multi-armed bandit model, which we\ncall the contextual bandit with restricted context, where only a limited number\nof features can be accessed by the learner at every iteration. This novel\nformulation is motivated by different online problems arising in clinical\ntrials, recommender systems and attention modeling. Herein, we adapt the\nstandard multi-armed bandit algorithm known as Thompson Sampling to take\nadvantage of our restricted context setting, and propose two novel algorithms,\ncalled the Thompson Sampling with Restricted Context(TSRC) and the Windows\nThompson Sampling with Restricted Context(WTSRC), for handling stationary and\nnonstationary environments, respectively. Our empirical results demonstrate\nadvantages of the proposed approaches on several real-life datasets\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 15:32:36 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 18:40:28 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Bouneffouf", "Djallel", ""], ["Rish", "Irina", ""], ["Cecchi", "Guillermo A.", ""], ["Feraud", "Raphael", ""]]}, {"id": "1705.04058", "submitter": "Yongcheng Jing", "authors": "Yongcheng Jing, Yezhou Yang, Zunlei Feng, Jingwen Ye, Yizhou Yu,\n  Mingli Song", "title": "Neural Style Transfer: A Review", "comments": "Project page: https://github.com/ycjing/Neural-Style-Transfer-Papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The seminal work of Gatys et al. demonstrated the power of Convolutional\nNeural Networks (CNNs) in creating artistic imagery by separating and\nrecombining image content and style. This process of using CNNs to render a\ncontent image in different styles is referred to as Neural Style Transfer\n(NST). Since then, NST has become a trending topic both in academic literature\nand industrial applications. It is receiving increasing attention and a variety\nof approaches are proposed to either improve or extend the original NST\nalgorithm. In this paper, we aim to provide a comprehensive overview of the\ncurrent progress towards NST. We first propose a taxonomy of current algorithms\nin the field of NST. Then, we present several evaluation methods and compare\ndifferent NST algorithms both qualitatively and quantitatively. The review\nconcludes with a discussion of various applications of NST and open problems\nfor future research. A list of papers discussed in this review, corresponding\ncodes, pre-trained models and more comparison results are publicly available at\nhttps://github.com/ycjing/Neural-Style-Transfer-Papers.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 08:08:44 GMT"}, {"version": "v2", "created": "Fri, 13 Apr 2018 09:21:36 GMT"}, {"version": "v3", "created": "Mon, 16 Apr 2018 13:25:07 GMT"}, {"version": "v4", "created": "Thu, 26 Apr 2018 12:35:19 GMT"}, {"version": "v5", "created": "Wed, 16 May 2018 11:59:51 GMT"}, {"version": "v6", "created": "Sun, 17 Jun 2018 08:40:41 GMT"}, {"version": "v7", "created": "Tue, 30 Oct 2018 09:48:05 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Jing", "Yongcheng", ""], ["Yang", "Yezhou", ""], ["Feng", "Zunlei", ""], ["Ye", "Jingwen", ""], ["Yu", "Yizhou", ""], ["Song", "Mingli", ""]]}, {"id": "1705.04138", "submitter": "Yue Yu", "authors": "Yue Yu, Longbo Huang", "title": "Fast Stochastic Variance Reduced ADMM for Stochastic Composition\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the stochastic composition optimization problem proposed in\n\\cite{wang2017stochastic}, which has applications ranging from estimation to\nstatistical and machine learning. We propose the first ADMM-based algorithm\nnamed com-SVR-ADMM, and show that com-SVR-ADMM converges linearly for strongly\nconvex and Lipschitz smooth objectives, and has a convergence rate of $O( \\log\nS/S)$, which improves upon the $O(S^{-4/9})$ rate in\n\\cite{wang2016accelerating} when the objective is convex and Lipschitz smooth.\nMoreover, com-SVR-ADMM possesses a rate of $O(1/\\sqrt{S})$ when the objective\nis convex but without Lipschitz smoothness. We also conduct experiments and\nshow that it outperforms existing algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 12:50:23 GMT"}, {"version": "v2", "created": "Mon, 22 May 2017 02:28:15 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Yu", "Yue", ""], ["Huang", "Longbo", ""]]}, {"id": "1705.04194", "submitter": "Md Ashad Alam PhD", "authors": "Md. Ashad Alam, Kenji Fukumizu, Yu-Ping Wang", "title": "Influence Function and Robust Variant of Kernel Canonical Correlation\n  Analysis", "comments": "arXiv admin note: text overlap with arXiv:1602.05563", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many unsupervised kernel methods rely on the estimation of the kernel\ncovariance operator (kernel CO) or kernel cross-covariance operator (kernel\nCCO). Both kernel CO and kernel CCO are sensitive to contaminated data, even\nwhen bounded positive definite kernels are used. To the best of our knowledge,\nthere are few well-founded robust kernel methods for statistical unsupervised\nlearning. In addition, while the influence function (IF) of an estimator can\ncharacterize its robustness, asymptotic properties and standard error, the IF\nof a standard kernel canonical correlation analysis (standard kernel CCA) has\nnot been derived yet. To fill this gap, we first propose a robust kernel\ncovariance operator (robust kernel CO) and a robust kernel cross-covariance\noperator (robust kernel CCO) based on a generalized loss function instead of\nthe quadratic loss function. Second, we derive the IF for robust kernel CCO and\nstandard kernel CCA. Using the IF of the standard kernel CCA, we can detect\ninfluential observations from two sets of data. Finally, we propose a method\nbased on the robust kernel CO and the robust kernel CCO, called {\\bf robust\nkernel CCA}, which is less sensitive to noise than the standard kernel CCA. The\nintroduced principles can also be applied to many other kernel methods\ninvolving kernel CO or kernel CCO. Our experiments on synthesized data and\nimaging genetics analysis demonstrate that the proposed IF of standard kernel\nCCA can identify outliers. It is also seen that the proposed robust kernel CCA\nmethod performs better for ideal and contaminated data than the standard kernel\nCCA.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 18:45:38 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Alam", "Md. Ashad", ""], ["Fukumizu", "Kenji", ""], ["Wang", "Yu-Ping", ""]]}, {"id": "1705.04267", "submitter": "Dufan Wu", "authors": "Dufan Wu, Kyungsang Kim, Georges El Fakhri, and Quanzheng Li", "title": "A Cascaded Convolutional Neural Network for X-ray Low-dose CT Image\n  Denoising", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image denoising techniques are essential to reducing noise levels and\nenhancing diagnosis reliability in low-dose computed tomography (CT). Machine\nlearning based denoising methods have shown great potential in removing the\ncomplex and spatial-variant noises in CT images. However, some residue\nartifacts would appear in the denoised image due to complexity of noises. A\ncascaded training network was proposed in this work, where the trained CNN was\napplied on the training dataset to initiate new trainings and remove artifacts\ninduced by denoising. A cascades of convolutional neural networks (CNN) were\nbuilt iteratively to achieve better performance with simple CNN structures.\nExperiments were carried out on 2016 Low-dose CT Grand Challenge datasets to\nevaluate the method's performance.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 16:32:55 GMT"}, {"version": "v2", "created": "Mon, 28 Aug 2017 13:48:08 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Wu", "Dufan", ""], ["Kim", "Kyungsang", ""], ["Fakhri", "Georges El", ""], ["Li", "Quanzheng", ""]]}, {"id": "1705.04293", "submitter": "Danica J. Sutherland", "authors": "Ho Chung Leon Law, Danica J. Sutherland, Dino Sejdinovic, Seth Flaxman", "title": "Bayesian Approaches to Distribution Regression", "comments": null, "journal-ref": "Proceedings of the Twenty-First International Conference on\n  Artificial Intelligence and Statistics (AISTATS 2018), PMLR 84:1167-1176", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distribution regression has recently attracted much interest as a generic\nsolution to the problem of supervised learning where labels are available at\nthe group level, rather than at the individual level. Current approaches,\nhowever, do not propagate the uncertainty in observations due to sampling\nvariability in the groups. This effectively assumes that small and large groups\nare estimated equally well, and should have equal weight in the final\nregression. We account for this uncertainty with a Bayesian distribution\nregression formalism, improving the robustness and performance of the model\nwhen group sizes vary. We frame our models in a neural network style, allowing\nfor simple MAP inference using backpropagation to learn the parameters, as well\nas MCMC-based inference which can fully propagate uncertainty. We demonstrate\nour approach on illustrative toy datasets, as well as on a challenging problem\nof predicting age from images.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 17:20:07 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 10:18:41 GMT"}, {"version": "v3", "created": "Thu, 22 Feb 2018 14:08:11 GMT"}, {"version": "v4", "created": "Thu, 14 Jan 2021 19:08:00 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Law", "Ho Chung Leon", ""], ["Sutherland", "Danica J.", ""], ["Sejdinovic", "Dino", ""], ["Flaxman", "Seth", ""]]}, {"id": "1705.04312", "submitter": "Alexej Gossmann", "authors": "Alexej Gossmann, Pascal Zille, Vince Calhoun, and Yu-Ping Wang", "title": "FDR-Corrected Sparse Canonical Correlation Analysis with Applications to\n  Imaging Genomics", "comments": "- Clarification of the definition of FDR for CCA in Section III;\n  results unchanged. - Corrected typos. - Added IEEE copyright notice for the\n  accepted article", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reducing the number of false discoveries is presently one of the most\npressing issues in the life sciences. It is of especially great importance for\nmany applications in neuroimaging and genomics, where datasets are typically\nhigh-dimensional, which means that the number of explanatory variables exceeds\nthe sample size. The false discovery rate (FDR) is a criterion that can be\nemployed to address that issue. Thus it has gained great popularity as a tool\nfor testing multiple hypotheses. Canonical correlation analysis (CCA) is a\nstatistical technique that is used to make sense of the cross-correlation of\ntwo sets of measurements collected on the same set of samples (e.g., brain\nimaging and genomic data for the same mental illness patients), and sparse CCA\nextends the classical method to high-dimensional settings. Here we propose a\nway of applying the FDR concept to sparse CCA, and a method to control the FDR.\nThe proposed FDR correction directly influences the sparsity of the solution,\nadapting it to the unknown true sparsity level. Theoretical derivation as well\nas simulation studies show that our procedure indeed keeps the FDR of the\ncanonical vectors below a user-specified target level. We apply the proposed\nmethod to an imaging genomics dataset from the Philadelphia Neurodevelopmental\nCohort. Our results link the brain connectivity profiles derived from brain\nactivity during an emotion identification task, as measured by functional\nmagnetic resonance imaging (fMRI), to the corresponding subjects' genomic data.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 17:57:40 GMT"}, {"version": "v2", "created": "Fri, 12 May 2017 01:28:42 GMT"}, {"version": "v3", "created": "Tue, 12 Dec 2017 00:12:42 GMT"}, {"version": "v4", "created": "Sat, 23 Jun 2018 04:45:44 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Gossmann", "Alexej", ""], ["Zille", "Pascal", ""], ["Calhoun", "Vince", ""], ["Wang", "Yu-Ping", ""]]}, {"id": "1705.04379", "submitter": "Alexander Jung", "authors": "Alexander Jung, Madelon Hulsebos", "title": "The Network Nullspace Property for Compressed Sensing of Big Data over\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel condition, which we term the net- work nullspace property,\nwhich ensures accurate recovery of graph signals representing massive\nnetwork-structured datasets from few signal values. The network nullspace\nproperty couples the cluster structure of the underlying network-structure with\nthe geometry of the sampling set. Our results can be used to design efficient\nsampling strategies based on the network topology.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 21:21:08 GMT"}, {"version": "v2", "created": "Thu, 25 May 2017 06:54:34 GMT"}, {"version": "v3", "created": "Wed, 6 Sep 2017 13:59:19 GMT"}, {"version": "v4", "created": "Tue, 13 Mar 2018 08:16:25 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Jung", "Alexander", ""], ["Hulsebos", "Madelon", ""]]}, {"id": "1705.04405", "submitter": "Luigi Acerbi", "authors": "Luigi Acerbi, Wei Ji Ma", "title": "Practical Bayesian Optimization for Model Fitting with Bayesian Adaptive\n  Direct Search", "comments": "To appear in Advances in Neural Information Processing Systems 30\n  (NIPS 2017). 21 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational models in fields such as computational neuroscience are often\nevaluated via stochastic simulation or numerical approximation. Fitting these\nmodels implies a difficult optimization problem over complex, possibly noisy\nparameter landscapes. Bayesian optimization (BO) has been successfully applied\nto solving expensive black-box problems in engineering and machine learning.\nHere we explore whether BO can be applied as a general tool for model fitting.\nFirst, we present a novel hybrid BO algorithm, Bayesian adaptive direct search\n(BADS), that achieves competitive performance with an affordable computational\noverhead for the running time of typical models. We then perform an extensive\nbenchmark of BADS vs. many common and state-of-the-art nonconvex,\nderivative-free optimizers, on a set of model-fitting problems with real data\nand models from six studies in behavioral, cognitive, and computational\nneuroscience. With default settings, BADS consistently finds comparable or\nbetter solutions than other methods, including `vanilla' BO, showing great\npromise for advanced BO techniques, and BADS in particular, as a general\nmodel-fitting tool.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 23:53:13 GMT"}, {"version": "v2", "created": "Thu, 2 Nov 2017 13:45:54 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Acerbi", "Luigi", ""], ["Ma", "Wei Ji", ""]]}, {"id": "1705.04524", "submitter": "Peng Su", "authors": "Peng Su, Xiao-Rong Ding, Yuan-Ting Zhang, Jing Liu, Fen Miao, Ni Zhao", "title": "Long-term Blood Pressure Prediction with Deep Recurrent Neural Networks", "comments": "To appear in IEEE BHI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods for arterial blood pressure (BP) estimation directly map the\ninput physiological signals to output BP values without explicitly modeling the\nunderlying temporal dependencies in BP dynamics. As a result, these models\nsuffer from accuracy decay over a long time and thus require frequent\ncalibration. In this work, we address this issue by formulating BP estimation\nas a sequence prediction problem in which both the input and target are\ntemporal sequences. We propose a novel deep recurrent neural network (RNN)\nconsisting of multilayered Long Short-Term Memory (LSTM) networks, which are\nincorporated with (1) a bidirectional structure to access larger-scale context\ninformation of input sequence, and (2) residual connections to allow gradients\nin deep RNN to propagate more effectively. The proposed deep RNN model was\ntested on a static BP dataset, and it achieved root mean square error (RMSE) of\n3.90 and 2.66 mmHg for systolic BP (SBP) and diastolic BP (DBP) prediction\nrespectively, surpassing the accuracy of traditional BP prediction models. On a\nmulti-day BP dataset, the deep RNN achieved RMSE of 3.84, 5.25, 5.80 and 5.81\nmmHg for the 1st day, 2nd day, 4th day and 6th month after the 1st day SBP\nprediction, and 1.80, 4.78, 5.0, 5.21 mmHg for corresponding DBP prediction,\nrespectively, which outperforms all previous models with notable improvement.\nThe experimental results suggest that modeling the temporal dependencies in BP\ndynamics significantly improves the long-term BP prediction accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 11:53:26 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 15:45:23 GMT"}, {"version": "v3", "created": "Sun, 14 Jan 2018 13:56:46 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Su", "Peng", ""], ["Ding", "Xiao-Rong", ""], ["Zhang", "Yuan-Ting", ""], ["Liu", "Jing", ""], ["Miao", "Fen", ""], ["Zhao", "Ni", ""]]}, {"id": "1705.04591", "submitter": "Mahdi Soltanolkotabi", "authors": "Mahdi Soltanolkotabi", "title": "Learning ReLUs via Gradient Descent", "comments": "arXiv admin note: text overlap with arXiv:1702.06175", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of learning Rectified Linear Units (ReLUs)\nwhich are functions of the form $max(0,<w,x>)$ with $w$ denoting the weight\nvector. We study this problem in the high-dimensional regime where the number\nof observations are fewer than the dimension of the weight vector. We assume\nthat the weight vector belongs to some closed set (convex or nonconvex) which\ncaptures known side-information about its structure. We focus on the realizable\nmodel where the inputs are chosen i.i.d.~from a Gaussian distribution and the\nlabels are generated according to a planted weight vector. We show that\nprojected gradient descent, when initialization at 0, converges at a linear\nrate to the planted model with a number of samples that is optimal up to\nnumerical constants. Our results on the dynamics of convergence of these very\nshallow neural nets may provide some insights towards understanding the\ndynamics of deeper architectures.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 22:06:46 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 23:29:56 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Soltanolkotabi", "Mahdi", ""]]}, {"id": "1705.04651", "submitter": "Hien Nguyen", "authors": "Hien D. Nguyen and Geoffrey J. McLachlan", "title": "Iteratively-Reweighted Least-Squares Fitting of Support Vector Machines:\n  A Majorization--Minimization Algorithm Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support vector machines (SVMs) are an important tool in modern data analysis.\nTraditionally, support vector machines have been fitted via quadratic\nprogramming, either using purpose-built or off-the-shelf algorithms. We present\nan alternative approach to SVM fitting via the majorization--minimization (MM)\nparadigm. Algorithms that are derived via MM algorithm constructions can be\nshown to monotonically decrease their objectives at each iteration, as well as\nbe globally convergent to stationary points. We demonstrate the construction of\niteratively-reweighted least-squares (IRLS) algorithms, via the MM paradigm,\nfor SVM risk minimization problems involving the hinge, least-square,\nsquared-hinge, and logistic losses, and 1-norm, 2-norm, and elastic net\npenalizations. Successful implementations of our algorithms are presented via\nsome numerical examples.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 16:44:03 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Nguyen", "Hien D.", ""], ["McLachlan", "Geoffrey J.", ""]]}, {"id": "1705.04662", "submitter": "Karl Ni", "authors": "Cory Stephenson, Patrick Callier, Abhinav Ganesh, Karl Ni", "title": "Monaural Audio Speaker Separation with Source Contrastive Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an algorithm to separate simultaneously speaking persons from each\nother, the \"cocktail party problem\", using a single microphone. Our approach\ninvolves a deep recurrent neural networks regression to a vector space that is\ndescriptive of independent speakers. Such a vector space can embed empirically\ndetermined speaker characteristics and is optimized by distinguishing between\nspeaker masks. We call this technique source-contrastive estimation. The\nmethodology is inspired by negative sampling, which has seen success in natural\nlanguage processing, where an embedding is learned by correlating and\nde-correlating a given input vector with output weights. Although the matrix\ndetermined by the output weights is dependent on a set of known speakers, we\nonly use the input vectors during inference. Doing so will ensure that source\nseparation is explicitly speaker-independent. Our approach is similar to recent\ndeep neural network clustering and permutation-invariant training research; we\nuse weighted spectral features and masks to augment individual speaker\nfrequencies while filtering out other speakers. We avoid, however, the severe\ncomputational burden of other approaches with our technique. Furthermore, by\ntraining a vector space rather than combinations of different speakers or\ndifferences thereof, we avoid the so-called permutation problem during\ntraining. Our algorithm offers an intuitive, computationally efficient response\nto the cocktail party problem, and most importantly boasts better empirical\nperformance than other current techniques.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 17:23:02 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Stephenson", "Cory", ""], ["Callier", "Patrick", ""], ["Ganesh", "Abhinav", ""], ["Ni", "Karl", ""]]}, {"id": "1705.04790", "submitter": "Madalina Fiterau", "authors": "Madalina Fiterau, Suvrat Bhooshan, Jason Fries, Charles Bournhonesque,\n  Jennifer Hicks, Eni Halilaj, Christopher R\\'e, Scott Delp", "title": "ShortFuse: Biomedical Time Series Representations in the Presence of\n  Structured Information", "comments": "Manuscript under review for the Machine Learning in Healthcare\n  Conference, 2017 (www.mucmd.org). 15 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In healthcare applications, temporal variables that encode movement, health\nstatus and longitudinal patient evolution are often accompanied by rich\nstructured information such as demographics, diagnostics and medical exam data.\nHowever, current methods do not jointly optimize over structured covariates and\ntime series in the feature extraction process. We present ShortFuse, a method\nthat boosts the accuracy of deep learning models for time series by explicitly\nmodeling temporal interactions and dependencies with structured covariates.\nShortFuse introduces hybrid convolutional and LSTM cells that incorporate the\ncovariates via weights that are shared across the temporal domain. ShortFuse\noutperforms competing models by 3% on two biomedical applications, forecasting\nosteoarthritis-related cartilage degeneration and predicting surgical outcomes\nfor cerebral palsy patients, matching or exceeding the accuracy of models that\nuse features engineered by domain experts.\n", "versions": [{"version": "v1", "created": "Sat, 13 May 2017 06:00:01 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 00:26:38 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Fiterau", "Madalina", ""], ["Bhooshan", "Suvrat", ""], ["Fries", "Jason", ""], ["Bournhonesque", "Charles", ""], ["Hicks", "Jennifer", ""], ["Halilaj", "Eni", ""], ["R\u00e9", "Christopher", ""], ["Delp", "Scott", ""]]}, {"id": "1705.04886", "submitter": "Meghana Kshirsagar", "authors": "Meghana Kshirsagar, Eunho Yang, Aur\\'elie C. Lozano", "title": "Learning task structure via sparsity grouped multitask learning", "comments": "ECML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse mapping has been a key methodology in many high-dimensional scientific\nproblems. When multiple tasks share the set of relevant features, learning them\njointly in a group drastically improves the quality of relevant feature\nselection. However, in practice this technique is used limitedly since such\ngrouping information is usually hidden. In this paper, our goal is to recover\nthe group structure on the sparsity patterns and leverage that information in\nthe sparse learning. Toward this, we formulate a joint optimization problem in\nthe task parameter and the group membership, by constructing an appropriate\nregularizer to encourage sparse learning as well as correct recovery of task\ngroups. We further demonstrate that our proposed method recovers groups and the\nsparsity patterns in the task parameters accurately by extensive experiments.\n", "versions": [{"version": "v1", "created": "Sat, 13 May 2017 21:16:13 GMT"}, {"version": "v2", "created": "Fri, 15 Sep 2017 02:03:26 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Kshirsagar", "Meghana", ""], ["Yang", "Eunho", ""], ["Lozano", "Aur\u00e9lie C.", ""]]}, {"id": "1705.04971", "submitter": "Babak Toghiani-Rizi", "authors": "Babak Toghiani-Rizi, Marcus Windmark", "title": "Musical Instrument Recognition Using Their Distinctive Characteristics\n  in Artificial Neural Networks", "comments": "Results based on a study conducted during the course Machine Learning\n  at Uppsala University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study an Artificial Neural Network was trained to classify musical\ninstruments, using audio samples transformed to the frequency domain. Different\nfeatures of the sound, in both time and frequency domain, were analyzed and\ncompared in relation to how much information that could be derived from that\nlimited data. The study concluded that in comparison with the base experiment,\nthat had an accuracy of 93.5%, using the attack only resulted in 80.2% and the\ninitial 100 Hz in 64.2%.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 14:43:10 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Toghiani-Rizi", "Babak", ""], ["Windmark", "Marcus", ""]]}, {"id": "1705.04977", "submitter": "Michael Tsang", "authors": "Michael Tsang, Dehua Cheng, Yan Liu", "title": "Detecting Statistical Interactions from Neural Network Weights", "comments": "Published in ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpreting neural networks is a crucial and challenging task in machine\nlearning. In this paper, we develop a novel framework for detecting statistical\ninteractions captured by a feedforward multilayer neural network by directly\ninterpreting its learned weights. Depending on the desired interactions, our\nmethod can achieve significantly better or similar interaction detection\nperformance compared to the state-of-the-art without searching an exponential\nsolution space of possible interactions. We obtain this accuracy and efficiency\nby observing that interactions between input features are created by the\nnon-additive effect of nonlinear activation functions, and that interacting\npaths are encoded in weight matrices. We demonstrate the performance of our\nmethod and the importance of discovered interactions via experimental results\non both synthetic datasets and real-world application datasets.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 15:35:29 GMT"}, {"version": "v2", "created": "Sun, 11 Jun 2017 22:27:48 GMT"}, {"version": "v3", "created": "Sun, 25 Feb 2018 02:09:25 GMT"}, {"version": "v4", "created": "Tue, 27 Feb 2018 18:58:21 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Tsang", "Michael", ""], ["Cheng", "Dehua", ""], ["Liu", "Yan", ""]]}, {"id": "1705.05035", "submitter": "Luke Metz", "authors": "Luke Metz, Julian Ibarz, Navdeep Jaitly, James Davidson", "title": "Discrete Sequential Prediction of Continuous Actions for Deep RL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has long been assumed that high dimensional continuous control problems\ncannot be solved effectively by discretizing individual dimensions of the\naction space due to the exponentially large number of bins over which policies\nwould have to be learned. In this paper, we draw inspiration from the recent\nsuccess of sequence-to-sequence models for structured prediction problems to\ndevelop policies over discretized spaces. Central to this method is the\nrealization that complex functions over high dimensional spaces can be modeled\nby neural networks that predict one dimension at a time. Specifically, we show\nhow Q-values and policies over continuous spaces can be modeled using a next\nstep prediction model over discretized dimensions. With this parameterization,\nit is possible to both leverage the compositional structure of action spaces\nduring learning, as well as compute maxima over action spaces (approximately).\nOn a simple example task we demonstrate empirically that our method can perform\nglobal search, which effectively gets around the local optimization issues that\nplague DDPG. We apply the technique to off-policy (Q-learning) methods and show\nthat our method can achieve the state-of-the-art for off-policy methods on\nseveral continuous control tasks.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 22:53:13 GMT"}, {"version": "v2", "created": "Sat, 16 Jun 2018 03:11:46 GMT"}, {"version": "v3", "created": "Sat, 8 Jun 2019 00:56:16 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Metz", "Luke", ""], ["Ibarz", "Julian", ""], ["Jaitly", "Navdeep", ""], ["Davidson", "James", ""]]}, {"id": "1705.05085", "submitter": "Vincent Zheng", "authors": "Hongyun Cai, Vincent W. Zheng, Kevin Chen-Chuan Chang", "title": "Active Learning for Graph Embedding", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph embedding provides an efficient solution for graph analysis by\nconverting the graph into a low-dimensional space which preserves the structure\ninformation. In contrast to the graph structure data, the i.i.d. node embedding\ncan be processed efficiently in terms of both time and space. Current\nsemi-supervised graph embedding algorithms assume the labelled nodes are given,\nwhich may not be always true in the real world. While manually label all\ntraining data is inapplicable, how to select the subset of training data to\nlabel so as to maximize the graph analysis task performance is of great\nimportance. This motivates our proposed active graph embedding (AGE) framework,\nin which we design a general active learning query strategy for any\nsemi-supervised graph embedding algorithm. AGE selects the most informative\nnodes as the training labelled nodes based on the graphical information (i.e.,\nnode centrality) as well as the learnt node embedding (i.e., node\nclassification uncertainty and node embedding representativeness). Different\nquery criteria are combined with the time-sensitive parameters which shift the\nfocus from graph based query criteria to embedding based criteria as the\nlearning progresses. Experiments have been conducted on three public data sets\nand the results verified the effectiveness of each component of our query\nstrategy and the power of combining them using time-sensitive parameters. Our\ncode is available online at: https://github.com/vwz/AGE.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 06:49:04 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Cai", "Hongyun", ""], ["Zheng", "Vincent W.", ""], ["Chang", "Kevin Chen-Chuan", ""]]}, {"id": "1705.05091", "submitter": "Ohad Shamir", "authors": "Nicol\\`o Cesa-Bianchi and Ohad Shamir", "title": "Bandit Regret Scaling with the Effective Loss Range", "comments": "The results in section 4 are incorrect as stated -- we have added an\n  erratum at the beginning of the document. The results in the other sections\n  are still valid. We thank \\'{E}tienne de Montbrun for locating the error", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how the regret guarantees of nonstochastic multi-armed bandits can\nbe improved, if the effective range of the losses in each round is small (e.g.\nthe maximal difference between two losses in a given round). Despite a recent\nimpossibility result, we show how this can be made possible under certain mild\nadditional assumptions, such as availability of rough estimates of the losses,\nor advance knowledge of the loss of a single, possibly unspecified arm. Along\nthe way, we develop a novel technique which might be of independent interest,\nto convert any multi-armed bandit algorithm with regret depending on the loss\nrange, to an algorithm with regret depending only on the effective range, while\navoiding predictably bad arms altogether.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 07:25:00 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 17:59:39 GMT"}, {"version": "v3", "created": "Thu, 2 Jan 2020 15:24:03 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Cesa-Bianchi", "Nicol\u00f2", ""], ["Shamir", "Ohad", ""]]}, {"id": "1705.05172", "submitter": "Thomas Moerland", "authors": "Thomas M. Moerland, Joost Broekens, Catholijn M. Jonker", "title": "Emotion in Reinforcement Learning Agents and Robots: A Survey", "comments": "To be published in Machine Learning Journal", "journal-ref": "Machine Learning 2017", "doi": "10.1007/s10994-017-5666-0", "report-no": null, "categories": "cs.LG cs.AI cs.HC cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article provides the first survey of computational models of emotion in\nreinforcement learning (RL) agents. The survey focuses on agent/robot emotions,\nand mostly ignores human user emotions. Emotions are recognized as functional\nin decision-making by influencing motivation and action selection. Therefore,\ncomputational emotion models are usually grounded in the agent's decision\nmaking architecture, of which RL is an important subclass. Studying emotions in\nRL-based agents is useful for three research fields. For machine learning (ML)\nresearchers, emotion models may improve learning efficiency. For the\ninteractive ML and human-robot interaction (HRI) community, emotions can\ncommunicate state and enhance user investment. Lastly, it allows affective\nmodelling (AM) researchers to investigate their emotion theories in a\nsuccessful AI agent class. This survey provides background on emotion theory\nand RL. It systematically addresses 1) from what underlying dimensions (e.g.,\nhomeostasis, appraisal) emotions can be derived and how these can be modelled\nin RL-agents, 2) what types of emotions have been derived from these\ndimensions, and 3) how these emotions may either influence the learning\nefficiency of the agent or be useful as social signals. We also systematically\ncompare evaluation criteria, and draw connections to important RL sub-domains\nlike (intrinsic) motivation and model-based RL. In short, this survey provides\nboth a practical overview for engineers wanting to implement emotions in their\nRL agents, and identifies challenges and directions for future emotion-RL\nresearch.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 11:49:56 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Moerland", "Thomas M.", ""], ["Broekens", "Joost", ""], ["Jonker", "Catholijn M.", ""]]}, {"id": "1705.05180", "submitter": "Ivan Kiskin", "authors": "Ivan Kiskin, Bernardo P\\'erez Orozco, Theo Windebank, Davide Zilli,\n  Marianne Sinka, Kathy Willis and Stephen Roberts", "title": "Mosquito Detection with Neural Networks: The Buzz of Deep Learning", "comments": "For data and software related to this paper, see\n  http://humbug.ac.uk/kiskin2017/. Submitted as a conference paper to ECML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SD stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world time-series analysis problems are characterised by scarce\ndata. Solutions typically rely on hand-crafted features extracted from the time\nor frequency domain allied with classification or regression engines which\ncondition on this (often low-dimensional) feature vector. The huge advances\nenjoyed by many application domains in recent years have been fuelled by the\nuse of deep learning architectures trained on large data sets. This paper\npresents an application of deep learning for acoustic event detection in a\nchallenging, data-scarce, real-world problem. Our candidate challenge is to\naccurately detect the presence of a mosquito from its acoustic signature. We\ndevelop convolutional neural networks (CNNs) operating on wavelet\ntransformations of audio recordings. Furthermore, we interrogate the network's\npredictive power by visualising statistics of network-excitatory samples. These\nvisualisations offer a deep insight into the relative informativeness of\ncomponents in the detection problem. We include comparisons with conventional\nclassifiers, conditioned on both hand-tuned and generic features, to stress the\nstrength of automatic deep feature learning. Detection is achieved with\nperformance metrics significantly surpassing those of existing algorithmic\nmethods, as well as marginally exceeding those attained by individual human\nexperts.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 12:19:15 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Kiskin", "Ivan", ""], ["Orozco", "Bernardo P\u00e9rez", ""], ["Windebank", "Theo", ""], ["Zilli", "Davide", ""], ["Sinka", "Marianne", ""], ["Willis", "Kathy", ""], ["Roberts", "Stephen", ""]]}, {"id": "1705.05197", "submitter": "Kishan Wimalawarne", "authors": "Kishan Wimalawarne, Makoto Yamada, Hiroshi Mamitsuka", "title": "Convex Coupled Matrix and Tensor Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a set of convex low rank inducing norms for a coupled matrices and\ntensors (hereafter coupled tensors), which shares information between matrices\nand tensors through common modes. More specifically, we propose a mixture of\nthe overlapped trace norm and the latent norms with the matrix trace norm, and\nthen, we propose a new completion algorithm based on the proposed norms. A key\nadvantage of the proposed norms is that it is convex and can find a globally\noptimal solution, while existing methods for coupled learning are non-convex.\nFurthermore, we analyze the excess risk bounds of the completion model\nregularized by our proposed norms which show that our proposed norms can\nexploit the low rankness of coupled tensors leading to better bounds compared\nto uncoupled norms. Through synthetic and real-world data experiments, we show\nthat the proposed completion algorithm compares favorably with existing\ncompletion algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 12:52:55 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 06:51:29 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Wimalawarne", "Kishan", ""], ["Yamada", "Makoto", ""], ["Mamitsuka", "Hiroshi", ""]]}, {"id": "1705.05264", "submitter": "Nicolas Papernot", "authors": "Nicolas Papernot and Patrick McDaniel", "title": "Extending Defensive Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is vulnerable to adversarial examples: inputs carefully\nmodified to force misclassification. Designing defenses against such inputs\nremains largely an open problem. In this work, we revisit defensive\ndistillation---which is one of the mechanisms proposed to mitigate adversarial\nexamples---to address its limitations. We view our results not only as an\neffective way of addressing some of the recently discovered attacks but also as\nreinforcing the importance of improved training techniques.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 14:25:15 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Papernot", "Nicolas", ""], ["McDaniel", "Patrick", ""]]}, {"id": "1705.05278", "submitter": "Christopher Beckham", "authors": "Christopher Beckham, Christopher Pal", "title": "Unimodal probability distributions for deep ordinal classification", "comments": "Accepted for publication for ICML2017. This is the camera-ready\n  version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probability distributions produced by the cross-entropy loss for ordinal\nclassification problems can possess undesired properties. We propose a\nstraightforward technique to constrain discrete ordinal probability\ndistributions to be unimodal via the use of the Poisson and binomial\nprobability distributions. We evaluate this approach in the context of deep\nlearning on two large ordinal image datasets, obtaining promising results.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 14:59:26 GMT"}, {"version": "v2", "created": "Thu, 22 Jun 2017 13:01:25 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Beckham", "Christopher", ""], ["Pal", "Christopher", ""]]}, {"id": "1705.05355", "submitter": "Nicol\\'o Fusi", "authors": "Nicolo Fusi, Rishit Sheth, Huseyn Melih Elibol", "title": "Probabilistic Matrix Factorization for Automated Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to achieve state-of-the-art performance, modern machine learning\ntechniques require careful data pre-processing and hyperparameter tuning.\nMoreover, given the ever increasing number of machine learning models being\ndeveloped, model selection is becoming increasingly important. Automating the\nselection and tuning of machine learning pipelines consisting of data\npre-processing methods and machine learning models, has long been one of the\ngoals of the machine learning community. In this paper, we tackle this\nmeta-learning task by combining ideas from collaborative filtering and Bayesian\noptimization. Using probabilistic matrix factorization techniques and\nacquisition functions from Bayesian optimization, we exploit experiments\nperformed in hundreds of different datasets to guide the exploration of the\nspace of possible pipelines. In our experiments, we show that our approach\nquickly identifies high-performing pipelines across a wide range of datasets,\nsignificantly outperforming the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 17:47:26 GMT"}, {"version": "v2", "created": "Tue, 1 May 2018 19:52:31 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Fusi", "Nicolo", ""], ["Sheth", "Rishit", ""], ["Elibol", "Huseyn Melih", ""]]}, {"id": "1705.05363", "submitter": "Deepak Pathak", "authors": "Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, Trevor Darrell", "title": "Curiosity-driven Exploration by Self-supervised Prediction", "comments": "In ICML 2017. Website at https://pathak22.github.io/noreward-rl/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world scenarios, rewards extrinsic to the agent are extremely\nsparse, or absent altogether. In such cases, curiosity can serve as an\nintrinsic reward signal to enable the agent to explore its environment and\nlearn skills that might be useful later in its life. We formulate curiosity as\nthe error in an agent's ability to predict the consequence of its own actions\nin a visual feature space learned by a self-supervised inverse dynamics model.\nOur formulation scales to high-dimensional continuous state spaces like images,\nbypasses the difficulties of directly predicting pixels, and, critically,\nignores the aspects of the environment that cannot affect the agent. The\nproposed approach is evaluated in two environments: VizDoom and Super Mario\nBros. Three broad settings are investigated: 1) sparse extrinsic reward, where\ncuriosity allows for far fewer interactions with the environment to reach the\ngoal; 2) exploration with no extrinsic reward, where curiosity pushes the agent\nto explore more efficiently; and 3) generalization to unseen scenarios (e.g.\nnew levels of the same game) where the knowledge gained from earlier experience\nhelps the agent explore new places much faster than starting from scratch. Demo\nvideo and code available at https://pathak22.github.io/noreward-rl/\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 17:56:22 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Pathak", "Deepak", ""], ["Agrawal", "Pulkit", ""], ["Efros", "Alexei A.", ""], ["Darrell", "Trevor", ""]]}, {"id": "1705.05396", "submitter": "Avi Pfeffer", "authors": "Avi Pfeffer", "title": "Learning Probabilistic Programs Using Backpropagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic modeling enables combining domain knowledge with learning from\ndata, thereby supporting learning from fewer training instances than purely\ndata-driven methods. However, learning probabilistic models is difficult and\nhas not achieved the level of performance of methods such as deep neural\nnetworks on many tasks. In this paper, we attempt to address this issue by\npresenting a method for learning the parameters of a probabilistic program\nusing backpropagation. Our approach opens the possibility to building deep\nprobabilistic programming models that are trained in a similar way to neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 18:07:31 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Pfeffer", "Avi", ""]]}, {"id": "1705.05403", "submitter": "Ludovica Bachschmid-Romano", "authors": "Ludovica Bachschmid-Romano and Manfred Opper", "title": "A statistical physics approach to learning curves for the Inverse Ising\n  problem", "comments": null, "journal-ref": null, "doi": "10.1088/1742-5468/aa727d", "report-no": null, "categories": "cond-mat.dis-nn stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using methods of statistical physics, we analyse the error of learning\ncouplings in large Ising models from independent data (the inverse Ising\nproblem). We concentrate on learning based on local cost functions, such as the\npseudo-likelihood method for which the couplings are inferred independently for\neach spin. Assuming that the data are generated from a true Ising model, we\ncompute the reconstruction error of the couplings using a combination of the\nreplica method with the cavity approach for densely connected systems. We show\nthat an explicit estimator based on a quadratic cost function achieves minimal\nreconstruction error, but requires the length of the true coupling vector as\nprior knowledge. A simple mean field estimator of the couplings which does not\nneed such knowledge is asymptotically optimal, i.e. when the number of\nobservations is much large than the number of spins. Comparison of the theory\nwith numerical simulations shows excellent agreement for data generated from\ntwo models with random couplings in the high temperature region: a model with\nindependent couplings (Sherrington-Kirkpatrick model), and a model where the\nmatrix of couplings has a Wishart distribution.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 18:22:03 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Bachschmid-Romano", "Ludovica", ""], ["Opper", "Manfred", ""]]}, {"id": "1705.05487", "submitter": "Franck Dernoncourt", "authors": "Franck Dernoncourt, Ji Young Lee, Peter Szolovits", "title": "NeuroNER: an easy-to-use program for named-entity recognition based on\n  neural networks", "comments": "The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named-entity recognition (NER) aims at identifying entities of interest in a\ntext. Artificial neural networks (ANNs) have recently been shown to outperform\nexisting NER systems. However, ANNs remain challenging to use for non-expert\nusers. In this paper, we present NeuroNER, an easy-to-use named-entity\nrecognition tool based on ANNs. Users can annotate entities using a graphical\nweb-based user interface (BRAT): the annotations are then used to train an ANN,\nwhich in turn predict entities' locations and categories in new texts. NeuroNER\nmakes this annotation-training-prediction flow smooth and accessible to anyone.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 00:03:19 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Dernoncourt", "Franck", ""], ["Lee", "Ji Young", ""], ["Szolovits", "Peter", ""]]}, {"id": "1705.05491", "submitter": "Lili Su", "authors": "Yudong Chen, Lili Su, Jiaming Xu", "title": "Distributed Statistical Machine Learning in Adversarial Settings:\n  Byzantine Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of distributed statistical machine learning in\nadversarial settings, where some unknown and time-varying subset of working\nmachines may be compromised and behave arbitrarily to prevent an accurate model\nfrom being learned. This setting captures the potential adversarial attacks\nfaced by Federated Learning -- a modern machine learning paradigm that is\nproposed by Google researchers and has been intensively studied for ensuring\nuser privacy. Formally, we focus on a distributed system consisting of a\nparameter server and $m$ working machines. Each working machine keeps $N/m$\ndata samples, where $N$ is the total number of samples. The goal is to\ncollectively learn the underlying true model parameter of dimension $d$.\n  In classical batch gradient descent methods, the gradients reported to the\nserver by the working machines are aggregated via simple averaging, which is\nvulnerable to a single Byzantine failure. In this paper, we propose a Byzantine\ngradient descent method based on the geometric median of means of the\ngradients. We show that our method can tolerate $q \\le (m-1)/2$ Byzantine\nfailures, and the parameter estimate converges in $O(\\log N)$ rounds with an\nestimation error of $\\sqrt{d(2q+1)/N}$, hence approaching the optimal error\nrate $\\sqrt{d/N}$ in the centralized and failure-free setting. The total\ncomputational complexity of our algorithm is of $O((Nd/m) \\log N)$ at each\nworking machine and $O(md + kd \\log^3 N)$ at the central server, and the total\ncommunication cost is of $O(m d \\log N)$. We further provide an application of\nour general results to the linear regression problem.\n  A key challenge arises in the above problem is that Byzantine failures create\narbitrary and unspecified dependency among the iterations and the aggregated\ngradients. We prove that the aggregated gradient converges uniformly to the\ntrue gradient function.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 00:20:49 GMT"}, {"version": "v2", "created": "Mon, 23 Oct 2017 01:16:25 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Chen", "Yudong", ""], ["Su", "Lili", ""], ["Xu", "Jiaming", ""]]}, {"id": "1705.05495", "submitter": "Adrian Wills", "authors": "Adrian G. Wills and Johannes Hendriks and Christopher Renton and Brett\n  Ninness", "title": "A Bayesian Filtering Algorithm for Gaussian Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bayesian filtering algorithm is developed for a class of state-space\nsystems that can be modelled via Gaussian mixtures. In general, the exact\nsolution to this filtering problem involves an exponential growth in the number\nof mixture terms and this is handled here by utilising a Gaussian mixture\nreduction step after both the time and measurement updates. In addition, a\nsquare-root implementation of the unified algorithm is presented and this\nalgorithm is profiled on several simulated systems. This includes the state\nestimation for two non-linear systems that are strictly outside the class\nconsidered in this paper.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 01:05:16 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Wills", "Adrian G.", ""], ["Hendriks", "Johannes", ""], ["Renton", "Christopher", ""], ["Ninness", "Brett", ""]]}, {"id": "1705.05502", "submitter": "David Rolnick", "authors": "David Rolnick (MIT), Max Tegmark (MIT)", "title": "The power of deeper networks for expressing natural functions", "comments": "Replaced to match version published at ICLR 2018. 14 pages, 2 figs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that neural networks are universal approximators, but that\ndeeper networks tend in practice to be more powerful than shallower ones. We\nshed light on this by proving that the total number of neurons $m$ required to\napproximate natural classes of multivariate polynomials of $n$ variables grows\nonly linearly with $n$ for deep neural networks, but grows exponentially when\nmerely a single hidden layer is allowed. We also provide evidence that when the\nnumber of hidden layers is increased from $1$ to $k$, the neuron requirement\ngrows exponentially not with $n$ but with $n^{1/k}$, suggesting that the\nminimum number of layers required for practical expressibility grows only\nlogarithmically with $n$.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 02:02:24 GMT"}, {"version": "v2", "created": "Fri, 27 Apr 2018 17:52:03 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Rolnick", "David", "", "MIT"], ["Tegmark", "Max", "", "MIT"]]}, {"id": "1705.05524", "submitter": "Dieterich Lawson", "authors": "Dieterich Lawson, Chung-Cheng Chiu, George Tucker, Colin Raffel, Kevin\n  Swersky, Navdeep Jaitly", "title": "Learning Hard Alignments with Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has recently been significant interest in hard attention models for\ntasks such as object recognition, visual captioning and speech recognition.\nHard attention can offer benefits over soft attention such as decreased\ncomputational cost, but training hard attention models can be difficult because\nof the discrete latent variables they introduce. Previous work used REINFORCE\nand Q-learning to approach these issues, but those methods can provide\nhigh-variance gradient estimates and be slow to train. In this paper, we tackle\nthe problem of learning hard attention for a sequential task using variational\ninference methods, specifically the recently introduced VIMCO and NVIL.\nFurthermore, we propose a novel baseline that adapts VIMCO to this setting. We\ndemonstrate our method on a phoneme recognition task in clean and noisy\nenvironments and show that our method outperforms REINFORCE, with the\ndifference being greater for a more complicated task.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 04:30:56 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 19:08:18 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Lawson", "Dieterich", ""], ["Chiu", "Chung-Cheng", ""], ["Tucker", "George", ""], ["Raffel", "Colin", ""], ["Swersky", "Kevin", ""], ["Jaitly", "Navdeep", ""]]}, {"id": "1705.05543", "submitter": "Sen Zhao", "authors": "Sen Zhao, Daniela Witten, Ali Shojaie", "title": "In Defense of the Indefensible: A Very Naive Approach to\n  High-Dimensional Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A great deal of interest has recently focused on conducting inference on the\nparameters in a high-dimensional linear model.\n  In this paper, we consider a simple and very na\\\"{i}ve two-step procedure for\nthis task, in which we (i) fit a lasso model in order to obtain a subset of the\nvariables, and (ii) fit a least squares model on the lasso-selected set.\nConventional statistical wisdom tells us that we cannot make use of the\nstandard statistical inference tools for the resulting least squares model\n(such as confidence intervals and $p$-values), since we peeked at the data\ntwice: once in running the lasso, and again in fitting the least squares model.\nHowever, in this paper, we show that under a certain set of assumptions, with\nhigh probability, the set of variables selected by the lasso is identical to\nthe one selected by the noiseless lasso and is hence deterministic.\nConsequently, the na\\\"{i}ve two-step approach can yield asymptotically valid\ninference. We utilize this finding to develop the \\emph{na\\\"ive confidence\ninterval}, which can be used to draw inference on the regression coefficients\nof the model selected by the lasso, as well as the \\emph{na\\\"ive score test},\nwhich can be used to test the hypotheses regarding the full-model regression\ncoefficients.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 06:05:18 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 21:06:24 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2020 03:16:30 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Zhao", "Sen", ""], ["Witten", "Daniela", ""], ["Shojaie", "Ali", ""]]}, {"id": "1705.05591", "submitter": "Ha Nguyen", "authors": "Ha Q. Nguyen and Emrah Bostan and Michael Unser", "title": "Learning Convex Regularizers for Optimal Bayesian Denoising", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2777407", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a data-driven algorithm for the maximum a posteriori (MAP)\nestimation of stochastic processes from noisy observations. The primary\nstatistical properties of the sought signal is specified by the penalty\nfunction (i.e., negative logarithm of the prior probability density function).\nOur alternating direction method of multipliers (ADMM)-based approach\ntranslates the estimation task into successive applications of the proximal\nmapping of the penalty function. Capitalizing on this direct link, we define\nthe proximal operator as a parametric spline curve and optimize the spline\ncoefficients by minimizing the average reconstruction error for a given\ntraining set. The key aspects of our learning method are that the associated\npenalty function is constrained to be convex and the convergence of the ADMM\niterations is proven. As a result of these theoretical guarantees, adaptation\nof the proposed framework to different levels of measurement noise is extremely\nsimple and does not require any retraining. We apply our method to estimation\nof both sparse and non-sparse models of L\\'{e}vy processes for which the\nminimum mean square error (MMSE) estimators are available. We carry out a\nsingle training session and perform comparisons at various signal-to-noise\nratio (SNR) values. Simulations illustrate that the performance of our\nalgorithm is practically identical to the one of the MMSE estimator\nirrespective of the noise power.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 08:39:46 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Nguyen", "Ha Q.", ""], ["Bostan", "Emrah", ""], ["Unser", "Michael", ""]]}, {"id": "1705.05598", "submitter": "Pieter-Jan Kindermans", "authors": "Pieter-Jan Kindermans, Kristof T. Sch\\\"utt, Maximilian Alber,\n  Klaus-Robert M\\\"uller, Dumitru Erhan, Been Kim, Sven D\\\"ahne", "title": "Learning how to explain neural networks: PatternNet and\n  PatternAttribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DeConvNet, Guided BackProp, LRP, were invented to better understand deep\nneural networks. We show that these methods do not produce the theoretically\ncorrect explanation for a linear model. Yet they are used on multi-layer\nnetworks with millions of parameters. This is a cause for concern since linear\nmodels are simple neural networks. We argue that explanation methods for neural\nnets should work reliably in the limit of simplicity, the linear models. Based\non our analysis of linear models we propose a generalization that yields two\nexplanation techniques (PatternNet and PatternAttribution) that are\ntheoretically sound for linear models and produce improved explanations for\ndeep networks.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 08:58:25 GMT"}, {"version": "v2", "created": "Tue, 24 Oct 2017 23:10:33 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Kindermans", "Pieter-Jan", ""], ["Sch\u00fctt", "Kristof T.", ""], ["Alber", "Maximilian", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Erhan", "Dumitru", ""], ["Kim", "Been", ""], ["D\u00e4hne", "Sven", ""]]}, {"id": "1705.05603", "submitter": "Luca Ambrogioni", "authors": "Luca Ambrogioni, Max Hinne, Marcel van Gerven and Eric Maris", "title": "GP CaKe: Effective brain connectivity with causal kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental goal in network neuroscience is to understand how activity in\none region drives activity elsewhere, a process referred to as effective\nconnectivity. Here we propose to model this causal interaction using\nintegro-differential equations and causal kernels that allow for a rich\nanalysis of effective connectivity. The approach combines the tractability and\nflexibility of autoregressive modeling with the biophysical interpretability of\ndynamic causal modeling. The causal kernels are learned nonparametrically using\nGaussian process regression, yielding an efficient framework for causal\ninference. We construct a novel class of causal covariance functions that\nenforce the desired properties of the causal kernels, an approach which we call\nGP CaKe. By construction, the model and its hyperparameters have biophysical\nmeaning and are therefore easily interpretable. We demonstrate the efficacy of\nGP CaKe on a number of simulations and give an example of a realistic\napplication on magnetoencephalography (MEG) data.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 09:07:13 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Ambrogioni", "Luca", ""], ["Hinne", "Max", ""], ["van Gerven", "Marcel", ""], ["Maris", "Eric", ""]]}, {"id": "1705.05615", "submitter": "Sami Abu-El-Haija", "authors": "Sami Abu-El-Haija, Bryan Perozzi, Rami Al-Rfou", "title": "Learning Edge Representations via Low-Rank Asymmetric Projections", "comments": null, "journal-ref": "ACM International Conference on Information and Knowledge\n  Management, 2017", "doi": "10.1145/3132847.3132959", "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for embedding graphs while preserving directed edge\ninformation. Learning such continuous-space vector representations (or\nembeddings) of nodes in a graph is an important first step for using network\ninformation (from social networks, user-item graphs, knowledge bases, etc.) in\nmany machine learning tasks.\n  Unlike previous work, we (1) explicitly model an edge as a function of node\nembeddings, and we (2) propose a novel objective, the \"graph likelihood\", which\ncontrasts information from sampled random walks with non-existent edges.\nIndividually, both of these contributions improve the learned representations,\nespecially when there are memory constraints on the total size of the\nembeddings. When combined, our contributions enable us to significantly improve\nthe state-of-the-art by learning more concise representations that better\npreserve the graph structure.\n  We evaluate our method on a variety of link-prediction task including social\nnetworks, collaboration networks, and protein interactions, showing that our\nproposed method learn representations with error reductions of up to 76% and\n55%, on directed and undirected graphs. In addition, we show that the\nrepresentations learned by our method are quite space efficient, producing\nembeddings which have higher structure-preserving accuracy but are 10 times\nsmaller.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 09:44:28 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 23:15:39 GMT"}, {"version": "v3", "created": "Mon, 29 May 2017 12:00:41 GMT"}, {"version": "v4", "created": "Wed, 13 Sep 2017 18:21:14 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Abu-El-Haija", "Sami", ""], ["Perozzi", "Bryan", ""], ["Al-Rfou", "Rami", ""]]}, {"id": "1705.05654", "submitter": "Philipp Probst", "authors": "Philipp Probst, Anne-Laure Boulesteix", "title": "To tune or not to tune the number of trees in random forest?", "comments": "20 pages, 4 figures", "journal-ref": "Journal of Machine Learning Research 18 (2018) 1-18", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of trees T in the random forest (RF) algorithm for supervised\nlearning has to be set by the user. It is controversial whether T should simply\nbe set to the largest computationally manageable value or whether a smaller T\nmay in some cases be better. While the principle underlying bagging is that\n\"more trees are better\", in practice the classification error rate sometimes\nreaches a minimum before increasing again for increasing number of trees. The\ngoal of this paper is four-fold: (i) providing theoretical results showing that\nthe expected error rate may be a non-monotonous function of the number of trees\nand explaining under which circumstances this happens; (ii) providing\ntheoretical results showing that such non-monotonous patterns cannot be\nobserved for other performance measures such as the Brier score and the\nlogarithmic loss (for classification) and the mean squared error (for\nregression); (iii) illustrating the extent of the problem through an\napplication to a large number (n = 306) of datasets from the public database\nOpenML; (iv) finally arguing in favor of setting it to a computationally\nfeasible large number, depending on convergence properties of the desired\nperformance measure.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 11:38:12 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Probst", "Philipp", ""], ["Boulesteix", "Anne-Laure", ""]]}, {"id": "1705.05681", "submitter": "Brijnesh Jain", "authors": "Brijnesh J. Jain and David Schultz", "title": "Optimal Warping Paths are unique for almost every Pair of Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Update rules for learning in dynamic time warping spaces are based on optimal\nwarping paths between parameter and input time series. In general, optimal\nwarping paths are not unique resulting in adverse effects in theory and\npractice. Under the assumption of squared error local costs, we show that no\ntwo warping paths have identical costs almost everywhere in a measure-theoretic\nsense. Two direct consequences of this result are: (i) optimal warping paths\nare unique almost everywhere, and (ii) the set of all pairs of time series with\nmultiple equal-cost warping paths coincides with the union of exponentially\nmany zero sets of quadratic forms. One implication of the proposed results is\nthat typical distance-based cost functions such as the k-means objective are\ndifferentiable almost everywhere and can be minimized by subgradient methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 12:41:25 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 11:18:36 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Jain", "Brijnesh J.", ""], ["Schultz", "David", ""]]}, {"id": "1705.05782", "submitter": "Claudio Gallicchio", "authors": "Claudio Gallicchio, Alessio Micheli, Luca Pedrelli", "title": "Hierarchical Temporal Representation in Linear Reservoir Computing", "comments": "This is a pre-print of the paper submitted to the 27th Italian\n  Workshop on Neural Networks, WIRN 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, studies on deep Reservoir Computing (RC) highlighted the role of\nlayering in deep recurrent neural networks (RNNs). In this paper, the use of\nlinear recurrent units allows us to bring more evidence on the intrinsic\nhierarchical temporal representation in deep RNNs through frequency analysis\napplied to the state signals. The potentiality of our approach is assessed on\nthe class of Multiple Superimposed Oscillator tasks. Furthermore, our\ninvestigation provides useful insights to open a discussion on the main aspects\nthat characterize the deep learning framework in the temporal domain.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 16:03:00 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 10:30:52 GMT"}, {"version": "v3", "created": "Mon, 5 Jun 2017 12:03:06 GMT"}, {"version": "v4", "created": "Thu, 8 Jun 2017 16:00:15 GMT"}, {"version": "v5", "created": "Mon, 10 Jul 2017 10:51:24 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Gallicchio", "Claudio", ""], ["Micheli", "Alessio", ""], ["Pedrelli", "Luca", ""]]}, {"id": "1705.05785", "submitter": "Sebastijan Dumancic", "authors": "Sebastijan Duman\\v{c}i\\'c and Hendrik Blockeel", "title": "Demystifying Relational Latent Representations", "comments": "12 pages, 8 figures; accepted to ILP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent features learned by deep learning approaches have proven to be a\npowerful tool for machine learning. They serve as a data abstraction that makes\nlearning easier by capturing regularities in data explicitly. Their benefits\nmotivated their adaptation to relational learning context. In our previous\nwork, we introduce an approach that learns relational latent features by means\nof clustering instances and their relations. The major drawback of latent\nrepresentations is that they are often black-box and difficult to interpret.\nThis work addresses these issues and shows that (1) latent features created by\nclustering are interpretable and capture interesting properties of data; (2)\nthey identify local regions of instances that match well with the label, which\npartially explains their benefit; and (3) although the number of latent\nfeatures generated by this approach is large, often many of them are highly\nredundant and can be removed without hurting performance much.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 16:06:59 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 13:46:17 GMT"}, {"version": "v3", "created": "Fri, 29 Sep 2017 07:36:32 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Duman\u010di\u0107", "Sebastijan", ""], ["Blockeel", "Hendrik", ""]]}, {"id": "1705.05804", "submitter": "Vamsi Ithapu", "authors": "Vamsi K. Ithapu, Risi Kondor, Sterling C. Johnson, Vikas Singh", "title": "The Incremental Multiresolution Matrix Factorization Algorithm", "comments": "Computer Vision and Pattern Recognition (CVPR) 2017, 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiresolution analysis and matrix factorization are foundational tools in\ncomputer vision. In this work, we study the interface between these two\ndistinct topics and obtain techniques to uncover hierarchical block structure\nin symmetric matrices -- an important aspect in the success of many vision\nproblems. Our new algorithm, the incremental multiresolution matrix\nfactorization, uncovers such structure one feature at a time, and hence scales\nwell to large matrices. We describe how this multiscale analysis goes much\nfarther than what a direct global factorization of the data can identify. We\nevaluate the efficacy of the resulting factorizations for relative leveraging\nwithin regression tasks using medical imaging data. We also use the\nfactorization on representations learned by popular deep networks, providing\nevidence of their ability to infer semantic relationships even when they are\nnot explicitly trained to do so. We show that this algorithm can be used as an\nexploratory tool to improve the network architecture, and within numerous other\nsettings in vision.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 17:10:05 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Ithapu", "Vamsi K.", ""], ["Kondor", "Risi", ""], ["Johnson", "Sterling C.", ""], ["Singh", "Vikas", ""]]}, {"id": "1705.05823", "submitter": "Oren Rippel", "authors": "Oren Rippel, Lubomir Bourdev", "title": "Real-Time Adaptive Image Compression", "comments": "Published at ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a machine learning-based approach to lossy image compression which\noutperforms all existing codecs, while running in real-time.\n  Our algorithm typically produces files 2.5 times smaller than JPEG and JPEG\n2000, 2 times smaller than WebP, and 1.7 times smaller than BPG on datasets of\ngeneric images across all quality levels. At the same time, our codec is\ndesigned to be lightweight and deployable: for example, it can encode or decode\nthe Kodak dataset in around 10ms per image on GPU.\n  Our architecture is an autoencoder featuring pyramidal analysis, an adaptive\ncoding module, and regularization of the expected codelength. We also\nsupplement our approach with adversarial training specialized towards use in a\ncompression setting: this enables us to produce visually pleasing\nreconstructions for very low bitrates.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 17:51:07 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Rippel", "Oren", ""], ["Bourdev", "Lubomir", ""]]}, {"id": "1705.05884", "submitter": "Babak Toghiani-Rizi", "authors": "Babak Toghiani-Rizi, Christofer Lind, Maria Svensson, Marcus Windmark", "title": "Static Gesture Recognition using Leap Motion", "comments": "Results based on a study conducted during the course Intelligent\n  Interactive Systems at Uppsala University, spring 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, an automated bartender system was developed for making orders\nin a bar using hand gestures. The gesture recognition of the system was\ndeveloped using Machine Learning techniques, where the model was trained to\nclassify gestures using collected data. The final model used in the system\nreached an average accuracy of 95%. The system raised ethical concerns both in\nterms of user interaction and having such a system in a real world scenario,\nbut it could initially work as a complement to a real bartender.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 19:38:20 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Toghiani-Rizi", "Babak", ""], ["Lind", "Christofer", ""], ["Svensson", "Maria", ""], ["Windmark", "Marcus", ""]]}, {"id": "1705.05907", "submitter": "Philipp Marquetand", "authors": "Michael Gastegger, J\\\"org Behler, Philipp Marquetand", "title": "Machine Learning Molecular Dynamics for the Simulation of Infrared\n  Spectra", "comments": "12 pages, 9 figures", "journal-ref": "Chem. Sci., 8, 6924-6935 (2017)", "doi": "10.1039/C7SC02267K", "report-no": null, "categories": "physics.chem-ph physics.bio-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has emerged as an invaluable tool in many research areas. In\nthe present work, we harness this power to predict highly accurate molecular\ninfrared spectra with unprecedented computational efficiency. To account for\nvibrational anharmonic and dynamical effects -- typically neglected by\nconventional quantum chemistry approaches -- we base our machine learning\nstrategy on ab initio molecular dynamics simulations. While these simulations\nare usually extremely time consuming even for small molecules, we overcome\nthese limitations by leveraging the power of a variety of machine learning\ntechniques, not only accelerating simulations by several orders of magnitude,\nbut also greatly extending the size of systems that can be treated. To this\nend, we develop a molecular dipole moment model based on environment dependent\nneural network charges and combine it with the neural network potentials of\nBehler and Parrinello. Contrary to the prevalent big data philosophy, we are\nable to obtain very accurate machine learning models for the prediction of\ninfrared spectra based on only a few hundreds of electronic structure reference\npoints. This is made possible through the introduction of a fully automated\nsampling scheme and the use of molecular forces during neural network potential\ntraining. We demonstrate the power of our machine learning approach by applying\nit to model the infrared spectra of a methanol molecule, n-alkanes containing\nup to 200 atoms and the protonated alanine tripeptide, which at the same time\nrepresents the first application of machine learning techniques to simulate the\ndynamics of a peptide. In all these case studies we find excellent agreement\nbetween the infrared spectra predicted via machine learning models and the\nrespective theoretical and experimental spectra.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 20:42:25 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Gastegger", "Michael", ""], ["Behler", "J\u00f6rg", ""], ["Marquetand", "Philipp", ""]]}, {"id": "1705.05933", "submitter": "Aurelien Lucchi", "authors": "Jonas Moritz Kohler and Aurelien Lucchi", "title": "Sub-sampled Cubic Regularization for Non-convex Optimization", "comments": "Proceedings of the 34th International Conference on Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the minimization of non-convex functions that typically arise in\nmachine learning. Specifically, we focus our attention on a variant of trust\nregion methods known as cubic regularization. This approach is particularly\nattractive because it escapes strict saddle points and it provides stronger\nconvergence guarantees than first- and second-order as well as classical trust\nregion methods. However, it suffers from a high computational complexity that\nmakes it impractical for large-scale learning. Here, we propose a novel method\nthat uses sub-sampling to lower this computational cost. By the use of\nconcentration inequalities we provide a sampling scheme that gives sufficiently\naccurate gradient and Hessian approximations to retain the strong global and\nlocal convergence guarantees of cubically regularized methods. To the best of\nour knowledge this is the first work that gives global convergence guarantees\nfor a sub-sampled variant of cubic regularization on non-convex functions.\nFurthermore, we provide experimental results supporting our theory.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 21:44:44 GMT"}, {"version": "v2", "created": "Fri, 23 Jun 2017 11:49:57 GMT"}, {"version": "v3", "created": "Sat, 1 Jul 2017 11:17:59 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Kohler", "Jonas Moritz", ""], ["Lucchi", "Aurelien", ""]]}, {"id": "1705.05950", "submitter": "Dmitrii Marin", "authors": "Dmitrii Marin, Meng Tang, Ismail Ben Ayed, Yuri Boykov", "title": "Kernel clustering: density biases and solutions", "comments": null, "journal-ref": null, "doi": "10.1109/TPAMI.2017.2780166", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel methods are popular in clustering due to their generality and\ndiscriminating power. However, we show that many kernel clustering criteria\nhave density biases theoretically explaining some practically significant\nartifacts empirically observed in the past. For example, we provide conditions\nand formally prove the density mode isolation bias in kernel K-means for a\ncommon class of kernels. We call it Breiman's bias due to its similarity to the\nhistogram mode isolation previously discovered by Breiman in decision tree\nlearning with Gini impurity. We also extend our analysis to other popular\nkernel clustering methods, e.g. average/normalized cut or dominant sets, where\ndensity biases can take different forms. For example, splitting isolated points\nby cut-based criteria is essentially the sparsest subset bias, which is the\nopposite of the density mode bias. Our findings suggest that a principled\nsolution for density biases in kernel clustering should directly address data\ninhomogeneity. We show that density equalization can be implicitly achieved\nusing either locally adaptive weights or locally adaptive kernels. Moreover,\ndensity equalization makes many popular kernel clustering objectives\nequivalent. Our synthetic and real data experiments illustrate density biases\nand proposed solutions. We anticipate that theoretical understanding of kernel\nclustering limitations and their principled solutions will be important for a\nbroad spectrum of data analysis applications across the disciplines.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 23:07:59 GMT"}, {"version": "v2", "created": "Thu, 1 Jun 2017 15:04:20 GMT"}, {"version": "v3", "created": "Thu, 8 Jun 2017 14:41:13 GMT"}, {"version": "v4", "created": "Fri, 30 Jun 2017 03:44:11 GMT"}, {"version": "v5", "created": "Wed, 6 Dec 2017 16:42:58 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Marin", "Dmitrii", ""], ["Tang", "Meng", ""], ["Ayed", "Ismail Ben", ""], ["Boykov", "Yuri", ""]]}, {"id": "1705.06025", "submitter": "Caifa Zhou", "authors": "Caifa Zhou and Yang Gu", "title": "Joint Positioning and Radio Map Generation Based on Stochastic\n  Variational Bayesian Inference for FWIPS", "comments": "10 pages, 16 figures, and 2 tables. A paper under review of IPIN 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerprinting based WLAN indoor positioning system (FWIPS) provides a\npromising indoor positioning solution to meet the growing interests for indoor\nlocation-based services (e.g., indoor way finding or geo-fencing). FWIPS is\npreferred because it requires no additional infrastructure for deploying an\nFWIPS and achieving the position estimation by reusing the available WLAN and\nmobile devices, and capable of providing absolute position estimation. For\nfingerprinting based positioning (FbP), a model is created to provide reference\nvalues of observable features (e.g., signal strength from access point (AP)) as\na function of location during offline stage. One widely applied method to build\na complete and an accurate reference database (i.e. radio map (RM)) for FWIPS\nis carrying out a site survey throughout the region of interest (RoI). Along\nthe site survey, the readings of received signal strength (RSS) from all\nvisible APs at each reference point (RP) are collected. This site survey,\nhowever, is time-consuming and labor-intensive, especially in the case that the\nRoI is large (e.g., an airport or a big mall). This bottleneck hinders the wide\ncommercial applications of FWIPS (e.g., proximity promotions in a shopping\ncenter). To diminish the cost of site survey, we propose a probabilistic model,\nwhich combines fingerprinting based positioning (FbP) and RM generation based\non stochastic variational Bayesian inference (SVBI). This SVBI based position\nand RSS estimation has three properties: i) being able to predict the\ndistribution of the estimated position and RSS, ii) treating each observation\nof RSS at each RP as an example to learn for FbP and RM generation instead of\nusing the whole RM as an example, and iii) requiring only one time training of\nthe SVBI model for both localization and RSS estimation. These benefits make it\noutperforms the previous proposed approaches.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 06:48:05 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Zhou", "Caifa", ""], ["Gu", "Yang", ""]]}, {"id": "1705.06168", "submitter": "Debarghya Ghoshdastidar", "authors": "Debarghya Ghoshdastidar, Maurilio Gutzeit, Alexandra Carpentier,\n  Ulrike von Luxburg", "title": "Two-Sample Tests for Large Random Graphs Using Network Statistics", "comments": "To be presented in COLT 2017 (author sequence, funding details and\n  minor typos updated in version 2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a two-sample hypothesis testing problem, where the distributions\nare defined on the space of undirected graphs, and one has access to only one\nobservation from each model. A motivating example for this problem is comparing\nthe friendship networks on Facebook and LinkedIn. The practical approach to\nsuch problems is to compare the networks based on certain network statistics.\nIn this paper, we present a general principle for two-sample hypothesis testing\nin such scenarios without making any assumption about the network generation\nprocess. The main contribution of the paper is a general formulation of the\nproblem based on concentration of network statistics, and consequently, a\nconsistent two-sample test that arises as the natural solution for this\nproblem. We also show that the proposed test is minimax optimal for certain\nnetwork statistics.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 14:05:59 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 13:27:07 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Ghoshdastidar", "Debarghya", ""], ["Gutzeit", "Maurilio", ""], ["Carpentier", "Alexandra", ""], ["von Luxburg", "Ulrike", ""]]}, {"id": "1705.06189", "submitter": "Charlotte Laclau", "authors": "Charlotte Laclau, Ievgen Redko, Basarab Matei, Youn\\`es Bennani,\n  Vincent Brault", "title": "Co-clustering through Optimal Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel method for co-clustering, an unsupervised\nlearning approach that aims at discovering homogeneous groups of data instances\nand features by grouping them simultaneously. The proposed method uses the\nentropy regularized optimal transport between empirical measures defined on\ndata instances and features in order to obtain an estimated joint probability\ndensity function represented by the optimal coupling matrix. This matrix is\nfurther factorized to obtain the induced row and columns partitions using\nmultiscale representations approach. To justify our method theoretically, we\nshow how the solution of the regularized optimal transport can be seen from the\nvariational inference perspective thus motivating its use for co-clustering.\nThe algorithm derived for the proposed method and its kernelized version based\non the notion of Gromov-Wasserstein distance are fast, accurate and can\ndetermine automatically the number of both row and column clusters. These\nfeatures are vividly demonstrated through extensive experimental evaluations.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 14:46:12 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 05:49:49 GMT"}, {"version": "v3", "created": "Fri, 19 May 2017 05:47:35 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Laclau", "Charlotte", ""], ["Redko", "Ievgen", ""], ["Matei", "Basarab", ""], ["Bennani", "Youn\u00e8s", ""], ["Brault", "Vincent", ""]]}, {"id": "1705.06211", "submitter": "Albert Berahas", "authors": "Albert S. Berahas, Raghu Bollapragada and Jorge Nocedal", "title": "An Investigation of Newton-Sketch and Subsampled Newton Methods", "comments": "36 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketching, a dimensionality reduction technique, has received much attention\nin the statistics community. In this paper, we study sketching in the context\nof Newton's method for solving finite-sum optimization problems in which the\nnumber of variables and data points are both large. We study two forms of\nsketching that perform dimensionality reduction in data space: Hessian\nsubsampling and randomized Hadamard transformations. Each has its own\nadvantages, and their relative tradeoffs have not been investigated in the\noptimization literature. Our study focuses on practical versions of the two\nmethods in which the resulting linear systems of equations are solved\napproximately, at every iteration, using an iterative solver. The advantages of\nusing the conjugate gradient method vs. a stochastic gradient iteration are\nrevealed through a set of numerical experiments, and a complexity analysis of\nthe Hessian subsampling method is presented.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 15:31:37 GMT"}, {"version": "v2", "created": "Sat, 20 May 2017 01:07:15 GMT"}, {"version": "v3", "created": "Tue, 24 Jul 2018 21:12:26 GMT"}, {"version": "v4", "created": "Thu, 30 May 2019 23:01:20 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Berahas", "Albert S.", ""], ["Bollapragada", "Raghu", ""], ["Nocedal", "Jorge", ""]]}, {"id": "1705.06262", "submitter": "Vincent Major", "authors": "Vincent Major, Alisa Surkis, and Yindalon Aphinyanaphongs", "title": "Utility of General and Specific Word Embeddings for Classifying\n  Translational Stages of Research", "comments": "10 pages. Accepted to AMIA 2018 Annual Symposium, San Francisco,\n  November 3-7, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional text classification models make a bag-of-words assumption\nreducing text into word occurrence counts per document. Recent algorithms such\nas word2vec are capable of learning semantic meaning and similarity between\nwords in an entirely unsupervised manner using a contextual window and doing so\nmuch faster than previous methods. Each word is projected into vector space\nsuch that similar meaning words such as \"strong\" and \"powerful\" are projected\ninto the same general Euclidean space. Open questions about these embeddings\ninclude their utility across classification tasks and the optimal properties\nand source of documents to construct broadly functional embeddings. In this\nwork, we demonstrate the usefulness of pre-trained embeddings for\nclassification in our task and demonstrate that custom word embeddings, built\nin the domain and for the tasks, can improve performance over word embeddings\nlearnt on more general data including news articles or Wikipedia.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 17:08:11 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 23:40:23 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Major", "Vincent", ""], ["Surkis", "Alisa", ""], ["Aphinyanaphongs", "Yindalon", ""]]}, {"id": "1705.06273", "submitter": "Franck Dernoncourt", "authors": "Ji Young Lee, Franck Dernoncourt, Peter Szolovits", "title": "Transfer Learning for Named-Entity Recognition with Neural Networks", "comments": "The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent approaches based on artificial neural networks (ANNs) have shown\npromising results for named-entity recognition (NER). In order to achieve high\nperformances, ANNs need to be trained on a large labeled dataset. However,\nlabels might be difficult to obtain for the dataset on which the user wants to\nperform NER: label scarcity is particularly pronounced for patient note\nde-identification, which is an instance of NER. In this work, we analyze to\nwhat extent transfer learning may address this issue. In particular, we\ndemonstrate that transferring an ANN model trained on a large labeled dataset\nto another dataset with a limited number of labels improves upon the\nstate-of-the-art results on two different datasets for patient note\nde-identification.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 17:45:15 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Lee", "Ji Young", ""], ["Dernoncourt", "Franck", ""], ["Szolovits", "Peter", ""]]}, {"id": "1705.06333", "submitter": "Aliasghar Mortazi", "authors": "Aliasghar Mortazi, Rashed Karim, Kawal Rhode, Jeremy Burt, Ulas Bagci", "title": "CardiacNET: Segmentation of Left Atrium and Proximal Pulmonary Veins\n  from MRI Using Multi-View CNN", "comments": "The paper is accepted by MICCAI 2017 for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anatomical and biophysical modeling of left atrium (LA) and proximal\npulmonary veins (PPVs) is important for clinical management of several cardiac\ndiseases. Magnetic resonance imaging (MRI) allows qualitative assessment of LA\nand PPVs through visualization. However, there is a strong need for an advanced\nimage segmentation method to be applied to cardiac MRI for quantitative\nanalysis of LA and PPVs. In this study, we address this unmet clinical need by\nexploring a new deep learning-based segmentation strategy for quantification of\nLA and PPVs with high accuracy and heightened efficiency. Our approach is based\non a multi-view convolutional neural network (CNN) with an adaptive fusion\nstrategy and a new loss function that allows fast and more accurate convergence\nof the backpropagation based optimization. After training our network from\nscratch by using more than 60K 2D MRI images (slices), we have evaluated our\nsegmentation strategy to the STACOM 2013 cardiac segmentation challenge\nbenchmark. Qualitative and quantitative evaluations, obtained from the\nsegmentation challenge, indicate that the proposed method achieved the\nstate-of-the-art sensitivity (90%), specificity (99%), precision (94%), and\nefficiency levels (10 seconds in GPU, and 7.5 minutes in CPU).\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 20:18:32 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 15:57:13 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Mortazi", "Aliasghar", ""], ["Karim", "Rashed", ""], ["Rhode", "Kawal", ""], ["Burt", "Jeremy", ""], ["Bagci", "Ulas", ""]]}, {"id": "1705.06364", "submitter": "Zhen Li", "authors": "Zhen Li, Jingtian Bai, Weilian Zhou", "title": "Learning Gaussian Graphical Models Using Discriminated Hub Graphical\n  Lasso", "comments": "13 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new method called Discriminated Hub Graphical Lasso (DHGL) based\non Hub Graphical Lasso (HGL) by providing prior information of hubs. We apply\nthis new method in two situations: with known hubs and without known hubs. Then\nwe compare DHGL with HGL using several measures of performance. When some hubs\nare known, we can always estimate the precision matrix better via DHGL than\nHGL. When no hubs are known, we use Graphical Lasso (GL) to provide information\nof hubs and find that the performance of DHGL will always be better than HGL if\ncorrect prior information is given and will seldom degenerate when the prior\ninformation is wrong.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 22:51:51 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Li", "Zhen", ""], ["Bai", "Jingtian", ""], ["Zhou", "Weilian", ""]]}, {"id": "1705.06371", "submitter": "Robert Durrant", "authors": "Xianghui Luo and Robert J. Durrant", "title": "Maximum Margin Principal Components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal Component Analysis (PCA) is a very successful dimensionality\nreduction technique, widely used in predictive modeling. A key factor in its\nwidespread use in this domain is the fact that the projection of a dataset onto\nits first $K$ principal components minimizes the sum of squared errors between\nthe original data and the projected data over all possible rank $K$\nprojections. Thus, PCA provides optimal low-rank representations of data for\nleast-squares linear regression under standard modeling assumptions. On the\nother hand, when the loss function for a prediction problem is not the\nleast-squares error, PCA is typically a heuristic choice of dimensionality\nreduction -- in particular for classification problems under the zero-one loss.\nIn this paper we target classification problems by proposing a straightforward\nalternative to PCA that aims to minimize the difference in margin distribution\nbetween the original and the projected data. Extensive experiments show that\nour simple approach typically outperforms PCA on any particular dataset, in\nterms of classification error, though this difference is not always\nstatistically significant, and despite being a filter method is frequently\ncompetitive with Partial Least Squares (PLS) and Lasso on a wide range of\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 23:45:11 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Luo", "Xianghui", ""], ["Durrant", "Robert J.", ""]]}, {"id": "1705.06391", "submitter": "Yangyang Xu", "authors": "Yangyang Xu", "title": "Asynchronous parallel primal-dual block coordinate update methods for\n  affinely constrained convex programs", "comments": null, "journal-ref": "Computational Optimization and Applications, 72(1), pp. 87-113,\n  2019", "doi": "10.1007/s10589-018-0037-8", "report-no": null, "categories": "math.OC cs.DC cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent several years have witnessed the surge of asynchronous (async-)\nparallel computing methods due to the extremely big data involved in many\nmodern applications and also the advancement of multi-core machines and\ncomputer clusters. In optimization, most works about async-parallel methods are\non unconstrained problems or those with block separable constraints.\n  In this paper, we propose an async-parallel method based on block coordinate\nupdate (BCU) for solving convex problems with nonseparable linear constraint.\nRunning on a single node, the method becomes a novel randomized primal-dual BCU\nwith adaptive stepsize for multi-block affinely constrained problems. For these\nproblems, Gauss-Seidel cyclic primal-dual BCU needs strong convexity to have\nconvergence. On the contrary, merely assuming convexity, we show that the\nobjective value sequence generated by the proposed algorithm converges in\nprobability to the optimal value and also the constraint residual to zero. In\naddition, we establish an ergodic $O(1/k)$ convergence result, where $k$ is the\nnumber of iterations. Numerical experiments are performed to demonstrate the\nefficiency of the proposed method and significantly better speed-up performance\nthan its sync-parallel counterpart.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 01:53:51 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 20:19:02 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Xu", "Yangyang", ""]]}, {"id": "1705.06400", "submitter": "Matthias Plappert", "authors": "Matthias Plappert, Christian Mandery, Tamim Asfour", "title": "Learning a bidirectional mapping between human whole-body motion and\n  natural language using deep recurrent neural networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.robot.2018.07.006", "report-no": null, "categories": "cs.LG cs.CL cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linking human whole-body motion and natural language is of great interest for\nthe generation of semantic representations of observed human behaviors as well\nas for the generation of robot behaviors based on natural language input. While\nthere has been a large body of research in this area, most approaches that\nexist today require a symbolic representation of motions (e.g. in the form of\nmotion primitives), which have to be defined a-priori or require complex\nsegmentation algorithms. In contrast, recent advances in the field of neural\nnetworks and especially deep learning have demonstrated that sub-symbolic\nrepresentations that can be learned end-to-end usually outperform more\ntraditional approaches, for applications such as machine translation. In this\npaper we propose a generative model that learns a bidirectional mapping between\nhuman whole-body motion and natural language using deep recurrent neural\nnetworks (RNNs) and sequence-to-sequence learning. Our approach does not\nrequire any segmentation or manual feature engineering and learns a distributed\nrepresentation, which is shared for all motions and descriptions. We evaluate\nour approach on 2,846 human whole-body motions and 6,187 natural language\ndescriptions thereof from the KIT Motion-Language Dataset. Our results clearly\ndemonstrate the effectiveness of the proposed model: We show that our model\ngenerates a wide variety of realistic motions only from descriptions thereof in\nform of a single sentence. Conversely, our model is also capable of generating\ncorrect and detailed natural language descriptions from human motions.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 02:50:40 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 10:07:57 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Plappert", "Matthias", ""], ["Mandery", "Christian", ""], ["Asfour", "Tamim", ""]]}, {"id": "1705.06408", "submitter": "Robert Durrant", "authors": "Nick Lim and Robert J. Durrant", "title": "Linear Dimensionality Reduction in Linear Time:\n  Johnson-Lindenstrauss-type Guarantees for Random Subspace", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of efficient randomized dimensionality reduction with\nnorm-preservation guarantees. Specifically we prove data-dependent\nJohnson-Lindenstrauss-type geometry preservation guarantees for Ho's random\nsubspace method: When data satisfy a mild regularity condition -- the extent of\nwhich can be estimated by sampling from the data -- then random subspace\napproximately preserves the Euclidean geometry of the data with high\nprobability. Our guarantees are of the same order as those for random\nprojection, namely the required dimension for projection is logarithmic in the\nnumber of data points, but have a larger constant term in the bound which\ndepends upon this regularity. A challenging situation is when the original data\nhave a sparse representation, since this implies a very large projection\ndimension is required: We show how this situation can be improved for sparse\nbinary data by applying an efficient `densifying' preprocessing, which neither\nchanges the Euclidean geometry of the data nor requires an explicit\nmatrix-matrix multiplication. We corroborate our theoretical findings with\nexperiments on both dense and sparse high-dimensional datasets from several\napplication domains.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 03:31:11 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Lim", "Nick", ""], ["Durrant", "Robert J.", ""]]}, {"id": "1705.06412", "submitter": "Gauri Jagatap", "authors": "Gauri Jagatap and Chinmay Hegde", "title": "Sample-Efficient Algorithms for Recovering Structured Signals from\n  Magnitude-Only Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recovering a signal $\\mathbf{x}^* \\in\n\\mathbf{R}^n$, from magnitude-only measurements $y_i =\n|\\left\\langle\\mathbf{a}_i,\\mathbf{x}^*\\right\\rangle|$ for $i=[m]$. Also called\nthe phase retrieval, this is a fundamental challenge in bio-,astronomical\nimaging and speech processing. The problem above is ill-posed; additional\nassumptions on the signal and/or the measurements are necessary. In this paper\nwe first study the case where the signal $\\mathbf{x}^*$ is $s$-sparse. We\ndevelop a novel algorithm that we call Compressive Phase Retrieval with\nAlternating Minimization, or CoPRAM. Our algorithm is simple; it combines the\nclassical alternating minimization approach for phase retrieval with the CoSaMP\nalgorithm for sparse recovery. Despite its simplicity, we prove that CoPRAM\nachieves a sample complexity of $O(s^2\\log n)$ with Gaussian measurements\n$\\mathbf{a}_i$, matching the best known existing results; moreover, it\ndemonstrates linear convergence in theory and practice. Additionally, it\nrequires no extra tuning parameters other than signal sparsity $s$ and is\nrobust to noise. When the sorted coefficients of the sparse signal exhibit a\npower law decay, we show that CoPRAM achieves a sample complexity of $O(s\\log\nn)$, which is close to the information-theoretic limit. We also consider the\ncase where the signal $\\mathbf{x}^*$ arises from structured sparsity models. We\nspecifically examine the case of block-sparse signals with uniform block size\nof $b$ and block sparsity $k=s/b$. For this problem, we design a recovery\nalgorithm Block CoPRAM that further reduces the sample complexity to $O(ks\\log\nn)$. For sufficiently large block lengths of $b=\\Theta(s)$, this bound equates\nto $O(s\\log n)$. To our knowledge, this constitutes the first end-to-end\nalgorithm for phase retrieval where the Gaussian sample complexity has a\nsub-quadratic dependence on the signal sparsity level.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 04:26:01 GMT"}, {"version": "v2", "created": "Sun, 26 Nov 2017 21:14:14 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Jagatap", "Gauri", ""], ["Hegde", "Chinmay", ""]]}, {"id": "1705.06452", "submitter": "Jernej Kos", "authors": "Jernej Kos, Dawn Song", "title": "Delving into adversarial attacks on deep policies", "comments": "ICLR 2017 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples have been shown to exist for a variety of deep learning\narchitectures. Deep reinforcement learning has shown promising results on\ntraining agent policies directly on raw inputs such as image pixels. In this\npaper we present a novel study into adversarial attacks on deep reinforcement\nlearning polices. We compare the effectiveness of the attacks using adversarial\nexamples vs. random noise. We present a novel method for reducing the number of\ntimes adversarial examples need to be injected for a successful attack, based\non the value function. We further explore how re-training on random noise and\nFGSM perturbations affects the resilience against adversarial examples.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 08:01:53 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Kos", "Jernej", ""], ["Song", "Dawn", ""]]}, {"id": "1705.06499", "submitter": "Lei Yang", "authors": "Lei Yang, Ting Kei Pong, Xiaojun Chen", "title": "A Non-monotone Alternating Updating Method for A Class of Matrix\n  Factorization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a general matrix factorization model which covers a\nlarge class of existing models with many applications in areas such as machine\nlearning and imaging sciences. To solve this possibly nonconvex, nonsmooth and\nnon-Lipschitz problem, we develop a non-monotone alternating updating method\nbased on a potential function. Our method essentially updates two blocks of\nvariables in turn by inexactly minimizing this potential function, and updates\nanother auxiliary block of variables using an explicit formula. The special\nstructure of our potential function allows us to take advantage of efficient\ncomputational strategies for non-negative matrix factorization to perform the\nalternating minimization over the two blocks of variables. A suitable line\nsearch criterion is also incorporated to improve the numerical performance.\nUnder some mild conditions, we show that the line search criterion is well\ndefined, and establish that the sequence generated is bounded and any cluster\npoint of the sequence is a stationary point. Finally, we conduct some numerical\nexperiments using real datasets to compare our method with some existing\nefficient methods for non-negative matrix factorization and matrix completion.\nThe numerical results show that our method can outperform these methods for\nthese specific applications.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 09:55:17 GMT"}, {"version": "v2", "created": "Tue, 15 May 2018 02:32:51 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Yang", "Lei", ""], ["Pong", "Ting Kei", ""], ["Chen", "Xiaojun", ""]]}, {"id": "1705.06566", "submitter": "Nikolay Jetchev", "authors": "Urs Bergmann, Nikolay Jetchev, Roland Vollgraf", "title": "Learning Texture Manifolds with the Periodic Spatial GAN", "comments": "Proceedings of the 34th International Conference on Machine Learning,\n  Sydney, Australia, 2017. JMLR: W&CP. Copyright 2017 by the author(s)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel approach to texture synthesis based on\ngenerative adversarial networks (GAN) (Goodfellow et al., 2014). We extend the\nstructure of the input noise distribution by constructing tensors with\ndifferent types of dimensions. We call this technique Periodic Spatial GAN\n(PSGAN). The PSGAN has several novel abilities which surpass the current state\nof the art in texture synthesis. First, we can learn multiple textures from\ndatasets of one or more complex large images. Second, we show that the image\ngeneration with PSGANs has properties of a texture manifold: we can smoothly\ninterpolate between samples in the structured noise space and generate novel\nsamples, which lie perceptually between the textures of the original dataset.\nIn addition, we can also accurately learn periodical textures. We make multiple\nexperiments which show that PSGANs can flexibly handle diverse texture and\nimage data sources. Our method is highly scalable and it can generate output\nimages of arbitrary large size.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 13:09:45 GMT"}, {"version": "v2", "created": "Fri, 8 Sep 2017 21:26:03 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Bergmann", "Urs", ""], ["Jetchev", "Nikolay", ""], ["Vollgraf", "Roland", ""]]}, {"id": "1705.06753", "submitter": "Konstantin Bauman", "authors": "Evgeny Bauman, Konstantin Bauman", "title": "Discovering the Graph Structure in the Clustering Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a standard cluster analysis, such as k-means, in addition to clusters\nlocations and distances between them, it's important to know if they are\nconnected or well separated from each other. The main focus of this paper is\ndiscovering the relations between the resulting clusters. We propose a new\nmethod which is based on pairwise overlapping k-means clustering, that in\naddition to means of clusters provides the graph structure of their relations.\nThe proposed method has a set of parameters that can be tuned in order to\ncontrol the sensitivity of the model and the desired relative size of the\npairwise overlapping interval between means of two adjacent clusters, i.e.,\nlevel of overlapping. We present the exact formula for calculating that\nparameter. The empirical study presented in the paper demonstrates that our\napproach works well not only on toy data but also compliments standard\nclustering results with a reasonable graph structure on real datasets, such as\nfinancial indices and restaurants.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 18:01:50 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Bauman", "Evgeny", ""], ["Bauman", "Konstantin", ""]]}, {"id": "1705.06772", "submitter": "Yun-Jhong Wu", "authors": "Yun-Jhong Wu, Elizaveta Levina, Ji Zhu", "title": "Generalized linear models with low rank effects for network data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks are a useful representation for data on connections between units of\ninterests, but the observed connections are often noisy and/or include missing\nvalues. One common approach to network analysis is to treat the network as a\nrealization from a random graph model, and estimate the underlying edge\nprobability matrix, which is sometimes referred to as network denoising. Here\nwe propose a generalized linear model with low rank effects to model network\nedges. This model can be applied to various types of networks, including\ndirected and undirected, binary and weighted, and it can naturally utilize\nadditional information such as node and/or edge covariates. We develop an\nefficient projected gradient ascent algorithm to fit the model, establish\nasymptotic consistency, and demonstrate empirical performance of the method on\nboth simulated and real networks.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 19:15:41 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Wu", "Yun-Jhong", ""], ["Levina", "Elizaveta", ""], ["Zhu", "Ji", ""]]}, {"id": "1705.06808", "submitter": "Kinjal Basu", "authors": "Kinjal Basu and Souvik Ghosh", "title": "Adaptive Rate of Convergence of Thompson Sampling for Gaussian Process\n  Optimization", "comments": "25 Pages, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of global optimization of a function over a\ncontinuous domain. In our setup, we can evaluate the function sequentially at\npoints of our choice and the evaluations are noisy. We frame it as a\ncontinuum-armed bandit problem with a Gaussian Process prior on the function.\nIn this regime, most algorithms have been developed to minimize some form of\nregret. In this paper, we study the convergence of the sequential point $x^t$\nto the global optimizer $x^*$ for the Thompson Sampling approach. Under some\nassumptions and regularity conditions, we prove concentration bounds for $x^t$\nwhere the probability that $x^t$ is bounded away from $x^*$ decays\nexponentially fast in $t$. Moreover, the result allows us to derive adaptive\nconvergence rates depending on the function structure.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 21:36:40 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 00:54:57 GMT"}, {"version": "v3", "created": "Mon, 12 Feb 2018 17:51:02 GMT"}, {"version": "v4", "created": "Sat, 18 Jul 2020 19:20:08 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Basu", "Kinjal", ""], ["Ghosh", "Souvik", ""]]}, {"id": "1705.06820", "submitter": "Hongyang Gao", "authors": "Hongyang Gao and Hao Yuan and Zhengyang Wang and Shuiwang Ji", "title": "Pixel Deconvolutional Networks", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deconvolutional layers have been widely used in a variety of deep models for\nup-sampling, including encoder-decoder networks for semantic segmentation and\ndeep generative models for unsupervised learning. One of the key limitations of\ndeconvolutional operations is that they result in the so-called checkerboard\nproblem. This is caused by the fact that no direct relationship exists among\nadjacent pixels on the output feature map. To address this problem, we propose\nthe pixel deconvolutional layer (PixelDCL) to establish direct relationships\namong adjacent pixels on the up-sampled feature map. Our method is based on a\nfresh interpretation of the regular deconvolution operation. The resulting\nPixelDCL can be used to replace any deconvolutional layer in a plug-and-play\nmanner without compromising the fully trainable capabilities of original\nmodels. The proposed PixelDCL may result in slight decrease in efficiency, but\nthis can be overcome by an implementation trick. Experimental results on\nsemantic segmentation demonstrate that PixelDCL can consider spatial features\nsuch as edges and shapes and yields more accurate segmentation outputs than\ndeconvolutional layers. When used in image generation tasks, our PixelDCL can\nlargely overcome the checkerboard problem suffered by regular deconvolution\noperations.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 22:31:26 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 17:17:07 GMT"}, {"version": "v3", "created": "Fri, 24 Nov 2017 18:17:31 GMT"}, {"version": "v4", "created": "Mon, 27 Nov 2017 02:53:39 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Gao", "Hongyang", ""], ["Yuan", "Hao", ""], ["Wang", "Zhengyang", ""], ["Ji", "Shuiwang", ""]]}, {"id": "1705.06821", "submitter": "Zhengyang Wang", "authors": "Zhengyang Wang, Hao Yuan, Shuiwang Ji", "title": "Spatial Variational Auto-Encoding via Matrix-Variate Normal\n  Distributions", "comments": "Accepted by SDM2019. Code is publicly available at\n  https://github.com/divelab/svae", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key idea of variational auto-encoders (VAEs) resembles that of\ntraditional auto-encoder models in which spatial information is supposed to be\nexplicitly encoded in the latent space. However, the latent variables in VAEs\nare vectors, which can be interpreted as multiple feature maps of size 1x1.\nSuch representations can only convey spatial information implicitly when\ncoupled with powerful decoders. In this work, we propose spatial VAEs that use\nfeature maps of larger size as latent variables to explicitly capture spatial\ninformation. This is achieved by allowing the latent variables to be sampled\nfrom matrix-variate normal (MVN) distributions whose parameters are computed\nfrom the encoder network. To increase dependencies among locations on latent\nfeature maps and reduce the number of parameters, we further propose spatial\nVAEs via low-rank MVN distributions. Experimental results show that the\nproposed spatial VAEs outperform original VAEs in capturing rich structural and\nspatial information.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 22:32:57 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2019 17:48:22 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Wang", "Zhengyang", ""], ["Yuan", "Hao", ""], ["Ji", "Shuiwang", ""]]}, {"id": "1705.06824", "submitter": "Zhengyang Wang", "authors": "Zhengyang Wang, Shuiwang Ji", "title": "Learning Convolutional Text Representations for Visual Question\n  Answering", "comments": "Conference paper at SDM 2018. https://github.com/divelab/svae", "journal-ref": "In proceedings of the 2018 SIAM International Conference on Data\n  Mining (pp. 594-602). 2018", "doi": "10.1137/1.9781611975321.67", "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering is a recently proposed artificial intelligence task\nthat requires a deep understanding of both images and texts. In deep learning,\nimages are typically modeled through convolutional neural networks, and texts\nare typically modeled through recurrent neural networks. While the requirement\nfor modeling images is similar to traditional computer vision tasks, such as\nobject recognition and image classification, visual question answering raises a\ndifferent need for textual representation as compared to other natural language\nprocessing tasks. In this work, we perform a detailed analysis on natural\nlanguage questions in visual question answering. Based on the analysis, we\npropose to rely on convolutional neural networks for learning textual\nrepresentations. By exploring the various properties of convolutional neural\nnetworks specialized for text data, such as width and depth, we present our\n\"CNN Inception + Gate\" model. We show that our model improves question\nrepresentations and thus the overall accuracy of visual question answering\nmodels. We also show that the text representation requirement in visual\nquestion answering is more complicated and comprehensive than that in\nconventional natural language processing tasks, making it a better task to\nevaluate textual representation methods. Shallow models like fastText, which\ncan obtain comparable results with deep learning models in tasks like text\nclassification, are not suitable in visual question answering.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 22:51:44 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 17:38:50 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Wang", "Zhengyang", ""], ["Ji", "Shuiwang", ""]]}, {"id": "1705.06884", "submitter": "Renbo Zhao", "authors": "Renbo Zhao, William B. Haskell, Jiashi Feng", "title": "A Unified Framework for Stochastic Matrix Factorization via Variance\n  Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a unified framework to speed up the existing stochastic matrix\nfactorization (SMF) algorithms via variance reduction. Our framework is general\nand it subsumes several well-known SMF formulations in the literature. We\nperform a non-asymptotic convergence analysis of our framework and derive\ncomputational and sample complexities for our algorithm to converge to an\n$\\epsilon$-stationary point in expectation. In addition, extensive experiments\nfor a wide class of SMF formulations demonstrate that our framework\nconsistently yields faster convergence and a more accurate output dictionary\nvis-\\`a-vis state-of-the-art frameworks.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 08:05:10 GMT"}, {"version": "v2", "created": "Mon, 22 May 2017 02:35:21 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Zhao", "Renbo", ""], ["Haskell", "William B.", ""], ["Feng", "Jiashi", ""]]}, {"id": "1705.06894", "submitter": "Mingda Qiao", "authors": "Haotian Jiang, Jian Li, Mingda Qiao", "title": "Practical Algorithms for Best-K Identification in Multi-Armed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Best-$K$ identification problem (Best-$K$-Arm), we are given $N$\nstochastic bandit arms with unknown reward distributions. Our goal is to\nidentify the $K$ arms with the largest means with high confidence, by drawing\nsamples from the arms adaptively. This problem is motivated by various\npractical applications and has attracted considerable attention in the past\ndecade. In this paper, we propose new practical algorithms for the Best-$K$-Arm\nproblem, which have nearly optimal sample complexity bounds (matching the lower\nbound up to logarithmic factors) and outperform the state-of-the-art algorithms\nfor the Best-$K$-Arm problem (even for $K=1$) in practice.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 08:49:29 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Jiang", "Haotian", ""], ["Li", "Jian", ""], ["Qiao", "Mingda", ""]]}, {"id": "1705.06899", "submitter": "Zhongmin Luo", "authors": "Raymond Brummelhuis and Zhongmin Luo", "title": "CDS Rate Construction Methods by Machine Learning Techniques", "comments": "51 pages; 21 Figures; 15 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG q-fin.RM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regulators require financial institutions to estimate counterparty default\nrisks from liquid CDS quotes for the valuation and risk management of OTC\nderivatives. However, the vast majority of counterparties do not have liquid\nCDS quotes and need proxy CDS rates. Existing methods cannot account for\ncounterparty-specific default risks; we propose to construct proxy CDS rates by\nassociating to illiquid counterparty liquid CDS Proxy based on Machine Learning\nTechniques. After testing 156 classifiers from 8 most popular classifier\nfamilies, we found that some classifiers achieve highly satisfactory accuracy\nrates. Furthermore, we have rank-ordered the performances and investigated\nperformance variations amongst and within the 8 classifier families. This paper\nis, to the best of our knowledge, the first systematic study of CDS Proxy\nconstruction by Machine Learning techniques, and the first systematic\nclassifier comparison study based entirely on financial market data. Its\nfindings both confirm and contrast existing classifier performance literature.\nGiven the typically highly correlated nature of financial data, we investigated\nthe impact of correlation on classifier performance. The techniques used in\nthis paper should be of interest for financial institutions seeking a CDS Proxy\nmethod, and can serve for proxy construction for other financial variables.\nSome directions for future research are indicated.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 09:20:30 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Brummelhuis", "Raymond", ""], ["Luo", "Zhongmin", ""]]}, {"id": "1705.06911", "submitter": "Taikai Takeda", "authors": "Taikai Takeda, Michiaki Hamada", "title": "Beyond similarity assessment: Selecting the optimal model for sequence\n  alignment via the Factorized Asymptotic Bayesian algorithm", "comments": "This article has been accepted for publication in Bioinformatics\n  Published by Oxford University Press", "journal-ref": "Bioinformatics, 2017, btx643", "doi": "10.1093/bioinformatics/btx643", "report-no": null, "categories": "q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pair Hidden Markov Models (PHMMs) are probabilistic models used for pairwise\nsequence alignment, a quintessential problem in bioinformatics. PHMMs include\nthree types of hidden states: match, insertion and deletion. Most previous\nstudies have used one or two hidden states for each PHMM state type. However,\nfew studies have examined the number of states suitable for representing\nsequence data or improving alignment accuracy.We developed a novel method to\nselect superior models (including the number of hidden states) for PHMM. Our\nmethod selects models with the highest posterior probability using Factorized\nInformation Criteria (FIC), which is widely utilised in model selection for\nprobabilistic models with hidden variables. Our simulations indicated this\nmethod has excellent model selection capabilities with slightly improved\nalignment accuracy. We applied our method to DNA datasets from 5 and 28\nspecies, ultimately selecting more complex models than those used in previous\nstudies.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 09:49:59 GMT"}, {"version": "v2", "created": "Sun, 15 Oct 2017 06:52:17 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Takeda", "Taikai", ""], ["Hamada", "Michiaki", ""]]}, {"id": "1705.07006", "submitter": "Hongyi Ding", "authors": "Hongyi Ding, Mohammad Emtiyaz Khan, Issei Sato, Masashi Sugiyama", "title": "Bayesian Nonparametric Poisson-Process Allocation for Time-Sequence\n  Modeling", "comments": "Revise the writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing the underlying structure of multiple time-sequences provides\ninsights into the understanding of social networks and human activities. In\nthis work, we present the \\emph{Bayesian nonparametric Poisson process\nallocation} (BaNPPA), a latent-function model for time-sequences, which\nautomatically infers the number of latent functions. We model the intensity of\neach sequence as an infinite mixture of latent functions, each of which is\nobtained using a function drawn from a Gaussian process. We show that a\ntechnical challenge for the inference of such mixture models is the\nunidentifiability of the weights of the latent functions. We propose to cope\nwith the issue by regulating the volume of each latent function within a\nvariational inference algorithm. Our algorithm is computationally efficient and\nscales well to large data sets. We demonstrate the usefulness of our proposed\nmodel through experiments on both synthetic and real-world data sets.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 14:15:13 GMT"}, {"version": "v2", "created": "Tue, 6 Jun 2017 09:00:06 GMT"}, {"version": "v3", "created": "Mon, 31 Jul 2017 04:12:34 GMT"}, {"version": "v4", "created": "Wed, 18 Oct 2017 06:43:04 GMT"}, {"version": "v5", "created": "Tue, 3 Apr 2018 09:05:40 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Ding", "Hongyi", ""], ["Khan", "Mohammad Emtiyaz", ""], ["Sato", "Issei", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1705.07019", "submitter": "Dave Zachariah", "authors": "Dave Zachariah and Petre Stoica", "title": "Model-Robust Counterfactual Prediction Method", "comments": null, "journal-ref": "ICML Workshop, ML for Causal Inference, Counterfactual Prediction,\n  and Autonomous Action, 2018", "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel method for counterfactual analysis based on observational\ndata using prediction intervals for units under different exposures. Unlike\nmethods that target heterogeneous or conditional average treatment effects of\nan exposure, the proposed approach aims to take into account the irreducible\ndispersions of counterfactual outcomes so as to quantify the relative impact of\ndifferent exposures. The prediction intervals are constructed in a\ndistribution-free and model-robust manner based on the conformal prediction\napproach. The computational obstacles to this approach are circumvented by\nleveraging properties of a tuning-free method that learns sparse additive\npredictor models for counterfactual outcomes. The method is illustrated using\nboth real and synthetic data.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 14:29:13 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 13:22:57 GMT"}, {"version": "v3", "created": "Wed, 20 Dec 2017 13:10:43 GMT"}, {"version": "v4", "created": "Wed, 7 Feb 2018 10:08:40 GMT"}, {"version": "v5", "created": "Mon, 28 May 2018 19:48:06 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Zachariah", "Dave", ""], ["Stoica", "Petre", ""]]}, {"id": "1705.07025", "submitter": "David Kale", "authors": "Sebastien Dubois, Nathanael Romano, David C. Kale, Nigam Shah, and\n  Kenneth Jung", "title": "Effective Representations of Clinical Notes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical notes are a rich source of information about patient state. However,\nusing them to predict clinical events with machine learning models is\nchallenging. They are very high dimensional, sparse and have complex structure.\nFurthermore, training data is often scarce because it is expensive to obtain\nreliable labels for many clinical events. These difficulties have traditionally\nbeen addressed by manual feature engineering encoding task specific domain\nknowledge. We explored the use of neural networks and transfer learning to\nlearn representations of clinical notes that are useful for predicting future\nclinical events of interest, such as all causes mortality, inpatient\nadmissions, and emergency room visits. Our data comprised 2.7 million notes and\n115 thousand patients at Stanford Hospital. We used the learned\nrepresentations, along with commonly used bag of words and topic model\nrepresentations, as features for predictive models of clinical events. We\nevaluated the effectiveness of these representations with respect to the\nperformance of the models trained on small datasets. Models using the neural\nnetwork derived representations performed significantly better than models\nusing the baseline representations with small ($N < 1000$) training datasets.\nThe learned representations offer significant performance gains over commonly\nused baseline representations for a range of predictive modeling tasks and\ncohort sizes, offering an effective alternative to task specific feature\nengineering when plentiful labeled training data is not available.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 14:42:48 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 03:31:52 GMT"}, {"version": "v3", "created": "Thu, 16 Aug 2018 22:56:41 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Dubois", "Sebastien", ""], ["Romano", "Nathanael", ""], ["Kale", "David C.", ""], ["Shah", "Nigam", ""], ["Jung", "Kenneth", ""]]}, {"id": "1705.07038", "submitter": "Pan Zhou", "authors": "Pan Zhou, Jiashi Feng", "title": "The Landscape of Deep Learning Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the landscape of empirical risk of deep neural networks by\ntheoretically analyzing its convergence behavior to the population risk as well\nas its stationary points and properties. For an $l$-layer linear neural\nnetwork, we prove its empirical risk uniformly converges to its population risk\nat the rate of $\\mathcal{O}(r^{2l}\\sqrt{d\\log(l)}/\\sqrt{n})$ with training\nsample size of $n$, the total weight dimension of $d$ and the magnitude bound\n$r$ of weight of each layer. We then derive the stability and generalization\nbounds for the empirical risk based on this result. Besides, we establish the\nuniform convergence of gradient of the empirical risk to its population\ncounterpart. We prove the one-to-one correspondence of the non-degenerate\nstationary points between the empirical and population risks with convergence\nguarantees, which describes the landscape of deep neural networks. In addition,\nwe analyze these properties for deep nonlinear neural networks with sigmoid\nactivation functions. We prove similar results for convergence behavior of\ntheir empirical risks as well as the gradients and analyze properties of their\nnon-degenerate stationary points.\n  To our best knowledge, this work is the first one theoretically\ncharacterizing landscapes of deep learning algorithms. Besides, our results\nprovide the sample complexity of training a good deep neural network. We also\nprovide theoretical understanding on how the neural network depth $l$, the\nlayer width, the network size $d$ and parameter magnitude determine the neural\nnetwork landscapes.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 15:07:07 GMT"}, {"version": "v2", "created": "Sat, 5 Aug 2017 12:30:25 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Zhou", "Pan", ""], ["Feng", "Jiashi", ""]]}, {"id": "1705.07048", "submitter": "Daniel Hsu", "authors": "Daniel Hsu, Kevin Shi, Xiaorui Sun", "title": "Linear regression without correspondence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article considers algorithmic and statistical aspects of linear\nregression when the correspondence between the covariates and the responses is\nunknown. First, a fully polynomial-time approximation scheme is given for the\nnatural least squares optimization problem in any constant dimension. Next, in\nan average-case and noise-free setting where the responses exactly correspond\nto a linear function of i.i.d. draws from a standard multivariate normal\ndistribution, an efficient algorithm based on lattice basis reduction is shown\nto exactly recover the unknown linear function in arbitrary dimension. Finally,\nlower bounds on the signal-to-noise ratio are established for approximate\nrecovery of the unknown linear function by any estimator.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 15:22:38 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 23:16:39 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Hsu", "Daniel", ""], ["Shi", "Kevin", ""], ["Sun", "Xiaorui", ""]]}, {"id": "1705.07057", "submitter": "George Papamakarios", "authors": "George Papamakarios, Theo Pavlakou, Iain Murray", "title": "Masked Autoregressive Flow for Density Estimation", "comments": "section 4.3 is corrected since the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoregressive models are among the best performing neural density\nestimators. We describe an approach for increasing the flexibility of an\nautoregressive model, based on modelling the random numbers that the model uses\ninternally when generating data. By constructing a stack of autoregressive\nmodels, each modelling the random numbers of the next model in the stack, we\nobtain a type of normalizing flow suitable for density estimation, which we\ncall Masked Autoregressive Flow. This type of flow is closely related to\nInverse Autoregressive Flow and is a generalization of Real NVP. Masked\nAutoregressive Flow achieves state-of-the-art performance in a range of\ngeneral-purpose density estimation tasks.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 15:42:54 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 19:51:14 GMT"}, {"version": "v3", "created": "Thu, 11 Jan 2018 15:27:21 GMT"}, {"version": "v4", "created": "Thu, 14 Jun 2018 10:28:12 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Papamakarios", "George", ""], ["Pavlakou", "Theo", ""], ["Murray", "Iain", ""]]}, {"id": "1705.07070", "submitter": "Mehmet Donmez", "authors": "Mehmet A. Donmez and Maxim Raginsky and Andrew C. Singer", "title": "EE-Grad: Exploration and Exploitation for Cost-Efficient Mini-Batch SGD", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generic framework for trading off fidelity and cost in computing\nstochastic gradients when the costs of acquiring stochastic gradients of\ndifferent quality are not known a priori. We consider a mini-batch oracle that\ndistributes a limited query budget over a number of stochastic gradients and\naggregates them to estimate the true gradient. Since the optimal mini-batch\nsize depends on the unknown cost-fidelity function, we propose an algorithm,\n{\\it EE-Grad}, that sequentially explores the performance of mini-batch oracles\nand exploits the accumulated knowledge to estimate the one achieving the best\nperformance in terms of cost-efficiency. We provide performance guarantees for\nEE-Grad with respect to the optimal mini-batch oracle, and illustrate these\nresults in the case of strongly convex objectives. We also provide a simple\nnumerical example that corroborates our theoretical findings.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 16:16:42 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Donmez", "Mehmet A.", ""], ["Raginsky", "Maxim", ""], ["Singer", "Andrew C.", ""]]}, {"id": "1705.07079", "submitter": "Stefan Bauer", "authors": "Nico S. Gorbach and Stefan Bauer and Joachim M. Buhmann", "title": "Scalable Variational Inference for Dynamical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient matching is a promising tool for learning parameters and state\ndynamics of ordinary differential equations. It is a grid free inference\napproach, which, for fully observable systems is at times competitive with\nnumerical integration. However, for many real-world applications, only sparse\nobservations are available or even unobserved variables are included in the\nmodel description. In these cases most gradient matching methods are difficult\nto apply or simply do not provide satisfactory results. That is why, despite\nthe high computational cost, numerical integration is still the gold standard\nin many applications. Using an existing gradient matching approach, we propose\na scalable variational inference framework which can infer states and\nparameters simultaneously, offers computational speedups, improved accuracy and\nworks well even under model misspecifications in a partially observable system.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 16:29:00 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 13:54:49 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Gorbach", "Nico S.", ""], ["Bauer", "Stefan", ""], ["Buhmann", "Joachim M.", ""]]}, {"id": "1705.07086", "submitter": "Emmanouil Antonios Platanios", "authors": "Emmanouil A. Platanios, Hoifung Poon, Tom M. Mitchell, Eric Horvitz", "title": "Estimating Accuracy from Unlabeled Data: A Probabilistic Logic Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient method to estimate the accuracy of classifiers using\nonly unlabeled data. We consider a setting with multiple classification\nproblems where the target classes may be tied together through logical\nconstraints. For example, a set of classes may be mutually exclusive, meaning\nthat a data instance can belong to at most one of them. The proposed method is\nbased on the intuition that: (i) when classifiers agree, they are more likely\nto be correct, and (ii) when the classifiers make a prediction that violates\nthe constraints, at least one classifier must be making an error. Experiments\non four real-world data sets produce accuracy estimates within a few percent of\nthe true accuracy, using solely unlabeled data. Our models also outperform\nexisting state-of-the-art solutions in both estimating accuracies, and\ncombining multiple classifier outputs. The results emphasize the utility of\nlogical constraints in estimating accuracy, thus validating our intuition.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 16:52:52 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Platanios", "Emmanouil A.", ""], ["Poon", "Hoifung", ""], ["Mitchell", "Tom M.", ""], ["Horvitz", "Eric", ""]]}, {"id": "1705.07104", "submitter": "Pablo A. Alvarado", "authors": "Pablo A. Alvarado, Dan Stowell", "title": "Efficient Learning of Harmonic Priors for Pitch Detection in Polyphonic\n  Music", "comments": "Updated version with appendix section about derivation of amplitude\n  modulated GP", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic music transcription (AMT) aims to infer a latent symbolic\nrepresentation of a piece of music (piano-roll), given a corresponding observed\naudio recording. Transcribing polyphonic music (when multiple notes are played\nsimultaneously) is a challenging problem, due to highly structured overlapping\nbetween harmonics. We study whether the introduction of physically inspired\nGaussian process (GP) priors into audio content analysis models improves the\nextraction of patterns required for AMT. Audio signals are described as a\nlinear combination of sources. Each source is decomposed into the product of an\namplitude-envelope, and a quasi-periodic component process. We introduce the\nMat\\'ern spectral mixture (MSM) kernel for describing frequency content of\nsingles notes. We consider two different regression approaches. In the sigmoid\nmodel every pitch-activation is independently non-linear transformed. In the\nsoftmax model several activation GPs are jointly non-linearly transformed. This\nintroduce cross-correlation between activations. We use variational Bayes for\napproximate inference. We empirically evaluate how these models work in\npractice transcribing polyphonic music. We demonstrate that rather than\nencourage dependency between activations, what is relevant for improving pitch\ndetection is to learnt priors that fit the frequency content of the sound\nevents to detect.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 17:33:12 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 16:58:37 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Alvarado", "Pablo A.", ""], ["Stowell", "Dan", ""]]}, {"id": "1705.07107", "submitter": "Yingzhen Li", "authors": "Yingzhen Li, Richard E. Turner", "title": "Gradient Estimators for Implicit Models", "comments": "v5 fixed a typo in Figure 3 of v4 (the version at ICLR 2018 main\n  conference)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit models, which allow for the generation of samples but not for\npoint-wise evaluation of probabilities, are omnipresent in real-world problems\ntackled by machine learning and a hot topic of current research. Some examples\ninclude data simulators that are widely used in engineering and scientific\nresearch, generative adversarial networks (GANs) for image synthesis, and\nhot-off-the-press approximate inference techniques relying on implicit\ndistributions. The majority of existing approaches to learning implicit models\nrely on approximating the intractable distribution or optimisation objective\nfor gradient-based optimisation, which is liable to produce inaccurate updates\nand thus poor models. This paper alleviates the need for such approximations by\nproposing the Stein gradient estimator, which directly estimates the score\nfunction of the implicitly defined distribution. The efficacy of the proposed\nestimator is empirically demonstrated by examples that include meta-learning\nfor approximate inference, and entropy regularised GANs that provide improved\nsample diversity.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 17:35:04 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 09:02:34 GMT"}, {"version": "v3", "created": "Mon, 19 Feb 2018 15:19:59 GMT"}, {"version": "v4", "created": "Fri, 23 Feb 2018 16:45:57 GMT"}, {"version": "v5", "created": "Thu, 26 Apr 2018 08:43:09 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Li", "Yingzhen", ""], ["Turner", "Richard E.", ""]]}, {"id": "1705.07109", "submitter": "Ya\\u{g}mur G\\\"u\\c{c}l\\\"ut\\\"urk", "authors": "Ya\\u{g}mur G\\\"u\\c{c}l\\\"ut\\\"urk, Umut G\\\"u\\c{c}l\\\"u, Katja Seeliger,\n  Sander Bosch, Rob van Lier, Marcel van Gerven", "title": "Deep adversarial neural decoding", "comments": "Added appendix and updated figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here, we present a novel approach to solve the problem of reconstructing\nperceived stimuli from brain responses by combining probabilistic inference\nwith deep learning. Our approach first inverts the linear transformation from\nlatent features to brain responses with maximum a posteriori estimation and\nthen inverts the nonlinear transformation from perceived stimuli to latent\nfeatures with adversarial training of convolutional neural networks. We test\nour approach with a functional magnetic resonance imaging experiment and show\nthat it can generate state-of-the-art reconstructions of perceived faces from\nbrain activations.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 17:43:01 GMT"}, {"version": "v2", "created": "Thu, 1 Jun 2017 13:15:25 GMT"}, {"version": "v3", "created": "Thu, 15 Jun 2017 16:56:34 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["G\u00fc\u00e7l\u00fct\u00fcrk", "Ya\u011fmur", ""], ["G\u00fc\u00e7l\u00fc", "Umut", ""], ["Seeliger", "Katja", ""], ["Bosch", "Sander", ""], ["van Lier", "Rob", ""], ["van Gerven", "Marcel", ""]]}, {"id": "1705.07111", "submitter": "Luca Ambrogioni", "authors": "Luca Ambrogioni, Umut G\\\"u\\c{c}l\\\"u, Marcel A. J. van Gerven and Eric\n  Maris", "title": "The Kernel Mixture Network: A Nonparametric Method for Conditional\n  Density Estimation of Continuous Random Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the kernel mixture network, a new method for\nnonparametric estimation of conditional probability densities using neural\nnetworks. We model arbitrarily complex conditional densities as linear\ncombinations of a family of kernel functions centered at a subset of training\npoints. The weights are determined by the outer layer of a deep neural network,\ntrained by minimizing the negative log likelihood. This generalizes the popular\nquantized softmax approach, which can be seen as a kernel mixture network with\nsquare and non-overlapping kernels. We test the performance of our method on\ntwo important applications, namely Bayesian filtering and generative modeling.\nIn the Bayesian filtering example, we show that the method can be used to\nfilter complex nonlinear and non-Gaussian signals defined on manifolds. The\nresulting kernel mixture network filter outperforms both the quantized softmax\nfilter and the extended Kalman filter in terms of model likelihood. Finally,\nour experiments on generative models show that, given the same architecture,\nthe kernel mixture network leads to higher test set likelihood, less\noverfitting and more diversified and realistic generated samples than the\nquantized softmax approach.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 17:45:19 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Ambrogioni", "Luca", ""], ["G\u00fc\u00e7l\u00fc", "Umut", ""], ["van Gerven", "Marcel A. J.", ""], ["Maris", "Eric", ""]]}, {"id": "1705.07120", "submitter": "Jakub Tomczak Ph.D.", "authors": "Jakub M. Tomczak and Max Welling", "title": "VAE with a VampPrior", "comments": "16 pages, final version, AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many different methods to train deep generative models have been introduced\nin the past. In this paper, we propose to extend the variational auto-encoder\n(VAE) framework with a new type of prior which we call \"Variational Mixture of\nPosteriors\" prior, or VampPrior for short. The VampPrior consists of a mixture\ndistribution (e.g., a mixture of Gaussians) with components given by\nvariational posteriors conditioned on learnable pseudo-inputs. We further\nextend this prior to a two layer hierarchical model and show that this\narchitecture with a coupled prior and posterior, learns significantly better\nmodels. The model also avoids the usual local optima issues related to useless\nlatent dimensions that plague VAEs. We provide empirical studies on six\ndatasets, namely, static and binary MNIST, OMNIGLOT, Caltech 101 Silhouettes,\nFrey Faces and Histopathology patches, and show that applying the hierarchical\nVampPrior delivers state-of-the-art results on all datasets in the unsupervised\npermutation invariant setting and the best results or comparable to SOTA\nmethods for the approach with convolutional networks.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 10:07:00 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 14:14:08 GMT"}, {"version": "v3", "created": "Mon, 21 Aug 2017 12:21:47 GMT"}, {"version": "v4", "created": "Fri, 13 Oct 2017 17:54:28 GMT"}, {"version": "v5", "created": "Mon, 26 Feb 2018 15:23:53 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Tomczak", "Jakub M.", ""], ["Welling", "Max", ""]]}, {"id": "1705.07131", "submitter": "Thang Bui", "authors": "Thang D. Bui, Cuong V. Nguyen, Richard E. Turner", "title": "Streaming Sparse Gaussian Process Approximations", "comments": "To appear at the 31st Conference on Neural Information Processing\n  Systems (NIPS 2017), Long Beach, CA, USA. The first two authors contributed\n  equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse pseudo-point approximations for Gaussian process (GP) models provide a\nsuite of methods that support deployment of GPs in the large data regime and\nenable analytic intractabilities to be sidestepped. However, the field lacks a\nprincipled method to handle streaming data in which both the posterior\ndistribution over function values and the hyperparameter estimates are updated\nin an online fashion. The small number of existing approaches either use\nsuboptimal hand-crafted heuristics for hyperparameter learning, or suffer from\ncatastrophic forgetting or slow updating when new data arrive. This paper\ndevelops a new principled framework for deploying Gaussian process\nprobabilistic models in the streaming setting, providing methods for learning\nhyperparameters and optimising pseudo-input locations. The proposed framework\nis assessed using synthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 18:01:28 GMT"}, {"version": "v2", "created": "Sun, 12 Nov 2017 11:04:02 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Bui", "Thang D.", ""], ["Nguyen", "Cuong V.", ""], ["Turner", "Richard E.", ""]]}, {"id": "1705.07136", "submitter": "Xuezhe Ma", "authors": "Xuezhe Ma, Pengcheng Yin, Jingzhou Liu, Graham Neubig, Eduard Hovy", "title": "Softmax Q-Distribution Estimation for Structured Prediction: A\n  Theoretical Interpretation for RAML", "comments": "Under Review of ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reward augmented maximum likelihood (RAML), a simple and effective learning\nframework to directly optimize towards the reward function in structured\nprediction tasks, has led to a number of impressive empirical successes. RAML\nincorporates task-specific reward by performing maximum-likelihood updates on\ncandidate outputs sampled according to an exponentiated payoff distribution,\nwhich gives higher probabilities to candidates that are close to the reference\noutput. While RAML is notable for its simplicity, efficiency, and its\nimpressive empirical successes, the theoretical properties of RAML, especially\nthe behavior of the exponentiated payoff distribution, has not been examined\nthoroughly. In this work, we introduce softmax Q-distribution estimation, a\nnovel theoretical interpretation of RAML, which reveals the relation between\nRAML and Bayesian decision theory. The softmax Q-distribution can be regarded\nas a smooth approximation of the Bayes decision boundary, and the Bayes\ndecision rule is achieved by decoding with this Q-distribution. We further show\nthat RAML is equivalent to approximately estimating the softmax Q-distribution,\nwith the temperature $\\tau$ controlling approximation error. We perform two\nexperiments, one on synthetic data of multi-class classification and one on\nreal data of image captioning, to demonstrate the relationship between RAML and\nthe proposed softmax Q-distribution estimation method, verifying our\ntheoretical analysis. Additional experiments on three structured prediction\ntasks with rewards defined on sequential (named entity recognition), tree-based\n(dependency parsing) and irregular (machine translation) structures show\nnotable improvements over maximum likelihood baselines.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 18:17:00 GMT"}, {"version": "v2", "created": "Thu, 25 May 2017 04:17:49 GMT"}, {"version": "v3", "created": "Fri, 27 Oct 2017 19:50:50 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Ma", "Xuezhe", ""], ["Yin", "Pengcheng", ""], ["Liu", "Jingzhou", ""], ["Neubig", "Graham", ""], ["Hovy", "Eduard", ""]]}, {"id": "1705.07152", "submitter": "Yang Kang", "authors": "Jose Blanchet, Yang Kang, Fan Zhang, and Karthyek Murthy", "title": "Data-driven Optimal Cost Selection for Distributionally Robust\n  Optimization", "comments": null, "journal-ref": null, "doi": "10.1109/WSC40007.2019.9004785", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, (Blanchet, Kang, and Murhy 2016, and Blanchet, and Kang 2017)\nshowed that several machine learning algorithms, such as square-root Lasso,\nSupport Vector Machines, and regularized logistic regression, among many\nothers, can be represented exactly as distributionally robust optimization\n(DRO) problems. The distributional uncertainty is defined as a neighborhood\ncentered at the empirical distribution. We propose a methodology which learns\nsuch neighborhood in a natural data-driven way. We show rigorously that our\nframework encompasses adaptive regularization as a particular case. Moreover,\nwe demonstrate empirically that our proposed methodology is able to improve\nupon a wide range of popular machine learning estimators.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 19:16:55 GMT"}, {"version": "v2", "created": "Thu, 21 Mar 2019 13:04:48 GMT"}, {"version": "v3", "created": "Fri, 29 Mar 2019 19:38:59 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Blanchet", "Jose", ""], ["Kang", "Yang", ""], ["Zhang", "Fan", ""], ["Murthy", "Karthyek", ""]]}, {"id": "1705.07164", "submitter": "Tianyi Lin", "authors": "Xin Guo, Johnny Hong, Tianyi Lin and Nan Yang", "title": "Relaxed Wasserstein with Applications to GANs", "comments": "Accepted by ICASSP 2021; add the references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wasserstein Generative Adversarial Networks (WGANs) provide a versatile class\nof models, which have attracted great attention in various applications.\nHowever, this framework has two main drawbacks: (i) Wasserstein-1 (or\nEarth-Mover) distance is restrictive such that WGANs cannot always fit data\ngeometry well; (ii) It is difficult to achieve fast training of WGANs. In this\npaper, we propose a new class of \\textit{Relaxed Wasserstein} (RW) distances by\ngeneralizing Wasserstein-1 distance with Bregman cost functions. We show that\nRW distances achieve nice statistical properties while not sacrificing the\ncomputational tractability. Combined with the GANs framework, we develop\nRelaxed WGANs (RWGANs) which are not only statistically flexible but can be\napproximated efficiently using heuristic approaches. Experiments on real images\ndemonstrate that the RWGAN with Kullback-Leibler (KL) cost function outperforms\nother competing approaches, e.g., WGANs, even with gradient penalty.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 19:51:34 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 08:39:34 GMT"}, {"version": "v3", "created": "Fri, 30 Mar 2018 01:54:01 GMT"}, {"version": "v4", "created": "Sun, 16 Sep 2018 20:50:00 GMT"}, {"version": "v5", "created": "Sat, 4 May 2019 08:49:44 GMT"}, {"version": "v6", "created": "Thu, 22 Oct 2020 08:18:42 GMT"}, {"version": "v7", "created": "Sat, 6 Feb 2021 09:33:22 GMT"}, {"version": "v8", "created": "Sat, 17 Jul 2021 06:03:54 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Guo", "Xin", ""], ["Hong", "Johnny", ""], ["Lin", "Tianyi", ""], ["Yang", "Nan", ""]]}, {"id": "1705.07168", "submitter": "Yang Kang", "authors": "Jose Blanchet, Yang Kang, Fan Zhang, Fei He, and Zhangyi Hu", "title": "Doubly Robust Data-Driven Distributionally Robust Optimization", "comments": "arXiv admin note: substantial text overlap with arXiv:1705.07152", "journal-ref": null, "doi": "10.1002/9781119821588.ch4", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven Distributionally Robust Optimization (DD-DRO) via optimal\ntransport has been shown to encompass a wide range of popular machine learning\nalgorithms. The distributional uncertainty size is often shown to correspond to\nthe regularization parameter. The type of regularization (e.g. the norm used to\nregularize) corresponds to the shape of the distributional uncertainty. We\npropose a data-driven robust optimization methodology to inform the\ntransportation cost underlying the definition of the distributional\nuncertainty. We show empirically that this additional layer of robustification,\nwhich produces a method we called doubly robust data-driven distributionally\nrobust optimization (DD-R-DRO), allows to enhance the generalization properties\nof regularized estimators while reducing testing error relative to\nstate-of-the-art classifiers in a wide range of data sets.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 20:05:53 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Blanchet", "Jose", ""], ["Kang", "Yang", ""], ["Zhang", "Fan", ""], ["He", "Fei", ""], ["Hu", "Zhangyi", ""]]}, {"id": "1705.07178", "submitter": "Michael Minyi Zhang", "authors": "Michael Minyi Zhang, Sinead A. Williamson, Fernando Perez-Cruz", "title": "Accelerated Parallel Non-conjugate Sampling for Bayesian Non-parametric\n  Models", "comments": "Previously known as \"Accelerated Inference for Latent Variable\n  Models\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference of latent feature models in the Bayesian nonparametric setting is\ngenerally difficult, especially in high dimensional settings, because it\nusually requires proposing features from some prior distribution. In special\ncases, where the integration is tractable, we can sample new feature\nassignments according to a predictive likelihood. However, this still may not\nbe efficient in high dimensions. We present a novel method to accelerate the\nmixing of latent variable model inference by proposing feature locations based\non the data, as opposed to the prior. First, we introduce an accelerated\nfeature proposal mechanism that we show is a valid Bayesian inference\nalgorithm. Next, we propose an approximate inference strategy to perform\naccelerated inference in parallel. A two-stage algorithm that combines the two\napproaches provides a computationally attractive method that exhibits good\nmixing behavior and converges to the posterior distribution of our model, while\nallowing us to exploit parallelization.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 20:38:55 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 17:01:44 GMT"}, {"version": "v3", "created": "Thu, 14 Mar 2019 01:39:48 GMT"}, {"version": "v4", "created": "Sat, 2 Nov 2019 18:58:26 GMT"}, {"version": "v5", "created": "Tue, 22 Dec 2020 02:22:04 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Zhang", "Michael Minyi", ""], ["Williamson", "Sinead A.", ""], ["Perez-Cruz", "Fernando", ""]]}, {"id": "1705.07204", "submitter": "Florian Tram\\`er", "authors": "Florian Tram\\`er, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow,\n  Dan Boneh, Patrick McDaniel", "title": "Ensemble Adversarial Training: Attacks and Defenses", "comments": "22 pages, 5 figures, International Conference on Learning\n  Representations (ICLR) 2018 (amended in April 2020 to include subsequent\n  attacks that significantly reduced the robustness of our models)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples are perturbed inputs designed to fool machine learning\nmodels. Adversarial training injects such examples into training data to\nincrease robustness. To scale this technique to large datasets, perturbations\nare crafted using fast single-step methods that maximize a linear approximation\nof the model's loss. We show that this form of adversarial training converges\nto a degenerate global minimum, wherein small curvature artifacts near the data\npoints obfuscate a linear approximation of the loss. The model thus learns to\ngenerate weak perturbations, rather than defend against strong ones. As a\nresult, we find that adversarial training remains vulnerable to black-box\nattacks, where we transfer perturbations computed on undefended models, as well\nas to a powerful novel single-step attack that escapes the non-smooth vicinity\nof the input data via a small random step. We further introduce Ensemble\nAdversarial Training, a technique that augments training data with\nperturbations transferred from other models. On ImageNet, Ensemble Adversarial\nTraining yields models with strong robustness to black-box attacks. In\nparticular, our most robust model won the first round of the NIPS 2017\ncompetition on Defenses against Adversarial Attacks. However, subsequent work\nfound that more elaborate black-box attacks could significantly enhance\ntransferability and reduce the accuracy of our models.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 21:56:43 GMT"}, {"version": "v2", "created": "Tue, 30 May 2017 17:00:00 GMT"}, {"version": "v3", "created": "Tue, 30 Jan 2018 18:47:39 GMT"}, {"version": "v4", "created": "Sun, 22 Jul 2018 23:43:46 GMT"}, {"version": "v5", "created": "Sun, 26 Apr 2020 22:20:25 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Tram\u00e8r", "Florian", ""], ["Kurakin", "Alexey", ""], ["Papernot", "Nicolas", ""], ["Goodfellow", "Ian", ""], ["Boneh", "Dan", ""], ["McDaniel", "Patrick", ""]]}, {"id": "1705.07210", "submitter": "Ehsan Amid", "authors": "Ehsan Amid, Manfred K. Warmuth, Sriram Srinivasan", "title": "Two-temperature logistic regression based on the Tsallis divergence", "comments": null, "journal-ref": "In The 22nd International Conference on Artificial Intelligence\n  and Statistics, pp. 2388-2396. 2019", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a variant of multiclass logistic regression that is significantly\nmore robust to noise. The algorithm has one weight vector per class and the\nsurrogate loss is a function of the linear activations (one per class). The\nsurrogate loss of an example with linear activation vector $\\mathbf{a}$ and\nclass $c$ has the form $-\\log_{t_1} \\exp_{t_2} (a_c - G_{t_2}(\\mathbf{a}))$\nwhere the two temperatures $t_1$ and $t_2$ ''temper'' the $\\log$ and $\\exp$,\nrespectively, and $G_{t_2}(\\mathbf{a})$ is a scalar value that generalizes the\nlog-partition function. We motivate this loss using the Tsallis divergence. Our\nmethod allows transitioning between non-convex and convex losses by the choice\nof the temperature parameters. As the temperature $t_1$ of the logarithm\nbecomes smaller than the temperature $t_2$ of the exponential, the surrogate\nloss becomes ''quasi convex''. Various tunings of the temperatures recover\nprevious methods and tuning the degree of non-convexity is crucial in the\nexperiments. In particular, quasi-convexity and boundedness of the loss provide\nsignificant robustness to the outliers. We explain this by showing that $t_1 <\n1$ caps the surrogate loss and $t_2 >1$ makes the predictive distribution have\na heavy tail.\n  We show that the surrogate loss is Bayes-consistent, even in the non-convex\ncase. Additionally, we provide efficient iterative algorithms for calculating\nthe log-partition value only in a few number of iterations. Our compelling\nexperimental results on large real-world datasets show the advantage of using\nthe two-temperature variant in the noisy as well as the noise free case.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 22:18:25 GMT"}, {"version": "v2", "created": "Sat, 3 Aug 2019 01:40:56 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Amid", "Ehsan", ""], ["Warmuth", "Manfred K.", ""], ["Srinivasan", "Sriram", ""]]}, {"id": "1705.07220", "submitter": "Dimitrios Berberidis", "authors": "Dimitris Berberidis, Georgios B. Giannakis", "title": "Data-adaptive Active Sampling for Efficient Graph-Cognizant\n  Classification", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2018.2866812", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present work deals with active sampling of graph nodes representing\ntraining data for binary classification. The graph may be given or constructed\nusing similarity measures among nodal features. Leveraging the graph for\nclassification builds on the premise that labels across neighboring nodes are\ncorrelated according to a categorical Markov random field (MRF). This model is\nfurther relaxed to a Gaussian (G)MRF with labels taking continuous values - an\napproximation that not only mitigates the combinatorial complexity of the\ncategorical model, but also offers optimal unbiased soft predictors of the\nunlabeled nodes. The proposed sampling strategy is based on querying the node\nwhose label disclosure is expected to inflict the largest change on the GMRF,\nand in this sense it is the most informative on average. Such a strategy\nsubsumes several measures of expected model change, including uncertainty\nsampling, variance minimization, and sampling based on the $\\Sigma-$optimality\ncriterion. A simple yet effective heuristic is also introduced for increasing\nthe exploration capabilities of the sampler, and reducing bias of the resultant\nclassifier, by taking into account the confidence on the model label\npredictions. The novel sampling strategies are based on quantities that are\nreadily available without the need for model retraining, rendering them\ncomputationally efficient and scalable to large graphs. Numerical tests using\nsynthetic and real data demonstrate that the proposed methods achieve accuracy\nthat is comparable or superior to the state-of-the-art even at reduced runtime.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 23:06:50 GMT"}, {"version": "v2", "created": "Sun, 4 Jun 2017 23:26:33 GMT"}, {"version": "v3", "created": "Tue, 26 Dec 2017 23:10:19 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Berberidis", "Dimitris", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1705.07224", "submitter": "Marco Cusumano-Towner", "authors": "Marco F. Cusumano-Towner, Vikash K. Mansinghka", "title": "AIDE: An algorithm for measuring the accuracy of probabilistic inference\n  algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate probabilistic inference algorithms are central to many fields.\nExamples include sequential Monte Carlo inference in robotics, variational\ninference in machine learning, and Markov chain Monte Carlo inference in\nstatistics. A key problem faced by practitioners is measuring the accuracy of\nan approximate inference algorithm on a specific data set. This paper\nintroduces the auxiliary inference divergence estimator (AIDE), an algorithm\nfor measuring the accuracy of approximate inference algorithms. AIDE is based\non the observation that inference algorithms can be treated as probabilistic\nmodels and the random variables used within the inference algorithm can be\nviewed as auxiliary variables. This view leads to a new estimator for the\nsymmetric KL divergence between the approximating distributions of two\ninference algorithms. The paper illustrates application of AIDE to algorithms\nfor inference in regression, hidden Markov, and Dirichlet process mixture\nmodels. The experiments show that AIDE captures the qualitative behavior of a\nbroad class of inference algorithms and can detect failure modes of inference\nalgorithms that are missed by standard heuristics.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 23:48:11 GMT"}, {"version": "v2", "created": "Sat, 4 Nov 2017 16:09:58 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Cusumano-Towner", "Marco F.", ""], ["Mansinghka", "Vikash K.", ""]]}, {"id": "1705.07256", "submitter": "Samet Oymak", "authors": "Samet Oymak, Mehrdad Mahdavi, Jiasi Chen", "title": "Learning Feature Nonlinearities with Non-Convex Regularized Binned\n  Regression", "comments": "22 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For various applications, the relations between the dependent and independent\nvariables are highly nonlinear. Consequently, for large scale complex problems,\nneural networks and regression trees are commonly preferred over linear models\nsuch as Lasso. This work proposes learning the feature nonlinearities by\nbinning feature values and finding the best fit in each quantile using\nnon-convex regularized linear regression. The algorithm first captures the\ndependence between neighboring quantiles by enforcing smoothness via\npiecewise-constant/linear approximation and then selects a sparse subset of\ngood features. We prove that the proposed algorithm is statistically and\ncomputationally efficient. In particular, it achieves linear rate of\nconvergence while requiring near-minimal number of samples. Evaluations on\nsynthetic and real datasets demonstrate that algorithm is competitive with\ncurrent state-of-the-art and accurately learns feature nonlinearities. Finally,\nwe explore an interesting connection between the binning stage of our algorithm\nand sparse Johnson-Lindenstrauss matrices.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 03:46:32 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Oymak", "Samet", ""], ["Mahdavi", "Mehrdad", ""], ["Chen", "Jiasi", ""]]}, {"id": "1705.07261", "submitter": "Lam Nguyen", "authors": "Lam M. Nguyen, Jie Liu, Katya Scheinberg, Martin Tak\\'a\\v{c}", "title": "Stochastic Recursive Gradient Algorithm for Nonconvex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study and analyze the mini-batch version of StochAstic\nRecursive grAdient algoritHm (SARAH), a method employing the stochastic\nrecursive gradient, for solving empirical loss minimization for the case of\nnonconvex losses. We provide a sublinear convergence rate (to stationary\npoints) for general nonconvex functions and a linear convergence rate for\ngradient dominated functions, both of which have some advantages compared to\nother modern stochastic gradient algorithms for nonconvex losses.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 05:09:33 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Nguyen", "Lam M.", ""], ["Liu", "Jie", ""], ["Scheinberg", "Katya", ""], ["Tak\u00e1\u010d", "Martin", ""]]}, {"id": "1705.07278", "submitter": "Biswa Sengupta", "authors": "Gerald K Cooray and Richard Rosch and Torsten Baldeweg and Louis\n  Lemieux and Karl Friston and Biswa Sengupta", "title": "Bayesian Belief Updating of Spatiotemporal Seizure Dynamics", "comments": "ICML 2017 Time Series Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.NE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epileptic seizure activity shows complicated dynamics in both space and time.\nTo understand the evolution and propagation of seizures spatially extended sets\nof data need to be analysed. We have previously described an efficient\nfiltering scheme using variational Laplace that can be used in the Dynamic\nCausal Modelling (DCM) framework [Friston, 2003] to estimate the temporal\ndynamics of seizures recorded using either invasive or non-invasive electrical\nrecordings (EEG/ECoG). Spatiotemporal dynamics are modelled using a partial\ndifferential equation -- in contrast to the ordinary differential equation used\nin our previous work on temporal estimation of seizure dynamics [Cooray, 2016].\nWe provide the requisite theoretical background for the method and test the\nensuing scheme on simulated seizure activity data and empirical invasive ECoG\ndata. The method provides a framework to assimilate the spatial and temporal\ndynamics of seizure activity, an aspect of great physiological and clinical\nimportance.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 08:06:05 GMT"}, {"version": "v2", "created": "Sun, 12 Nov 2017 09:27:16 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Cooray", "Gerald K", ""], ["Rosch", "Richard", ""], ["Baldeweg", "Torsten", ""], ["Lemieux", "Louis", ""], ["Friston", "Karl", ""], ["Sengupta", "Biswa", ""]]}, {"id": "1705.07283", "submitter": "Arsenii Ashukha", "authors": "Kirill Neklyudov, Dmitry Molchanov, Arsenii Ashukha, Dmitry Vetrov", "title": "Structured Bayesian Pruning via Log-Normal Multiplicative Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout-based regularization methods can be regarded as injecting random\nnoise with pre-defined magnitude to different parts of the neural network\nduring training. It was recently shown that Bayesian dropout procedure not only\nimproves generalization but also leads to extremely sparse neural architectures\nby automatically setting the individual noise magnitude per weight. However,\nthis sparsity can hardly be used for acceleration since it is unstructured. In\nthe paper, we propose a new Bayesian model that takes into account the\ncomputational structure of neural networks and provides structured sparsity,\ne.g. removes neurons and/or convolutional channels in CNNs. To do this we\ninject noise to the neurons outputs while keeping the weights unregularized. We\nestablish the probabilistic model with a proper truncated log-uniform prior\nover the noise and truncated log-normal variational approximation that ensures\nthat the KL-term in the evidence lower bound is computed in closed-form. The\nmodel leads to structured sparsity by removing elements with a low SNR from the\ncomputation graph and provides significant acceleration on a number of deep\nneural architectures. The model is easy to implement as it can be formulated as\na separate dropout-like layer.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 09:20:06 GMT"}, {"version": "v2", "created": "Sat, 4 Nov 2017 22:14:42 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Neklyudov", "Kirill", ""], ["Molchanov", "Dmitry", ""], ["Ashukha", "Arsenii", ""], ["Vetrov", "Dmitry", ""]]}, {"id": "1705.07321", "submitter": "John Healy", "authors": "Leland McInnes and John Healy", "title": "Accelerated Hierarchical Density Clustering", "comments": null, "journal-ref": "2017 IEEE International Conference on Data Mining Workshops\n  (ICDMW), 2017, pp. 33-42", "doi": "10.1109/ICDMW.2017.12", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an accelerated algorithm for hierarchical density based\nclustering. Our new algorithm improves upon HDBSCAN*, which itself provided a\nsignificant qualitative improvement over the popular DBSCAN algorithm. The\naccelerated HDBSCAN* algorithm provides comparable performance to DBSCAN, while\nsupporting variable density clusters, and eliminating the need for the\ndifficult to tune distance scale parameter. This makes accelerated HDBSCAN* the\ndefault choice for density based clustering.\n  Library available at: https://github.com/scikit-learn-contrib/hdbscan\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 15:32:50 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 23:36:30 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["McInnes", "Leland", ""], ["Healy", "John", ""]]}, {"id": "1705.07347", "submitter": "Xiuyuan Lu", "authors": "Xiuyuan Lu, Benjamin Van Roy", "title": "Ensemble Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thompson sampling has emerged as an effective heuristic for a broad range of\nonline decision problems. In its basic form, the algorithm requires computing\nand sampling from a posterior distribution over models, which is tractable only\nfor simple special cases. This paper develops ensemble sampling, which aims to\napproximate Thompson sampling while maintaining tractability even in the face\nof complex models such as neural networks. Ensemble sampling dramatically\nexpands on the range of applications for which Thompson sampling is viable. We\nestablish a theoretical basis that supports the approach and present\ncomputational results that offer further insight.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 19:36:36 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 21:11:12 GMT"}, {"version": "v3", "created": "Wed, 22 Nov 2017 21:48:42 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Lu", "Xiuyuan", ""], ["Van Roy", "Benjamin", ""]]}, {"id": "1705.07348", "submitter": "Arun Srinivasan", "authors": "Arun Srinivasan", "title": "Calibrating Black Box Classification Models through the Thresholding\n  Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high-dimensional classification settings, we wish to seek a balance\nbetween high power and ensuring control over a desired loss function. In many\nsettings, the points most likely to be misclassified are those who lie near the\ndecision boundary of the given classification method. Often, these\nuninformative points should not be classified as they are noisy and do not\nexhibit strong signals. In this paper, we introduce the Thresholding Method to\nparameterize the problem of determining which points exhibit strong signals and\nshould be classified. We demonstrate the empirical performance of this novel\ncalibration method in providing loss function control at a desired level, as\nwell as explore how the method assuages the effect of overfitting. We explore\nthe benefits of error control through the Thresholding Method in difficult,\nhigh-dimensional, simulated settings. Finally, we show the flexibility of the\nThresholding Method through applying the method in a variety of real data\nsettings.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 19:38:36 GMT"}, {"version": "v2", "created": "Mon, 5 Jun 2017 15:20:18 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Srinivasan", "Arun", ""]]}, {"id": "1705.07349", "submitter": "Ning Xu", "authors": "Ning Xu, Jian Hong, Timothy C.G. Fisher", "title": "$\\left( \\beta, \\varpi \\right)$-stability for cross-validation and the\n  choice of the number of folds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new concept of stability for cross-validation,\ncalled the $\\left( \\beta, \\varpi \\right)$-stability, and use it as a new\nperspective to build the general theory for cross-validation. The $\\left(\n\\beta, \\varpi \\right)$-stability mathematically connects the generalization\nability and the stability of the cross-validated model via the Rademacher\ncomplexity. Our result reveals mathematically the effect of cross-validation\nfrom two sides: on one hand, cross-validation picks the model with the best\nempirical generalization ability by validating all the alternatives on test\nsets; on the other hand, cross-validation may compromise the stability of the\nmodel selection by causing subsampling error. Moreover, the difference between\ntraining and test errors in q\\textsuperscript{th} round, sometimes referred to\nas the generalization error, might be autocorrelated on q. Guided by the ideas\nabove, the $\\left( \\beta, \\varpi \\right)$-stability help us derivd a new class\nof Rademacher bounds, referred to as the one-round/convoluted Rademacher\nbounds, for the stability of cross-validation in both the i.i.d.\\ and\nnon-i.i.d.\\ cases. For both light-tail and heavy-tail losses, the new bounds\nquantify the stability of the one-round/average test error of the\ncross-validated model in terms of its one-round/average training error, the\nsample sizes $n$, number of folds $K$, the tail property of the loss (encoded\nas Orlicz-$\\Psi_\\nu$ norms) and the Rademacher complexity of the model class\n$\\Lambda$. The new class of bounds not only quantitatively reveals the\nstability of the generalization ability of the cross-validated model, it also\nshows empirically the optimal choice for number of folds $K$, at which the\nupper bound of the one-round/average test error is lowest, or, to put it in\nanother way, where the test error is most stable.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 19:46:01 GMT"}, {"version": "v2", "created": "Sat, 27 May 2017 22:53:42 GMT"}, {"version": "v3", "created": "Tue, 30 May 2017 01:44:13 GMT"}, {"version": "v4", "created": "Mon, 19 Jun 2017 10:54:21 GMT"}, {"version": "v5", "created": "Thu, 6 Jul 2017 00:21:03 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Xu", "Ning", ""], ["Hong", "Jian", ""], ["Fisher", "Timothy C. G.", ""]]}, {"id": "1705.07362", "submitter": "Abolfazl Saghafi Dr.", "authors": "Abolfazl Saghafi, Chris P. Tsokos", "title": "Honey Bee Dance Modeling in Real-time using Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The waggle dance that honeybees perform is an astonishing way of\ncommunicating the location of food source. After over 60 years of its\ndiscovery, researchers still use manual labeling by watching hours of dance\nvideos to detect different transitions between dance components thus extracting\ninformation regarding the distance and direction to the food source. We propose\nan automated process to monitor and segment different components of honeybee\nwaggle dance. The process is highly accurate, runs in real-time, and can use\nshared information between multiple dances.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 22:25:05 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Saghafi", "Abolfazl", ""], ["Tsokos", "Chris P.", ""]]}, {"id": "1705.07366", "submitter": "Jeffrey Humpherys", "authors": "Kevin Miller, Chris Hettinger, Jeffrey Humpherys, Tyler Jarvis, and\n  David Kartchner", "title": "Forward Thinking: Building Deep Random Forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep neural networks has inspired many to wonder whether other\nlearners could benefit from deep, layered architectures. We present a general\nframework called forward thinking for deep learning that generalizes the\narchitectural flexibility and sophistication of deep neural networks while also\nallowing for (i) different types of learning functions in the network, other\nthan neurons, and (ii) the ability to adaptively deepen the network as needed\nto improve results. This is done by training one layer at a time, and once a\nlayer is trained, the input data are mapped forward through the layer to create\na new learning problem. The process is then repeated, transforming the data\nthrough multiple layers, one at a time, rendering a new dataset, which is\nexpected to be better behaved, and on which a final output layer can achieve\ngood performance. In the case where the neurons of deep neural nets are\nreplaced with decision trees, we call the result a Forward Thinking Deep Random\nForest (FTDRF). We demonstrate a proof of concept by applying FTDRF on the\nMNIST dataset. We also provide a general mathematical formulation that allows\nfor other types of deep learning problems to be considered.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 22:39:51 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Miller", "Kevin", ""], ["Hettinger", "Chris", ""], ["Humpherys", "Jeffrey", ""], ["Jarvis", "Tyler", ""], ["Kartchner", "David", ""]]}, {"id": "1705.07377", "submitter": "Nathan Kallus", "authors": "Nathan Kallus", "title": "Instrument-Armed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the classic multi-armed bandit (MAB) model to the setting of\nnoncompliance, where the arm pull is a mere instrument and the treatment\napplied may differ from it, which gives rise to the instrument-armed bandit\n(IAB) problem. The IAB setting is relevant whenever the experimental units are\nhuman since free will, ethics, and the law may prohibit unrestricted or forced\napplication of treatment. In particular, the setting is relevant in bandit\nmodels of dynamic clinical trials and other controlled trials on human\ninterventions. Nonetheless, the setting has not been fully investigate in the\nbandit literature. We show that there are various and divergent notions of\nregret in this setting, all of which coincide only in the classic MAB setting.\nWe characterize the behavior of these regrets and analyze standard MAB\nalgorithms. We argue for a particular kind of regret that captures the causal\neffect of treatments but show that standard MAB algorithms cannot achieve\nsublinear control on this regret. Instead, we develop new algorithms for the\nIAB problem, prove new regret bounds for them, and compare them to standard MAB\nalgorithms in numerical examples.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 02:23:36 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Kallus", "Nathan", ""]]}, {"id": "1705.07384", "submitter": "Nathan Kallus", "authors": "Nathan Kallus", "title": "Balanced Policy Evaluation and Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach to the problems of evaluating and learning\npersonalized decision policies from observational data of past contexts,\ndecisions, and outcomes. Only the outcome of the enacted decision is available\nand the historical policy is unknown. These problems arise in personalized\nmedicine using electronic health records and in internet advertising. Existing\napproaches use inverse propensity weighting (or, doubly robust versions) to\nmake historical outcome (or, residual) data look like it were generated by a\nnew policy being evaluated or learned. But this relies on a plug-in approach\nthat rejects data points with a decision that disagrees with the new policy,\nleading to high variance estimates and ineffective learning. We propose a new,\nbalance-based approach that too makes the data look like the new policy but\ndoes so directly by finding weights that optimize for balance between the\nweighted data and the target policy in the given, finite sample, which is\nequivalent to minimizing worst-case or posterior conditional mean square error.\nOur policy learner proceeds as a two-level optimization problem over policies\nand weights. We demonstrate that this approach markedly outperforms existing\nones both in evaluation and learning, which is unsurprising given the wider\nsupport of balance-based weights. We establish extensive theoretical\nconsistency guarantees and regret bounds that support this empirical success.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 03:03:27 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 13:54:48 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Kallus", "Nathan", ""]]}, {"id": "1705.07443", "submitter": "Matthew Staib", "authors": "Matthew Staib, Sebastian Claici, Justin Solomon, Stefanie Jegelka", "title": "Parallel Streaming Wasserstein Barycenters", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficiently aggregating data from different sources is a challenging problem,\nparticularly when samples from each source are distributed differently. These\ndifferences can be inherent to the inference task or present for other reasons:\nsensors in a sensor network may be placed far apart, affecting their individual\nmeasurements. Conversely, it is computationally advantageous to split Bayesian\ninference tasks across subsets of data, but data need not be identically\ndistributed across subsets. One principled way to fuse probability\ndistributions is via the lens of optimal transport: the Wasserstein barycenter\nis a single distribution that summarizes a collection of input measures while\nrespecting their geometry. However, computing the barycenter scales poorly and\nrequires discretization of all input distributions and the barycenter itself.\nImproving on this situation, we present a scalable, communication-efficient,\nparallel algorithm for computing the Wasserstein barycenter of arbitrary\ndistributions. Our algorithm can operate directly on continuous input\ndistributions and is optimized for streaming data. Our method is even robust to\nnonstationary input distributions and produces a barycenter estimate that\ntracks the input measures over time. The algorithm is semi-discrete, needing to\ndiscretize only the barycenter estimate. To the best of our knowledge, we also\nprovide the first bounds on the quality of the approximate barycenter as the\ndiscretization becomes finer. Finally, we demonstrate the practical\neffectiveness of our method, both in tracking moving distributions on a sphere,\nas well as in a large-scale Bayesian inference task.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 12:24:27 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 04:19:11 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Staib", "Matthew", ""], ["Claici", "Sebastian", ""], ["Solomon", "Justin", ""], ["Jegelka", "Stefanie", ""]]}, {"id": "1705.07461", "submitter": "Nir Levine", "authors": "Nir Levine, Tom Zahavy, Daniel J. Mankowitz, Aviv Tamar, Shie Mannor", "title": "Shallow Updates for Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning (DRL) methods such as the Deep Q-Network (DQN)\nhave achieved state-of-the-art results in a variety of challenging,\nhigh-dimensional domains. This success is mainly attributed to the power of\ndeep neural networks to learn rich domain representations for approximating the\nvalue function or policy. Batch reinforcement learning methods with linear\nrepresentations, on the other hand, are more stable and require less hyper\nparameter tuning. Yet, substantial feature engineering is necessary to achieve\ngood results. In this work we propose a hybrid approach -- the Least Squares\nDeep Q-Network (LS-DQN), which combines rich feature representations learned by\na DRL algorithm with the stability of a linear least squares method. We do this\nby periodically re-training the last hidden layer of a DRL network with a batch\nleast squares update. Key to our approach is a Bayesian regularization term for\nthe least squares update, which prevents over-fitting to the more recent data.\nWe tested LS-DQN on five Atari games and demonstrate significant improvement\nover vanilla DQN and Double-DQN. We also investigated the reasons for the\nsuperior performance of our method. Interestingly, we found that the\nperformance improvement can be attributed to the large batch size used by the\nLS method when optimizing the last layer.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 15:20:15 GMT"}, {"version": "v2", "created": "Thu, 2 Nov 2017 19:00:40 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Levine", "Nir", ""], ["Zahavy", "Tom", ""], ["Mankowitz", "Daniel J.", ""], ["Tamar", "Aviv", ""], ["Mannor", "Shie", ""]]}, {"id": "1705.07469", "submitter": "Mohammadreza Soltani", "authors": "Mohammadreza Soltani and Chinmay Hegde", "title": "Improved Algorithms for Matrix Recovery from Rank-One Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimation of a low-rank matrix from a limited\nnumber of noisy rank-one projections. In particular, we propose two fast,\nnon-convex \\emph{proper} algorithms for matrix recovery and support them with\nrigorous theoretical analysis. We show that the proposed algorithms enjoy\nlinear convergence and that their sample complexity is independent of the\ncondition number of the unknown true low-rank matrix. By leveraging recent\nadvances in low-rank matrix approximation techniques, we show that our\nalgorithms achieve computational speed-ups over existing methods. Finally, we\ncomplement our theory with some numerical experiments.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 16:20:55 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Soltani", "Mohammadreza", ""], ["Hegde", "Chinmay", ""]]}, {"id": "1705.07474", "submitter": "Alex Townsend", "authors": "Madeleine Udell and Alex Townsend", "title": "Why are Big Data Matrices Approximately Low Rank?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrices of (approximate) low rank are pervasive in data science, appearing\nin recommender systems, movie preferences, topic models, medical records, and\ngenomics. While there is a vast literature on how to exploit low rank structure\nin these datasets, there is less attention on explaining why the low rank\nstructure appears in the first place. Here, we explain the effectiveness of low\nrank models in data science by considering a simple generative model for these\nmatrices: we suppose that each row or column is associated to a (possibly high\ndimensional) bounded latent variable, and entries of the matrix are generated\nby applying a piecewise analytic function to these latent variables. These\nmatrices are in general full rank. However, we show that we can approximate\nevery entry of an $m \\times n$ matrix drawn from this model to within a fixed\nabsolute error by a low rank matrix whose rank grows as $\\mathcal O(\\log(m +\nn))$. Hence any sufficiently large matrix from such a latent variable model can\nbe approximated, up to a small entrywise error, by a low rank matrix.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 16:49:36 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 18:23:30 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Udell", "Madeleine", ""], ["Townsend", "Alex", ""]]}, {"id": "1705.07477", "submitter": "Tianyang Li", "authors": "Tianyang Li, Liu Liu, Anastasios Kyrillidis, Constantine Caramanis", "title": "Statistical inference using SGD", "comments": "To appear in AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for frequentist statistical inference in\n$M$-estimation problems, based on stochastic gradient descent (SGD) with a\nfixed step size: we demonstrate that the average of such SGD sequences can be\nused for statistical inference, after proper scaling. An intuitive analysis\nusing the Ornstein-Uhlenbeck process suggests that such averages are\nasymptotically normal. From a practical perspective, our SGD-based inference\nprocedure is a first order method, and is well-suited for large scale problems.\nTo show its merits, we apply it to both synthetic and real datasets, and\ndemonstrate that its accuracy is comparable to classical statistical methods,\nwhile requiring potentially far less computation.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 17:01:51 GMT"}, {"version": "v2", "created": "Sun, 19 Nov 2017 20:59:52 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Li", "Tianyang", ""], ["Liu", "Liu", ""], ["Kyrillidis", "Anastasios", ""], ["Caramanis", "Constantine", ""]]}, {"id": "1705.07505", "submitter": "Arash Mehrjou", "authors": "Arash Mehrjou, Bernhard Sch\\\"olkopf, Saeed Saremi", "title": "Annealed Generative Adversarial Networks", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel framework for adversarial training where the target\ndistribution is annealed between the uniform distribution and the data\ndistribution. We posited a conjecture that learning under continuous annealing\nin the nonparametric regime is stable irrespective of the divergence measures\nin the objective function and proposed an algorithm, dubbed {\\ss}-GAN, in\ncorollary. In this framework, the fact that the initial support of the\ngenerative network is the whole ambient space combined with annealing are key\nto balancing the minimax game. In our experiments on synthetic data, MNIST, and\nCelebA, {\\ss}-GAN with a fixed annealing schedule was stable and did not suffer\nfrom mode collapse.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 20:05:59 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Mehrjou", "Arash", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Saremi", "Saeed", ""]]}, {"id": "1705.07538", "submitter": "Peter Bailis", "authors": "Peter Bailis, Kunle Olukotun, Christopher Re, Matei Zaharia", "title": "Infrastructure for Usable Machine Learning: The Stanford DAWN Project", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Despite incredible recent advances in machine learning, building machine\nlearning applications remains prohibitively time-consuming and expensive for\nall but the best-trained, best-funded engineering organizations. This expense\ncomes not from a need for new and improved statistical models but instead from\na lack of systems and tools for supporting end-to-end machine learning\napplication development, from data preparation and labeling to\nproductionization and monitoring. In this document, we outline opportunities\nfor infrastructure supporting usable, end-to-end machine learning applications\nin the context of the nascent DAWN (Data Analytics for What's Next) project at\nStanford.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 02:28:19 GMT"}, {"version": "v2", "created": "Fri, 9 Jun 2017 02:13:09 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Bailis", "Peter", ""], ["Olukotun", "Kunle", ""], ["Re", "Christopher", ""], ["Zaharia", "Matei", ""]]}, {"id": "1705.07541", "submitter": "Takashi Ishida", "authors": "Takashi Ishida, Gang Niu, Weihua Hu, Masashi Sugiyama", "title": "Learning from Complementary Labels", "comments": "NIPS 2017 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collecting labeled data is costly and thus a critical bottleneck in\nreal-world classification tasks. To mitigate this problem, we propose a novel\nsetting, namely learning from complementary labels for multi-class\nclassification. A complementary label specifies a class that a pattern does not\nbelong to. Collecting complementary labels would be less laborious than\ncollecting ordinary labels, since users do not have to carefully choose the\ncorrect class from a long list of candidate classes. However, complementary\nlabels are less informative than ordinary labels and thus a suitable approach\nis needed to better learn from them. In this paper, we show that an unbiased\nestimator to the classification risk can be obtained only from complementarily\nlabeled data, if a loss function satisfies a particular symmetric condition. We\nderive estimation error bounds for the proposed method and prove that the\noptimal parametric convergence rate is achieved. We further show that learning\nfrom complementary labels can be easily combined with learning from ordinary\nlabels (i.e., ordinary supervised learning), providing a highly practical\nimplementation of the proposed method. Finally, we experimentally demonstrate\nthe usefulness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 02:40:47 GMT"}, {"version": "v2", "created": "Sun, 12 Nov 2017 13:38:55 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Ishida", "Takashi", ""], ["Niu", "Gang", ""], ["Hu", "Weihua", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1705.07562", "submitter": "Lei Li", "authors": "Wenqing Hu, Chris Junchi Li, Lei Li, Jian-Guo Liu", "title": "On the diffusion approximation of nonconvex stochastic gradient descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Stochastic Gradient Descent (SGD) method in nonconvex\noptimization problems from the point of view of approximating diffusion\nprocesses. We prove rigorously that the diffusion process can approximate the\nSGD algorithm weakly using the weak form of master equation for probability\nevolution. In the small step size regime and the presence of omnidirectional\nnoise, our weak approximating diffusion process suggests the following dynamics\nfor the SGD iteration starting from a local minimizer (resp.~saddle point): it\nescapes in a number of iterations exponentially (resp.~almost linearly)\ndependent on the inverse stepsize. The results are obtained using the theory\nfor random perturbations of dynamical systems (theory of large deviations for\nlocal minimizers and theory of exiting for unstable stationary points). In\naddition, we discuss the effects of batch size for the deep neural networks,\nand we find that small batch size is helpful for SGD algorithms to escape\nunstable stationary points and sharp minimizers. Our theory indicates that one\nshould increase the batch size at later stage for the SGD to be trapped in flat\nminimizers for better generalization.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 05:34:00 GMT"}, {"version": "v2", "created": "Sat, 3 Mar 2018 15:31:03 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Hu", "Wenqing", ""], ["Li", "Chris Junchi", ""], ["Li", "Lei", ""], ["Liu", "Jian-Guo", ""]]}, {"id": "1705.07585", "submitter": "Michael Mahoney", "authors": "Kristofer E. Bouchard, Alejandro F. Bujan, Farbod Roosta-Khorasani,\n  Shashanka Ubaru, Prabhat, Antoine M. Snijders, Jian-Hua Mao, Edward F. Chang,\n  Michael W. Mahoney, and Sharmodeep Bhattacharyya", "title": "Union of Intersections (UoI) for Interpretable Data Driven Discovery and\n  Prediction", "comments": "42 pages; a conference version is in NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing size and complexity of scientific data could dramatically\nenhance discovery and prediction for basic scientific applications. Realizing\nthis potential, however, requires novel statistical analysis methods that are\nboth interpretable and predictive. We introduce Union of Intersections (UoI), a\nflexible, modular, and scalable framework for enhanced model selection and\nestimation. Methods based on UoI perform model selection and model estimation\nthrough intersection and union operations, respectively. We show that UoI-based\nmethods achieve low-variance and nearly unbiased estimation of a small number\nof interpretable features, while maintaining high-quality prediction accuracy.\nWe perform extensive numerical investigation to evaluate a UoI algorithm\n($UoI_{Lasso}$) on synthetic and real data. In doing so, we demonstrate the\nextraction of interpretable functional networks from human electrophysiology\nrecordings as well as accurate prediction of phenotypes from genotype-phenotype\ndata with reduced features. We also show (with the $UoI_{L1Logistic}$ and\n$UoI_{CUR}$ variants of the basic framework) improved prediction parsimony for\nclassification and matrix factorization on several benchmark biomedical data\nsets. These results suggest that methods based on the UoI framework could\nimprove interpretation and prediction in data-driven discovery across\nscientific fields.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 07:28:55 GMT"}, {"version": "v2", "created": "Thu, 2 Nov 2017 06:43:05 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Bouchard", "Kristofer E.", ""], ["Bujan", "Alejandro F.", ""], ["Roosta-Khorasani", "Farbod", ""], ["Ubaru", "Shashanka", ""], ["Prabhat", "", ""], ["Snijders", "Antoine M.", ""], ["Mao", "Jian-Hua", ""], ["Chang", "Edward F.", ""], ["Mahoney", "Michael W.", ""], ["Bhattacharyya", "Sharmodeep", ""]]}, {"id": "1705.07592", "submitter": "J. Andrew Howe PhD", "authors": "J. Andrew Howe", "title": "Improved Clustering with Augmented k-means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying a set of homogeneous clusters in a heterogeneous dataset is one\nof the most important classes of problems in statistical modeling. In the realm\nof unsupervised partitional clustering, k-means is a very important algorithm\nfor this. In this technical report, we develop a new k-means variant called\nAugmented k-means, which is a hybrid of k-means and logistic regression. During\neach iteration, logistic regression is used to predict the current cluster\nlabels, and the cluster belonging probabilities are used to control the\nsubsequent re-estimation of cluster means. Observations which can't be firmly\nidentified into clusters are excluded from the re-estimation step. This can be\nvaluable when the data exhibit many characteristics of real datasets such as\nheterogeneity, non-sphericity, substantial overlap, and high scatter. Augmented\nk-means frequently outperforms k-means by more accurately classifying\nobservations into known clusters and / or converging in fewer iterations. We\ndemonstrate this on both simulated and real datasets. Our algorithm is\nimplemented in Python and will be available with this report.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 07:47:24 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Howe", "J. Andrew", ""]]}, {"id": "1705.07600", "submitter": "Art\\\"ur Manukyan", "authors": "Art\\\"ur Manukyan and Elvan Ceyhan", "title": "Classification Using Proximity Catch Digraphs (Technical Report)", "comments": "41 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We employ random geometric digraphs to construct semi-parametric classifiers.\nThese data-random digraphs are from parametrized random digraph families called\nproximity catch digraphs (PCDs). A related geometric digraph family, class\ncover catch digraph (CCCD), has been used to solve the class cover problem by\nusing its approximate minimum dominating set. CCCDs showed relatively good\nperformance in the classification of imbalanced data sets, and although CCCDs\nhave a convenient construction in $\\mathbb{R}^d$, finding minimum dominating\nsets is NP-hard and its probabilistic behaviour is not mathematically tractable\nexcept for $d=1$. On the other hand, a particular family of PCDs, called\n\\emph{proportional-edge} PCDs (PE-PCDs), has mathematical tractable minimum\ndominating sets in $\\mathbb{R}^d$; however their construction in higher\ndimensions may be computationally demanding. More specifically, we show that\nthe classifiers based on PE-PCDs are prototype-based classifiers such that the\nexact minimum number of prototypes (equivalent to minimum dominating sets) are\nfound in polynomial time on the number of observations. We construct two types\nof classifiers based on PE-PCDs. One is a family of hybrid classifiers depend\non the location of the points of the training data set, and another type is a\nfamily of classifiers solely based on class covers. We assess the\nclassification performance of our PE-PCD based classifiers by extensive Monte\nCarlo simulations, and compare them with that of other commonly used\nclassifiers. We also show that, similar to CCCD classifiers, our classifiers\nare relatively better in classification in the presence of class imbalance.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 08:09:29 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Manukyan", "Art\u00fcr", ""], ["Ceyhan", "Elvan", ""]]}, {"id": "1705.07603", "submitter": "Vlad Niculae", "authors": "Mathieu Blondel, Vlad Niculae, Takuma Otsuka and Naonori Ueda", "title": "Multi-output Polynomial Networks and Factorization Machines", "comments": "Published at NIPS 2017. 17 pages, including appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factorization machines and polynomial networks are supervised polynomial\nmodels based on an efficient low-rank decomposition. We extend these models to\nthe multi-output setting, i.e., for learning vector-valued functions, with\napplication to multi-class or multi-task problems. We cast this as the problem\nof learning a 3-way tensor whose slices share a common basis and propose a\nconvex formulation of that problem. We then develop an efficient conditional\ngradient algorithm and prove its global convergence, despite the fact that it\ninvolves a non-convex basis selection step. On classification tasks, we show\nthat our algorithm achieves excellent accuracy with much sparser models than\nexisting methods. On recommendation system tasks, we show how to combine our\nalgorithm with a reduction from ordinal regression to multi-output\nclassification and show that the resulting algorithm outperforms simple\nbaselines in terms of ranking accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 08:20:31 GMT"}, {"version": "v2", "created": "Sat, 4 Nov 2017 18:28:53 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Blondel", "Mathieu", ""], ["Niculae", "Vlad", ""], ["Otsuka", "Takuma", ""], ["Ueda", "Naonori", ""]]}, {"id": "1705.07606", "submitter": "Voot Tangkaratt", "authors": "Voot Tangkaratt, Abbas Abdolmaleki, Masashi Sugiyama", "title": "Guide Actor-Critic for Continuous Control", "comments": "ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Actor-critic methods solve reinforcement learning problems by updating a\nparameterized policy known as an actor in a direction that increases an\nestimate of the expected return known as a critic. However, existing\nactor-critic methods only use values or gradients of the critic to update the\npolicy parameter. In this paper, we propose a novel actor-critic method called\nthe guide actor-critic (GAC). GAC firstly learns a guide actor that locally\nmaximizes the critic and then it updates the policy parameter based on the\nguide actor by supervised learning. Our main theoretical contributions are two\nfolds. First, we show that GAC updates the guide actor by performing\nsecond-order optimization in the action space where the curvature matrix is\nbased on the Hessians of the critic. Second, we show that the deterministic\npolicy gradient method is a special case of GAC when the Hessians are ignored.\nThrough experiments, we show that our method is a promising reinforcement\nlearning method for continuous controls.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 08:32:10 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 04:32:24 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Tangkaratt", "Voot", ""], ["Abdolmaleki", "Abbas", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1705.07642", "submitter": "Ilya Tolstikhin", "authors": "Olivier Bousquet, Sylvain Gelly, Ilya Tolstikhin, Carl-Johann\n  Simon-Gabriel, Bernhard Schoelkopf", "title": "From optimal transport to generative modeling: the VEGAN cookbook", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study unsupervised generative modeling in terms of the optimal transport\n(OT) problem between true (but unknown) data distribution $P_X$ and the latent\nvariable model distribution $P_G$. We show that the OT problem can be\nequivalently written in terms of probabilistic encoders, which are constrained\nto match the posterior and prior distributions over the latent space. When\nrelaxed, this constrained optimization problem leads to a penalized optimal\ntransport (POT) objective, which can be efficiently minimized using stochastic\ngradient descent by sampling from $P_X$ and $P_G$. We show that POT for the\n2-Wasserstein distance coincides with the objective heuristically employed in\nadversarial auto-encoders (AAE) (Makhzani et al., 2016), which provides the\nfirst theoretical justification for AAEs known to the authors. We also compare\nPOT to other popular techniques like variational auto-encoders (VAE) (Kingma\nand Welling, 2014). Our theoretical results include (a) a better understanding\nof the commonly observed blurriness of images generated by VAEs, and (b)\nestablishing duality between Wasserstein GAN (Arjovsky and Bottou, 2017) and\nPOT for the 1-Wasserstein distance.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 10:14:05 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Bousquet", "Olivier", ""], ["Gelly", "Sylvain", ""], ["Tolstikhin", "Ilya", ""], ["Simon-Gabriel", "Carl-Johann", ""], ["Schoelkopf", "Bernhard", ""]]}, {"id": "1705.07654", "submitter": "Regev Schweiger", "authors": "Matan Gavish, Regev Schweiger, Elior Rahmani, Eran Halperin", "title": "ReFACTor: Practical Low-Rank Matrix Estimation Under Column-Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various problems in data analysis and statistical genetics call for recovery\nof a column-sparse, low-rank matrix from noisy observations. We propose\nReFACTor, a simple variation of the classical Truncated Singular Value\nDecomposition (TSVD) algorithm. In contrast to previous sparse principal\ncomponent analysis (PCA) algorithms, our algorithm can provably reveal a\nlow-rank signal matrix better, and often significantly better, than the widely\nused TSVD, making it the algorithm of choice whenever column-sparsity is\nsuspected. Empirically, we observe that ReFACTor consistently outperforms TSVD\neven when the underlying signal is not sparse, suggesting that it is generally\nsafe to use ReFACTor instead of TSVD and PCA. The algorithm is extremely simple\nto implement and its running time is dominated by the runtime of PCA, making it\nas practical as standard principal component analysis.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 10:43:09 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Gavish", "Matan", ""], ["Schweiger", "Regev", ""], ["Rahmani", "Elior", ""], ["Halperin", "Eran", ""]]}, {"id": "1705.07673", "submitter": "Wittawat Jitkrittum", "authors": "Wittawat Jitkrittum, Wenkai Xu, Zoltan Szabo, Kenji Fukumizu, Arthur\n  Gretton", "title": "A Linear-Time Kernel Goodness-of-Fit Test", "comments": "Accepted to NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel adaptive test of goodness-of-fit, with computational cost\nlinear in the number of samples. We learn the test features that best indicate\nthe differences between observed samples and a reference model, by minimizing\nthe false negative rate. These features are constructed via Stein's method,\nmeaning that it is not necessary to compute the normalising constant of the\nmodel. We analyse the asymptotic Bahadur efficiency of the new test, and prove\nthat under a mean-shift alternative, our test always has greater relative\nefficiency than a previous linear-time kernel test, regardless of the choice of\nparameters for that test. In experiments, the performance of our method exceeds\nthat of the earlier linear-time test, and matches or exceeds the power of a\nquadratic-time kernel test. In high dimensions and where model structure may be\nexploited, our goodness of fit test performs far better than a quadratic-time\ntwo-sample test based on the Maximum Mean Discrepancy, with samples drawn from\nthe model.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 11:26:46 GMT"}, {"version": "v2", "created": "Tue, 24 Oct 2017 12:45:00 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Jitkrittum", "Wittawat", ""], ["Xu", "Wenkai", ""], ["Szabo", "Zoltan", ""], ["Fukumizu", "Kenji", ""], ["Gretton", "Arthur", ""]]}, {"id": "1705.07704", "submitter": "Mathieu Blondel", "authors": "Vlad Niculae and Mathieu Blondel", "title": "A Regularized Framework for Sparse and Structured Neural Attention", "comments": "In proceedings of NeurIPS 2017; added errata", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern neural networks are often augmented with an attention mechanism, which\ntells the network where to focus within the input. We propose in this paper a\nnew framework for sparse and structured attention, building upon a smoothed max\noperator. We show that the gradient of this operator defines a mapping from\nreal values to probabilities, suitable as an attention mechanism. Our framework\nincludes softmax and a slight generalization of the recently-proposed sparsemax\nas special cases. However, we also show how our framework can incorporate\nmodern structured penalties, resulting in more interpretable attention\nmechanisms, that focus on entire segments or groups of an input. We derive\nefficient algorithms to compute the forward and backward passes of our\nattention mechanisms, enabling their use in a neural network trained with\nbackpropagation. To showcase their potential as a drop-in replacement for\nexisting ones, we evaluate our attention mechanisms on three large-scale tasks:\ntextual entailment, machine translation, and sentence summarization. Our\nattention mechanisms improve interpretability without sacrificing performance;\nnotably, on textual entailment and summarization, we outperform the standard\nattention mechanisms based on softmax and sparsemax.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 13:11:24 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 23:58:20 GMT"}, {"version": "v3", "created": "Sat, 23 Feb 2019 01:00:47 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Niculae", "Vlad", ""], ["Blondel", "Mathieu", ""]]}, {"id": "1705.07751", "submitter": "Bikash Joshi", "authors": "Bikash Joshi, Franck Iutzeler and Massih-Reza Amini", "title": "An Asynchronous Distributed Framework for Large-scale Learning Based on\n  Parameter Exchanges", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many distributed learning problems, the heterogeneous loading of computing\nmachines may harm the overall performance of synchronous strategies. In this\npaper, we propose an effective asynchronous distributed framework for the\nminimization of a sum of smooth functions, where each machine performs\niterations in parallel on its local function and updates a shared parameter\nasynchronously. In this way, all machines can continuously work even though\nthey do not have the latest version of the shared parameter. We prove the\nconvergence of the consistency of this general distributed asynchronous method\nfor gradient iterations then show its efficiency on the matrix factorization\nproblem for recommender systems and on binary classification.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 13:58:58 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Joshi", "Bikash", ""], ["Iutzeler", "Franck", ""], ["Amini", "Massih-Reza", ""]]}, {"id": "1705.07761", "submitter": "Akash Srivastava", "authors": "Akash Srivastava, Lazar Valkov, Chris Russell, Michael U. Gutmann and\n  Charles Sutton", "title": "VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational\n  Learning", "comments": "Published as a conference paper at NIPS, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models provide powerful tools for distributions over\ncomplicated manifolds, such as those of natural images. But many of these\nmethods, including generative adversarial networks (GANs), can be difficult to\ntrain, in part because they are prone to mode collapse, which means that they\ncharacterize only a few modes of the true distribution. To address this, we\nintroduce VEEGAN, which features a reconstructor network, reversing the action\nof the generator by mapping from data to noise. Our training objective retains\nthe original asymptotic consistency guarantee of GANs, and can be interpreted\nas a novel autoencoder loss over the noise. In sharp contrast to a traditional\nautoencoder over data points, VEEGAN does not require specifying a loss\nfunction over the data, but rather only over the representations, which are\nstandard normal by assumption. On an extensive set of synthetic and real world\nimage datasets, VEEGAN indeed resists mode collapsing to a far greater extent\nthan other recent GAN variants, and produces more realistic samples.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 14:17:57 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 14:11:00 GMT"}, {"version": "v3", "created": "Mon, 6 Nov 2017 21:24:56 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Srivastava", "Akash", ""], ["Valkov", "Lazar", ""], ["Russell", "Chris", ""], ["Gutmann", "Michael U.", ""], ["Sutton", "Charles", ""]]}, {"id": "1705.07774", "submitter": "Lukas Balles", "authors": "Lukas Balles and Philipp Hennig", "title": "Dissecting Adam: The Sign, Magnitude and Variance of Stochastic\n  Gradients", "comments": "Presented at the 35th International Conference on Machine Learning\n  (ICML), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ADAM optimizer is exceedingly popular in the deep learning community.\nOften it works very well, sometimes it doesn't. Why? We interpret ADAM as a\ncombination of two aspects: for each weight, the update direction is determined\nby the sign of stochastic gradients, whereas the update magnitude is determined\nby an estimate of their relative variance. We disentangle these two aspects and\nanalyze them in isolation, gaining insight into the mechanisms underlying ADAM.\nThis analysis also extends recent results on adverse effects of ADAM on\ngeneralization, isolating the sign aspect as the problematic one. Transferring\nthe variance adaptation to SGD gives rise to a novel method, completing the\npractitioner's toolbox for problems where ADAM fails.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 14:38:16 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 15:43:10 GMT"}, {"version": "v3", "created": "Wed, 20 Jun 2018 15:00:28 GMT"}, {"version": "v4", "created": "Sun, 13 Dec 2020 14:41:08 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Balles", "Lukas", ""], ["Hennig", "Philipp", ""]]}, {"id": "1705.07795", "submitter": "Francesco Orabona", "authors": "Francesco Orabona and Tatiana Tommasi", "title": "Training Deep Networks without Learning Rates Through Coin Betting", "comments": "Camera-ready version for NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods achieve state-of-the-art performance in many\napplication scenarios. Yet, these methods require a significant amount of\nhyperparameters tuning in order to achieve the best results. In particular,\ntuning the learning rates in the stochastic optimization process is still one\nof the main bottlenecks. In this paper, we propose a new stochastic gradient\ndescent procedure for deep networks that does not require any learning rate\nsetting. Contrary to previous methods, we do not adapt the learning rates nor\nwe make use of the assumed curvature of the objective function. Instead, we\nreduce the optimization process to a game of betting on a coin and propose a\nlearning-rate-free optimal algorithm for this scenario. Theoretical convergence\nis proven for convex and quasi-convex functions and empirical evidence shows\nthe advantage of our algorithm over popular stochastic gradient algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 15:04:05 GMT"}, {"version": "v2", "created": "Tue, 30 May 2017 17:21:27 GMT"}, {"version": "v3", "created": "Sat, 4 Nov 2017 21:19:04 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Orabona", "Francesco", ""], ["Tommasi", "Tatiana", ""]]}, {"id": "1705.07798", "submitter": "Gergely Neu", "authors": "Gergely Neu and Anders Jonsson and Vicen\\c{c} G\\'omez", "title": "A unified view of entropy-regularized Markov decision processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework for entropy-regularized average-reward\nreinforcement learning in Markov decision processes (MDPs). Our approach is\nbased on extending the linear-programming formulation of policy optimization in\nMDPs to accommodate convex regularization functions. Our key result is showing\nthat using the conditional entropy of the joint state-action distributions as\nregularization yields a dual optimization problem closely resembling the\nBellman optimality equations. This result enables us to formalize a number of\nstate-of-the-art entropy-regularized reinforcement learning algorithms as\napproximate variants of Mirror Descent or Dual Averaging, and thus to argue\nabout the convergence properties of these methods. In particular, we show that\nthe exact version of the TRPO algorithm of Schulman et al. (2015) actually\nconverges to the optimal policy, while the entropy-regularized policy gradient\nmethods of Mnih et al. (2016) may fail to converge to a fixed point. Finally,\nwe illustrate empirically the effects of using various regularization\ntechniques on learning performance in a simple reinforcement learning setup.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 15:06:25 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Neu", "Gergely", ""], ["Jonsson", "Anders", ""], ["G\u00f3mez", "Vicen\u00e7", ""]]}, {"id": "1705.07809", "submitter": "Maxim Raginsky", "authors": "Aolin Xu and Maxim Raginsky", "title": "Information-theoretic analysis of generalization capability of learning\n  algorithms", "comments": "Final version, accepted to NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive upper bounds on the generalization error of a learning algorithm in\nterms of the mutual information between its input and output. The bounds\nprovide an information-theoretic understanding of generalization in learning\nproblems, and give theoretical guidelines for striking the right balance\nbetween data fit and generalization by controlling the input-output mutual\ninformation. We propose a number of methods for this purpose, among which are\nalgorithms that regularize the ERM algorithm with relative entropy or with\nrandom noise. Our work extends and leads to nontrivial improvements on the\nrecent results of Russo and Zou.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 15:38:22 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 18:58:37 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Xu", "Aolin", ""], ["Raginsky", "Maxim", ""]]}, {"id": "1705.07815", "submitter": "Jaeho Lee", "authors": "Jaeho Lee and Maxim Raginsky", "title": "Minimax Statistical Learning with Wasserstein Distances", "comments": "published as a conference paper at NIPS 2018, change in title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As opposed to standard empirical risk minimization (ERM), distributionally\nrobust optimization aims to minimize the worst-case risk over a larger\nambiguity set containing the original empirical distribution of the training\ndata. In this work, we describe a minimax framework for statistical learning\nwith ambiguity sets given by balls in Wasserstein space. In particular, we\nprove generalization bounds that involve the covering number properties of the\noriginal ERM problem. As an illustrative example, we provide generalization\nguarantees for transport-based domain adaptation problems where the Wasserstein\ndistance between the source and target domain distributions can be reliably\nestimated from unlabeled samples.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 15:50:47 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 21:31:53 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Lee", "Jaeho", ""], ["Raginsky", "Maxim", ""]]}, {"id": "1705.07819", "submitter": "Swami Sankaranarayanan", "authors": "Swami Sankaranarayanan, Arpit Jain, Rama Chellappa and Ser Nam Lim", "title": "Regularizing deep networks using efficient layerwise adversarial\n  training", "comments": "Published at the Thirty-Second AAAI Conference on Artificial\n  Intelligence (AAAI-18). Official link:\n  https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16634", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training has been shown to regularize deep neural networks in\naddition to increasing their robustness to adversarial examples. However, its\nimpact on very deep state of the art networks has not been fully investigated.\nIn this paper, we present an efficient approach to perform adversarial training\nby perturbing intermediate layer activations and study the use of such\nperturbations as a regularizer during training. We use these perturbations to\ntrain very deep models such as ResNets and show improvement in performance both\non adversarial and original test data. Our experiments highlight the benefits\nof perturbing intermediate layer activations compared to perturbing only the\ninputs. The results on CIFAR-10 and CIFAR-100 datasets show the merits of the\nproposed adversarial training approach. Additional results on WideResNets show\nthat our approach provides significant improvement in classification accuracy\nfor a given base model, outperforming dropout and other base models of larger\nsize.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 15:55:42 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 02:27:51 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Sankaranarayanan", "Swami", ""], ["Jain", "Arpit", ""], ["Chellappa", "Rama", ""], ["Lim", "Ser Nam", ""]]}, {"id": "1705.07832", "submitter": "Yarin Gal", "authors": "Yarin Gal, Jiri Hron, Alex Kendall", "title": "Concrete Dropout", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout is used as a practical tool to obtain uncertainty estimates in large\nvision models and reinforcement learning (RL) tasks. But to obtain\nwell-calibrated uncertainty estimates, a grid-search over the dropout\nprobabilities is necessary - a prohibitive operation with large models, and an\nimpossible one with RL. We propose a new dropout variant which gives improved\nperformance and better calibrated uncertainties. Relying on recent developments\nin Bayesian deep learning, we use a continuous relaxation of dropout's discrete\nmasks. Together with a principled optimisation objective, this allows for\nautomatic tuning of the dropout probability in large models, and as a result\nfaster experimentation cycles. In RL this allows the agent to adapt its\nuncertainty dynamically as more data is observed. We analyse the proposed\nvariant extensively on a range of tasks, and give insights into common practice\nin the field where larger dropout probabilities are often used in deeper model\nlayers.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 16:25:02 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Gal", "Yarin", ""], ["Hron", "Jiri", ""], ["Kendall", "Alex", ""]]}, {"id": "1705.07837", "submitter": "Napat Rujeerapaiboon", "authors": "Napat Rujeerapaiboon, Kilian Schindler, Daniel Kuhn, Wolfram Wiesemann", "title": "Size Matters: Cardinality-Constrained Clustering and Outlier Detection\n  via Conic Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plain vanilla K-means clustering has proven to be successful in practice, yet\nit suffers from outlier sensitivity and may produce highly unbalanced clusters.\nTo mitigate both shortcomings, we formulate a joint outlier detection and\nclustering problem, which assigns a prescribed number of datapoints to an\nauxiliary outlier cluster and performs cardinality-constrained K-means\nclustering on the residual dataset, treating the cluster cardinalities as a\ngiven input. We cast this problem as a mixed-integer linear program (MILP) that\nadmits tractable semidefinite and linear programming relaxations. We propose\ndeterministic rounding schemes that transform the relaxed solutions to feasible\nsolutions for the MILP. We also prove that these solutions are optimal in the\nMILP if a cluster separation condition holds.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 16:32:23 GMT"}, {"version": "v2", "created": "Thu, 5 Oct 2017 11:08:49 GMT"}, {"version": "v3", "created": "Thu, 10 Jan 2019 12:17:00 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Rujeerapaiboon", "Napat", ""], ["Schindler", "Kilian", ""], ["Kuhn", "Daniel", ""], ["Wiesemann", "Wolfram", ""]]}, {"id": "1705.07857", "submitter": "Yarin Gal", "authors": "Piotr Dabkowski, Yarin Gal", "title": "Real Time Image Saliency for Black Box Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we develop a fast saliency detection method that can be applied\nto any differentiable image classifier. We train a masking model to manipulate\nthe scores of the classifier by masking salient parts of the input image. Our\nmodel generalises well to unseen images and requires a single forward pass to\nperform saliency detection, therefore suitable for use in real-time systems. We\ntest our approach on CIFAR-10 and ImageNet datasets and show that the produced\nsaliency maps are easily interpretable, sharp, and free of artifacts. We\nsuggest a new metric for saliency and test our method on the ImageNet object\nlocalisation task. We achieve results outperforming other weakly supervised\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 17:00:55 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Dabkowski", "Piotr", ""], ["Gal", "Yarin", ""]]}, {"id": "1705.07860", "submitter": "Graham Neubig", "authors": "Graham Neubig and Yoav Goldberg and Chris Dyer", "title": "On-the-fly Operation Batching in Dynamic Computation Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic neural network toolkits such as PyTorch, DyNet, and Chainer offer\nmore flexibility for implementing models that cope with data of varying\ndimensions and structure, relative to toolkits that operate on statically\ndeclared computations (e.g., TensorFlow, CNTK, and Theano). However, existing\ntoolkits - both static and dynamic - require that the developer organize the\ncomputations into the batches necessary for exploiting high-performance\nalgorithms and hardware. This batching task is generally difficult, but it\nbecomes a major hurdle as architectures become complex. In this paper, we\npresent an algorithm, and its implementation in the DyNet toolkit, for\nautomatically batching operations. Developers simply write minibatch\ncomputations as aggregations of single instance computations, and the batching\nalgorithm seamlessly executes them, on the fly, using computationally efficient\nbatched operations. On a variety of tasks, we obtain throughput similar to that\nobtained with manual batches, as well as comparable speedups over\nsingle-instance learning on architectures that are impractical to batch\nmanually.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 17:04:56 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Neubig", "Graham", ""], ["Goldberg", "Yoav", ""], ["Dyer", "Chris", ""]]}, {"id": "1705.07874", "submitter": "Scott Lundberg", "authors": "Scott Lundberg and Su-In Lee", "title": "A Unified Approach to Interpreting Model Predictions", "comments": "To appear in NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding why a model makes a certain prediction can be as crucial as the\nprediction's accuracy in many applications. However, the highest accuracy for\nlarge modern datasets is often achieved by complex models that even experts\nstruggle to interpret, such as ensemble or deep learning models, creating a\ntension between accuracy and interpretability. In response, various methods\nhave recently been proposed to help users interpret the predictions of complex\nmodels, but it is often unclear how these methods are related and when one\nmethod is preferable over another. To address this problem, we present a\nunified framework for interpreting predictions, SHAP (SHapley Additive\nexPlanations). SHAP assigns each feature an importance value for a particular\nprediction. Its novel components include: (1) the identification of a new class\nof additive feature importance measures, and (2) theoretical results showing\nthere is a unique solution in this class with a set of desirable properties.\nThe new class unifies six existing methods, notable because several recent\nmethods in the class lack the proposed desirable properties. Based on insights\nfrom this unification, we present new methods that show improved computational\nperformance and/or better consistency with human intuition than previous\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 17:38:10 GMT"}, {"version": "v2", "created": "Sat, 25 Nov 2017 03:53:32 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Lundberg", "Scott", ""], ["Lee", "Su-In", ""]]}, {"id": "1705.07880", "submitter": "Andrew Miller", "authors": "Andrew C. Miller, Nicholas J. Foti, Alexander D'Amour, Ryan P. Adams", "title": "Reducing Reparameterization Gradient Variance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization with noisy gradients has become ubiquitous in statistics and\nmachine learning. Reparameterization gradients, or gradient estimates computed\nvia the \"reparameterization trick,\" represent a class of noisy gradients often\nused in Monte Carlo variational inference (MCVI). However, when these gradient\nestimators are too noisy, the optimization procedure can be slow or fail to\nconverge. One way to reduce noise is to use more samples for the gradient\nestimate, but this can be computationally expensive. Instead, we view the noisy\ngradient as a random variable, and form an inexpensive approximation of the\ngenerating procedure for the gradient sample. This approximation has high\ncorrelation with the noisy gradient by construction, making it a useful control\nvariate for variance reduction. We demonstrate our approach on non-conjugate\nmulti-level hierarchical models and a Bayesian neural net where we observed\ngradient variance reductions of multiple orders of magnitude (20-2,000x).\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 17:51:13 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Miller", "Andrew C.", ""], ["Foti", "Nicholas J.", ""], ["D'Amour", "Alexander", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1705.07881", "submitter": "Lin Yang", "authors": "Lin F. Yang, Vladimir Braverman, Tuo Zhao, Mengdi Wang", "title": "Online Factorization and Partition of Complex Networks From Random Walks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the reduced-dimensional structure is critical to understanding\ncomplex networks. Existing approaches such as spectral clustering are\napplicable only when the full network is explicitly observed. In this paper, we\nfocus on the online factorization and partition of implicit large-scale\nnetworks based on observations from an associated random walk. We formulate\nthis into a nonconvex stochastic factorization problem and propose an efficient\nand scalable stochastic generalized Hebbian algorithm. The algorithm is able to\nprocess dependent state-transition data dynamically generated by the underlying\nnetwork and learn a low-dimensional representation for each vertex. By applying\na diffusion approximation analysis, we show that the continuous-time limiting\nprocess of the stochastic algorithm converges globally to the \"principal\ncomponents\" of the Markov chain and achieves a nearly optimal sample\ncomplexity. Once given the learned low-dimensional representations, we further\napply clustering techniques to recover the network partition. We show that when\nthe associated Markov process is lumpable, one can recover the partition\nexactly with high probability. We apply the proposed approach to model the\ntraffic flow of Manhattan as city-wide random walks. By using our algorithm to\nanalyze the taxi trip data, we discover a latent partition of the Manhattan\ncity that closely matches the traffic dynamics.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 17:51:33 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 16:23:29 GMT"}, {"version": "v3", "created": "Sun, 19 Nov 2017 21:29:08 GMT"}, {"version": "v4", "created": "Mon, 11 Dec 2017 20:47:50 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Yang", "Lin F.", ""], ["Braverman", "Vladimir", ""], ["Zhao", "Tuo", ""], ["Wang", "Mengdi", ""]]}, {"id": "1705.07904", "submitter": "Chris Donahue", "authors": "Chris Donahue, Zachary C. Lipton, Akshay Balsubramani, Julian McAuley", "title": "Semantically Decomposing the Latent Spaces of Generative Adversarial\n  Networks", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm for training generative adversarial networks that\njointly learns latent codes for both identities (e.g. individual humans) and\nobservations (e.g. specific photographs). By fixing the identity portion of the\nlatent codes, we can generate diverse images of the same subject, and by fixing\nthe observation portion, we can traverse the manifold of subjects while\nmaintaining contingent aspects such as lighting and pose. Our algorithm\nfeatures a pairwise training scheme in which each sample from the generator\nconsists of two images with a common identity code. Corresponding samples from\nthe real dataset consist of two distinct photographs of the same subject. In\norder to fool the discriminator, the generator must produce pairs that are\nphotorealistic, distinct, and appear to depict the same individual. We augment\nboth the DCGAN and BEGAN approaches with Siamese discriminators to facilitate\npairwise training. Experiments with human judges and an off-the-shelf face\nverification system demonstrate our algorithm's ability to generate convincing,\nidentity-matched photographs.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 18:00:02 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 18:00:05 GMT"}, {"version": "v3", "created": "Thu, 22 Feb 2018 19:36:33 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Donahue", "Chris", ""], ["Lipton", "Zachary C.", ""], ["Balsubramani", "Akshay", ""], ["McAuley", "Julian", ""]]}, {"id": "1705.07957", "submitter": "Mark Eisen", "authors": "Mark Eisen, Aryan Mokhtari, Alejandro Ribeiro", "title": "Large Scale Empirical Risk Minimization via Truncated Adaptive Newton\n  Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider large scale empirical risk minimization (ERM) problems, where\nboth the problem dimension and variable size is large. In these cases, most\nsecond order methods are infeasible due to the high cost in both computing the\nHessian over all samples and computing its inverse in high dimensions. In this\npaper, we propose a novel adaptive sample size second-order method, which\nreduces the cost of computing the Hessian by solving a sequence of ERM problems\ncorresponding to a subset of samples and lowers the cost of computing the\nHessian inverse using a truncated eigenvalue decomposition. We show that while\nwe geometrically increase the size of the training set at each stage, a single\niteration of the truncated Newton method is sufficient to solve the new ERM\nwithin its statistical accuracy. Moreover, for a large number of samples we are\nallowed to double the size of the training set at each stage, and the proposed\nmethod subsequently reaches the statistical accuracy of the full training set\napproximately after two effective passes. In addition to this theoretical\nresult, we show empirically on a number of well known data sets that the\nproposed truncated adaptive sample size algorithm outperforms stochastic\nalternatives for solving ERM problems.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 19:23:02 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Eisen", "Mark", ""], ["Mokhtari", "Aryan", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "1705.07967", "submitter": "Tiago Peixoto", "authors": "Toni Vall\\`es-Catal\\`a, Tiago P. Peixoto, Roger Guimer\\`a, Marta\n  Sales-Pardo", "title": "Consistencies and inconsistencies between model selection and link\n  prediction in networks", "comments": "12 pages, 6 figures, 1 table", "journal-ref": "Phys. Rev. E 97, 062316 (2018)", "doi": "10.1103/PhysRevE.97.062316", "report-no": null, "categories": "stat.ML cond-mat.dis-nn cond-mat.stat-mech", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  A principled approach to understand network structures is to formulate\ngenerative models. Given a collection of models, however, an outstanding key\ntask is to determine which one provides a more accurate description of the\nnetwork at hand, discounting statistical fluctuations. This problem can be\napproached using two principled criteria that at first may seem equivalent:\nselecting the most plausible model in terms of its posterior probability; or\nselecting the model with the highest predictive performance in terms of\nidentifying missing links. Here we show that while these two approaches yield\nconsistent results in most of cases, there are also notable instances where\nthey do not, that is, where the most plausible model is not the most\npredictive. We show that in the latter case the improvement of predictive\nperformance can in fact lead to overfitting both in artificial and empirical\nsettings. Furthermore, we show that, in general, the predictive performance is\nhigher when we average over collections of models that are individually less\nplausible, than when we consider only the single most plausible model.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 22:43:23 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2018 14:58:21 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Vall\u00e8s-Catal\u00e0", "Toni", ""], ["Peixoto", "Tiago P.", ""], ["Guimer\u00e0", "Roger", ""], ["Sales-Pardo", "Marta", ""]]}, {"id": "1705.08006", "submitter": "Mainak Jas", "authors": "Mainak Jas and Tom Dupr\\'e La Tour and Umut \\c{S}im\\c{s}ekli and\n  Alexandre Gramfort", "title": "Learning the Morphology of Brain Signals Using Alpha-Stable\n  Convolutional Sparse Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural time-series data contain a wide variety of prototypical signal\nwaveforms (atoms) that are of significant importance in clinical and cognitive\nresearch. One of the goals for analyzing such data is hence to extract such\n'shift-invariant' atoms. Even though some success has been reported with\nexisting algorithms, they are limited in applicability due to their heuristic\nnature. Moreover, they are often vulnerable to artifacts and impulsive noise,\nwhich are typically present in raw neural recordings. In this study, we address\nthese issues and propose a novel probabilistic convolutional sparse coding\n(CSC) model for learning shift-invariant atoms from raw neural signals\ncontaining potentially severe artifacts. In the core of our model, which we\ncall $\\alpha$CSC, lies a family of heavy-tailed distributions called\n$\\alpha$-stable distributions. We develop a novel, computationally efficient\nMonte Carlo expectation-maximization algorithm for inference. The maximization\nstep boils down to a weighted CSC problem, for which we develop a\ncomputationally efficient optimization algorithm. Our results show that the\nproposed algorithm achieves state-of-the-art convergence speeds. Besides,\n$\\alpha$CSC is significantly more robust to artifacts when compared to three\ncompeting algorithms: it can extract spike bursts, oscillations, and even\nreveal more subtle phenomena such as cross-frequency coupling when applied to\nnoisy neural time series.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 21:09:13 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 12:51:41 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Jas", "Mainak", ""], ["La Tour", "Tom Dupr\u00e9", ""], ["\u015eim\u015fekli", "Umut", ""], ["Gramfort", "Alexandre", ""]]}, {"id": "1705.08014", "submitter": "Tayfun Gokmen", "authors": "Tayfun Gokmen, O. Murat Onen, Wilfried Haensch", "title": "Training Deep Convolutional Neural Networks with Resistive Cross-Point\n  Devices", "comments": "22 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a previous work we have detailed the requirements to obtain a maximal\nperformance benefit by implementing fully connected deep neural networks (DNN)\nin form of arrays of resistive devices for deep learning. This concept of\nResistive Processing Unit (RPU) devices we extend here towards convolutional\nneural networks (CNNs). We show how to map the convolutional layers to RPU\narrays such that the parallelism of the hardware can be fully utilized in all\nthree cycles of the backpropagation algorithm. We find that the noise and bound\nlimitations imposed due to analog nature of the computations performed on the\narrays effect the training accuracy of the CNNs. Noise and bound management\ntechniques are presented that mitigate these problems without introducing any\nadditional complexity in the analog circuits and can be addressed by the\ndigital circuits. In addition, we discuss digitally programmable update\nmanagement and device variability reduction techniques that can be used\nselectively for some of the layers in a CNN. We show that combination of all\nthose techniques enables a successful application of the RPU concept for\ntraining CNNs. The techniques discussed here are more general and can be\napplied beyond CNN architectures and therefore enables applicability of RPU\napproach for large class of neural network architectures.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 21:40:56 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Gokmen", "Tayfun", ""], ["Onen", "O. Murat", ""], ["Haensch", "Wilfried", ""]]}, {"id": "1705.08030", "submitter": "Saeed Maleki", "authors": "Saeed Maleki, Madanlal Musuvathi, Todd Mytkowicz", "title": "Parallel Stochastic Gradient Descent with Sound Combiners", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) is a well known method for regression and\nclassification tasks. However, it is an inherently sequential algorithm at each\nstep, the processing of the current example depends on the parameters learned\nfrom the previous examples. Prior approaches to parallelizing linear learners\nusing SGD, such as HOGWILD! and ALLREDUCE, do not honor these dependencies\nacross threads and thus can potentially suffer poor convergence rates and/or\npoor scalability. This paper proposes SYMSGD, a parallel SGD algorithm that, to\na first-order approximation, retains the sequential semantics of SGD. Each\nthread learns a local model in addition to a model combiner, which allows local\nmodels to be combined to produce the same result as what a sequential SGD would\nhave produced. This paper evaluates SYMSGD's accuracy and performance on 6\ndatasets on a shared-memory machine shows upto 11x speedup over our heavily\noptimized sequential baseline on 16 cores and 2.2x, on average, faster than\nHOGWILD!.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 22:32:28 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Maleki", "Saeed", ""], ["Musuvathi", "Madanlal", ""], ["Mytkowicz", "Todd", ""]]}, {"id": "1705.08039", "submitter": "Maximilian Nickel", "authors": "Maximilian Nickel, Douwe Kiela", "title": "Poincar\\'e Embeddings for Learning Hierarchical Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning has become an invaluable approach for learning from\nsymbolic data such as text and graphs. However, while complex symbolic datasets\noften exhibit a latent hierarchical structure, state-of-the-art methods\ntypically learn embeddings in Euclidean vector spaces, which do not account for\nthis property. For this purpose, we introduce a new approach for learning\nhierarchical representations of symbolic data by embedding them into hyperbolic\nspace -- or more precisely into an n-dimensional Poincar\\'e ball. Due to the\nunderlying hyperbolic geometry, this allows us to learn parsimonious\nrepresentations of symbolic data by simultaneously capturing hierarchy and\nsimilarity. We introduce an efficient algorithm to learn the embeddings based\non Riemannian optimization and show experimentally that Poincar\\'e embeddings\noutperform Euclidean embeddings significantly on data with latent hierarchies,\nboth in terms of representation capacity and in terms of generalization\nability.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 23:14:36 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 17:40:55 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Nickel", "Maximilian", ""], ["Kiela", "Douwe", ""]]}, {"id": "1705.08045", "submitter": "Sylvestre-Alvise Rebuffi", "authors": "Sylvestre-Alvise Rebuffi, Hakan Bilen, Andrea Vedaldi", "title": "Learning multiple visual domains with residual adapters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing interest in learning data representations that work well\nfor many different types of problems and data. In this paper, we look in\nparticular at the task of learning a single visual representation that can be\nsuccessfully utilized in the analysis of very different types of images, from\ndog breeds to stop signs and digits. Inspired by recent work on learning\nnetworks that predict the parameters of another, we develop a tunable deep\nnetwork architecture that, by means of adapter residual modules, can be steered\non the fly to diverse visual domains. Our method achieves a high degree of\nparameter sharing while maintaining or even improving the accuracy of\ndomain-specific representations. We also introduce the Visual Decathlon\nChallenge, a benchmark that evaluates the ability of representations to capture\nsimultaneously ten very different visual domains and measures their ability to\nrecognize well uniformly.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 23:59:23 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 23:05:40 GMT"}, {"version": "v3", "created": "Tue, 27 Jun 2017 16:56:16 GMT"}, {"version": "v4", "created": "Tue, 22 Aug 2017 07:27:04 GMT"}, {"version": "v5", "created": "Mon, 27 Nov 2017 17:35:38 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Rebuffi", "Sylvestre-Alvise", ""], ["Bilen", "Hakan", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1705.08051", "submitter": "Shuai Xiao", "authors": "Shuai Xiao, Mehrdad Farajtabar, Xiaojing Ye, Junchi Yan, Le Song,\n  Hongyuan Zha", "title": "Wasserstein Learning of Deep Generative Point Process Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point processes are becoming very popular in modeling asynchronous sequential\ndata due to their sound mathematical foundation and strength in modeling a\nvariety of real-world phenomena. Currently, they are often characterized via\nintensity function which limits model's expressiveness due to unrealistic\nassumptions on its parametric form used in practice. Furthermore, they are\nlearned via maximum likelihood approach which is prone to failure in\nmulti-modal distributions of sequences. In this paper, we propose an\nintensity-free approach for point processes modeling that transforms nuisance\nprocesses to a target one. Furthermore, we train the model using a\nlikelihood-free leveraging Wasserstein distance between point processes.\nExperiments on various synthetic and real-world data substantiate the\nsuperiority of the proposed point process model over conventional ones.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 01:08:38 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Xiao", "Shuai", ""], ["Farajtabar", "Mehrdad", ""], ["Ye", "Xiaojing", ""], ["Yan", "Junchi", ""], ["Song", "Le", ""], ["Zha", "Hongyuan", ""]]}, {"id": "1705.08056", "submitter": "Nan Yang", "authors": "Xin Guo, Johnny Hong, Nan Yang", "title": "Ambiguity set and learning via Bregman and Wasserstein", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Construction of ambiguity set in robust optimization relies on the choice of\ndivergences between probability distributions. In distribution learning,\nchoosing appropriate probability distributions based on observed data is\ncritical for approximating the true distribution. To improve the performance of\nmachine learning models, there has recently been interest in designing\nobjective functions based on Lp-Wasserstein distance rather than the classical\nKullback-Leibler (KL) divergence. In this paper, we derive concentration and\nasymptotic results using Bregman divergence. We propose a novel asymmetric\nstatistical divergence called Wasserstein-Bregman divergence as a\ngeneralization of L2-Wasserstein distance. We discuss how these results can be\napplied to the construction of ambiguity set in robust optimization.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 01:58:02 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Guo", "Xin", ""], ["Hong", "Johnny", ""], ["Yang", "Nan", ""]]}, {"id": "1705.08079", "submitter": "Luca Pappalardo", "authors": "Alessio Rossi and Luca Pappalardo and Paolo Cintia and Marcello Iaia\n  and Javier Fernandez and Daniel Medina", "title": "Effective injury forecasting in soccer with GPS training data and\n  machine learning", "comments": null, "journal-ref": "PLoS One 13(7) 2018", "doi": "10.1371/journal.pone.0201264", "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Injuries have a great impact on professional soccer, due to their large\ninfluence on team performance and the considerable costs of rehabilitation for\nplayers. Existing studies in the literature provide just a preliminary\nunderstanding of which factors mostly affect injury risk, while an evaluation\nof the potential of statistical models in forecasting injuries is still\nmissing. In this paper, we propose a multi-dimensional approach to injury\nforecasting in professional soccer that is based on GPS measurements and\nmachine learning. By using GPS tracking technology, we collect data describing\nthe training workload of players in a professional soccer club during a season.\nWe then construct an injury forecaster and show that it is both accurate and\ninterpretable by providing a set of case studies of interest to soccer\npractitioners. Our approach opens a novel perspective on injury prevention,\nproviding a set of simple and practical rules for evaluating and interpreting\nthe complex relations between injury risk and training performance in\nprofessional soccer.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 05:21:02 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 14:06:49 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Rossi", "Alessio", ""], ["Pappalardo", "Luca", ""], ["Cintia", "Paolo", ""], ["Iaia", "Marcello", ""], ["Fernandez", "Javier", ""], ["Medina", "Daniel", ""]]}, {"id": "1705.08118", "submitter": "Carlo Ciliberto", "authors": "Carlo Ciliberto, Alessandro Rudi, Lorenzo Rosasco, Massimiliano Pontil", "title": "Consistent Multitask Learning with Nonlinear Output Relations", "comments": "25 pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Key to multitask learning is exploiting relationships between different tasks\nto improve prediction performance. If the relations are linear, regularization\napproaches can be used successfully. However, in practice assuming the tasks to\nbe linearly related might be restrictive, and allowing for nonlinear structures\nis a challenge. In this paper, we tackle this issue by casting the problem\nwithin the framework of structured prediction. Our main contribution is a novel\nalgorithm for learning multiple tasks which are related by a system of\nnonlinear equations that their joint outputs need to satisfy. We show that the\nalgorithm is consistent and can be efficiently implemented. Experimental\nresults show the potential of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 08:24:06 GMT"}, {"version": "v2", "created": "Thu, 10 Aug 2017 13:47:31 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Ciliberto", "Carlo", ""], ["Rudi", "Alessandro", ""], ["Rosasco", "Lorenzo", ""], ["Pontil", "Massimiliano", ""]]}, {"id": "1705.08142", "submitter": "Sebastian Ruder", "authors": "Sebastian Ruder, Joachim Bingel, Isabelle Augenstein, Anders\n  S{\\o}gaard", "title": "Latent Multi-task Architecture Learning", "comments": "To appear in Proceedings of AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning (MTL) allows deep neural networks to learn from related\ntasks by sharing parameters with other networks. In practice, however, MTL\ninvolves searching an enormous space of possible parameter sharing\narchitectures to find (a) the layers or subspaces that benefit from sharing,\n(b) the appropriate amount of sharing, and (c) the appropriate relative weights\nof the different task losses. Recent work has addressed each of the above\nproblems in isolation. In this work we present an approach that learns a latent\nmulti-task architecture that jointly addresses (a)--(c). We present experiments\non synthetic data and data from OntoNotes 5.0, including four different tasks\nand seven different domains. Our extension consistently outperforms previous\napproaches to learning latent architectures for multi-task problems and\nachieves up to 15% average error reductions over common approaches to MTL.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 08:58:09 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 14:05:37 GMT"}, {"version": "v3", "created": "Mon, 19 Nov 2018 10:30:52 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Ruder", "Sebastian", ""], ["Bingel", "Joachim", ""], ["Augenstein", "Isabelle", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "1705.08153", "submitter": "Jos van der Westhuizen", "authors": "Jos van der Westhuizen and Joan Lasenby", "title": "Techniques for visualizing LSTMs applied to electrocardiograms", "comments": "presented at 2018 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2018), Stockholm, Sweden", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores four different visualization techniques for long\nshort-term memory (LSTM) networks applied to continuous-valued time series. On\nthe datasets analysed, we find that the best visualization technique is to\nlearn an input deletion mask that optimally reduces the true class score. With\na specific focus on single-lead electrocardiograms from the MIT-BIH arrhythmia\ndataset, we show that salient input features for the LSTM classifier align well\nwith medical theory.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 09:35:39 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 16:03:15 GMT"}, {"version": "v3", "created": "Fri, 15 Jun 2018 13:45:59 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["van der Westhuizen", "Jos", ""], ["Lasenby", "Joan", ""]]}, {"id": "1705.08197", "submitter": "Jure Sokolic", "authors": "Jure Sokolic, Qiang Qiu, Miguel R. D. Rodrigues, Guillermo Sapiro", "title": "Learning to Succeed while Teaching to Fail: Privacy in Closed Machine\n  Learning Systems", "comments": "14 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security, privacy, and fairness have become critical in the era of data\nscience and machine learning. More and more we see that achieving universally\nsecure, private, and fair systems is practically impossible. We have seen for\nexample how generative adversarial networks can be used to learn about the\nexpected private training data; how the exploitation of additional data can\nreveal private information in the original one; and how what looks like\nunrelated features can teach us about each other. Confronted with this\nchallenge, in this paper we open a new line of research, where the security,\nprivacy, and fairness is learned and used in a closed environment. The goal is\nto ensure that a given entity (e.g., the company or the government), trusted to\ninfer certain information with our data, is blocked from inferring protected\ninformation from it. For example, a hospital might be allowed to produce\ndiagnosis on the patient (the positive task), without being able to infer the\ngender of the subject (negative task). Similarly, a company can guarantee that\ninternally it is not using the provided data for any undesired task, an\nimportant goal that is not contradicting the virtually impossible challenge of\nblocking everybody from the undesired task. We design a system that learns to\nsucceed on the positive task while simultaneously fail at the negative one, and\nillustrate this with challenging cases where the positive task is actually\nharder than the negative one being blocked. Fairness, to the information in the\nnegative task, is often automatically obtained as a result of this proposed\napproach. The particular framework and examples open the door to security,\nprivacy, and fairness in very important closed scenarios, ranging from private\ndata accumulation companies like social networks to law-enforcement and\nhospitals.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 11:53:02 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Sokolic", "Jure", ""], ["Qiu", "Qiang", ""], ["Rodrigues", "Miguel R. D.", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1705.08236", "submitter": "Adri\\`a Casamitjana D\\'iaz", "authors": "Adri\\`a Casamitjana, Santi Puch, Asier Aduriz, and Ver\\'onica\n  Vilaplana", "title": "3D Convolutional Neural Networks for Brain Tumor Segmentation: A\n  Comparison of Multi-resolution Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes the use of 3D Convolutional Neural Networks for brain\ntumor segmentation in MR images. We address the problem using three different\narchitectures that combine fine and coarse features to obtain the final\nsegmentation. We compare three different networks that use multi-resolution\nfeatures in terms of both design and performance and we show that they improve\ntheir single-resolution counterparts.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 13:28:59 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Casamitjana", "Adri\u00e0", ""], ["Puch", "Santi", ""], ["Aduriz", "Asier", ""], ["Vilaplana", "Ver\u00f3nica", ""]]}, {"id": "1705.08292", "submitter": "Rebecca Roelofs", "authors": "Ashia C. Wilson and Rebecca Roelofs and Mitchell Stern and Nathan\n  Srebro and Benjamin Recht", "title": "The Marginal Value of Adaptive Gradient Methods in Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive optimization methods, which perform local optimization with a metric\nconstructed from the history of iterates, are becoming increasingly popular for\ntraining deep neural networks. Examples include AdaGrad, RMSProp, and Adam. We\nshow that for simple overparameterized problems, adaptive methods often find\ndrastically different solutions than gradient descent (GD) or stochastic\ngradient descent (SGD). We construct an illustrative binary classification\nproblem where the data is linearly separable, GD and SGD achieve zero test\nerror, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to\nhalf. We additionally study the empirical generalization capability of adaptive\nmethods on several state-of-the-art deep learning models. We observe that the\nsolutions found by adaptive methods generalize worse (often significantly\nworse) than SGD, even when these solutions have better training performance.\nThese results suggest that practitioners should reconsider the use of adaptive\nmethods to train neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 14:11:34 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 00:10:53 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Wilson", "Ashia C.", ""], ["Roelofs", "Rebecca", ""], ["Stern", "Mitchell", ""], ["Srebro", "Nathan", ""], ["Recht", "Benjamin", ""]]}, {"id": "1705.08360", "submitter": "Danica J. Sutherland", "authors": "Danica J. Sutherland, Heiko Strathmann, Michael Arbel, Arthur Gretton", "title": "Efficient and principled score estimation with Nystr\\\"om kernel\n  exponential families", "comments": null, "journal-ref": "Proceedings of the Twenty-First International Conference on\n  Artificial Intelligence and Statistics (AISTATS 2018), PMLR 84:652-660", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a fast method with statistical guarantees for learning an\nexponential family density model where the natural parameter is in a\nreproducing kernel Hilbert space, and may be infinite-dimensional. The model is\nlearned by fitting the derivative of the log density, the score, thus avoiding\nthe need to compute a normalization constant. Our approach improves the\ncomputational efficiency of an earlier solution by using a low-rank,\nNystr\\\"om-like solution. The new solution retains the consistency and\nconvergence rates of the full-rank solution (exactly in Fisher distance, and\nnearly in other distances), with guarantees on the degree of cost and storage\nreduction. We evaluate the method in experiments on density estimation and in\nthe construction of an adaptive Hamiltonian Monte Carlo sampler. Compared to an\nexisting score learning approach using a denoising autoencoder, our estimator\nis empirically more data-efficient when estimating the score, runs faster, and\nhas fewer parameters (which can be tuned in a principled and interpretable\nway), in addition to providing statistical guarantees.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 15:29:00 GMT"}, {"version": "v2", "created": "Sun, 4 Jun 2017 22:38:44 GMT"}, {"version": "v3", "created": "Mon, 19 Jun 2017 16:19:42 GMT"}, {"version": "v4", "created": "Fri, 13 Oct 2017 21:43:03 GMT"}, {"version": "v5", "created": "Tue, 13 Mar 2018 19:06:52 GMT"}, {"version": "v6", "created": "Thu, 14 Jan 2021 05:43:23 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Sutherland", "Danica J.", ""], ["Strathmann", "Heiko", ""], ["Arbel", "Michael", ""], ["Gretton", "Arthur", ""]]}, {"id": "1705.08391", "submitter": "Yingjie Fei", "authors": "Yingjie Fei and Yudong Chen", "title": "Exponential error rates of SDP for block models: Beyond Grothendieck's\n  inequality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.SI math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the cluster estimation problem under the Stochastic\nBlock Model. We show that the semidefinite programming (SDP) formulation for\nthis problem achieves an error rate that decays exponentially in the\nsignal-to-noise ratio. The error bound implies weak recovery in the sparse\ngraph regime with bounded expected degrees, as well as exact recovery in the\ndense regime. An immediate corollary of our results yields error bounds under\nthe Censored Block Model. Moreover, these error bounds are robust, continuing\nto hold under heterogeneous edge probabilities and a form of the so-called\nmonotone attack.\n  Significantly, this error rate is achieved by the SDP solution itself without\nany further pre- or post-processing, and improves upon existing\npolynomially-decaying error bounds proved using the Grothendieck\\textquoteright\ns inequality. Our analysis has two key ingredients: (i) showing that the graph\nhas a well-behaved spectrum, even in the sparse regime, after discounting an\nexponentially small number of edges, and (ii) an order-statistics argument that\ngoverns the final error rate. Both arguments highlight the implicit\nregularization effect of the SDP formulation.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 16:14:41 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Fei", "Yingjie", ""], ["Chen", "Yudong", ""]]}, {"id": "1705.08395", "submitter": "Ari Seff", "authors": "Ari Seff, Alex Beatson, Daniel Suo, Han Liu", "title": "Continual Learning in Generative Adversarial Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developments in deep generative models have allowed for tractable learning of\nhigh-dimensional data distributions. While the employed learning procedures\ntypically assume that training data is drawn i.i.d. from the distribution of\ninterest, it may be desirable to model distinct distributions which are\nobserved sequentially, such as when different classes are encountered over\ntime. Although conditional variations of deep generative models permit multiple\ndistributions to be modeled by a single network in a disentangled fashion, they\nare susceptible to catastrophic forgetting when the distributions are\nencountered sequentially. In this paper, we adapt recent work in reducing\ncatastrophic forgetting to the task of training generative adversarial networks\non a sequence of distinct distributions, enabling continual generative\nmodeling.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 16:27:19 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Seff", "Ari", ""], ["Beatson", "Alex", ""], ["Suo", "Daniel", ""], ["Liu", "Han", ""]]}, {"id": "1705.08409", "submitter": "Leye Wang", "authors": "Leye Wang, Xu Geng, Jintao Ke, Chen Peng, Xiaojuan Ma, Daqing Zhang,\n  Qiang Yang", "title": "Ridesourcing Car Detection by Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ridesourcing platforms like Uber and Didi are getting more and more popular\naround the world. However, unauthorized ridesourcing activities taking\nadvantages of the sharing economy can greatly impair the healthy development of\nthis emerging industry. As the first step to regulate on-demand ride services\nand eliminate black market, we design a method to detect ridesourcing cars from\na pool of cars based on their trajectories. Since licensed ridesourcing car\ntraces are not openly available and may be completely missing in some cities\ndue to legal issues, we turn to transferring knowledge from public transport\nopen data, i.e, taxis and buses, to ridesourcing detection among ordinary\nvehicles. We propose a two-stage transfer learning framework. In Stage 1, we\ntake taxi and bus data as input to learn a random forest (RF) classifier using\ntrajectory features shared by taxis/buses and ridesourcing/other cars. Then, we\nuse the RF to label all the candidate cars. In Stage 2, leveraging the subset\nof high confident labels from the previous stage as input, we further learn a\nconvolutional neural network (CNN) classifier for ridesourcing detection, and\niteratively refine RF and CNN, as well as the feature set, via a co-training\nprocess. Finally, we use the resulting ensemble of RF and CNN to identify the\nridesourcing cars in the candidate pool. Experiments on real car, taxi and bus\ntraces show that our transfer learning framework, with no need of a pre-labeled\nridesourcing dataset, can achieve similar accuracy as the supervised learning\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 16:59:29 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Wang", "Leye", ""], ["Geng", "Xu", ""], ["Ke", "Jintao", ""], ["Peng", "Chen", ""], ["Ma", "Xiaojuan", ""], ["Zhang", "Daqing", ""], ["Yang", "Qiang", ""]]}, {"id": "1705.08415", "submitter": "Zhengdao Chen", "authors": "Zhengdao Chen, Xiang Li, Joan Bruna", "title": "Supervised Community Detection with Line Graph Neural Networks", "comments": "Published at International Conference on Learning Representations\n  (ICLR 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, community detection in graphs can be solved using spectral\nmethods or posterior inference under probabilistic graphical models. Focusing\non random graph families such as the stochastic block model, recent research\nhas unified both approaches and identified both statistical and computational\ndetection thresholds in terms of the signal-to-noise ratio. By recasting\ncommunity detection as a node-wise classification problem on graphs, we can\nalso study it from a learning perspective. We present a novel family of Graph\nNeural Networks (GNNs) for solving community detection problems in a supervised\nlearning setting. We show that, in a data-driven manner and without access to\nthe underlying generative models, they can match or even surpass the\nperformance of the belief propagation algorithm on binary and multi-class\nstochastic block models, which is believed to reach the computational\nthreshold. In particular, we propose to augment GNNs with the non-backtracking\noperator defined on the line graph of edge adjacencies. Our models also achieve\ngood performance on real-world datasets. In addition, we perform the first\nanalysis of the optimization landscape of training linear GNNs for community\ndetection problems, demonstrating that under certain simplifications and\nassumptions, the loss values at local and global minima are not far apart.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 17:03:33 GMT"}, {"version": "v2", "created": "Sat, 27 May 2017 14:18:07 GMT"}, {"version": "v3", "created": "Thu, 15 Feb 2018 01:57:13 GMT"}, {"version": "v4", "created": "Thu, 24 May 2018 13:28:54 GMT"}, {"version": "v5", "created": "Thu, 25 Oct 2018 14:29:19 GMT"}, {"version": "v6", "created": "Sat, 8 Aug 2020 21:21:09 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Chen", "Zhengdao", ""], ["Li", "Xiang", ""], ["Bruna", "Joan", ""]]}, {"id": "1705.08417", "submitter": "Viktoriya Krakovna", "authors": "Tom Everitt, Victoria Krakovna, Laurent Orseau, Marcus Hutter, Shane\n  Legg", "title": "Reinforcement Learning with a Corrupted Reward Channel", "comments": "A shorter version of this report was accepted to IJCAI 2017 AI and\n  Autonomy track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  No real-world reward function is perfect. Sensory errors and software bugs\nmay result in RL agents observing higher (or lower) rewards than they should.\nFor example, a reinforcement learning agent may prefer states where a sensory\nerror gives it the maximum reward, but where the true reward is actually small.\nWe formalise this problem as a generalised Markov Decision Problem called\nCorrupt Reward MDP. Traditional RL methods fare poorly in CRMDPs, even under\nstrong simplifying assumptions and when trying to compensate for the possibly\ncorrupt rewards. Two ways around the problem are investigated. First, by giving\nthe agent richer data, such as in inverse reinforcement learning and\nsemi-supervised reinforcement learning, reward corruption stemming from\nsystematic sensory errors may sometimes be completely managed. Second, by using\nrandomisation to blunt the agent's optimisation, reward corruption can be\npartially managed under some assumptions.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 17:06:56 GMT"}, {"version": "v2", "created": "Sat, 19 Aug 2017 05:01:16 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Everitt", "Tom", ""], ["Krakovna", "Victoria", ""], ["Orseau", "Laurent", ""], ["Hutter", "Marcus", ""], ["Legg", "Shane", ""]]}, {"id": "1705.08435", "submitter": "Aur\\'elien Bellet", "authors": "Aur\\'elien Bellet, Rachid Guerraoui, Mahsa Taziki, Marc Tommasi", "title": "Personalized and Private Peer-to-Peer Machine Learning", "comments": "20 pages, to appear in the Proceedings of the 21st International\n  Conference on Artificial Intelligence and Statistics (AISTATS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of connected personal devices together with privacy concerns call\nfor machine learning algorithms capable of leveraging the data of a large\nnumber of agents to learn personalized models under strong privacy\nrequirements. In this paper, we introduce an efficient algorithm to address the\nabove problem in a fully decentralized (peer-to-peer) and asynchronous fashion,\nwith provable convergence rate. We show how to make the algorithm\ndifferentially private to protect against the disclosure of information about\nthe personal datasets, and formally analyze the trade-off between utility and\nprivacy. Our experiments show that our approach dramatically outperforms\nprevious work in the non-private case, and that under privacy constraints, we\ncan significantly improve over models learned in isolation.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 17:43:18 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 09:43:47 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Bellet", "Aur\u00e9lien", ""], ["Guerraoui", "Rachid", ""], ["Taziki", "Mahsa", ""], ["Tommasi", "Marc", ""]]}, {"id": "1705.08475", "submitter": "Matthias Hein", "authors": "Matthias Hein, Maksym Andriushchenko", "title": "Formal Guarantees on the Robustness of a Classifier against Adversarial\n  Manipulation", "comments": "final version accepted at NIPS 2017, fixed bug in implementation of\n  Cross-Lipschitz regularization and lower bound computation, now results are\n  better", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that state-of-the-art classifiers are quite brittle, in\nthe sense that a small adversarial change of an originally with high confidence\ncorrectly classified input leads to a wrong classification again with high\nconfidence. This raises concerns that such classifiers are vulnerable to\nattacks and calls into question their usage in safety-critical systems. We show\nin this paper for the first time formal guarantees on the robustness of a\nclassifier by giving instance-specific lower bounds on the norm of the input\nmanipulation required to change the classifier decision. Based on this analysis\nwe propose the Cross-Lipschitz regularization functional. We show that using\nthis form of regularization in kernel methods resp. neural networks improves\nthe robustness of the classifier without any loss in prediction performance.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 18:48:20 GMT"}, {"version": "v2", "created": "Sun, 5 Nov 2017 20:58:09 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Hein", "Matthias", ""], ["Andriushchenko", "Maksym", ""]]}, {"id": "1705.08481", "submitter": "Cuong Nguyen", "authors": "Cuong V. Nguyen, Lam Si Tung Ho, Huan Xu, Vu Dinh, Binh Nguyen", "title": "Bayesian Pool-based Active Learning With Abstention Feedbacks", "comments": "There is a new version at arXiv:1906.02179", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study pool-based active learning with abstention feedbacks, where a\nlabeler can abstain from labeling a queried example with some unknown\nabstention rate. This is an important problem with many useful applications. We\ntake a Bayesian approach to the problem and develop two new greedy algorithms\nthat learn both the classification problem and the unknown abstention rate at\nthe same time. These are achieved by simply incorporating the estimated\nabstention rate into the greedy criteria. We prove that both of our algorithms\nhave near-optimality guarantees: they respectively achieve a\n${(1-\\frac{1}{e})}$ constant factor approximation of the optimal expected or\nworst-case value of a useful utility function. Our experiments show the\nalgorithms perform well in various practical scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 18:58:51 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 13:57:16 GMT"}, {"version": "v3", "created": "Sun, 3 Jan 2021 01:03:43 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Nguyen", "Cuong V.", ""], ["Ho", "Lam Si Tung", ""], ["Xu", "Huan", ""], ["Dinh", "Vu", ""], ["Nguyen", "Binh", ""]]}, {"id": "1705.08525", "submitter": "Wei-Cheng Chang", "authors": "Wei-Cheng Chang, Chun-Liang Li, Yiming Yang, Barnabas Poczos", "title": "Data-driven Random Fourier Features using Stein Effect", "comments": "To appear in International Joint Conference on Artificial\n  Intelligence (IJCAI), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale kernel approximation is an important problem in machine learning\nresearch. Approaches using random Fourier features have become increasingly\npopular [Rahimi and Recht, 2007], where kernel approximation is treated as\nempirical mean estimation via Monte Carlo (MC) or Quasi-Monte Carlo (QMC)\nintegration [Yang et al., 2014]. A limitation of the current approaches is that\nall the features receive an equal weight summing to 1. In this paper, we\npropose a novel shrinkage estimator from \"Stein effect\", which provides a\ndata-driven weighting strategy for random features and enjoys theoretical\njustifications in terms of lowering the empirical risk. We further present an\nefficient randomized algorithm for large-scale applications of the proposed\nmethod. Our empirical results on six benchmark data sets demonstrate the\nadvantageous performance of this approach over representative baselines in both\nkernel approximation and supervised learning tasks.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 20:27:19 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Chang", "Wei-Cheng", ""], ["Li", "Chun-Liang", ""], ["Yang", "Yiming", ""], ["Poczos", "Barnabas", ""]]}, {"id": "1705.08551", "submitter": "Felix Berkenkamp", "authors": "Felix Berkenkamp, Matteo Turchetta, Angela P. Schoellig, Andreas\n  Krause", "title": "Safe Model-based Reinforcement Learning with Stability Guarantees", "comments": "Proc. of Neural Information Processing Systems (NIPS), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning is a powerful paradigm for learning optimal policies\nfrom experimental data. However, to find optimal policies, most reinforcement\nlearning algorithms explore all possible actions, which may be harmful for\nreal-world systems. As a consequence, learning algorithms are rarely applied on\nsafety-critical systems in the real world. In this paper, we present a learning\nalgorithm that explicitly considers safety, defined in terms of stability\nguarantees. Specifically, we extend control-theoretic results on Lyapunov\nstability verification and show how to use statistical models of the dynamics\nto obtain high-performance control policies with provable stability\ncertificates. Moreover, under additional regularity assumptions in terms of a\nGaussian process prior, we prove that one can effectively and safely collect\ndata in order to learn about the dynamics and thus both improve control\nperformance and expand the safe region of the state space. In our experiments,\nwe show how the resulting algorithm can safely optimize a neural network policy\non a simulated inverted pendulum, without the pendulum ever falling down.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 22:20:08 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 10:14:13 GMT"}, {"version": "v3", "created": "Mon, 13 Nov 2017 18:49:54 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Berkenkamp", "Felix", ""], ["Turchetta", "Matteo", ""], ["Schoellig", "Angela P.", ""], ["Krause", "Andreas", ""]]}, {"id": "1705.08557", "submitter": "Ankit Vani", "authors": "Ankit Vani, Yacine Jernite, David Sontag", "title": "Grounded Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present the Grounded Recurrent Neural Network (GRNN), a\nrecurrent neural network architecture for multi-label prediction which\nexplicitly ties labels to specific dimensions of the recurrent hidden state (we\ncall this process \"grounding\"). The approach is particularly well-suited for\nextracting large numbers of concepts from text. We apply the new model to\naddress an important problem in healthcare of understanding what medical\nconcepts are discussed in clinical text. Using a publicly available dataset\nderived from Intensive Care Units, we learn to label a patient's diagnoses and\nprocedures from their discharge summary. Our evaluation shows a clear advantage\nto using our proposed architecture over a variety of strong baselines.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 23:17:49 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Vani", "Ankit", ""], ["Jernite", "Yacine", ""], ["Sontag", "David", ""]]}, {"id": "1705.08562", "submitter": "Kun He", "authors": "Kun He, Fatih Cakir, Sarah Adel Bargal, Stan Sclaroff", "title": "Hashing as Tie-Aware Learning to Rank", "comments": "15 pages, 3 figures. IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing, or learning binary embeddings of data, is frequently used in nearest\nneighbor retrieval. In this paper, we develop learning to rank formulations for\nhashing, aimed at directly optimizing ranking-based evaluation metrics such as\nAverage Precision (AP) and Normalized Discounted Cumulative Gain (NDCG). We\nfirst observe that the integer-valued Hamming distance often leads to tied\nrankings, and propose to use tie-aware versions of AP and NDCG to evaluate\nhashing for retrieval. Then, to optimize tie-aware ranking metrics, we derive\ntheir continuous relaxations, and perform gradient-based optimization with deep\nneural networks. Our results establish the new state-of-the-art for image\nretrieval by Hamming ranking in common benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 23:42:46 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 02:07:11 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 04:42:56 GMT"}, {"version": "v4", "created": "Tue, 9 Oct 2018 20:37:00 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["He", "Kun", ""], ["Cakir", "Fatih", ""], ["Bargal", "Sarah Adel", ""], ["Sclaroff", "Stan", ""]]}, {"id": "1705.08564", "submitter": "Wenbo Guo", "authors": "Wenbo Guo, Kaixuan Zhang, Lin Lin, Sui Huang, Xinyu Xing", "title": "Towards Interrogating Discriminative Machine Learning Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is oftentimes impossible to understand how machine learning models reach a\ndecision. While recent research has proposed various technical approaches to\nprovide some clues as to how a learning model makes individual decisions, they\ncannot provide users with ability to inspect a learning model as a complete\nentity. In this work, we propose a new technical approach that augments a\nBayesian regression mixture model with multiple elastic nets. Using the\nenhanced mixture model, we extract explanations for a target model through\nglobal approximation. To demonstrate the utility of our approach, we evaluate\nit on different learning models covering the tasks of text mining and image\nrecognition. Our results indicate that the proposed approach not only\noutperforms the state-of-the-art technique in explaining individual decisions\nbut also provides users with an ability to discover the vulnerabilities of a\nlearning model.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 23:51:37 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Guo", "Wenbo", ""], ["Zhang", "Kaixuan", ""], ["Lin", "Lin", ""], ["Huang", "Sui", ""], ["Xing", "Xinyu", ""]]}, {"id": "1705.08580", "submitter": "Bowei Yan", "authors": "Bowei Yan, Purnamrita Sarkar, Xiuyuan Cheng", "title": "Provable Estimation of the Number of Blocks in Block Models", "comments": "12 pages, 4 figure; AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection is a fundamental unsupervised learning problem for\nunlabeled networks which has a broad range of applications. Many community\ndetection algorithms assume that the number of clusters $r$ is known apriori.\nIn this paper, we propose an approach based on semi-definite relaxations, which\ndoes not require prior knowledge of model parameters like many existing convex\nrelaxation methods and recovers the number of clusters and the clustering\nmatrix exactly under a broad parameter regime, with probability tending to one.\nOn a variety of simulated and real data experiments, we show that the proposed\nmethod often outperforms state-of-the-art techniques for estimating the number\nof clusters.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 01:42:50 GMT"}, {"version": "v2", "created": "Fri, 13 Oct 2017 22:34:00 GMT"}, {"version": "v3", "created": "Sun, 18 Mar 2018 19:46:34 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Yan", "Bowei", ""], ["Sarkar", "Purnamrita", ""], ["Cheng", "Xiuyuan", ""]]}, {"id": "1705.08584", "submitter": "Wei-Cheng Chang", "authors": "Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, Barnab\\'as\n  P\\'oczos", "title": "MMD GAN: Towards Deeper Understanding of Moment Matching Network", "comments": "In the Proceedings of Thirty-first Annual Conference on Neural\n  Information Processing Systems (NIPS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative moment matching network (GMMN) is a deep generative model that\ndiffers from Generative Adversarial Network (GAN) by replacing the\ndiscriminator in GAN with a two-sample test based on kernel maximum mean\ndiscrepancy (MMD). Although some theoretical guarantees of MMD have been\nstudied, the empirical performance of GMMN is still not as competitive as that\nof GAN on challenging and large benchmark datasets. The computational\nefficiency of GMMN is also less desirable in comparison with GAN, partially due\nto its requirement for a rather large batch size during the training. In this\npaper, we propose to improve both the model expressiveness of GMMN and its\ncomputational efficiency by introducing adversarial kernel learning techniques,\nas the replacement of a fixed Gaussian kernel in the original GMMN. The new\napproach combines the key ideas in both GMMN and GAN, hence we name it MMD GAN.\nThe new distance measure in MMD GAN is a meaningful loss that enjoys the\nadvantage of weak topology and can be optimized via gradient descent with\nrelatively small batch sizes. In our evaluation on multiple benchmark datasets,\nincluding MNIST, CIFAR- 10, CelebA and LSUN, the performance of MMD-GAN\nsignificantly outperforms GMMN, and is competitive with other representative\nGAN works.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 02:20:29 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 17:05:42 GMT"}, {"version": "v3", "created": "Mon, 27 Nov 2017 14:04:35 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Li", "Chun-Liang", ""], ["Chang", "Wei-Cheng", ""], ["Cheng", "Yu", ""], ["Yang", "Yiming", ""], ["P\u00f3czos", "Barnab\u00e1s", ""]]}, {"id": "1705.08618", "submitter": "Aniket Anand Deshmukh", "authors": "Aniket Anand Deshmukh, Urun Dogan, Clayton Scott", "title": "Multi-Task Learning for Contextual Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual bandits are a form of multi-armed bandit in which the agent has\naccess to predictive side information (known as the context) for each arm at\neach time step, and have been used to model personalized news recommendation,\nad placement, and other applications. In this work, we propose a multi-task\nlearning framework for contextual bandit problems. Like multi-task learning in\nthe batch setting, the goal is to leverage similarities in contexts for\ndifferent arms so as to improve the agent's ability to predict rewards from\ncontexts. We propose an upper confidence bound-based multi-task learning\nalgorithm for contextual bandits, establish a corresponding regret bound, and\ninterpret this bound to quantify the advantages of learning in the presence of\nhigh task (arm) similarity. We also describe an effective scheme for estimating\ntask similarity from data, and demonstrate our algorithm's performance on\nseveral data sets.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 05:47:52 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Deshmukh", "Aniket Anand", ""], ["Dogan", "Urun", ""], ["Scott", "Clayton", ""]]}, {"id": "1705.08621", "submitter": "Julian Katz-Samuels", "authors": "Julian Katz-Samuels and Clayton Scott", "title": "Nonparametric Preference Completion", "comments": "AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of collaborative preference completion: given a pool of\nitems, a pool of users and a partially observed item-user rating matrix, the\ngoal is to recover the \\emph{personalized ranking} of each user over all of the\nitems. Our approach is nonparametric: we assume that each item $i$ and each\nuser $u$ have unobserved features $x_i$ and $y_u$, and that the associated\nrating is given by $g_u(f(x_i,y_u))$ where $f$ is Lipschitz and $g_u$ is a\nmonotonic transformation that depends on the user. We propose a $k$-nearest\nneighbors-like algorithm and prove that it is consistent. To the best of our\nknowledge, this is the first consistency result for the collaborative\npreference completion problem in a nonparametric setting. Finally, we\ndemonstrate the performance of our algorithm with experiments on the Netflix\nand Movielens datasets.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 06:04:58 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 17:03:04 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Katz-Samuels", "Julian", ""], ["Scott", "Clayton", ""]]}, {"id": "1705.08664", "submitter": "Yuting Zhang", "authors": "Anna C. Gilbert, Yi Zhang, Kibok Lee, Yuting Zhang, Honglak Lee", "title": "Towards Understanding the Invertibility of Convolutional Neural Networks", "comments": null, "journal-ref": "IJCAI 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recent works have empirically observed that Convolutional Neural Nets\n(CNNs) are (approximately) invertible. To understand this approximate\ninvertibility phenomenon and how to leverage it more effectively, we focus on a\ntheoretical explanation and develop a mathematical model of sparse signal\nrecovery that is consistent with CNNs with random weights. We give an exact\nconnection to a particular model of model-based compressive sensing (and its\nrecovery algorithms) and random-weight CNNs. We show empirically that several\nlearned networks are consistent with our mathematical analysis and then\ndemonstrate that with such a simple theoretical framework, we can obtain\nreasonable re- construction results on real images. We also discuss gaps\nbetween our model assumptions and the CNN trained for classification in\npractical scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 09:02:52 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Gilbert", "Anna C.", ""], ["Zhang", "Yi", ""], ["Lee", "Kibok", ""], ["Zhang", "Yuting", ""], ["Lee", "Honglak", ""]]}, {"id": "1705.08665", "submitter": "Christos Louizos", "authors": "Christos Louizos, Karen Ullrich and Max Welling", "title": "Bayesian Compression for Deep Learning", "comments": "Published as a conference paper at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compression and computational efficiency in deep learning have become a\nproblem of great significance. In this work, we argue that the most principled\nand effective way to attack this problem is by adopting a Bayesian point of\nview, where through sparsity inducing priors we prune large parts of the\nnetwork. We introduce two novelties in this paper: 1) we use hierarchical\npriors to prune nodes instead of individual weights, and 2) we use the\nposterior uncertainties to determine the optimal fixed point precision to\nencode the weights. Both factors significantly contribute to achieving the\nstate of the art in terms of compression rates, while still staying competitive\nwith methods designed to optimize for speed or energy efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 09:07:01 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 04:59:44 GMT"}, {"version": "v3", "created": "Thu, 10 Aug 2017 04:03:01 GMT"}, {"version": "v4", "created": "Mon, 6 Nov 2017 12:46:40 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Louizos", "Christos", ""], ["Ullrich", "Karen", ""], ["Welling", "Max", ""]]}, {"id": "1705.08707", "submitter": "Tatiana Likhomanenko", "authors": "Tatiana Likhomanenko, Denis Derkach, Alex Rogozhnikov", "title": "Inclusive Flavour Tagging Algorithm", "comments": "5 pages, 5 figures, 17th International workshop on Advanced Computing\n  and Analysis Techniques in physics research (ACAT-2016)", "journal-ref": "Likhomanenko, T., Derkach, D., & Rogozhnikov, A. (2016, October).\n  Inclusive Flavour Tagging Algorithm. In Journal of Physics: Conference Series\n  (Vol. 762, No. 1, p. 012045). IOP Publishing", "doi": "10.1088/1742-6596/762/1/012045", "report-no": null, "categories": "hep-ex stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the flavour of neutral $B$ mesons production is one of the most\nimportant components needed in the study of time-dependent $CP$ violation. The\nharsh environment of the Large Hadron Collider makes it particularly hard to\nsucceed in this task. We present an inclusive flavour-tagging algorithm as an\nupgrade of the algorithms currently used by the LHCb experiment. Specifically,\na probabilistic model which efficiently combines information from reconstructed\nvertices and tracks using machine learning is proposed. The algorithm does not\nuse information about underlying physics process. It reduces the dependence on\nthe performance of lower level identification capacities and thus increases the\noverall performance. The proposed inclusive flavour-tagging algorithm is\napplicable to tag the flavour of $B$ mesons in any proton-proton experiment.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 11:45:46 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Likhomanenko", "Tatiana", ""], ["Derkach", "Denis", ""], ["Rogozhnikov", "Alex", ""]]}, {"id": "1705.08716", "submitter": "Bertrand Lebichot", "authors": "Bertrand Lebichot and Marco Saerens", "title": "An experimental study of graph-based semi-supervised classification with\n  additional node information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The volume of data generated by internet and social networks is increasing\nevery day, and there is a clear need for efficient ways of extracting useful\ninformation from them. As those data can take different forms, it is important\nto use all the available data representations for prediction.\n  In this paper, we focus our attention on supervised classification using both\nregular plain, tabular, data and structural information coming from a network\nstructure. 14 techniques are investigated and compared in this study and can be\ndivided in three classes: the first one uses only the plain data to build a\nclassification model, the second uses only the graph structure and the last\nuses both information sources. The relative performances in these three cases\nare investigated. Furthermore, the effect of using a graph embedding and\nwell-known indicators in spatial statistics is also studied.\n  Possible applications are automatic classification of web pages or other\nlinked documents, of people in a social network or of proteins in a biological\ncomplex system, to name a few.\n  Based on our comparison, we draw some general conclusions and advices to\ntackle this particular classification task: some datasets can be better\nexplained by their graph structure (graph-driven), or by their feature set\n(features-driven). The most efficient methods are discussed in both cases.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 12:05:44 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Lebichot", "Bertrand", ""], ["Saerens", "Marco", ""]]}, {"id": "1705.08736", "submitter": "Sami Remes", "authors": "Sami Remes, Markus Heinonen, Samuel Kaski", "title": "Non-Stationary Spectral Kernels", "comments": "16 pages, 5 figures", "journal-ref": "Advances in Neural Information Processing Systems 30 (2017),\n  4642-4651", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose non-stationary spectral kernels for Gaussian process regression.\nWe propose to model the spectral density of a non-stationary kernel function as\na mixture of input-dependent Gaussian process frequency density surfaces. We\nsolve the generalised Fourier transform with such a model, and present a family\nof non-stationary and non-monotonic kernels that can learn input-dependent and\npotentially long-range, non-monotonic covariances between inputs. We derive\nefficient inference using model whitening and marginalized posterior, and show\nwith case studies that these kernels are necessary when modelling even rather\nsimple time series, image or geospatial data with non-stationary\ncharacteristics.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 12:58:58 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Remes", "Sami", ""], ["Heinonen", "Markus", ""], ["Kaski", "Samuel", ""]]}, {"id": "1705.08741", "submitter": "Elad Hoffer", "authors": "Elad Hoffer, Itay Hubara, Daniel Soudry", "title": "Train longer, generalize better: closing the generalization gap in large\n  batch training of neural networks", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 30 2017; pages\n  1729-1739;\n  http://papers.nips.cc/paper/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Deep learning models are typically trained using stochastic\ngradient descent or one of its variants. These methods update the weights using\ntheir gradient, estimated from a small fraction of the training data. It has\nbeen observed that when using large batch sizes there is a persistent\ndegradation in generalization performance - known as the \"generalization gap\"\nphenomena. Identifying the origin of this gap and closing it had remained an\nopen problem.\n  Contributions: We examine the initial high learning rate training phase. We\nfind that the weight distance from its initialization grows logarithmically\nwith the number of weight updates. We therefore propose a \"random walk on\nrandom landscape\" statistical model which is known to exhibit similar\n\"ultra-slow\" diffusion behavior. Following this hypothesis we conducted\nexperiments to show empirically that the \"generalization gap\" stems from the\nrelatively small number of updates rather than the batch size, and can be\ncompletely eliminated by adapting the training regime used. We further\ninvestigate different techniques to train models in the large-batch regime and\npresent a novel algorithm named \"Ghost Batch Normalization\" which enables\nsignificant decrease in the generalization gap without increasing the number of\nupdates. To validate our findings we conduct several additional experiments on\nMNIST, CIFAR-10, CIFAR-100 and ImageNet. Finally, we reassess common practices\nand beliefs concerning training of deep models and suggest they may not be\noptimal to achieve good generalization.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 13:17:27 GMT"}, {"version": "v2", "created": "Mon, 1 Jan 2018 08:49:43 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Hoffer", "Elad", ""], ["Hubara", "Itay", ""], ["Soudry", "Daniel", ""]]}, {"id": "1705.08804", "submitter": "Sirui Yao", "authors": "Sirui Yao, Bert Huang", "title": "Beyond Parity: Fairness Objectives for Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study fairness in collaborative-filtering recommender systems, which are\nsensitive to discrimination that exists in historical data. Biased data can\nlead collaborative-filtering methods to make unfair predictions for users from\nminority groups. We identify the insufficiency of existing fairness metrics and\npropose four new metrics that address different forms of unfairness. These\nfairness metrics can be optimized by adding fairness terms to the learning\nobjective. Experiments on synthetic and real data show that our new metrics can\nbetter measure fairness than the baseline, and that the fairness objectives\neffectively help reduce unfairness.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 14:52:06 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 21:11:25 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Yao", "Sirui", ""], ["Huang", "Bert", ""]]}, {"id": "1705.08814", "submitter": "Odalric-Ambrym Maillard", "authors": "Odalric-Ambrym Maillard", "title": "Boundary Crossing Probabilities for General Exponential Families", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider parametric exponential families of dimension $K$ on the real\nline. We study a variant of \\textit{boundary crossing probabilities} coming\nfrom the multi-armed bandit literature, in the case when the real-valued\ndistributions form an exponential family of dimension $K$. Formally, our result\nis a concentration inequality that bounds the probability that\n$\\mathcal{B}^\\psi(\\hat \\theta_n,\\theta^\\star)\\geq f(t/n)/n$, where\n$\\theta^\\star$ is the parameter of an unknown target distribution, $\\hat\n\\theta_n$ is the empirical parameter estimate built from $n$ observations,\n$\\psi$ is the log-partition function of the exponential family and\n$\\mathcal{B}^\\psi$ is the corresponding Bregman divergence. From the\nperspective of stochastic multi-armed bandits, we pay special attention to the\ncase when the boundary function $f$ is logarithmic, as it is enables to analyze\nthe regret of the state-of-the-art \\KLUCB\\ and \\KLUCBp\\ strategies, whose\nanalysis was left open in such generality. Indeed, previous results only hold\nfor the case when $K=1$, while we provide results for arbitrary finite\ndimension $K$, thus considerably extending the existing results. Perhaps\nsurprisingly, we highlight that the proof techniques to achieve these strong\nresults already existed three decades ago in the work of T.L. Lai, and were\napparently forgotten in the bandit community. We provide a modern rewriting of\nthese beautiful techniques that we believe are useful beyond the application to\nstochastic multi-armed bandits.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 15:19:05 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Maillard", "Odalric-Ambrym", ""]]}, {"id": "1705.08815", "submitter": "Francesco Fusco", "authors": "Francesco Fusco, Seshu Tirupathi and Robert Gormally", "title": "Power Systems Data Fusion based on Belief Propagation", "comments": "Version as accepted for publication at the 7th IEEE International\n  Conference on Innovative Smart Grid Technologies (ISGT) Europe 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing complexity of the power grid, due to higher penetration of\ndistributed resources and the growing availability of interconnected,\ndistributed metering devices re- quires novel tools for providing a unified and\nconsistent view of the system. A computational framework for power systems data\nfusion, based on probabilistic graphical models, capable of combining\nheterogeneous data sources with classical state estimation nodes and other\ncustomised computational nodes, is proposed. The framework allows flexible\nextension of the notion of grid state beyond the view of flows and injection in\nbus-branch models, and an efficient, naturally distributed inference algorithm\ncan be derived. An application of the data fusion model to the quantification\nof distributed solar energy is proposed through numerical examples based on\nsemi-synthetic simulations of the standard IEEE 14-bus test case.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 15:20:11 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Fusco", "Francesco", ""], ["Tirupathi", "Seshu", ""], ["Gormally", "Robert", ""]]}, {"id": "1705.08821", "submitter": "Christos Louizos", "authors": "Christos Louizos, Uri Shalit, Joris Mooij, David Sontag, Richard Zemel\n  and Max Welling", "title": "Causal Effect Inference with Deep Latent-Variable Models", "comments": "Published as a conference paper at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning individual-level causal effects from observational data, such as\ninferring the most effective medication for a specific patient, is a problem of\ngrowing importance for policy makers. The most important aspect of inferring\ncausal effects from observational data is the handling of confounders, factors\nthat affect both an intervention and its outcome. A carefully designed\nobservational study attempts to measure all important confounders. However,\neven if one does not have direct access to all confounders, there may exist\nnoisy and uncertain measurement of proxies for confounders. We build on recent\nadvances in latent variable modeling to simultaneously estimate the unknown\nlatent space summarizing the confounders and the causal effect. Our method is\nbased on Variational Autoencoders (VAE) which follow the causal structure of\ninference with proxies. We show our method is significantly more robust than\nexisting methods, and matches the state-of-the-art on previous benchmarks\nfocused on individual treatment effects.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 15:33:48 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 13:09:24 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Louizos", "Christos", ""], ["Shalit", "Uri", ""], ["Mooij", "Joris", ""], ["Sontag", "David", ""], ["Zemel", "Richard", ""], ["Welling", "Max", ""]]}, {"id": "1705.08826", "submitter": "Yanbo Fan", "authors": "Yanbo Fan, Siwei Lyu, Yiming Ying, Bao-Gang Hu", "title": "Learning with Average Top-k Loss", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce the {\\em average top-$k$} (\\atk) loss as a new\naggregate loss for supervised learning, which is the average over the $k$\nlargest individual losses over a training dataset. We show that the \\atk loss\nis a natural generalization of the two widely used aggregate losses, namely the\naverage loss and the maximum loss, but can combine their advantages and\nmitigate their drawbacks to better adapt to different data distributions.\nFurthermore, it remains a convex function over all individual losses, which can\nlead to convex optimization problems that can be solved effectively with\nconventional gradient-based methods. We provide an intuitive interpretation of\nthe \\atk loss based on its equivalent effect on the continuous individual loss\nfunctions, suggesting that it can reduce the penalty on correctly classified\ndata. We further give a learning theory analysis of \\matk learning on the\nclassification calibration of the \\atk loss and the error bounds of \\atk-SVM.\nWe demonstrate the applicability of minimum average top-$k$ learning for binary\nclassification and regression using synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 15:49:55 GMT"}, {"version": "v2", "created": "Wed, 20 Dec 2017 07:16:13 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Fan", "Yanbo", ""], ["Lyu", "Siwei", ""], ["Ying", "Yiming", ""], ["Hu", "Bao-Gang", ""]]}, {"id": "1705.08841", "submitter": "Diane Bouchacourt", "authors": "Diane Bouchacourt, Ryota Tomioka, Sebastian Nowozin", "title": "Multi-Level Variational Autoencoder: Learning Disentangled\n  Representations from Grouped Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We would like to learn a representation of the data which decomposes an\nobservation into factors of variation which we can independently control.\nSpecifically, we want to use minimal supervision to learn a latent\nrepresentation that reflects the semantics behind a specific grouping of the\ndata, where within a group the samples share a common factor of variation. For\nexample, consider a collection of face images grouped by identity. We wish to\nanchor the semantics of the grouping into a relevant and disentangled\nrepresentation that we can easily exploit. However, existing deep probabilistic\nmodels often assume that the observations are independent and identically\ndistributed. We present the Multi-Level Variational Autoencoder (ML-VAE), a new\ndeep probabilistic model for learning a disentangled representation of a set of\ngrouped observations. The ML-VAE separates the latent representation into\nsemantically meaningful parts by working both at the group level and the\nobservation level, while retaining efficient test-time inference. Quantitative\nand qualitative evaluations show that the ML-VAE model (i) learns a\nsemantically meaningful disentanglement of grouped data, (ii) enables\nmanipulation of the latent representation, and (iii) generalises to unseen\ngroups.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 16:14:54 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Bouchacourt", "Diane", ""], ["Tomioka", "Ryota", ""], ["Nowozin", "Sebastian", ""]]}, {"id": "1705.08848", "submitter": "Nicolas Courty", "authors": "Nicolas Courty, R\\'emi Flamary, Amaury Habrard and Alain Rakotomamonjy", "title": "Joint Distribution Optimal Transportation for Domain Adaptation", "comments": "Accepted for publication at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the unsupervised domain adaptation problem, where one\nwants to estimate a prediction function $f$ in a given target domain without\nany labeled sample by exploiting the knowledge available from a source domain\nwhere labels are known. Our work makes the following assumption: there exists a\nnon-linear transformation between the joint feature/label space distributions\nof the two domain $\\mathcal{P}_s$ and $\\mathcal{P}_t$. We propose a solution of\nthis problem with optimal transport, that allows to recover an estimated target\n$\\mathcal{P}^f_t=(X,f(X))$ by optimizing simultaneously the optimal coupling\nand $f$. We show that our method corresponds to the minimization of a bound on\nthe target error, and provide an efficient algorithmic solution, for which\nconvergence is proved. The versatility of our approach, both in terms of class\nof hypothesis or loss functions is demonstrated with real world classification\nand regression problems, for which we reach or surpass state-of-the-art\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 16:34:41 GMT"}, {"version": "v2", "created": "Sun, 22 Oct 2017 12:16:35 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Courty", "Nicolas", ""], ["Flamary", "R\u00e9mi", ""], ["Habrard", "Amaury", ""], ["Rakotomamonjy", "Alain", ""]]}, {"id": "1705.08850", "submitter": "Abhishek Kumar", "authors": "Abhishek Kumar, Prasanna Sattigeri, P. Thomas Fletcher", "title": "Semi-supervised Learning with GANs: Manifold Invariance with Improved\n  Inference", "comments": "NIPS 2017 accepted version, including appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning methods using Generative Adversarial Networks (GANs)\nhave shown promising empirical success recently. Most of these methods use a\nshared discriminator/classifier which discriminates real examples from fake\nwhile also predicting the class label. Motivated by the ability of the GANs\ngenerator to capture the data manifold well, we propose to estimate the tangent\nspace to the data manifold using GANs and employ it to inject invariances into\nthe classifier. In the process, we propose enhancements over existing methods\nfor learning the inverse mapping (i.e., the encoder) which greatly improves in\nterms of semantic similarity of the reconstructed sample with the input sample.\nWe observe considerable empirical gains in semi-supervised learning over\nbaselines, particularly in the cases when the number of labeled examples is\nlow. We also provide insights into how fake examples influence the\nsemi-supervised learning procedure.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 16:35:37 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 18:34:17 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Kumar", "Abhishek", ""], ["Sattigeri", "Prasanna", ""], ["Fletcher", "P. Thomas", ""]]}, {"id": "1705.08858", "submitter": "Galina Lavrentyeva", "authors": "Galina Lavrentyeva, Sergey Novoselov, Egor Malykh, Alexander Kozlov,\n  Oleg Kudashev and Vadim Shchemelinin", "title": "Audio-replay attack detection countermeasures", "comments": "11 pages, 3 figures, accepted for Specom 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the Speech Technology Center (STC) replay attack\ndetection systems proposed for Automatic Speaker Verification Spoofing and\nCountermeasures Challenge 2017. In this study we focused on comparison of\ndifferent spoofing detection approaches. These were GMM based methods, high\nlevel features extraction with simple classifier and deep learning frameworks.\nExperiments performed on the development and evaluation parts of the challenge\ndataset demonstrated stable efficiency of deep learning approaches in case of\nchanging acoustic conditions. At the same time SVM classifier with high level\nfeatures provided a substantial input in the efficiency of the resulting STC\nsystems according to the fusion systems results.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 16:48:03 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Lavrentyeva", "Galina", ""], ["Novoselov", "Sergey", ""], ["Malykh", "Egor", ""], ["Kozlov", "Alexander", ""], ["Kudashev", "Oleg", ""], ["Shchemelinin", "Vadim", ""]]}, {"id": "1705.08865", "submitter": "Galina Lavrentyeva", "authors": "Galina Lavrentyeva, Sergey Novoselov and Konstantin Simonchik", "title": "Anti-spoofing Methods for Automatic SpeakerVerification System", "comments": "12 pages, 0 figures, published in Springer Communications in Computer\n  and Information Science (CCIS) vol. 661", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Growing interest in automatic speaker verification (ASV)systems has lead to\nsignificant quality improvement of spoofing attackson them. Many research works\nconfirm that despite the low equal er-ror rate (EER) ASV systems are still\nvulnerable to spoofing attacks. Inthis work we overview different acoustic\nfeature spaces and classifiersto determine reliable and robust countermeasures\nagainst spoofing at-tacks. We compared several spoofing detection systems,\npresented so far,on the development and evaluation datasets of the Automatic\nSpeakerVerification Spoofing and Countermeasures (ASVspoof) Challenge\n2015.Experimental results presented in this paper demonstrate that the useof\nmagnitude and phase information combination provides a substantialinput into\nthe efficiency of the spoofing detection systems. Also wavelet-based features\nshow impressive results in terms of equal error rate. Inour overview we compare\nspoofing performance for systems based on dif-ferent classifiers. Comparison\nresults demonstrate that the linear SVMclassifier outperforms the conventional\nGMM approach. However, manyresearchers inspired by the great success of deep\nneural networks (DNN)approaches in the automatic speech recognition, applied\nDNN in thespoofing detection task and obtained quite low EER for known and\nun-known type of spoofing attacks.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 16:58:03 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Lavrentyeva", "Galina", ""], ["Novoselov", "Sergey", ""], ["Simonchik", "Konstantin", ""]]}, {"id": "1705.08868", "submitter": "Aditya Grover", "authors": "Aditya Grover, Manik Dhar, Stefano Ermon", "title": "Flow-GAN: Combining Maximum Likelihood and Adversarial Learning in\n  Generative Models", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial learning of probabilistic models has recently emerged as a\npromising alternative to maximum likelihood. Implicit models such as generative\nadversarial networks (GAN) often generate better samples compared to explicit\nmodels trained by maximum likelihood. Yet, GANs sidestep the characterization\nof an explicit density which makes quantitative evaluations challenging. To\nbridge this gap, we propose Flow-GANs, a generative adversarial network for\nwhich we can perform exact likelihood evaluation, thus supporting both\nadversarial and maximum likelihood training. When trained adversarially,\nFlow-GANs generate high-quality samples but attain extremely poor\nlog-likelihood scores, inferior even to a mixture model memorizing the training\ndata; the opposite is true when trained by maximum likelihood. Results on MNIST\nand CIFAR-10 demonstrate that hybrid training can attain high held-out\nlikelihoods while retaining visual fidelity in the generated samples.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 17:11:25 GMT"}, {"version": "v2", "created": "Wed, 3 Jan 2018 21:47:01 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Grover", "Aditya", ""], ["Dhar", "Manik", ""], ["Ermon", "Stefano", ""]]}, {"id": "1705.08881", "submitter": "Jun Li", "authors": "Jun Li, Yongjun Chen, Lei Cai, Ian Davidson, Shuiwang Ji", "title": "Dense Transformer Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key idea of current deep learning methods for dense prediction is to\napply a model on a regular patch centered on each pixel to make pixel-wise\npredictions. These methods are limited in the sense that the patches are\ndetermined by network architecture instead of learned from data. In this work,\nwe propose the dense transformer networks, which can learn the shapes and sizes\nof patches from data. The dense transformer networks employ an encoder-decoder\narchitecture, and a pair of dense transformer modules are inserted into each of\nthe encoder and decoder paths. The novelty of this work is that we provide\ntechnical solutions for learning the shapes and sizes of patches from data and\nefficiently restoring the spatial correspondence required for dense prediction.\nThe proposed dense transformer modules are differentiable, thus the entire\nnetwork can be trained. We apply the proposed networks on natural and\nbiological image segmentation tasks and show superior performance is achieved\nin comparison to baseline methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 17:50:32 GMT"}, {"version": "v2", "created": "Thu, 8 Jun 2017 00:10:04 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Li", "Jun", ""], ["Chen", "Yongjun", ""], ["Cai", "Lei", ""], ["Davidson", "Ian", ""], ["Ji", "Shuiwang", ""]]}, {"id": "1705.08918", "submitter": "Liang Zhao", "authors": "Liang Zhao, Yang Wang, Yi Yang, Wei Xu", "title": "Unsupervised Learning Layers for Video Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents two unsupervised learning layers (UL layers) for\nlabel-free video analysis: one for fully connected layers, and the other for\nconvolutional ones. The proposed UL layers can play two roles: they can be the\ncost function layer for providing global training signal; meanwhile they can be\nadded to any regular neural network layers for providing local training signals\nand combined with the training signals backpropagated from upper layers for\nextracting both slow and fast changing features at layers of different depths.\nTherefore, the UL layers can be used in either pure unsupervised or\nsemi-supervised settings. Both a closed-form solution and an online learning\nalgorithm for two UL layers are provided. Experiments with unlabeled synthetic\nand real-world videos demonstrated that the neural networks equipped with UL\nlayers and trained with the proposed online learning algorithm can extract\nshape and motion information from video sequences of moving objects. The\nexperiments demonstrated the potential applications of UL layers and online\nlearning algorithm to head orientation estimation and moving object\nlocalization.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 18:22:41 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Zhao", "Liang", ""], ["Wang", "Yang", ""], ["Yang", "Yi", ""], ["Xu", "Wei", ""]]}, {"id": "1705.08921", "submitter": "Efr\\'en Cruz Cort\\'es", "authors": "Efr\\'en Cruz Cort\\'es, Clayton Scott", "title": "Consistent Kernel Density Estimation with Non-Vanishing Bandwidth", "comments": "17 pages, updated abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consistency of the kernel density estimator requires that the kernel\nbandwidth tends to zero as the sample size grows. In this paper we investigate\nthe question of whether consistency is possible when the bandwidth is fixed, if\nwe consider a more general class of weighted KDEs. To answer this question in\nthe affirmative, we introduce the fixed-bandwidth KDE (fbKDE), obtained by\nsolving a quadratic program, and prove that it consistently estimates any\ncontinuous square-integrable density. We also establish rates of convergence\nfor the fbKDE with radial kernels and the box kernel under appropriate\nsmoothness assumptions. Furthermore, in an experimental study we demonstrate\nthat the fbKDE compares favorably to the standard KDE and the previously\nproposed variable bandwidth KDE.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 18:30:45 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 16:53:22 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Cort\u00e9s", "Efr\u00e9n Cruz", ""], ["Scott", "Clayton", ""]]}, {"id": "1705.08922", "submitter": "Huizi Mao", "authors": "Huizi Mao, Song Han, Jeff Pool, Wenshuo Li, Xingyu Liu, Yu Wang,\n  William J. Dally", "title": "Exploring the Regularity of Sparse Structure in Convolutional Neural\n  Networks", "comments": "submitted to NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sparsity helps reduce the computational complexity of deep neural networks by\nskipping zeros. Taking advantage of sparsity is listed as a high priority in\nnext generation DNN accelerators such as TPU. The structure of sparsity, i.e.,\nthe granularity of pruning, affects the efficiency of hardware accelerator\ndesign as well as the prediction accuracy. Coarse-grained pruning creates\nregular sparsity patterns, making it more amenable for hardware acceleration\nbut more challenging to maintain the same accuracy. In this paper we\nquantitatively measure the trade-off between sparsity regularity and prediction\naccuracy, providing insights in how to maintain accuracy while having more a\nmore structured sparsity pattern. Our experimental results show that\ncoarse-grained pruning can achieve a sparsity ratio similar to unstructured\npruning without loss of accuracy. Moreover, due to the index saving effect,\ncoarse-grained pruning is able to obtain a better compression ratio than\nfine-grained sparsity at the same accuracy threshold. Based on the recent\nsparse convolutional neural network accelerator (SCNN), our experiments further\ndemonstrate that coarse-grained sparsity saves about 2x the memory references\ncompared to fine-grained sparsity. Since memory reference is more than two\norders of magnitude more expensive than arithmetic operations, the regularity\nof sparse structure leads to more efficient hardware design.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 18:35:41 GMT"}, {"version": "v2", "created": "Thu, 1 Jun 2017 20:34:06 GMT"}, {"version": "v3", "created": "Mon, 5 Jun 2017 00:22:25 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Mao", "Huizi", ""], ["Han", "Song", ""], ["Pool", "Jeff", ""], ["Li", "Wenshuo", ""], ["Liu", "Xingyu", ""], ["Wang", "Yu", ""], ["Dally", "William J.", ""]]}, {"id": "1705.08931", "submitter": "Jaan Altosaar", "authors": "Jaan Altosaar, Rajesh Ranganath, David M. Blei", "title": "Proximity Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference is a powerful approach for approximate posterior\ninference. However, it is sensitive to initialization and can be subject to\npoor local optima. In this paper, we develop proximity variational inference\n(PVI). PVI is a new method for optimizing the variational objective that\nconstrains subsequent iterates of the variational parameters to robustify the\noptimization path. Consequently, PVI is less sensitive to initialization and\noptimization quirks and finds better local optima. We demonstrate our method on\nthree proximity statistics. We study PVI on a Bernoulli factor model and\nsigmoid belief network with both real and synthetic data and compare to\ndeterministic annealing (Katahira et al., 2008). We highlight the flexibility\nof PVI by designing a proximity statistic for Bayesian deep learning models\nsuch as the variational autoencoder (Kingma and Welling, 2014; Rezende et al.,\n2014). Empirically, we show that PVI consistently finds better local optima and\ngives better predictive performance.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 19:06:14 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Altosaar", "Jaan", ""], ["Ranganath", "Rajesh", ""], ["Blei", "David M.", ""]]}, {"id": "1705.08933", "submitter": "Hugh Salimbeni", "authors": "Hugh Salimbeni, Marc Deisenroth", "title": "Doubly Stochastic Variational Inference for Deep Gaussian Processes", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs) are a good choice for function approximation as they\nare flexible, robust to over-fitting, and provide well-calibrated predictive\nuncertainty. Deep Gaussian processes (DGPs) are multi-layer generalisations of\nGPs, but inference in these models has proved challenging. Existing approaches\nto inference in DGP models assume approximate posteriors that force\nindependence between the layers, and do not work well in practice. We present a\ndoubly stochastic variational inference algorithm, which does not force\nindependence between layers. With our method of inference we demonstrate that a\nDGP model can be used effectively on data ranging in size from hundreds to a\nbillion points. We provide strong empirical evidence that our inference scheme\nfor DGPs works well in practice in both classification and regression.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 19:10:17 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 15:18:25 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Salimbeni", "Hugh", ""], ["Deisenroth", "Marc", ""]]}, {"id": "1705.08948", "submitter": "Praneeth Narayanamurthy", "authors": "Praneeth Narayanamurthy and Namrata Vaswani", "title": "Provable Dynamic Robust PCA or Robust Subspace Tracking", "comments": "Minor writing edits. The paper has been accepted to IEEE Transactions\n  on Information Theory", "journal-ref": "IEEE Transactions on Information Theory (Volume: 65 , Issue: 3 ,\n  March 2019)", "doi": "10.1109/TIT.2018.2872023", "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic robust PCA refers to the dynamic (time-varying) extension of robust\nPCA (RPCA). It assumes that the true (uncorrupted) data lies in a\nlow-dimensional subspace that can change with time, albeit slowly. The goal is\nto track this changing subspace over time in the presence of sparse outliers.\nWe develop and study a novel algorithm, that we call simple-ReProCS, based on\nthe recently introduced Recursive Projected Compressive Sensing (ReProCS)\nframework. Our work provides the first guarantee for dynamic RPCA that holds\nunder weakened versions of standard RPCA assumptions, slow subspace change and\na lower bound assumption on most outlier magnitudes. Our result is significant\nbecause (i) it removes the strong assumptions needed by the two previous\ncomplete guarantees for ReProCS-based algorithms; (ii) it shows that it is\npossible to achieve significantly improved outlier tolerance, compared with all\nexisting RPCA or dynamic RPCA solutions by exploiting the above two simple\nextra assumptions; and (iii) it proves that simple-ReProCS is online (after\ninitialization), fast, and, has near-optimal memory complexity.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 19:55:51 GMT"}, {"version": "v2", "created": "Sun, 22 Oct 2017 05:10:46 GMT"}, {"version": "v3", "created": "Fri, 25 May 2018 21:06:00 GMT"}, {"version": "v4", "created": "Wed, 19 Sep 2018 17:43:44 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Narayanamurthy", "Praneeth", ""], ["Vaswani", "Namrata", ""]]}, {"id": "1705.08982", "submitter": "Shuai Xiao", "authors": "Shuai Xiao, Junchi Yan, Stephen M. Chu, Xiaokang Yang, Hongyuan Zha", "title": "Modeling The Intensity Function Of Point Process Via Recurrent Neural\n  Networks", "comments": "Accepted at Thirty-First AAAI Conference on Artificial Intelligence\n  (AAAI17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event sequence, asynchronously generated with random timestamp, is ubiquitous\namong applications. The precise and arbitrary timestamp can carry important\nclues about the underlying dynamics, and has lent the event data fundamentally\ndifferent from the time-series whereby series is indexed with fixed and equal\ntime interval. One expressive mathematical tool for modeling event is point\nprocess. The intensity functions of many point processes involve two\ncomponents: the background and the effect by the history. Due to its inherent\nspontaneousness, the background can be treated as a time series while the other\nneed to handle the history events. In this paper, we model the background by a\nRecurrent Neural Network (RNN) with its units aligned with time series indexes\nwhile the history effect is modeled by another RNN whose units are aligned with\nasynchronous events to capture the long-range dynamics. The whole model with\nevent type and timestamp prediction output layers can be trained end-to-end.\nOur approach takes an RNN perspective to point process, and models its\nbackground and history effect. For utility, our method allows a black-box\ntreatment for modeling the intensity which is often a pre-defined parametric\nform in point processes. Meanwhile end-to-end training opens the venue for\nreusing existing rich techniques in deep network for point process modeling. We\napply our model to the predictive maintenance problem using a log dataset by\nmore than 1000 ATMs from a global bank headquartered in North America.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 22:23:14 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Xiao", "Shuai", ""], ["Yan", "Junchi", ""], ["Chu", "Stephen M.", ""], ["Yang", "Xiaokang", ""], ["Zha", "Hongyuan", ""]]}, {"id": "1705.08991", "submitter": "Shuang Liu", "authors": "Shuang Liu, Olivier Bousquet, Kamalika Chaudhuri", "title": "Approximation and Convergence Properties of Generative Adversarial\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GAN) approximate a target data distribution\nby jointly optimizing an objective function through a \"two-player game\" between\na generator and a discriminator. Despite their empirical success, however, two\nvery basic questions on how well they can approximate the target distribution\nremain unanswered. First, it is not known how restricting the discriminator\nfamily affects the approximation quality. Second, while a number of different\nobjective functions have been proposed, we do not understand when convergence\nto the global minima of the objective function leads to convergence to the\ntarget distribution under various notions of distributional convergence.\n  In this paper, we address these questions in a broad and unified setting by\ndefining a notion of adversarial divergences that includes a number of recently\nproposed objective functions. We show that if the objective function is an\nadversarial divergence with some additional conditions, then using a restricted\ndiscriminator family has a moment-matching effect. Additionally, we show that\nfor objective functions that are strict adversarial divergences, convergence in\nthe objective function implies weak convergence, thus generalizing previous\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 22:53:22 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Liu", "Shuang", ""], ["Bousquet", "Olivier", ""], ["Chaudhuri", "Kamalika", ""]]}, {"id": "1705.08997", "submitter": "Farhan Tejani", "authors": "Himanshu Sahni, Saurabh Kumar, Farhan Tejani, Yannick Schroecker,\n  Charles Isbell", "title": "State Space Decomposition and Subgoal Creation for Transfer in Deep\n  Reinforcement Learning", "comments": "5 pages, 6 figures; 3rd Multidisciplinary Conference on Reinforcement\n  Learning and Decision Making (RLDM 2017), Ann Arbor, Michigan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical reinforcement learning (RL) agents learn to complete tasks specified\nby reward functions tailored to their domain. As such, the policies they learn\ndo not generalize even to similar domains. To address this issue, we develop a\nframework through which a deep RL agent learns to generalize policies from\nsmaller, simpler domains to more complex ones using a recurrent attention\nmechanism. The task is presented to the agent as an image and an instruction\nspecifying the goal. This meta-controller guides the agent towards its goal by\ndesigning a sequence of smaller subtasks on the part of the state space within\nthe attention, effectively decomposing it. As a baseline, we consider a setup\nwithout attention as well. Our experiments show that the meta-controller learns\nto create subgoals within the attention.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 23:19:44 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Sahni", "Himanshu", ""], ["Kumar", "Saurabh", ""], ["Tejani", "Farhan", ""], ["Schroecker", "Yannick", ""], ["Isbell", "Charles", ""]]}, {"id": "1705.09026", "submitter": "Bert Huang", "authors": "Walid Chaabene and Bert Huang", "title": "Best-Choice Edge Grafting for Efficient Structure Learning of Markov\n  Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incremental methods for structure learning of pairwise Markov random fields\n(MRFs), such as grafting, improve scalability by avoiding inference over the\nentire feature space in each optimization step. Instead, inference is performed\nover an incrementally grown active set of features. In this paper, we address\nkey computational bottlenecks that current incremental techniques still suffer\nby introducing best-choice edge grafting, an incremental, structured method\nthat activates edges as groups of features in a streaming setting. The method\nuses a reservoir of edges that satisfy an activation condition, approximating\nthe search for the optimal edge to activate. It also reorganizes the search\nspace using search-history and structure heuristics. Experiments show a\nsignificant speedup for structure learning and a controllable trade-off between\nthe speed and quality of learning.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 02:28:53 GMT"}, {"version": "v2", "created": "Sun, 20 May 2018 01:28:19 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Chaabene", "Walid", ""], ["Huang", "Bert", ""]]}, {"id": "1705.09031", "submitter": "Eric Strobl", "authors": "Eric V. Strobl, Shyam Visweswaran, Peter L. Spirtes", "title": "Fast Causal Inference with Non-Random Missingness by Test-Wise Deletion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real datasets contain values missing not at random (MNAR). In this\nscenario, investigators often perform list-wise deletion, or delete samples\nwith any missing values, before applying causal discovery algorithms. List-wise\ndeletion is a sound and general strategy when paired with algorithms such as\nFCI and RFCI, but the deletion procedure also eliminates otherwise good samples\nthat contain only a few missing values. In this report, we show that we can\nmore efficiently utilize the observed values with test-wise deletion while\nstill maintaining algorithmic soundness. Here, test-wise deletion refers to the\nprocess of list-wise deleting samples only among the variables required for\neach conditional independence (CI) test used in constraint-based searches.\nTest-wise deletion therefore often saves more samples than list-wise deletion\nfor each CI test, especially when we have a sparse underlying graph. Our\ntheoretical results show that test-wise deletion is sound under the justifiable\nassumption that none of the missingness mechanisms causally affect each other\nin the underlying causal graph. We also find that FCI and RFCI with test-wise\ndeletion outperform their list-wise deletion and imputation counterparts on\naverage when MNAR holds in both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 02:52:05 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Strobl", "Eric V.", ""], ["Visweswaran", "Shyam", ""], ["Spirtes", "Peter L.", ""]]}, {"id": "1705.09036", "submitter": "Oliver Hennigh", "authors": "Oliver Hennigh", "title": "Lat-Net: Compressing Lattice Boltzmann Flow Simulations using Deep\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational Fluid Dynamics (CFD) is a hugely important subject with\napplications in almost every engineering field, however, fluid simulations are\nextremely computationally and memory demanding. Towards this end, we present\nLat-Net, a method for compressing both the computation time and memory usage of\nLattice Boltzmann flow simulations using deep neural networks. Lat-Net employs\nconvolutional autoencoders and residual connections in a fully differentiable\nscheme to compress the state size of a simulation and learn the dynamics on\nthis compressed form. The result is a computationally and memory efficient\nneural network that can be iterated and queried to reproduce a fluid\nsimulation. We show that once Lat-Net is trained, it can generalize to large\ngrid sizes and complex geometries while maintaining accuracy. We also show that\nLat-Net is a general method for compressing other Lattice Boltzmann based\nsimulations such as Electromagnetism.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 03:33:23 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Hennigh", "Oliver", ""]]}, {"id": "1705.09046", "submitter": "Futoshi Futami", "authors": "Futoshi Futami, Issei Sato, Masashi Sugiyama", "title": "Expectation Propagation for t-Exponential Family Using Q-Algebra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exponential family distributions are highly useful in machine learning since\ntheir calculation can be performed efficiently through natural parameters. The\nexponential family has recently been extended to the t-exponential family,\nwhich contains Student-t distributions as family members and thus allows us to\nhandle noisy data well. However, since the t-exponential family is denied by\nthe deformed exponential, we cannot derive an efficient learning algorithm for\nthe t-exponential family such as expectation propagation (EP). In this paper,\nwe borrow the mathematical tools of q-algebra from statistical physics and show\nthat the pseudo additivity of distributions allows us to perform calculation of\nt-exponential family distributions through natural parameters. We then develop\nan expectation propagation (EP) algorithm for the t-exponential family, which\nprovides a deterministic approximation to the posterior or predictive\ndistribution with simple moment matching. We finally apply the proposed EP\nalgorithm to the Bayes point machine and Student-t process classication, and\ndemonstrate their performance numerically.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 04:56:09 GMT"}, {"version": "v2", "created": "Sun, 28 May 2017 13:20:04 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Futami", "Futoshi", ""], ["Sato", "Issei", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1705.09048", "submitter": "Xiang Cheng", "authors": "Xiang Cheng and Peter Bartlett", "title": "Convergence of Langevin MCMC in KL-divergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Langevin diffusion is a commonly used tool for sampling from a given\ndistribution. In this work, we establish that when the target density $p^*$ is\nsuch that $\\log p^*$ is $L$ smooth and $m$ strongly convex, discrete Langevin\ndiffusion produces a distribution $p$ with $KL(p||p^*)\\leq \\epsilon$ in\n$\\tilde{O}(\\frac{d}{\\epsilon})$ steps, where $d$ is the dimension of the sample\nspace. We also study the convergence rate when the strong-convexity assumption\nis absent. By considering the Langevin diffusion as a gradient flow in the\nspace of probability distributions, we obtain an elegant analysis that applies\nto the stronger property of convergence in KL-divergence and gives a\nconceptually simpler proof of the best-known convergence results in weaker\nmetrics.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 05:08:26 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 03:37:58 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Cheng", "Xiang", ""], ["Bartlett", "Peter", ""]]}, {"id": "1705.09056", "submitter": "Xiangru Lian", "authors": "Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, Ji Liu", "title": "Can Decentralized Algorithms Outperform Centralized Algorithms? A Case\n  Study for Decentralized Parallel Stochastic Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most distributed machine learning systems nowadays, including TensorFlow and\nCNTK, are built in a centralized fashion. One bottleneck of centralized\nalgorithms lies on high communication cost on the central node. Motivated by\nthis, we ask, can decentralized algorithms be faster than its centralized\ncounterpart?\n  Although decentralized PSGD (D-PSGD) algorithms have been studied by the\ncontrol community, existing analysis and theory do not show any advantage over\ncentralized PSGD (C-PSGD) algorithms, simply assuming the application scenario\nwhere only the decentralized network is available. In this paper, we study a\nD-PSGD algorithm and provide the first theoretical analysis that indicates a\nregime in which decentralized algorithms might outperform centralized\nalgorithms for distributed stochastic gradient descent. This is because D-PSGD\nhas comparable total computational complexities to C-PSGD but requires much\nless communication cost on the busiest node. We further conduct an empirical\nstudy to validate our theoretical analysis across multiple frameworks (CNTK and\nTorch), different network configurations, and computation platforms up to 112\nGPUs. On network configurations with low bandwidth or high latency, D-PSGD can\nbe up to one order of magnitude faster than its well-optimized centralized\ncounterparts.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 05:58:17 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 00:22:51 GMT"}, {"version": "v3", "created": "Sat, 10 Jun 2017 05:08:21 GMT"}, {"version": "v4", "created": "Fri, 21 Jul 2017 13:50:26 GMT"}, {"version": "v5", "created": "Mon, 11 Sep 2017 04:21:43 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Lian", "Xiangru", ""], ["Zhang", "Ce", ""], ["Zhang", "Huan", ""], ["Hsieh", "Cho-Jui", ""], ["Zhang", "Wei", ""], ["Liu", "Ji", ""]]}, {"id": "1705.09185", "submitter": "Timur Pekhovsky", "authors": "Timur Pekhovsky, Maxim Korenevsky", "title": "Investigation of Using VAE for i-Vector Speaker Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New system for i-vector speaker recognition based on variational autoencoder\n(VAE) is investigated. VAE is a promising approach for developing accurate deep\nnonlinear generative models of complex data. Experiments show that VAE provides\nspeaker embedding and can be effectively trained in an unsupervised manner. LLR\nestimate for VAE is developed. Experiments on NIST SRE 2010 data demonstrate\nits correctness. Additionally, we show that the performance of VAE-based system\nin the i-vectors space is close to that of the diagonal PLDA. Several\ninteresting results are also observed in the experiments with $\\beta$-VAE. In\nparticular, we found that for $\\beta\\ll 1$, VAE can be trained to capture the\nfeatures of complex input data distributions in an effective way, which is hard\nto obtain in the standard VAE ($\\beta=1$).\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 13:59:18 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Pekhovsky", "Timur", ""], ["Korenevsky", "Maxim", ""]]}, {"id": "1705.09199", "submitter": "Ambrish Rawat", "authors": "Mathieu Sinn, Ambrish Rawat", "title": "Non-parametric estimation of Jensen-Shannon Divergence in Generative\n  Adversarial Network training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have become a widely popular framework\nfor generative modelling of high-dimensional datasets. However their training\nis well-known to be difficult. This work presents a rigorous statistical\nanalysis of GANs providing straight-forward explanations for common training\npathologies such as vanishing gradients. Furthermore, it proposes a new\ntraining objective, Kernel GANs, and demonstrates its practical effectiveness\non large-scale real-world data sets. A key element in the analysis is the\ndistinction between training with respect to the (unknown) data distribution,\nand its empirical counterpart. To overcome issues in GAN training, we pursue\nthe idea of smoothing the Jensen-Shannon Divergence (JSD) by incorporating\nnoise in the input distributions of the discriminator. As we show, this\neffectively leads to an empirical version of the JSD in which the true and the\ngenerator densities are replaced by kernel density estimates, which leads to\nKernel GANs.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 14:32:26 GMT"}, {"version": "v2", "created": "Tue, 29 Aug 2017 09:44:41 GMT"}, {"version": "v3", "created": "Sun, 15 Oct 2017 18:52:24 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Sinn", "Mathieu", ""], ["Rawat", "Ambrish", ""]]}, {"id": "1705.09236", "submitter": "Kirthevasan Kandasamy", "authors": "Kirthevasan Kandasamy, Akshay Krishnamurthy, Jeff Schneider and\n  Barnabas Poczos", "title": "Asynchronous Parallel Bayesian Optimisation via Thompson Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design and analyse variations of the classical Thompson sampling (TS)\nprocedure for Bayesian optimisation (BO) in settings where function evaluations\nare expensive, but can be performed in parallel. Our theoretical analysis shows\nthat a direct application of the sequential Thompson sampling algorithm in\neither synchronous or asynchronous parallel settings yields a surprisingly\npowerful result: making $n$ evaluations distributed among $M$ workers is\nessentially equivalent to performing $n$ evaluations in sequence. Further, by\nmodeling the time taken to complete a function evaluation, we show that, under\na time constraint, asynchronously parallel TS achieves asymptotically lower\nregret than both the synchronous and sequential versions. These results are\ncomplemented by an experimental analysis, showing that asynchronous TS\noutperforms a suite of existing parallel BO algorithms in simulations and in a\nhyper-parameter tuning application in convolutional neural networks. In\naddition to these, the proposed procedure is conceptually and computationally\nmuch simpler than existing work for parallel BO.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 15:46:23 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Kandasamy", "Kirthevasan", ""], ["Krishnamurthy", "Akshay", ""], ["Schneider", "Jeff", ""], ["Poczos", "Barnabas", ""]]}, {"id": "1705.09279", "submitter": "Chris J. Maddison", "authors": "Chris J. Maddison, Dieterich Lawson, George Tucker, Nicolas Heess,\n  Mohammad Norouzi, Andriy Mnih, Arnaud Doucet, Yee Whye Teh", "title": "Filtering Variational Objectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When used as a surrogate objective for maximum likelihood estimation in\nlatent variable models, the evidence lower bound (ELBO) produces\nstate-of-the-art results. Inspired by this, we consider the extension of the\nELBO to a family of lower bounds defined by a particle filter's estimator of\nthe marginal likelihood, the filtering variational objectives (FIVOs). FIVOs\ntake the same arguments as the ELBO, but can exploit a model's sequential\nstructure to form tighter bounds. We present results that relate the tightness\nof FIVO's bound to the variance of the particle filter's estimator by\nconsidering the generic case of bounds defined as log-transformed likelihood\nestimators. Experimentally, we show that training with FIVO results in\nsubstantial improvements over training the same model architecture with the\nELBO on sequential data.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 17:52:41 GMT"}, {"version": "v2", "created": "Mon, 28 Aug 2017 17:58:51 GMT"}, {"version": "v3", "created": "Sun, 12 Nov 2017 20:38:13 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Maddison", "Chris J.", ""], ["Lawson", "Dieterich", ""], ["Tucker", "George", ""], ["Heess", "Nicolas", ""], ["Norouzi", "Mohammad", ""], ["Mnih", "Andriy", ""], ["Doucet", "Arnaud", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1705.09280", "submitter": "Suriya Gunasekar", "authors": "Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam\n  Neyshabur, Nathan Srebro", "title": "Implicit Regularization in Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study implicit regularization when optimizing an underdetermined quadratic\nobjective over a matrix $X$ with gradient descent on a factorization of $X$. We\nconjecture and provide empirical and theoretical evidence that with small\nenough step sizes and initialization close enough to the origin, gradient\ndescent on a full dimensional factorization converges to the minimum nuclear\nnorm solution.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 17:55:24 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Gunasekar", "Suriya", ""], ["Woodworth", "Blake", ""], ["Bhojanapalli", "Srinadh", ""], ["Neyshabur", "Behnam", ""], ["Srebro", "Nathan", ""]]}, {"id": "1705.09283", "submitter": "Peng Jiao", "authors": "Lei Deng, Peng Jiao, Jing Pei, Zhenzhi Wu and Guoqi Li", "title": "GXNOR-Net: Training deep neural networks with ternary weights and\n  activations without full-precision memory under a unified discretization\n  framework", "comments": "11 pages, 13 figures", "journal-ref": "Neural Networks(Volume 100,April 2018)", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a pressing need to build an architecture that could subsume these\nnetworks under a unified framework that achieves both higher performance and\nless overhead. To this end, two fundamental issues are yet to be addressed. The\nfirst one is how to implement the back propagation when neuronal activations\nare discrete. The second one is how to remove the full-precision hidden weights\nin the training phase to break the bottlenecks of memory/computation\nconsumption. To address the first issue, we present a multi-step neuronal\nactivation discretization method and a derivative approximation technique that\nenable the implementing the back propagation algorithm on discrete DNNs. While\nfor the second issue, we propose a discrete state transition (DST) methodology\nto constrain the weights in a discrete space without saving the hidden weights.\nThrough this way, we build a unified framework that subsumes the binary or\nternary networks as its special cases, and under which a heuristic algorithm is\nprovided at the website https://github.com/AcrossV/Gated-XNOR. More\nparticularly, we find that when both the weights and activations become ternary\nvalues, the DNNs can be reduced to sparse binary networks, termed as gated XNOR\nnetworks (GXNOR-Nets) since only the event of non-zero weight and non-zero\nactivation enables the control gate to start the XNOR logic operations in the\noriginal binary networks. This promises the event-driven hardware design for\nefficient mobile intelligence. We achieve advanced performance compared with\nstate-of-the-art algorithms. Furthermore, the computational sparsity and the\nnumber of states in the discrete space can be flexibly modified to make it\nsuitable for various hardware platforms.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 17:59:41 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 03:01:18 GMT"}, {"version": "v3", "created": "Fri, 18 Aug 2017 07:43:44 GMT"}, {"version": "v4", "created": "Mon, 9 Apr 2018 05:25:13 GMT"}, {"version": "v5", "created": "Wed, 2 May 2018 17:30:40 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Deng", "Lei", ""], ["Jiao", "Peng", ""], ["Pei", "Jing", ""], ["Wu", "Zhenzhi", ""], ["Li", "Guoqi", ""]]}, {"id": "1705.09296", "submitter": "Dallas Card", "authors": "Dallas Card and Chenhao Tan and Noah A. Smith", "title": "Neural Models for Documents with Metadata", "comments": "13 pages, 3 figures, 6 tables; updating to version published at ACL\n  2018", "journal-ref": "Dallas Card, Chenhao Tan, and Noah A. Smith. (2018). Neural Models\n  for Documents with Metadata. In Proceedings of the 56th Annual Meeting of the\n  Association for Computational Linguistics (Volume 1: Long Papers)", "doi": "10.18653/v1/P18-1189", "report-no": null, "categories": "stat.ML cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most real-world document collections involve various types of metadata, such\nas author, source, and date, and yet the most commonly-used approaches to\nmodeling text corpora ignore this information. While specialized models have\nbeen developed for particular applications, few are widely used in practice, as\ncustomization typically requires derivation of a custom inference algorithm. In\nthis paper, we build on recent advances in variational inference methods and\npropose a general neural framework, based on topic models, to enable flexible\nincorporation of metadata and allow for rapid exploration of alternative\nmodels. Our approach achieves strong performance, with a manageable tradeoff\nbetween perplexity, coherence, and sparsity. Finally, we demonstrate the\npotential of our framework through an exploration of a corpus of articles about\nUS immigration.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 18:00:03 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 20:26:37 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Card", "Dallas", ""], ["Tan", "Chenhao", ""], ["Smith", "Noah A.", ""]]}, {"id": "1705.09303", "submitter": "Matt Feiszli", "authors": "Matt Feiszli", "title": "Latent Geometry and Memorization in Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It can be difficult to tell whether a trained generative model has learned to\ngenerate novel examples or has simply memorized a specific set of outputs. In\npublished work, it is common to attempt to address this visually, for example\nby displaying a generated example and its nearest neighbor(s) in the training\nset (in, for example, the L2 metric). As any generative model induces a\nprobability density on its output domain, we propose studying this density\ndirectly. We first study the geometry of the latent representation and\ngenerator, relate this to the output density, and then develop techniques to\ncompute and inspect the output density. As an application, we demonstrate that\n\"memorization\" tends to a density made of delta functions concentrated on the\nmemorized examples. We note that without first understanding the geometry, the\nmeasurement would be essentially impossible to make.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 18:00:19 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Feiszli", "Matt", ""]]}, {"id": "1705.09319", "submitter": "L\\'eon Bottou", "authors": "Jean Lafond, Nicolas Vasilache, L\\'eon Bottou", "title": "Diagonal Rescaling For Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a second-order neural network stochastic gradient training\nalgorithm whose block-diagonal structure effectively amounts to normalizing the\nunit activations. Investigating why this algorithm lacks in robustness then\nreveals two interesting insights. The first insight suggests a new way to scale\nthe stepsizes, clarifying popular algorithms such as RMSProp as well as old\nneural network tricks such as fanin stepsize scaling. The second insight\nstresses the practical importance of dealing with fast changes of the curvature\nof the cost.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 18:33:24 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Lafond", "Jean", ""], ["Vasilache", "Nicolas", ""], ["Bottou", "L\u00e9on", ""]]}, {"id": "1705.09339", "submitter": "Senthil Yogamani", "authors": "B Ravi Kiran, Arindam Das and Senthil Yogamani", "title": "Rejection-Cascade of Gaussians: Real-time adaptive background\n  subtraction framework", "comments": "Accepted for National Conference on Computer Vision, Pattern\n  Recognition, Image Processing and Graphics (NCVPRIPG 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background-Foreground classification is a well-studied problem in computer\nvision. Due to the pixel-wise nature of modeling and processing in the\nalgorithm, it is usually difficult to satisfy real-time constraints. There is a\ntrade-off between the speed (because of model complexity) and accuracy.\nInspired by the rejection cascade of Viola-Jones classifier, we decompose the\nGaussian Mixture Model (GMM) into an adaptive cascade of Gaussians(CoG). We\nachieve a good improvement in speed without compromising the accuracy with\nrespect to the baseline GMM model. We demonstrate a speed-up factor of 4-5x and\n17 percent average improvement in accuracy over Wallflowers surveillance\ndatasets. The CoG is then demonstrated to over the latent space representation\nof images of a convolutional variational autoencoder(VAE). We provide initial\nresults over CDW-2014 dataset, which could speed up background subtraction for\ndeep architectures.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 19:50:45 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2019 16:51:04 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Kiran", "B Ravi", ""], ["Das", "Arindam", ""], ["Yogamani", "Senthil", ""]]}, {"id": "1705.09353", "submitter": "Carlton Macdonald Downey", "authors": "Carlton Downey, Ahmed Hefny, Boyue Li, Byron Boots, Geoffrey Gordon", "title": "Predictive State Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new model, Predictive State Recurrent Neural Networks (PSRNNs),\nfor filtering and prediction in dynamical systems. PSRNNs draw on insights from\nboth Recurrent Neural Networks (RNNs) and Predictive State Representations\n(PSRs), and inherit advantages from both types of models. Like many successful\nRNN architectures, PSRNNs use (potentially deeply composed) bilinear transfer\nfunctions to combine information from multiple sources. We show that such\nbilinear functions arise naturally from state updates in Bayes filters like\nPSRs, in which observations can be viewed as gating belief states. We also show\nthat PSRNNs can be learned effectively by combining Backpropogation Through\nTime (BPTT) with an initialization derived from a statistically consistent\nlearning algorithm for PSRs called two-stage regression (2SR). Finally, we show\nthat PSRNNs can be factorized using tensor decomposition, reducing model size\nand suggesting interesting connections to existing multiplicative architectures\nsuch as LSTMs. We applied PSRNNs to 4 datasets, and showed that we outperform\nseveral popular alternative approaches to modeling dynamical systems in all\ncases.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 20:40:13 GMT"}, {"version": "v2", "created": "Sun, 18 Jun 2017 00:01:46 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Downey", "Carlton", ""], ["Hefny", "Ahmed", ""], ["Li", "Boyue", ""], ["Boots", "Byron", ""], ["Gordon", "Geoffrey", ""]]}, {"id": "1705.09367", "submitter": "Kevin Roth", "authors": "Kevin Roth, Aurelien Lucchi, Sebastian Nowozin, Thomas Hofmann", "title": "Stabilizing Training of Generative Adversarial Networks through\n  Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models based on Generative Adversarial Networks (GANs) have\ndemonstrated impressive sample quality but in order to work they require a\ncareful choice of architecture, parameter initialization, and selection of\nhyper-parameters. This fragility is in part due to a dimensional mismatch or\nnon-overlapping support between the model distribution and the data\ndistribution, causing their density ratio and the associated f-divergence to be\nundefined. We overcome this fundamental limitation and propose a new\nregularization approach with low computational cost that yields a stable GAN\ntraining procedure. We demonstrate the effectiveness of this regularizer across\nseveral architectures trained on common benchmark image generation tasks. Our\nregularization turns GAN models into reliable building blocks for deep\nlearning.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 21:17:50 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 11:17:04 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Roth", "Kevin", ""], ["Lucchi", "Aurelien", ""], ["Nowozin", "Sebastian", ""], ["Hofmann", "Thomas", ""]]}, {"id": "1705.09407", "submitter": "Giuseppe Nuti", "authors": "Giuseppe Nuti", "title": "An Efficient Algorithm for Bayesian Nearest Neighbours", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  K-Nearest Neighbours (k-NN) is a popular classification and regression\nalgorithm, yet one of its main limitations is the difficulty in choosing the\nnumber of neighbours. We present a Bayesian algorithm to compute the posterior\nprobability distribution for k given a target point within a data-set,\nefficiently and without the use of Markov Chain Monte Carlo (MCMC) methods or\nsimulation - alongside an exact solution for distributions within the\nexponential family. The central idea is that data points around our target are\ngenerated by the same probability distribution, extending outwards over the\nappropriate, though unknown, number of neighbours. Once the data is projected\nonto a distance metric of choice, we can transform the choice of k into a\nchange-point detection problem, for which there is an efficient solution: we\nrecursively compute the probability of the last change-point as we move towards\nour target, and thus de facto compute the posterior probability distribution\nover k. Applying this approach to both a classification and a regression UCI\ndata-sets, we compare favourably and, most importantly, by removing the need\nfor simulation, we are able to compute the posterior probability of k exactly\nand rapidly. As an example, the computational time for the Ripley data-set is a\nfew milliseconds compared to a few hours when using a MCMC approach.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 01:36:15 GMT"}, {"version": "v2", "created": "Fri, 2 Jun 2017 02:29:13 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Nuti", "Giuseppe", ""]]}, {"id": "1705.09416", "submitter": "Huahui Liu", "authors": "Huahui Liu, Mingrui Zhu, Xiaonan Meng, Yi Hu and Hao Wang", "title": "Dual Based DSP Bidding Strategy and its Application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, RTB(Real Time Bidding) becomes a popular online\nadvertisement trading method. During the auction, each DSP(Demand Side\nPlatform) is supposed to evaluate current opportunity and respond with an ad\nand corresponding bid price. It's essential for DSP to find an optimal ad\nselection and bid price determination strategy which maximizes revenue or\nperformance under budget and ROI(Return On Investment) constraints in P4P(Pay\nFor Performance) or P4U(Pay For Usage) mode. We solve this problem by 1)\nformalizing the DSP problem as a constrained optimization problem, 2) proposing\nthe augmented MMKP(Multi-choice Multi-dimensional Knapsack Problem) with\ngeneral solution, 3) and demonstrating the DSP problem is a special case of the\naugmented MMKP and deriving specialized strategy. Our strategy is verified\nthrough simulation and outperforms state-of-the-art strategies in real\napplication. To the best of our knowledge, our solution is the first dual based\nDSP bidding framework that is derived from strict second price auction\nassumption and generally applicable to the multiple ads scenario with various\nobjectives and constraints.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 02:43:08 GMT"}, {"version": "v2", "created": "Wed, 27 Dec 2017 11:10:13 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Liu", "Huahui", ""], ["Zhu", "Mingrui", ""], ["Meng", "Xiaonan", ""], ["Hu", "Yi", ""], ["Wang", "Hao", ""]]}, {"id": "1705.09552", "submitter": "Seyed-Mohsen Moosavi-Dezfooli", "authors": "Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard,\n  Stefano Soatto", "title": "Classification regions of deep neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to analyze the geometric properties of deep neural\nnetwork classifiers in the input space. We specifically study the topology of\nclassification regions created by deep networks, as well as their associated\ndecision boundary. Through a systematic empirical investigation, we show that\nstate-of-the-art deep nets learn connected classification regions, and that the\ndecision boundary in the vicinity of datapoints is flat along most directions.\nWe further draw an essential connection between two seemingly unrelated\nproperties of deep networks: their sensitivity to additive perturbations in the\ninputs, and the curvature of their decision boundary. The directions where the\ndecision boundary is curved in fact remarkably characterize the directions to\nwhich the classifier is the most vulnerable. We finally leverage a fundamental\nasymmetry in the curvature of the decision boundary of deep nets, and propose a\nmethod to discriminate between original images, and images perturbed with small\nadversarial examples. We show the effectiveness of this purely geometric\napproach for detecting small adversarial perturbations in images, and for\nrecovering the labels of perturbed images.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 12:38:48 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Fawzi", "Alhussein", ""], ["Moosavi-Dezfooli", "Seyed-Mohsen", ""], ["Frossard", "Pascal", ""], ["Soatto", "Stefano", ""]]}, {"id": "1705.09554", "submitter": "Seyed-Mohsen Moosavi-Dezfooli", "authors": "Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, Pascal\n  Frossard, Stefano Soatto", "title": "Robustness of classifiers to universal perturbations: a geometric\n  perspective", "comments": "Published at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks have recently been shown to be vulnerable to universal\nperturbations: there exist very small image-agnostic perturbations that cause\nmost natural images to be misclassified by such classifiers. In this paper, we\npropose the first quantitative analysis of the robustness of classifiers to\nuniversal perturbations, and draw a formal link between the robustness to\nuniversal perturbations, and the geometry of the decision boundary.\nSpecifically, we establish theoretical bounds on the robustness of classifiers\nunder two decision boundary models (flat and curved models). We show in\nparticular that the robustness of deep networks to universal perturbations is\ndriven by a key property of their curvature: there exists shared directions\nalong which the decision boundary of deep networks is systematically positively\ncurved. Under such conditions, we prove the existence of small universal\nperturbations. Our analysis further provides a novel geometric method for\ncomputing universal perturbations, in addition to explaining their properties.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 12:42:55 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 20:45:00 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Moosavi-Dezfooli", "Seyed-Mohsen", ""], ["Fawzi", "Alhussein", ""], ["Fawzi", "Omar", ""], ["Frossard", "Pascal", ""], ["Soatto", "Stefano", ""]]}, {"id": "1705.09558", "submitter": "Andrew Wilson", "authors": "Yunus Saatchi, Andrew Gordon Wilson", "title": "Bayesian GAN", "comments": "Updated to the version that appears at Advances in Neural Information\n  Processing Systems 30 (NIPS), 2017", "journal-ref": "Advances in Neural Information Processing Systems 30 (NIPS), 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) can implicitly learn rich\ndistributions over images, audio, and data which are hard to model with an\nexplicit likelihood. We present a practical Bayesian formulation for\nunsupervised and semi-supervised learning with GANs. Within this framework, we\nuse stochastic gradient Hamiltonian Monte Carlo to marginalize the weights of\nthe generator and discriminator networks. The resulting approach is\nstraightforward and obtains good performance without any standard interventions\nsuch as feature matching, or mini-batch discrimination. By exploring an\nexpressive posterior over the parameters of the generator, the Bayesian GAN\navoids mode-collapse, produces interpretable and diverse candidate samples, and\nprovides state-of-the-art quantitative results for semi-supervised learning on\nbenchmarks including SVHN, CelebA, and CIFAR-10, outperforming DCGAN,\nWasserstein GANs, and DCGAN ensembles.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 12:47:56 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 07:54:47 GMT"}, {"version": "v3", "created": "Wed, 8 Nov 2017 17:52:21 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Saatchi", "Yunus", ""], ["Wilson", "Andrew Gordon", ""]]}, {"id": "1705.09580", "submitter": "Agostino Capponi", "authors": "Agostino Capponi and Reza Ghanadan and Matt Stern", "title": "Risk-Sensitive Cooperative Games for Human-Machine Systems", "comments": "15 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous systems can substantially enhance a human's efficiency and\neffectiveness in complex environments. Machines, however, are often unable to\nobserve the preferences of the humans that they serve. Despite the fact that\nthe human's and machine's objectives are aligned, asymmetric information, along\nwith heterogeneous sensitivities to risk by the human and machine, make their\njoint optimization process a game with strategic interactions. We propose a\nframework based on risk-sensitive dynamic games; the human seeks to optimize\nher risk-sensitive criterion according to her true preferences, while the\nmachine seeks to adaptively learn the human's preferences and at the same time\nprovide a good service to the human. We develop a class of performance measures\nfor the proposed framework based on the concept of regret. We then evaluate\ntheir dependence on the risk-sensitivity and the degree of uncertainty. We\npresent applications of our framework to self-driving taxis, and robo-financial\nadvising.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 13:35:28 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Capponi", "Agostino", ""], ["Ghanadan", "Reza", ""], ["Stern", "Matt", ""]]}, {"id": "1705.09605", "submitter": "James Grant", "authors": "James A. Grant, David S. Leslie, Kevin Glazebrook, Roberto Szechtman", "title": "Combinatorial Multi-Armed Bandits with Filtered Feedback", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by problems in search and detection we present a solution to a\nCombinatorial Multi-Armed Bandit (CMAB) problem with both heavy-tailed reward\ndistributions and a new class of feedback, filtered semibandit feedback. In a\nCMAB problem an agent pulls a combination of arms from a set $\\{1,...,k\\}$ in\neach round, generating random outcomes from probability distributions\nassociated with these arms and receiving an overall reward. Under semibandit\nfeedback it is assumed that the random outcomes generated are all observed.\nFiltered semibandit feedback allows the outcomes that are observed to be\nsampled from a second distribution conditioned on the initial random outcomes.\nThis feedback mechanism is valuable as it allows CMAB methods to be applied to\nsequential search and detection problems where combinatorial actions are made,\nbut the true rewards (number of objects of interest appearing in the round) are\nnot observed, rather a filtered reward (the number of objects the searcher\nsuccessfully finds, which must by definition be less than the number that\nappear). We present an upper confidence bound type algorithm, Robust-F-CUCB,\nand associated regret bound of order $\\mathcal{O}(\\ln(n))$ to balance\nexploration and exploitation in the face of both filtering of reward and heavy\ntailed reward distributions.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 14:53:46 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Grant", "James A.", ""], ["Leslie", "David S.", ""], ["Glazebrook", "Kevin", ""], ["Szechtman", "Roberto", ""]]}, {"id": "1705.09620", "submitter": "Lev Utkin", "authors": "Lev V. Utkin and Mikhail A. Ryabinin", "title": "Discriminative Metric Learning with Deep Forest", "comments": "arXiv admin note: substantial text overlap with arXiv:1704.08715", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Discriminative Deep Forest (DisDF) as a metric learning algorithm is\nproposed in the paper. It is based on the Deep Forest or gcForest proposed by\nZhou and Feng and can be viewed as a gcForest modification. The case of the\nfully supervised learning is studied when the class labels of individual\ntraining examples are known. The main idea underlying the algorithm is to\nassign weights to decision trees in random forest in order to reduce distances\nbetween objects from the same class and to increase them between objects from\ndifferent classes. The weights are training parameters. A specific objective\nfunction which combines Euclidean and Manhattan distances and simplifies the\noptimization problem for training the DisDF is proposed. The numerical\nexperiments illustrate the proposed distance metric algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 12:24:11 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Utkin", "Lev V.", ""], ["Ryabinin", "Mikhail A.", ""]]}, {"id": "1705.09634", "submitter": "Jonathan Weed", "authors": "Jason Altschuler, Jonathan Weed, Philippe Rigollet", "title": "Near-linear time approximation algorithms for optimal transport via\n  Sinkhorn iteration", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 30 (NIPS 2017),\n  1961-1971", "doi": null, "report-no": null, "categories": "cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing optimal transport distances such as the earth mover's distance is a\nfundamental problem in machine learning, statistics, and computer vision.\nDespite the recent introduction of several algorithms with good empirical\nperformance, it is unknown whether general optimal transport distances can be\napproximated in near-linear time. This paper demonstrates that this ambitious\ngoal is in fact achieved by Cuturi's Sinkhorn Distances. This result relies on\na new analysis of Sinkhorn iteration, which also directly suggests a new greedy\ncoordinate descent algorithm, Greenkhorn, with the same theoretical guarantees.\nNumerical simulations illustrate that Greenkhorn significantly outperforms the\nclassical Sinkhorn algorithm in practice.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 16:14:38 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 18:55:19 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Altschuler", "Jason", ""], ["Weed", "Jonathan", ""], ["Rigollet", "Philippe", ""]]}, {"id": "1705.09644", "submitter": "AmirEmad Ghassami", "authors": "AmirEmad Ghassami, Saber Salehkaleybar, Negar Kiyavash, Kun Zhang", "title": "Learning Causal Structures Using Regression Invariance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study causal inference in a multi-environment setting, in which the\nfunctional relations for producing the variables from their direct causes\nremain the same across environments, while the distribution of exogenous noises\nmay vary. We introduce the idea of using the invariance of the functional\nrelations of the variables to their causes across a set of environments. We\ndefine a notion of completeness for a causal inference algorithm in this\nsetting and prove the existence of such algorithm by proposing the baseline\nalgorithm. Additionally, we present an alternate algorithm that has\nsignificantly improved computational and sample complexity compared to the\nbaseline algorithm. The experiment results show that the proposed algorithm\noutperforms the other existing algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 16:34:13 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Ghassami", "AmirEmad", ""], ["Salehkaleybar", "Saber", ""], ["Kiyavash", "Negar", ""], ["Zhang", "Kun", ""]]}, {"id": "1705.09675", "submitter": "Tom Sercu", "authors": "Youssef Mroueh, Tom Sercu", "title": "Fisher GAN", "comments": "Published at NIPS 2017. v2: added inception score table & plot\n  update, relation to f-gan, illustration (Figure 1). v3: added strong SSL\n  results for critic without batch normalization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) are powerful models for learning\ncomplex distributions. Stable training of GANs has been addressed in many\nrecent works which explore different metrics between distributions. In this\npaper we introduce Fisher GAN which fits within the Integral Probability\nMetrics (IPM) framework for training GANs. Fisher GAN defines a critic with a\ndata dependent constraint on its second order moments. We show in this paper\nthat Fisher GAN allows for stable and time efficient training that does not\ncompromise the capacity of the critic, and does not need data independent\nconstraints such as weight clipping. We analyze our Fisher IPM theoretically\nand provide an algorithm based on Augmented Lagrangian for Fisher GAN. We\nvalidate our claims on both image sample generation and semi-supervised\nclassification using Fisher GAN.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 18:22:24 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 16:42:53 GMT"}, {"version": "v3", "created": "Fri, 3 Nov 2017 18:26:22 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Mroueh", "Youssef", ""], ["Sercu", "Tom", ""]]}, {"id": "1705.09684", "submitter": "Han Zhao", "authors": "Han Zhao, Shanghang Zhang, Guanhang Wu, Jo\\~ao P. Costeira, Jos\\'e M.\n  F. Moura, Geoffrey J. Gordon", "title": "Multiple Source Domain Adaptation with Adversarial Training of Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While domain adaptation has been actively researched in recent years, most\ntheoretical results and algorithms focus on the single-source-single-target\nadaptation setting. Naive application of such algorithms on multiple source\ndomain adaptation problem may lead to suboptimal solutions. As a step toward\nbridging the gap, we propose a new generalization bound for domain adaptation\nwhen there are multiple source domains with labeled instances and one target\ndomain with unlabeled instances. Compared with existing bounds, the new bound\ndoes not require expert knowledge about the target distribution, nor the\noptimal combination rule for multisource domains. Interestingly, our theory\nalso leads to an efficient learning strategy using adversarial neural networks:\nwe show how to interpret it as learning feature representations that are\ninvariant to the multiple domain shifts while still being discriminative for\nthe learning task. To this end, we propose two models, both of which we call\nmultisource domain adversarial networks (MDANs): the first model optimizes\ndirectly our bound, while the second model is a smoothed approximation of the\nfirst one, leading to a more data-efficient and task-adaptive model. The\noptimization tasks of both models are minimax saddle point problems that can be\noptimized by adversarial training. To demonstrate the effectiveness of MDANs,\nwe conduct extensive experiments showing superior adaptation performance on\nthree real-world datasets: sentiment analysis, digit classification, and\nvehicle counting.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 19:10:56 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 22:10:46 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Zhao", "Han", ""], ["Zhang", "Shanghang", ""], ["Wu", "Guanhang", ""], ["Costeira", "Jo\u00e3o P.", ""], ["Moura", "Jos\u00e9 M. F.", ""], ["Gordon", "Geoffrey J.", ""]]}, {"id": "1705.09700", "submitter": "Rad Niazadeh", "authors": "S\\'ebastien Bubeck, Nikhil R. Devanur, Zhiyi Huang, Rad Niazadeh", "title": "Multi-scale Online Learning and its Applications to Online Auctions", "comments": "Preliminary conference version In the Proc. of 18th ACM conference on\n  Economics and Computation (EC 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider revenue maximization in online auction/pricing problems. A seller\nsells an identical item in each period to a new buyer, or a new set of buyers.\nFor the online posted pricing problem, we show regret bounds that scale with\nthe best fixed price, rather than the range of the values. We also show regret\nbounds that are almost scale free, and match the offline sample complexity,\nwhen comparing to a benchmark that requires a lower bound on the market share.\nThese results are obtained by generalizing the classical learning from experts\nand multi-armed bandit problems to their multi-scale versions. In this version,\nthe reward of each action is in a different range, and the regret w.r.t. a\ngiven action scales with its own range, rather than the maximum range.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 19:59:50 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 18:59:34 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Devanur", "Nikhil R.", ""], ["Huang", "Zhiyi", ""], ["Niazadeh", "Rad", ""]]}, {"id": "1705.09755", "submitter": "Andrew Landgraf", "authors": "Andrew J. Landgraf, Jeremy Bellay", "title": "word2vec Skip-Gram with Negative Sampling is a Weighted Logistic PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the skip-gram formulation of word2vec trained with negative\nsampling is equivalent to a weighted logistic PCA. This connection allows us to\nbetter understand the objective, compare it to other word embedding methods,\nand extend it to higher dimensional models.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 02:23:18 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Landgraf", "Andrew J.", ""], ["Bellay", "Jeremy", ""]]}, {"id": "1705.09778", "submitter": "Mathurin Massias", "authors": "Mathurin Massias, Olivier Fercoq, Alexandre Gramfort and Joseph Salmon", "title": "Generalized Concomitant Multi-Task Lasso for sparse multimodal\n  regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high dimension, it is customary to consider Lasso-type estimators to\nenforce sparsity. For standard Lasso theory to hold, the regularization\nparameter should be proportional to the noise level, yet the latter is\ngenerally unknown in practice. A possible remedy is to consider estimators,\nsuch as the Concomitant/Scaled Lasso, which jointly optimize over the\nregression coefficients as well as over the noise level, making the choice of\nthe regularization independent of the noise level. However, when data from\ndifferent sources are pooled to increase sample size, or when dealing with\nmultimodal datasets, noise levels typically differ and new dedicated estimators\nare needed. In this work we provide new statistical and computational solutions\nto deal with such heteroscedastic regression models, with an emphasis on\nfunctional brain imaging with combined magneto- and electroencephalographic\n(M/EEG) signals. Adopting the formulation of Concomitant Lasso-type estimators,\nwe propose a jointly convex formulation to estimate both the regression\ncoefficients and the (square root of the) noise covariance. When our framework\nis instantiated to de-correlated noise, it leads to an efficient algorithm\nwhose computational cost is not higher than for the Lasso and Concomitant\nLasso, while addressing more complex noise structures. Numerical experiments\ndemonstrate that our estimator yields improved prediction and support\nidentification while correctly estimating the noise (square root) covariance.\nResults on multimodal neuroimaging problems with M/EEG data are also reported.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 07:24:38 GMT"}, {"version": "v2", "created": "Wed, 18 Oct 2017 07:53:40 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Massias", "Mathurin", ""], ["Fercoq", "Olivier", ""], ["Gramfort", "Alexandre", ""], ["Salmon", "Joseph", ""]]}, {"id": "1705.09786", "submitter": "Ryota Tomioka", "authors": "Alexander L. Gaunt, Matthew A. Johnson, Maik Riechert, Daniel Tarlow,\n  Ryota Tomioka, Dimitrios Vytiniotis, Sam Webster", "title": "AMPNet: Asynchronous Model-Parallel Training for Dynamic Neural Networks", "comments": "17 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New types of machine learning hardware in development and entering the market\nhold the promise of revolutionizing deep learning in a manner as profound as\nGPUs. However, existing software frameworks and training algorithms for deep\nlearning have yet to evolve to fully leverage the capability of the new wave of\nsilicon. We already see the limitations of existing algorithms for models that\nexploit structured input via complex and instance-dependent control flow, which\nprohibits minibatching. We present an asynchronous model-parallel (AMP)\ntraining algorithm that is specifically motivated by training on networks of\ninterconnected devices. Through an implementation on multi-core CPUs, we show\nthat AMP training converges to the same accuracy as conventional synchronous\ntraining algorithms in a similar number of epochs, but utilizes the available\nhardware more efficiently even for small minibatch sizes, resulting in\nsignificantly shorter overall training times. Our framework opens the door for\nscaling up a new class of deep learning models that cannot be efficiently\ntrained today.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 08:10:40 GMT"}, {"version": "v2", "created": "Sun, 11 Jun 2017 19:51:03 GMT"}, {"version": "v3", "created": "Thu, 22 Jun 2017 17:34:30 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Gaunt", "Alexander L.", ""], ["Johnson", "Matthew A.", ""], ["Riechert", "Maik", ""], ["Tarlow", "Daniel", ""], ["Tomioka", "Ryota", ""], ["Vytiniotis", "Dimitrios", ""], ["Webster", "Sam", ""]]}, {"id": "1705.09847", "submitter": "Jason Ramapuram", "authors": "Jason Ramapuram, Magda Gregorova, Alexandros Kalousis", "title": "Lifelong Generative Modeling", "comments": "32 pages", "journal-ref": "Neurocomputing 2020, Volume 404, Pages 381-400", "doi": "10.1016/j.neucom.2020.02.115", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lifelong learning is the problem of learning multiple consecutive tasks in a\nsequential manner, where knowledge gained from previous tasks is retained and\nused to aid future learning over the lifetime of the learner. It is essential\ntowards the development of intelligent machines that can adapt to their\nsurroundings. In this work we focus on a lifelong learning approach to\nunsupervised generative modeling, where we continuously incorporate newly\nobserved distributions into a learned model. We do so through a student-teacher\nVariational Autoencoder architecture which allows us to learn and preserve all\nthe distributions seen so far, without the need to retain the past data nor the\npast models. Through the introduction of a novel cross-model regularizer,\ninspired by a Bayesian update rule, the student model leverages the information\nlearned by the teacher, which acts as a probabilistic knowledge store. The\nregularizer reduces the effect of catastrophic interference that appears when\nwe learn over sequences of distributions. We validate our model's performance\non sequential variants of MNIST, FashionMNIST, PermutedMNIST, SVHN and Celeb-A\nand demonstrate that our model mitigates the effects of catastrophic\ninterference faced by neural networks in sequential learning scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 17:34:15 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 10:35:15 GMT"}, {"version": "v3", "created": "Mon, 28 May 2018 23:01:23 GMT"}, {"version": "v4", "created": "Tue, 18 Sep 2018 14:29:40 GMT"}, {"version": "v5", "created": "Fri, 10 May 2019 09:17:09 GMT"}, {"version": "v6", "created": "Thu, 14 Nov 2019 17:04:58 GMT"}, {"version": "v7", "created": "Tue, 8 Sep 2020 14:50:12 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Ramapuram", "Jason", ""], ["Gregorova", "Magda", ""], ["Kalousis", "Alexandros", ""]]}, {"id": "1705.09851", "submitter": "Matthew Dixon", "authors": "Matthew F. Dixon, Nicholas G. Polson, Vadim O. Sokolov", "title": "Deep Learning for Spatio-Temporal Modeling: Dynamic Traffic Flows and\n  High Frequency Trading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning applies hierarchical layers of hidden variables to construct\nnonlinear high dimensional predictors. Our goal is to develop and train deep\nlearning architectures for spatio-temporal modeling. Training a deep\narchitecture is achieved by stochastic gradient descent (SGD) and drop-out (DO)\nfor parameter regularization with a goal of minimizing out-of-sample predictive\nmean squared error. To illustrate our methodology, we predict the sharp\ndiscontinuities in traffic flow data, and secondly, we develop a classification\nrule to predict short-term futures market prices as a function of the order\nbook depth. Finally, we conclude with directions for future research.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 18:17:58 GMT"}, {"version": "v2", "created": "Mon, 7 May 2018 16:51:59 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Dixon", "Matthew F.", ""], ["Polson", "Nicholas G.", ""], ["Sokolov", "Vadim O.", ""]]}, {"id": "1705.09862", "submitter": "Zhenwen Dai", "authors": "Zhenwen Dai, Mauricio A. \\'Alvarez, Neil D. Lawrence", "title": "Efficient Modeling of Latent Information in Supervised Learning using\n  Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often in machine learning, data are collected as a combination of multiple\nconditions, e.g., the voice recordings of multiple persons, each labeled with\nan ID. How could we build a model that captures the latent information related\nto these conditions and generalize to a new one with few data? We present a new\nmodel called Latent Variable Multiple Output Gaussian Processes (LVMOGP) and\nthat allows to jointly model multiple conditions for regression and generalize\nto a new condition with a few data points at test time. LVMOGP infers the\nposteriors of Gaussian processes together with a latent space representing the\ninformation about different conditions. We derive an efficient variational\ninference method for LVMOGP, of which the computational complexity is as low as\nsparse Gaussian processes. We show that LVMOGP significantly outperforms\nrelated Gaussian process methods on various tasks with both synthetic and real\ndata.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 20:41:15 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Dai", "Zhenwen", ""], ["\u00c1lvarez", "Mauricio A.", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1705.09866", "submitter": "Allon G. Percus", "authors": "Manuel Valera, Zhengyang Guo, Priscilla Kelly, Sean Matz, Vito Adrian\n  Cantu, Allon G. Percus, Jeffrey D. Hyman, Gowri Srinivasan, Hari S.\n  Viswanathan", "title": "Machine learning for graph-based representations of three-dimensional\n  discrete fracture networks", "comments": "Computational Geosciences (2018)", "journal-ref": "Computational Geosciences 22, 695-710 (2018)", "doi": "10.1007/s10596-018-9720-1", "report-no": "LA-UR-17-24300", "categories": "physics.geo-ph cs.SI physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural and topological information play a key role in modeling flow and\ntransport through fractured rock in the subsurface. Discrete fracture network\n(DFN) computational suites such as dfnWorks are designed to simulate flow and\ntransport in such porous media. Flow and transport calculations reveal that a\nsmall backbone of fractures exists, where most flow and transport occurs.\nRestricting the flowing fracture network to this backbone provides a\nsignificant reduction in the network's effective size. However, the particle\ntracking simulations needed to determine the reduction are computationally\nintensive. Such methods may be impractical for large systems or for robust\nuncertainty quantification of fracture networks, where thousands of forward\nsimulations are needed to bound system behavior.\n  In this paper, we develop an alternative network reduction approach to\ncharacterizing transport in DFNs, by combining graph theoretical and machine\nlearning methods. We consider a graph representation where nodes signify\nfractures and edges denote their intersections. Using random forest and support\nvector machines, we rapidly identify a subnetwork that captures the flow\npatterns of the full DFN, based primarily on node centrality features in the\ngraph. Our supervised learning techniques train on particle-tracking backbone\npaths found by dfnWorks, but run in negligible time compared to those\nsimulations. We find that our predictions can reduce the network to\napproximately 20% of its original size, while still generating breakthrough\ncurves consistent with those of the original network.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 21:12:23 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 20:17:37 GMT"}, {"version": "v3", "created": "Mon, 29 Jan 2018 07:17:06 GMT"}, {"version": "v4", "created": "Tue, 30 Jan 2018 01:40:56 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Valera", "Manuel", ""], ["Guo", "Zhengyang", ""], ["Kelly", "Priscilla", ""], ["Matz", "Sean", ""], ["Cantu", "Vito Adrian", ""], ["Percus", "Allon G.", ""], ["Hyman", "Jeffrey D.", ""], ["Srinivasan", "Gowri", ""], ["Viswanathan", "Hari S.", ""]]}, {"id": "1705.09869", "submitter": "Allon G. Percus", "authors": "Justin Sunu, Allon G. Percus", "title": "Dimensionality reduction for acoustic vehicle classification with\n  spectral embedding", "comments": "Proceedings of the 15th IEEE International Conference on Networking,\n  Sensing and Control (2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for recognizing moving vehicles, using data from roadside\naudio sensors. This problem has applications ranging widely, from traffic\nanalysis to surveillance. We extract a frequency signature from the audio\nsignal using a short-time Fourier transform, and treat each time window as an\nindividual data point to be classified. By applying a spectral embedding, we\ndecrease the dimensionality of the data sufficiently for K-nearest neighbors to\nprovide accurate vehicle identification.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 21:39:47 GMT"}, {"version": "v2", "created": "Sat, 17 Feb 2018 19:42:29 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Sunu", "Justin", ""], ["Percus", "Allon G.", ""]]}, {"id": "1705.09874", "submitter": "Oleg Sofrygin", "authors": "Oleg Sofrygin, Zheng Zhu, Julie A Schmittdiel, Alyce S. Adams, Richard\n  W. Grant, Mark J. van der Laan, and Romain Neugebauer", "title": "Targeted Learning with Daily EHR Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic health records (EHR) data provide a cost and time-effective\nopportunity to conduct cohort studies of the effects of multiple time-point\ninterventions in the diverse patient population found in real-world clinical\nsettings. Because the computational cost of analyzing EHR data at daily (or\nmore granular) scale can be quite high, a pragmatic approach has been to\npartition the follow-up into coarser intervals of pre-specified length. Current\nguidelines suggest employing a 'small' interval, but the feasibility and\npractical impact of this recommendation has not been evaluated and no formal\nmethodology to inform this choice has been developed. We start filling these\ngaps by leveraging large-scale EHR data from a diabetes study to develop and\nillustrate a fast and scalable targeted learning approach that allows to follow\nthe current recommendation and study its practical impact on inference. More\nspecifically, we map daily EHR data into four analytic datasets using 90, 30,\n15 and 5-day intervals. We apply a semi-parametric and doubly robust estimation\napproach, the longitudinal TMLE, to estimate the causal effects of four dynamic\ntreatment rules with each dataset, and compare the resulting inferences. To\novercome the computational challenges presented by the size of these data, we\npropose a novel TMLE implementation, the 'long-format TMLE', and rely on the\nlatest advances in scalable data-adaptive machine-learning software, xgboost\nand h2o, for estimation of the TMLE nuisance parameters.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 22:43:08 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 21:53:24 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Sofrygin", "Oleg", ""], ["Zhu", "Zheng", ""], ["Schmittdiel", "Julie A", ""], ["Adams", "Alyce S.", ""], ["Grant", "Richard W.", ""], ["van der Laan", "Mark J.", ""], ["Neugebauer", "Romain", ""]]}, {"id": "1705.09887", "submitter": "Yanwei  Fu", "authors": "Yanwei Fu, HanZe Dong, Yu-feng Ma, Zhengjun Zhang, Xiangyang Xue", "title": "Vocabulary-informed Extreme Value Learning", "comments": "we significantly change the content of this paper which makes it\n  another paper. In order not to misleading, we decided to withdraw it", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The novel unseen classes can be formulated as the extreme values of known\nclasses. This inspired the recent works on open-set recognition\n\\cite{Scheirer_2013_TPAMI,Scheirer_2014_TPAMIb,EVM}, which however can have no\nway of naming the novel unseen classes. To solve this problem, we propose the\nExtreme Value Learning (EVL) formulation to learn the mapping from visual\nfeature to semantic space. To model the margin and coverage distributions of\neach class, the Vocabulary-informed Learning (ViL) is adopted by using vast\nopen vocabulary in the semantic space. Essentially, by incorporating the EVL\nand ViL, we for the first time propose a novel semantic embedding paradigm --\nVocabulary-informed Extreme Value Learning (ViEVL), which embeds the visual\nfeatures into semantic space in a probabilistic way. The learned embedding can\nbe directly used to solve supervised learning, zero-shot and open set\nrecognition simultaneously. Experiments on two benchmark datasets demonstrate\nthe effectiveness of proposed frameworks.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 02:13:06 GMT"}, {"version": "v2", "created": "Fri, 26 Jan 2018 23:27:27 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Fu", "Yanwei", ""], ["Dong", "HanZe", ""], ["Ma", "Yu-feng", ""], ["Zhang", "Zhengjun", ""], ["Xue", "Xiangyang", ""]]}, {"id": "1705.09919", "submitter": "Likai Du", "authors": "Fang Liu, Likai Du, Dongju Zhang, Jun Gao", "title": "Direct Mapping Hidden Excited State Interaction Patterns from ab initio\n  Dynamics and Its Implications on Force Field Development", "comments": "25 pages, 7 figures", "journal-ref": "Scientific Reports 7, Article number: 8737 (2017)", "doi": "10.1038/s41598-017-09347-2", "report-no": null, "categories": "physics.chem-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The excited states of polyatomic systems are rather complex, and often\nexhibit meta-stable dynamical behaviors. Static analysis of reaction pathway\noften fails to sufficiently characterize excited state motions due to their\nhighly non-equilibrium nature. Here, we proposed a time series guided\nclustering algorithm to generate most relevant meta-stable patterns directly\nfrom ab initio dynamic trajectories. Based on the knowledge of these\nmeta-stable patterns, we suggested an interpolation scheme with only a concrete\nand finite set of known patterns to accurately predict the ground and excited\nstate properties of the entire dynamics trajectories. As illustrated with the\nexample of sinapic acids, the estimation error for both ground and excited\nstate is very close, which indicates one could predict the ground and excited\nstate molecular properties with similar accuracy. These results may provide us\nsome insights to construct an excited state force field with compatible energy\nterms as traditional ones.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 09:41:37 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Liu", "Fang", ""], ["Du", "Likai", ""], ["Zhang", "Dongju", ""], ["Gao", "Jun", ""]]}, {"id": "1705.09944", "submitter": "SueYeon Chung", "authors": "SueYeon Chung, Uri Cohen, Haim Sompolinsky, Daniel D. Lee", "title": "Learning Data Manifolds with a Cutting Plane Method", "comments": null, "journal-ref": "Neural Computation. Volume:30, Issue:10, (2018) pp.2593-2615", "doi": "10.1162/neco_a_01119", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of classifying data manifolds where each manifold\nrepresents invariances that are parameterized by continuous degrees of freedom.\nConventional data augmentation methods rely upon sampling large numbers of\ntraining examples from these manifolds; instead, we propose an iterative\nalgorithm called M_{CP} based upon a cutting-plane approach that efficiently\nsolves a quadratic semi-infinite programming problem to find the maximum margin\nsolution. We provide a proof of convergence as well as a polynomial bound on\nthe number of iterations required for a desired tolerance in the objective\nfunction. The efficiency and performance of M_{CP} are demonstrated in\nhigh-dimensional simulations and on image manifolds generated from the ImageNet\ndataset. Our results indicate that M_{CP} is able to rapidly learn good\nclassifiers and shows superior generalization performance compared with\nconventional maximum margin methods using data augmentation methods.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 14:51:11 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Chung", "SueYeon", ""], ["Cohen", "Uri", ""], ["Sompolinsky", "Haim", ""], ["Lee", "Daniel D.", ""]]}, {"id": "1705.09952", "submitter": "Anders Bredahl Kock", "authors": "Anders Bredahl Kock and Martin Thyrsgaard", "title": "Optimal sequential treatment allocation", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In treatment allocation problems the individuals to be treated often arrive\nsequentially. We study a problem in which the policy maker is not only\ninterested in the expected cumulative welfare but is also concerned about the\nuncertainty/risk of the treatment outcomes. At the outset, the total number of\ntreatment assignments to be made may even be unknown. A sequential treatment\npolicy which attains the minimax optimal regret is proposed. We also\ndemonstrate that the expected number of suboptimal treatments only grows slowly\nin the number of treatments. Finally, we study a setting where outcomes are\nonly observed with delay.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 15:41:12 GMT"}, {"version": "v2", "created": "Tue, 3 Oct 2017 16:10:24 GMT"}, {"version": "v3", "created": "Thu, 11 Jan 2018 10:27:54 GMT"}, {"version": "v4", "created": "Wed, 22 Aug 2018 18:52:38 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Kock", "Anders Bredahl", ""], ["Thyrsgaard", "Martin", ""]]}, {"id": "1705.09966", "submitter": "Yongyi Lu", "authors": "Yongyi Lu, Yu-Wing Tai, Chi-Keung Tang", "title": "Attribute-Guided Face Generation Using Conditional CycleGAN", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in attribute-guided face generation: given a low-res face\ninput image, an attribute vector that can be extracted from a high-res image\n(attribute image), our new method generates a high-res face image for the\nlow-res input that satisfies the given attributes. To address this problem, we\ncondition the CycleGAN and propose conditional CycleGAN, which is designed to\n1) handle unpaired training data because the training low/high-res and high-res\nattribute images may not necessarily align with each other, and to 2) allow\neasy control of the appearance of the generated face via the input attributes.\nWe demonstrate impressive results on the attribute-guided conditional CycleGAN,\nwhich can synthesize realistic face images with appearance easily controlled by\nuser-supplied attributes (e.g., gender, makeup, hair color, eyeglasses). Using\nthe attribute image as identity to produce the corresponding conditional vector\nand by incorporating a face verification network, the attribute-guided network\nbecomes the identity-guided conditional CycleGAN which produces impressive and\ninteresting results on identity transfer. We demonstrate three applications on\nidentity-guided conditional CycleGAN: identity-preserving face superresolution,\nface swapping, and frontal face generation, which consistently show the\nadvantage of our new method.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 17:37:23 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 13:35:49 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Lu", "Yongyi", ""], ["Tai", "Yu-Wing", ""], ["Tang", "Chi-Keung", ""]]}, {"id": "1705.10015", "submitter": "Songting Shi", "authors": "Songting Shi, Xiang Li, Arkadiusz Sitek and Quanzheng Li", "title": "Learning the Sparse and Low Rank PARAFAC Decomposition via the Elastic\n  Net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we derive a Bayesian model to learning the sparse and low\nrank PARAFAC decomposition for the observed tensor with missing values via the\nelastic net, with property to find the true rank and sparse factor matrix which\nis robust to the noise. We formulate efficient block coordinate descent\nalgorithm and admax stochastic block coordinate descent algorithm to solve it,\nwhich can be used to solve the large scale problem. To choose the appropriate\nrank and sparsity in PARAFAC decomposition, we will give a solution path by\ngradually increasing the regularization to increase the sparsity and decrease\nthe rank. When we find the sparse structure of the factor matrix, we can fixed\nthe sparse structure, using a small to regularization to decreasing the\nrecovery error, and one can choose the proper decomposition from the solution\npath with sufficient sparse factor matrix with low recovery error. We test the\npower of our algorithm on the simulation data and real data, which show it is\npowerful.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 02:14:05 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Shi", "Songting", ""], ["Li", "Xiang", ""], ["Sitek", "Arkadiusz", ""], ["Li", "Quanzheng", ""]]}, {"id": "1705.10033", "submitter": "Chao Qin", "authors": "Chao Qin, Diego Klabjan, and Daniel Russo", "title": "Improving the Expected Improvement Algorithm", "comments": "Submitted to NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expected improvement (EI) algorithm is a popular strategy for information\ncollection in optimization under uncertainty. The algorithm is widely known to\nbe too greedy, but nevertheless enjoys wide use due to its simplicity and\nability to handle uncertainty and noise in a coherent decision theoretic\nframework. To provide rigorous insight into EI, we study its properties in a\nsimple setting of Bayesian optimization where the domain consists of a finite\ngrid of points. This is the so-called best-arm identification problem, where\nthe goal is to allocate measurement effort wisely to confidently identify the\nbest arm using a small number of measurements. In this framework, one can show\nformally that EI is far from optimal. To overcome this shortcoming, we\nintroduce a simple modification of the expected improvement algorithm.\nSurprisingly, this simple change results in an algorithm that is asymptotically\noptimal for Gaussian best-arm identification problems, and provably outperforms\nstandard EI by an order of magnitude.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 04:02:27 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Qin", "Chao", ""], ["Klabjan", "Diego", ""], ["Russo", "Daniel", ""]]}, {"id": "1705.10087", "submitter": "Thomas Moreau", "authors": "Thomas Moreau, Laurent Oudre, Nicolas Vayatis", "title": "DICOD: Distributed Convolutional Sparse Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce DICOD, a convolutional sparse coding algorithm\nwhich builds shift invariant representations for long signals. This algorithm\nis designed to run in a distributed setting, with local message passing, making\nit communication efficient. It is based on coordinate descent and uses locally\ngreedy updates which accelerate the resolution compared to greedy coordinate\nselection. We prove the convergence of this algorithm and highlight its\ncomputational speed-up which is super-linear in the number of cores used. We\nalso provide empirical evidence for the acceleration properties of our\nalgorithm compared to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 09:21:30 GMT"}, {"version": "v2", "created": "Sun, 13 May 2018 14:04:25 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Moreau", "Thomas", ""], ["Oudre", "Laurent", ""], ["Vayatis", "Nicolas", ""]]}, {"id": "1705.10102", "submitter": "Jiasen Yang", "authors": "Agniva Chowdhury, Jiasen Yang, Petros Drineas", "title": "Structural Conditions for Projection-Cost Preservation via Randomized\n  Matrix Multiplication", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Projection-cost preservation is a low-rank approximation guarantee which\nensures that the cost of any rank-$k$ projection can be preserved using a\nsmaller sketch of the original data matrix. We present a general structural\nresult outlining four sufficient conditions to achieve projection-cost\npreservation. These conditions can be satisfied using tools from the Randomized\nLinear Algebra literature.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 10:29:23 GMT"}, {"version": "v2", "created": "Fri, 17 Aug 2018 21:38:55 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Chowdhury", "Agniva", ""], ["Yang", "Jiasen", ""], ["Drineas", "Petros", ""]]}, {"id": "1705.10119", "submitter": "Jiaxin Shi", "authors": "Jiaxin Shi, Shengyang Sun, Jun Zhu", "title": "Kernel Implicit Variational Inference", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in variational inference has paid much attention to the\nflexibility of variational posteriors. One promising direction is to use\nimplicit distributions, i.e., distributions without tractable densities as the\nvariational posterior. However, existing methods on implicit posteriors still\nface challenges of noisy estimation and computational infeasibility when\napplied to models with high-dimensional latent variables. In this paper, we\npresent a new approach named Kernel Implicit Variational Inference that\naddresses these challenges. As far as we know, for the first time implicit\nvariational inference is successfully applied to Bayesian neural networks,\nwhich shows promising results on both regression and classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 11:11:35 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 13:49:00 GMT"}, {"version": "v3", "created": "Fri, 23 Feb 2018 15:45:59 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Shi", "Jiaxin", ""], ["Sun", "Shengyang", ""], ["Zhu", "Jun", ""]]}, {"id": "1705.10182", "submitter": "Taiji Suzuki", "authors": "Taiji Suzuki", "title": "Fast learning rate of deep learning via a kernel perspective", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new theoretical framework to analyze the generalization error of\ndeep learning, and derive a new fast learning rate for two representative\nalgorithms: empirical risk minimization and Bayesian deep learning. The series\nof theoretical analyses of deep learning has revealed its high expressive power\nand universal approximation capability. Although these analyses are highly\nnonparametric, existing generalization error analyses have been developed\nmainly in a fixed dimensional parametric model. To compensate this gap, we\ndevelop an infinite dimensional model that is based on an integral form as\nperformed in the analysis of the universal approximation capability. This\nallows us to define a reproducing kernel Hilbert space corresponding to each\nlayer. Our point of view is to deal with the ordinary finite dimensional deep\nneural network as a finite approximation of the infinite dimensional one. The\napproximation error is evaluated by the degree of freedom of the reproducing\nkernel Hilbert space in each layer. To estimate a good finite dimensional\nmodel, we consider both of empirical risk minimization and Bayesian deep\nlearning. We derive its generalization error bound and it is shown that there\nappears bias-variance trade-off in terms of the number of parameters of the\nfinite dimensional approximation. We show that the optimal width of the\ninternal layers can be determined through the degree of freedom and the\nconvergence rate can be faster than $O(1/\\sqrt{n})$ rate which has been shown\nin the existing studies.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 13:47:44 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Suzuki", "Taiji", ""]]}, {"id": "1705.10194", "submitter": "Feng Nan", "authors": "Feng Nan, Venkatesh Saligrama", "title": "Adaptive Classification for Prediction Under a Budget", "comments": "arXiv admin note: substantial text overlap with arXiv:1704.07505", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel adaptive approximation approach for test-time\nresource-constrained prediction. Given an input instance at test-time, a gating\nfunction identifies a prediction model for the input among a collection of\nmodels. Our objective is to minimize overall average cost without sacrificing\naccuracy. We learn gating and prediction models on fully labeled training data\nby means of a bottom-up strategy. Our novel bottom-up method first trains a\nhigh-accuracy complex model. Then a low-complexity gating and prediction model\nare subsequently learned to adaptively approximate the high-accuracy model in\nregions where low-cost models are capable of making highly accurate\npredictions. We pose an empirical loss minimization problem with cost\nconstraints to jointly train gating and prediction models. On a number of\nbenchmark datasets our method outperforms state-of-the-art achieving higher\naccuracy for the same cost.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 12:28:42 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Nan", "Feng", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1705.10225", "submitter": "Tiago Peixoto", "authors": "Tiago P. Peixoto", "title": "Bayesian stochastic blockmodeling", "comments": "44 pages, 16 figures. Code is freely available as part of graph-tool\n  at https://graph-tool.skewed.de . See also the HOWTO at\n  https://graph-tool.skewed.de/static/doc/demos/inference/inference.html", "journal-ref": "\"Advances in Network Clustering and Blockmodeling\", edited by P.\n  Doreian, V. Batagelj, A. Ferligoj, (Wiley, New York, 2019)", "doi": "10.1002/9781119483298.ch11", "report-no": null, "categories": "stat.ML cond-mat.stat-mech physics.data-an", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This chapter provides a self-contained introduction to the use of Bayesian\ninference to extract large-scale modular structures from network data, based on\nthe stochastic blockmodel (SBM), as well as its degree-corrected and\noverlapping generalizations. We focus on nonparametric formulations that allow\ntheir inference in a manner that prevents overfitting, and enables model\nselection. We discuss aspects of the choice of priors, in particular how to\navoid underfitting via increased Bayesian hierarchies, and we contrast the task\nof sampling network partitions from the posterior distribution with finding the\nsingle point estimate that maximizes it, while describing efficient algorithms\nto perform either one. We also show how inferring the SBM can be used to\npredict missing and spurious links, and shed light on the fundamental\nlimitations of the detectability of modular structures in networks.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 14:53:42 GMT"}, {"version": "v2", "created": "Tue, 26 Sep 2017 13:21:54 GMT"}, {"version": "v3", "created": "Wed, 29 Nov 2017 11:53:30 GMT"}, {"version": "v4", "created": "Fri, 19 Jan 2018 04:07:21 GMT"}, {"version": "v5", "created": "Mon, 9 Apr 2018 17:35:23 GMT"}, {"version": "v6", "created": "Sun, 9 Sep 2018 21:09:50 GMT"}, {"version": "v7", "created": "Mon, 26 Nov 2018 10:15:06 GMT"}, {"version": "v8", "created": "Thu, 6 Feb 2020 13:26:54 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Peixoto", "Tiago P.", ""]]}, {"id": "1705.10229", "submitter": "Tsung-Hsien Wen", "authors": "Tsung-Hsien Wen, Yishu Miao, Phil Blunsom, Steve Young", "title": "Latent Intention Dialogue Models", "comments": "Accepted at ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing a dialogue agent that is capable of making autonomous decisions\nand communicating by natural language is one of the long-term goals of machine\nlearning research. Traditional approaches either rely on hand-crafting a small\nstate-action set for applying reinforcement learning that is not scalable or\nconstructing deterministic models for learning dialogue sentences that fail to\ncapture natural conversational variability. In this paper, we propose a Latent\nIntention Dialogue Model (LIDM) that employs a discrete latent variable to\nlearn underlying dialogue intentions in the framework of neural variational\ninference. In a goal-oriented dialogue scenario, these latent intentions can be\ninterpreted as actions guiding the generation of machine responses, which can\nbe further refined autonomously by reinforcement learning. The experimental\nevaluation of LIDM shows that the model out-performs published benchmarks for\nboth corpus-based and human evaluation, demonstrating the effectiveness of\ndiscrete latent variable models for learning goal-oriented dialogues.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 15:01:44 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Wen", "Tsung-Hsien", ""], ["Miao", "Yishu", ""], ["Blunsom", "Phil", ""], ["Young", "Steve", ""]]}, {"id": "1705.10245", "submitter": "Tristan Sylvain", "authors": "Margaux Luck, Tristan Sylvain, H\\'elo\\\"ise Cardinal, Andrea Lodi,\n  Yoshua Bengio", "title": "Deep Learning for Patient-Specific Kidney Graft Survival Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An accurate model of patient-specific kidney graft survival distributions can\nhelp to improve shared-decision making in the treatment and care of patients.\nIn this paper, we propose a deep learning method that directly models the\nsurvival function instead of estimating the hazard function to predict survival\ntimes for graft patients based on the principle of multi-task learning. By\nlearning to jointly predict the time of the event, and its rank in the cox\npartial log likelihood framework, our deep learning approach outperforms, in\nterms of survival time prediction quality and concordance index, other common\nmethods for survival analysis, including the Cox Proportional Hazards model and\na network trained on the cox partial log-likelihood.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 15:17:14 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Luck", "Margaux", ""], ["Sylvain", "Tristan", ""], ["Cardinal", "H\u00e9lo\u00efse", ""], ["Lodi", "Andrea", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1705.10246", "submitter": "Gil Keren", "authors": "Gil Keren, Sivan Sabato, Bj\\\"orn Schuller", "title": "Fast Single-Class Classification and the Principle of Logit Separation", "comments": "Published as a conference paper in ICDM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider neural network training, in applications in which there are many\npossible classes, but at test-time, the task is a binary classification task of\ndetermining whether the given example belongs to a specific class, where the\nclass of interest can be different each time the classifier is applied. For\ninstance, this is the case for real-time image search. We define the Single\nLogit Classification (SLC) task: training the network so that at test-time, it\nwould be possible to accurately identify whether the example belongs to a given\nclass in a computationally efficient manner, based only on the output logit for\nthis class. We propose a natural principle, the Principle of Logit Separation,\nas a guideline for choosing and designing losses suitable for the SLC. We show\nthat the cross-entropy loss function is not aligned with the Principle of Logit\nSeparation. In contrast, there are known loss functions, as well as novel batch\nloss functions that we propose, which are aligned with this principle. In\ntotal, we study seven loss functions. Our experiments show that indeed in\nalmost all cases, losses that are aligned with the Principle of Logit\nSeparation obtain at least 20% relative accuracy improvement in the SLC task\ncompared to losses that are not aligned with it, and sometimes considerably\nmore. Furthermore, we show that fast SLC does not cause any drop in binary\nclassification accuracy, compared to standard classification in which all\nlogits are computed, and yields a speedup which grows with the number of\nclasses. For instance, we demonstrate a 10x speedup when the number of classes\nis 400,000. Tensorflow code for optimizing the new batch losses is publicly\navailable at https://github.com/cruvadom/Logit Separation.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 15:18:59 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 13:44:15 GMT"}, {"version": "v3", "created": "Thu, 26 Oct 2017 08:35:28 GMT"}, {"version": "v4", "created": "Mon, 17 Sep 2018 15:05:39 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Keren", "Gil", ""], ["Sabato", "Sivan", ""], ["Schuller", "Bj\u00f6rn", ""]]}, {"id": "1705.10257", "submitter": "Gergely Neu", "authors": "Nicol\\`o Cesa-Bianchi and Claudio Gentile and G\\'abor Lugosi and\n  Gergely Neu", "title": "Boltzmann Exploration Done Right", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boltzmann exploration is a classic strategy for sequential decision-making\nunder uncertainty, and is one of the most standard tools in Reinforcement\nLearning (RL). Despite its widespread use, there is virtually no theoretical\nunderstanding about the limitations or the actual benefits of this exploration\nscheme. Does it drive exploration in a meaningful way? Is it prone to\nmisidentifying the optimal actions or spending too much time exploring the\nsuboptimal ones? What is the right tuning for the learning rate? In this paper,\nwe address several of these questions in the classic setup of stochastic\nmulti-armed bandits. One of our main results is showing that the Boltzmann\nexploration strategy with any monotone learning-rate sequence will induce\nsuboptimal behavior. As a remedy, we offer a simple non-monotone schedule that\nguarantees near-optimal performance, albeit only when given prior access to key\nproblem parameters that are typically not available in practical situations\n(like the time horizon $T$ and the suboptimality gap $\\Delta$). More\nimportantly, we propose a novel variant that uses different learning rates for\ndifferent arms, and achieves a distribution-dependent regret bound of order\n$\\frac{K\\log^2 T}{\\Delta}$ and a distribution-independent bound of order\n$\\sqrt{KT}\\log K$ without requiring such prior knowledge. To demonstrate the\nflexibility of our technique, we also propose a variant that guarantees the\nsame performance bounds even if the rewards are heavy-tailed.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 15:33:29 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 18:08:29 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Cesa-Bianchi", "Nicol\u00f2", ""], ["Gentile", "Claudio", ""], ["Lugosi", "G\u00e1bor", ""], ["Neu", "Gergely", ""]]}, {"id": "1705.10301", "submitter": "Maruan Al-Shedivat", "authors": "Maruan Al-Shedivat, Avinava Dubey, Eric P. Xing", "title": "Contextual Explanation Networks", "comments": "48 pages, 18 figures, to appear in JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern learning algorithms excel at producing accurate but complex models of\nthe data. However, deploying such models in the real-world requires extra care:\nwe must ensure their reliability, robustness, and absence of undesired biases.\nThis motivates the development of models that are equally accurate but can be\nalso easily inspected and assessed beyond their predictive performance. To this\nend, we introduce contextual explanation networks (CEN)---a class of\narchitectures that learn to predict by generating and utilizing intermediate,\nsimplified probabilistic models. Specifically, CENs generate parameters for\nintermediate graphical models which are further used for prediction and play\nthe role of explanations. Contrary to the existing post-hoc model-explanation\ntools, CENs learn to predict and to explain simultaneously. Our approach offers\ntwo major advantages: (i) for each prediction valid, instance-specific\nexplanation is generated with no computational overhead and (ii) prediction via\nexplanation acts as a regularizer and boosts performance in data-scarce\nsettings. We analyze the proposed framework theoretically and experimentally.\nOur results on image and text classification and survival analysis tasks\ndemonstrate that CENs are not only competitive with the state-of-the-art\nmethods but also offer additional insights behind each prediction, that can be\nvaluable for decision support. We also show that while post-hoc methods may\nproduce misleading explanations in certain cases, CENs are consistent and allow\nto detect such cases systematically.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 17:39:51 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 00:06:02 GMT"}, {"version": "v3", "created": "Tue, 18 Dec 2018 22:33:40 GMT"}, {"version": "v4", "created": "Wed, 9 Sep 2020 14:20:44 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Al-Shedivat", "Maruan", ""], ["Dubey", "Avinava", ""], ["Xing", "Eric P.", ""]]}, {"id": "1705.10306", "submitter": "Tuan Anh Le", "authors": "Tuan Anh Le, Maximilian Igl, Tom Rainforth, Tom Jin, Frank Wood", "title": "Auto-Encoding Sequential Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We build on auto-encoding sequential Monte Carlo (AESMC): a method for model\nand proposal learning based on maximizing the lower bound to the log marginal\nlikelihood in a broad family of structured probabilistic models. Our approach\nrelies on the efficiency of sequential Monte Carlo (SMC) for performing\ninference in structured probabilistic models and the flexibility of deep neural\nnetworks to model complex conditional probability distributions. We develop\nadditional theoretical insights and introduce a new training procedure which\nimproves both model and proposal learning. We demonstrate that our approach\nprovides a fast, easy-to-implement and scalable means for simultaneous model\nlearning and proposal adaptation in deep generative models.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 17:54:11 GMT"}, {"version": "v2", "created": "Thu, 5 Apr 2018 09:27:36 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Le", "Tuan Anh", ""], ["Igl", "Maximilian", ""], ["Rainforth", "Tom", ""], ["Jin", "Tom", ""], ["Wood", "Frank", ""]]}, {"id": "1705.10359", "submitter": "Benjamin Chamberlain", "authors": "Benjamin Paul Chamberlain, James Clough, Marc Peter Deisenroth", "title": "Neural Embeddings of Graphs in Hyperbolic Space", "comments": "7 pages, 5 figures", "journal-ref": "13th international workshop on mining and learning from graphs\n  held in conjunction with KDD, 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural embeddings have been used with great success in Natural Language\nProcessing (NLP). They provide compact representations that encapsulate word\nsimilarity and attain state-of-the-art performance in a range of linguistic\ntasks. The success of neural embeddings has prompted significant amounts of\nresearch into applications in domains other than language. One such domain is\ngraph-structured data, where embeddings of vertices can be learned that\nencapsulate vertex similarity and improve performance on tasks including edge\nprediction and vertex labelling. For both NLP and graph based tasks, embeddings\nhave been learned in high-dimensional Euclidean spaces. However, recent work\nhas shown that the appropriate isometric space for embedding complex networks\nis not the flat Euclidean space, but negatively curved, hyperbolic space. We\npresent a new concept that exploits these recent insights and propose learning\nneural embeddings of graphs in hyperbolic space. We provide experimental\nevidence that embedding graphs in their natural geometry significantly improves\nperformance on downstream tasks for several real-world public datasets.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 18:47:30 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Chamberlain", "Benjamin Paul", ""], ["Clough", "James", ""], ["Deisenroth", "Marc Peter", ""]]}, {"id": "1705.10378", "submitter": "Razieh Nabi", "authors": "Razieh Nabi and Ilya Shpitser", "title": "Fair Inference On Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of fair statistical inference\ninvolving outcome variables. Examples include classification and regression\nproblems, and estimating treatment effects in randomized trials or\nobservational data. The issue of fairness arises in such problems where some\ncovariates or treatments are \"sensitive,\" in the sense of having potential of\ncreating discrimination. In this paper, we argue that the presence of\ndiscrimination can be formalized in a sensible way as the presence of an effect\nof a sensitive covariate on the outcome along certain causal pathways, a view\nwhich generalizes (Pearl, 2009). A fair outcome model can then be learned by\nsolving a constrained optimization problem. We discuss a number of\ncomplications that arise in classical statistical inference due to this view\nand provide workarounds based on recent work in causal and semi-parametric\ninference.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 19:51:38 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 22:21:09 GMT"}, {"version": "v3", "created": "Mon, 20 Nov 2017 14:55:55 GMT"}, {"version": "v4", "created": "Sun, 21 Jan 2018 06:29:53 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Nabi", "Razieh", ""], ["Shpitser", "Ilya", ""]]}, {"id": "1705.10388", "submitter": "Soumya Ghosh", "authors": "Soumya Ghosh and Finale Doshi-Velez", "title": "Model Selection in Bayesian Neural Networks via Horseshoe Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian Neural Networks (BNNs) have recently received increasing attention\nfor their ability to provide well-calibrated posterior uncertainties. However,\nmodel selection---even choosing the number of nodes---remains an open question.\nIn this work, we apply a horseshoe prior over node pre-activations of a\nBayesian neural network, which effectively turns off nodes that do not help\nexplain the data. We demonstrate that our prior prevents the BNN from\nunder-fitting even when the number of nodes required is grossly over-estimated.\nMoreover, this model selection over the number of nodes doesn't come at the\nexpense of predictive or computational performance; in fact, we learn smaller\nnetworks with comparable predictive performance to current approaches.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 20:35:42 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Ghosh", "Soumya", ""], ["Doshi-Velez", "Finale", ""]]}, {"id": "1705.10407", "submitter": "Gang Wang", "authors": "Gang Wang and Georgios B. Giannakis and Yousef Saad and Jie Chen", "title": "Solving Almost all Systems of Random Quadratic Equations", "comments": "27 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with finding an $n$-dimensional solution $x$ to a system of\nquadratic equations of the form $y_i=|\\langle{a}_i,x\\rangle|^2$ for $1\\le i \\le\nm$, which is also known as phase retrieval and is NP-hard in general. We put\nforth a novel procedure for minimizing the amplitude-based least-squares\nempirical loss, that starts with a weighted maximal correlation initialization\nobtainable with a few power or Lanczos iterations, followed by successive\nrefinements based upon a sequence of iteratively reweighted (generalized)\ngradient iterations. The two (both the initialization and gradient flow) stages\ndistinguish themselves from prior contributions by the inclusion of a fresh\n(re)weighting regularization technique. The overall algorithm is conceptually\nsimple, numerically scalable, and easy-to-implement. For certain random\nmeasurement models, the novel procedure is shown capable of finding the true\nsolution $x$ in time proportional to reading the data $\\{(a_i;y_i)\\}_{1\\le i\n\\le m}$. This holds with high probability and without extra assumption on the\nsignal $x$ to be recovered, provided that the number $m$ of equations is some\nconstant $c>0$ times the number $n$ of unknowns in the signal vector, namely,\n$m>cn$. Empirically, the upshots of this contribution are: i) (almost) $100\\%$\nperfect signal recovery in the high-dimensional (say e.g., $n\\ge 2,000$) regime\ngiven only an information-theoretic limit number of noiseless equations,\nnamely, $m=2n-1$ in the real-valued Gaussian case; and, ii) (nearly) optimal\nstatistical accuracy in the presence of additive noise of bounded support.\nFinally, substantial numerical tests using both synthetic data and real images\ncorroborate markedly improved signal recovery performance and computational\nefficiency of our novel procedure relative to state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 22:18:05 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Wang", "Gang", ""], ["Giannakis", "Georgios B.", ""], ["Saad", "Yousef", ""], ["Chen", "Jie", ""]]}, {"id": "1705.10412", "submitter": "Simon Du", "authors": "Simon S. Du, Chi Jin, Jason D. Lee, Michael I. Jordan, Barnabas\n  Poczos, Aarti Singh", "title": "Gradient Descent Can Take Exponential Time to Escape Saddle Points", "comments": "Accepted by NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although gradient descent (GD) almost always escapes saddle points\nasymptotically [Lee et al., 2016], this paper shows that even with fairly\nnatural random initialization schemes and non-pathological functions, GD can be\nsignificantly slowed down by saddle points, taking exponential time to escape.\nOn the other hand, gradient descent with perturbations [Ge et al., 2015, Jin et\nal., 2017] is not slowed down by saddle points - it can find an approximate\nlocal minimizer in polynomial time. This result implies that GD is inherently\nslower than perturbed GD, and justifies the importance of adding perturbations\nfor efficient non-convex optimization. While our focus is theoretical, we also\npresent experiments that illustrate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 23:03:01 GMT"}, {"version": "v2", "created": "Sun, 5 Nov 2017 16:35:27 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Du", "Simon S.", ""], ["Jin", "Chi", ""], ["Lee", "Jason D.", ""], ["Jordan", "Michael I.", ""], ["Poczos", "Barnabas", ""], ["Singh", "Aarti", ""]]}, {"id": "1705.10467", "submitter": "Virginia Smith", "authors": "Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, Ameet Talwalkar", "title": "Federated Multi-Task Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning poses new statistical and systems challenges in training\nmachine learning models over distributed networks of devices. In this work, we\nshow that multi-task learning is naturally suited to handle the statistical\nchallenges of this setting, and propose a novel systems-aware optimization\nmethod, MOCHA, that is robust to practical systems issues. Our method and\ntheory for the first time consider issues of high communication cost,\nstragglers, and fault tolerance for distributed multi-task learning. The\nresulting method achieves significant speedups compared to alternatives in the\nfederated setting, as we demonstrate through simulations on real-world\nfederated datasets.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 06:20:31 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 07:29:26 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Smith", "Virginia", ""], ["Chiang", "Chao-Kai", ""], ["Sanjabi", "Maziar", ""], ["Talwalkar", "Ameet", ""]]}, {"id": "1705.10470", "submitter": "Weiyang Liu", "authors": "Weiyang Liu, Bo Dai, Ahmad Humayun, Charlene Tay, Chen Yu, Linda B.\n  Smith, James M. Rehg, Le Song", "title": "Iterative Machine Teaching", "comments": "Published in ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of machine teaching, the inverse\nproblem of machine learning. Different from traditional machine teaching which\nviews the learners as batch algorithms, we study a new paradigm where the\nlearner uses an iterative algorithm and a teacher can feed examples\nsequentially and intelligently based on the current performance of the learner.\nWe show that the teaching complexity in the iterative case is very different\nfrom that in the batch case. Instead of constructing a minimal training set for\nlearners, our iterative machine teaching focuses on achieving fast convergence\nin the learner model. Depending on the level of information the teacher has\nfrom the learner model, we design teaching algorithms which can provably reduce\nthe number of teaching examples and achieve faster convergence than learning\nwithout teachers. We also validate our theoretical findings with extensive\nexperiments on different data distribution and real image datasets.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 06:35:29 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 05:42:32 GMT"}, {"version": "v3", "created": "Fri, 17 Nov 2017 22:19:21 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Liu", "Weiyang", ""], ["Dai", "Bo", ""], ["Humayun", "Ahmad", ""], ["Tay", "Charlene", ""], ["Yu", "Chen", ""], ["Smith", "Linda B.", ""], ["Rehg", "James M.", ""], ["Song", "Le", ""]]}, {"id": "1705.10494", "submitter": "Baruch Epstein", "authors": "Baruch Epstein. Ron Meir, Tomer Michaeli", "title": "Joint auto-encoders: a flexible multi-task learning framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The incorporation of prior knowledge into learning is essential in achieving\ngood performance based on small noisy samples. Such knowledge is often\nincorporated through the availability of related data arising from domains and\ntasks similar to the one of current interest. Ideally one would like to allow\nboth the data for the current task and for previous related tasks to\nself-organize the learning system in such a way that commonalities and\ndifferences between the tasks are learned in a data-driven fashion. We develop\na framework for learning multiple tasks simultaneously, based on sharing\nfeatures that are common to all tasks, achieved through the use of a modular\ndeep feedforward neural network consisting of shared branches, dealing with the\ncommon features of all tasks, and private branches, learning the specific\nunique aspects of each task. Once an appropriate weight sharing architecture\nhas been established, learning takes place through standard algorithms for\nfeedforward networks, e.g., stochastic gradient descent and its variations. The\nmethod deals with domain adaptation and multi-task learning in a unified\nfashion, and can easily deal with data arising from different types of sources.\nNumerical experiments demonstrate the effectiveness of learning in domain\nadaptation and transfer learning setups, and provide evidence for the flexible\nand task-oriented representations arising in the network.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 07:51:42 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Meir", "Baruch Epstein. Ron", ""], ["Michaeli", "Tomer", ""]]}, {"id": "1705.10498", "submitter": "Guillaume Gautier", "authors": "Guillaume Gautier, R\\'emi Bardenet, Michal Valko", "title": "Zonotope hit-and-run for efficient sampling from projection DPPs", "comments": "12 pages, 12 figures, 2 columns, accepted to ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal point processes (DPPs) are distributions over sets of items\nthat model diversity using kernels. Their applications in machine learning\ninclude summary extraction and recommendation systems. Yet, the cost of\nsampling from a DPP is prohibitive in large-scale applications, which has\ntriggered an effort towards efficient approximate samplers. We build a novel\nMCMC sampler that combines ideas from combinatorial geometry, linear\nprogramming, and Monte Carlo methods to sample from DPPs with a fixed sample\ncardinality, also called projection DPPs. Our sampler leverages the ability of\nthe hit-and-run MCMC kernel to efficiently move across convex bodies. Previous\ntheoretical results yield a fast mixing time of our chain when targeting a\ndistribution that is close to a projection DPP, but not a DPP in general. Our\nempirical results demonstrate that this extends to sampling projection DPPs,\ni.e., our sampler is more sample-efficient than previous approaches which in\nturn translates to faster convergence when dealing with costly-to-evaluate\nfunctions, such as summary extraction in our experiments.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 08:07:19 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 08:36:00 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Gautier", "Guillaume", ""], ["Bardenet", "R\u00e9mi", ""], ["Valko", "Michal", ""]]}, {"id": "1705.10499", "submitter": "Kfir Levy Yehuda", "authors": "Kfir Y. Levy", "title": "Online to Offline Conversions, Universality and Adaptive Minibatch Sizes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach towards convex optimization that relies on a novel\nscheme which converts online adaptive algorithms into offline methods. In the\noffline optimization setting, our derived methods are shown to obtain\nfavourable adaptive guarantees which depend on the harmonic sum of the queried\ngradients. We further show that our methods implicitly adapt to the objective's\nstructure: in the smooth case fast convergence rates are ensured without any\nprior knowledge of the smoothness parameter, while still maintaining guarantees\nin the non-smooth setting. Our approach has a natural extension to the\nstochastic setting, resulting in a lazy version of SGD (stochastic GD), where\nminibathces are chosen \\emph{adaptively} depending on the magnitude of the\ngradients. Thus providing a principled approach towards choosing minibatch\nsizes.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 08:08:36 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 07:22:55 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Levy", "Kfir Y.", ""]]}, {"id": "1705.10524", "submitter": "Zhanyu Ma", "authors": "Zhanyu Ma, Jing-Hao Xue, Arne Leijon, Zheng-Hua Tan, Zhen Yang, and\n  Jun Guo", "title": "Decorrelation of Neutral Vector Variables: Theory and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose novel strategies for neutral vector variable\ndecorrelation. Two fundamental invertible transformations, namely serial\nnonlinear transformation and parallel nonlinear transformation, are proposed to\ncarry out the decorrelation. For a neutral vector variable, which is not\nmultivariate Gaussian distributed, the conventional principal component\nanalysis (PCA) cannot yield mutually independent scalar variables. With the two\nproposed transformations, a highly negatively correlated neutral vector can be\ntransformed to a set of mutually independent scalar variables with the same\ndegrees of freedom. We also evaluate the decorrelation performances for the\nvectors generated from a single Dirichlet distribution and a mixture of\nDirichlet distributions. The mutual independence is verified with the distance\ncorrelation measurement. The advantages of the proposed decorrelation\nstrategies are intensively studied and demonstrated with synthesized data and\npractical application evaluations.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 09:53:11 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Ma", "Zhanyu", ""], ["Xue", "Jing-Hao", ""], ["Leijon", "Arne", ""], ["Tan", "Zheng-Hua", ""], ["Yang", "Zhen", ""], ["Guo", "Jun", ""]]}, {"id": "1705.10743", "submitter": "Marc G. Bellemare", "authors": "Marc G. Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji\n  Lakshminarayanan, Stephan Hoyer, R\\'emi Munos", "title": "The Cramer Distance as a Solution to Biased Wasserstein Gradients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wasserstein probability metric has received much attention from the\nmachine learning community. Unlike the Kullback-Leibler divergence, which\nstrictly measures change in probability, the Wasserstein metric reflects the\nunderlying geometry between outcomes. The value of being sensitive to this\ngeometry has been demonstrated, among others, in ordinal regression and\ngenerative modelling. In this paper we describe three natural properties of\nprobability divergences that reflect requirements from machine learning: sum\ninvariance, scale sensitivity, and unbiased sample gradients. The Wasserstein\nmetric possesses the first two properties but, unlike the Kullback-Leibler\ndivergence, does not possess the third. We provide empirical evidence\nsuggesting that this is a serious issue in practice. Leveraging insights from\nprobabilistic forecasting we propose an alternative to the Wasserstein metric,\nthe Cram\\'er distance. We show that the Cram\\'er distance possesses all three\ndesired properties, combining the best of the Wasserstein and Kullback-Leibler\ndivergences. To illustrate the relevance of the Cram\\'er distance in practice\nwe design a new algorithm, the Cram\\'er Generative Adversarial Network (GAN),\nand show that it performs significantly better than the related Wasserstein\nGAN.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 16:53:12 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Bellemare", "Marc G.", ""], ["Danihelka", "Ivo", ""], ["Dabney", "Will", ""], ["Mohamed", "Shakir", ""], ["Lakshminarayanan", "Balaji", ""], ["Hoyer", "Stephan", ""], ["Munos", "R\u00e9mi", ""]]}, {"id": "1705.10750", "submitter": "Junier Oliva", "authors": "Junier B. Oliva, Kumar Avinava Dubey, Barnabas Poczos, Eric Xing, Jeff\n  Schneider", "title": "Recurrent Estimation of Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the recurrent estimation of distributions (RED) for\nmodeling real-valued data in a semiparametric fashion. RED models make two\nnovel uses of recurrent neural networks (RNNs) for density estimation of\ngeneral real-valued data. First, RNNs are used to transform input covariates\ninto a latent space to better capture conditional dependencies in inputs.\nAfter, an RNN is used to compute the conditional distributions of the latent\ncovariates. The resulting model is efficient to train, compute, and sample\nfrom, whilst producing normalized pdfs. The effectiveness of RED is shown via\nseveral real-world data experiments. Our results show that RED models achieve a\nlower held-out negative log-likelihood than other neural network approaches\nacross multiple dataset sizes and dimensionalities. Further context of the\nefficacy of RED is provided by considering anomaly detection tasks, where we\nalso observe better performance over alternative models.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 17:00:59 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Oliva", "Junier B.", ""], ["Dubey", "Kumar Avinava", ""], ["Poczos", "Barnabas", ""], ["Xing", "Eric", ""], ["Schneider", "Jeff", ""]]}, {"id": "1705.10757", "submitter": "Yongcan Cao", "authors": "Samuel Silva, Rengan Suresh, Feng Tao, Johnathan Votion, Yongcan Cao", "title": "A Multi-Layer K-means Approach for Multi-Sensor Data Pattern Recognition\n  in Multi-Target Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-target association is an important step in multi-target localization for\nthe intelligent operation of un- manned systems in numerous applications such\nas search and rescue, traffic management and surveillance. The objective of\nthis paper is to present an innovative data association learning approach named\nmulti-layer K-means (MLKM) based on leveraging the advantages of some existing\nmachine learning approaches, including K-means, K-means++, and deep neural\nnetworks. To enable the accurate data association from different sensors for\nefficient target localization, MLKM relies on the clustering capabilities of\nK-means++ structured in a multi-layer framework with the error correction\nfeature that is motivated by the backpropogation that is well-known in deep\nlearning research. To show the effectiveness of the MLKM method, numerous\nsimulation examples are conducted to compare its performance with K-means,\nK-means++, and deep neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 17:13:05 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Silva", "Samuel", ""], ["Suresh", "Rengan", ""], ["Tao", "Feng", ""], ["Votion", "Johnathan", ""], ["Cao", "Yongcan", ""]]}, {"id": "1705.10762", "submitter": "Ramakrishna Vedantam", "authors": "Ramakrishna Vedantam, Ian Fischer, Jonathan Huang, Kevin Murphy", "title": "Generative Models of Visually Grounded Imagination", "comments": "International Conference on Learning Representations (ICLR), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is easy for people to imagine what a man with pink hair looks like, even\nif they have never seen such a person before. We call the ability to create\nimages of novel semantic concepts visually grounded imagination. In this paper,\nwe show how we can modify variational auto-encoders to perform this task. Our\nmethod uses a novel training objective, and a novel product-of-experts\ninference network, which can handle partially specified (abstract) concepts in\na principled and efficient way. We also propose a set of easy-to-compute\nevaluation metrics that capture our intuitive notions of what it means to have\ngood visual imagination, namely correctness, coverage, and compositionality\n(the 3 C's). Finally, we perform a detailed comparison of our method with two\nexisting joint image-attribute VAE methods (the JMVAE method of Suzuki et.al.\nand the BiVCCA method of Wang et.al.) by applying them to two datasets: the\nMNIST-with-attributes dataset (which we introduce here), and the CelebA\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 17:32:26 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 22:38:31 GMT"}, {"version": "v3", "created": "Sat, 28 Oct 2017 21:10:46 GMT"}, {"version": "v4", "created": "Thu, 2 Nov 2017 18:38:45 GMT"}, {"version": "v5", "created": "Wed, 15 Nov 2017 04:58:10 GMT"}, {"version": "v6", "created": "Mon, 12 Feb 2018 05:04:18 GMT"}, {"version": "v7", "created": "Sun, 25 Feb 2018 18:14:07 GMT"}, {"version": "v8", "created": "Fri, 9 Nov 2018 08:16:22 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Vedantam", "Ramakrishna", ""], ["Fischer", "Ian", ""], ["Huang", "Jonathan", ""], ["Murphy", "Kevin", ""]]}, {"id": "1705.10770", "submitter": "Giorgos Borboudakis", "authors": "Giorgos Borboudakis and Ioannis Tsamardinos", "title": "Forward-Backward Selection with Early Dropping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forward-backward selection is one of the most basic and commonly-used feature\nselection algorithms available. It is also general and conceptually applicable\nto many different types of data. In this paper, we propose a heuristic that\nsignificantly improves its running time, while preserving predictive accuracy.\nThe idea is to temporarily discard the variables that are conditionally\nindependent with the outcome given the selected variable set. Depending on how\nthose variables are reconsidered and reintroduced, this heuristic gives rise to\na family of algorithms with increasingly stronger theoretical guarantees. In\ndistributions that can be faithfully represented by Bayesian networks or\nmaximal ancestral graphs, members of this algorithmic family are able to\ncorrectly identify the Markov blanket in the sample limit. In experiments we\nshow that the proposed heuristic increases computational efficiency by about\ntwo orders of magnitude in high-dimensional problems, while selecting fewer\nvariables and retaining predictive performance. Furthermore, we show that the\nproposed algorithm and feature selection with LASSO perform similarly when\nrestricted to select the same number of variables, making the proposed\nalgorithm an attractive alternative for problems where no (efficient) algorithm\nfor LASSO exists.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 17:47:56 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Borboudakis", "Giorgos", ""], ["Tsamardinos", "Ioannis", ""]]}, {"id": "1705.10813", "submitter": "Vladimir Feinberg", "authors": "Vladimir Feinberg, Li-Fang Cheng, Kai Li, Barbara E Engelhardt", "title": "Large Linear Multi-output Gaussian Process Learning", "comments": "9 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs), or distributions over arbitrary functions in a\ncontinuous domain, can be generalized to the multi-output case: a linear model\nof coregionalization (LMC) is one approach. LMCs estimate and exploit\ncorrelations across the multiple outputs. While model estimation can be\nperformed efficiently for single-output GPs, these assume stationarity, but in\nthe multi-output case the cross-covariance interaction is not stationary. We\npropose Large Linear GP (LLGP), which circumvents the need for stationarity by\ninducing structure in the LMC kernel through a common grid of inputs shared\nbetween outputs, enabling optimization of GP hyperparameters for\nmulti-dimensional outputs and low-dimensional inputs. When applied to synthetic\ntwo-dimensional and real time series data, we find our theoretical improvement\nrelative to the current solutions for multi-output GPs is realized with LLGP\nreducing training time while improving or maintaining predictive mean accuracy.\nMoreover, by using a direct likelihood approximation rather than a variational\none, model confidence estimates are significantly improved.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 18:19:16 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 17:02:01 GMT"}, {"version": "v3", "created": "Mon, 23 Oct 2017 16:15:16 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Feinberg", "Vladimir", ""], ["Cheng", "Li-Fang", ""], ["Li", "Kai", ""], ["Engelhardt", "Barbara E", ""]]}, {"id": "1705.10817", "submitter": "Leonardo Gutierrez", "authors": "Leonardo Gutierrez Gomez, Benjamin Chiem, Jean-Charles Delvenne", "title": "Dynamics Based Features For Graph Classification", "comments": "This paper is under review as a conference paper at ECML-PKDD 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous social, medical, engineering and biological challenges can be framed\nas graph-based learning tasks. Here, we propose a new feature based approach to\nnetwork classification. We show how dynamics on a network can be useful to\nreveal patterns about the organization of the components of the underlying\ngraph where the process takes place. We define generalized assortativities on\nnetworks and use them as generalized features across multiple time scales.\nThese features turn out to be suitable signatures for discriminating between\ndifferent classes of networks. Our method is evaluated empirically on\nestablished network benchmarks. We also introduce a new dataset of human brain\nnetworks (connectomes) and use it to evaluate our method. Results reveal that\nour dynamics based features are competitive and often outperform state of the\nart accuracies.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 18:33:44 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Gomez", "Leonardo Gutierrez", ""], ["Chiem", "Benjamin", ""], ["Delvenne", "Jean-Charles", ""]]}, {"id": "1705.10819", "submitter": "Joan Bruna", "authors": "Ilya Kostrikov, Zhongshi Jiang, Daniele Panozzo, Denis Zorin, Joan\n  Bruna", "title": "Surface Networks", "comments": null, "journal-ref": "CVPR 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study data-driven representations for three-dimensional triangle meshes,\nwhich are one of the prevalent objects used to represent 3D geometry. Recent\nworks have developed models that exploit the intrinsic geometry of manifolds\nand graphs, namely the Graph Neural Networks (GNNs) and its spectral variants,\nwhich learn from the local metric tensor via the Laplacian operator. Despite\noffering excellent sample complexity and built-in invariances, intrinsic\ngeometry alone is invariant to isometric deformations, making it unsuitable for\nmany applications. To overcome this limitation, we propose several upgrades to\nGNNs to leverage extrinsic differential geometry properties of\nthree-dimensional surfaces, increasing its modeling power.\n  In particular, we propose to exploit the Dirac operator, whose spectrum\ndetects principal curvature directions --- this is in stark contrast with the\nclassical Laplace operator, which directly measures mean curvature. We coin the\nresulting models \\emph{Surface Networks (SN)}. We prove that these models\ndefine shape representations that are stable to deformation and to\ndiscretization, and we demonstrate the efficiency and versatility of SNs on two\nchallenging tasks: temporal prediction of mesh deformations under non-linear\ndynamics and generative models using a variational autoencoder framework with\nencoders/decoders given by SNs.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 18:40:47 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 14:42:28 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Kostrikov", "Ilya", ""], ["Jiang", "Zhongshi", ""], ["Panozzo", "Daniele", ""], ["Zorin", "Denis", ""], ["Bruna", "Joan", ""]]}, {"id": "1705.10843", "submitter": "Benjamin Sanchez-Lengeling", "authors": "Gabriel Lima Guimaraes, Benjamin Sanchez-Lengeling, Carlos Outeiral,\n  Pedro Luis Cunha Farias, Al\\'an Aspuru-Guzik", "title": "Objective-Reinforced Generative Adversarial Networks (ORGAN) for\n  Sequence Generation Models", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In unsupervised data generation tasks, besides the generation of a sample\nbased on previous observations, one would often like to give hints to the model\nin order to bias the generation towards desirable metrics. We propose a method\nthat combines Generative Adversarial Networks (GANs) and reinforcement learning\n(RL) in order to accomplish exactly that. While RL biases the data generation\nprocess towards arbitrary metrics, the GAN component of the reward function\nensures that the model still remembers information learned from data. We build\nupon previous results that incorporated GANs and RL in order to generate\nsequence data and test this model in several settings for the generation of\nmolecules encoded as text sequences (SMILES) and in the context of music\ngeneration, showing for each case that we can effectively bias the generation\nprocess towards desired metrics.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 19:58:08 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 18:41:15 GMT"}, {"version": "v3", "created": "Wed, 7 Feb 2018 04:58:57 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Guimaraes", "Gabriel Lima", ""], ["Sanchez-Lengeling", "Benjamin", ""], ["Outeiral", "Carlos", ""], ["Farias", "Pedro Luis Cunha", ""], ["Aspuru-Guzik", "Al\u00e1n", ""]]}, {"id": "1705.10865", "submitter": "Xiaotong Suo", "authors": "Xiaotong Suo, Victor Minden, Bradley Nelson, Robert Tibshirani,\n  Michael Saunders", "title": "Sparse canonical correlation analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical correlation analysis was proposed by Hotelling [6] and it measures\nlinear relationship between two multidimensional variables. In high dimensional\nsetting, the classical canonical correlation analysis breaks down. We propose a\nsparse canonical correlation analysis by adding l1 constraints on the canonical\nvectors and show how to solve it efficiently using linearized alternating\ndirection method of multipliers (ADMM) and using TFOCS as a black box. We\nillustrate this idea on simulated data.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 20:54:17 GMT"}, {"version": "v2", "created": "Fri, 2 Jun 2017 20:04:55 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Suo", "Xiaotong", ""], ["Minden", "Victor", ""], ["Nelson", "Bradley", ""], ["Tibshirani", "Robert", ""], ["Saunders", "Michael", ""]]}, {"id": "1705.10882", "submitter": "David Rolnick", "authors": "David Rolnick, Yaron Meirovitch, Toufiq Parag, Hanspeter Pfister,\n  Viren Jain, Jeff W. Lichtman, Edward S. Boyden, Nir Shavit", "title": "Morphological Error Detection in 3D Segmentations", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning algorithms for connectomics rely upon localized classification,\nrather than overall morphology. This leads to a high incidence of erroneously\nmerged objects. Humans, by contrast, can easily detect such errors by acquiring\nintuition for the correct morphology of objects. Biological neurons have\ncomplicated and variable shapes, which are challenging to learn, and merge\nerrors take a multitude of different forms. We present an algorithm, MergeNet,\nthat shows 3D ConvNets can, in fact, detect merge errors from high-level\nneuronal morphology. MergeNet follows unsupervised training and operates across\ndatasets. We demonstrate the performance of MergeNet both on a variety of\nconnectomics data and on a dataset created from merged MNIST images.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 22:25:44 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Rolnick", "David", ""], ["Meirovitch", "Yaron", ""], ["Parag", "Toufiq", ""], ["Pfister", "Hanspeter", ""], ["Jain", "Viren", ""], ["Lichtman", "Jeff W.", ""], ["Boyden", "Edward S.", ""], ["Shavit", "Nir", ""]]}, {"id": "1705.10883", "submitter": "Velibor Mi\\v{s}i\\'c", "authors": "Velibor V. Mi\\v{s}i\\'c", "title": "Optimization of Tree Ensembles", "comments": "49 pages, 10 figures; to appear in Operations Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree ensemble models such as random forests and boosted trees are among the\nmost widely used and practically successful predictive models in applied\nmachine learning and business analytics. Although such models have been used to\nmake predictions based on exogenous, uncontrollable independent variables, they\nare increasingly being used to make predictions where the independent variables\nare controllable and are also decision variables. In this paper, we study the\nproblem of tree ensemble optimization: given a tree ensemble that predicts some\ndependent variable using controllable independent variables, how should we set\nthese variables so as to maximize the predicted value? We formulate the problem\nas a mixed-integer optimization problem. We theoretically examine the strength\nof our formulation, provide a hierarchy of approximate formulations with bounds\non approximation quality and exploit the structure of the problem to develop\ntwo large-scale solution methods, one based on Benders decomposition and one\nbased on iteratively generating tree split constraints. We test our methodology\non real data sets, including two case studies in drug design and customized\npricing, and show that our methodology can efficiently solve large-scale\ninstances to near or full optimality, and outperforms solutions obtained by\nheuristic approaches. In our drug design case, we show how our approach can\nidentify compounds that efficiently trade-off predicted performance and novelty\nwith respect to existing, known compounds. In our customized pricing case, we\nshow how our approach can efficiently determine optimal store-level prices\nunder a random forest model that delivers excellent predictive accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 22:37:22 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 21:03:32 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Mi\u0161i\u0107", "Velibor V.", ""]]}, {"id": "1705.10886", "submitter": "Qilong Gu", "authors": "Qilong Gu and Arindam Banerjee", "title": "High Dimensional Structured Superposition Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional superposition models characterize observations using\nparameters which can be written as a sum of multiple component parameters, each\nwith its own structure, e.g., sum of low rank and sparse matrices, sum of\nsparse and rotated sparse vectors, etc. In this paper, we consider general\nsuperposition models which allow sum of any number of component parameters, and\neach component structure can be characterized by any norm. We present a simple\nestimator for such models, give a geometric condition under which the\ncomponents can be accurately estimated, characterize sample complexity of the\nestimator, and give high probability non-asymptotic bounds on the componentwise\nestimation error. We use tools from empirical processes and generic chaining\nfor the statistical analysis, and our results, which substantially generalize\nprior work on superposition models, are in terms of Gaussian widths of suitable\nsets.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 23:00:34 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Gu", "Qilong", ""], ["Banerjee", "Arindam", ""]]}, {"id": "1705.10887", "submitter": "Javier Turek", "authors": "Javier S. Turek, Alexander Huth", "title": "Efficient, sparse representation of manifold distance matrices for\n  classical scaling", "comments": "Conference CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geodesic distance matrices can reveal shape properties that are largely\ninvariant to non-rigid deformations, and thus are often used to analyze and\nrepresent 3-D shapes. However, these matrices grow quadratically with the\nnumber of points. Thus for large point sets it is common to use a low-rank\napproximation to the distance matrix, which fits in memory and can be\nefficiently analyzed using methods such as multidimensional scaling (MDS). In\nthis paper we present a novel sparse method for efficiently representing\ngeodesic distance matrices using biharmonic interpolation. This method exploits\nknowledge of the data manifold to learn a sparse interpolation operator that\napproximates distances using a subset of points. We show that our method is 2x\nfaster and uses 20x less memory than current leading methods for solving MDS on\nlarge point sets, with similar quality. This enables analyses of large point\nsets that were previously infeasible.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 23:18:18 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 17:35:03 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Turek", "Javier S.", ""], ["Huth", "Alexander", ""]]}, {"id": "1705.10888", "submitter": "Stefanos Eleftheriadis PhD", "authors": "Stefanos Eleftheriadis, Thomas F.W. Nicholson, Marc Peter Deisenroth,\n  James Hensman", "title": "Identification of Gaussian Process State Space Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gaussian process state space model (GPSSM) is a non-linear dynamical\nsystem, where unknown transition and/or measurement mappings are described by\nGPs. Most research in GPSSMs has focussed on the state estimation problem,\ni.e., computing a posterior of the latent state given the model. However, the\nkey challenge in GPSSMs has not been satisfactorily addressed yet: system\nidentification, i.e., learning the model. To address this challenge, we impose\na structured Gaussian variational posterior distribution over the latent\nstates, which is parameterised by a recognition model in the form of a\nbi-directional recurrent neural network. Inference with this structure allows\nus to recover a posterior smoothed over sequences of data. We provide a\npractical algorithm for efficiently computing a lower bound on the marginal\nlikelihood using the reparameterisation trick. This further allows for the use\nof arbitrary kernels within the GPSSM. We demonstrate that the learnt GPSSM can\nefficiently generate plausible future trajectories of the identified system\nafter only observing a small number of episodes from the true system.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 23:27:44 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 18:08:39 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Eleftheriadis", "Stefanos", ""], ["Nicholson", "Thomas F. W.", ""], ["Deisenroth", "Marc Peter", ""], ["Hensman", "James", ""]]}, {"id": "1705.10915", "submitter": "Emily Denton", "authors": "Emily Denton, Vighnesh Birodkar", "title": "Unsupervised Learning of Disentangled Representations from Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new model DrNET that learns disentangled image representations\nfrom video. Our approach leverages the temporal coherence of video and a novel\nadversarial loss to learn a representation that factorizes each frame into a\nstationary part and a temporally varying component. The disentangled\nrepresentation can be used for a range of tasks. For example, applying a\nstandard LSTM to the time-vary components enables prediction of future frames.\nWe evaluate our approach on a range of synthetic and real videos, demonstrating\nthe ability to coherently generate hundreds of steps into the future.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 02:12:19 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Denton", "Emily", ""], ["Birodkar", "Vighnesh", ""]]}, {"id": "1705.10918", "submitter": "Nikolaos Sahinidis", "authors": "Zachary T. Wilson and Nikolaos V. Sahinidis", "title": "The ALAMO approach to machine learning", "comments": null, "journal-ref": null, "doi": "10.1016/j.compchemeng.2017.02.010", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ALAMO is a computational methodology for leaning algebraic functions from\ndata. Given a data set, the approach begins by building a low-complexity,\nlinear model composed of explicit non-linear transformations of the independent\nvariables. Linear combinations of these non-linear transformations allow a\nlinear model to better approximate complex behavior observed in real processes.\nThe model is refined, as additional data are obtained in an adaptive fashion\nthrough error maximization sampling using derivative-free optimization. Models\nbuilt using ALAMO can enforce constraints on the response variables to\nincorporate first-principles knowledge. The ability of ALAMO to generate simple\nand accurate models for a number of reaction problems is demonstrated. The\nerror maximization sampling is compared with Latin hypercube designs to\ndemonstrate its sampling efficiency. ALAMO's constrained regression methodology\nis used to further refine concentration models, resulting in models that\nperform better on validation data and satisfy upper and lower bounds placed on\nmodel outputs.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 02:28:04 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Wilson", "Zachary T.", ""], ["Sahinidis", "Nikolaos V.", ""]]}, {"id": "1705.10924", "submitter": "Feng Nan", "authors": "Henghui Zhu, Feng Nan, Ioannis Paschalidis, Venkatesh Saligrama", "title": "Sequential Dynamic Decision Making with Deep Neural Nets on a Test-Time\n  Budget", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network (DNN) based approaches hold significant potential for\nreinforcement learning (RL) and have already shown remarkable gains over\nstate-of-art methods in a number of applications. The effectiveness of DNN\nmethods can be attributed to leveraging the abundance of supervised data to\nlearn value functions, Q-functions, and policy function approximations without\nthe need for feature engineering. Nevertheless, the deployment of DNN-based\npredictors with very deep architectures can pose an issue due to computational\nand other resource constraints at test-time in a number of applications. We\npropose a novel approach for reducing the average latency by learning a\ncomputationally efficient gating function that is capable of recognizing states\nin a sequential decision process for which policy prescriptions of a shallow\nnetwork suffices and deeper layers of the DNN have little marginal utility. The\noverall system is adaptive in that it dynamically switches control actions\nbased on state-estimates in order to reduce average latency without sacrificing\nterminal performance. We experiment with a number of alternative loss-functions\nto train gating functions and shallow policies and show that in a number of\napplications a speed-up of up to almost 5X can be obtained with little loss in\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 02:45:55 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Zhu", "Henghui", ""], ["Nan", "Feng", ""], ["Paschalidis", "Ioannis", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1705.10929", "submitter": "Sandeep Subramanian", "authors": "Sai Rajeswar, Sandeep Subramanian, Francis Dutil, Christopher Pal,\n  Aaron Courville", "title": "Adversarial Generation of Natural Language", "comments": "11 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have gathered a lot of attention from\nthe computer vision community, yielding impressive results for image\ngeneration. Advances in the adversarial generation of natural language from\nnoise however are not commensurate with the progress made in generating images,\nand still lag far behind likelihood based methods. In this paper, we take a\nstep towards generating natural language with a GAN objective alone. We\nintroduce a simple baseline that addresses the discrete output space problem\nwithout relying on gradient estimators and show that it is able to achieve\nstate-of-the-art results on a Chinese poem generation dataset. We present\nquantitative results on generating sentences from context-free and\nprobabilistic context-free grammars, and qualitative language modeling results.\nA conditional version is also described that can generate sequences conditioned\non sentence characteristics.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 03:06:39 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Rajeswar", "Sai", ""], ["Subramanian", "Sandeep", ""], ["Dutil", "Francis", ""], ["Pal", "Christopher", ""], ["Courville", "Aaron", ""]]}, {"id": "1705.10934", "submitter": "Eduardo Pavez", "authors": "Eduardo Pavez, Hilmi E. Egilmez, Antonio Ortega", "title": "Learning Graphs with Monotone Topology Properties and Multiple Connected\n  Components", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2018.2813337", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent papers have formulated the problem of learning graphs from data as an\ninverse covariance estimation with graph Laplacian constraints. While such\nproblems are convex, existing methods cannot guarantee that solutions will have\nspecific graph topology properties (e.g., being $k$-partite), which are\ndesirable for some applications. In fact, the problem of learning a graph with\ngiven topology properties, e.g., finding the $k$-partite graph that best\nmatches the data, is in general non-convex. In this paper, we develop novel\ntheoretical results that provide performance guarantees for an approach to\nsolve these problems. Our solution decomposes this problem into two\nsub-problems, for which efficient solutions are known. Specifically, a graph\ntopology inference (GTI) step is employed to select a feasible graph topology,\ni.e., one having the desired property. Then, a graph weight estimation (GWE)\nstep is performed by solving a generalized graph Laplacian estimation problem,\nwhere edges are constrained by the topology found in the GTI step. Our main\nresult is a bound on the error of the GWE step as a function of the error in\nthe GTI step. This error bound indicates that the GTI step should be solved\nusing an algorithm that approximates the similarity matrix by another matrix\nwhose entries have been thresholded to zero to have the desired type of graph\ntopology. The GTI stage can leverage existing methods (e.g., state of the art\napproaches for graph coloring) which are typically based on minimizing the\ntotal weight of removed edges. Since the GWE stage is formulated as an inverse\ncovariance estimation problem with linear constraints, it can be solved using\nexisting convex optimization methods. We demonstrate that our two step approach\ncan achieve good results for both synthetic and texture image data.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 03:38:14 GMT"}, {"version": "v2", "created": "Sat, 10 Jun 2017 00:01:48 GMT"}, {"version": "v3", "created": "Fri, 22 Dec 2017 20:35:15 GMT"}, {"version": "v4", "created": "Wed, 28 Feb 2018 06:47:25 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Pavez", "Eduardo", ""], ["Egilmez", "Hilmi E.", ""], ["Ortega", "Antonio", ""]]}, {"id": "1705.10941", "submitter": "Yuichi Yoshida", "authors": "Yuichi Yoshida and Takeru Miyato", "title": "Spectral Norm Regularization for Improving the Generalizability of Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the generalizability of deep learning based on the sensitivity\nto input perturbation. We hypothesize that the high sensitivity to the\nperturbation of data degrades the performance on it. To reduce the sensitivity\nto perturbation, we propose a simple and effective regularization method,\nreferred to as spectral norm regularization, which penalizes the high spectral\nnorm of weight matrices in neural networks. We provide supportive evidence for\nthe abovementioned hypothesis by experimentally confirming that the models\ntrained using spectral norm regularization exhibit better generalizability than\nother baseline methods.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 04:56:25 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Yoshida", "Yuichi", ""], ["Miyato", "Takeru", ""]]}, {"id": "1705.10958", "submitter": "Alessandro Rudi", "authors": "Alessandro Rudi, Luigi Carratino and Lorenzo Rosasco", "title": "FALKON: An Optimal Large Scale Kernel Method", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel methods provide a principled way to perform non linear, nonparametric\nlearning. They rely on solid functional analytic foundations and enjoy optimal\nstatistical properties. However, at least in their basic form, they have\nlimited applicability in large scale scenarios because of stringent\ncomputational requirements in terms of time and especially memory. In this\npaper, we take a substantial step in scaling up kernel methods, proposing\nFALKON, a novel algorithm that allows to efficiently process millions of\npoints. FALKON is derived combining several algorithmic principles, namely\nstochastic subsampling, iterative solvers and preconditioning. Our theoretical\nanalysis shows that optimal statistical accuracy is achieved requiring\nessentially $O(n)$ memory and $O(n\\sqrt{n})$ time. An extensive experimental\nanalysis on large scale datasets shows that, even with a single machine, FALKON\noutperforms previous state of the art solutions, which exploit\nparallel/distributed architectures.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 06:56:53 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 10:43:36 GMT"}, {"version": "v3", "created": "Wed, 31 Jan 2018 18:35:40 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Rudi", "Alessandro", ""], ["Carratino", "Luigi", ""], ["Rosasco", "Lorenzo", ""]]}, {"id": "1705.10993", "submitter": "Julien Perez", "authors": "Julien Perez and Tomi Silander", "title": "Non-Markovian Control with Gated End-to-End Memory Policy Networks", "comments": "11 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partially observable environments present an important open challenge in the\ndomain of sequential control learning with delayed rewards. Despite numerous\nattempts during the two last decades, the majority of reinforcement learning\nalgorithms and associated approximate models, applied to this context, still\nassume Markovian state transitions. In this paper, we explore the use of a\nrecently proposed attention-based model, the Gated End-to-End Memory Network,\nfor sequential control. We call the resulting model the Gated End-to-End Memory\nPolicy Network. More precisely, we use a model-free value-based algorithm to\nlearn policies for partially observed domains using this memory-enhanced neural\nnetwork. This model is end-to-end learnable and it features unbounded memory.\nIndeed, because of its attention mechanism and associated non-parametric\nmemory, the proposed model allows us to define an attention mechanism over the\nobservation stream unlike recurrent models. We show encouraging results that\nillustrate the capability of our attention-based model in the context of the\ncontinuous-state non-stationary control problem of stock trading. We also\npresent an OpenAI Gym environment for simulated stock exchange and explain its\nrelevance as a benchmark for the field of non-Markovian decision process\nlearning.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 09:00:44 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Perez", "Julien", ""], ["Silander", "Tomi", ""]]}, {"id": "1705.11041", "submitter": "Francesco Locatello", "authors": "Francesco Locatello, Michael Tschannen, Gunnar R\\\"atsch, Martin Jaggi", "title": "Greedy Algorithms for Cone Constrained Optimization with Convergence\n  Guarantees", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Greedy optimization methods such as Matching Pursuit (MP) and Frank-Wolfe\n(FW) algorithms regained popularity in recent years due to their simplicity,\neffectiveness and theoretical guarantees. MP and FW address optimization over\nthe linear span and the convex hull of a set of atoms, respectively. In this\npaper, we consider the intermediate case of optimization over the convex cone,\nparametrized as the conic hull of a generic atom set, leading to the first\nprincipled definitions of non-negative MP algorithms for which we give explicit\nconvergence rates and demonstrate excellent empirical performance. In\nparticular, we derive sublinear ($\\mathcal{O}(1/t)$) convergence on general\nsmooth and convex objectives, and linear convergence ($\\mathcal{O}(e^{-t})$) on\nstrongly convex objectives, in both cases for general sets of atoms.\nFurthermore, we establish a clear correspondence of our algorithms to known\nalgorithms from the MP and FW literature. Our novel algorithms and analyses\ntarget general atom sets and general objective functions, and hence are\ndirectly applicable to a large variety of learning settings.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 11:47:55 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 07:57:03 GMT"}, {"version": "v3", "created": "Sun, 19 Nov 2017 16:17:07 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Locatello", "Francesco", ""], ["Tschannen", "Michael", ""], ["R\u00e4tsch", "Gunnar", ""], ["Jaggi", "Martin", ""]]}, {"id": "1705.11140", "submitter": "Christian A. Naesseth", "authors": "Christian A. Naesseth and Scott W. Linderman and Rajesh Ranganath and\n  David M. Blei", "title": "Variational Sequential Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent advances in large scale probabilistic inference rely on\nvariational methods. The success of variational approaches depends on (i)\nformulating a flexible parametric family of distributions, and (ii) optimizing\nthe parameters to find the member of this family that most closely approximates\nthe exact posterior. In this paper we present a new approximating family of\ndistributions, the variational sequential Monte Carlo (VSMC) family, and show\nhow to optimize it in variational inference. VSMC melds variational inference\n(VI) and sequential Monte Carlo (SMC), providing practitioners with flexible,\naccurate, and powerful Bayesian inference. The VSMC family is a variational\nfamily that can approximate the posterior arbitrarily well, while still\nallowing for efficient optimization of its parameters. We demonstrate its\nutility on state space models, stochastic volatility models for financial data,\nand deep Markov models of brain neural circuits.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 15:23:42 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2018 08:58:41 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Naesseth", "Christian A.", ""], ["Linderman", "Scott W.", ""], ["Ranganath", "Rajesh", ""], ["Blei", "David M.", ""]]}, {"id": "1705.11146", "submitter": "Friedemann Zenke", "authors": "Friedemann Zenke and Surya Ganguli", "title": "SuperSpike: Supervised learning in multi-layer spiking neural networks", "comments": null, "journal-ref": null, "doi": "10.1162/neco_a_01086", "report-no": null, "categories": "q-bio.NC cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A vast majority of computation in the brain is performed by spiking neural\nnetworks. Despite the ubiquity of such spiking, we currently lack an\nunderstanding of how biological spiking neural circuits learn and compute\nin-vivo, as well as how we can instantiate such capabilities in artificial\nspiking circuits in-silico. Here we revisit the problem of supervised learning\nin temporally coding multi-layer spiking neural networks. First, by using a\nsurrogate gradient approach, we derive SuperSpike, a nonlinear voltage-based\nthree factor learning rule capable of training multi-layer networks of\ndeterministic integrate-and-fire neurons to perform nonlinear computations on\nspatiotemporal spike patterns. Second, inspired by recent results on feedback\nalignment, we compare the performance of our learning rule under different\ncredit assignment strategies for propagating output errors to hidden units.\nSpecifically, we test uniform, symmetric and random feedback, finding that\nsimpler tasks can be solved with any type of feedback, while more complex tasks\nrequire symmetric feedback. In summary, our results open the door to obtaining\na better scientific understanding of learning and computation in spiking neural\nnetworks by advancing our ability to train them to solve nonlinear problems\ninvolving transformations between different spatiotemporal spike-time patterns.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 15:31:26 GMT"}, {"version": "v2", "created": "Sat, 14 Oct 2017 15:08:04 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Zenke", "Friedemann", ""], ["Ganguli", "Surya", ""]]}]