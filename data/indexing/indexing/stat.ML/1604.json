[{"id": "1604.00126", "submitter": "Ardavan Saeedi", "authors": "Kayhan Batmanghelich, Ardavan Saeedi, Karthik Narasimhan, Sam Gershman", "title": "Nonparametric Spherical Topic Modeling with Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional topic models do not account for semantic regularities in\nlanguage. Recent distributional representations of words exhibit semantic\nconsistency over directional metrics such as cosine similarity. However,\nneither categorical nor Gaussian observational distributions used in existing\ntopic models are appropriate to leverage such correlations. In this paper, we\npropose to use the von Mises-Fisher distribution to model the density of words\nover a unit sphere. Such a representation is well-suited for directional data.\nWe use a Hierarchical Dirichlet Process for our base topic model and propose an\nefficient inference algorithm based on Stochastic Variational Inference. This\nmodel enables us to naturally exploit the semantic structures of word\nembeddings while flexibly discovering the number of topics. Experiments\ndemonstrate that our method outperforms competitive approaches in terms of\ntopic coherence on two different text corpora while offering efficient\ninference.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 04:36:58 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Batmanghelich", "Kayhan", ""], ["Saeedi", "Ardavan", ""], ["Narasimhan", "Karthik", ""], ["Gershman", "Sam", ""]]}, {"id": "1604.00151", "submitter": "Arunselvan Ramaswamy", "authors": "Arunselvan Ramaswamy and Shalabh Bhatnagar", "title": "Analysis of gradient descent methods with non-diminishing, bounded\n  errors", "comments": "arXiv admin note: text overlap with arXiv:1502.01953, IEEE\n  Transactions on Automatic Control, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main aim of this paper is to provide an analysis of gradient descent (GD)\nalgorithms with gradient errors that do not necessarily vanish, asymptotically.\nIn particular, sufficient conditions are presented for both stability (almost\nsure boundedness of the iterates) and convergence of GD with bounded,\n(possibly) non-diminishing gradient errors. In addition to ensuring stability,\nsuch an algorithm is shown to converge to a small neighborhood of the minimum\nset, which depends on the gradient errors. It is worth noting that the main\nresult of this paper can be used to show that GD with asymptotically vanishing\nerrors indeed converges to the minimum set. The results presented herein are\nnot only more general when compared to previous results, but our analysis of GD\nwith errors is new to the literature to the best of our knowledge. Our work\nextends the contributions of Mangasarian & Solodov, Bertsekas & Tsitsiklis and\nTadic & Doucet. Using our framework, a simple yet effective implementation of\nGD using simultaneous perturbation stochastic approximations (SP SA), with\nconstant sensitivity parameters, is presented. Another important improvement\nover many previous results is that there are no `additional' restrictions\nimposed on the step-sizes. In machine learning applications where step-sizes\nare related to learning rates, our assumptions, unlike those of other papers,\ndo not affect these learning rates. Finally, we present experimental results to\nvalidate our theory.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 07:03:46 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 14:36:07 GMT"}, {"version": "v3", "created": "Mon, 18 Sep 2017 08:56:56 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Ramaswamy", "Arunselvan", ""], ["Bhatnagar", "Shalabh", ""]]}, {"id": "1604.00169", "submitter": "Hildo Bijl", "authors": "Hildo Bijl, Thomas B. Sch\\\"on, Jan-Willem van Wingerden, and Michel\n  Verhaegen", "title": "A sequential Monte Carlo approach to Thompson sampling for Bayesian\n  optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization through Gaussian process regression is an effective\nmethod of optimizing an unknown function for which every measurement is\nexpensive. It approximates the objective function and then recommends a new\nmeasurement point to try out. This recommendation is usually selected by\noptimizing a given acquisition function. After a sufficient number of\nmeasurements, a recommendation about the maximum is made. However, a key\nrealization is that the maximum of a Gaussian process is not a deterministic\npoint, but a random variable with a distribution of its own. This distribution\ncannot be calculated analytically. Our main contribution is an algorithm,\ninspired by sequential Monte Carlo samplers, that approximates this maximum\ndistribution. Subsequently, by taking samples from this distribution, we enable\nThompson sampling to be applied to (armed-bandit) optimization problems with a\ncontinuous input space. All this is done without requiring the optimization of\na nonlinear acquisition function. Experiments have shown that the resulting\noptimization method has a competitive performance at keeping the cumulative\nregret limited.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 09:07:26 GMT"}, {"version": "v2", "created": "Wed, 6 Jul 2016 10:44:59 GMT"}, {"version": "v3", "created": "Tue, 16 May 2017 16:26:48 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Bijl", "Hildo", ""], ["Sch\u00f6n", "Thomas B.", ""], ["van Wingerden", "Jan-Willem", ""], ["Verhaegen", "Michel", ""]]}, {"id": "1604.00255", "submitter": "Tiago Peixoto", "authors": "Darko Hric, Tiago P. Peixoto, Santo Fortunato", "title": "Network structure, metadata and the prediction of missing nodes and\n  annotations", "comments": "15 pages, 6 figures, 1 table", "journal-ref": "Phys. Rev. X 6, 031038 (2016)", "doi": "10.1103/PhysRevX.6.031038", "report-no": null, "categories": "physics.soc-ph cs.SI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The empirical validation of community detection methods is often based on\navailable annotations on the nodes that serve as putative indicators of the\nlarge-scale network structure. Most often, the suitability of the annotations\nas topological descriptors itself is not assessed, and without this it is not\npossible to ultimately distinguish between actual shortcomings of the community\ndetection algorithms on one hand, and the incompleteness, inaccuracy or\nstructured nature of the data annotations themselves on the other. In this work\nwe present a principled method to access both aspects simultaneously. We\nconstruct a joint generative model for the data and metadata, and a\nnonparametric Bayesian framework to infer its parameters from annotated\ndatasets. We assess the quality of the metadata not according to its direct\nalignment with the network communities, but rather in its capacity to predict\nthe placement of edges in the network. We also show how this feature can be\nused to predict the connections to missing nodes when only the metadata is\navailable, as well as missing metadata. By investigating a wide range of\ndatasets, we show that while there are seldom exact agreements between metadata\ntokens and the inferred data groups, the metadata is often informative of the\nnetwork structure nevertheless, and can improve the prediction of missing\nnodes. This shows that the method uncovers meaningful patterns in both the data\nand metadata, without requiring or expecting a perfect agreement between the\ntwo.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 14:18:04 GMT"}, {"version": "v2", "created": "Thu, 29 Sep 2016 12:40:38 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Hric", "Darko", ""], ["Peixoto", "Tiago P.", ""], ["Fortunato", "Santo", ""]]}, {"id": "1604.00268", "submitter": "David Schwab", "authors": "DJ Strouse, David J Schwab", "title": "The deterministic information bottleneck", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.stat-mech cs.IT math.IT q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lossy compression and clustering fundamentally involve a decision about what\nfeatures are relevant and which are not. The information bottleneck method (IB)\nby Tishby, Pereira, and Bialek formalized this notion as an\ninformation-theoretic optimization problem and proposed an optimal tradeoff\nbetween throwing away as many bits as possible, and selectively keeping those\nthat are most important. In the IB, compression is measure my mutual\ninformation. Here, we introduce an alternative formulation that replaces mutual\ninformation with entropy, which we call the deterministic information\nbottleneck (DIB), that we argue better captures this notion of compression. As\nsuggested by its name, the solution to the DIB problem turns out to be a\ndeterministic encoder, or hard clustering, as opposed to the stochastic\nencoder, or soft clustering, that is optimal under the IB. We compare the IB\nand DIB on synthetic data, showing that the IB and DIB perform similarly in\nterms of the IB cost function, but that the DIB significantly outperforms the\nIB in terms of the DIB cost function. We also empirically find that the DIB\noffers a considerable gain in computational efficiency over the IB, over a\nrange of convergence parameters. Our derivation of the DIB also suggests a\nmethod for continuously interpolating between the soft clustering of the IB and\nthe hard clustering of the DIB.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 14:48:31 GMT"}, {"version": "v2", "created": "Mon, 19 Dec 2016 05:26:11 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Strouse", "DJ", ""], ["Schwab", "David J", ""]]}, {"id": "1604.00289", "submitter": "Brenden Lake", "authors": "Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, Samuel J.\n  Gershman", "title": "Building Machines That Learn and Think Like People", "comments": "In press at Behavioral and Brain Sciences. Open call for commentary\n  proposals (until Nov. 22, 2016).\n  https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/information/calls-for-commentary/open-calls-for-commentary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in artificial intelligence (AI) has renewed interest in\nbuilding systems that learn and think like people. Many advances have come from\nusing deep neural networks trained end-to-end in tasks such as object\nrecognition, video games, and board games, achieving performance that equals or\neven beats humans in some respects. Despite their biological inspiration and\nperformance achievements, these systems differ from human intelligence in\ncrucial ways. We review progress in cognitive science suggesting that truly\nhuman-like learning and thinking machines will have to reach beyond current\nengineering trends in both what they learn, and how they learn it.\nSpecifically, we argue that these machines should (a) build causal models of\nthe world that support explanation and understanding, rather than merely\nsolving pattern recognition problems; (b) ground learning in intuitive theories\nof physics and psychology, to support and enrich the knowledge that is learned;\nand (c) harness compositionality and learning-to-learn to rapidly acquire and\ngeneralize knowledge to new tasks and situations. We suggest concrete\nchallenges and promising routes towards these goals that can combine the\nstrengths of recent neural network advances with more structured cognitive\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 15:37:57 GMT"}, {"version": "v2", "created": "Sat, 7 May 2016 18:03:53 GMT"}, {"version": "v3", "created": "Wed, 2 Nov 2016 17:26:50 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Lake", "Brenden M.", ""], ["Ullman", "Tomer D.", ""], ["Tenenbaum", "Joshua B.", ""], ["Gershman", "Samuel J.", ""]]}, {"id": "1604.00647", "submitter": "Ernesto Diaz-Aviles", "authors": "Lucas Drumond, Ernesto Diaz-Aviles, and Lars Schmidt-Thieme", "title": "Multi-Relational Learning at Scale with ADMM", "comments": "Keywords: Multi-Relational Learning, Distributed Learning,\n  Factorization Models, ADMM", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from multiple-relational data which contains noise, ambiguities, or\nduplicate entities is essential to a wide range of applications such as\nstatistical inference based on Web Linked Data, recommender systems,\ncomputational biology, and natural language processing. These tasks usually\nrequire working with very large and complex datasets - e.g., the Web graph -\nhowever, current approaches to multi-relational learning are not practical for\nsuch scenarios due to their high computational complexity and poor scalability\non large data.\n  In this paper, we propose a novel and scalable approach for multi-relational\nfactorization based on consensus optimization. Our model, called ConsMRF, is\nbased on the Alternating Direction Method of Multipliers (ADMM) framework,\nwhich enables us to optimize each target relation using a smaller set of\nparameters than the state-of-the-art competitors in this task.\n  Due to ADMM's nature, ConsMRF can be easily parallelized which makes it\nsuitable for large multi-relational data. Experiments on large Web datasets -\nderived from DBpedia, Wikipedia and YAGO - show the efficiency and performance\nimprovement of ConsMRF over strong competitors. In addition, ConsMRF\nnear-linear scalability indicates great potential to tackle Web-scale problem\nsizes.\n", "versions": [{"version": "v1", "created": "Sun, 3 Apr 2016 15:42:36 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Drumond", "Lucas", ""], ["Diaz-Aviles", "Ernesto", ""], ["Schmidt-Thieme", "Lars", ""]]}, {"id": "1604.00653", "submitter": "Weiwei Pan", "authors": "W. Pan, F. Doshi-Velez", "title": "A Characterization of the Non-Uniqueness of Nonnegative Matrix\n  Factorizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF) is a popular dimension reduction\ntechnique that produces interpretable decomposition of the data into parts.\nHowever, this decompostion is not generally identifiable (even up to\npermutation and scaling). While other studies have provide criteria under which\nNMF is identifiable, we present the first (to our knowledge) characterization\nof the non-identifiability of NMF. We describe exactly when and how\nnon-uniqueness can occur, which has important implications for algorithms to\nefficiently discover alternate solutions, if they exist.\n", "versions": [{"version": "v1", "created": "Sun, 3 Apr 2016 16:32:01 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Pan", "W.", ""], ["Doshi-Velez", "F.", ""]]}, {"id": "1604.00772", "submitter": "Nikolaus Hansen", "authors": "Nikolaus Hansen (Inria)", "title": "The CMA Evolution Strategy: A Tutorial", "comments": "ArXiv e-prints, arXiv:1604.xxxxx", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This tutorial introduces the CMA Evolution Strategy (ES), where CMA stands\nfor Covariance Matrix Adaptation. The CMA-ES is a stochastic, or randomized,\nmethod for real-parameter (continuous domain) optimization of non-linear,\nnon-convex functions. We try to motivate and derive the algorithm from\nintuitive concepts and from requirements of non-linear, non-convex search in\ncontinuous domain.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 08:16:12 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Hansen", "Nikolaus", "", "Inria"]]}, {"id": "1604.00974", "submitter": "Luiz Gustavo Hafemann", "authors": "Luiz G. Hafemann, Robert Sabourin, Luiz S. Oliveira", "title": "Writer-independent Feature Learning for Offline Signature Verification\n  using Deep Convolutional Neural Networks", "comments": "Accepted as a conference paper to The International Joint Conference\n  on Neural Networks (IJCNN) 2016", "journal-ref": null, "doi": "10.1109/IJCNN.2016.7727521", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic Offline Handwritten Signature Verification has been researched over\nthe last few decades from several perspectives, using insights from graphology,\ncomputer vision, signal processing, among others. In spite of the advancements\non the field, building classifiers that can separate between genuine signatures\nand skilled forgeries (forgeries made targeting a particular signature) is\nstill hard. We propose approaching the problem from a feature learning\nperspective. Our hypothesis is that, in the absence of a good model of the data\ngeneration process, it is better to learn the features from data, instead of\nusing hand-crafted features that have no resemblance to the signature\ngeneration process. To this end, we use Deep Convolutional Neural Networks to\nlearn features in a writer-independent format, and use this model to obtain a\nfeature representation on another set of users, where we train writer-dependent\nclassifiers. We tested our method in two datasets: GPDS-960 and Brazilian\nPUC-PR. Our experimental results show that the features learned in a subset of\nthe users are discriminative for the other users, including across different\ndatasets, reaching close to the state-of-the-art in the GPDS dataset, and\nimproving the state-of-the-art in the Brazilian PUC-PR dataset.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 18:26:48 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Hafemann", "Luiz G.", ""], ["Sabourin", "Robert", ""], ["Oliveira", "Luiz S.", ""]]}, {"id": "1604.01075", "submitter": "Luis Reyes Castro", "authors": "Luis I. Reyes-Castro and Andres G. Abad", "title": "A Dynamic Bayesian Network Model for Inventory Level Estimation in\n  Retail Marketing", "comments": "Paper is 6 pages (as required by the conference submission rules) and\n  contains 2 figures. Paper will appear on the Proceedings of the 2016\n  Industrial and Systems Engineering Research Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many retailers today employ inventory management systems based on Re-Order\nPoint Policies, most of which rely on the assumption that all decreases in\nproduct inventory levels result from product sales. Unfortunately, it usually\nhappens that small but random quantities of the product get lost, stolen or\nbroken without record as time passes, e.g., as a consequence of shoplifting.\nThis is usual for retailers handling large varieties of inexpensive products,\ne.g., grocery stores. In turn, over time these discrepancies lead to stock\nfreezing problems, i.e., situations where the system believes the stock is\nabove the re-order point but the actual stock is at zero, and so no\nreplenishments or sales occur. Motivated by these issues, we model the\ninteraction between sales, losses, replenishments and inventory levels as a\nDynamic Bayesian Network (DBN), where the inventory levels are unobserved\n(i.e., hidden) variables we wish to estimate. We present an\nExpectation-Maximization (EM) algorithm to estimate the parameters of the sale\nand loss distributions, which relies on solving a one-dimensional dynamic\nprogram for the E-step and on solving two separate one-dimensional nonlinear\nprograms for the M-step.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 21:49:22 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Reyes-Castro", "Luis I.", ""], ["Abad", "Andres G.", ""]]}, {"id": "1604.01171", "submitter": "Yohann De Castro", "authors": "Sandrine Dallaporta and Yohann De Castro", "title": "Sparse Recovery from Extreme Eigenvalues Deviation Inequalities", "comments": "33 pages, 1 figure, final version", "journal-ref": null, "doi": "10.1051/ps/2018024", "report-no": null, "categories": "math.ST cs.IT math.IT math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article provides a new toolbox to derive sparse recovery guarantees from\nsmall deviations on extreme singular values or extreme eigenvalues obtained in\nRandom Matrix Theory. This work is based on Restricted Isometry Constants\n(RICs) which are a pivotal notion in Compressed Sensing and High-Dimensional\nStatistics as these constants finely assess how a linear operator is\nconditioned on the set of sparse vectors and hence how it performs in SRSR.\nWhile it is an open problem to construct deterministic matrices with apposite\nRICs, one can prove that such matrices exist using random matrices models. In\nthis paper, we show upper bounds on RICs for Gaussian and Rademacher matrices\nusing state-of-the-art small deviation estimates on their extreme eigenvalues.\nThis allows us to derive a lower bound on the probability of getting SRSR. One\nbenefit of this paper is a direct and explicit derivation of upper bounds on\nRICs and lower bounds on SRSR from small deviations on the extreme eigenvalues\ngiven by Random Matrix theory.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 08:37:20 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 10:36:44 GMT"}, {"version": "v3", "created": "Mon, 4 Dec 2017 10:47:46 GMT"}, {"version": "v4", "created": "Wed, 14 Nov 2018 12:34:01 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Dallaporta", "Sandrine", ""], ["De Castro", "Yohann", ""]]}, {"id": "1604.01219", "submitter": "Yuting Qaing", "authors": "Yuting Qiang, Yanwei Fu, Yanwen Guo, Zhi-Hua Zhou and Leonid Sigal", "title": "Learning to Generate Posters of Scientific Papers", "comments": "in Proceedings of the 30th AAAI Conference on Artificial Intelligence\n  (AAAI'16), Phoenix, AZ, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.HC cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers often summarize their work in the form of posters. Posters\nprovide a coherent and efficient way to convey core ideas from scientific\npapers. Generating a good scientific poster, however, is a complex and time\nconsuming cognitive task, since such posters need to be readable, informative,\nand visually aesthetic. In this paper, for the first time, we study the\nchallenging problem of learning to generate posters from scientific papers. To\nthis end, a data-driven framework, that utilizes graphical models, is proposed.\nSpecifically, given content to display, the key elements of a good poster,\nincluding panel layout and attributes of each panel, are learned and inferred\nfrom data. Then, given inferred layout and attributes, composition of graphical\nelements within each panel is synthesized. To learn and validate our model, we\ncollect and make public a Poster-Paper dataset, which consists of scientific\npapers and corresponding posters with exhaustively labelled panels and\nattributes. Qualitative and quantitative results indicate the effectiveness of\nour approach.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 11:18:04 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Qiang", "Yuting", ""], ["Fu", "Yanwei", ""], ["Guo", "Yanwen", ""], ["Zhou", "Zhi-Hua", ""], ["Sigal", "Leonid", ""]]}, {"id": "1604.01250", "submitter": "Alvin Chua", "authors": "Christopher J. Moore, Alvin J. K. Chua, Christopher P. L. Berry,\n  Jonathan R. Gair", "title": "Fast methods for training Gaussian processes on large data sets", "comments": "Fixed missing references", "journal-ref": "R. Soc. Open Sci. 3, 160125 (2016)", "doi": "10.1098/rsos.160125", "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process regression (GPR) is a non-parametric Bayesian technique for\ninterpolating or fitting data. The main barrier to further uptake of this\npowerful tool rests in the computational costs associated with the matrices\nwhich arise when dealing with large data sets. Here, we derive some simple\nresults which we have found useful for speeding up the learning stage in the\nGPR algorithm, and especially for performing Bayesian model comparison between\ndifferent covariance functions. We apply our techniques to both synthetic and\nreal data and quantify the speed-up relative to using nested sampling to\nnumerically evaluate model evidences.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 13:29:15 GMT"}, {"version": "v2", "created": "Fri, 13 May 2016 10:38:28 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Moore", "Christopher J.", ""], ["Chua", "Alvin J. K.", ""], ["Berry", "Christopher P. L.", ""], ["Gair", "Jonathan R.", ""]]}, {"id": "1604.01272", "submitter": "Despoina Christou", "authors": "Despoina Christou", "title": "Feature extraction using Latent Dirichlet Allocation and Neural\n  Networks: A case study on movie synopses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature extraction has gained increasing attention in the field of machine\nlearning, as in order to detect patterns, extract information, or predict\nfuture observations from big data, the urge of informative features is crucial.\nThe process of extracting features is highly linked to dimensionality reduction\nas it implies the transformation of the data from a sparse high-dimensional\nspace, to higher level meaningful abstractions. This dissertation employs\nNeural Networks for distributed paragraph representations, and Latent Dirichlet\nAllocation to capture higher level features of paragraph vectors. Although\nNeural Networks for distributed paragraph representations are considered the\nstate of the art for extracting paragraph vectors, we show that a quick topic\nanalysis model such as Latent Dirichlet Allocation can provide meaningful\nfeatures too. We evaluate the two methods on the CMU Movie Summary Corpus, a\ncollection of 25,203 movie plot summaries extracted from Wikipedia. Finally,\nfor both approaches, we use K-Nearest Neighbors to discover similar movies, and\nplot the projected representations using T-Distributed Stochastic Neighbor\nEmbedding to depict the context similarities. These similarities, expressed as\nmovie distances, can be used for movies recommendation. The recommended movies\nof this approach are compared with the recommended movies from IMDB, which use\na collaborative filtering recommendation approach, to show that our two models\ncould constitute either an alternative or a supplementary recommendation\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 14:32:48 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Christou", "Despoina", ""]]}, {"id": "1604.01348", "submitter": "Kenji Kawaguchi", "authors": "Kenji Kawaguchi, Leslie Pack Kaelbling, Tom\\'as Lozano-P\\'erez", "title": "Bayesian Optimization with Exponential Convergence", "comments": "In NIPS 2015 (Advances in Neural Information Processing Systems 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Bayesian optimization method with exponential\nconvergence without the need of auxiliary optimization and without the\ndelta-cover sampling. Most Bayesian optimization methods require auxiliary\noptimization: an additional non-convex global optimization problem, which can\nbe time-consuming and hard to implement in practice. Also, the existing\nBayesian optimization method with exponential convergence requires access to\nthe delta-cover sampling, which was considered to be impractical. Our approach\neliminates both requirements and achieves an exponential convergence rate.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 17:53:59 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Kawaguchi", "Kenji", ""], ["Kaelbling", "Leslie Pack", ""], ["Lozano-P\u00e9rez", "Tom\u00e1s", ""]]}, {"id": "1604.01351", "submitter": "Shaofeng Zou", "authors": "Shaofeng Zou, Yingbin Liang, H. Vincent Poor", "title": "Nonparametric Detection of Geometric Structures over Networks", "comments": "Submitted for journal publication in November 2015. arXiv admin note:\n  text overlap with arXiv:1404.0298", "journal-ref": null, "doi": "10.1109/TSP.2017.2718977", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric detection of existence of an anomalous structure over a network\nis investigated. Nodes corresponding to the anomalous structure (if one exists)\nreceive samples generated by a distribution q, which is different from a\ndistribution p generating samples for other nodes. If an anomalous structure\ndoes not exist, all nodes receive samples generated by p. It is assumed that\nthe distributions p and q are arbitrary and unknown. The goal is to design\nstatistically consistent tests with probability of errors converging to zero as\nthe network size becomes asymptotically large. Kernel-based tests are proposed\nbased on maximum mean discrepancy that measures the distance between mean\nembeddings of distributions into a reproducing kernel Hilbert space. Detection\nof an anomalous interval over a line network is first studied. Sufficient\nconditions on minimum and maximum sizes of candidate anomalous intervals are\ncharacterized in order to guarantee the proposed test to be consistent. It is\nalso shown that certain necessary conditions must hold to guarantee any test to\nbe universally consistent. Comparison of sufficient and necessary conditions\nyields that the proposed test is order-level optimal and nearly optimal\nrespectively in terms of minimum and maximum sizes of candidate anomalous\nintervals. Generalization of the results to other networks is further\ndeveloped. Numerical results are provided to demonstrate the performance of the\nproposed tests.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 18:00:45 GMT"}, {"version": "v2", "created": "Wed, 6 Apr 2016 14:10:10 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Zou", "Shaofeng", ""], ["Liang", "Yingbin", ""], ["Poor", "H. Vincent", ""]]}, {"id": "1604.01433", "submitter": "Leonardo Rey Vega", "authors": "Mat\\'ias Vera, Leonardo Rey Vega, Pablo Piantanida", "title": "Collaborative Information Bottleneck", "comments": "Submitted to IEEE Transactions on Information Theory (revised, 64\n  pages, 7 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates a multi-terminal source coding problem under a\nlogarithmic loss fidelity which does not necessarily lead to an additive\ndistortion measure. The problem is motivated by an extension of the Information\nBottleneck method to a multi-source scenario where several encoders have to\nbuild cooperatively rate-limited descriptions of their sources in order to\nmaximize information with respect to other unobserved (hidden) sources. More\nprecisely, we study fundamental information-theoretic limits of the so-called:\n(i) Two-way Collaborative Information Bottleneck (TW-CIB) and (ii) the\nCollaborative Distributed Information Bottleneck (CDIB) problems. The TW-CIB\nproblem consists of two distant encoders that separately observe marginal\n(dependent) components $X_1$ and $X_2$ and can cooperate through multiple\nexchanges of limited information with the aim of extracting information about\nhidden variables $(Y_1,Y_2)$, which can be arbitrarily dependent on\n$(X_1,X_2)$. On the other hand, in CDIB there are two cooperating encoders\nwhich separately observe $X_1$ and $X_2$ and a third node which can listen to\nthe exchanges between the two encoders in order to obtain information about a\nhidden variable $Y$. The relevance (figure-of-merit) is measured in terms of a\nnormalized (per-sample) multi-letter mutual information metric (log-loss\nfidelity) and an interesting tradeoff arises by constraining the complexity of\ndescriptions, measured in terms of the rates needed for the exchanges between\nthe encoders and decoders involved. Inner and outer bounds to the\ncomplexity-relevance region of these problems are derived from which optimality\nis characterized for several cases of interest. Our resulting theoretical\ncomplexity-relevance regions are finally evaluated for binary symmetric and\nGaussian statistical models.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 21:37:05 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 13:23:10 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Vera", "Mat\u00edas", ""], ["Vega", "Leonardo Rey", ""], ["Piantanida", "Pablo", ""]]}, {"id": "1604.01515", "submitter": "Sylvain Arlot", "authors": "Sylvain Arlot (LMO, SELECT), Robin Genuer (ISPED, SISTM)", "title": "Comments on: \"A Random Forest Guided Tour\" by G. Biau and E. Scornet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a comment on the survey paper by Biau and Scornet (2016) about\nrandom forests. We focus on the problem of quantifying the impact of each\ningredient of random forests on their performance. We show that such a\nquantification is possible for a simple pure forest , leading to conclusions\nthat could apply more generally. Then, we consider \"hold-out\" random forests,\nwhich are a good middle point between \"toy\" pure forests and Breiman's original\nrandom forests.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 07:23:17 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Arlot", "Sylvain", "", "LMO, SELECT"], ["Genuer", "Robin", "", "ISPED, SISTM"]]}, {"id": "1604.01602", "submitter": "Jonas Nordhaug Myhre", "authors": "Jonas Nordhaug Myhre, Matineh Shaker, Devrim Kaba, Robert Jenssen,\n  Deniz Erdogmus", "title": "Manifold unwrapping using density ridges", "comments": "43 pages, 29 figures, submitted to the Journal of Machine Learning\n  Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on manifold learning within a density ridge estimation framework has\nshown great potential in recent work for both estimation and de-noising of\nmanifolds, building on the intuitive and well-defined notion of principal\ncurves and surfaces. However, the problem of unwrapping or unfolding manifolds\nhas received relatively little attention within the density ridge approach,\ndespite being an integral part of manifold learning in general. This paper\nproposes two novel algorithms for unwrapping manifolds based on estimated\nprincipal curves and surfaces for one- and multi-dimensional manifolds\nrespectively. The methods of unwrapping are founded in the realization that\nboth principal curves and principal surfaces will have inherent local maxima of\nthe probability density function. Following this observation, coordinate\nsystems that follow the shape of the manifold can be computed by following the\nintegral curves of the gradient flow of a kernel density estimate on the\nmanifold. Furthermore, since integral curves of the gradient flow of a kernel\ndensity estimate is inherently local, we propose to stitch together local\ncoordinate systems using parallel transport along the manifold. We provide\nnumerical experiments on both real and synthetic data that illustrates clear\nand intuitive unwrapping results comparable to state-of-the-art manifold\nlearning algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 13:12:27 GMT"}, {"version": "v2", "created": "Fri, 8 Apr 2016 09:38:25 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["Myhre", "Jonas Nordhaug", ""], ["Shaker", "Matineh", ""], ["Kaba", "Devrim", ""], ["Jenssen", "Robert", ""], ["Erdogmus", "Deniz", ""]]}, {"id": "1604.01662", "submitter": "Hao Wang", "authors": "Hao Wang and Dit-Yan Yeung", "title": "A Survey on Bayesian Deep Learning", "comments": "Published in ACM Computing Surveys (CSUR) 2020. Constantly updating\n  project page at https://github.com/js05212/BayesianDeepLearning-Survey", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A comprehensive artificial intelligence system needs to not only perceive the\nenvironment with different `senses' (e.g., seeing and hearing) but also infer\nthe world's conditional (or even causal) relations and corresponding\nuncertainty. The past decade has seen major advances in many perception tasks\nsuch as visual object recognition and speech recognition using deep learning\nmodels. For higher-level inference, however, probabilistic graphical models\nwith their Bayesian nature are still more powerful and flexible. In recent\nyears, Bayesian deep learning has emerged as a unified probabilistic framework\nto tightly integrate deep learning and Bayesian models. In this general\nframework, the perception of text or images using deep learning can boost the\nperformance of higher-level inference and in turn, the feedback from the\ninference process is able to enhance the perception of text or images. This\nsurvey provides a comprehensive introduction to Bayesian deep learning and\nreviews its recent applications on recommender systems, topic models, control,\netc. Besides, we also discuss the relationship and differences between Bayesian\ndeep learning and other related topics such as Bayesian treatment of neural\nnetworks. For a constantly updating project page, please refer to\nhttps://github.com/js05212/BayesianDeepLearning-Survey.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 15:35:08 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 06:17:44 GMT"}, {"version": "v3", "created": "Thu, 2 Jul 2020 03:52:57 GMT"}, {"version": "v4", "created": "Wed, 6 Jan 2021 03:14:13 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Wang", "Hao", ""], ["Yeung", "Dit-Yan", ""]]}, {"id": "1604.01733", "submitter": "Matthew Blaschko", "authors": "Wacha Bounliphone (L2S, CVN, GALEN), Matthew Blaschko", "title": "A U-statistic Approach to Hypothesis Testing for Structure Discovery in\n  Undirected Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structure discovery in graphical models is the determination of the topology\nof a graph that encodes conditional independence properties of the joint\ndistribution of all variables in the model. For some class of probability\ndistributions, an edge between two variables is present if and only if the\ncorresponding entry in the precision matrix is non-zero. For a finite sample\nestimate of the precision matrix, entries close to zero may be due to low\nsample effects, or due to an actual association between variables; these two\ncases are not readily distinguishable. %Fisher provided a hypothesis test based\non a parametric approximation to the distribution of an entry in the precision\nmatrix of a Gaussian distribution, but this may not provide valid upper bounds\non $p$-values for non-Gaussian distributions. Many related works on this topic\nconsider potentially restrictive distributional or sparsity assumptions that\nmay not apply to a data sample of interest, and direct estimation of the\nuncertainty of an estimate of the precision matrix for general distributions\nremains challenging. Consequently, we make use of results for $U$-statistics\nand apply them to the covariance matrix. By probabilistically bounding the\ndistortion of the covariance matrix, we can apply Weyl's theorem to bound the\ndistortion of the precision matrix, yielding a conservative, but sound test\nthreshold for a much wider class of distributions than considered in previous\nworks. The resulting test enables one to answer with statistical significance\nwhether an edge is present in the graph, and convergence results are known for\na wide range of distributions. The computational complexities is linear in the\nsample size enabling the application of the test to large data samples for\nwhich computation time becomes a limiting factor. We experimentally validate\nthe correctness and scalability of the test on multivariate distributions for\nwhich the distributional assumptions of competing tests result in\nunderestimates of the false positive ratio. By contrast, the proposed test\nremains sound, promising to be a useful tool for hypothesis testing for diverse\nreal-world problems.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 19:05:34 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Bounliphone", "Wacha", "", "L2S, CVN, GALEN"], ["Blaschko", "Matthew", ""]]}, {"id": "1604.01854", "submitter": "Tim Leathart", "authors": "Tim Leathart, Bernhard Pfahringer and Eibe Frank", "title": "Building Ensembles of Adaptive Nested Dichotomies with Random-Pair\n  Selection", "comments": "ECMLPKDD 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A system of nested dichotomies is a method of decomposing a multi-class\nproblem into a collection of binary problems. Such a system recursively splits\nthe set of classes into two subsets, and trains a binary classifier to\ndistinguish between each subset. Even though ensembles of nested dichotomies\nwith random structure have been shown to perform well in practice, using a more\nsophisticated class subset selection method can be used to improve\nclassification accuracy. We investigate an approach to this problem called\nrandom-pair selection, and evaluate its effectiveness compared to other\npublished methods of subset selection. We show that our method outperforms\nother methods in many cases when forming ensembles of nested dichotomies, and\nis at least on par in all other cases.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 02:56:19 GMT"}, {"version": "v2", "created": "Tue, 5 Jul 2016 04:25:25 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Leathart", "Tim", ""], ["Pfahringer", "Bernhard", ""], ["Frank", "Eibe", ""]]}, {"id": "1604.01952", "submitter": "David Balduzzi", "authors": "David Balduzzi", "title": "Deep Online Convex Optimization with Gated Games", "comments": "13 pages. This paper renders arXiv:1509.01851 obsolete. It contains\n  the same basic results, with major changes to exposition and minor changes to\n  terminology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods from convex optimization are widely used as building blocks for deep\nlearning algorithms. However, the reasons for their empirical success are\nunclear, since modern convolutional networks (convnets), incorporating\nrectifier units and max-pooling, are neither smooth nor convex. Standard\nguarantees therefore do not apply. This paper provides the first convergence\nrates for gradient descent on rectifier convnets. The proof utilizes the\nparticular structure of rectifier networks which consists in binary\nactive/inactive gates applied on top of an underlying linear network. The\napproach generalizes to max-pooling, dropout and maxout. In other words, to\nprecisely the neural networks that perform best empirically. The key step is to\nintroduce gated games, an extension of convex games with similar convergence\nproperties that capture the gating function of rectifiers. The main result is\nthat rectifier convnets converge to a critical point at a rate controlled by\nthe gated-regret of the units in the network. Corollaries of the main result\ninclude: (i) a game-theoretic description of the representations learned by a\nneural network; (ii) a logarithmic-regret algorithm for training neural nets;\nand (iii) a formal setting for analyzing conditional computation in neural nets\nthat can be applied to recently developed models of attention.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 10:46:54 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Balduzzi", "David", ""]]}, {"id": "1604.01955", "submitter": "Renyu Zhao", "authors": "Renyu Zhao", "title": "Monitoring Chinese Population Migration in Consecutive Weekly Basis from\n  Intra-city scale to Inter-province scale by Didi's Bigdata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Population migration is valuable information which leads to proper decision\nin urban-planning strategy, massive investment, and many other fields. For\ninstance, inter-city migration is a posterior evidence to see if the\ngovernment's constrain of population works, and inter-community immigration\nmight be a prior evidence of real estate price hike. With timely data, it is\nalso impossible to compare which city is more favorable for the people, suppose\nthe cities release different new regulations, we could also compare the\ncustomers of different real estate development groups, where they come from,\nwhere they probably will go. Unfortunately these data was not available.\n  In this paper, leveraging the data generated by positioning team in Didi, we\npropose a novel approach that timely monitoring population migration from\ncommunity scale to provincial scale. Migration can be detected as soon as in a\nweek. It could be faster, the setting of a week is for statistical purpose. A\nmonitoring system is developed, then applied nation wide in China, some\nobservations derived from the system will be presented in this paper.\n  This new method of migration perception is origin from the insight that\nnowadays people mostly moving with their personal Access Point (AP), also known\nas WiFi hotspot. Assume that the ratio of AP moving to the migration of\npopulation is constant, analysis of comparative population migration would be\nfeasible. More exact quantitative research would also be done with few sample\nresearch and model regression.\n  The procedures of processing data includes many steps: eliminating the impact\nof pseudo-migration AP, for instance pocket WiFi, and second-hand traded\nrouter; distinguishing moving of population with moving of companies;\nidentifying shifting of AP by the finger print clusters, etc..\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 11:08:45 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Zhao", "Renyu", ""]]}, {"id": "1604.01972", "submitter": "Ulrich Paquet", "authors": "Marco Fraccaro and Ulrich Paquet and Ole Winther", "title": "An Adaptive Resample-Move Algorithm for Estimating Normalizing Constants", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of normalizing constants is a fundamental step in\nprobabilistic model comparison. Sequential Monte Carlo methods may be used for\nthis task and have the advantage of being inherently parallelizable. However,\nthe standard choice of using a fixed number of particles at each iteration is\nsuboptimal because some steps will contribute disproportionately to the\nvariance of the estimate. We introduce an adaptive version of the Resample-Move\nalgorithm, in which the particle set is adaptively expanded whenever a better\napproximation of an intermediate distribution is needed. The algorithm builds\non the expression for the optimal number of particles and the corresponding\nminimum variance found under ideal conditions. Benchmark results on challenging\nGaussian Process Classification and Restricted Boltzmann Machine applications\nshow that Adaptive Resample-Move (ARM) estimates the normalizing constant with\na smaller variance, using less computational resources, than either\nResample-Move with a fixed number of particles or Annealed Importance Sampling.\nA further advantage over Annealed Importance Sampling is that ARM is easier to\ntune.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 12:15:54 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 09:39:33 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Fraccaro", "Marco", ""], ["Paquet", "Ulrich", ""], ["Winther", "Ole", ""]]}, {"id": "1604.01999", "submitter": "Varun Kanade", "authors": "Vincent Cohen-Addad, Varun Kanade", "title": "Online Optimization of Smoothed Piecewise Constant Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study online optimization of smoothed piecewise constant functions over\nthe domain [0, 1). This is motivated by the problem of adaptively picking\nparameters of learning algorithms as in the recently introduced framework by\nGupta and Roughgarden (2016). Majority of the machine learning literature has\nfocused on Lipschitz-continuous functions or functions with bounded gradients.\n1 This is with good reason---any learning algorithm suffers linear regret even\nagainst piecewise constant functions that are chosen adversarially, arguably\nthe simplest of non-Lipschitz continuous functions. The smoothed setting we\nconsider is inspired by the seminal work of Spielman and Teng (2004) and the\nrecent work of Gupta and Roughgarden---in this setting, the sequence of\nfunctions may be chosen by an adversary, however, with some uncertainty in the\nlocation of discontinuities. We give algorithms that achieve sublinear regret\nin the full information and bandit settings.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 13:52:47 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 12:05:37 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Cohen-Addad", "Vincent", ""], ["Kanade", "Varun", ""]]}, {"id": "1604.02027", "submitter": "Ke Jiang", "authors": "Ke Jiang and Suvrit Sra and Brian Kulis", "title": "Combinatorial Topic Models using Small-Variance Asymptotics", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models have emerged as fundamental tools in unsupervised machine\nlearning. Most modern topic modeling algorithms take a probabilistic view and\nderive inference algorithms based on Latent Dirichlet Allocation (LDA) or its\nvariants. In contrast, we study topic modeling as a combinatorial optimization\nproblem, and propose a new objective function derived from LDA by passing to\nthe small-variance limit. We minimize the derived objective by using ideas from\ncombinatorial optimization, which results in a new, fast, and high-quality\ntopic modeling algorithm. In particular, we show that our results are\ncompetitive with popular LDA-based topic modeling approaches, and also discuss\nthe (dis)similarities between our approach and its probabilistic counterparts.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 15:04:16 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 03:11:02 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Jiang", "Ke", ""], ["Sra", "Suvrit", ""], ["Kulis", "Brian", ""]]}, {"id": "1604.02100", "submitter": "Xiaobo Qu", "authors": "Jiaxi Ying, Hengfa Lu, Qingtao Wei, Jian-Feng Cai, Di Guo, Jihui Wu,\n  Zhong Chen, Xiaobo Qu", "title": "Hankel Matrix Nuclear Norm Regularized Tensor Completion for\n  $N$-dimensional Exponential Signals", "comments": "15 pages, 12 figures", "journal-ref": null, "doi": "10.1109/TSP.2017.2695566", "report-no": null, "categories": "stat.ML cs.IT cs.NA math.IT math.SP physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signals are generally modeled as a superposition of exponential functions in\nspectroscopy of chemistry, biology and medical imaging. For fast data\nacquisition or other inevitable reasons, however, only a small amount of\nsamples may be acquired and thus how to recover the full signal becomes an\nactive research topic. But existing approaches can not efficiently recover\n$N$-dimensional exponential signals with $N\\geq 3$. In this paper, we study the\nproblem of recovering N-dimensional (particularly $N\\geq 3$) exponential\nsignals from partial observations, and formulate this problem as a low-rank\ntensor completion problem with exponential factor vectors. The full signal is\nreconstructed by simultaneously exploiting the CANDECOMP/PARAFAC structure and\nthe exponential structure of the associated factor vectors. The latter is\npromoted by minimizing an objective function involving the nuclear norm of\nHankel matrices. Experimental results on simulated and real magnetic resonance\nspectroscopy data show that the proposed approach can successfully recover full\nsignals from very limited samples and is robust to the estimated tensor rank.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 12:51:07 GMT"}, {"version": "v2", "created": "Fri, 31 Mar 2017 12:02:13 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Ying", "Jiaxi", ""], ["Lu", "Hengfa", ""], ["Wei", "Qingtao", ""], ["Cai", "Jian-Feng", ""], ["Guo", "Di", ""], ["Wu", "Jihui", ""], ["Chen", "Zhong", ""], ["Qu", "Xiaobo", ""]]}, {"id": "1604.02123", "submitter": "Talayeh Razzaghi", "authors": "Talayeh Razzaghi, Oleg Roderick, Ilya Safro, Nicholas Marko", "title": "Multilevel Weighted Support Vector Machine for Classification on\n  Healthcare Data with Missing Values", "comments": "arXiv admin note: substantial text overlap with arXiv:1503.06250", "journal-ref": null, "doi": "10.1371/journal.pone.0155119", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is motivated by the needs of predictive analytics on healthcare\ndata as represented by Electronic Medical Records. Such data is invariably\nproblematic: noisy, with missing entries, with imbalance in classes of\ninterests, leading to serious bias in predictive modeling. Since standard data\nmining methods often produce poor performance measures, we argue for\ndevelopment of specialized techniques of data-preprocessing and classification.\nIn this paper, we propose a new method to simultaneously classify large\ndatasets and reduce the effects of missing values. It is based on a multilevel\nframework of the cost-sensitive SVM and the expected maximization imputation\nmethod for missing values, which relies on iterated regression analyses. We\ncompare classification results of multilevel SVM-based algorithms on public\nbenchmark datasets with imbalanced classes and missing values as well as real\ndata in health applications, and show that our multilevel SVM-based method\nproduces fast, and more accurate and robust classification results.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 19:19:52 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Razzaghi", "Talayeh", ""], ["Roderick", "Oleg", ""], ["Safro", "Ilya", ""], ["Marko", "Nicholas", ""]]}, {"id": "1604.02181", "submitter": "Igor Fedorov", "authors": "Igor Fedorov, Alican Nalci, Ritwik Giri, Bhaskar D. Rao, Truong Q.\n  Nguyen, Harinath Garudadri", "title": "A Unified Framework for Sparse Non-Negative Least Squares using\n  Multiplicative Updates and the Non-Negative Matrix Factorization Problem", "comments": "To appear in Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the sparse non-negative least squares (S-NNLS) problem. S-NNLS\noccurs naturally in a wide variety of applications where an unknown,\nnon-negative quantity must be recovered from linear measurements. We present a\nunified framework for S-NNLS based on a rectified power exponential scale\nmixture prior on the sparse codes. We show that the proposed framework\nencompasses a large class of S-NNLS algorithms and provide a computationally\nefficient inference procedure based on multiplicative update rules. Such update\nrules are convenient for solving large sets of S-NNLS problems simultaneously,\nwhich is required in contexts like sparse non-negative matrix factorization\n(S-NMF). We provide theoretical justification for the proposed approach by\nshowing that the local minima of the objective function being optimized are\nsparse and the S-NNLS algorithms presented are guaranteed to converge to a set\nof stationary points of the objective function. We then extend our framework to\nS-NMF, showing that our framework leads to many well known S-NMF algorithms\nunder specific choices of prior and providing a guarantee that a popular\nsubclass of the proposed algorithms converges to a set of stationary points of\nthe objective function. Finally, we study the performance of the proposed\napproaches on synthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 21:35:42 GMT"}, {"version": "v2", "created": "Tue, 12 Apr 2016 01:32:38 GMT"}, {"version": "v3", "created": "Thu, 2 Mar 2017 07:18:28 GMT"}, {"version": "v4", "created": "Sat, 22 Jul 2017 19:07:19 GMT"}, {"version": "v5", "created": "Wed, 11 Oct 2017 18:26:07 GMT"}, {"version": "v6", "created": "Tue, 2 Jan 2018 17:18:55 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Fedorov", "Igor", ""], ["Nalci", "Alican", ""], ["Giri", "Ritwik", ""], ["Rao", "Bhaskar D.", ""], ["Nguyen", "Truong Q.", ""], ["Garudadri", "Harinath", ""]]}, {"id": "1604.02218", "submitter": "Hao Yu", "authors": "Hao Yu and Michael J. Neely", "title": "A Low Complexity Algorithm with $O(\\sqrt{T})$ Regret and $O(1)$\n  Constraint Violations for Online Convex Optimization with Long Term\n  Constraints", "comments": "This paper is published in JMLR. The title is changed to emphasize\n  that constraint violations attained by our algorithm is independent of the\n  number of rounds $T$. In this version, we also analyze the regret and\n  constraint violations for our algorithm without requiring the Slater\n  condition", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers online convex optimization over a complicated constraint\nset, which typically consists of multiple functional constraints and a set\nconstraint. The conventional online projection algorithm (Zinkevich, 2003) can\nbe difficult to implement due to the potentially high computation complexity of\nthe projection operation. In this paper, we relax the functional constraints by\nallowing them to be violated at each round but still requiring them to be\nsatisfied in the long term. This type of relaxed online convex optimization\n(with long term constraints) was first considered in Mahdavi et al. (2012).\nThat prior work proposes an algorithm to achieve $O(\\sqrt{T})$ regret and\n$O(T^{3/4})$ constraint violations for general problems and another algorithm\nto achieve an $O(T^{2/3})$ bound for both regret and constraint violations when\nthe constraint set can be described by a finite number of linear constraints. A\nrecent extension in \\citet{Jenatton16ICML} can achieve\n$O(T^{\\max\\{\\theta,1-\\theta\\}})$ regret and $O(T^{1-\\theta/2})$ constraint\nviolations where $\\theta\\in (0,1)$. The current paper proposes a new simple\nalgorithm that yields improved performance in comparison to prior works. The\nnew algorithm achieves an $O(\\sqrt{T})$ regret bound with $O(1)$ constraint\nviolations.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 03:37:52 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2016 00:31:37 GMT"}, {"version": "v3", "created": "Mon, 18 May 2020 06:31:30 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Yu", "Hao", ""], ["Neely", "Michael J.", ""]]}, {"id": "1604.02275", "submitter": "Rocco De Rosa rd", "authors": "Rocco De Rosa, Thomas Mensink and Barbara Caputo", "title": "Online Open World Recognition", "comments": "keywords{Open world recognition, Open set, Incremental Learning,\n  Metric Learning, Nonparametric methods, Classification confidence}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As we enter into the big data age and an avalanche of images have become\nreadily available, recognition systems face the need to move from close, lab\nsettings where the number of classes and training data are fixed, to dynamic\nscenarios where the number of categories to be recognized grows continuously\nover time, as well as new data providing useful information to update the\nsystem. Recent attempts, like the open world recognition framework, tried to\ninject dynamics into the system by detecting new unknown classes and adding\nthem incrementally, while at the same time continuously updating the models for\nthe known classes. incrementally adding new classes and detecting instances\nfrom unknown classes, while at the same time continuously updating the models\nfor the known classes. In this paper we argue that to properly capture the\nintrinsic dynamic of open world recognition, it is necessary to add to these\naspects (a) the incremental learning of the underlying metric, (b) the\nincremental estimate of confidence thresholds for the unknown classes, and (c)\nthe use of local learning to precisely describe the space of classes. We extend\nthree existing metric learning algorithms towards these goals by using online\nmetric learning. Experimentally we validate our approach on two large-scale\ndatasets in different learning scenarios. For all these scenarios our proposed\nmethods outperform their non-online counterparts. We conclude that local and\nonline learning is important to capture the full dynamics of open world\nrecognition.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 08:43:15 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["De Rosa", "Rocco", ""], ["Mensink", "Thomas", ""], ["Caputo", "Barbara", ""]]}, {"id": "1604.02492", "submitter": "Samuel Elder", "authors": "Sam Elder", "title": "Challenges in Bayesian Adaptive Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional statistical analysis requires that the analysis process and data\nare independent. By contrast, the new field of adaptive data analysis hopes to\nunderstand and provide algorithms and accuracy guarantees for research as it is\ncommonly performed in practice, as an iterative process of interacting\nrepeatedly with the same data set, such as repeated tests against a holdout\nset. Previous work has defined a model with a rather strong lower bound on\nsample complexity in terms of the number of queries, $n\\sim\\sqrt q$, arguing\nthat adaptive data analysis is much harder than static data analysis, where\n$n\\sim\\log q$ is possible. Instead, we argue that those strong lower bounds\npoint to a limitation of the previous model in that it must consider wildly\nasymmetric scenarios which do not hold in typical applications.\n  To better understand other difficulties of adaptivity, we propose a new\nBayesian version of the problem that mandates symmetry. Since the other lower\nbound techniques are ruled out, we can more effectively see difficulties that\nmight otherwise be overshadowed. As a first contribution to this model, we\nproduce a new problem using error-correcting codes on which a large family of\nmethods, including all previously proposed algorithms, require roughly\n$n\\sim\\sqrt[4]q$. These early results illustrate new difficulties in adaptive\ndata analysis regarding slightly correlated queries on problems with\nconcentrated uncertainty.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 21:56:24 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 19:15:56 GMT"}, {"version": "v3", "created": "Fri, 4 Nov 2016 21:05:14 GMT"}, {"version": "v4", "created": "Mon, 21 Nov 2016 22:20:12 GMT"}, {"version": "v5", "created": "Mon, 20 Mar 2017 20:00:41 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Elder", "Sam", ""]]}, {"id": "1604.02606", "submitter": "Bo Li", "authors": "Bo Li, Yevgeniy Vorobeychik, Xinyun Chen", "title": "A General Retraining Framework for Scalable Adversarial Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional classification algorithms assume that training and test data come\nfrom similar distributions. This assumption is violated in adversarial\nsettings, where malicious actors modify instances to evade detection. A number\nof custom methods have been developed for both adversarial evasion attacks and\nrobust learning. We propose the first systematic and general-purpose retraining\nframework which can: a) boost robustness of an \\emph{arbitrary} learning\nalgorithm, in the face of b) a broader class of adversarial models than any\nprior methods. We show that, under natural conditions, the retraining framework\nminimizes an upper bound on optimal adversarial risk, and show how to extend\nthis result to account for approximations of evasion attacks. Extensive\nexperimental evaluation demonstrates that our retraining methods are nearly\nindistinguishable from state-of-the-art algorithms for optimizing adversarial\nrisk, but are more general and far more scalable. The experiments also confirm\nthat without retraining, our adversarial framework dramatically reduces the\neffectiveness of learning. In contrast, retraining significantly boosts\nrobustness to evasion attacks without significantly compromising overall\naccuracy.\n", "versions": [{"version": "v1", "created": "Sat, 9 Apr 2016 20:14:36 GMT"}, {"version": "v2", "created": "Sat, 26 Nov 2016 09:31:57 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Li", "Bo", ""], ["Vorobeychik", "Yevgeniy", ""], ["Chen", "Xinyun", ""]]}, {"id": "1604.02631", "submitter": "Dionysios Kalogerias", "authors": "Dionysios S. Kalogerias, Athina P. Petropulu", "title": "Grid Based Nonlinear Filtering Revisited: Recursive Estimation &\n  Asymptotic Optimality", "comments": "38 pages. To appear in the IEEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2016.2557311", "report-no": null, "categories": "math.ST cs.IT math.IT math.OC stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the development of grid based recursive approximate filtering of\ngeneral Markov processes in discrete time, partially observed in conditionally\nGaussian noise. The grid based filters considered rely on two types of state\nquantization: The \\textit{Markovian} type and the \\textit{marginal} type. We\npropose a set of novel, relaxed sufficient conditions, ensuring strong and\nfully characterized pathwise convergence of these filters to the respective\nMMSE state estimator. In particular, for marginal state quantizations, we\nintroduce the notion of \\textit{conditional regularity of stochastic kernels},\nwhich, to the best of our knowledge, constitutes the most relaxed condition\nproposed, under which asymptotic optimality of the respective grid based\nfilters is guaranteed. Further, we extend our convergence results, including\nfiltering of bounded and continuous functionals of the state, as well as\nrecursive approximate state prediction. For both Markovian and marginal\nquantizations, the whole development of the respective grid based filters\nrelies more on linear-algebraic techniques and less on measure theoretic\narguments, making the presentation considerably shorter and technically\nsimpler.\n", "versions": [{"version": "v1", "created": "Sun, 10 Apr 2016 03:06:23 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Kalogerias", "Dionysios S.", ""], ["Petropulu", "Athina P.", ""]]}, {"id": "1604.02634", "submitter": "Renbo Zhao", "authors": "Renbo Zhao and Vincent Y. F. Tan", "title": "Online Nonnegative Matrix Factorization with Outliers", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2620967", "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a unified and systematic framework for performing online\nnonnegative matrix factorization in the presence of outliers. Our framework is\nparticularly suited to large-scale data. We propose two solvers based on\nprojected gradient descent and the alternating direction method of multipliers.\nWe prove that the sequence of objective values converges almost surely by\nappealing to the quasi-martingale convergence theorem. We also show the\nsequence of learned dictionaries converges to the set of stationary points of\nthe expected loss function almost surely. In addition, we extend our basic\nproblem formulation to various settings with different constraints and\nregularizers. We also adapt the solvers and analyses to each setting. We\nperform extensive experiments on both synthetic and real datasets. These\nexperiments demonstrate the computational efficiency and efficacy of our\nalgorithms on tasks such as (parts-based) basis learning, image denoising,\nshadow removal and foreground-background separation.\n", "versions": [{"version": "v1", "created": "Sun, 10 Apr 2016 04:02:57 GMT"}, {"version": "v2", "created": "Sat, 15 Oct 2016 12:01:30 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Zhao", "Renbo", ""], ["Tan", "Vincent Y. F.", ""]]}, {"id": "1604.02668", "submitter": "ShengLi Tzeng", "authors": "ShengLi Tzeng, Christian Hennig, Yu-Fen Li, Chien-Ju Lin", "title": "Distance for Functional Data Clustering Based on Smoothing Parameter\n  Commutation", "comments": null, "journal-ref": "Statistical Methods in Medical Research, 27 (2018)", "doi": "10.1177/0962280217710050", "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method to determine the dissimilarity between subjects for\nfunctional data clustering. Spline smoothing or interpolation is common to deal\nwith data of such type. Instead of estimating the best-representing curve for\neach subject as fixed during clustering, we measure the dissimilarity between\nsubjects based on varying curve estimates with commutation of smoothing\nparameters pair-by-pair (of subjects). The intuitions are that smoothing\nparameters of smoothing splines reflect inverse signal-to-noise ratios and that\napplying an identical smoothing parameter the smoothed curves for two similar\nsubjects are expected to be close. The effectiveness of our proposal is shown\nthrough simulations comparing to other dissimilarity measures. It also has\nseveral pragmatic advantages. First, missing values or irregular time points\ncan be handled directly, thanks to the nature of smoothing splines. Second,\nconventional clustering method based on dissimilarity can be employed\nstraightforward, and the dissimilarity also serves as a useful tool for outlier\ndetection. Third, the implementation is almost handy since subroutines for\nsmoothing splines and numerical integration are widely available. Fourth, the\ncomputational complexity does not increase and is parallel with that in\ncalculating Euclidean distance between curves estimated by smoothing splines.\n", "versions": [{"version": "v1", "created": "Sun, 10 Apr 2016 10:55:52 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Tzeng", "ShengLi", ""], ["Hennig", "Christian", ""], ["Li", "Yu-Fen", ""], ["Lin", "Chien-Ju", ""]]}, {"id": "1604.02737", "submitter": "Luis Ortiz", "authors": "Luis E. Ortiz and Boshen Wang and Ze Gong", "title": "Correlated Equilibria for Approximate Variational Inference in MRFs", "comments": "54 pages, 8 figures, 20 plots, Extension of Section 4 of a manuscript\n  by the first author first drafted on August 25, 2009 (see\n  http://www-personal.umd.umich.edu/~leortiz/papers/infeq.pdf). Changes:\n  experiments with multiplicative-weight learning algorithms on larger (12x12)\n  synthetic Ising models and 28x28 Ising models learned from MNIST dataset; and\n  misc. edits to improve presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Almost all of the work in graphical models for game theory has mirrored\nprevious work in probabilistic graphical models. Our work considers the\nopposite direction: Taking advantage of recent advances in equilibrium\ncomputation for probabilistic inference. We present formulations of inference\nproblems in Markov random fields (MRFs) as computation of equilibria in a\ncertain class of game-theoretic graphical models. We concretely establishes the\nprecise connection between variational probabilistic inference in MRFs and\ncorrelated equilibria. No previous work exploits recent theoretical and\nempirical results from the literature on algorithmic and computational game\ntheory on the tractable, polynomial-time computation of exact or approximate\ncorrelated equilibria in graphical games with arbitrary, loopy graph structure.\nWe discuss how to design new algorithms with equally tractable guarantees for\nthe computation of approximate variational inference in MRFs. Also, inspired by\na previously stated game-theoretic view of state-of-the-art tree-reweighed\n(TRW) message-passing techniques for belief inference as zero-sum game, we\npropose a different, general-sum potential game to design approximate\nfictitious-play techniques. We perform synthetic experiments evaluating our\nproposed approximation algorithms with standard methods and TRW on several\nclasses of classical Ising models (i.e., with binary random variables). We also\nevaluate the algorithms using Ising models learned from the MNIST dataset. Our\nexperiments show that our global approach is competitive, particularly shinning\nin a class of Ising models with constant, \"highly attractive\" edge-weights, in\nwhich it is often better than all other alternatives we evaluated. With a\nnotable exception, our more local approach was not as effective. Yet, in\nfairness, almost all of the alternatives are often no better than a simple\nbaseline: estimate 0.5.\n", "versions": [{"version": "v1", "created": "Sun, 10 Apr 2016 21:21:00 GMT"}, {"version": "v2", "created": "Sat, 7 Oct 2017 13:18:40 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Ortiz", "Luis E.", ""], ["Wang", "Boshen", ""], ["Gong", "Ze", ""]]}, {"id": "1604.02855", "submitter": "Rocco De Rosa rd", "authors": "Rocco De Rosa, Ilaria Gori, Fabio Cuzzolin, Barbara Caputo and\n  Nicol\\`o Cesa-Bianchi", "title": "Active Learning for Online Recognition of Human Activities from\n  Streaming Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognising human activities from streaming videos poses unique challenges to\nlearning algorithms: predictive models need to be scalable, incrementally\ntrainable, and must remain bounded in size even when the data stream is\narbitrarily long. Furthermore, as parameter tuning is problematic in a\nstreaming setting, suitable approaches should be parameterless, and make no\nassumptions on what class labels may occur in the stream. We present here an\napproach to the recognition of human actions from streaming data which meets\nall these requirements by: (1) incrementally learning a model which adaptively\ncovers the feature space with simple local classifiers; (2) employing an active\nlearning strategy to reduce annotation requests; (3) achieving promising\naccuracy within a fixed model size. Extensive experiments on standard\nbenchmarks show that our approach is competitive with state-of-the-art\nnon-incremental methods, and outperforms the existing active incremental\nbaselines.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 09:32:51 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["De Rosa", "Rocco", ""], ["Gori", "Ilaria", ""], ["Cuzzolin", "Fabio", ""], ["Caputo", "Barbara", ""], ["Cesa-Bianchi", "Nicol\u00f2", ""]]}, {"id": "1604.02917", "submitter": "Stefanos Eleftheriadis", "authors": "Stefanos Eleftheriadis and Ognjen Rudovic and Marc P. Deisenroth and\n  Maja Pantic", "title": "Gaussian Process Domain Experts for Model Adaptation in Facial Behavior\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for supervised domain adaptation that is based\nupon the probabilistic framework of Gaussian processes (GPs). Specifically, we\nintroduce domain-specific GPs as local experts for facial expression\nclassification from face images. The adaptation of the classifier is\nfacilitated in probabilistic fashion by conditioning the target expert on\nmultiple source experts. Furthermore, in contrast to existing adaptation\napproaches, we also learn a target expert from available target data solely.\nThen, a single and confident classifier is obtained by combining the\npredictions from multiple experts based on their confidence. Learning of the\nmodel is efficient and requires no retraining/reweighting of the source\nclassifiers. We evaluate the proposed approach on two publicly available\ndatasets for multi-class (MultiPIE) and multi-label (DISFA) facial expression\nclassification. To this end, we perform adaptation of two contextual factors:\n'where' (view) and 'who' (subject). We show in our experiments that the\nproposed approach consistently outperforms both source and target classifiers,\nwhile using as few as 30 target examples. It also outperforms the\nstate-of-the-art approaches for supervised domain adaptation.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 12:37:36 GMT"}, {"version": "v2", "created": "Mon, 2 May 2016 18:54:08 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Eleftheriadis", "Stefanos", ""], ["Rudovic", "Ognjen", ""], ["Deisenroth", "Marc P.", ""], ["Pantic", "Maja", ""]]}, {"id": "1604.03006", "submitter": "Sewoong Oh", "authors": "Weihao Gao, Sewoong Oh, Pramod Viswanath", "title": "Demystifying Fixed k-Nearest Neighbor Information Estimators", "comments": "55 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating mutual information from i.i.d. samples drawn from an unknown joint\ndensity function is a basic statistical problem of broad interest with\nmultitudinous applications. The most popular estimator is one proposed by\nKraskov and St\\\"ogbauer and Grassberger (KSG) in 2004, and is nonparametric and\nbased on the distances of each sample to its $k^{\\rm th}$ nearest neighboring\nsample, where $k$ is a fixed small integer. Despite its widespread use (part of\nscientific software packages), theoretical properties of this estimator have\nbeen largely unexplored. In this paper we demonstrate that the estimator is\nconsistent and also identify an upper bound on the rate of convergence of the\nbias as a function of number of samples. We argue that the superior performance\nbenefits of the KSG estimator stems from a curious \"correlation boosting\"\neffect and build on this intuition to modify the KSG estimator in novel ways to\nconstruct a superior estimator. As a byproduct of our investigations, we obtain\nnearly tight rates of convergence of the $\\ell_2$ error of the well known fixed\n$k$ nearest neighbor estimator of differential entropy by Kozachenko and\nLeonenko.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 15:47:05 GMT"}, {"version": "v2", "created": "Wed, 10 Aug 2016 16:45:42 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Gao", "Weihao", ""], ["Oh", "Sewoong", ""], ["Viswanath", "Pramod", ""]]}, {"id": "1604.03053", "submitter": "Yuan Zhao", "authors": "Yuan Zhao, Il Memming Park", "title": "Variational Latent Gaussian Process for Recovering Single-Trial Dynamics\n  from Population Spike Trains", "comments": null, "journal-ref": "Neural Computation, May 2017, Vol. 29, No. 5 , Pages: 1293-1316", "doi": "10.1162/NECO_a_00953", "report-no": null, "categories": "stat.ML q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When governed by underlying low-dimensional dynamics, the interdependence of\nsimultaneously recorded population of neurons can be explained by a small\nnumber of shared factors, or a low-dimensional trajectory. Recovering these\nlatent trajectories, particularly from single-trial population recordings, may\nhelp us understand the dynamics that drive neural computation. However, due to\nthe biophysical constraints and noise in the spike trains, inferring\ntrajectories from data is a challenging statistical problem in general. Here,\nwe propose a practical and efficient inference method, called the variational\nlatent Gaussian process (vLGP). The vLGP combines a generative model with a\nhistory-dependent point process observation together with a smoothness prior on\nthe latent trajectories. The vLGP improves upon earlier methods for recovering\nlatent trajectories, which assume either observation models inappropriate for\npoint processes or linear dynamics. We compare and validate vLGP on both\nsimulated datasets and population recordings from the primary visual cortex. In\nthe V1 dataset, we find that vLGP achieves substantially higher performance\nthan previous methods for predicting omitted spike trains, as well as capturing\nboth the toroidal topology of visual stimuli space, and the noise-correlation.\nThese results show that vLGP is a robust method with a potential to reveal\nhidden neural dynamics from large-scale neural recordings.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 18:25:09 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2016 14:55:59 GMT"}, {"version": "v3", "created": "Fri, 22 Apr 2016 23:38:28 GMT"}, {"version": "v4", "created": "Wed, 14 Sep 2016 13:30:48 GMT"}, {"version": "v5", "created": "Fri, 23 Dec 2016 18:32:40 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Zhao", "Yuan", ""], ["Park", "Il Memming", ""]]}, {"id": "1604.03114", "submitter": "Justine Zhang", "authors": "Justine Zhang, Ravi Kumar, Sujith Ravi, Cristian\n  Danescu-Niculescu-Mizil", "title": "Conversational flow in Oxford-style debates", "comments": "To appear at NAACL 2016. 5 pp, 1 fig. Data and other info available\n  at http://www.cs.cornell.edu/~cristian/debates", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Public debates are a common platform for presenting and juxtaposing diverging\nviews on important issues. In this work we propose a methodology for tracking\nhow ideas flow between participants throughout a debate. We use this approach\nin a case study of Oxford-style debates---a competitive format where the winner\nis determined by audience votes---and show how the outcome of a debate depends\non aspects of conversational flow. In particular, we find that winners tend to\nmake better use of a debate's interactive component than losers, by actively\npursuing their opponents' points rather than promoting their own ideas over the\ncourse of the conversation.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 20:00:04 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Zhang", "Justine", ""], ["Kumar", "Ravi", ""], ["Ravi", "Sujith", ""], ["Danescu-Niculescu-Mizil", "Cristian", ""]]}, {"id": "1604.03159", "submitter": "Pin-Yu Chen", "authors": "Pin-Yu Chen and Alfred O. Hero", "title": "Phase Transitions and a Model Order Selection Criterion for Spectral\n  Graph Clustering", "comments": "Accepted to IEEE Transactions on Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the longstanding open problems in spectral graph clustering (SGC) is\nthe so-called model order selection problem: automated selection of the correct\nnumber of clusters. This is equivalent to the problem of finding the number of\nconnected components or communities in an undirected graph. We propose\nautomated model order selection (AMOS), a solution to the SGC model selection\nproblem under a random interconnection model (RIM) using a novel selection\ncriterion that is based on an asymptotic phase transition analysis. AMOS can\nmore generally be applied to discovering hidden block diagonal structure in\nsymmetric non-negative matrices. Numerical experiments on simulated graphs\nvalidate the phase transition analysis, and real-world network data is used to\nvalidate the performance of the proposed model selection procedure.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 21:42:30 GMT"}, {"version": "v2", "created": "Mon, 18 Apr 2016 03:32:34 GMT"}, {"version": "v3", "created": "Tue, 23 May 2017 18:09:58 GMT"}, {"version": "v4", "created": "Fri, 11 May 2018 18:11:49 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Chen", "Pin-Yu", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1604.03227", "submitter": "Jason Kuen", "authors": "Jason Kuen, Zhenhua Wang, Gang Wang", "title": "Recurrent Attentional Networks for Saliency Detection", "comments": "CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional-deconvolution networks can be adopted to perform end-to-end\nsaliency detection. But, they do not work well with objects of multiple scales.\nTo overcome such a limitation, in this work, we propose a recurrent attentional\nconvolutional-deconvolution network (RACDNN). Using spatial transformer and\nrecurrent network units, RACDNN is able to iteratively attend to selected image\nsub-regions to perform saliency refinement progressively. Besides tackling the\nscale problem, RACDNN can also learn context-aware features from past\niterations to enhance saliency refinement in future iterations. Experiments on\nseveral challenging saliency detection datasets validate the effectiveness of\nRACDNN, and show that RACDNN outperforms state-of-the-art saliency detection\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 03:03:04 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Kuen", "Jason", ""], ["Wang", "Zhenhua", ""], ["Wang", "Gang", ""]]}, {"id": "1604.03257", "submitter": "Tianbao Yang", "authors": "Tianbao Yang, Qihang Lin, Zhe Li", "title": "Unified Convergence Analysis of Stochastic Momentum Methods for Convex\n  and Non-convex Optimization", "comments": "Added some references and more empirical results", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, {\\it stochastic momentum} methods have been widely adopted in\ntraining deep neural networks. However, their convergence analysis is still\nunderexplored at the moment, in particular for non-convex optimization. This\npaper fills the gap between practice and theory by developing a basic\nconvergence analysis of two stochastic momentum methods, namely stochastic\nheavy-ball method and the stochastic variant of Nesterov's accelerated gradient\nmethod. We hope that the basic convergence results developed in this paper can\nserve the reference to the convergence of stochastic momentum methods and also\nserve the baselines for comparison in future development of stochastic momentum\nmethods. The novelty of convergence analysis presented in this paper is a\nunified framework, revealing more insights about the similarities and\ndifferences between different stochastic momentum methods and stochastic\ngradient method. The unified framework exhibits a continuous change from the\ngradient method to Nesterov's accelerated gradient method and finally the\nheavy-ball method incurred by a free parameter, which can help explain a\nsimilar change observed in the testing error convergence behavior for deep\nlearning. Furthermore, our empirical results for optimizing deep neural\nnetworks demonstrate that the stochastic variant of Nesterov's accelerated\ngradient method achieves a good tradeoff (between speed of convergence in\ntraining error and robustness of convergence in testing error) among the three\nstochastic methods.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 06:24:19 GMT"}, {"version": "v2", "created": "Wed, 4 May 2016 23:11:39 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Yang", "Tianbao", ""], ["Lin", "Qihang", ""], ["Li", "Zhe", ""]]}, {"id": "1604.03278", "submitter": "Rocco De Rosa rd", "authors": "Rocco De Rosa", "title": "Confidence Decision Trees via Online and Active Learning for Streaming\n  (BIG) Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision tree classifiers are a widely used tool in data stream mining. The\nuse of confidence intervals to estimate the gain associated with each split\nleads to very effective methods, like the popular Hoeffding tree algorithm.\nFrom a statistical viewpoint, the analysis of decision tree classifiers in a\nstreaming setting requires knowing when enough new information has been\ncollected to justify splitting a leaf. Although some of the issues in the\nstatistical analysis of Hoeffding trees have been already clarified, a general\nand rigorous study of confidence intervals for splitting criteria is missing.\nWe fill this gap by deriving accurate confidence intervals to estimate the\nsplitting gain in decision tree learning with respect to three criteria:\nentropy, Gini index, and a third index proposed by Kearns and Mansour. Our\nconfidence intervals depend in a more detailed way on the tree parameters. We\nalso extend our confidence analysis to a selective sampling setting, in which\nthe decision tree learner adaptively decides which labels to query in the\nstream. We furnish theoretical guarantee bounding the probability that the\nclassification is non-optimal learning the decision tree via our selective\nsampling strategy. Experiments on real and synthetic data in a streaming\nsetting show that our trees are indeed more accurate than trees with the same\nnumber of leaves generated by other techniques and our active learning module\npermits to save labeling cost. In addition, comparing our labeling strategy\nwith recent methods, we show that our approach is more robust and consistent\nrespect all the other techniques applied to incremental decision trees.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 07:59:55 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["De Rosa", "Rocco", ""]]}, {"id": "1604.03343", "submitter": "Jan Leike", "authors": "Daniel Filan, Marcus Hutter, Jan Leike", "title": "Loss Bounds and Time Complexity for Speed Priors", "comments": "AISTATS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes for the first time the predictive performance of speed\npriors and their computational complexity. A speed prior is essentially a\nprobability distribution that puts low probability on strings that are not\nefficiently computable. We propose a variant to the original speed prior\n(Schmidhuber, 2002), and show that our prior can predict sequences drawn from\nprobability measures that are estimable in polynomial time. Our speed prior is\ncomputable in doubly-exponential time, but not in polynomial time. On a\npolynomial time computable sequence our speed prior is computable in\nexponential time. We show better upper complexity bounds for Schmidhuber's\nspeed prior under the same conditions, and that it predicts deterministic\nsequences that are computable in polynomial time; however, we also show that it\nis not computable in polynomial time, and the question of its predictive\nproperties for stochastic sequences remains open.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 11:26:12 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Filan", "Daniel", ""], ["Hutter", "Marcus", ""], ["Leike", "Jan", ""]]}, {"id": "1604.03373", "submitter": "Jiaqian Yu", "authors": "Jiaqian Yu (CVC, GALEN), Matthew Blaschko", "title": "A Convex Surrogate Operator for General Non-Modular Loss Functions", "comments": "in The 19th International Conference on Artificial Intelligence and\n  Statistics, May 2016, Cadiz, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical risk minimization frequently employs convex surrogates to\nunderlying discrete loss functions in order to achieve computational\ntractability during optimization. However, classical convex surrogates can only\ntightly bound modular loss functions, sub-modular functions or supermodular\nfunctions separately while maintaining polynomial time computation. In this\nwork, a novel generic convex surrogate for general non-modular loss functions\nis introduced, which provides for the first time a tractable solution for loss\nfunctions that are neither super-modular nor submodular. This convex surro-gate\nis based on a submodular-supermodular decomposition for which the existence and\nuniqueness is proven in this paper. It takes the sum of two convex surrogates\nthat separately bound the supermodular component and the submodular component\nusing slack-rescaling and the Lov{\\'a}sz hinge, respectively. It is further\nproven that this surrogate is convex , piecewise linear, an extension of the\nloss function, and for which subgradient computation is polynomial time.\nEmpirical results are reported on a non-submodular loss based on the\nS{{\\o}}rensen-Dice difference function, and a real-world face track dataset\nwith tens of thousands of frames, demonstrating the improved performance,\nefficiency, and scalabil-ity of the novel convex surrogate.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 12:31:59 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Yu", "Jiaqian", "", "CVC, GALEN"], ["Blaschko", "Matthew", ""]]}, {"id": "1604.03392", "submitter": "Florimond Gueniat", "authors": "Florimond Gu\\'eniat and Lionel Mathelin and M. Yousuff Hussaini", "title": "A statistical learning strategy for closed-loop control of fluid flows", "comments": null, "journal-ref": null, "doi": "10.1007/s00162-016-0392-y", "report-no": null, "categories": "stat.ML math.OC physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work discusses a closed-loop control strategy for complex systems\nutilizing scarce and streaming data. A discrete embedding space is first built\nusing hash functions applied to the sensor measurements from which a Markov\nprocess model is derived, approximating the complex system's dynamics. A\ncontrol strategy is then learned using reinforcement learning once rewards\nrelevant with respect to the control objective are identified. This method is\ndesigned for experimental configurations, requiring no computations nor prior\nknowledge of the system, and enjoys intrinsic robustness. It is illustrated on\ntwo systems: the control of the transitions of a Lorenz 63 dynamical system,\nand the control of the drag of a cylinder flow. The method is shown to perform\nwell.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 11:13:42 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Gu\u00e9niat", "Florimond", ""], ["Mathelin", "Lionel", ""], ["Hussaini", "M. Yousuff", ""]]}, {"id": "1604.03427", "submitter": "Danica Greetham", "authors": "Nathaniel Charlton, Colin Singleton, Danica Vukadinovi\\'c Greetham", "title": "In the mood: the dynamics of collective sentiments on Twitter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the relationship between the sentiment levels of Twitter users and\nthe evolving network structure that the users created by @-mentioning each\nother. We use a large dataset of tweets to which we apply three sentiment\nscoring algorithms, including the open source SentiStrength program.\nSpecifically we make three contributions. Firstly we find that people who have\npotentially the largest communication reach (according to a dynamic centrality\nmeasure) use sentiment differently than the average user: for example they use\npositive sentiment more often and negative sentiment less often. Secondly we\nfind that when we follow structurally stable Twitter communities over a period\nof months, their sentiment levels are also stable, and sudden changes in\ncommunity sentiment from one day to the next can in most cases be traced to\nexternal events affecting the community. Thirdly, based on our findings, we\ncreate and calibrate a simple agent-based model that is capable of reproducing\nmeasures of emotive response comparable to those obtained from our empirical\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 16:24:22 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Charlton", "Nathaniel", ""], ["Singleton", "Colin", ""], ["Greetham", "Danica Vukadinovi\u0107", ""]]}, {"id": "1604.03463", "submitter": "Farideh Fazayeli", "authors": "Farideh Fazayeli and Arindam Banerjee", "title": "The Matrix Generalized Inverse Gaussian Distribution: Properties and\n  Applications", "comments": "Updated Figure 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the Matrix Generalized Inverse Gaussian ($\\mathcal{MGIG}$) distribution\narises naturally in some settings as a distribution over symmetric positive\nsemi-definite matrices, certain key properties of the distribution and\neffective ways of sampling from the distribution have not been carefully\nstudied. In this paper, we show that the $\\mathcal{MGIG}$ is unimodal, and the\nmode can be obtained by solving an Algebraic Riccati Equation (ARE) equation\n[7]. Based on the property, we propose an importance sampling method for the\n$\\mathcal{MGIG}$ where the mode of the proposal distribution matches that of\nthe target. The proposed sampling method is more efficient than existing\napproaches [32, 33], which use proposal distributions that may have the mode\nfar from the $\\mathcal{MGIG}$'s mode. Further, we illustrate that the the\nposterior distribution in latent factor models, such as probabilistic matrix\nfactorization (PMF) [25], when marginalized over one latent factor has the\n$\\mathcal{MGIG}$ distribution. The characterization leads to a novel Collapsed\nMonte Carlo (CMC) inference algorithm for such latent factor models. We\nillustrate that CMC has a lower log loss or perplexity than MCMC, and needs\nfewer samples.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 16:03:31 GMT"}, {"version": "v2", "created": "Mon, 22 Aug 2016 16:05:42 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Fazayeli", "Farideh", ""], ["Banerjee", "Arindam", ""]]}, {"id": "1604.03492", "submitter": "Sheng Chen", "authors": "Sheng Chen and Arindam Banerjee", "title": "Structured Matrix Recovery via the Generalized Dantzig Selector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, structured matrix recovery problems have gained considerable\nattention for its real world applications, such as recommender systems and\ncomputer vision. Much of the existing work has focused on matrices with\nlow-rank structure, and limited progress has been made matrices with other\ntypes of structure. In this paper we present non-asymptotic analysis for\nestimation of generally structured matrices via the generalized Dantzig\nselector under generic sub-Gaussian measurements. We show that the estimation\nerror can always be succinctly expressed in terms of a few geometric measures\nof suitable sets which only depend on the structure of the underlying true\nmatrix. In addition, we derive the general bounds on these geometric measures\nfor structures characterized by unitarily invariant norms, which is a large\nfamily covering most matrix norms of practical interest. Examples are provided\nto illustrate the utility of our theoretical development.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 17:54:14 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Chen", "Sheng", ""], ["Banerjee", "Arindam", ""]]}, {"id": "1604.03601", "submitter": "Yuan Li", "authors": "Yuan Li", "title": "Community Detection with Node Attributes and its Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection algorithms are fundamental tools to understand\norganizational principles in social networks. With the increasing power of\nsocial media platforms, when detecting communities there are two possi- ble\nsources of information one can use: the structure of social network and node\nattributes. However structure of social networks and node attributes are often\ninterpreted separately in the research of community detection. When these two\nsources are interpreted simultaneously, one common as- sumption shared by\nprevious studies is that nodes attributes are correlated with communities. In\nthis paper, we present a model that is capable of combining topology\ninformation and nodes attributes information with- out assuming correlation.\nThis new model can recover communities with higher accuracy even when node\nattributes and communities are uncorre- lated. We derive the detectability\nthreshold for this model and use Belief Propagation (BP) to make inference.\nThis algorithm is optimal in the sense that it can recover community all the\nway down to the threshold. This new model is also with the potential to handle\nedge content and dynamic settings.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 22:09:02 GMT"}], "update_date": "2016-04-14", "authors_parsed": [["Li", "Yuan", ""]]}, {"id": "1604.03736", "submitter": "Sebastian Urban", "authors": "Wiebke K\\\"opp, Patrick van der Smagt, Sebastian Urban", "title": "A Differentiable Transition Between Additive and Multiplicative Neurons", "comments": "ICLR 2016 extended abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches to combine both additive and multiplicative neural units\neither use a fixed assignment of operations or require discrete optimization to\ndetermine what function a neuron should perform. However, this leads to an\nextensive increase in the computational complexity of the training procedure.\n  We present a novel, parameterizable transfer function based on the\nmathematical concept of non-integer functional iteration that allows the\noperation each neuron performs to be smoothly and, most importantly,\ndifferentiablely adjusted between addition and multiplication. This allows the\ndecision between addition and multiplication to be integrated into the standard\nbackpropagation training procedure.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 12:38:27 GMT"}], "update_date": "2016-04-14", "authors_parsed": [["K\u00f6pp", "Wiebke", ""], ["van der Smagt", "Patrick", ""], ["Urban", "Sebastian", ""]]}, {"id": "1604.03744", "submitter": "Mihai-Alin Badiu", "authors": "Mihai-Alin Badiu, Thomas Lundgaard Hansen and Bernard Henri Fleury", "title": "Variational Bayesian Inference of Line Spectra", "comments": "15 pages, 8 figures, accepted for publication in IEEE Transactions on\n  Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2017.2655489", "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the fundamental problem of line spectral estimation\nin a Bayesian framework. We target model order and parameter estimation via\nvariational inference in a probabilistic model in which the frequencies are\ncontinuous-valued, i.e., not restricted to a grid; and the coefficients are\ngoverned by a Bernoulli-Gaussian prior model turning model order selection into\nbinary sequence detection. Unlike earlier works which retain only point\nestimates of the frequencies, we undertake a more complete Bayesian treatment\nby estimating the posterior probability density functions (pdfs) of the\nfrequencies and computing expectations over them. Thus, we additionally capture\nand operate with the uncertainty of the frequency estimates. Aiming to maximize\nthe model evidence, variational optimization provides analytic approximations\nof the posterior pdfs and also gives estimates of the additional parameters. We\npropose an accurate representation of the pdfs of the frequencies by mixtures\nof von Mises pdfs, which yields closed-form expectations. We define the\nalgorithm VALSE in which the estimates of the pdfs and parameters are\niteratively updated. VALSE is a gridless, convergent method, does not require\nparameter tuning, can easily include prior knowledge about the frequencies and\nprovides approximate posterior pdfs based on which the uncertainty in line\nspectral estimation can be quantified. Simulation results show that accounting\nfor the uncertainty of frequency estimates, rather than computing just point\nestimates, significantly improves the performance. The performance of VALSE is\nsuperior to that of state-of-the-art methods and closely approaches the\nCram\\'er-Rao bound computed for the true model order.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 12:51:51 GMT"}, {"version": "v2", "created": "Mon, 9 Jan 2017 17:26:05 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Badiu", "Mihai-Alin", ""], ["Hansen", "Thomas Lundgaard", ""], ["Fleury", "Bernard Henri", ""]]}, {"id": "1604.03763", "submitter": "Shun Zheng", "authors": "Shun Zheng, Jialei Wang, Fen Xia, Wei Xu, Tong Zhang", "title": "A General Distributed Dual Coordinate Optimization Framework for\n  Regularized Loss Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern large-scale machine learning applications, the training data are\noften partitioned and stored on multiple machines. It is customary to employ\nthe \"data parallelism\" approach, where the aggregated training loss is\nminimized without moving data across machines. In this paper, we introduce a\nnovel distributed dual formulation for regularized loss minimization problems\nthat can directly handle data parallelism in the distributed setting. This\nformulation allows us to systematically derive dual coordinate optimization\nprocedures, which we refer to as Distributed Alternating Dual Maximization\n(DADM). The framework extends earlier studies described in (Boyd et al., 2011;\nMa et al., 2015a; Jaggi et al., 2014; Yang, 2013) and has rigorous theoretical\nanalyses. Moreover with the help of the new formulation, we develop the\naccelerated version of DADM (Acc-DADM) by generalizing the acceleration\ntechnique from (Shalev-Shwartz and Zhang, 2014) to the distributed setting. We\nalso provide theoretical results for the proposed accelerated version and the\nnew result improves previous ones (Yang, 2013; Ma et al., 2015a) whose runtimes\ngrow linearly on the condition number. Our empirical studies validate our\ntheory and show that our accelerated approach significantly improves the\nprevious state-of-the-art distributed dual coordinate optimization algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 13:33:32 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 15:00:24 GMT"}, {"version": "v3", "created": "Fri, 25 Aug 2017 02:42:32 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Zheng", "Shun", ""], ["Wang", "Jialei", ""], ["Xia", "Fen", ""], ["Xu", "Wei", ""], ["Zhang", "Tong", ""]]}, {"id": "1604.03853", "submitter": "Mehmet Basbug", "authors": "Mehmet E. Basbug, Barbara E. Engelhardt", "title": "Hierarchical Compound Poisson Factorization", "comments": "Will appear on Proceedings of the 33 rd International Conference on\n  Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Non-negative matrix factorization models based on a hierarchical\nGamma-Poisson structure capture user and item behavior effectively in extremely\nsparse data sets, making them the ideal choice for collaborative filtering\napplications. Hierarchical Poisson factorization (HPF) in particular has proved\nsuccessful for scalable recommendation systems with extreme sparsity. HPF,\nhowever, suffers from a tight coupling of sparsity model (absence of a rating)\nand response model (the value of the rating), which limits the expressiveness\nof the latter. Here, we introduce hierarchical compound Poisson factorization\n(HCPF) that has the favorable Gamma-Poisson structure and scalability of HPF to\nhigh-dimensional extremely sparse matrices. More importantly, HCPF decouples\nthe sparsity model from the response model, allowing us to choose the most\nsuitable distribution for the response. HCPF can capture binary, non-negative\ndiscrete, non-negative continuous, and zero-inflated continuous responses. We\ncompare HCPF with HPF on nine discrete and three continuous data sets and\nconclude that HCPF captures the relationship between sparsity and response\nbetter than HPF.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 16:12:01 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 11:09:19 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Basbug", "Mehmet E.", ""], ["Engelhardt", "Barbara E.", ""]]}, {"id": "1604.03887", "submitter": "Zhiqiang Zhou", "authors": "Guanghui Lan and Zhiqiang Zhou", "title": "Algorithms for stochastic optimization with functional or expectation\n  constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of minimizing an expectation function over a\nclosed convex set, coupled with a {\\color{black} functional or expectation}\nconstraint on either decision variables or problem parameters. We first present\na new stochastic approximation (SA) type algorithm, namely the cooperative SA\n(CSA), to handle problems with the constraint on devision variables. We show\nthat this algorithm exhibits the optimal ${\\cal O}(1/\\epsilon^2)$ rate of\nconvergence, in terms of both optimality gap and constraint violation, when the\nobjective and constraint functions are generally convex, where $\\epsilon$\ndenotes the optimality gap and infeasibility. Moreover, we show that this rate\nof convergence can be improved to ${\\cal O}(1/\\epsilon)$ if the objective and\nconstraint functions are strongly convex. We then present a variant of CSA,\nnamely the cooperative stochastic parameter approximation (CSPA) algorithm, to\ndeal with the situation when the constraint is defined over problem parameters\nand show that it exhibits similar optimal rate of convergence to CSA. It is\nworth noting that CSA and CSPA are primal methods which do not require the\niterations on the dual space and/or the estimation on the size of the dual\nvariables. To the best of our knowledge, this is the first time that such\noptimal SA methods for solving functional or expectation constrained stochastic\noptimization are presented in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 17:49:30 GMT"}, {"version": "v2", "created": "Sat, 20 Aug 2016 03:19:35 GMT"}, {"version": "v3", "created": "Mon, 29 May 2017 16:08:33 GMT"}, {"version": "v4", "created": "Thu, 6 Sep 2018 16:53:58 GMT"}, {"version": "v5", "created": "Fri, 30 Nov 2018 17:03:12 GMT"}, {"version": "v6", "created": "Thu, 6 Dec 2018 22:27:52 GMT"}, {"version": "v7", "created": "Thu, 8 Aug 2019 19:16:58 GMT"}, {"version": "v8", "created": "Fri, 2 Oct 2020 15:28:50 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Lan", "Guanghui", ""], ["Zhou", "Zhiqiang", ""]]}, {"id": "1604.03912", "submitter": "Michael Herman", "authors": "Michael Herman, Tobias Gindele, J\\\"org Wagner, Felix Schmitt, Wolfram\n  Burgard", "title": "Inverse Reinforcement Learning with Simultaneous Estimation of Rewards\n  and Dynamics", "comments": "accepted to appear in AISTATS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse Reinforcement Learning (IRL) describes the problem of learning an\nunknown reward function of a Markov Decision Process (MDP) from observed\nbehavior of an agent. Since the agent's behavior originates in its policy and\nMDP policies depend on both the stochastic system dynamics as well as the\nreward function, the solution of the inverse problem is significantly\ninfluenced by both. Current IRL approaches assume that if the transition model\nis unknown, additional samples from the system's dynamics are accessible, or\nthe observed behavior provides enough samples of the system's dynamics to solve\nthe inverse problem accurately. These assumptions are often not satisfied. To\novercome this, we present a gradient-based IRL approach that simultaneously\nestimates the system's dynamics. By solving the combined optimization problem,\nour approach takes into account the bias of the demonstrations, which stems\nfrom the generating policy. The evaluation on a synthetic MDP and a transfer\nlearning task shows improvements regarding the sample efficiency as well as the\naccuracy of the estimated reward functions and transition models.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 19:06:41 GMT"}], "update_date": "2016-04-14", "authors_parsed": [["Herman", "Michael", ""], ["Gindele", "Tobias", ""], ["Wagner", "J\u00f6rg", ""], ["Schmitt", "Felix", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1604.03930", "submitter": "Chi Jin", "authors": "Rong Ge, Chi Jin, Sham M. Kakade, Praneeth Netrapalli, Aaron Sidford", "title": "Efficient Algorithms for Large-scale Generalized Eigenvector Computation\n  and Canonical Correlation Analysis", "comments": "International Conference on Machine Learning (ICML) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of canonical-correlation analysis (CCA)\n(Hotelling, 1936) and, more broadly, the generalized eigenvector problem for a\npair of symmetric matrices. These are two fundamental problems in data analysis\nand scientific computing with numerous applications in machine learning and\nstatistics (Shi and Malik, 2000; Hardoon et al., 2004; Witten et al., 2009).\n  We provide simple iterative algorithms, with improved runtimes, for solving\nthese problems that are globally linearly convergent with moderate dependencies\non the condition numbers and eigenvalue gaps of the matrices involved.\n  We obtain our results by reducing CCA to the top-$k$ generalized eigenvector\nproblem. We solve this problem through a general framework that simply requires\nblack box access to an approximate linear system solver. Instantiating this\nframework with accelerated gradient descent we obtain a running time of\n$O(\\frac{z k \\sqrt{\\kappa}}{\\rho} \\log(1/\\epsilon) \\log\n\\left(k\\kappa/\\rho\\right))$ where $z$ is the total number of nonzero entries,\n$\\kappa$ is the condition number and $\\rho$ is the relative eigenvalue gap of\nthe appropriate matrices.\n  Our algorithm is linear in the input size and the number of components $k$ up\nto a $\\log(k)$ factor. This is essential for handling large-scale matrices that\nappear in practice. To the best of our knowledge this is the first such\nalgorithm with global linear convergence. We hope that our results prompt\nfurther research and ultimately improve the practical running time for\nperforming these important data analysis procedures on large data sets.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 19:57:46 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 18:03:11 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Ge", "Rong", ""], ["Jin", "Chi", ""], ["Kakade", "Sham M.", ""], ["Netrapalli", "Praneeth", ""], ["Sidford", "Aaron", ""]]}, {"id": "1604.04054", "submitter": "Gilles Blanchard", "authors": "Gilles Blanchard, Nicole M\\\"ucke", "title": "Optimal Rates For Regularization Of Statistical Inverse Learning\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a statistical inverse learning problem, where we observe the\nimage of a function $f$ through a linear operator $A$ at i.i.d. random design\npoints $X_i$, superposed with an additive noise. The distribution of the design\npoints is unknown and can be very general. We analyze simultaneously the direct\n(estimation of $Af$) and the inverse (estimation of $f$) learning problems. In\nthis general framework, we obtain strong and weak minimax optimal rates of\nconvergence (as the number of observations $n$ grows large) for a large class\nof spectral regularization methods over regularity classes defined through\nappropriate source conditions. This improves on or completes previous results\nobtained in related settings. The optimality of the obtained rates is shown not\nonly in the exponent in $n$ but also in the explicit dependency of the constant\nfactor in the variance of the noise and the radius of the source condition set.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 07:23:56 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Blanchard", "Gilles", ""], ["M\u00fccke", "Nicole", ""]]}, {"id": "1604.04173", "submitter": "Jing Lei", "authors": "Jing Lei, Max G'Sell, Alessandro Rinaldo, Ryan J. Tibshirani and Larry\n  Wasserman", "title": "Distribution-Free Predictive Inference For Regression", "comments": "50 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a general framework for distribution-free predictive inference in\nregression, using conformal inference. The proposed methodology allows for the\nconstruction of a prediction band for the response variable using any estimator\nof the regression function. The resulting prediction band preserves the\nconsistency properties of the original estimator under standard assumptions,\nwhile guaranteeing finite-sample marginal coverage even when these assumptions\ndo not hold. We analyze and compare, both empirically and theoretically, the\ntwo major variants of our conformal framework: full conformal inference and\nsplit conformal inference, along with a related jackknife method. These methods\noffer different tradeoffs between statistical accuracy (length of resulting\nprediction intervals) and computational efficiency. As extensions, we develop a\nmethod for constructing valid in-sample prediction intervals called {\\it\nrank-one-out} conformal inference, which has essentially the same computational\nefficiency as split conformal inference. We also describe an extension of our\nprocedures for producing prediction bands with locally varying length, in order\nto adapt to heteroskedascity in the data. Finally, we propose a model-free\nnotion of variable importance, called {\\it leave-one-covariate-out} or LOCO\ninference. Accompanying this paper is an R package {\\tt conformalInference}\nthat implements all of the proposals we have introduced. In the spirit of\nreproducibility, all of our empirical results can also be easily (re)generated\nusing this package.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 14:46:16 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 18:52:14 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Lei", "Jing", ""], ["G'Sell", "Max", ""], ["Rinaldo", "Alessandro", ""], ["Tibshirani", "Ryan J.", ""], ["Wasserman", "Larry", ""]]}, {"id": "1604.04182", "submitter": "Garrett Bernstein", "authors": "Garrett Bernstein and Daniel Sheldon", "title": "Consistently Estimating Markov Chains with Noisy Aggregate Data", "comments": "AISTATS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of estimating the parameters of a time-homogeneous\nMarkov chain given only noisy, aggregate data. This arises when a population of\nindividuals behave independently according to a Markov chain, but individual\nsample paths cannot be observed due to limitations of the observation process\nor the need to protect privacy. Instead, only population-level counts of the\nnumber of individuals in each state at each time step are available. When these\ncounts are exact, a conditional least squares (CLS) estimator is known to be\nconsistent and asymptotically normal. We initiate the study of method of\nmoments estimators for this problem to handle the more realistic case when\nobservations are additionally corrupted by noise. We show that CLS can be\ninterpreted as a simple \"plug-in\" method of moments estimator. However, when\nobservations are noisy, it is not consistent because it fails to account for\nadditional variance introduced by the noise. We develop a new, simpler method\nof moments estimator that bypasses this problem and is consistent under noisy\nobservations.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 15:13:06 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Bernstein", "Garrett", ""], ["Sheldon", "Daniel", ""]]}, {"id": "1604.04191", "submitter": "Pierre Alquier", "authors": "Vincent Cottet and Pierre Alquier", "title": "1-bit Matrix Completion: PAC-Bayesian Analysis of a Variational\n  Approximation", "comments": null, "journal-ref": "Machine Learning, 2018, vol. 107, no. 3, pp. 579-603", "doi": "10.1007/s10994-017-5667-z", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to challenging applications such as collaborative filtering, the matrix\ncompletion problem has been widely studied in the past few years. Different\napproaches rely on different structure assumptions on the matrix in hand. Here,\nwe focus on the completion of a (possibly) low-rank matrix with binary entries,\nthe so-called 1-bit matrix completion problem. Our approach relies on tools\nfrom machine learning theory: empirical risk minimization and its convex\nrelaxations. We propose an algorithm to compute a variational approximation of\nthe pseudo-posterior. Thanks to the convex relaxation, the corresponding\nminimization problem is bi-convex, and thus the method behaves well in\npractice. We also study the performance of this variational approximation\nthrough PAC-Bayesian learning bounds. On the contrary to previous works that\nfocused on upper bounds on the estimation error of M with various matrix norms,\nwe are able to derive from this analysis a PAC bound on the prediction error of\nour algorithm.\n  We focus essentially on convex relaxation through the hinge loss, for which\nwe present the complete analysis, a complete simulation study and a test on the\nMovieLens data set. However, we also discuss a variational approximation to\ndeal with the logistic loss.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 15:42:03 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Cottet", "Vincent", ""], ["Alquier", "Pierre", ""]]}, {"id": "1604.04198", "submitter": "Christian Huemmer M.Sc.", "authors": "Christian Huemmer, Christian Hofmann, Roland Maas, Walter Kellermann", "title": "Estimating parameters of nonlinear systems using the elitist particle\n  filter based on evolutionary strategies", "comments": "13 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we present the elitist particle filter based on evolutionary\nstrategies (EPFES) as an efficient approach for nonlinear system\nidentification. The EPFES is derived from the frequently-employed state-space\nmodel, where the relevant information of the nonlinear system is captured by an\nunknown state vector. Similar to classical particle filtering, the EPFES\nconsists of a set of particles and respective weights which represent different\nrealizations of the latent state vector and their likelihood of being the\nsolution of the optimization problem. As main innovation, the EPFES includes an\nevolutionary elitist-particle selection which combines long-term information\nwith instantaneous sampling from an approximated continuous posterior\ndistribution. In this article, we propose two advancements of the\npreviously-published elitist-particle selection process. Further, the EPFES is\nshown to be a generalization of the widely-used Gaussian particle filter and\nthus evaluated with respect to the latter for two completely different\nscenarios: First, we consider the so-called univariate nonstationary growth\nmodel with time-variant latent state variable, where the evolutionary selection\nof elitist particles is evaluated for non-recursively calculated particle\nweights. Second, the problem of nonlinear acoustic echo cancellation is\naddressed in a simulated scenario with speech as input signal: By using\nlong-term fitness measures, we highlight the efficacy of the well-generalizing\nEPFES in estimating the nonlinear system even for large search spaces. Finally,\nwe illustrate similarities between the EPFES and evolutionary algorithms to\noutline future improvements by fusing the achievements of both fields of\nresearch.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 15:59:35 GMT"}, {"version": "v2", "created": "Fri, 15 Apr 2016 09:07:05 GMT"}, {"version": "v3", "created": "Tue, 24 May 2016 09:41:22 GMT"}, {"version": "v4", "created": "Wed, 25 May 2016 05:24:48 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Huemmer", "Christian", ""], ["Hofmann", "Christian", ""], ["Maas", "Roland", ""], ["Kellermann", "Walter", ""]]}, {"id": "1604.04280", "submitter": "Fan Zhang", "authors": "Fan Zhang, Patrick Flaherty", "title": "Variational inference for rare variant detection in deep, heterogeneous\n  next-generation sequencing data", "comments": "19 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of rare variants is important for understanding the genetic\nheterogeneity in mixed samples. Recently, next-generation sequencing (NGS)\ntechnologies have enabled the identification of single nucleotide variants\n(SNVs) in mixed samples with high resolution. Yet, the noise inherent in the\nbiological processes involved in next-generation sequencing necessitates the\nuse of statistical methods to identify true rare variants. We propose a novel\nBayesian statistical model and a variational expectation-maximization (EM)\nalgorithm to estimate non-reference allele frequency (NRAF) and identify SNVs\nin heterogeneous cell populations. We demonstrate that our variational EM\nalgorithm has comparable sensitivity and specificity compared with a Markov\nChain Monte Carlo (MCMC) sampling inference algorithm, and is more\ncomputationally efficient on tests of low coverage ($27\\times$ and $298\\times$)\ndata. Furthermore, we show that our model with a variational EM inference\nalgorithm has higher specificity than many state-of-the-art algorithms. In an\nanalysis of a directed evolution longitudinal yeast data set, we are able to\nidentify a time-series trend in non-reference allele frequency and detect novel\nvariants that have not yet been reported. Our model also detects the emergence\nof a beneficial variant earlier than was previously shown, and a pair of\nconcomitant variants.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 19:58:38 GMT"}, {"version": "v2", "created": "Fri, 22 Apr 2016 03:05:18 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Zhang", "Fan", ""], ["Flaherty", "Patrick", ""]]}, {"id": "1604.04348", "submitter": "Fei Wen", "authors": "Fei Wen, Yuan Yang, Peilin Liu, and Robert C. Qiu", "title": "Positive Definite Estimation of Large Covariance Matrix Using\n  Generalized Nonconvex Penalties", "comments": "15 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the issue of large covariance matrix estimation in\nhigh-dimensional statistical analysis. Recently, improved iterative algorithms\nwith positive-definite guarantee have been developed. However, these algorithms\ncannot be directly extended to use a nonconvex penalty for sparsity inducing.\nGenerally, a nonconvex penalty has the capability of ameliorating the bias\nproblem of the popular convex lasso penalty, and thus is more advantageous. In\nthis work, we propose a class of positive-definite covariance estimators using\ngeneralized nonconvex penalties. We develop a first-order algorithm based on\nthe alternating direction method framework to solve the nonconvex optimization\nproblem efficiently. The convergence of this algorithm has been proved.\nFurther, the statistical properties of the new estimators have been analyzed\nfor generalized nonconvex penalties. Moreover, extension of this algorithm to\ncovariance estimation from sketched measurements has been considered. The\nperformances of the new estimators have been demonstrated by both a simulation\nstudy and a gene clustering example for tumor tissues. Code for the proposed\nestimators is available at https://github.com/FWen/Nonconvex-PDLCE.git.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 03:50:57 GMT"}, {"version": "v2", "created": "Tue, 14 Jun 2016 03:58:42 GMT"}, {"version": "v3", "created": "Thu, 28 Jul 2016 13:23:57 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Wen", "Fei", ""], ["Yang", "Yuan", ""], ["Liu", "Peilin", ""], ["Qiu", "Robert C.", ""]]}, {"id": "1604.04434", "submitter": "Chaobing Song", "authors": "Chaobing Song, Shu-Tao Xia", "title": "Bayesian linear regression with Student-t assumptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an automatic method of determining model complexity using the training\ndata alone, Bayesian linear regression provides us a principled way to select\nhyperparameters. But one often needs approximation inference if distribution\nassumption is beyond Gaussian distribution. In this paper, we propose a\nBayesian linear regression model with Student-t assumptions (BLRS), which can\nbe inferred exactly. In this framework, both conjugate prior and expectation\nmaximization (EM) algorithm are generalized. Meanwhile, we prove that the\nmaximum likelihood solution is equivalent to the standard Bayesian linear\nregression with Gaussian assumptions (BLRG). The $q$-EM algorithm for BLRS is\nnearly identical to the EM algorithm for BLRG. It is showed that $q$-EM for\nBLRS can converge faster than EM for BLRG for the task of predicting online\nnews popularity.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 11:21:27 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Song", "Chaobing", ""], ["Xia", "Shu-Tao", ""]]}, {"id": "1604.04451", "submitter": "Josef Kittler", "authors": "Josef Kittler and Cemre Zor", "title": "Delta divergence: A novel decision cognizant measure of classifier\n  incongruence", "comments": "12 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disagreement between two classifiers regarding the class membership of an\nobservation in pattern recognition can be indicative of an anomaly and its\nnuance. As in general classifiers base their decision on class aposteriori\nprobabilities, the most natural approach to detecting classifier incongruence\nis to use divergence. However, existing divergences are not particularly\nsuitable to gauge classifier incongruence. In this paper, we postulate the\nproperties that a divergence measure should satisfy and propose a novel\ndivergence measure, referred to as Delta divergence. In contrast to existing\nmeasures, it is decision cognizant. The focus in Delta divergence on the\ndominant hypotheses has a clutter reducing property, the significance of which\ngrows with increasing number of classes. The proposed measure satisfies other\nimportant properties such as symmetry, and independence of classifier\nconfidence. The relationship of the proposed divergence to some baseline\nmeasures is demonstrated experimentally, showing its superiority.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 12:06:48 GMT"}, {"version": "v2", "created": "Mon, 4 Jul 2016 13:18:52 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Kittler", "Josef", ""], ["Zor", "Cemre", ""]]}, {"id": "1604.04505", "submitter": "Andreas Christmann", "authors": "Andreas Christmann, Florian Dumpert, and Dao-Hong Xiang", "title": "A short note on extension theorems and their connection to universal\n  consistency in machine learning", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical machine learning plays an important role in modern statistics and\ncomputer science. One main goal of statistical machine learning is to provide\nuniversally consistent algorithms, i.e., the estimator converges in probability\nor in some stronger sense to the Bayes risk or to the Bayes decision function.\nKernel methods based on minimizing the regularized risk over a reproducing\nkernel Hilbert space (RKHS) belong to these statistical machine learning\nmethods. It is in general unknown which kernel yields optimal results for a\nparticular data set or for the unknown probability measure. Hence various\nkernel learning methods were proposed to choose the kernel and therefore also\nits RKHS in a data adaptive manner. Nevertheless, many practitioners often use\nthe classical Gaussian RBF kernel or certain Sobolev kernels with good success.\nThe goal of this short note is to offer one possible theoretical explanation\nfor this empirical fact.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 13:51:57 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Christmann", "Andreas", ""], ["Dumpert", "Florian", ""], ["Xiang", "Dao-Hong", ""]]}, {"id": "1604.04562", "submitter": "Tsung-Hsien Wen", "authors": "Tsung-Hsien Wen, David Vandyke, Nikola Mrksic, Milica Gasic, Lina M.\n  Rojas-Barahona, Pei-Hao Su, Stefan Ultes, Steve Young", "title": "A Network-based End-to-End Trainable Task-oriented Dialogue System", "comments": "published at EACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Teaching machines to accomplish tasks by conversing naturally with humans is\nchallenging. Currently, developing task-oriented dialogue systems requires\ncreating multiple components and typically this involves either a large amount\nof handcrafting, or acquiring costly labelled datasets to solve a statistical\nlearning problem for each component. In this work we introduce a neural\nnetwork-based text-in, text-out end-to-end trainable goal-oriented dialogue\nsystem along with a new way of collecting dialogue data based on a novel\npipe-lined Wizard-of-Oz framework. This approach allows us to develop dialogue\nsystems easily and without making too many assumptions about the task at hand.\nThe results show that the model can converse with human subjects naturally\nwhilst helping them to accomplish tasks in a restaurant search domain.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 16:40:49 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 14:03:58 GMT"}, {"version": "v3", "created": "Mon, 24 Apr 2017 10:55:12 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Wen", "Tsung-Hsien", ""], ["Vandyke", "David", ""], ["Mrksic", "Nikola", ""], ["Gasic", "Milica", ""], ["Rojas-Barahona", "Lina M.", ""], ["Su", "Pei-Hao", ""], ["Ultes", "Stefan", ""], ["Young", "Steve", ""]]}, {"id": "1604.04600", "submitter": "Dong Xia", "authors": "Dong Xia and Vladimir Koltchinskii", "title": "Estimation of low rank density matrices: bounds in Schatten norms and\n  other distances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let ${\\mathcal S}_m$ be the set of all $m\\times m$ density matrices\n(Hermitian positively semi-definite matrices of unit trace). Consider a problem\nof estimation of an unknown density matrix $\\rho\\in {\\mathcal S}_m$ based on\noutcomes of $n$ measurements of observables $X_1,\\dots, X_n\\in {\\mathbb H}_m$\n(${\\mathbb H}_m$ being the space of $m\\times m$ Hermitian matrices) for a\nquantum system identically prepared $n$ times in state $\\rho.$ Outcomes\n$Y_1,\\dots, Y_n$ of such measurements could be described by a trace regression\nmodel in which ${\\mathbb E}_{\\rho}(Y_j|X_j)={\\rm tr}(\\rho X_j), j=1,\\dots, n.$\nThe design variables $X_1,\\dots, X_n$ are often sampled at random from the\nuniform distribution in an orthonormal basis $\\{E_1,\\dots, E_{m^2}\\}$ of\n${\\mathbb H}_m$ (such as Pauli basis). The goal is to estimate the unknown\ndensity matrix $\\rho$ based on the data $(X_1,Y_1), \\dots, (X_n,Y_n).$ Let $$\n\\hat Z:=\\frac{m^2}{n}\\sum_{j=1}^n Y_j X_j $$ and let $\\check \\rho$ be the\nprojection of $\\hat Z$ onto the convex set ${\\mathcal S}_m$ of density\nmatrices. It is shown that for estimator $\\check \\rho$ the minimax lower bounds\nin classes of low rank density matrices (established earlier) are attained up\nlogarithmic factors for all Schatten $p$-norm distances, $p\\in [1,\\infty]$ and\nfor Bures version of quantum Hellinger distance. Moreover, for a slightly\nmodified version of estimator $\\check \\rho$ the same property holds also for\nquantum relative entropy (Kullback-Leibler) distance between density matrices.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 18:56:35 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Xia", "Dong", ""], ["Koltchinskii", "Vladimir", ""]]}, {"id": "1604.04615", "submitter": "Shuchin Aeron", "authors": "Wenqi Wang, Shuchin Aeron, Vaneet Aggarwal", "title": "On deterministic conditions for subspace clustering under missing data", "comments": "To appear in proceedings of ISIT 2016", "journal-ref": "IEEE International Symposium on Information Theory (ISIT),\n  Barcelona, pp. 850-854 (2016)", "doi": "10.1109/ISIT.2016.7541419", "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present deterministic analysis of sufficient conditions for\nsparse subspace clustering under missing data, when data is assumed to come\nfrom a Union of Subspaces (UoS) model. In this context we consider two cases,\nnamely Case I when all the points are sampled at the same co-ordinates, and\nCase II when points are sampled at different locations. We show that results\nfor Case I directly follow from several existing results in the literature,\nwhile results for Case II are not as straightforward and we provide a set of\ndual conditions under which, perfect clustering holds true. We provide\nextensive set of simulation results for clustering as well as completion of\ndata under missing entries, under the UoS model. Our experimental results\nindicate that in contrast to the full data case, accurate clustering does not\nimply accurate subspace identification and completion, indicating the natural\norder of relative hardness of these problems.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 19:47:25 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Wang", "Wenqi", ""], ["Aeron", "Shuchin", ""], ["Aggarwal", "Vaneet", ""]]}, {"id": "1604.04661", "submitter": "Shihao Ji", "authors": "Shihao Ji, Nadathur Satish, Sheng Li, and Pradeep Dubey", "title": "Parallelizing Word2Vec in Shared and Distributed Memory", "comments": "Added more results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word2Vec is a widely used algorithm for extracting low-dimensional vector\nrepresentations of words. It generated considerable excitement in the machine\nlearning and natural language processing (NLP) communities recently due to its\nexceptional performance in many NLP applications such as named entity\nrecognition, sentiment analysis, machine translation and question answering.\nState-of-the-art algorithms including those by Mikolov et al. have been\nparallelized for multi-core CPU architectures but are based on vector-vector\noperations that are memory-bandwidth intensive and do not efficiently use\ncomputational resources. In this paper, we improve reuse of various data\nstructures in the algorithm through the use of minibatching, hence allowing us\nto express the problem using matrix multiply operations. We also explore\ndifferent techniques to distribute word2vec computation across nodes in a\ncompute cluster, and demonstrate good strong scalability up to 32 nodes. In\ncombination, these techniques allow us to scale up the computation near\nlinearly across cores and nodes, and process hundreds of millions of words per\nsecond, which is the fastest word2vec implementation to the best of our\nknowledge.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 23:40:04 GMT"}, {"version": "v2", "created": "Mon, 8 Aug 2016 17:45:00 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Ji", "Shihao", ""], ["Satish", "Nadathur", ""], ["Li", "Sheng", ""], ["Dubey", "Pradeep", ""]]}, {"id": "1604.04706", "submitter": "Parameswaran Raman", "authors": "Parameswaran Raman, Sriram Srinivasan, Shin Matsushima, Xinhua Zhang,\n  Hyokun Yun, S.V.N. Vishwanathan", "title": "DS-MLR: Exploiting Double Separability for Scaling up Distributed\n  Multinomial Logistic Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scaling multinomial logistic regression to datasets with very large number of\ndata points and classes is challenging. This is primarily because one needs to\ncompute the log-partition function on every data point. This makes distributing\nthe computation hard. In this paper, we present a distributed stochastic\ngradient descent based optimization method (DS-MLR) for scaling up multinomial\nlogistic regression problems to massive scale datasets without hitting any\nstorage constraints on the data and model parameters. Our algorithm exploits\ndouble-separability, an attractive property that allows us to achieve both data\nas well as model parallelism simultaneously. In addition, we introduce a\nnon-blocking and asynchronous variant of our algorithm that avoids\nbulk-synchronization. We demonstrate the versatility of DS-MLR to various\nscenarios in data and model parallelism, through an extensive empirical study\nusing several real-world datasets. In particular, we demonstrate the\nscalability of DS-MLR by solving an extreme multi-class classification problem\non the Reddit dataset (159 GB data, 358 GB parameters) where, to the best of\nour knowledge, no other existing methods apply.\n", "versions": [{"version": "v1", "created": "Sat, 16 Apr 2016 07:26:58 GMT"}, {"version": "v2", "created": "Fri, 31 Mar 2017 18:45:59 GMT"}, {"version": "v3", "created": "Tue, 23 May 2017 08:06:02 GMT"}, {"version": "v4", "created": "Thu, 15 Feb 2018 01:02:54 GMT"}, {"version": "v5", "created": "Wed, 18 Apr 2018 01:15:04 GMT"}, {"version": "v6", "created": "Mon, 21 May 2018 23:44:36 GMT"}, {"version": "v7", "created": "Fri, 3 Aug 2018 22:13:06 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Raman", "Parameswaran", ""], ["Srinivasan", "Sriram", ""], ["Matsushima", "Shin", ""], ["Zhang", "Xinhua", ""], ["Yun", "Hyokun", ""], ["Vishwanathan", "S. V. N.", ""]]}, {"id": "1604.04741", "submitter": "Cheng Luo", "authors": "Cheng Luo, Yang Xiang and Richard Yi Da Xu", "title": "Smoothed Hierarchical Dirichlet Process: A Non-Parametric Approach to\n  Constraint Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-varying mixture densities occur in many scenarios, for example, the\ndistributions of keywords that appear in publications may evolve from year to\nyear, video frame features associated with multiple targets may evolve in a\nsequence. Any models that realistically cater to this phenomenon must exhibit\ntwo important properties: the underlying mixture densities must have an unknown\nnumber of mixtures, and there must be some \"smoothness\" constraints in place\nfor the adjacent mixture densities. The traditional Hierarchical Dirichlet\nProcess (HDP) may be suited to the first property, but certainly not the\nsecond. This is due to how each random measure in the lower hierarchies is\nsampled independent of each other and hence does not facilitate any temporal\ncorrelations. To overcome such shortcomings, we proposed a new Smoothed\nHierarchical Dirichlet Process (sHDP). The key novelty of this model is that we\nplace a temporal constraint amongst the nearby discrete measures $\\{G_j\\}$ in\nthe form of symmetric Kullback-Leibler (KL) Divergence with a fixed bound $B$.\nAlthough the constraint we place only involves a single scalar value, it\nnonetheless allows for flexibility in the corresponding successive measures.\nRemarkably, it also led us to infer the model within the stick-breaking process\nwhere the traditional Beta distribution used in stick-breaking is now replaced\nby a new constraint calculated from $B$. We present the inference algorithm and\nelaborate on its solutions. Our experiment using NIPS keywords has shown the\ndesirable effect of the model.\n", "versions": [{"version": "v1", "created": "Sat, 16 Apr 2016 12:35:30 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Luo", "Cheng", ""], ["Xiang", "Yang", ""], ["Da Xu", "Richard Yi", ""]]}, {"id": "1604.04834", "submitter": "Daniel Jakubisin", "authors": "Daniel J. Jakubisin, R. Michael Buehrer, and Claudio R. C. M. da Silva", "title": "Probabilistic Receiver Architecture Combining BP, MF, and EP for\n  Multi-Signal Detection", "comments": "30 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Receiver algorithms which combine belief propagation (BP) with the mean field\n(MF) approximation are well-suited for inference of both continuous and\ndiscrete random variables. In wireless scenarios involving detection of\nmultiple signals, the standard construction of the combined BP-MF framework\nincludes the equalization or multi-user detection functions within the MF\nsubgraph. In this paper, we show that the MF approximation is not particularly\neffective for multi-signal detection. We develop a new factor graph\nconstruction for application of the BP-MF framework to problems involving the\ndetection of multiple signals. We then develop a low-complexity variant to the\nproposed construction in which Gaussian BP is applied to the equalization\nfactors. In this case, the factor graph of the joint probability distribution\nis divided into three subgraphs: (i) a MF subgraph comprised of the observation\nfactors and channel estimation, (ii) a Gaussian BP subgraph which is applied to\nmulti-signal detection, and (iii) a discrete BP subgraph which is applied to\ndemodulation and decoding. Expectation propagation is used to approximate\ndiscrete distributions with a Gaussian distribution and links the discrete BP\nand Gaussian BP subgraphs. The result is a probabilistic receiver architecture\nwith strong theoretical justification which can be applied to multi-signal\ndetection.\n", "versions": [{"version": "v1", "created": "Sun, 17 Apr 2016 06:50:42 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Jakubisin", "Daniel J.", ""], ["Buehrer", "R. Michael", ""], ["da Silva", "Claudio R. C. M.", ""]]}, {"id": "1604.04931", "submitter": "Arno Solin", "authors": "Arno Solin, Pasi Jyl\\\"anki, Jaakko Kauram\\\"aki, Tom Heskes, Marcel A.\n  J. van Gerven, Simo S\\\"arkk\\\"a", "title": "Regularizing Solutions to the MEG Inverse Problem Using Space-Time\n  Separable Covariance Functions", "comments": "25 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In magnetoencephalography (MEG) the conventional approach to source\nreconstruction is to solve the underdetermined inverse problem independently\nover time and space. Here we present how the conventional approach can be\nextended by regularizing the solution in space and time by a Gaussian process\n(Gaussian random field) model. Assuming a separable covariance function in\nspace and time, the computational complexity of the proposed model becomes\n(without any further assumptions or restrictions) $\\mathcal{O}(t^3 + n^3 +\nm^2n)$, where $t$ is the number of time steps, $m$ is the number of sources,\nand $n$ is the number of sensors. We apply the method to both simulated and\nempirical data, and demonstrate the efficiency and generality of our Bayesian\nsource reconstruction approach which subsumes various classical approaches in\nthe literature.\n", "versions": [{"version": "v1", "created": "Sun, 17 Apr 2016 21:16:37 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Solin", "Arno", ""], ["Jyl\u00e4nki", "Pasi", ""], ["Kauram\u00e4ki", "Jaakko", ""], ["Heskes", "Tom", ""], ["van Gerven", "Marcel A. J.", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "1604.04939", "submitter": "Andreas Damianou Dr", "authors": "Andreas Damianou, Neil D. Lawrence, Carl Henrik Ek", "title": "Multi-view Learning as a Nonparametric Nonlinear Inter-Battery Factor\n  Analysis", "comments": "49 pages including appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factor analysis aims to determine latent factors, or traits, which summarize\na given data set. Inter-battery factor analysis extends this notion to multiple\nviews of the data. In this paper we show how a nonlinear, nonparametric version\nof these models can be recovered through the Gaussian process latent variable\nmodel. This gives us a flexible formalism for multi-view learning where the\nlatent variables can be used both for exploratory purposes and for learning\nrepresentations that enable efficient inference for ambiguous estimation tasks.\nLearning is performed in a Bayesian manner through the formulation of a\nvariational compression scheme which gives a rigorous lower bound on the log\nlikelihood. Our Bayesian framework provides strong regularization during\ntraining, allowing the structure of the latent space to be determined\nefficiently and automatically. We demonstrate this by producing the first (to\nour knowledge) published results of learning from dozens of views, even when\ndata is scarce. We further show experimental results on several different types\nof multi-view data sets and for different kinds of tasks, including exploratory\ndata analysis, generation, ambiguity modelling through latent priors and\nclassification.\n", "versions": [{"version": "v1", "created": "Sun, 17 Apr 2016 23:13:50 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Damianou", "Andreas", ""], ["Lawrence", "Neil D.", ""], ["Ek", "Carl Henrik", ""]]}, {"id": "1604.04942", "submitter": "Martha White", "authors": "Lei Le and Martha White", "title": "Identifying global optimality for dictionary learning", "comments": "Updates to previous version include a small modification to\n  Proposition 2, to only use normed regularizers, and a modification to the\n  main theorem (previously Theorem 13) to focus on the overcomplete, full rank\n  setting and to better characterize non-differentiable induced regularizers.\n  The theory has been significantly modified since version 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning new representations of input observations in machine learning is\noften tackled using a factorization of the data. For many such problems,\nincluding sparse coding and matrix completion, learning these factorizations\ncan be difficult, in terms of efficiency and to guarantee that the solution is\na global minimum. Recently, a general class of objectives have been\nintroduced-which we term induced dictionary learning models (DLMs)-that have an\ninduced convex form that enables global optimization. Though attractive\ntheoretically, this induced form is impractical, particularly for large or\ngrowing datasets. In this work, we investigate the use of practical alternating\nminimization algorithms for induced DLMs, that ensure convergence to global\noptima. We characterize the stationary points of these models, and, using these\ninsights, highlight practical choices for the objectives. We then provide\ntheoretical and empirical evidence that alternating minimization, from a random\ninitialization, converges to global minima for a large subclass of induced\nDLMs. In particular, we take advantage of the existence of the (potentially\nunknown) convex induced form, to identify when stationary points are global\nminima for the dictionary learning objective. We then provide an empirical\ninvestigation into practical optimization choices for using alternating\nminimization for induced DLMs, for both batch and stochastic gradient descent.\n", "versions": [{"version": "v1", "created": "Sun, 17 Apr 2016 23:46:04 GMT"}, {"version": "v2", "created": "Thu, 6 Oct 2016 01:13:04 GMT"}, {"version": "v3", "created": "Sun, 19 Feb 2017 16:10:17 GMT"}, {"version": "v4", "created": "Mon, 7 Aug 2017 02:04:52 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Le", "Lei", ""], ["White", "Martha", ""]]}, {"id": "1604.04960", "submitter": "Seungjin Choi", "authors": "Suwon Suh and Seungjin Choi", "title": "Gaussian Copula Variational Autoencoders for Mixed Data", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The variational autoencoder (VAE) is a generative model with continuous\nlatent variables where a pair of probabilistic encoder (bottom-up) and decoder\n(top-down) is jointly learned by stochastic gradient variational Bayes. We\nfirst elaborate Gaussian VAE, approximating the local covariance matrix of the\ndecoder as an outer product of the principal direction at a position determined\nby a sample drawn from Gaussian distribution. We show that this model, referred\nto as VAE-ROC, better captures the data manifold, compared to the standard\nGaussian VAE where independent multivariate Gaussian was used to model the\ndecoder. Then we extend the VAE-ROC to handle mixed categorical and continuous\ndata. To this end, we employ Gaussian copula to model the local dependency in\nmixed categorical and continuous data, leading to {\\em Gaussian copula\nvariational autoencoder} (GCVAE). As in VAE-ROC, we use the rank-one\napproximation for the covariance in the Gaussian copula, to capture the local\ndependency structure in the mixed data. Experiments on various datasets\ndemonstrate the useful behaviour of VAE-ROC and GCVAE, compared to the standard\nVAE.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 02:14:07 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Suh", "Suwon", ""], ["Choi", "Seungjin", ""]]}, {"id": "1604.05129", "submitter": "Pedro Alejandro Ortega", "authors": "Pedro A. Ortega and Naftali Tishby", "title": "Memory shapes time perception and intertemporal choices", "comments": "24 pages, 4 figures, 2 tables. Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a consensus that human and non-human subjects experience temporal\ndistortions in many stages of their perceptual and decision-making systems.\nSimilarly, intertemporal choice research has shown that decision-makers\nundervalue future outcomes relative to immediate ones. Here we combine\ntechniques from information theory and artificial intelligence to show how both\ntemporal distortions and intertemporal choice preferences can be explained as a\nconsequence of the coding efficiency of sensorimotor representation. In\nparticular, the model implies that interactions that constrain future behavior\nare perceived as being both longer in duration and more valuable. Furthermore,\nusing simulations of artificial agents, we investigate how memory constraints\nenforce a renormalization of the perceived timescales. Our results show that\nqualitatively different discount functions, such as exponential and hyperbolic\ndiscounting, arise as a consequence of an agent's probabilistic model of the\nworld.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 13:17:55 GMT"}, {"version": "v2", "created": "Sun, 29 May 2016 18:39:52 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Ortega", "Pedro A.", ""], ["Tishby", "Naftali", ""]]}, {"id": "1604.05198", "submitter": "Baogang Hu", "authors": "Linlin Cao, Ran He, Bao-Gang Hu", "title": "Locally Imposing Function for Generalized Constraint Neural Networks - A\n  Study on Equality Constraints", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is a further study on the Generalized Constraint Neural Network\n(GCNN) model [1], [2]. Two challenges are encountered in the study, that is, to\nembed any type of prior information and to select its imposing schemes. The\nwork focuses on the second challenge and studies a new constraint imposing\nscheme for equality constraints. A new method called locally imposing function\n(LIF) is proposed to provide a local correction to the GCNN prediction\nfunction, which therefore falls within Locally Imposing Scheme (LIS). In\ncomparison, the conventional Lagrange multiplier method is considered as\nGlobally Imposing Scheme (GIS) because its added constraint term exhibits a\nglobal impact to its objective function. Two advantages are gained from LIS\nover GIS. First, LIS enables constraints to fire locally and explicitly in the\ndomain only where they need on the prediction function. Second, constraints can\nbe implemented within a network setting directly. We attempt to interpret\nseveral constraint methods graphically from a viewpoint of the locality\nprinciple. Numerical examples confirm the advantages of the proposed method. In\nsolving boundary value problems with Dirichlet and Neumann constraints, the\nGCNN model with LIF is possible to achieve an exact satisfaction of the\nconstraints.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 15:11:13 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Cao", "Linlin", ""], ["He", "Ran", ""], ["Hu", "Bao-Gang", ""]]}, {"id": "1604.05251", "submitter": "Carl-Johann Simon-Gabriel", "authors": "Carl-Johann Simon-Gabriel and Bernhard Sch\\\"olkopf", "title": "Kernel Distribution Embeddings: Universal Kernels, Characteristic\n  Kernels and Kernel Metrics on Distributions", "comments": "Old and longer version of the JMLR paper with same title (published\n  2018). Please start with the JMLR version. 55 pages (33 pages main text, 22\n  pages appendix), 2 tables, 1 figure (in appendix)", "journal-ref": "Journal of Machine Learning Research, 19(44):1-29, 2018", "doi": null, "report-no": null, "categories": "stat.ML math.FA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel mean embeddings have recently attracted the attention of the machine\nlearning community. They map measures $\\mu$ from some set $M$ to functions in a\nreproducing kernel Hilbert space (RKHS) with kernel $k$. The RKHS distance of\ntwo mapped measures is a semi-metric $d_k$ over $M$. We study three questions.\n(I) For a given kernel, what sets $M$ can be embedded? (II) When is the\nembedding injective over $M$ (in which case $d_k$ is a metric)? (III) How does\nthe $d_k$-induced topology compare to other topologies on $M$? The existing\nmachine learning literature has addressed these questions in cases where $M$ is\n(a subset of) the finite regular Borel measures. We unify, improve and\ngeneralise those results. Our approach naturally leads to continuous and\npossibly even injective embeddings of (Schwartz-) distributions, i.e.,\ngeneralised measures, but the reader is free to focus on measures only. In\nparticular, we systemise and extend various (partly known) equivalences between\ndifferent notions of universal, characteristic and strictly positive definite\nkernels, and show that on an underlying locally compact Hausdorff space, $d_k$\nmetrises the weak convergence of probability measures if and only if $k$ is\ncontinuous and characteristic.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 17:16:22 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 12:53:48 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Simon-Gabriel", "Carl-Johann", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1604.05263", "submitter": "Alan Saul D", "authors": "Alan D. Saul, James Hensman, Aki Vehtari, Neil D. Lawrence", "title": "Chained Gaussian Processes", "comments": "Appearing in Proceedings of the 19th International Conference on\n  Artificial Intelligence and Statistics (AISTATS) 2016, Cadiz, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process models are flexible, Bayesian non-parametric approaches to\nregression. Properties of multivariate Gaussians mean that they can be combined\nlinearly in the manner of additive models and via a link function (like in\ngeneralized linear models) to handle non-Gaussian data. However, the link\nfunction formalism is restrictive, link functions are always invertible and\nmust convert a parameter of interest to a linear combination of the underlying\nprocesses. There are many likelihoods and models where a non-linear combination\nis more appropriate. We term these more general models Chained Gaussian\nProcesses: the transformation of the GPs to the likelihood parameters will not\ngenerally be invertible, and that implies that linearisation would only be\npossible with multiple (localized) links, i.e. a chain. We develop an\napproximate inference procedure for Chained GPs that is scalable and applicable\nto any factorized likelihood. We demonstrate the approximation on a range of\nlikelihood functions.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 17:46:23 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Saul", "Alan D.", ""], ["Hensman", "James", ""], ["Vehtari", "Aki", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1604.05266", "submitter": "Ikjyot Singh Kohli", "authors": "Ikjyot Singh Kohli", "title": "Finding Common Characteristics Among NBA Playoff and Championship Teams:\n  A Machine Learning Approach", "comments": "Updated contents to reflect most recent data and corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we employ machine learning techniques to analyze seventeen\nseasons (1999-2000 to 2015-2016) of NBA regular season data from every team to\ndetermine the common characteristics among NBA playoff teams. Each team was\ncharacterized by 26 predictor variables and one binary response variable taking\non a value of \"TRUE\" if a team had made the playoffs, and value of \"FALSE\" if a\nteam had missed the playoffs. After fitting an initial classification tree to\nthis problem, this tree was then pruned which decreased the test error rate.\nFurther to this, a random forest of classification trees was grown which\nprovided a very accurate model from which a variable importance plot was\ngenerated to determine which predictor variables had the greatest influence on\nthe response variable. The result of this work was the conclusion that the most\nimportant factors in characterizing a team's playoff eligibility are a team's\nopponent number of assists per game, a team's opponent number of made two point\nshots per game, and a team's number of steals per game. This seems to suggest\nthat defensive factors as opposed to offensive factors are the most important\ncharacteristics shared among NBA playoff teams. We then use neural networks to\nclassify championship teams based on regular season data. From this, we show\nthat the most important factor in a team not winning a championship is that\nteam's opponent number of made three-point shots per game. This once again\nimplies that defensive characteristics are of great importance in not only\ndetermining a team's playoff eligibility, but certainly, one can conclude that\na lack of perimeter defense negatively impacts a team's championship chances in\na given season. Further, it is shown that made two-point shots and defensive\nrebounding are by far the most important factor in a team's chances at winning\na championship in a given season.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 17:57:21 GMT"}, {"version": "v2", "created": "Thu, 5 May 2016 17:25:16 GMT"}, {"version": "v3", "created": "Mon, 9 May 2016 21:42:59 GMT"}, {"version": "v4", "created": "Tue, 22 Nov 2016 18:10:17 GMT"}, {"version": "v5", "created": "Thu, 5 Jan 2017 00:54:24 GMT"}, {"version": "v6", "created": "Tue, 21 Feb 2017 16:59:03 GMT"}, {"version": "v7", "created": "Mon, 3 Apr 2017 23:00:00 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Kohli", "Ikjyot Singh", ""]]}, {"id": "1604.05307", "submitter": "Hemant Tyagi", "authors": "Hemant Tyagi, Anastasios Kyrillidis, Bernd G\\\"artner, Andreas Krause", "title": "Learning Sparse Additive Models with Interactions in High Dimensions", "comments": "23 pages, to appear in Proceedings of the 19th International\n  Conference on Artificial Intelligence and Statistics (AISTATS) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A function $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is referred to as a\nSparse Additive Model (SPAM), if it is of the form $f(\\mathbf{x}) = \\sum_{l \\in\n\\mathcal{S}}\\phi_{l}(x_l)$, where $\\mathcal{S} \\subset [d]$, $|\\mathcal{S}| \\ll\nd$. Assuming $\\phi_l$'s and $\\mathcal{S}$ to be unknown, the problem of\nestimating $f$ from its samples has been studied extensively. In this work, we\nconsider a generalized SPAM, allowing for second order interaction terms. For\nsome $\\mathcal{S}_1 \\subset [d], \\mathcal{S}_2 \\subset {[d] \\choose 2}$, the\nfunction $f$ is assumed to be of the form: $$f(\\mathbf{x}) = \\sum_{p \\in\n\\mathcal{S}_1}\\phi_{p} (x_p) + \\sum_{(l,l^{\\prime}) \\in\n\\mathcal{S}_2}\\phi_{(l,l^{\\prime})} (x_{l},x_{l^{\\prime}}).$$ Assuming\n$\\phi_{p},\\phi_{(l,l^{\\prime})}$, $\\mathcal{S}_1$ and, $\\mathcal{S}_2$ to be\nunknown, we provide a randomized algorithm that queries $f$ and exactly\nrecovers $\\mathcal{S}_1,\\mathcal{S}_2$. Consequently, this also enables us to\nestimate the underlying $\\phi_p, \\phi_{(l,l^{\\prime})}$. We derive sample\ncomplexity bounds for our scheme and also extend our analysis to include the\nsituation where the queries are corrupted with noise -- either stochastic, or\narbitrary but bounded. Lastly, we provide simulation results on synthetic data,\nthat validate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 17:09:48 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Tyagi", "Hemant", ""], ["Kyrillidis", "Anastasios", ""], ["G\u00e4rtner", "Bernd", ""], ["Krause", "Andreas", ""]]}, {"id": "1604.05377", "submitter": "Artit Wangperawong", "authors": "Artit Wangperawong, Cyrille Brun, Olav Laudy, Rujikorn Pavasuthipaisit", "title": "Churn analysis using deep convolutional neural networks and autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Customer temporal behavioral data was represented as images in order to\nperform churn prediction by leveraging deep learning architectures prominent in\nimage classification. Supervised learning was performed on labeled data of over\n6 million customers using deep convolutional neural networks, which achieved an\nAUC of 0.743 on the test dataset using no more than 12 temporal features for\neach customer. Unsupervised learning was conducted using autoencoders to better\nunderstand the reasons for customer churn. Images that maximally activate the\nhidden units of an autoencoder trained with churned customers reveal ample\nopportunities for action to be taken to prevent churn among strong data, no\nvoice users.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 23:18:23 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Wangperawong", "Artit", ""], ["Brun", "Cyrille", ""], ["Laudy", "Olav", ""], ["Pavasuthipaisit", "Rujikorn", ""]]}, {"id": "1604.05417", "submitter": "Swami Sankaranarayanan", "authors": "Swami Sankaranarayanan, Azadeh Alavi, Carlos Castillo, Rama Chellappa", "title": "Triplet Probabilistic Embedding for Face Verification and Clustering", "comments": "Oral Paper in BTAS 2016; NVIDIA Best paper Award\n  (http://ieee-biometrics.org/btas2016/awards.html)", "journal-ref": null, "doi": "10.1109/BTAS.2016.7791205", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant progress made over the past twenty five years,\nunconstrained face verification remains a challenging problem. This paper\nproposes an approach that couples a deep CNN-based approach with a\nlow-dimensional discriminative embedding learned using triplet probability\nconstraints to solve the unconstrained face verification problem. Aside from\nyielding performance improvements, this embedding provides significant\nadvantages in terms of memory and for post-processing operations like subject\nspecific clustering. Experiments on the challenging IJB-A dataset show that the\nproposed algorithm performs comparably or better than the state of the art\nmethods in verification and identification metrics, while requiring much less\ntraining data and training time. The superior performance of the proposed\nmethod on the CFP dataset shows that the representation learned by our deep CNN\nis robust to extreme pose variation. Furthermore, we demonstrate the robustness\nof the deep features to challenges including age, pose, blur and clutter by\nperforming simple clustering experiments on both IJB-A and LFW datasets.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 03:29:56 GMT"}, {"version": "v2", "created": "Sun, 8 May 2016 16:04:02 GMT"}, {"version": "v3", "created": "Wed, 18 Jan 2017 03:10:44 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Sankaranarayanan", "Swami", ""], ["Alavi", "Azadeh", ""], ["Castillo", "Carlos", ""], ["Chellappa", "Rama", ""]]}, {"id": "1604.05449", "submitter": "Dacheng Tao", "authors": "Shan You, Chang Xu, Yunhe Wang, Chao Xu and Dacheng Tao", "title": "Streaming Label Learning for Modeling Labels on the Fly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is challenging to handle a large volume of labels in multi-label learning.\nHowever, existing approaches explicitly or implicitly assume that all the\nlabels in the learning process are given, which could be easily violated in\nchanging environments. In this paper, we define and study streaming label\nlearning (SLL), i.e., labels are arrived on the fly, to model newly arrived\nlabels with the help of the knowledge learned from past labels. The core of SLL\nis to explore and exploit the relationships between new labels and past labels\nand then inherit the relationship into hypotheses of labels to boost the\nperformance of new classifiers. In specific, we use the label\nself-representation to model the label relationship, and SLL will be divided\ninto two steps: a regression problem and a empirical risk minimization (ERM)\nproblem. Both problems are simple and can be efficiently solved. We further\nshow that SLL can generate a tighter generalization error bound for new labels\nthan the general ERM framework with trace norm or Frobenius norm\nregularization. Finally, we implement extensive experiments on various\nbenchmark datasets to validate the new setting. And results show that SLL can\neffectively handle the constantly emerging new labels and provides excellent\nclassification performance.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 07:12:29 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["You", "Shan", ""], ["Xu", "Chang", ""], ["Wang", "Yunhe", ""], ["Xu", "Chao", ""], ["Tao", "Dacheng", ""]]}, {"id": "1604.05819", "submitter": "Suchi Saria", "authors": "Daniel P. Robinson and Suchi Saria", "title": "Trading-Off Cost of Deployment Versus Accuracy in Learning Predictive\n  Models", "comments": "Authors contributed equally to this work. To appear in IJCAI 2016,\n  Twenty-Fifth International Joint Conference on Artificial Intelligence, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive models are finding an increasing number of applications in many\nindustries. As a result, a practical means for trading-off the cost of\ndeploying a model versus its effectiveness is needed. Our work is motivated by\nrisk prediction problems in healthcare. Cost-structures in domains such as\nhealthcare are quite complex, posing a significant challenge to existing\napproaches. We propose a novel framework for designing cost-sensitive\nstructured regularizers that is suitable for problems with complex cost\ndependencies. We draw upon a surprising connection to boolean circuits. In\nparticular, we represent the problem costs as a multi-layer boolean circuit,\nand then use properties of boolean circuits to define an extended feature\nvector and a group regularizer that exactly captures the underlying cost\nstructure. The resulting regularizer may then be combined with a fidelity\nfunction to perform model prediction, for example. For the challenging\nreal-world application of risk prediction for sepsis in intensive care units,\nthe use of our regularizer leads to models that are in harmony with the\nunderlying cost structure and thus provide an excellent prediction accuracy\nversus cost tradeoff.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 04:59:08 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Robinson", "Daniel P.", ""], ["Saria", "Suchi", ""]]}, {"id": "1604.05878", "submitter": "Johannes Welbl", "authors": "Johannes Welbl, Guillaume Bouchard, Sebastian Riedel", "title": "A Factorization Machine Framework for Testing Bigram Embeddings in\n  Knowledgebase Completion", "comments": "accepted for AKBC 2016 workshop, 6pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding-based Knowledge Base Completion models have so far mostly combined\ndistributed representations of individual entities or relations to compute\ntruth scores of missing links. Facts can however also be represented using\npairwise embeddings, i.e. embeddings for pairs of entities and relations. In\nthis paper we explore such bigram embeddings with a flexible Factorization\nMachine model and several ablations from it. We investigate the relevance of\nvarious bigram types on the fb15k237 dataset and find relative improvements\ncompared to a compositional model.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 09:58:56 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Welbl", "Johannes", ""], ["Bouchard", "Guillaume", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1604.05976", "submitter": "Zhaobin Kuang", "authors": "Zhaobin Kuang, James Thomson, Michael Caldwell, Peggy Peissig, Ron\n  Stewart, David Page", "title": "Computational Drug Repositioning Using Continuous Self-controlled Case\n  Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational Drug Repositioning (CDR) is the task of discovering potential\nnew indications for existing drugs by mining large-scale heterogeneous\ndrug-related data sources. Leveraging the patient-level temporal ordering\ninformation between numeric physiological measurements and various drug\nprescriptions provided in Electronic Health Records (EHRs), we propose a\nContinuous Self-controlled Case Series (CSCCS) model for CDR. As an initial\nevaluation, we look for drugs that can control Fasting Blood Glucose (FBG)\nlevel in our experiments. Applying CSCCS to the Marshfield Clinic EHR,\nwell-known drugs that are indicated for controlling blood glucose level are\nrediscovered. Furthermore, some drugs with recent literature support for the\npotential effect of blood glucose level control are also identified.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 14:28:44 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Kuang", "Zhaobin", ""], ["Thomson", "James", ""], ["Caldwell", "Michael", ""], ["Peissig", "Peggy", ""], ["Stewart", "Ron", ""], ["Page", "David", ""]]}, {"id": "1604.06020", "submitter": "Stefano Teso", "authors": "Stefano Teso, Andrea Passerini, Paolo Viappiani", "title": "Constructive Preference Elicitation by Setwise Max-margin Learning", "comments": "7 pages. A conference version of this work is accepted by the 25th\n  International Joint Conference on Artificial Intelligence (IJCAI-16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an approach to preference elicitation that is\nsuitable to large configuration spaces beyond the reach of existing\nstate-of-the-art approaches. Our setwise max-margin method can be viewed as a\ngeneralization of max-margin learning to sets, and can produce a set of\n\"diverse\" items that can be used to ask informative queries to the user.\nMoreover, the approach can encourage sparsity in the parameter space, in order\nto favor the assessment of utility towards combinations of weights that\nconcentrate on just few features. We present a mixed integer linear programming\nformulation and show how our approach compares favourably with Bayesian\npreference elicitation alternatives and easily scales to realistic datasets.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 16:22:01 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Teso", "Stefano", ""], ["Passerini", "Andrea", ""], ["Viappiani", "Paolo", ""]]}, {"id": "1604.06036", "submitter": "Matthew Shum", "authors": "Khai X. Chiong and Matthew Shum", "title": "Random Projection Estimation of Discrete-Choice Models with Large Choice\n  Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce sparse random projection, an important dimension-reduction tool\nfrom machine learning, for the estimation of discrete-choice models with\nhigh-dimensional choice sets. Initially, high-dimensional data are compressed\ninto a lower-dimensional Euclidean space using random projections.\nSubsequently, estimation proceeds using cyclic monotonicity moment inequalities\nimplied by the multinomial choice model; the estimation procedure is\nsemi-parametric and does not require explicit distributional assumptions to be\nmade regarding the random utility errors. The random projection procedure is\njustified via the Johnson-Lindenstrauss Lemma -- the pairwise distances between\ndata points are preserved during data compression, which we exploit to show\nconvergence of our estimator. The estimator works well in simulations and in an\napplication to a supermarket scanner dataset.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 17:11:44 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Chiong", "Khai X.", ""], ["Shum", "Matthew", ""]]}, {"id": "1604.06057", "submitter": "Karthik Narasimhan", "authors": "Tejas D. Kulkarni, Karthik R. Narasimhan, Ardavan Saeedi, Joshua B.\n  Tenenbaum", "title": "Hierarchical Deep Reinforcement Learning: Integrating Temporal\n  Abstraction and Intrinsic Motivation", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning goal-directed behavior in environments with sparse feedback is a\nmajor challenge for reinforcement learning algorithms. The primary difficulty\narises due to insufficient exploration, resulting in an agent being unable to\nlearn robust value functions. Intrinsically motivated agents can explore new\nbehavior for its own sake rather than to directly solve problems. Such\nintrinsic behaviors could eventually help the agent solve tasks posed by the\nenvironment. We present hierarchical-DQN (h-DQN), a framework to integrate\nhierarchical value functions, operating at different temporal scales, with\nintrinsically motivated deep reinforcement learning. A top-level value function\nlearns a policy over intrinsic goals, and a lower-level function learns a\npolicy over atomic actions to satisfy the given goals. h-DQN allows for\nflexible goal specifications, such as functions over entities and relations.\nThis provides an efficient space for exploration in complicated environments.\nWe demonstrate the strength of our approach on two problems with very sparse,\ndelayed feedback: (1) a complex discrete stochastic decision process, and (2)\nthe classic ATARI game `Montezuma's Revenge'.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 18:47:48 GMT"}, {"version": "v2", "created": "Tue, 31 May 2016 14:45:58 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Kulkarni", "Tejas D.", ""], ["Narasimhan", "Karthik R.", ""], ["Saeedi", "Ardavan", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1604.06194", "submitter": "Aleksandr Aravkin", "authors": "Aleksandr Y. Aravkin, Kush R. Varshney, and Liu Yang", "title": "Dynamic matrix factorization with social influence", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.SI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix factorization is a key component of collaborative filtering-based\nrecommendation systems because it allows us to complete sparse user-by-item\nratings matrices under a low-rank assumption that encodes the belief that\nsimilar users give similar ratings and that similar items garner similar\nratings. This paradigm has had immeasurable practical success, but it is not\nthe complete story for understanding and inferring the preferences of people.\nFirst, peoples' preferences and their observable manifestations as ratings\nevolve over time along general patterns of trajectories. Second, an individual\nperson's preferences evolve over time through influence of their social\nconnections. In this paper, we develop a unified process model for both types\nof dynamics within a state space approach, together with an efficient\noptimization scheme for estimation within that model. The model combines\nelements from recent developments in dynamic matrix factorization, opinion\ndynamics and social learning, and trust-based recommendation. The estimation\nbuilds upon recent advances in numerical nonlinear optimization. Empirical\nresults on a large-scale data set from the Epinions website demonstrate\nconsistent reduction in root mean squared error by consideration of the two\ntypes of dynamics.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 06:51:22 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Aravkin", "Aleksandr Y.", ""], ["Varshney", "Kush R.", ""], ["Yang", "Liu", ""]]}, {"id": "1604.06335", "submitter": "Adam Kashlak", "authors": "Adam B. Kashlak, Eoin Devane, Helge Dietert, Henry Jackson", "title": "Markov models for ocular fixation locations in the presence and absence\n  of colour", "comments": "12 pages, 8 Figures, 1 Table", "journal-ref": "JRSS-C 67 (2018) 201-215", "doi": "10.1111/rssc.12223", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to model the fixation locations of the human eye when observing a\nstill image by a Markovian point process in R 2 . Our approach is data driven\nusing k-means clustering of the fixation locations to identify distinct salient\nregions of the image, which in turn correspond to the states of our Markov\nchain. Bayes factors are computed as model selection criterion to determine the\nnumber of clusters. Furthermore, we demonstrate that the behaviour of the human\neye differs from this model when colour information is removed from the given\nimage.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 14:50:27 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Kashlak", "Adam B.", ""], ["Devane", "Eoin", ""], ["Dietert", "Helge", ""], ["Jackson", "Henry", ""]]}, {"id": "1604.06443", "submitter": "Gautam Kamath", "authors": "Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Ankur\n  Moitra, Alistair Stewart", "title": "Robust Estimators in High Dimensions without the Computational\n  Intractability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study high-dimensional distribution learning in an agnostic setting where\nan adversary is allowed to arbitrarily corrupt an $\\varepsilon$-fraction of the\nsamples. Such questions have a rich history spanning statistics, machine\nlearning and theoretical computer science. Even in the most basic settings, the\nonly known approaches are either computationally inefficient or lose\ndimension-dependent factors in their error guarantees. This raises the\nfollowing question:Is high-dimensional agnostic distribution learning even\npossible, algorithmically?\n  In this work, we obtain the first computationally efficient algorithms with\ndimension-independent error guarantees for agnostically learning several\nfundamental classes of high-dimensional distributions: (1) a single Gaussian,\n(2) a product distribution on the hypercube, (3) mixtures of two product\ndistributions (under a natural balancedness condition), and (4) mixtures of\nspherical Gaussians. Our algorithms achieve error that is independent of the\ndimension, and in many cases scales nearly-linearly with the fraction of\nadversarially corrupted samples. Moreover, we develop a general recipe for\ndetecting and correcting corruptions in high-dimensions, that may be applicable\nto many other problems.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 19:54:24 GMT"}, {"version": "v2", "created": "Fri, 15 Mar 2019 02:31:22 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kamath", "Gautam", ""], ["Kane", "Daniel", ""], ["Li", "Jerry", ""], ["Moitra", "Ankur", ""], ["Stewart", "Alistair", ""]]}, {"id": "1604.06498", "submitter": "Yuting Ma", "authors": "Yuting Ma, Tian Zheng", "title": "Stabilized Sparse Online Learning for Sparse Data", "comments": "45 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) is commonly used for optimization in\nlarge-scale machine learning problems. Langford et al. (2009) introduce a\nsparse online learning method to induce sparsity via truncated gradient. With\nhigh-dimensional sparse data, however, the method suffers from slow convergence\nand high variance due to the heterogeneity in feature sparsity. To mitigate\nthis issue, we introduce a stabilized truncated stochastic gradient descent\nalgorithm. We employ a soft-thresholding scheme on the weight vector where the\nimposed shrinkage is adaptive to the amount of information available in each\nfeature. The variability in the resulted sparse weight vector is further\ncontrolled by stability selection integrated with the informative truncation.\nTo facilitate better convergence, we adopt an annealing strategy on the\ntruncation rate, which leads to a balanced trade-off between exploration and\nexploitation in learning a sparse weight vector. Numerical experiments show\nthat our algorithm compares favorably with the original algorithm in terms of\nprediction accuracy, achieved sparsity and stability.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 21:34:34 GMT"}, {"version": "v2", "created": "Mon, 20 Feb 2017 22:41:12 GMT"}, {"version": "v3", "created": "Tue, 9 May 2017 00:50:38 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Ma", "Yuting", ""], ["Zheng", "Tian", ""]]}, {"id": "1604.06518", "submitter": "Vu Nguyen", "authors": "Trung Le and Tu Dinh Nguyen and Vu Nguyen and Dinh Phung", "title": "Approximation Vector Machines for Large-scale Online Learning", "comments": "54 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most challenging problems in kernel online learning is to bound\nthe model size and to promote the model sparsity. Sparse models not only\nimprove computation and memory usage, but also enhance the generalization\ncapacity, a principle that concurs with the law of parsimony. However,\ninappropriate sparsity modeling may also significantly degrade the performance.\nIn this paper, we propose Approximation Vector Machine (AVM), a model that can\nsimultaneously encourage the sparsity and safeguard its risk in compromising\nthe performance. When an incoming instance arrives, we approximate this\ninstance by one of its neighbors whose distance to it is less than a predefined\nthreshold. Our key intuition is that since the newly seen instance is expressed\nby its nearby neighbor the optimal performance can be analytically formulated\nand maintained. We develop theoretical foundations to support this intuition\nand further establish an analysis to characterize the gap between the\napproximation and optimal solutions. This gap crucially depends on the\nfrequency of approximation and the predefined threshold. We perform the\nconvergence analysis for a wide spectrum of loss functions including Hinge,\nsmooth Hinge, and Logistic for classification task, and $l_1$, $l_2$, and\n$\\epsilon$-insensitive for regression task. We conducted extensive experiments\nfor classification task in batch and online modes, and regression task in\nonline mode over several benchmark datasets. The results show that our proposed\nAVM achieved a comparable predictive performance with current state-of-the-art\nmethods while simultaneously achieving significant computational speed-up due\nto the ability of the proposed AVM in maintaining the model size.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 01:57:01 GMT"}, {"version": "v2", "created": "Mon, 25 Apr 2016 01:16:21 GMT"}, {"version": "v3", "created": "Wed, 5 Apr 2017 01:43:29 GMT"}, {"version": "v4", "created": "Sun, 28 May 2017 01:26:48 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Le", "Trung", ""], ["Nguyen", "Tu Dinh", ""], ["Nguyen", "Vu", ""], ["Phung", "Dinh", ""]]}, {"id": "1604.06626", "submitter": "Brijnesh Jain", "authors": "Brijnesh J. Jain", "title": "The Mean Partition Theorem of Consensus Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To devise efficient solutions for approximating a mean partition in consensus\nclustering, Dimitriadou et al. [3] presented a necessary condition of\noptimality for a consensus function based on least square distances. We show\nthat their result is pivotal for deriving interesting properties of consensus\nclustering beyond optimization. For this, we present the necessary condition of\noptimality in a slightly stronger form in terms of the Mean Partition Theorem\nand extend it to the Expected Partition Theorem. To underpin its versatility,\nwe show three examples that apply the Mean Partition Theorem: (i) equivalence\nof the mean partition and optimal multiple alignment, (ii) construction of\nprofiles and motifs, and (iii) relationship between consensus clustering and\ncluster stability.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 12:32:37 GMT"}, {"version": "v2", "created": "Tue, 26 Apr 2016 06:55:06 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Jain", "Brijnesh J.", ""]]}, {"id": "1604.06637", "submitter": "Takayuki Kawashima", "authors": "Takayuki Kawashima, Hironori Fujisawa", "title": "Robust and Sparse Regression via $\\gamma$-divergence", "comments": "25 pages", "journal-ref": null, "doi": "10.3390/e19110608", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high-dimensional data, many sparse regression methods have been proposed.\nHowever, they may not be robust against outliers. Recently, the use of density\npower weight has been studied for robust parameter estimation and the\ncorresponding divergences have been discussed. One of such divergences is the\n$\\gamma$-divergence and the robust estimator using the $\\gamma$-divergence is\nknown for having a strong robustness. In this paper, we consider the robust and\nsparse regression based on $\\gamma$-divergence. We extend the\n$\\gamma$-divergence to the regression problem and show that it has a strong\nrobustness under heavy contamination even when outliers are heterogeneous. The\nloss function is constructed by an empirical estimate of the\n$\\gamma$-divergence with sparse regularization and the parameter estimate is\ndefined as the minimizer of the loss function. To obtain the robust and sparse\nestimate, we propose an efficient update algorithm which has a monotone\ndecreasing property of the loss function. Particularly, we discuss a linear\nregression problem with $L_1$ regularization in detail. In numerical\nexperiments and real data analyses, we see that the proposed method outperforms\npast robust and sparse methods.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 12:53:27 GMT"}, {"version": "v2", "created": "Thu, 28 Jul 2016 09:07:10 GMT"}, {"version": "v3", "created": "Mon, 29 Aug 2016 06:52:11 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Kawashima", "Takayuki", ""], ["Fujisawa", "Hironori", ""]]}, {"id": "1604.06727", "submitter": "Chee Chun Gan", "authors": "Chee Chun Gan and Gerard Learmonth", "title": "An improved chromosome formulation for genetic algorithms applied to\n  variable selection with the inclusion of interaction terms", "comments": "20 pages, 4 figures, 4 tables, 2 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic algorithms are a well-known method for tackling the problem of\nvariable selection. As they are non-parametric and can use a large variety of\nfitness functions, they are well-suited as a variable selection wrapper that\ncan be applied to many different models. In almost all cases, the chromosome\nformulation used in these genetic algorithms consists of a binary vector of\nlength n for n potential variables indicating the presence or absence of the\ncorresponding variables. While the aforementioned chromosome formulation has\nexhibited good performance for relatively small n, there are potential problems\nwhen the size of n grows very large, especially when interaction terms are\nconsidered. We introduce a modification to the standard chromosome formulation\nthat allows for better scalability and model sparsity when interaction terms\nare included in the predictor search space. Experimental results show that the\nindexed chromosome formulation demonstrates improved computational efficiency\nand sparsity on high-dimensional datasets with interaction terms compared to\nthe standard chromosome formulation.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 16:14:55 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Gan", "Chee Chun", ""], ["Learmonth", "Gerard", ""]]}, {"id": "1604.06730", "submitter": "Chee Chun Gan", "authors": "Chee Chun Gan and Gerard Learmonth", "title": "Developing an ICU scoring system with interaction terms using a genetic\n  algorithm", "comments": "21 pages, 6 tables, 2 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ICU mortality scoring systems attempt to predict patient mortality using\npredictive models with various clinical predictors. Examples of such systems\nare APACHE, SAPS and MPM. However, most such scoring systems do not actively\nlook for and include interaction terms, despite physicians intuitively taking\nsuch interactions into account when making a diagnosis. One barrier to\nincluding such terms in predictive models is the difficulty of using most\nvariable selection methods in high-dimensional datasets. A genetic algorithm\nframework for variable selection with logistic regression models is used to\nsearch for two-way interaction terms in a clinical dataset of adult ICU\npatients, with separate models being built for each category of diagnosis upon\nadmittance to the ICU. The models had good discrimination across all\ncategories, with a weighted average AUC of 0.84 (>0.90 for several categories)\nand the genetic algorithm was able to find several significant interaction\nterms, which may be able to provide greater insight into mortality prediction\nfor health practitioners. The GA selected models had improved performance\nagainst stepwise selection and random forest models, and provides greater\nflexibility in terms of variable selection by being able to optimize over any\nmodeler-defined model performance metric instead of a specific variable\nimportance metric.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 16:20:29 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Gan", "Chee Chun", ""], ["Learmonth", "Gerard", ""]]}, {"id": "1604.06749", "submitter": "Mina Karzand", "authors": "Guy Bresler and Mina Karzand", "title": "Learning a Tree-Structured Ising Model in Order to Make Predictions", "comments": "43 pages, 7 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning a tree Ising model from samples such that\nsubsequent predictions made using the model are accurate. The prediction task\nconsidered in this paper is that of predicting the values of a subset of\nvariables given values of some other subset of variables. Virtually all\nprevious work on graphical model learning has focused on recovering the true\nunderlying graph. We define a distance (\"small set TV\" or ssTV) between\ndistributions $P$ and $Q$ by taking the maximum, over all subsets $\\mathcal{S}$\nof a given size, of the total variation between the marginals of $P$ and $Q$ on\n$\\mathcal{S}$; this distance captures the accuracy of the prediction task of\ninterest. We derive non-asymptotic bounds on the number of samples needed to\nget a distribution (from the same class) with small ssTV relative to the one\ngenerating the samples. One of the main messages of this paper is that far\nfewer samples are needed than for recovering the underlying tree, which means\nthat accurate predictions are possible using the wrong tree.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 16:57:30 GMT"}, {"version": "v2", "created": "Mon, 8 Aug 2016 20:11:52 GMT"}, {"version": "v3", "created": "Thu, 14 Jun 2018 12:01:53 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Bresler", "Guy", ""], ["Karzand", "Mina", ""]]}, {"id": "1604.06815", "submitter": "Christian M\\\"uller", "authors": "Jacob Bien, Irina Gaynanova, Johannes Lederer, Christian M\\\"uller", "title": "Non-convex Global Minimization and False Discovery Rate Control for the\n  TREX", "comments": null, "journal-ref": "Journal of Computational and Graphical Statistics 2017, Vol. 27,\n  No. 1, 23-33", "doi": "10.1080/10618600.2017.1341414", "report-no": null, "categories": "stat.ML cs.OH stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The TREX is a recently introduced method for performing sparse\nhigh-dimensional regression. Despite its statistical promise as an alternative\nto the lasso, square-root lasso, and scaled lasso, the TREX is computationally\nchallenging in that it requires solving a non-convex optimization problem. This\npaper shows a remarkable result: despite the non-convexity of the TREX problem,\nthere exists a polynomial-time algorithm that is guaranteed to find the global\nminimum. This result adds the TREX to a very short list of non-convex\noptimization problems that can be globally optimized (principal components\nanalysis being a famous example). After deriving and developing this new\napproach, we demonstrate that (i) the ability of the preexisting TREX heuristic\nto reach the global minimum is strongly dependent on the difficulty of the\nunderlying statistical problem, (ii) the new polynomial-time algorithm for TREX\npermits a novel variable ranking and selection scheme, (iii) this scheme can be\nincorporated into a rule that controls the false discovery rate (FDR) of\nincluded features in the model. To achieve this last aim, we provide an\nextension of the results of Barber & Candes (2015) to establish that the\nknockoff filter framework can be applied to the TREX. This investigation thus\nprovides both a rare case study of a heuristic for non-convex optimization and\na novel way of exploiting non-convexity for statistical inference.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 20:28:55 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 20:07:35 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Bien", "Jacob", ""], ["Gaynanova", "Irina", ""], ["Lederer", "Johannes", ""], ["M\u00fcller", "Christian", ""]]}, {"id": "1604.06952", "submitter": "Fionn Murtagh", "authors": "Fionn Murtagh and Giuseppe Iurato", "title": "Visualization of Jacques Lacan's Registers of the Psychoanalytic Field,\n  and Discovery of Metaphor and of Metonymy. Analytical Case Study of Edgar\n  Allan Poe's \"The Purloined Letter\"", "comments": "34 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We start with a description of Lacan's work that we then take into our\nanalytics methodology. In a first investigation, a Lacan-motivated template of\nthe Poe story is fitted to the data. A segmentation of the storyline is used in\norder to map out the diachrony. Based on this, it will be shown how synchronous\naspects, potentially related to Lacanian registers, can be sought. This\ndemonstrates the effectiveness of an approach based on a model template of the\nstoryline narrative. In a second and more comprehensive investigation, we\ndevelop an approach for revealing, that is, uncovering, Lacanian register\nrelationships. Objectives of this work include the wide and general application\nof our methodology. This methodology is strongly based on the \"letting the data\nspeak\" Correspondence Analysis analytics platform of Jean-Paul Benz\\'ecri, that\nis also the geometric data analysis, both qualitative and quantitative\nanalytics, developed by Pierre Bourdieu.\n", "versions": [{"version": "v1", "created": "Sat, 23 Apr 2016 20:48:50 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2016 20:54:20 GMT"}, {"version": "v3", "created": "Mon, 30 Jan 2017 21:33:18 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Murtagh", "Fionn", ""], ["Iurato", "Giuseppe", ""]]}, {"id": "1604.06968", "submitter": "Kevin A. Lai", "authors": "Kevin A. Lai, Anup B. Rao, Santosh Vempala", "title": "Agnostic Estimation of Mean and Covariance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the mean and covariance of a\ndistribution from iid samples in $\\mathbb{R}^n$, in the presence of an $\\eta$\nfraction of malicious noise; this is in contrast to much recent work where the\nnoise itself is assumed to be from a distribution of known type. The agnostic\nproblem includes many interesting special cases, e.g., learning the parameters\nof a single Gaussian (or finding the best-fit Gaussian) when $\\eta$ fraction of\ndata is adversarially corrupted, agnostically learning a mixture of Gaussians,\nagnostic ICA, etc. We present polynomial-time algorithms to estimate the mean\nand covariance with error guarantees in terms of information-theoretic lower\nbounds. As a corollary, we also obtain an agnostic algorithm for Singular Value\nDecomposition.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2016 00:23:51 GMT"}, {"version": "v2", "created": "Sun, 14 Aug 2016 19:50:58 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Lai", "Kevin A.", ""], ["Rao", "Anup B.", ""], ["Vempala", "Santosh", ""]]}, {"id": "1604.07070", "submitter": "Shuai Zheng", "authors": "Shuai Zheng and James T. Kwok", "title": "Stochastic Variance-Reduced ADMM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The alternating direction method of multipliers (ADMM) is a powerful\noptimization solver in machine learning. Recently, stochastic ADMM has been\nintegrated with variance reduction methods for stochastic gradient, leading to\nSAG-ADMM and SDCA-ADMM that have fast convergence rates and low iteration\ncomplexities. However, their space requirements can still be high. In this\npaper, we propose an integration of ADMM with the method of stochastic variance\nreduced gradient (SVRG). Unlike another recent integration attempt called\nSCAS-ADMM, the proposed algorithm retains the fast convergence benefits of\nSAG-ADMM and SDCA-ADMM, but is more advantageous in that its storage\nrequirement is very low, even independent of the sample size $n$. We also\nextend the proposed method for nonconvex problems, and obtain a convergence\nrate of $O(1/T)$. Experimental results demonstrate that it is as fast as\nSAG-ADMM and SDCA-ADMM, much faster than SCAS-ADMM, and can be used on much\nbigger data sets.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2016 18:50:58 GMT"}, {"version": "v2", "created": "Wed, 12 Oct 2016 16:59:55 GMT"}, {"version": "v3", "created": "Sun, 16 Oct 2016 18:56:57 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Zheng", "Shuai", ""], ["Kwok", "James T.", ""]]}, {"id": "1604.07093", "submitter": "Yanwei  Fu", "authors": "Yanwei Fu, Leonid Sigal", "title": "Semi-supervised Vocabulary-informed Learning", "comments": "10 pages, Accepted by CVPR 2016 as an oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant progress in object categorization, in recent years, a\nnumber of important challenges remain, mainly, ability to learn from limited\nlabeled data and ability to recognize object classes within large, potentially\nopen, set of labels. Zero-shot learning is one way of addressing these\nchallenges, but it has only been shown to work with limited sized class\nvocabularies and typically requires separation between supervised and\nunsupervised classes, allowing former to inform the latter but not vice versa.\nWe propose the notion of semi-supervised vocabulary-informed learning to\nalleviate the above mentioned challenges and address problems of supervised,\nzero-shot and open set recognition using a unified framework. Specifically, we\npropose a maximum margin framework for semantic manifold-based recognition that\nincorporates distance constraints from (both supervised and unsupervised)\nvocabulary atoms, ensuring that labeled samples are projected closest to their\ncorrect prototypes, in the embedding space, than to others. We show that\nresulting model shows improvements in supervised, zero-shot, and large open set\nrecognition, with up to 310K class vocabulary on AwA and ImageNet datasets.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2016 23:36:36 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Fu", "Yanwei", ""], ["Sigal", "Leonid", ""]]}, {"id": "1604.07101", "submitter": "Huasen Wu", "authors": "Huasen Wu and Xin Liu", "title": "Double Thompson Sampling for Dueling Bandits", "comments": "27 pages, 5 figures, 9 tables; accepted by 30th Conference on Neural\n  Information Processing Systems (NIPS), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a Double Thompson Sampling (D-TS) algorithm for\ndueling bandit problems. As indicated by its name, D-TS selects both the first\nand the second candidates according to Thompson Sampling. Specifically, D-TS\nmaintains a posterior distribution for the preference matrix, and chooses the\npair of arms for comparison by sampling twice from the posterior distribution.\nThis simple algorithm applies to general Copeland dueling bandits, including\nCondorcet dueling bandits as its special case. For general Copeland dueling\nbandits, we show that D-TS achieves $O(K^2 \\log T)$ regret. For Condorcet\ndueling bandits, we further simplify the D-TS algorithm and show that the\nsimplified D-TS algorithm achieves $O(K \\log T + K^2 \\log \\log T)$ regret.\nSimulation results based on both synthetic and real-world data demonstrate the\nefficiency of the proposed D-TS algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 00:38:16 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 17:36:57 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Wu", "Huasen", ""], ["Liu", "Xin", ""]]}, {"id": "1604.07143", "submitter": "Erwan Scornet", "authors": "G\\'erard Biau (LPMA, LSTA), Erwan Scornet (LSTA), Johannes Welbl (UCL)", "title": "Neural Random Forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an ensemble of randomized regression trees, it is possible to\nrestructure them as a collection of multilayered neural networks with\nparticular connection weights. Following this principle, we reformulate the\nrandom forest method of Breiman (2001) into a neural network setting, and in\nturn propose two new hybrid procedures that we call neural random forests. Both\npredictors exploit prior knowledge of regression trees for their architecture,\nhave less parameters to tune than standard networks, and less restrictions on\nthe geometry of the decision boundaries than trees. Consistency results are\nproved, and substantial numerical evidence is provided on both synthetic and\nreal data sets to assess the excellent performance of our methods in a large\nvariety of prediction problems.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 06:43:47 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 07:42:50 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Biau", "G\u00e9rard", "", "LPMA, LSTA"], ["Scornet", "Erwan", "", "LSTA"], ["Welbl", "Johannes", "", "UCL"]]}, {"id": "1604.07178", "submitter": "Muhammad Yousefnezhad", "authors": "Muhammad Yousefnezhad, Daoqiang Zhang", "title": "Weighted Spectral Cluster Ensemble", "comments": "IEEE International Conference on Data Mining (ICDM), 2015", "journal-ref": null, "doi": "10.1109/ICDM.2015.145", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering explores meaningful patterns in the non-labeled data sets. Cluster\nEnsemble Selection (CES) is a new approach, which can combine individual\nclustering results for increasing the performance of the final results.\nAlthough CES can achieve better final results in comparison with individual\nclustering algorithms and cluster ensemble methods, its performance can be\ndramatically affected by its consensus diversity metric and thresholding\nprocedure. There are two problems in CES: 1) most of the diversity metrics is\nbased on heuristic Shannon's entropy and 2) estimating threshold values are\nreally hard in practice. The main goal of this paper is proposing a robust\napproach for solving the above mentioned problems. Accordingly, this paper\ndevelops a novel framework for clustering problems, which is called Weighted\nSpectral Cluster Ensemble (WSCE), by exploiting some concepts from community\ndetection arena and graph based clustering. Under this framework, a new version\nof spectral clustering, which is called Two Kernels Spectral Clustering, is\nused for generating graphs based individual clustering results. Further, by\nusing modularity, which is a famous metric in the community detection, on the\ntransformed graph representation of individual clustering results, our approach\nprovides an effective diversity estimation for individual clustering results.\nMoreover, this paper introduces a new approach for combining the evaluated\nindividual clustering results without the procedure of thresholding.\nExperimental study on varied data sets demonstrates that the prosed approach\nachieves superior performance to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 09:29:21 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Yousefnezhad", "Muhammad", ""], ["Zhang", "Daoqiang", ""]]}, {"id": "1604.07356", "submitter": "Francois Fagan", "authors": "Krzysztof Choromanski, Francois Fagan", "title": "Fast nonlinear embeddings via structured matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new paradigm for speeding up randomized computations of several\nfrequently used functions in machine learning. In particular, our paradigm can\nbe applied for improving computations of kernels based on random embeddings.\nAbove that, the presented framework covers multivariate randomized functions.\nAs a byproduct, we propose an algorithmic approach that also leads to a\nsignificant reduction of space complexity. Our method is based on careful\nrecycling of Gaussian vectors into structured matrices that share properties of\nfully random matrices. The quality of the proposed structured approach follows\nfrom combinatorial properties of the graphs encoding correlations between rows\nof these structured matrices. Our framework covers as special cases already\nknown structured approaches such as the Fast Johnson-Lindenstrauss Transform,\nbut is much more general since it can be applied also to highly nonlinear\nembeddings. We provide strong concentration results showing the quality of the\npresented paradigm.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 18:33:59 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Choromanski", "Krzysztof", ""], ["Fagan", "Francois", ""]]}, {"id": "1604.07407", "submitter": "Vlad Niculae", "authors": "Vlad Niculae and Cristian Danescu-Niculescu-Mizil", "title": "Conversational Markers of Constructive Discussions", "comments": "To appear at NAACL-HLT 2016. 11pp, 5 fig. Data and other info\n  available at http://vene.ro/constructive/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group discussions are essential for organizing every aspect of modern life,\nfrom faculty meetings to senate debates, from grant review panels to papal\nconclaves. While costly in terms of time and organization effort, group\ndiscussions are commonly seen as a way of reaching better decisions compared to\nsolutions that do not require coordination between the individuals (e.g.\nvoting)---through discussion, the sum becomes greater than the parts. However,\nthis assumption is not irrefutable: anecdotal evidence of wasteful discussions\nabounds, and in our own experiments we find that over 30% of discussions are\nunproductive.\n  We propose a framework for analyzing conversational dynamics in order to\ndetermine whether a given task-oriented discussion is worth having or not. We\nexploit conversational patterns reflecting the flow of ideas and the balance\nbetween the participants, as well as their linguistic choices. We apply this\nframework to conversations naturally occurring in an online collaborative world\nexploration game developed and deployed to support this research. Using this\nsetting, we show that linguistic cues and conversational patterns extracted\nfrom the first 20 seconds of a team discussion are predictive of whether it\nwill be a wasteful or a productive one.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 20:00:02 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Niculae", "Vlad", ""], ["Danescu-Niculescu-Mizil", "Cristian", ""]]}, {"id": "1604.07451", "submitter": "Guo Yu", "authors": "Guo Yu and Jacob Bien", "title": "Learning Local Dependence In Ordered Data", "comments": null, "journal-ref": "Journal of Machine Learning (2017) 18(42) 1-60", "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, data come with a natural ordering. This ordering can\noften induce local dependence among nearby variables. However, in complex data,\nthe width of this dependence may vary, making simple assumptions such as a\nconstant neighborhood size unrealistic. We propose a framework for learning\nthis local dependence based on estimating the inverse of the Cholesky factor of\nthe covariance matrix. Penalized maximum likelihood estimation of this matrix\nyields a simple regression interpretation for local dependence in which\nvariables are predicted by their neighbors. Our proposed method involves\nsolving a convex, penalized Gaussian likelihood problem with a hierarchical\ngroup lasso penalty. The problem decomposes into independent subproblems which\ncan be solved efficiently in parallel using first-order methods. Our method\nyields a sparse, symmetric, positive definite estimator of the precision\nmatrix, encoding a Gaussian graphical model. We derive theoretical results not\nfound in existing methods attaining this structure. In particular, our\nconditions for signed support recovery and estimation consistency rates in\nmultiple norms are as mild as those in a regression problem. Empirical results\nshow our method performing favorably compared to existing methods. We apply our\nmethod to genomic data to flexibly model linkage disequilibrium. Our method is\nalso applied to improve the performance of discriminant analysis in sound\nrecording classification.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 21:20:51 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2016 21:42:25 GMT"}, {"version": "v3", "created": "Mon, 8 May 2017 03:15:08 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Yu", "Guo", ""], ["Bien", "Jacob", ""]]}, {"id": "1604.07463", "submitter": "Mohsen Bayati", "authors": "Sheng Qiang and Mohsen Bayati", "title": "Dynamic Pricing with Demand Covariates", "comments": "28 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a firm that sells products over $T$ periods without knowing the\ndemand function. The firm sequentially sets prices to earn revenue and to learn\nthe underlying demand function simultaneously. A natural heuristic for this\nproblem, commonly used in practice, is greedy iterative least squares (GILS).\nAt each time period, GILS estimates the demand as a linear function of the\nprice by applying least squares to the set of prior prices and realized\ndemands. Then a price that maximizes the revenue, given the estimated demand\nfunction, is used for the next time period. The performance is measured by the\nregret, which is the expected revenue loss from the optimal (oracle) pricing\npolicy when the demand function is known. Recently, den Boer and Zwart (2014)\nand Keskin and Zeevi (2014) demonstrated that GILS is sub-optimal. They\nintroduced algorithms which integrate forced price dispersion with GILS and\nachieve asymptotically optimal performance.\n  In this paper, we consider this dynamic pricing problem in a data-rich\nenvironment. In particular, we assume that the firm knows the expected demand\nunder a particular price from historical data, and in each period, before\nsetting the price, the firm has access to extra information (demand covariates)\nwhich may be predictive of the demand. We prove that in this setting GILS\nachieves asymptotically optimal regret of order $\\log(T)$. We also show the\nfollowing surprising result: in the original dynamic pricing problem of den\nBoer and Zwart (2014) and Keskin and Zeevi (2014), inclusion of any set of\ncovariates in GILS as potential demand covariates (even though they could carry\nno information) would make GILS asymptotically optimal. We validate our results\nvia extensive numerical simulations on synthetic and real data sets.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 22:22:11 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Qiang", "Sheng", ""], ["Bayati", "Mohsen", ""]]}, {"id": "1604.07464", "submitter": "Mingyuan Zhou", "authors": "Mingyuan Zhou", "title": "Nonparametric Bayesian Negative Binomial Factor Analysis", "comments": "To appear in Bayesian Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common approach to analyze a covariate-sample count matrix, an element of\nwhich represents how many times a covariate appears in a sample, is to\nfactorize it under the Poisson likelihood. We show its limitation in capturing\nthe tendency for a covariate present in a sample to both repeat itself and\nexcite related ones. To address this limitation, we construct negative binomial\nfactor analysis (NBFA) to factorize the matrix under the negative binomial\nlikelihood, and relate it to a Dirichlet-multinomial distribution based\nmixed-membership model. To support countably infinite factors, we propose the\nhierarchical gamma-negative binomial process. By exploiting newly proved\nconnections between discrete distributions, we construct two blocked and a\ncollapsed Gibbs sampler that all adaptively truncate their number of factors,\nand demonstrate that the blocked Gibbs sampler developed under a compound\nPoisson representation converges fast and has low computational complexity.\nExample results show that NBFA has a distinct mechanism in adjusting its number\nof inferred factors according to the sample lengths, and provides clear\nadvantages in parsimonious representation, predictive power, and computational\ncomplexity over previously proposed discrete latent variable models, which\neither completely ignore burstiness, or model only the burstiness of the\ncovariates but not that of the factors.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 22:27:25 GMT"}, {"version": "v2", "created": "Thu, 5 Oct 2017 15:54:43 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Zhou", "Mingyuan", ""]]}, {"id": "1604.07484", "submitter": "Maziar Raissi", "authors": "Maziar Raissi and George Karniadakis", "title": "Deep Multi-fidelity Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel multi-fidelity framework that goes far beyond the\nclassical AR(1) Co-kriging scheme of Kennedy and O'Hagan (2000). Our method can\nhandle general discontinuous cross-correlations among systems with different\nlevels of fidelity. A combination of multi-fidelity Gaussian Processes (AR(1)\nCo-kriging) and deep neural networks enables us to construct a method that is\nimmune to discontinuities. We demonstrate the effectiveness of the new\ntechnology using standard benchmark problems designed to resemble the outputs\nof complicated high- and low-fidelity codes.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 00:44:42 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Raissi", "Maziar", ""], ["Karniadakis", "George", ""]]}, {"id": "1604.07554", "submitter": "Muhammad Yousefnezhad", "authors": "Maziar Kazemi, Muhammad Yousefnezhad, Saber Nourian", "title": "A New Approach in Persian Handwritten Letters Recognition Using Error\n  Correcting Output Coding", "comments": "Journal of Advances in Computer Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification Ensemble, which uses the weighed polling of outputs, is the\nart of combining a set of basic classifiers for generating high-performance,\nrobust and more stable results. This study aims to improve the results of\nidentifying the Persian handwritten letters using Error Correcting Output\nCoding (ECOC) ensemble method. Furthermore, the feature selection is used to\nreduce the costs of errors in our proposed method. ECOC is a method for\ndecomposing a multi-way classification problem into many binary classification\ntasks; and then combining the results of the subtasks into a hypothesized\nsolution to the original problem. Firstly, the image features are extracted by\nPrincipal Components Analysis (PCA). After that, ECOC is used for\nidentification the Persian handwritten letters which it uses Support Vector\nMachine (SVM) as the base classifier. The empirical results of applying this\nensemble method using 10 real-world data sets of Persian handwritten letters\nindicate that this method has better results in identifying the Persian\nhandwritten letters than other ensemble methods and also single\nclassifications. Moreover, by testing a number of different features, this\npaper found that we can reduce the additional cost in feature selection stage\nby using this method.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 07:43:59 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Kazemi", "Maziar", ""], ["Yousefnezhad", "Muhammad", ""], ["Nourian", "Saber", ""]]}, {"id": "1604.07706", "submitter": "Shuai Li", "authors": "Nathan Korda and Balazs Szorenyi and Shuai Li", "title": "Distributed Clustering of Linear Bandits in Peer to Peer Networks", "comments": "The 33rd ICML, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide two distributed confidence ball algorithms for solving linear\nbandit problems in peer to peer networks with limited communication\ncapabilities. For the first, we assume that all the peers are solving the same\nlinear bandit problem, and prove that our algorithm achieves the optimal\nasymptotic regret rate of any centralised algorithm that can instantly\ncommunicate information between the peers. For the second, we assume that there\nare clusters of peers solving the same bandit problem within each cluster, and\nwe prove that our algorithm discovers these clusters, while achieving the\noptimal asymptotic regret rate within each one. Through experiments on several\nreal-world datasets, we demonstrate the performance of proposed algorithms\ncompared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 14:59:43 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 06:12:46 GMT"}, {"version": "v3", "created": "Tue, 7 Jun 2016 08:06:23 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Korda", "Nathan", ""], ["Szorenyi", "Balazs", ""], ["Li", "Shuai", ""]]}, {"id": "1604.07711", "submitter": "Brijnesh Jain", "authors": "Brijnesh J. Jain", "title": "Condorcet's Jury Theorem for Consensus Clustering and its Implications\n  for Diversity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Condorcet's Jury Theorem has been invoked for ensemble classifiers to\nindicate that the combination of many classifiers can have better predictive\nperformance than a single classifier. Such a theoretical underpinning is\nunknown for consensus clustering. This article extends Condorcet's Jury Theorem\nto the mean partition approach under the additional assumptions that a unique\nground-truth partition exists and sample partitions are drawn from a\nsufficiently small ball containing the ground-truth. As an implication of\npractical relevance, we question the claim that the quality of consensus\nclustering depends on the diversity of the sample partitions. Instead, we\nconjecture that limiting the diversity of the mean partitions is necessary for\ncontrolling the quality.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 15:14:49 GMT"}, {"version": "v2", "created": "Mon, 10 Oct 2016 10:06:28 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Jain", "Brijnesh J.", ""]]}, {"id": "1604.07796", "submitter": "Henry Lo", "authors": "Henry Z. Lo and Kevin Amaral and Wei Ding", "title": "Scale Normalization", "comments": "Preliminary version submitted to ICLR workshop 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the difficulties of training deep neural networks is caused by\nimproper scaling between layers. Scaling issues introduce exploding / gradient\nproblems, and have typically been addressed by careful scale-preserving\ninitialization. We investigate the value of preserving scale, or isometry,\nbeyond the initial weights. We propose two methods of maintaing isometry, one\nexact and one stochastic. Preliminary experiments show that for both\ndeterminant and scale-normalization effectively speeds up learning. Results\nsuggest that isometry is important in the beginning of learning, and\nmaintaining it leads to faster learning.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 19:04:59 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Lo", "Henry Z.", ""], ["Amaral", "Kevin", ""], ["Ding", "Wei", ""]]}, {"id": "1604.07878", "submitter": "Muhammad Yousefnezhad", "authors": "Ali Reihanian, Behrouz Minaei-Bidgoli, Muhammad Yousefnezhad", "title": "Evaluating the effect of topic consideration in identifying communities\n  of rating-based social networks", "comments": "International Conference on Information and Knowledge Technology\n  (IKT) 2015", "journal-ref": null, "doi": "10.1109/IKT.2015.7288793", "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding meaningful communities in social network has attracted the attentions\nof many researchers. The community structure of complex networks reveals both\ntheir organization and hidden relations among their constituents. Most of the\nresearches in the field of community detection mainly focus on the topological\nstructure of the network without performing any content analysis. Nowadays,\nreal world social networks are containing a vast range of information including\nshared objects, comments, following information, etc. In recent years, a number\nof researches have proposed approaches which consider both the contents that\nare interchanged in the networks and the topological structures of the networks\nin order to find more meaningful communities. In this research, the effect of\ntopic analysis in finding more meaningful communities in social networking\nsites in which the users express their feelings toward different objects (like\nmovies) by the means of rating is demonstrated by performing extensive\nexperiments.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 22:38:47 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Reihanian", "Ali", ""], ["Minaei-Bidgoli", "Behrouz", ""], ["Yousefnezhad", "Muhammad", ""]]}, {"id": "1604.07928", "submitter": "Shandian Zhe", "authors": "Shandian Zhe, Kai Zhang, Pengyuan Wang, Kuang-chih Lee, Zenglin Xu,\n  Yuan Qi, Zoubin Ghahramani", "title": "Distributed Flexible Nonlinear Tensor Factorization", "comments": "Gaussian process, tensor factorization, multidimensional arrays,\n  large scale, spark, map-reduce", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor factorization is a powerful tool to analyse multi-way data. Compared\nwith traditional multi-linear methods, nonlinear tensor factorization models\nare capable of capturing more complex relationships in the data. However, they\nare computationally expensive and may suffer severe learning bias in case of\nextreme data sparsity. To overcome these limitations, in this paper we propose\na distributed, flexible nonlinear tensor factorization model. Our model can\neffectively avoid the expensive computations and structural restrictions of the\nKronecker-product in existing TGP formulations, allowing an arbitrary subset of\ntensorial entries to be selected to contribute to the training. At the same\ntime, we derive a tractable and tight variational evidence lower bound (ELBO)\nthat enables highly decoupled, parallel computations and high-quality\ninference. Based on the new bound, we develop a distributed inference algorithm\nin the MapReduce framework, which is key-value-free and can fully exploit the\nmemory cache mechanism in fast MapReduce systems such as SPARK. Experimental\nresults fully demonstrate the advantages of our method over several\nstate-of-the-art approaches, in terms of both predictive performance and\ncomputational efficiency. Moreover, our approach shows a promising potential in\nthe application of Click-Through-Rate (CTR) prediction for online advertising.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 04:18:32 GMT"}, {"version": "v2", "created": "Sun, 22 May 2016 00:00:23 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Zhe", "Shandian", ""], ["Zhang", "Kai", ""], ["Wang", "Pengyuan", ""], ["Lee", "Kuang-chih", ""], ["Xu", "Zenglin", ""], ["Qi", "Yuan", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1604.07990", "submitter": "Andres Masegosa R", "authors": "Andres R. Masegosa, Ana M. Martinez, Hanen Borchani", "title": "Probabilistic Graphical Models on Multi-Core CPUs using Java 8", "comments": "Pre-print version of the paper presented in the special issue on\n  Computational Intelligence Software at IEEE Computational Intelligence\n  Magazine journal", "journal-ref": "IEEE Computational Intelligence Magazine, 11(2), 41-54. 2016", "doi": "10.1109/MCI.2016.2532267", "report-no": null, "categories": "cs.AI cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss software design issues related to the development\nof parallel computational intelligence algorithms on multi-core CPUs, using the\nnew Java 8 functional programming features. In particular, we focus on\nprobabilistic graphical models (PGMs) and present the parallelisation of a\ncollection of algorithms that deal with inference and learning of PGMs from\ndata. Namely, maximum likelihood estimation, importance sampling, and greedy\nsearch for solving combinatorial optimisation problems. Through these concrete\nexamples, we tackle the problem of defining efficient data structures for PGMs\nand parallel processing of same-size batches of data sets using Java 8\nfeatures. We also provide straightforward techniques to code parallel\nalgorithms that seamlessly exploit multi-core processors. The experimental\nanalysis, carried out using our open source AMIDST (Analysis of MassIve Data\nSTreams) Java toolbox, shows the merits of the proposed solutions.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 09:28:27 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Masegosa", "Andres R.", ""], ["Martinez", "Ana M.", ""], ["Borchani", "Hanen", ""]]}, {"id": "1604.08079", "submitter": "Paula Branco", "authors": "Paula Branco, Rita P. Ribeiro, Luis Torgo", "title": "UBL: an R package for Utility-based Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document describes the R package UBL that allows the use of several\nmethods for handling utility-based learning problems. Classification and\nregression problems that assume non-uniform costs and/or benefits pose serious\nchallenges to predictive analytic tasks. In the context of meteorology,\nfinance, medicine, ecology, among many other, specific domain information\nconcerning the preference bias of the users must be taken into account to\nenhance the models predictive performance. To deal with this problem, a large\nnumber of techniques was proposed by the research community for both\nclassification and regression tasks. The main goal of UBL package is to\nfacilitate the utility-based predictive analytic task by providing a set of\nmethods to deal with this type of problems in the R environment. It is a\nversatile tool that provides mechanisms to handle both regression and\nclassification (binary and multiclass) tasks. Moreover, UBL package allows the\nuser to specify his domain preferences, but it also provides some automatic\nmethods that try to infer those preference bias from the domain, considering\nsome common known settings.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 14:13:11 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 23:08:46 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Branco", "Paula", ""], ["Ribeiro", "Rita P.", ""], ["Torgo", "Luis", ""]]}, {"id": "1604.08098", "submitter": "Lei Han", "authors": "Lei Han, Kean Ming Tan, Ting Yang and Tong Zhang", "title": "Local Uncertainty Sampling for Large-Scale Multi-Class Logistic\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge for building statistical models in the big data era is that\nthe available data volume far exceeds the computational capability. A common\napproach for solving this problem is to employ a subsampled dataset that can be\nhandled by available computational resources. In this paper, we propose a\ngeneral subsampling scheme for large-scale multi-class logistic regression and\nexamine the variance of the resulting estimator. We show that asymptotically,\nthe proposed method always achieves a smaller variance than that of the uniform\nrandom sampling. Moreover, when the classes are conditionally imbalanced,\nsignificant improvement over uniform sampling can be achieved. Empirical\nperformance of the proposed method is compared to other methods on both\nsimulated and real-world datasets, and these results match and confirm our\ntheoretical analysis.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 15:00:12 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 09:05:18 GMT"}, {"version": "v3", "created": "Thu, 13 Sep 2018 05:08:08 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Han", "Lei", ""], ["Tan", "Kean Ming", ""], ["Yang", "Ting", ""], ["Zhang", "Tong", ""]]}, {"id": "1604.08102", "submitter": "Dennis Prangle", "authors": "Dennis Prangle and Richard G. Everitt", "title": "An ABC interpretation of the multiple auxiliary variable method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the auxiliary variable method (M{\\o}ller et al., 2006; Murray et\nal., 2006) for inference of Markov random fields can be viewed as an\napproximate Bayesian computation method for likelihood estimation.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 15:07:48 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Prangle", "Dennis", ""], ["Everitt", "Richard G.", ""]]}, {"id": "1604.08153", "submitter": "Kai Arulkumaran", "authors": "Kai Arulkumaran, Nat Dilokthanakul, Murray Shanahan, Anil Anthony\n  Bharath", "title": "Classifying Options for Deep Reinforcement Learning", "comments": "IJCAI 2016 Workshop on Deep Reinforcement Learning: Frontiers and\n  Challenges", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we combine one method for hierarchical reinforcement learning -\nthe options framework - with deep Q-networks (DQNs) through the use of\ndifferent \"option heads\" on the policy network, and a supervisory network for\nchoosing between the different options. We utilise our setup to investigate the\neffects of architectural constraints in subtasks with positive and negative\ntransfer, across a range of network capacities. We empirically show that our\naugmented DQN has lower sample complexity when simultaneously learning subtasks\nwith negative transfer, without degrading performance when learning subtasks\nwith positive transfer.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 17:48:39 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 16:05:31 GMT"}, {"version": "v3", "created": "Mon, 19 Jun 2017 15:34:58 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Arulkumaran", "Kai", ""], ["Dilokthanakul", "Nat", ""], ["Shanahan", "Murray", ""], ["Bharath", "Anil Anthony", ""]]}, {"id": "1604.08201", "submitter": "Wojciech Samek", "authors": "Irene Sturm, Sebastian Bach, Wojciech Samek, Klaus-Robert M\\\"uller", "title": "Interpretable Deep Neural Networks for Single-Trial EEG Classification", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: In cognitive neuroscience the potential of Deep Neural Networks\n(DNNs) for solving complex classification tasks is yet to be fully exploited.\nThe most limiting factor is that DNNs as notorious 'black boxes' do not provide\ninsight into neurophysiological phenomena underlying a decision. Layer-wise\nRelevance Propagation (LRP) has been introduced as a novel method to explain\nindividual network decisions. New Method: We propose the application of DNNs\nwith LRP for the first time for EEG data analysis. Through LRP the single-trial\nDNN decisions are transformed into heatmaps indicating each data point's\nrelevance for the outcome of the decision. Results: DNN achieves classification\naccuracies comparable to those of CSP-LDA. In subjects with low performance\nsubject-to-subject transfer of trained DNNs can improve the results. The\nsingle-trial LRP heatmaps reveal neurophysiologically plausible patterns,\nresembling CSP-derived scalp maps. Critically, while CSP patterns represent\nclass-wise aggregated information, LRP heatmaps pinpoint neural patterns to\nsingle time points in single trials. Comparison with Existing Method(s): We\ncompare the classification performance of DNNs to that of linear CSP-LDA on two\ndata sets related to motor-imaginery BCI. Conclusion: We have demonstrated that\nDNN is a powerful non-linear tool for EEG analysis. With LRP a new quality of\nhigh-resolution assessment of neural activity can be reached. LRP is a\npotential remedy for the lack of interpretability of DNNs that has limited\ntheir utility in neuroscientific applications. The extreme specificity of the\nLRP-derived heatmaps opens up new avenues for investigating neural activity\nunderlying complex perception or decision-related processes.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 19:50:40 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Sturm", "Irene", ""], ["Bach", "Sebastian", ""], ["Samek", "Wojciech", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1604.08291", "submitter": "Dacheng Tao", "authors": "Chang Xu, Dacheng Tao, Chao Xu", "title": "Streaming View Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An underlying assumption in conventional multi-view learning algorithms is\nthat all views can be simultaneously accessed. However, due to various factors\nwhen collecting and pre-processing data from different views, the streaming\nview setting, in which views arrive in a streaming manner, is becoming more\ncommon. By assuming that the subspaces of a multi-view model trained over past\nviews are stable, here we fine tune their combination weights such that the\nwell-trained multi-view model is compatible with new views. This largely\novercomes the burden of learning new view functions and updating past view\nfunctions. We theoretically examine convergence issues and the influence of\nstreaming views in the proposed algorithm. Experimental results on real-world\ndatasets suggest that studying the streaming views problem in multi-view\nlearning is significant and that the proposed algorithm can effectively handle\nstreaming views in different applications.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 02:37:03 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Xu", "Chang", ""], ["Tao", "Dacheng", ""], ["Xu", "Chao", ""]]}, {"id": "1604.08320", "submitter": "Xun Huan", "authors": "Xun Huan, Youssef M. Marzouk", "title": "Sequential Bayesian optimal experimental design via approximate dynamic\n  programming", "comments": "Preprint 34 pages, 12 figures (36 small figures). v1 submitted to the\n  SIAM/ASA Journal on Uncertainty Quantification on April 27, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of multiple experiments is commonly undertaken via suboptimal\nstrategies, such as batch (open-loop) design that omits feedback or greedy\n(myopic) design that does not account for future effects. This paper introduces\nnew strategies for the optimal design of sequential experiments. First, we\nrigorously formulate the general sequential optimal experimental design (sOED)\nproblem as a dynamic program. Batch and greedy designs are shown to result from\nspecial cases of this formulation. We then focus on sOED for parameter\ninference, adopting a Bayesian formulation with an information theoretic design\nobjective. To make the problem tractable, we develop new numerical approaches\nfor nonlinear design with continuous parameter, design, and observation spaces.\nWe approximate the optimal policy by using backward induction with regression\nto construct and refine value function approximations in the dynamic program.\nThe proposed algorithm iteratively generates trajectories via exploration and\nexploitation to improve approximation accuracy in frequently visited regions of\nthe state space. Numerical results are verified against analytical solutions in\na linear-Gaussian setting. Advantages over batch and greedy design are then\ndemonstrated on a nonlinear source inversion problem where we seek an optimal\npolicy for sequential sensing.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 06:32:27 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Huan", "Xun", ""], ["Marzouk", "Youssef M.", ""]]}, {"id": "1604.08402", "submitter": "Wenjie Zheng", "authors": "Wenjie Zheng", "title": "Two Differentially Private Rating Collection Mechanisms for Recommender\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design two mechanisms for the recommender system to collect user ratings.\nOne is modified Laplace mechanism, and the other is randomized response\nmechanism. We prove that they are both differentially private and preserve the\ndata utility.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 13:11:54 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Zheng", "Wenjie", ""]]}, {"id": "1604.08524", "submitter": "Andres G. Abad", "authors": "Andres G. Abad and Luis I. Reyes Castro", "title": "A Probabilistic Adaptive Search System for Exploring the Face Space", "comments": "6 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recall is a basic human cognitive process performed routinely, e.g.,\nwhen meeting someone and determining if we have met that person before.\nAssisting a subject during face recall by suggesting candidate faces can be\nchallenging. One of the reasons is that the search space - the face space - is\nquite large and lacks structure. A commercial application of face recall is\nfacial composite systems - such as Identikit, PhotoFIT, and CD-FIT - where a\nwitness searches for an image of a face that resembles his memory of a\nparticular offender. The inherent uncertainty and cost in the evaluation of the\nobjective function, the large size and lack of structure of the search space,\nand the unavailability of the gradient concept makes this problem inappropriate\nfor traditional optimization methods. In this paper we propose a novel\nevolutionary approach for searching the face space that can be used as a facial\ncomposite system. The approach is inspired by methods of Bayesian optimization\nand differs from other applications in the use of the skew-normal distribution\nas its acquisition function. This choice of acquisition function provides\ngreater granularity, with regularized, conservative, and realistic results.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 17:23:42 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Abad", "Andres G.", ""], ["Castro", "Luis I. Reyes", ""]]}, {"id": "1604.08634", "submitter": "Gautier Marti", "authors": "Gautier Marti, S\\'ebastien Andler, Frank Nielsen, Philippe Donnat", "title": "Optimal Transport vs. Fisher-Rao distance between Copulas for Clustering\n  Multivariate Time Series", "comments": "Accepted at IEEE Workshop on Statistical Signal Processing (SSP 2016)", "journal-ref": null, "doi": "10.1109/SSP.2016.7551770", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a methodology for clustering N objects which are described by\nmultivariate time series, i.e. several sequences of real-valued random\nvariables. This clustering methodology leverages copulas which are\ndistributions encoding the dependence structure between several random\nvariables. To take fully into account the dependence information while\nclustering, we need a distance between copulas. In this work, we compare\nrenowned distances between distributions: the Fisher-Rao geodesic distance,\nrelated divergences and optimal transport, and discuss their advantages and\ndisadvantages. Applications of such methodology can be found in the clustering\nof financial assets. A tutorial, experiments and implementation for\nreproducible research can be found at www.datagrapple.com/Tech.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 22:10:30 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2016 10:50:11 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Marti", "Gautier", ""], ["Andler", "S\u00e9bastien", ""], ["Nielsen", "Frank", ""], ["Donnat", "Philippe", ""]]}, {"id": "1604.08697", "submitter": "Kean Ming Tan", "authors": "Kean Ming Tan, Zhaoran Wang, Han Liu, Tong Zhang", "title": "Sparse Generalized Eigenvalue Problem: Optimal Statistical Rates via\n  Truncated Rayleigh Flow", "comments": "To appear in JRSSB", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse generalized eigenvalue problem (GEP) plays a pivotal role in a large\nfamily of high-dimensional statistical models, including sparse Fisher's\ndiscriminant analysis, canonical correlation analysis, and sufficient dimension\nreduction. Sparse GEP involves solving a non-convex optimization problem. Most\nexisting methods and theory in the context of specific statistical models that\nare special cases of the sparse GEP require restrictive structural assumptions\non the input matrices. In this paper, we propose a two-stage computational\nframework to solve the sparse GEP. At the first stage, we solve a convex\nrelaxation of the sparse GEP. Taking the solution as an initial value, we then\nexploit a nonconvex optimization perspective and propose the truncated Rayleigh\nflow method (Rifle) to estimate the leading generalized eigenvector. We show\nthat Rifle converges linearly to a solution with the optimal statistical rate\nof convergence for many statistical models. Theoretically, our method\nsignificantly improves upon the existing literature by eliminating structural\nassumptions on the input matrices for both stages. To achieve this, our\nanalysis involves two key ingredients: (i) a new analysis of the gradient based\nmethod on nonconvex objective functions, and (ii) a fine-grained\ncharacterization of the evolution of sparsity patterns along the solution path.\nThorough numerical studies are provided to validate the theoretical results.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 06:12:19 GMT"}, {"version": "v2", "created": "Fri, 17 Aug 2018 21:48:29 GMT"}, {"version": "v3", "created": "Fri, 31 Aug 2018 19:48:51 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Tan", "Kean Ming", ""], ["Wang", "Zhaoran", ""], ["Liu", "Han", ""], ["Zhang", "Tong", ""]]}, {"id": "1604.08716", "submitter": "Huy Phan", "authors": "Huy Phan, Marco Maass, Lars Hertel, Radoslaw Mazur, Ian McLoughlin,\n  Alfred Mertins", "title": "Learning Compact Structural Representations for Audio Events Using\n  Regressor Banks", "comments": "To appear in Proceedings of IEEE ICASSP 2016", "journal-ref": null, "doi": "10.1109/ICASSP.2016.7471667", "report-no": null, "categories": "cs.SD cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new learned descriptor for audio signals which is efficient\nfor event representation. The entries of the descriptor are produced by\nevaluating a set of regressors on the input signal. The regressors are\nclass-specific and trained using the random regression forests framework. Given\nan input signal, each regressor estimates the onset and offset positions of the\ntarget event. The estimation confidence scores output by a regressor are then\nused to quantify how the target event aligns with the temporal structure of the\ncorresponding category. Our proposed descriptor has two advantages. First, it\nis compact, i.e. the dimensionality of the descriptor is equal to the number of\nevent classes. Second, we show that even simple linear classification models,\ntrained on our descriptor, yield better accuracies on audio event\nclassification task than not only the nonlinear baselines but also the\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 07:46:59 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Phan", "Huy", ""], ["Maass", "Marco", ""], ["Hertel", "Lars", ""], ["Mazur", "Radoslaw", ""], ["McLoughlin", "Ian", ""], ["Mertins", "Alfred", ""]]}, {"id": "1604.08772", "submitter": "Frederic Besse", "authors": "Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka\n  and Daan Wierstra", "title": "Towards Conceptual Compression", "comments": "14 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a simple recurrent variational auto-encoder architecture that\nsignificantly improves image modeling. The system represents the\nstate-of-the-art in latent variable models for both the ImageNet and Omniglot\ndatasets. We show that it naturally separates global conceptual information\nfrom lower level details, thus addressing one of the fundamentally desired\nproperties of unsupervised learning. Furthermore, the possibility of\nrestricting ourselves to storing only global information about an image allows\nus to achieve high quality 'conceptual compression'.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 11:02:52 GMT"}], "update_date": "2016-05-02", "authors_parsed": [["Gregor", "Karol", ""], ["Besse", "Frederic", ""], ["Rezende", "Danilo Jimenez", ""], ["Danihelka", "Ivo", ""], ["Wierstra", "Daan", ""]]}, {"id": "1604.08859", "submitter": "Alexandre de Br\\'ebisson", "authors": "Alexandre de Br\\'ebisson, Pascal Vincent", "title": "The Z-loss: a shift and scale invariant classification loss belonging to\n  the Spherical Family", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite being the standard loss function to train multi-class neural\nnetworks, the log-softmax has two potential limitations. First, it involves\ncomputations that scale linearly with the number of output classes, which can\nrestrict the size of problems we are able to tackle with current hardware.\nSecond, it remains unclear how close it matches the task loss such as the top-k\nerror rate or other non-differentiable evaluation metrics which we aim to\noptimize ultimately. In this paper, we introduce an alternative classification\nloss function, the Z-loss, which is designed to address these two issues.\nUnlike the log-softmax, it has the desirable property of belonging to the\nspherical loss family (Vincent et al., 2015), a class of loss functions for\nwhich training can be performed very efficiently with a complexity independent\nof the number of output classes. We show experimentally that it significantly\noutperforms the other spherical loss functions previously investigated.\nFurthermore, we show on a word language modeling task that it also outperforms\nthe log-softmax with respect to certain ranking scores, such as top-k scores,\nsuggesting that the Z-loss has the flexibility to better match the task loss.\nThese qualities thus makes the Z-loss an appealing candidate to train very\nefficiently large output networks such as word-language models or other extreme\nclassification problems. On the One Billion Word (Chelba et al., 2014) dataset,\nwe are able to train a model with the Z-loss 40 times faster than the\nlog-softmax and more than 4 times faster than the hierarchical softmax.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 14:53:00 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 15:17:34 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["de Br\u00e9bisson", "Alexandre", ""], ["Vincent", "Pascal", ""]]}, {"id": "1604.08880", "submitter": "Shane Halloran", "authors": "Nils Y. Hammerla, Shane Halloran and Thomas Ploetz", "title": "Deep, Convolutional, and Recurrent Models for Human Activity Recognition\n  using Wearables", "comments": "Extended version has been accepted for publication at International\n  Joint Conference on Artificial Intelligence (IJCAI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human activity recognition (HAR) in ubiquitous computing is beginning to\nadopt deep learning to substitute for well-established analysis techniques that\nrely on hand-crafted feature extraction and classification techniques. From\nthese isolated applications of custom deep architectures it is, however,\ndifficult to gain an overview of their suitability for problems ranging from\nthe recognition of manipulative gestures to the segmentation and identification\nof physical activities like running or ascending stairs. In this paper we\nrigorously explore deep, convolutional, and recurrent approaches across three\nrepresentative datasets that contain movement data captured with wearable\nsensors. We describe how to train recurrent approaches in this setting,\nintroduce a novel regularisation approach, and illustrate how they outperform\nthe state-of-the-art on a large benchmark dataset. Across thousands of\nrecognition experiments with randomly sampled model configurations we\ninvestigate the suitability of each model for different tasks in HAR, explore\nthe impact of hyperparameters using the fANOVA framework, and provide\nguidelines for the practitioner who wants to apply deep learning in their\nproblem setting.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 15:38:44 GMT"}], "update_date": "2016-05-02", "authors_parsed": [["Hammerla", "Nils Y.", ""], ["Halloran", "Shane", ""], ["Ploetz", "Thomas", ""]]}, {"id": "1604.08934", "submitter": "Sebastijan Dumancic", "authors": "Sebastijan Dumancic and Hendrik Blockeel", "title": "An expressive dissimilarity measure for relational clustering using\n  neighbourhood trees", "comments": "9 pages, 3 figures, 4 tables, submitted to ECMLPKDD 2017", "journal-ref": null, "doi": "10.1007/s10994-017-5644-6", "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is an underspecified task: there are no universal criteria for\nwhat makes a good clustering. This is especially true for relational data,\nwhere similarity can be based on the features of individuals, the relationships\nbetween them, or a mix of both. Existing methods for relational clustering have\nstrong and often implicit biases in this respect. In this paper, we introduce a\nnovel similarity measure for relational data. It is the first measure to\nincorporate a wide variety of types of similarity, including similarity of\nattributes, similarity of relational context, and proximity in a hypergraph. We\nexperimentally evaluate how using this similarity affects the quality of\nclustering on very different types of datasets. The experiments demonstrate\nthat (a) using this similarity in standard clustering methods consistently\ngives good results, whereas other measures work well only on datasets that\nmatch their bias; and (b) on most datasets, the novel similarity outperforms\neven the best among the existing ones.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 18:48:53 GMT"}, {"version": "v2", "created": "Tue, 7 Mar 2017 08:45:19 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Dumancic", "Sebastijan", ""], ["Blockeel", "Hendrik", ""]]}]