[{"id": "1412.0156", "submitter": "Francis Bach", "authors": "Alexandre D\\'efossez (LIENS, INRIA Paris - Rocquencourt), Francis Bach\n  (LIENS, INRIA Paris - Rocquencourt)", "title": "Constant Step Size Least-Mean-Square: Bias-Variance Trade-offs and\n  Optimal Sampling Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the least-squares regression problem and provide a detailed\nasymptotic analysis of the performance of averaged constant-step-size\nstochastic gradient descent (a.k.a. least-mean-squares). In the strongly-convex\ncase, we provide an asymptotic expansion up to explicit exponentially decaying\nterms. Our analysis leads to new insights into stochastic approximation\nalgorithms: (a) it gives a tighter bound on the allowed step-size; (b) the\ngeneralization error may be divided into a variance term which is decaying as\nO(1/n), independently of the step-size $\\gamma$, and a bias term that decays as\nO(1/$\\gamma$ 2 n 2); (c) when allowing non-uniform sampling, the choice of a\ngood sampling density depends on whether the variance or bias terms dominate.\nIn particular, when the variance term dominates, optimal sampling densities do\nnot lead to much gain, while when the bias term dominates, we can choose larger\nstep-sizes that leads to significant improvements.\n", "versions": [{"version": "v1", "created": "Sat, 29 Nov 2014 21:29:15 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["D\u00e9fossez", "Alexandre", "", "LIENS, INRIA Paris - Rocquencourt"], ["Bach", "Francis", "", "LIENS, INRIA Paris - Rocquencourt"]]}, {"id": "1412.0473", "submitter": "Isabell Franck", "authors": "Isabell M. Franck, P.S. Koutsourelakis", "title": "Sparse Variational Bayesian Approximations for Nonlinear Inverse\n  Problems: applications in nonlinear elastography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.NA physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an efficient Bayesian framework for solving nonlinear,\nhigh-dimensional model calibration problems. It is based on a Variational\nBayesian formulation that aims at approximating the exact posterior by means of\nsolving an optimization problem over an appropriately selected family of\ndistributions. The goal is two-fold. Firstly, to find lower-dimensional\nrepresentations of the unknown parameter vector that capture as much as\npossible of the associated posterior density, and secondly to enable the\ncomputation of the approximate posterior density with as few forward calls as\npossible. We discuss how these objectives can be achieved by using a fully\nBayesian argumentation and employing the marginal likelihood or evidence as the\nultimate model validation metric for any proposed dimensionality reduction. We\ndemonstrate the performance of the proposed methodology for problems in\nnonlinear elastography where the identification of the mechanical properties of\nbiological materials can inform non-invasive, medical diagnosis. An Importance\nSampling scheme is finally employed in order to validate the results and assess\nthe efficacy of the approximations provided.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 13:40:02 GMT"}, {"version": "v2", "created": "Tue, 2 Dec 2014 10:13:17 GMT"}, {"version": "v3", "created": "Wed, 17 Dec 2014 10:07:27 GMT"}, {"version": "v4", "created": "Fri, 30 Oct 2015 18:09:54 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Franck", "Isabell M.", ""], ["Koutsourelakis", "P. S.", ""]]}, {"id": "1412.0543", "submitter": "Panayotis Mertikopoulos", "authors": "Steven Perkins and Panayotis Mertikopoulos and David S. Leslie", "title": "Game-theoretical control with continuous action sets", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.GT cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the recent applications of game-theoretical learning techniques\nto the design of distributed control systems, we study a class of control\nproblems that can be formulated as potential games with continuous action sets,\nand we propose an actor-critic reinforcement learning algorithm that provably\nconverges to equilibrium in this class of problems. The method employed is to\nanalyse the learning process under study through a mean-field dynamical system\nthat evolves in an infinite-dimensional function space (the space of\nprobability distributions over the players' continuous controls). To do so, we\nextend the theory of finite-dimensional two-timescale stochastic approximation\nto an infinite-dimensional, Banach space setting, and we prove that the\ncontinuous dynamics of the process converge to equilibrium in the case of\npotential games. These results combine to give a provably-convergent learning\nalgorithm in which players do not need to keep track of the controls selected\nby the other agents.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 17:07:34 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Perkins", "Steven", ""], ["Mertikopoulos", "Panayotis", ""], ["Leslie", "David S.", ""]]}, {"id": "1412.0607", "submitter": "Cristian Rojas", "authors": "Cristian R. Rojas and Bo Wahlberg", "title": "How to monitor and mitigate stair-casing in l1 trend filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SY stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the estimation of changing trends in time-series using\n$\\ell_1$ trend filtering. This method generalizes 1D Total Variation (TV)\ndenoising for detection of step changes in means to detecting changes in\ntrends, and it relies on a convex optimization problem for which there are very\nefficient numerical algorithms. It is known that TV denoising suffers from the\nso-called stair-case effect, which leads to detecting false change points. The\nobjective of this paper is to show that $\\ell_1$ trend filtering also suffers\nfrom a certain stair-case problem. The analysis is based on an interpretation\nof the dual variables of the optimization problem in the method as integrated\nrandom walk. We discuss consistency conditions for $\\ell_1$ trend filtering,\nhow to monitor their fulfillment, and how to modify the algorithm to avoid the\nstair-case false detection problem.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 19:31:51 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Rojas", "Cristian R.", ""], ["Wahlberg", "Bo", ""]]}, {"id": "1412.0614", "submitter": "Francesco Renna", "authors": "Francesco Renna, Liming Wang, Xin Yuan, Jianbo Yang, Galen Reeves,\n  Robert Calderbank, Lawrence Carin, Miguel R. D. Rodrigues", "title": "Classification and Reconstruction of High-Dimensional Signals from\n  Low-Dimensional Features in the Presence of Side Information", "comments": "62 pages, 11 figures, submitted to IEEE Transactions on Information\n  Theory. The abstract of the paper is not reported entirely in the metadata\n  due to length limitations", "journal-ref": null, "doi": "10.1109/TIT.2016.2606646", "report-no": null, "categories": "cs.IT cs.CV math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper offers a characterization of fundamental limits on the\nclassification and reconstruction of high-dimensional signals from\nlow-dimensional features, in the presence of side information. We consider a\nscenario where a decoder has access both to linear features of the signal of\ninterest and to linear features of the side information signal; while the side\ninformation may be in a compressed form, the objective is recovery or\nclassification of the primary signal, not the side information. The signal of\ninterest and the side information are each assumed to have (distinct) latent\ndiscrete labels; conditioned on these two labels, the signal of interest and\nside information are drawn from a multivariate Gaussian distribution. With\njoint probabilities on the latent labels, the overall signal-(side information)\nrepresentation is defined by a Gaussian mixture model. We then provide sharp\nsufficient and/or necessary conditions for these quantities to approach zero\nwhen the covariance matrices of the Gaussians are nearly low-rank. These\nconditions, which are reminiscent of the well-known Slepian-Wolf and Wyner-Ziv\nconditions, are a function of the number of linear features extracted from the\nsignal of interest, the number of linear features extracted from the side\ninformation signal, and the geometry of these signals and their interplay.\nMoreover, on assuming that the signal of interest and the side information obey\nsuch an approximately low-rank model, we derive expansions of the\nreconstruction error as a function of the deviation from an exactly low-rank\nmodel; such expansions also allow identification of operational regimes where\nthe impact of side information on signal reconstruction is most relevant. Our\nframework, which offers a principled mechanism to integrate side information in\nhigh-dimensional data problems, is also tested in the context of imaging\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 19:53:25 GMT"}, {"version": "v2", "created": "Thu, 17 Mar 2016 13:35:11 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Renna", "Francesco", ""], ["Wang", "Liming", ""], ["Yuan", "Xin", ""], ["Yang", "Jianbo", ""], ["Reeves", "Galen", ""], ["Calderbank", "Robert", ""], ["Carin", "Lawrence", ""], ["Rodrigues", "Miguel R. D.", ""]]}, {"id": "1412.0694", "submitter": "Alexander Tank", "authors": "Alex Tank, Nicholas J. Foti, Emily B. Fox", "title": "Streaming Variational Inference for Bayesian Nonparametric Mixture\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In theory, Bayesian nonparametric (BNP) models are well suited to streaming\ndata scenarios due to their ability to adapt model complexity with the observed\ndata. Unfortunately, such benefits have not been fully realized in practice;\nexisting inference algorithms are either not applicable to streaming\napplications or not extensible to BNP models. For the special case of Dirichlet\nprocesses, streaming inference has been considered. However, there is growing\ninterest in more flexible BNP models building on the class of normalized random\nmeasures (NRMs). We work within this general framework and present a streaming\nvariational inference algorithm for NRM mixture models. Our algorithm is based\non assumed density filtering (ADF), leading straightforwardly to expectation\npropagation (EP) for large-scale batch inference as well. We demonstrate the\nefficacy of the algorithm on clustering documents in large, streaming text\ncorpora.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 21:26:33 GMT"}, {"version": "v2", "created": "Tue, 10 Feb 2015 05:27:39 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2015 05:36:13 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Tank", "Alex", ""], ["Foti", "Nicholas J.", ""], ["Fox", "Emily B.", ""]]}, {"id": "1412.0744", "submitter": "Artemy Kolchinsky", "authors": "Artemy Kolchinsky, An\\'alia Louren\\c{c}o, Heng-Yi Wu, Lang Li, Luis M.\n  Rocha", "title": "Extraction of Pharmacokinetic Evidence of Drug-drug Interactions from\n  the Literature", "comments": "PLOS One (2015)", "journal-ref": null, "doi": "10.1371/journal.pone.0122199", "report-no": null, "categories": "stat.ML cs.IR q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drug-drug interaction (DDI) is a major cause of morbidity and mortality and a\nsubject of intense scientific interest. Biomedical literature mining can aid\nDDI research by extracting evidence for large numbers of potential interactions\nfrom published literature and clinical databases. Though DDI is investigated in\ndomains ranging in scale from intracellular biochemistry to human populations,\nliterature mining has not been used to extract specific types of experimental\nevidence, which are reported differently for distinct experimental goals. We\nfocus on pharmacokinetic evidence for DDI, essential for identifying causal\nmechanisms of putative interactions and as input for further pharmacological\nand pharmaco-epidemiology investigations. We used manually curated corpora of\nPubMed abstracts and annotated sentences to evaluate the efficacy of literature\nmining on two tasks: first, identifying PubMed abstracts containing\npharmacokinetic evidence of DDIs; second, extracting sentences containing such\nevidence from abstracts. We implemented a text mining pipeline and evaluated it\nusing several linear classifiers and a variety of feature transforms. The most\nimportant textual features in the abstract and sentence classification tasks\nwere analyzed. We also investigated the performance benefits of using features\nderived from PubMed metadata fields, various publicly available named entity\nrecognizers, and pharmacokinetic dictionaries. Several classifiers performed\nvery well in distinguishing relevant and irrelevant abstracts (reaching\nF1~=0.93, MCC~=0.74, iAUC~=0.99) and sentences (F1~=0.76, MCC~=0.65,\niAUC~=0.83). We found that word bigram features were important for achieving\noptimal classifier performance and that features derived from Medical Subject\nHeadings (MeSH) terms significantly improved abstract classification. ...\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 00:01:39 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 16:45:42 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Kolchinsky", "Artemy", ""], ["Louren\u00e7o", "An\u00e1lia", ""], ["Wu", "Heng-Yi", ""], ["Li", "Lang", ""], ["Rocha", "Luis M.", ""]]}, {"id": "1412.1058", "submitter": "Rie Johnson", "authors": "Rie Johnson and Tong Zhang", "title": "Effective Use of Word Order for Text Categorization with Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN) is a neural network that can make use of\nthe internal structure of data such as the 2D structure of image data. This\npaper studies CNN on text categorization to exploit the 1D structure (namely,\nword order) of text data for accurate prediction. Instead of using\nlow-dimensional word vectors as input as is often done, we directly apply CNN\nto high-dimensional text data, which leads to directly learning embedding of\nsmall text regions for use in classification. In addition to a straightforward\nadaptation of CNN from image to text, a simple but new variation which employs\nbag-of-word conversion in the convolution layer is proposed. An extension to\ncombine multiple convolution layers is also explored for higher accuracy. The\nexperiments demonstrate the effectiveness of our approach in comparison with\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 16:19:51 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2015 12:59:35 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Johnson", "Rie", ""], ["Zhang", "Tong", ""]]}, {"id": "1412.1074", "submitter": "Alexandre Drouin", "authors": "Alexandre Drouin, S\\'ebastien Gigu\\`ere, Vladana Sagatovich, Maxime\n  D\\'eraspe, Fran\\c{c}ois Laviolette, Mario Marchand, Jacques Corbeil", "title": "Learning interpretable models of phenotypes from whole genome sequences\n  with the Set Covering Machine", "comments": "Presented at Machine Learning in Computational Biology 2014,\n  Montr\\'eal, Qu\\'ebec, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.CE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increased affordability of whole genome sequencing has motivated its use\nfor phenotypic studies. We address the problem of learning interpretable models\nfor discrete phenotypes from whole genomes. We propose a general approach that\nrelies on the Set Covering Machine and a k-mer representation of the genomes.\nWe show results for the problem of predicting the resistance of Pseudomonas\nAeruginosa, an important human pathogen, against 4 antibiotics. Our results\ndemonstrate that extremely sparse models which are biologically relevant can be\nlearnt using this approach.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 13:26:50 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Drouin", "Alexandre", ""], ["Gigu\u00e8re", "S\u00e9bastien", ""], ["Sagatovich", "Vladana", ""], ["D\u00e9raspe", "Maxime", ""], ["Laviolette", "Fran\u00e7ois", ""], ["Marchand", "Mario", ""], ["Corbeil", "Jacques", ""]]}, {"id": "1412.1193", "submitter": "James Martens", "authors": "James Martens", "title": "New insights and perspectives on the natural gradient method", "comments": "Minor corrections from previous version and fixed typos. Official\n  JMLR version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural gradient descent is an optimization method traditionally motivated\nfrom the perspective of information geometry, and works well for many\napplications as an alternative to stochastic gradient descent. In this paper we\ncritically analyze this method and its properties, and show how it can be\nviewed as a type of 2nd-order optimization method, with the Fisher information\nmatrix acting as a substitute for the Hessian. In many important cases, the\nFisher information matrix is shown to be equivalent to the Generalized\nGauss-Newton matrix, which both approximates the Hessian, but also has certain\nproperties that favor its use over the Hessian. This perspective turns out to\nhave significant implications for the design of a practical and robust natural\ngradient optimizer, as it motivates the use of techniques like trust regions\nand Tikhonov regularization. Additionally, we make a series of contributions to\nthe understanding of natural gradient and 2nd-order methods, including: a\nthorough analysis of the convergence speed of stochastic natural gradient\ndescent (and more general stochastic 2nd-order methods) as applied to convex\nquadratics, a critical examination of the oft-used \"empirical\" approximation of\nthe Fisher matrix, and an analysis of the (approximate) parameterization\ninvariance property possessed by natural gradient methods (which we show also\nholds for certain other curvature, but notably not the Hessian).\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 05:21:13 GMT"}, {"version": "v10", "created": "Sun, 7 Jun 2020 22:48:03 GMT"}, {"version": "v11", "created": "Sat, 19 Sep 2020 15:16:47 GMT"}, {"version": "v2", "created": "Sat, 13 Dec 2014 02:31:33 GMT"}, {"version": "v3", "created": "Wed, 11 Feb 2015 00:30:02 GMT"}, {"version": "v4", "created": "Wed, 8 Apr 2015 08:52:47 GMT"}, {"version": "v5", "created": "Thu, 1 Oct 2015 00:54:03 GMT"}, {"version": "v6", "created": "Tue, 3 May 2016 23:43:13 GMT"}, {"version": "v7", "created": "Mon, 30 May 2016 21:09:07 GMT"}, {"version": "v8", "created": "Mon, 13 Mar 2017 13:27:59 GMT"}, {"version": "v9", "created": "Tue, 21 Nov 2017 12:15:01 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Martens", "James", ""]]}, {"id": "1412.1271", "submitter": "Xiao-Lei Zhang", "authors": "Xiao-Lei Zhang", "title": "Deep Distributed Random Samplings for Supervised Learning: An\n  Alternative to Random Forests?", "comments": "This paper has been withdrawn by the author. The idea is wrong and is\n  no longer to be posed on site. The paper will no longer be updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In (\\cite{zhang2014nonlinear,zhang2014nonlinear2}), we have viewed machine\nlearning as a coding and dimensionality reduction problem, and further proposed\na simple unsupervised dimensionality reduction method, entitled deep\ndistributed random samplings (DDRS). In this paper, we further extend it to\nsupervised learning incrementally. The key idea here is to incorporate label\ninformation into the coding process by reformulating that each center in DDRS\nhas multiple output units indicating which class the center belongs to. The\nsupervised learning method seems somewhat similar with random forests\n(\\cite{breiman2001random}), here we emphasize their differences as follows. (i)\nEach layer of our method considers the relationship between part of the data\npoints in training data with all training data points, while random forests\nfocus on building each decision tree on only part of training data points\nindependently. (ii) Our method builds gradually-narrowed network by sampling\nless and less data points, while random forests builds gradually-narrowed\nnetwork by merging subclasses. (iii) Our method is trained more straightforward\nfrom bottom layer to top layer, while random forests build each tree from top\nlayer to bottom layer by splitting. (iv) Our method encodes output targets\nimplicitly in sparse codes, while random forests encode output targets by\nremembering the class attributes of the activated nodes. Therefore, our method\nis a simpler, more straightforward, and maybe a better alternative choice,\nthough both methods use two very basic elements---randomization and nearest\nneighbor optimization---as the core. This preprint is used to protect the\nincremental idea from (\\cite{zhang2014nonlinear,zhang2014nonlinear2}). Full\nempirical evaluation will be announced carefully later.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 10:57:35 GMT"}, {"version": "v2", "created": "Wed, 28 Jan 2015 19:23:17 GMT"}], "update_date": "2015-01-29", "authors_parsed": [["Zhang", "Xiao-Lei", ""]]}, {"id": "1412.1353", "submitter": "Anastasia Pentina", "authors": "Anastasia Pentina and Viktoriia Sharmanska and Christoph H. Lampert", "title": "Curriculum Learning of Multiple Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sharing information between multiple tasks enables algorithms to achieve good\ngeneralization performance even from small amounts of training data. However,\nin a realistic scenario of multi-task learning not all tasks are equally\nrelated to each other, hence it could be advantageous to transfer information\nonly between the most related tasks. In this work we propose an approach that\nprocesses multiple tasks in a sequence with sharing between subsequent tasks\ninstead of solving all tasks jointly. Subsequently, we address the question of\ncurriculum learning of tasks, i.e. finding the best order of tasks to be\nlearned. Our approach is based on a generalization bound criterion for choosing\nthe task order that optimizes the average expected classification performance\nover all tasks. Our experimental results show that learning multiple related\ntasks sequentially can be more effective than learning them jointly, the order\nin which tasks are being solved affects the overall performance, and that our\nmodel is able to automatically discover the favourable order of tasks.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 15:02:51 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Pentina", "Anastasia", ""], ["Sharmanska", "Viktoriia", ""], ["Lampert", "Christoph H.", ""]]}, {"id": "1412.1370", "submitter": "James Hensman", "authors": "James Hensman, Neil D. Lawrence", "title": "Nested Variational Compression in Deep Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Gaussian processes provide a flexible approach to probabilistic\nmodelling of data using either supervised or unsupervised learning. For\ntractable inference approximations to the marginal likelihood of the model must\nbe made. The original approach to approximate inference in these models used\nvariational compression to allow for approximate variational marginalization of\nthe hidden variables leading to a lower bound on the marginal likelihood of the\nmodel [Damianou and Lawrence, 2013]. In this paper we extend this idea with a\nnested variational compression. The resulting lower bound on the likelihood can\nbe easily parallelized or adapted for stochastic variational inference.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 15:39:55 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Hensman", "James", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1412.1443", "submitter": "Guy Bresler", "authors": "Guy Bresler, David Gamarnik, and Devavrat Shah", "title": "Structure learning of antiferromagnetic Ising models", "comments": "15 pages. NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the computational complexity of learning the\ngraph structure underlying a discrete undirected graphical model from i.i.d.\nsamples. We first observe that the notoriously difficult problem of learning\nparities with noise can be captured as a special case of learning graphical\nmodels. This leads to an unconditional computational lower bound of $\\Omega\n(p^{d/2})$ for learning general graphical models on $p$ nodes of maximum degree\n$d$, for the class of so-called statistical algorithms recently introduced by\nFeldman et al (2013). The lower bound suggests that the $O(p^d)$ runtime\nrequired to exhaustively search over neighborhoods cannot be significantly\nimproved without restricting the class of models.\n  Aside from structural assumptions on the graph such as it being a tree,\nhypertree, tree-like, etc., many recent papers on structure learning assume\nthat the model has the correlation decay property. Indeed, focusing on\nferromagnetic Ising models, Bento and Montanari (2009) showed that all known\nlow-complexity algorithms fail to learn simple graphs when the interaction\nstrength exceeds a number related to the correlation decay threshold. Our\nsecond set of results gives a class of repelling (antiferromagnetic) models\nthat have the opposite behavior: very strong interaction allows efficient\nlearning in time $O(p^2)$. We provide an algorithm whose performance\ninterpolates between $O(p^2)$ and $O(p^{d+2})$ depending on the strength of the\nrepulsion.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 19:08:55 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Bresler", "Guy", ""], ["Gamarnik", "David", ""], ["Shah", "Devavrat", ""]]}, {"id": "1412.1559", "submitter": "Qing Zhou", "authors": "Yuliya Marchetti and Qing Zhou", "title": "Iterative Subsampling in Solution Path Clustering of Noisy Big Data", "comments": "17 pages, 7 figures", "journal-ref": "Statistics and Its Interface, 9: 415-431 (2016)", "doi": "10.4310/SII.2016.v9.n4.a2", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an iterative subsampling approach to improve the computational\nefficiency of our previous work on solution path clustering (SPC). The SPC\nmethod achieves clustering by concave regularization on the pairwise distances\nbetween cluster centers. This clustering method has the important capability to\nrecognize noise and to provide a short path of clustering solutions; however,\nit is not sufficiently fast for big datasets. Thus, we propose a method that\niterates between clustering a small subsample of the full data and sequentially\nassigning the other data points to attain orders of magnitude of computational\nsavings. The new method preserves the ability to isolate noise, includes a\nsolution selection mechanism that ultimately provides one clustering solution\nwith an estimated number of clusters, and is shown to be able to extract small\ntight clusters from noisy data. The method's relatively minor losses in\naccuracy are demonstrated through simulation studies, and its ability to handle\nlarge datasets is illustrated through applications to gene expression datasets.\nAn R package, SPClustering, for the SPC method with iterative subsampling is\navailable at http://www.stat.ucla.edu/~zhou/Software.html.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 06:05:59 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2015 19:09:58 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Marchetti", "Yuliya", ""], ["Zhou", "Qing", ""]]}, {"id": "1412.1576", "submitter": "Xun Zheng", "authors": "Jinhui Yuan, Fei Gao, Qirong Ho, Wei Dai, Jinliang Wei, Xun Zheng,\n  Eric P. Xing, Tie-Yan Liu, Wei-Ying Ma", "title": "LightLDA: Big Topic Models on Modest Compute Clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When building large-scale machine learning (ML) programs, such as big topic\nmodels or deep neural nets, one usually assumes such tasks can only be\nattempted with industrial-sized clusters with thousands of nodes, which are out\nof reach for most practitioners or academic researchers. We consider this\nchallenge in the context of topic modeling on web-scale corpora, and show that\nwith a modest cluster of as few as 8 machines, we can train a topic model with\n1 million topics and a 1-million-word vocabulary (for a total of 1 trillion\nparameters), on a document collection with 200 billion tokens -- a scale not\nyet reported even with thousands of machines. Our major contributions include:\n1) a new, highly efficient O(1) Metropolis-Hastings sampling algorithm, whose\nrunning cost is (surprisingly) agnostic of model size, and empirically\nconverges nearly an order of magnitude faster than current state-of-the-art\nGibbs samplers; 2) a structure-aware model-parallel scheme, which leverages\ndependencies within the topic model, yielding a sampling strategy that is\nfrugal on machine memory and network communication; 3) a differential\ndata-structure for model storage, which uses separate data structures for high-\nand low-frequency words to allow extremely large models to fit in memory, while\nmaintaining high inference speed; and 4) a bounded asynchronous data-parallel\nscheme, which allows efficient distributed processing of massive data via a\nparameter server. Our distribution strategy is an instance of the\nmodel-and-data-parallel programming model underlying the Petuum framework for\ngeneral distributed ML, and was implemented on top of the Petuum open-source\nsystem. We provide experimental evidence showing how this development puts\nmassive models within reach on a small cluster while still enjoying\nproportional time cost reductions with increasing cluster size, in comparison\nwith alternative options.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 07:49:12 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Yuan", "Jinhui", ""], ["Gao", "Fei", ""], ["Ho", "Qirong", ""], ["Dai", "Wei", ""], ["Wei", "Jinliang", ""], ["Zheng", "Xun", ""], ["Xing", "Eric P.", ""], ["Liu", "Tie-Yan", ""], ["Ma", "Wei-Ying", ""]]}, {"id": "1412.1602", "submitter": "Jan Chorowski", "authors": "Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio", "title": "End-to-end Continuous Speech Recognition using Attention-based Recurrent\n  NN: First Results", "comments": "As accepted to: Deep Learning and Representation Learning Workshop,\n  NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We replace the Hidden Markov Model (HMM) which is traditionally used in in\ncontinuous speech recognition with a bi-directional recurrent neural network\nencoder coupled to a recurrent neural network decoder that directly emits a\nstream of phonemes. The alignment between the input and output sequences is\nestablished using an attention mechanism: the decoder emits each symbol based\non a context created with a subset of input symbols elected by the attention\nmechanism. We report initial results demonstrating that this new approach\nachieves phoneme error rates that are comparable to the state-of-the-art\nHMM-based decoders, on the TIMIT dataset.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 10:00:19 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Chorowski", "Jan", ""], ["Bahdanau", "Dzmitry", ""], ["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1412.1684", "submitter": "Yang Feng", "authors": "Diego Franco Saldana, Yi Yu, and Yang Feng", "title": "How Many Communities Are There?", "comments": "26 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic blockmodels and variants thereof are among the most widely used\napproaches to community detection for social networks and relational data. A\nstochastic blockmodel partitions the nodes of a network into disjoint sets,\ncalled communities. The approach is inherently related to clustering with\nmixture models; and raises a similar model selection problem for the number of\ncommunities. The Bayesian information criterion (BIC) is a popular solution,\nhowever, for stochastic blockmodels, the conditional independence assumption\ngiven the communities of the endpoints among different edges is usually\nviolated in practice. In this regard, we propose composite likelihood BIC\n(CL-BIC) to select the number of communities, and we show it is robust against\npossible misspecifications in the underlying stochastic blockmodel assumptions.\nWe derive the requisite methodology and illustrate the approach using both\nsimulated and real data. Supplementary materials containing the relevant\ncomputer code are available online.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 14:47:47 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2015 18:56:30 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Saldana", "Diego Franco", ""], ["Yu", "Yi", ""], ["Feng", "Yang", ""]]}, {"id": "1412.1716", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen, Christopher R. Genovese, Ryan J. Tibshirani, Larry\n  Wasserman", "title": "Nonparametric modal regression", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1373 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2016, Vol. 44, No. 2, 489-514", "doi": "10.1214/15-AOS1373", "report-no": "IMS-AOS-AOS1373", "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modal regression estimates the local modes of the distribution of $Y$ given\n$X=x$, instead of the mean, as in the usual regression sense, and can hence\nreveal important structure missed by usual regression methods. We study a\nsimple nonparametric method for modal regression, based on a kernel density\nestimate (KDE) of the joint distribution of $Y$ and $X$. We derive asymptotic\nerror bounds for this method, and propose techniques for constructing\nconfidence sets and prediction sets. The latter is used to select the smoothing\nbandwidth of the underlying KDE. The idea behind modal regression is connected\nto many others, such as mixture regression and density ridge estimation, and we\ndiscuss these ties as well.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 16:18:42 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2015 00:50:10 GMT"}, {"version": "v3", "created": "Wed, 30 Mar 2016 11:54:46 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Chen", "Yen-Chi", ""], ["Genovese", "Christopher R.", ""], ["Tibshirani", "Ryan J.", ""], ["Wasserman", "Larry", ""]]}, {"id": "1412.1740", "submitter": "Matthew Kusner", "authors": "Matt J. Kusner, Nicholas I. Kolkin, Stephen Tyree, Kilian Q.\n  Weinberger", "title": "Image Data Compression for Covariance and Histogram Descriptors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covariance and histogram image descriptors provide an effective way to\ncapture information about images. Both excel when used in combination with\nspecial purpose distance metrics. For covariance descriptors these metrics\nmeasure the distance along the non-Euclidean Riemannian manifold of symmetric\npositive definite matrices. For histogram descriptors the Earth Mover's\ndistance measures the optimal transport between two histograms. Although more\nprecise, these distance metrics are very expensive to compute, making them\nimpractical in many applications, even for data sets of only a few thousand\nexamples. In this paper we present two methods to compress the size of\ncovariance and histogram datasets with only marginal increases in test error\nfor k-nearest neighbor classification. Specifically, we show that we can reduce\ndata sets to 16% and in some cases as little as 2% of their original size,\nwhile approximately matching the test error of kNN classification on the full\ntraining set. In fact, because the compressed set is learned in a supervised\nfashion, it sometimes even outperforms the full data set, while requiring only\na fraction of the space and drastically reducing test-time computation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 17:22:22 GMT"}, {"version": "v2", "created": "Sat, 23 May 2015 17:07:59 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Kusner", "Matt J.", ""], ["Kolkin", "Nicholas I.", ""], ["Tyree", "Stephen", ""], ["Weinberger", "Kilian Q.", ""]]}, {"id": "1412.1927", "submitter": "Sylvain Sardy", "authors": "Jairo Diaz-Rodriguez and Sylvain Sardy", "title": "Quantile universal threshold: model selection at the detection edge for\n  high-dimensional linear regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To estimate a sparse linear model from data with Gaussian noise, consilience\nfrom lasso and compressed sensing literatures is that thresholding estimators\nlike lasso and the Dantzig selector have the ability in some situations to\nidentify with high probability part of the significant covariates\nasymptotically, and are numerically tractable thanks to convexity.\n  Yet, the selection of a threshold parameter $\\lambda$ remains crucial in\npractice. To that aim we propose Quantile Universal Thresholding, a selection\nof $\\lambda$ at the detection edge. We show with extensive simulations and real\ndata that an excellent compromise between high true positive rate and low false\ndiscovery rate is achieved, leading also to good predictive risk.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 09:18:31 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Diaz-Rodriguez", "Jairo", ""], ["Sardy", "Sylvain", ""]]}, {"id": "1412.2041", "submitter": "Daniel Bartz", "authors": "Daniel Bartz and Johannes H\\\"ohne and Klaus-Robert M\\\"uller", "title": "Multi-Target Shrinkage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stein showed that the multivariate sample mean is outperformed by \"shrinking\"\nto a constant target vector. Ledoit and Wolf extended this approach to the\nsample covariance matrix and proposed a multiple of the identity as shrinkage\ntarget. In a general framework, independent of a specific estimator, we extend\nthe shrinkage concept by allowing simultaneous shrinkage to a set of targets.\nApplication scenarios include settings with (A) additional data sets from\npotentially similar distributions, (B) non-stationarity, (C) a natural grouping\nof the data or (D) multiple alternative estimators which could serve as\ntargets.\n  We show that this Multi-Target Shrinkage can be translated into a quadratic\nprogram and derive conditions under which the estimation of the shrinkage\nintensities yields optimal expected squared error in the limit. For the sample\nmean and the sample covariance as specific instances, we derive conditions\nunder which the optimality of MTS is applicable. We consider two asymptotic\nsettings: the large dimensional limit (LDL), where the dimensionality and the\nnumber of observations go to infinity at the same rate, and the finite\nobservations large dimensional limit (FOLDL), where only the dimensionality\ngoes to infinity while the number of observations remains constant. We then\nshow the effectiveness in extensive simulations and on real world data.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 16:02:50 GMT"}], "update_date": "2014-12-08", "authors_parsed": [["Bartz", "Daniel", ""], ["H\u00f6hne", "Johannes", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1412.2113", "submitter": "Suriya Gunasekar", "authors": "Suriya Gunasekar, Makoto Yamada, Dawei Yin, Yi Chang", "title": "Consistent Collective Matrix Completion under Joint Low Rank Structure", "comments": "19 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the collective matrix completion problem of jointly recovering a\ncollection of matrices with shared structure from partial (and potentially\nnoisy) observations. To ensure well--posedness of the problem, we impose a\njoint low rank structure, wherein each component matrix is low rank and the\nlatent space of the low rank factors corresponding to each entity is shared\nacross the entire collection. We first develop a rigorous algebra for\nrepresenting and manipulating collective--matrix structure, and identify\nsufficient conditions for consistent estimation of collective matrices. We then\npropose a tractable convex estimator for solving the collective matrix\ncompletion problem, and provide the first non--trivial theoretical guarantees\nfor consistency of collective matrix completion. We show that under reasonable\nassumptions stated in Section 3.1, with high probability, the proposed\nestimator exactly recovers the true matrices whenever sample complexity\nrequirements dictated by Theorem 1 are met. The sample complexity requirement\nderived in the paper are optimum up to logarithmic factors, and significantly\nimprove upon the requirements obtained by trivial extensions of standard matrix\ncompletion. Finally, we propose a scalable approximate algorithm to solve the\nproposed convex program, and corroborate our results through simulated\nexperiments.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 19:42:22 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2015 04:42:13 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2015 21:18:44 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Gunasekar", "Suriya", ""], ["Yamada", "Makoto", ""], ["Yin", "Dawei", ""], ["Chang", "Yi", ""]]}, {"id": "1412.2129", "submitter": "Diana Cai", "authors": "Diana Cai and Nathanael Ackerman and Cameron Freer", "title": "An iterative step-function estimator for graphons", "comments": "27 pages, 8 figures. Updated and expanded throughout", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exchangeable graphs arise via a sampling procedure from measurable functions\nknown as graphons. A natural estimation problem is how well we can recover a\ngraphon given a single graph sampled from it. One general framework for\nestimating a graphon uses step-functions obtained by partitioning the nodes of\nthe graph according to some clustering algorithm. We propose an iterative\nstep-function estimator (ISFE) that, given an initial partition, iteratively\nclusters nodes based on their edge densities with respect to the previous\niteration's partition. We analyze ISFE and demonstrate its performance in\ncomparison with other graphon estimation techniques.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 20:59:21 GMT"}, {"version": "v2", "created": "Tue, 12 May 2015 00:18:50 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Cai", "Diana", ""], ["Ackerman", "Nathanael", ""], ["Freer", "Cameron", ""]]}, {"id": "1412.2295", "submitter": "Han Liu", "authors": "Yang Ning and Tianqi Zhao and Han Liu", "title": "A Likelihood Ratio Framework for High Dimensional Semiparametric\n  Regression", "comments": "51 pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a likelihood ratio based inferential framework for high\ndimensional semiparametric generalized linear models. This framework addresses\na variety of challenging problems in high dimensional data analysis, including\nincomplete data, selection bias, and heterogeneous multitask learning. Our work\nhas three main contributions. (i) We develop a regularized statistical\nchromatography approach to infer the parameter of interest under the proposed\nsemiparametric generalized linear model without the need of estimating the\nunknown base measure function. (ii) We propose a new framework to construct\npost-regularization confidence regions and tests for the low dimensional\ncomponents of high dimensional parameters. Unlike existing post-regularization\ninferential methods, our approach is based on a novel directional likelihood.\nIn particular, the framework naturally handles generic regularized estimators\nwith nonconvex penalty functions and it can be used to infer least false\nparameters under misspecified models. (iii) We develop new concentration\ninequalities and normal approximation results for U-statistics with unbounded\nkernels, which are of independent interest. We demonstrate the consequences of\nthe general theory by using an example of missing data problem. Extensive\nsimulation studies and real data analysis are provided to illustrate our\nproposed approach.\n", "versions": [{"version": "v1", "created": "Sat, 6 Dec 2014 22:52:52 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 04:35:44 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Ning", "Yang", ""], ["Zhao", "Tianqi", ""], ["Liu", "Han", ""]]}, {"id": "1412.2309", "submitter": "Krzysztof Chalupka", "authors": "Krzysztof Chalupka and Pietro Perona and Frederick Eberhardt", "title": "Visual Causal Feature Learning", "comments": "Accepted at UAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a rigorous definition of the visual cause of a behavior that is\nbroadly applicable to the visually driven behavior in humans, animals, neurons,\nrobots and other perceiving systems. Our framework generalizes standard\naccounts of causal learning to settings in which the causal variables need to\nbe constructed from micro-variables. We prove the Causal Coarsening Theorem,\nwhich allows us to gain causal knowledge from observational data with minimal\nexperimental effort. The theorem provides a connection to standard inference\ntechniques in machine learning that identify features of an image that\ncorrelate with, but may not cause, the target behavior. Finally, we propose an\nactive learning scheme to learn a manipulator function that performs optimal\nmanipulations on the image to automatically identify the visual cause of a\ntarget behavior. We illustrate our inference and learning algorithms in\nexperiments based on both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Sun, 7 Dec 2014 03:13:27 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2015 22:35:30 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Chalupka", "Krzysztof", ""], ["Perona", "Pietro", ""], ["Eberhardt", "Frederick", ""]]}, {"id": "1412.2316", "submitter": "Mehdi Korki", "authors": "Mehdi Korki, Jingxin Zhang, Cishen Zhang, and Hadi Zayyani", "title": "Iterative Bayesian Reconstruction of Non-IID Block-Sparse Signals", "comments": "13 pages, 7 figures, Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel Block Iterative Bayesian Algorithm (Block-IBA)\nfor reconstructing block-sparse signals with unknown block structures. Unlike\nthe existing algorithms for block sparse signal recovery which assume the\ncluster structure of the nonzero elements of the unknown signal to be\nindependent and identically distributed (i.i.d.), we use a more realistic\nBernoulli-Gaussian hidden Markov model (BGHMM) to characterize the non-i.i.d.\nblock-sparse signals commonly encountered in practice. The Block-IBA\niteratively estimates the amplitudes and positions of the block-sparse signal\nusing the steepest-ascent based Expectation-Maximization (EM), and optimally\nselects the nonzero elements of the block-sparse signal by adaptive\nthresholding. The global convergence of Block-IBA is analyzed and proved, and\nthe effectiveness of Block-IBA is demonstrated by numerical experiments and\nsimulations on synthetic and real-life data.\n", "versions": [{"version": "v1", "created": "Sun, 7 Dec 2014 04:44:43 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Korki", "Mehdi", ""], ["Zhang", "Jingxin", ""], ["Zhang", "Cishen", ""], ["Zayyani", "Hadi", ""]]}, {"id": "1412.2404", "submitter": "Devansh Arpit", "authors": "Devansh Arpit, Ifeoma Nwogu, Venu Govindaraju", "title": "Dimensionality Reduction with Subspace Structure Preservation", "comments": "Published in NIPS 2014; v2: minor updates to the algorithm and added\n  a few lines addressing application to large-scale/high-dimensional data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling data as being sampled from a union of independent subspaces has been\nwidely applied to a number of real world applications. However, dimensionality\nreduction approaches that theoretically preserve this independence assumption\nhave not been well studied. Our key contribution is to show that $2K$\nprojection vectors are sufficient for the independence preservation of any $K$\nclass data sampled from a union of independent subspaces. It is this\nnon-trivial observation that we use for designing our dimensionality reduction\ntechnique. In this paper, we propose a novel dimensionality reduction algorithm\nthat theoretically preserves this structure for a given dataset. We support our\ntheoretical analysis with empirical results on both synthetic and real world\ndata achieving \\textit{state-of-the-art} results compared to popular\ndimensionality reduction techniques.\n", "versions": [{"version": "v1", "created": "Sun, 7 Dec 2014 22:02:33 GMT"}, {"version": "v2", "created": "Sun, 31 May 2015 22:30:47 GMT"}, {"version": "v3", "created": "Wed, 6 Apr 2016 23:11:46 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Arpit", "Devansh", ""], ["Nwogu", "Ifeoma", ""], ["Govindaraju", "Venu", ""]]}, {"id": "1412.2432", "submitter": "Edward Meeds", "authors": "Edward Meeds and Remco Hendriks and Said Al Faraby and Magiel Bruntink\n  and Max Welling", "title": "MLitB: Machine Learning in the Browser", "comments": "Revised for PeerJ Computer Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With few exceptions, the field of Machine Learning (ML) research has largely\nignored the browser as a computational engine. Beyond an educational resource\nfor ML, the browser has vast potential to not only improve the state-of-the-art\nin ML research, but also, inexpensively and on a massive scale, to bring\nsophisticated ML learning and prediction to the public at large. This paper\nintroduces MLitB, a prototype ML framework written entirely in JavaScript,\ncapable of performing large-scale distributed computing with heterogeneous\nclasses of devices. The development of MLitB has been driven by several\nunderlying objectives whose aim is to make ML learning and usage ubiquitous (by\nusing ubiquitous compute devices), cheap and effortlessly distributed, and\ncollaborative. This is achieved by allowing every internet capable device to\nrun training algorithms and predictive models with no software installation and\nby saving models in universally readable formats. Our prototype library is\ncapable of training deep neural networks with synchronized, distributed\nstochastic gradient descent. MLitB offers several important opportunities for\nnovel ML research, including: development of distributed learning algorithms,\nadvancement of web GPU algorithms, novel field and mobile applications, privacy\npreserving computing, and green grid-computing. MLitB is available as open\nsource software.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 02:23:40 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2015 13:11:41 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Meeds", "Edward", ""], ["Hendriks", "Remco", ""], ["Faraby", "Said Al", ""], ["Bruntink", "Magiel", ""], ["Welling", "Max", ""]]}, {"id": "1412.2632", "submitter": "Joseph Salmon", "authors": "Jean Lafond (LTCI), Olga Klopp (MODAL'X, CREST-INSEE), Eric Moulines\n  (LTCI), Jospeh Salmon (LTCI)", "title": "Probabilistic low-rank matrix completion on finite alphabets", "comments": "arXiv admin note: text overlap with arXiv:1408.6218", "journal-ref": "NIPS, Dec 2014, Montreal, Canada", "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of reconstructing a matrix given a sample of observedentries is\nknown as the matrix completion problem. It arises ina wide range of problems,\nincluding recommender systems, collaborativefiltering, dimensionality\nreduction, image processing, quantum physics or multi-class classificationto\nname a few. Most works have focused on recovering an unknown real-valued\nlow-rankmatrix from randomly sub-sampling its entries.Here, we investigate the\ncase where the observations take a finite number of values, corresponding for\nexamples to ratings in recommender systems or labels in multi-class\nclassification.We also consider a general sampling scheme (not necessarily\nuniform) over the matrix entries.The performance of a nuclear-norm penalized\nestimator is analyzed theoretically.More precisely, we derive bounds for the\nKullback-Leibler divergence between the true and estimated distributions.In\npractice, we have also proposed an efficient algorithm based on lifted\ncoordinate gradient descent in order to tacklepotentially high dimensional\nsettings.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 15:57:40 GMT"}], "update_date": "2014-12-20", "authors_parsed": [["Lafond", "Jean", "", "LTCI"], ["Klopp", "Olga", "", "MODAL'X, CREST-INSEE"], ["Moulines", "Eric", "", "LTCI"], ["Salmon", "Jospeh", "", "LTCI"]]}, {"id": "1412.2669", "submitter": "Sampurna Biswas", "authors": "Sampurna Biswas, Sunrita Poddar, Soura Dasgupta, Raghuraman Mudumbai\n  and Mathews Jacob", "title": "Two step recovery of jointly sparse and low-rank matrices: theoretical\n  guarantees", "comments": "4 pages, 4 figures, ISBI 2015 conference submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a two step algorithm with theoretical guarantees to recover a\njointly sparse and low-rank matrix from undersampled measurements of its\ncolumns. The algorithm first estimates the row subspace of the matrix using a\nset of common measurements of the columns. In the second step, the subspace\naware recovery of the matrix is solved using a simple least square algorithm.\nThe results are verified in the context of recovering CINE data from\nundersampled measurements; we obtain good recovery when the sampling conditions\nare satisfied.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 18:19:31 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2015 18:39:18 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Biswas", "Sampurna", ""], ["Poddar", "Sunrita", ""], ["Dasgupta", "Soura", ""], ["Mudumbai", "Raghuraman", ""], ["Jacob", "Mathews", ""]]}, {"id": "1412.2693", "submitter": "Hanie Sedghi", "authors": "Hanie Sedghi and Anima Anandkumar", "title": "Provable Methods for Training Neural Networks with Sparse Connectivity", "comments": "Accepted for presentation at Neural Information Processing\n  Systems(NIPS) 2014 Deep Learning workshop and Accepted as a workshop\n  contribution at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide novel guaranteed approaches for training feedforward neural\nnetworks with sparse connectivity. We leverage on the techniques developed\npreviously for learning linear networks and show that they can also be\neffectively adopted to learn non-linear networks. We operate on the moments\ninvolving label and the score function of the input, and show that their\nfactorization provably yields the weight matrix of the first layer of a deep\nnetwork under mild conditions. In practice, the output of our method can be\nemployed as effective initializers for gradient descent.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 18:45:22 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 20:40:26 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2015 11:38:16 GMT"}, {"version": "v4", "created": "Tue, 28 Apr 2015 11:23:38 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Sedghi", "Hanie", ""], ["Anandkumar", "Anima", ""]]}, {"id": "1412.2812", "submitter": "Ehsan Khoddam", "authors": "Ivan Titov and Ehsan Khoddam", "title": "Unsupervised Induction of Semantic Roles within a Reconstruction-Error\n  Minimization Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new approach to unsupervised estimation of feature-rich\nsemantic role labeling models. Our model consists of two components: (1) an\nencoding component: a semantic role labeling model which predicts roles given a\nrich set of syntactic and lexical features; (2) a reconstruction component: a\ntensor factorization model which relies on roles to predict argument fillers.\nWhen the components are estimated jointly to minimize errors in argument\nreconstruction, the induced roles largely correspond to roles defined in\nannotated resources. Our method performs on par with most accurate role\ninduction methods on English and German, even though, unlike these previous\napproaches, we do not incorporate any prior linguistic knowledge about the\nlanguages.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 23:40:41 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Titov", "Ivan", ""], ["Khoddam", "Ehsan", ""]]}, {"id": "1412.2859", "submitter": "James P. Crutchfield", "authors": "Sarah Marzen and James P. Crutchfield", "title": "Circumventing the Curse of Dimensionality in Prediction: Causal\n  Rate-Distortion for Infinite-Order Markov Processes", "comments": "25 pages, 14 figures;\n  http://csc.ucdavis.edu/~cmg/compmech/pubs/cn.htm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech cs.LG nlin.CD q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive rate-distortion analysis suffers from the curse of dimensionality:\nclustering arbitrarily long pasts to retain information about arbitrarily long\nfutures requires resources that typically grow exponentially with length. The\nchallenge is compounded for infinite-order Markov processes, since conditioning\non finite sequences cannot capture all of their past dependencies. Spectral\narguments show that algorithms which cluster finite-length sequences fail\ndramatically when the underlying process has long-range temporal correlations\nand can fail even for processes generated by finite-memory hidden Markov\nmodels. We circumvent the curse of dimensionality in rate-distortion analysis\nof infinite-order processes by casting predictive rate-distortion objective\nfunctions in terms of the forward- and reverse-time causal states of\ncomputational mechanics. Examples demonstrate that the resulting causal\nrate-distortion theory substantially improves current predictive\nrate-distortion analyses.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 05:23:27 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Marzen", "Sarah", ""], ["Crutchfield", "James P.", ""]]}, {"id": "1412.2863", "submitter": "Majid Janzamin", "authors": "Majid Janzamin, Hanie Sedghi, Anima Anandkumar", "title": "Score Function Features for Discriminative Learning: Matrix and Tensor\n  Framework", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature learning forms the cornerstone for tackling challenging learning\nproblems in domains such as speech, computer vision and natural language\nprocessing. In this paper, we consider a novel class of matrix and\ntensor-valued features, which can be pre-trained using unlabeled samples. We\npresent efficient algorithms for extracting discriminative information, given\nthese pre-trained features and labeled samples for any related task. Our class\nof features are based on higher-order score functions, which capture local\nvariations in the probability density function of the input. We establish a\ntheoretical framework to characterize the nature of discriminative information\nthat can be extracted from score-function features, when used in conjunction\nwith labeled samples. We employ efficient spectral decomposition algorithms (on\nmatrices and tensors) for extracting discriminative components. The advantage\nof employing tensor-valued features is that we can extract richer\ndiscriminative information in the form of an overcomplete representations.\nThus, we present a novel framework for employing generative models of the input\nfor discriminative learning.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 06:22:19 GMT"}, {"version": "v2", "created": "Thu, 11 Dec 2014 19:12:31 GMT"}], "update_date": "2014-12-12", "authors_parsed": [["Janzamin", "Majid", ""], ["Sedghi", "Hanie", ""], ["Anandkumar", "Anima", ""]]}, {"id": "1412.2929", "submitter": "Yao-Hsiang Yang", "authors": "Yao-Hsiang Yang, Lu-Hung Chen, Chieh-Chih Wang, and Chu-Song Chen", "title": "Bayesian Fisher's Discriminant for Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian framework of Gaussian process in order to extend\nFisher's discriminant to classify functional data such as spectra and images.\nThe probability structure for our extended Fisher's discriminant is explicitly\nformulated, and we utilize the smoothness assumptions of functional data as\nprior probabilities. Existing methods which directly employ the smoothness\nassumption of functional data can be shown as special cases within this\nframework given corresponding priors while their estimates of the unknowns are\none-step approximations to the proposed MAP estimates. Empirical results on\nvarious simulation studies and different real applications show that the\nproposed method significantly outperforms the other Fisher's discriminant\nmethods for functional data.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 12:01:03 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Yang", "Yao-Hsiang", ""], ["Chen", "Lu-Hung", ""], ["Wang", "Chieh-Chih", ""], ["Chen", "Chu-Song", ""]]}, {"id": "1412.2954", "submitter": "Ying Xiao", "authors": "Santosh S. Vempala and Ying Xiao", "title": "Max vs Min: Tensor Decomposition and ICA with nearly Linear Sample\n  Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple, general technique for reducing the sample complexity of\nmatrix and tensor decomposition algorithms applied to distributions. We use the\ntechnique to give a polynomial-time algorithm for standard ICA with sample\ncomplexity nearly linear in the dimension, thereby improving substantially on\nprevious bounds. The analysis is based on properties of random polynomials,\nnamely the spacings of an ensemble of polynomials. Our technique also applies\nto other applications of tensor decompositions, including spherical Gaussian\nmixture models.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 13:33:04 GMT"}, {"version": "v2", "created": "Wed, 10 Dec 2014 14:31:06 GMT"}, {"version": "v3", "created": "Fri, 19 Jun 2015 19:00:25 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Vempala", "Santosh S.", ""], ["Xiao", "Ying", ""]]}, {"id": "1412.3046", "submitter": "Hanie Sedghi", "authors": "Hanie Sedghi, Majid Janzamin, Anima Anandkumar", "title": "Provable Tensor Methods for Learning Mixtures of Generalized Linear\n  Models", "comments": "To appear in Proceeding of AI and Statistics (AISTATS) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning mixtures of generalized linear models\n(GLM) which arise in classification and regression problems. Typical learning\napproaches such as expectation maximization (EM) or variational Bayes can get\nstuck in spurious local optima. In contrast, we present a tensor decomposition\nmethod which is guaranteed to correctly recover the parameters. The key insight\nis to employ certain feature transformations of the input, which depend on the\ninput generative model. Specifically, we employ score function tensors of the\ninput and compute their cross-correlation with the response variable. We\nestablish that the decomposition of this tensor consistently recovers the\nparameters, under mild non-degeneracy conditions. We demonstrate that the\ncomputational and sample complexity of our method is a low order polynomial of\nthe input and the latent dimensions.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 18:27:48 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2015 04:43:36 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2015 06:54:54 GMT"}, {"version": "v4", "created": "Wed, 13 Jan 2016 00:19:00 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Sedghi", "Hanie", ""], ["Janzamin", "Majid", ""], ["Anandkumar", "Anima", ""]]}, {"id": "1412.3051", "submitter": "Edward Meeds", "authors": "Edward Meeds and Michael Chiang and Mary Lee and Olivier Cinquin and\n  John Lowengrub and Max Welling", "title": "POPE: Post Optimization Posterior Evaluation of Likelihood Free Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many domains, scientists build complex simulators of natural phenomena\nthat encode their hypotheses about the underlying processes. These simulators\ncan be deterministic or stochastic, fast or slow, constrained or unconstrained,\nand so on. Optimizing the simulators with respect to a set of parameter values\nis common practice, resulting in a single parameter setting that minimizes an\nobjective subject to constraints. We propose a post optimization posterior\nanalysis that computes and visualizes all the models that can generate equally\ngood or better simulation results, subject to constraints. These optimization\nposteriors are desirable for a number of reasons among which easy\ninterpretability, automatic parameter sensitivity and correlation analysis and\nposterior predictive analysis. We develop a new sampling framework based on\napproximate Bayesian computation (ABC) with one-sided kernels. In collaboration\nwith two groups of scientists we applied POPE to two important biological\nsimulators: a fast and stochastic simulator of stem-cell cycling and a slow and\ndeterministic simulator of tumor growth patterns.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 18:51:07 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Meeds", "Edward", ""], ["Chiang", "Michael", ""], ["Lee", "Mary", ""], ["Cinquin", "Olivier", ""], ["Lowengrub", "John", ""], ["Welling", "Max", ""]]}, {"id": "1412.3078", "submitter": "Jun Wei Ng", "authors": "Jun Wei Ng and Marc Peter Deisenroth", "title": "Hierarchical Mixture-of-Experts Model for Large-Scale Gaussian Process\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a practical and scalable Gaussian process model for large-scale\nnonlinear probabilistic regression. Our mixture-of-experts model is\nconceptually simple and hierarchically recombines computations for an overall\napproximation of a full Gaussian process. Closed-form and distributed\ncomputations allow for efficient and massive parallelisation while keeping the\nmemory consumption small. Given sufficient computing resources, our model can\nhandle arbitrarily large data sets, without explicit sparse approximations. We\nprovide strong experimental evidence that our model can be applied to large\ndata sets of sizes far beyond millions. Hence, our model has the potential to\nlay the foundation for general large-scale Gaussian process research.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 20:03:06 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Ng", "Jun Wei", ""], ["Deisenroth", "Marc Peter", ""]]}, {"id": "1412.3276", "submitter": "Christos Dimitrakakis", "authors": "Emmanouil G. Androulakis, Christos Dimitrakakis", "title": "Generalised Entropy MDPs and Minimax Regret", "comments": "7 pages, NIPS workshop \"From bad models to good policies\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian methods suffer from the problem of how to specify prior beliefs. One\ninteresting idea is to consider worst-case priors. This requires solving a\nstochastic zero-sum game. In this paper, we extend well-known results from\nbandit theory in order to discover minimax-Bayes policies and discuss when they\nare practical.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 12:28:34 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["Androulakis", "Emmanouil G.", ""], ["Dimitrakakis", "Christos", ""]]}, {"id": "1412.3297", "submitter": "Vladimir Temlyakov", "authors": "Vladimir Temlyakov", "title": "Convergence and rate of convergence of some greedy algorithms in convex\n  optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper gives a systematic study of the approximate versions of three\ngreedy-type algorithms that are widely used in convex optimization. By\napproximate version we mean the one where some of evaluations are made with an\nerror. Importance of such versions of greedy-type algorithms in convex\noptimization and in approximation theory was emphasized in previous literature.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 13:22:38 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["Temlyakov", "Vladimir", ""]]}, {"id": "1412.3411", "submitter": "Jacquelyn Shelton", "authors": "Jacquelyn A. Shelton, Jan Gasthaus, Zhenwen Dai, Joerg Luecke, Arthur\n  Gretton", "title": "GP-select: Accelerating EM using adaptive subspace preselection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a nonparametric procedure to achieve fast inference in generative\ngraphical models when the number of latent states is very large. The approach\nis based on iterative latent variable preselection, where we alternate between\nlearning a 'selection function' to reveal the relevant latent variables, and\nuse this to obtain a compact approximation of the posterior distribution for\nEM; this can make inference possible where the number of possible latent states\nis e.g. exponential in the number of latent variables, whereas an exact\napproach would be computationally unfeasible. We learn the selection function\nentirely from the observed data and current EM state via Gaussian process\nregression. This is by contrast with earlier approaches, where selection\nfunctions were manually-designed for each problem setting. We show that our\napproach performs as well as these bespoke selection functions on a wide\nvariety of inference problems: in particular, for the challenging case of a\nhierarchical model for object localization with occlusion, we achieve results\nthat match a customized state-of-the-art selection method, at a far lower\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 19:04:52 GMT"}, {"version": "v2", "created": "Sun, 17 Jul 2016 08:20:25 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Shelton", "Jacquelyn A.", ""], ["Gasthaus", "Jan", ""], ["Dai", "Zhenwen", ""], ["Luecke", "Joerg", ""], ["Gretton", "Arthur", ""]]}, {"id": "1412.3432", "submitter": "Yuan Zhang", "authors": "Yuan Zhang, Elizaveta Levina, Ji Zhu", "title": "Detecting Overlapping Communities in Networks Using Spectral Methods", "comments": "29 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection is a fundamental problem in network analysis which is\nmade more challenging by overlaps between communities which often occur in\npractice. Here we propose a general, flexible, and interpretable generative\nmodel for overlapping communities, which can be thought of as a generalization\nof the degree-corrected stochastic block model. We develop an efficient\nspectral algorithm for estimating the community memberships, which deals with\nthe overlaps by employing the K-medians algorithm rather than the usual K-means\nfor clustering in the spectral domain. We show that the algorithm is\nasymptotically consistent when networks are not too sparse and the overlaps\nbetween communities not too large. Numerical experiments on both simulated\nnetworks and many real social networks demonstrate that our method performs\nvery well compared to a number of benchmark methods for overlapping community\ndetection.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 20:06:24 GMT"}, {"version": "v2", "created": "Fri, 12 Dec 2014 00:43:51 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2015 03:15:18 GMT"}, {"version": "v4", "created": "Thu, 12 Mar 2015 23:12:26 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Zhang", "Yuan", ""], ["Levina", "Elizaveta", ""], ["Zhu", "Ji", ""]]}, {"id": "1412.3617", "submitter": "Paul Fearnhead", "authors": "Kaylea Haynes, Idris A. Eckley and Paul Fearnhead", "title": "Efficient penalty search for multiple changepoint problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the multiple changepoint setting, various search methods have been\nproposed which involve optimising either a constrained or penalised cost\nfunction over possible numbers and locations of changepoints using dynamic\nprogramming. Such methods are typically computationally intensive. Recent work\nin the penalised optimisation setting has focussed on developing a\npruning-based approach which gives an improved computational cost that, under\ncertain conditions, is linear in the number of data points. Such an approach\nnaturally requires the specification of a penalty to avoid under/over-fitting.\nWork has been undertaken to identify the appropriate penalty choice for data\ngenerating processes with known distributional form, but in many applications\nthe model assumed for the data is not correct and these penalty choices are not\nalways appropriate. Consequently it is desirable to have an approach that\nenables us to compare segmentations for different choices of penalty. To this\nend we present a method to obtain optimal changepoint segmentations of data\nsequences for all penalty values across a continuous range. This permits an\nevaluation of the various segmentations to identify a suitably parsimonious\npenalty choice. The computational complexity of this approach can be linear in\nthe number of data points and linear in the difference between the number of\nchangepoints in the optimal segmentations for the smallest and largest penalty\nvalues. This can be orders of magnitude faster than alternative approaches that\nfind optimal segmentations for a range of the number of changepoints.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 11:49:47 GMT"}], "update_date": "2014-12-12", "authors_parsed": [["Haynes", "Kaylea", ""], ["Eckley", "Idris A.", ""], ["Fearnhead", "Paul", ""]]}, {"id": "1412.3705", "submitter": "Weicong Ding", "authors": "Weicong Ding, Prakash Ishwar, Venkatesh Saligrama", "title": "A Topic Modeling Approach to Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a topic modeling approach to the prediction of preferences in\npairwise comparisons. We develop a new generative model for pairwise\ncomparisons that accounts for multiple shared latent rankings that are\nprevalent in a population of users. This new model also captures inconsistent\nuser behavior in a natural way. We show how the estimation of latent rankings\nin the new generative model can be formally reduced to the estimation of topics\nin a statistically equivalent topic modeling problem. We leverage recent\nadvances in the topic modeling literature to develop an algorithm that can\nlearn shared latent rankings with provable consistency as well as sample and\ncomputational complexity guarantees. We demonstrate that the new approach is\nempirically competitive with the current state-of-the-art approaches in\npredicting preferences on some semi-synthetic and real world datasets.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 16:15:53 GMT"}, {"version": "v2", "created": "Fri, 12 Dec 2014 22:01:20 GMT"}, {"version": "v3", "created": "Sun, 25 Jan 2015 22:32:10 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Ding", "Weicong", ""], ["Ishwar", "Prakash", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1412.3708", "submitter": "Marc Goessling", "authors": "Marc Goessling and Yali Amit", "title": "Compact Compositional Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning compact and interpretable representations is a very natural task,\nwhich has not been solved satisfactorily even for simple binary datasets. In\nthis paper, we review various ways of composing experts for binary data and\nargue that competitive forms of interaction are best suited to learn\nlow-dimensional representations. We propose a new composition rule that\ndiscourages experts from focusing on similar structures and that penalizes\nopposing votes strongly so that abstaining from voting becomes more attractive.\nWe also introduce a novel sequential initialization procedure, which is based\non a process of oversimplification and correction. Experiments show that with\nour approach very intuitive models can be learned.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 16:19:56 GMT"}, {"version": "v2", "created": "Wed, 25 Feb 2015 19:23:27 GMT"}, {"version": "v3", "created": "Wed, 8 Apr 2015 22:02:42 GMT"}, {"version": "v4", "created": "Sat, 29 Oct 2016 22:49:39 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Goessling", "Marc", ""], ["Amit", "Yali", ""]]}, {"id": "1412.3756", "submitter": "Suresh Venkatasubramanian", "authors": "Michael Feldman and Sorelle Friedler and John Moeller and Carlos\n  Scheidegger and Suresh Venkatasubramanian", "title": "Certifying and removing disparate impact", "comments": "Extended version of paper accepted at 2015 ACM SIGKDD Conference on\n  Knowledge Discovery and Data Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What does it mean for an algorithm to be biased? In U.S. law, unintentional\nbias is encoded via disparate impact, which occurs when a selection process has\nwidely different outcomes for different groups, even as it appears to be\nneutral. This legal determination hinges on a definition of a protected class\n(ethnicity, gender, religious practice) and an explicit description of the\nprocess.\n  When the process is implemented using computers, determining disparate impact\n(and hence bias) is harder. It might not be possible to disclose the process.\nIn addition, even if the process is open, it might be hard to elucidate in a\nlegal setting how the algorithm makes its decisions. Instead of requiring\naccess to the algorithm, we propose making inferences based on the data the\nalgorithm uses.\n  We make four contributions to this problem. First, we link the legal notion\nof disparate impact to a measure of classification accuracy that while known,\nhas received relatively little attention. Second, we propose a test for\ndisparate impact based on analyzing the information leakage of the protected\nclass from the other data attributes. Third, we describe methods by which data\nmight be made unbiased. Finally, we present empirical evidence supporting the\neffectiveness of our test for disparate impact and our approach for both\nmasking bias and preserving relevant information in the data. Interestingly,\nour approach resembles some actual selection practices that have recently\nreceived legal scrutiny.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 18:42:59 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 03:48:17 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2015 00:54:05 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Feldman", "Michael", ""], ["Friedler", "Sorelle", ""], ["Moeller", "John", ""], ["Scheidegger", "Carlos", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "1412.3773", "submitter": "Joris Mooij", "authors": "Joris M. Mooij, Jonas Peters, Dominik Janzing, Jakob Zscheischler,\n  Bernhard Sch\\\"olkopf", "title": "Distinguishing cause from effect using observational data: methods and\n  benchmarks", "comments": "101 pages, second revision submitted to Journal of Machine Learning\n  Research", "journal-ref": "Journal of Machine Learning Research 17(32):1-102, 2016", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discovery of causal relationships from purely observational data is a\nfundamental problem in science. The most elementary form of such a causal\ndiscovery problem is to decide whether X causes Y or, alternatively, Y causes\nX, given joint observations of two variables X, Y. An example is to decide\nwhether altitude causes temperature, or vice versa, given only joint\nmeasurements of both variables. Even under the simplifying assumptions of no\nconfounding, no feedback loops, and no selection bias, such bivariate causal\ndiscovery problems are challenging. Nevertheless, several approaches for\naddressing those problems have been proposed in recent years. We review two\nfamilies of such methods: Additive Noise Methods (ANM) and Information\nGeometric Causal Inference (IGCI). We present the benchmark CauseEffectPairs\nthat consists of data for 100 different cause-effect pairs selected from 37\ndatasets from various domains (e.g., meteorology, biology, medicine,\nengineering, economy, etc.) and motivate our decisions regarding the \"ground\ntruth\" causal directions of all pairs. We evaluate the performance of several\nbivariate causal discovery methods on these real-world benchmark data and in\naddition on artificially simulated data. Our empirical results on real-world\ndata indicate that certain methods are indeed able to distinguish cause from\neffect using only purely observational data, although more benchmark data would\nbe needed to obtain statistically significant conclusions. One of the best\nperforming methods overall is the additive-noise method originally proposed by\nHoyer et al. (2009), which obtains an accuracy of 63+-10 % and an AUC of\n0.74+-0.05 on the real-world benchmark. As the main theoretical contribution of\nthis work we prove the consistency of that method.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 19:34:39 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2015 14:51:36 GMT"}, {"version": "v3", "created": "Thu, 24 Dec 2015 11:37:57 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Mooij", "Joris M.", ""], ["Peters", "Jonas", ""], ["Janzing", "Dominik", ""], ["Zscheischler", "Jakob", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1412.3919", "submitter": "Alexandre Abraham", "authors": "Alexandre Abraham (NEUROSPIN, INRIA Saclay - Ile de France), Fabian\n  Pedregosa (INRIA Saclay - Ile de France), Michael Eickenberg (LNAO, INRIA\n  Saclay - Ile de France), Philippe Gervais (NEUROSPIN, INRIA Saclay - Ile de\n  France, LNAO), Andreas Muller, Jean Kossaifi, Alexandre Gramfort (NEUROSPIN,\n  LTCI), Bertrand Thirion (NEUROSPIN, INRIA Saclay - Ile de France), G\\\"ael\n  Varoquaux (NEUROSPIN, INRIA Saclay - Ile de France, LNAO)", "title": "Machine Learning for Neuroimaging with Scikit-Learn", "comments": "Frontiers in neuroscience, Frontiers Research Foundation, 2013, pp.15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical machine learning methods are increasingly used for neuroimaging\ndata analysis. Their main virtue is their ability to model high-dimensional\ndatasets, e.g. multivariate analysis of activation images or resting-state time\nseries. Supervised learning is typically used in decoding or encoding settings\nto relate brain images to behavioral or clinical observations, while\nunsupervised learning can uncover hidden structures in sets of images (e.g.\nresting state functional MRI) or find sub-populations in large cohorts. By\nconsidering different functional neuroimaging applications, we illustrate how\nscikit-learn, a Python machine learning library, can be used to perform some\nkey analysis steps. Scikit-learn contains a very large set of statistical\nlearning algorithms, both supervised and unsupervised, and its application to\nneuroimaging data provides a versatile tool to study the brain.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 08:38:35 GMT"}], "update_date": "2014-12-15", "authors_parsed": [["Abraham", "Alexandre", "", "NEUROSPIN, INRIA Saclay - Ile de France"], ["Pedregosa", "Fabian", "", "INRIA Saclay - Ile de France"], ["Eickenberg", "Michael", "", "LNAO, INRIA\n  Saclay - Ile de France"], ["Gervais", "Philippe", "", "NEUROSPIN, INRIA Saclay - Ile de\n  France, LNAO"], ["Muller", "Andreas", "", "NEUROSPIN,\n  LTCI"], ["Kossaifi", "Jean", "", "NEUROSPIN,\n  LTCI"], ["Gramfort", "Alexandre", "", "NEUROSPIN,\n  LTCI"], ["Thirion", "Bertrand", "", "NEUROSPIN, INRIA Saclay - Ile de France"], ["Varoquaux", "G\u00e4el", "", "NEUROSPIN, INRIA Saclay - Ile de France, LNAO"]]}, {"id": "1412.4005", "submitter": "Jerome Bobin", "authors": "Jerome Bobin and Jeremy Rapin and Anthony Larue and Jean-Luc Starck", "title": "Sparsity and adaptivity for the blind separation of partially correlated\n  sources", "comments": "submitted to IEEE Transactions on signal processing", "journal-ref": null, "doi": "10.1109/TSP.2015.2391071", "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind source separation (BSS) is a very popular technique to analyze\nmultichannel data. In this context, the data are modeled as the linear\ncombination of sources to be retrieved. For that purpose, standard BSS methods\nall rely on some discrimination principle, whether it is statistical\nindependence or morphological diversity, to distinguish between the sources.\nHowever, dealing with real-world data reveals that such assumptions are rarely\nvalid in practice: the signals of interest are more likely partially\ncorrelated, which generally hampers the performances of standard BSS methods.\nIn this article, we introduce a novel sparsity-enforcing BSS method coined\nAdaptive Morphological Component Analysis (AMCA), which is designed to retrieve\nsparse and partially correlated sources. More precisely, it makes profit of an\nadaptive re-weighting scheme to favor/penalize samples based on their level of\ncorrelation. Extensive numerical experiments have been carried out which show\nthat the proposed method is robust to the partial correlation of sources while\nstandard BSS techniques fail. The AMCA algorithm is evaluated in the field of\nastrophysics for the separation of physical components from microwave data.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 14:41:14 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Bobin", "Jerome", ""], ["Rapin", "Jeremy", ""], ["Larue", "Anthony", ""], ["Starck", "Jean-Luc", ""]]}, {"id": "1412.4044", "submitter": "Jun He", "authors": "Jun He, Yue Zhang", "title": "Adaptive Stochastic Gradient Descent on the Grassmannian for Robust\n  Low-Rank Subspace Recovery and Clustering", "comments": "13 pages, 12 figures and 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present GASG21 (Grassmannian Adaptive Stochastic Gradient\nfor $L_{2,1}$ norm minimization), an adaptive stochastic gradient algorithm to\nrobustly recover the low-rank subspace from a large matrix. In the presence of\ncolumn outliers, we reformulate the batch mode matrix $L_{2,1}$ norm\nminimization with rank constraint problem as a stochastic optimization approach\nconstrained on Grassmann manifold. For each observed data vector, the low-rank\nsubspace $\\mathcal{S}$ is updated by taking a gradient step along the geodesic\nof Grassmannian. In order to accelerate the convergence rate of the stochastic\ngradient method, we choose to adaptively tune the constant step-size by\nleveraging the consecutive gradients. Furthermore, we demonstrate that with\nproper initialization, the K-subspaces extension, K-GASG21, can robustly\ncluster a large number of corrupted data vectors into a union of subspaces.\nNumerical experiments on synthetic and real data demonstrate the efficiency and\naccuracy of the proposed algorithms even with heavy column outliers corruption.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 16:32:48 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2015 08:37:45 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["He", "Jun", ""], ["Zhang", "Yue", ""]]}, {"id": "1412.4052", "submitter": "Mathieu Lagrange", "authors": "Mathieu Lagrange (IRCCyN), Gr\\'egoire Lafay (IRCCyN), Boris\n  Defreville, Jean-Julien Aucouturier", "title": "The bag-of-frames approach: a not so sufficient model for urban\n  soundscapes", "comments": null, "journal-ref": "JASA Express Letters, 2015, 138 (5), pp.487-492", "doi": null, "report-no": null, "categories": "cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \"bag-of-frames\" approach (BOF), which encodes audio signals as the\nlong-term statistical distribution of short-term spectral features, is commonly\nregarded as an effective and sufficient way to represent environmental sound\nrecordings (soundscapes) since its introduction in an influential 2007 article.\nThe present paper describes a concep-tual replication of this seminal article\nusing several new soundscape datasets, with results strongly questioning the\nadequacy of the BOF approach for the task. We show that the good accuracy\noriginally re-ported with BOF likely result from a particularly thankful\ndataset with low within-class variability, and that for more realistic\ndatasets, BOF in fact does not perform significantly better than a mere\none-point av-erage of the signal's features. Soundscape modeling, therefore,\nmay not be the closed case it was once thought to be. Progress, we ar-gue,\ncould lie in reconsidering the problem of considering individual acoustical\nevents within each soundscape.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 05:11:54 GMT"}, {"version": "v2", "created": "Sun, 12 Jun 2016 08:08:41 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Lagrange", "Mathieu", "", "IRCCyN"], ["Lafay", "Gr\u00e9goire", "", "IRCCyN"], ["Defreville", "Boris", ""], ["Aucouturier", "Jean-Julien", ""]]}, {"id": "1412.4056", "submitter": "Riccardo Sven Risuleo", "authors": "Giulio Bottegal, Riccardo S. Risuleo and H{\\aa}kan Hjalmarsson", "title": "Blind system identification using kernel-based methods", "comments": "15 pages; accepted for publication at IFAC Sysid 2015", "journal-ref": null, "doi": "10.1016/j.ifacol.2015.12.172", "report-no": null, "categories": "cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for blind system identification. Resorting to a\nGaussian regression framework, we model the impulse response of the unknown\nlinear system as a realization of a Gaussian process. The structure of the\ncovariance matrix (or kernel) of such a process is given by the stable spline\nkernel, which has been recently introduced for system identification purposes\nand depends on an unknown hyperparameter. We assume that the input can be\nlinearly described by few parameters. We estimate these parameters, together\nwith the kernel hyperparameter and the noise variance, using an empirical Bayes\napproach. The related optimization problem is efficiently solved with a novel\niterative scheme based on the Expectation-Maximization method. In particular,\nwe show that each iteration consists of a set of simple update rules. We show,\nthrough some numerical experiments, very promising performance of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 17:04:42 GMT"}, {"version": "v2", "created": "Thu, 19 May 2016 11:29:34 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Bottegal", "Giulio", ""], ["Risuleo", "Riccardo S.", ""], ["Hjalmarsson", "H\u00e5kan", ""]]}, {"id": "1412.4080", "submitter": "Antoine Bonnefoy", "authors": "Antoine Bonnefoy, Valentin Emiya, Liva Ralaivola, R\\'emi Gribonval\n  (INRIA - IRISA)", "title": "Dynamic Screening: Accelerating First-Order Algorithms for the Lasso and\n  Group-Lasso", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2015.2447503", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent computational strategies based on screening tests have been proposed\nto accelerate algorithms addressing penalized sparse regression problems such\nas the Lasso. Such approaches build upon the idea that it is worth dedicating\nsome small computational effort to locate inactive atoms and remove them from\nthe dictionary in a preprocessing stage so that the regression algorithm\nworking with a smaller dictionary will then converge faster to the solution of\nthe initial problem. We believe that there is an even more efficient way to\nscreen the dictionary and obtain a greater acceleration: inside each iteration\nof the regression algorithm, one may take advantage of the algorithm\ncomputations to obtain a new screening test for free with increasing screening\neffects along the iterations. The dictionary is henceforth dynamically screened\ninstead of being screened statically, once and for all, before the first\niteration. We formalize this dynamic screening principle in a general\nalgorithmic scheme and apply it by embedding inside a number of first-order\nalgorithms adapted existing screening tests to solve the Lasso or new screening\ntests to solve the Group-Lasso. Computational gains are assessed in a large set\nof experiments on synthetic data as well as real-world sounds and images. They\nshow both the screening efficiency and the gain in terms running times.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 18:39:56 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Bonnefoy", "Antoine", "", "INRIA - IRISA"], ["Emiya", "Valentin", "", "INRIA - IRISA"], ["Ralaivola", "Liva", "", "INRIA - IRISA"], ["Gribonval", "R\u00e9mi", "", "INRIA - IRISA"]]}, {"id": "1412.4098", "submitter": "Cencheng Shen", "authors": "Cencheng Shen, Joshua T. Vogelstein, Carey E. Priebe", "title": "Manifold Matching using Shortest-Path Distance and Joint Neighborhood\n  Selection", "comments": "13 pages, 8 figures, 2 tables", "journal-ref": "Pattern Recognition Letters, vol. 92, pp. 41-48, 2017", "doi": "10.1016/j.patrec.2017.04.005", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching datasets of multiple modalities has become an important task in data\nanalysis. Existing methods often rely on the embedding and transformation of\neach single modality without utilizing any correspondence information, which\noften results in sub-optimal matching performance. In this paper, we propose a\nnonlinear manifold matching algorithm using shortest-path distance and joint\nneighborhood selection. Specifically, a joint nearest-neighbor graph is built\nfor all modalities. Then the shortest-path distance within each modality is\ncalculated from the joint neighborhood graph, followed by embedding into and\nmatching in a common low-dimensional Euclidean space. Compared to existing\nalgorithms, our approach exhibits superior performance for matching disparate\ndatasets of multiple modalities.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 19:51:22 GMT"}, {"version": "v2", "created": "Mon, 12 Jan 2015 13:22:34 GMT"}, {"version": "v3", "created": "Sun, 6 Mar 2016 22:51:40 GMT"}, {"version": "v4", "created": "Tue, 11 Apr 2017 00:35:51 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Shen", "Cencheng", ""], ["Vogelstein", "Joshua T.", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1412.4128", "submitter": "Mu Zhu", "authors": "W. James Murdoch and Mu Zhu", "title": "Expanded Alternating Optimization of Nonconvex Functions with\n  Applications to Matrix Factorization and Penalized Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general technique for improving alternating optimization (AO) of\nnonconvex functions. Starting from the solution given by AO, we conduct another\nsequence of searches over subspaces that are both meaningful to the\noptimization problem at hand and different from those used by AO. To\ndemonstrate the utility of our approach, we apply it to the matrix\nfactorization (MF) algorithm for recommender systems and the coordinate descent\nalgorithm for penalized regression (PR), and show meaningful improvements using\nboth real-world (for MF) and simulated (for PR) data sets. Moreover, we\ndemonstrate for MF that, by constructing search spaces customized to the given\ndata set, we can significantly increase the convergence rate of our technique.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 21:04:15 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Murdoch", "W. James", ""], ["Zhu", "Mu", ""]]}, {"id": "1412.4182", "submitter": "Stefan Wager", "authors": "Jacob Steinhardt, Stefan Wager, and Percy Liang", "title": "The Statistics of Streaming Sparse Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a sparse analogue to stochastic gradient descent that is\nguaranteed to perform well under similar conditions to the lasso. In the linear\nregression setup with irrepresentable noise features, our algorithm recovers\nthe support set of the optimal parameter vector with high probability, and\nachieves a statistically quasi-optimal rate of convergence of Op(k log(d)/T),\nwhere k is the sparsity of the solution, d is the number of features, and T is\nthe number of training examples. Meanwhile, our algorithm does not require any\nmore computational resources than stochastic gradient descent. In our\nexperiments, we find that our method substantially out-performs existing\nstreaming algorithms on both real and simulated data.\n", "versions": [{"version": "v1", "created": "Sat, 13 Dec 2014 02:32:06 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Steinhardt", "Jacob", ""], ["Wager", "Stefan", ""], ["Liang", "Percy", ""]]}, {"id": "1412.4237", "submitter": "Alex Sawatzky", "authors": "Martin Burger, Alex Sawatzky, Gabriele Steidl", "title": "First order algorithms in variational image processing", "comments": "60 pages, 33 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational methods in imaging are nowadays developing towards a quite\nuniversal and flexible tool, allowing for highly successful approaches on tasks\nlike denoising, deblurring, inpainting, segmentation, super-resolution,\ndisparity, and optical flow estimation. The overall structure of such\napproaches is of the form ${\\cal D}(Ku) + \\alpha {\\cal R} (u) \\rightarrow\n\\min_u$ ; where the functional ${\\cal D}$ is a data fidelity term also\ndepending on some input data $f$ and measuring the deviation of $Ku$ from such\nand ${\\cal R}$ is a regularization functional. Moreover $K$ is a (often linear)\nforward operator modeling the dependence of data on an underlying image, and\n$\\alpha$ is a positive regularization parameter. While ${\\cal D}$ is often\nsmooth and (strictly) convex, the current practice almost exclusively uses\nnonsmooth regularization functionals. The majority of successful techniques is\nusing nonsmooth and convex functionals like the total variation and\ngeneralizations thereof or $\\ell_1$-norms of coefficients arising from scalar\nproducts with some frame system. The efficient solution of such variational\nproblems in imaging demands for appropriate algorithms. Taking into account the\nspecific structure as a sum of two very different terms to be minimized,\nsplitting algorithms are a quite canonical choice. Consequently this field has\nrevived the interest in techniques like operator splittings or augmented\nLagrangians. Here we shall provide an overview of methods currently developed\nand recent results as well as some computational studies providing a comparison\nof different methods and also illustrating their success in applications.\n", "versions": [{"version": "v1", "created": "Sat, 13 Dec 2014 14:06:41 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Burger", "Martin", ""], ["Sawatzky", "Alex", ""], ["Steidl", "Gabriele", ""]]}, {"id": "1412.4271", "submitter": "Ardavan Salehi Nobandegani", "authors": "Ardavan Salehi Nobandegani, Ioannis N. Psaromiligkos", "title": "Multi-Context Models for Reasoning under Partial Knowledge: Generative\n  Process and Inference Grammar", "comments": "To appear in the Proceedings of the 31st Conference on Uncertainty in\n  Artificial Intelligence (UAI 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.LO math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arriving at the complete probabilistic knowledge of a domain, i.e., learning\nhow all variables interact, is indeed a demanding task. In reality, settings\noften arise for which an individual merely possesses partial knowledge of the\ndomain, and yet, is expected to give adequate answers to a variety of posed\nqueries. That is, although precise answers to some queries, in principle,\ncannot be achieved, a range of plausible answers is attainable for each query\ngiven the available partial knowledge. In this paper, we propose the\nMulti-Context Model (MCM), a new graphical model to represent the state of\npartial knowledge as to a domain. MCM is a middle ground between Probabilistic\nLogic, Bayesian Logic, and Probabilistic Graphical Models. For this model we\ndiscuss: (i) the dynamics of constructing a contradiction-free MCM, i.e., to\nform partial beliefs regarding a domain in a gradual and probabilistically\nconsistent way, and (ii) how to perform inference, i.e., to evaluate a\nprobability of interest involving some variables of the domain.\n", "versions": [{"version": "v1", "created": "Sat, 13 Dec 2014 19:13:09 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 18:19:08 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Nobandegani", "Ardavan Salehi", ""], ["Psaromiligkos", "Ioannis N.", ""]]}, {"id": "1412.4446", "submitter": "Pascal Germain", "authors": "Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\\c{c}ois Laviolette,\n  Mario Marchand", "title": "Domain-Adversarial Neural Networks", "comments": "The first version of this paper was accepted at the \"Second Workshop\n  on Transfer and Multi-Task Learning: Theory meets Practice\" (NIPS 2014,\n  Montreal, Canada). See: https://sites.google.com/site/multitaskwsnips2014/", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new representation learning algorithm suited to the context of\ndomain adaptation, in which data at training and test time come from similar\nbut different distributions. Our algorithm is directly inspired by theory on\ndomain adaptation suggesting that, for effective domain transfer to be\nachieved, predictions must be made based on a data representation that cannot\ndiscriminate between the training (source) and test (target) domains. We\npropose a training objective that implements this idea in the context of a\nneural network, whose hidden layer is trained to be predictive of the\nclassification task, but uninformative as to the domain of the input. Our\nexperiments on a sentiment analysis classification benchmark, where the target\ndomain data available at training time is unlabeled, show that our neural\nnetwork for domain adaption algorithm has better performance than either a\nstandard neural network or an SVM, even if trained on input features extracted\nwith the state-of-the-art marginalized stacked denoising autoencoders of Chen\net al. (2012).\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 02:16:07 GMT"}, {"version": "v2", "created": "Mon, 9 Feb 2015 17:52:03 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Ajakan", "Hana", ""], ["Germain", "Pascal", ""], ["Larochelle", "Hugo", ""], ["Laviolette", "Fran\u00e7ois", ""], ["Marchand", "Mario", ""]]}, {"id": "1412.4659", "submitter": "Ju Sun", "authors": "Qing Qu, Ju Sun, John Wright", "title": "Finding a sparse vector in a subspace: Linear sparsity using alternating\n  directions", "comments": "Accepted by IEEE Trans. Information Theory. The paper has been\n  revised by the reviewers' comments. The proofs have been streamlined", "journal-ref": "IEEE Transaction on Information Theory, 62(10):5855 - 5880, 2016", "doi": "10.1109/TIT.2016.2601599", "report-no": null, "categories": "cs.IT cs.CV cs.LG math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is it possible to find the sparsest vector (direction) in a generic subspace\n$\\mathcal{S} \\subseteq \\mathbb{R}^p$ with $\\mathrm{dim}(\\mathcal{S})= n < p$?\nThis problem can be considered a homogeneous variant of the sparse recovery\nproblem, and finds connections to sparse dictionary learning, sparse PCA, and\nmany other problems in signal processing and machine learning. In this paper,\nwe focus on a **planted sparse model** for the subspace: the target sparse\nvector is embedded in an otherwise random subspace. Simple convex heuristics\nfor this planted recovery problem provably break down when the fraction of\nnonzero entries in the target sparse vector substantially exceeds\n$O(1/\\sqrt{n})$. In contrast, we exhibit a relatively simple nonconvex approach\nbased on alternating directions, which provably succeeds even when the fraction\nof nonzero entries is $\\Omega(1)$. To the best of our knowledge, this is the\nfirst practical algorithm to achieve linear scaling under the planted sparse\nmodel. Empirically, our proposed algorithm also succeeds in more challenging\ndata models, e.g., sparse dictionary learning.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 16:27:29 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 03:23:33 GMT"}, {"version": "v3", "created": "Wed, 20 Jul 2016 00:54:41 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Qu", "Qing", ""], ["Sun", "Ju", ""], ["Wright", "John", ""]]}, {"id": "1412.4679", "submitter": "Suleiman Khan", "authors": "Suleiman A. Khan, Eemeli Lepp\\\"aaho, Samuel Kaski", "title": "Bayesian multi-tensor factorization", "comments": "R Implementation / source code:\n  http://research.cs.aalto.fi/pml/software/mtf/", "journal-ref": "Machine Learning, 105(2), 233-253, 2016", "doi": "10.1007/s10994-016-5563-y", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Bayesian multi-tensor factorization, a model that is the first\nBayesian formulation for joint factorization of multiple matrices and tensors.\nThe research problem generalizes the joint matrix-tensor factorization problem\nto arbitrary sets of tensors of any depth, including matrices, can be\ninterpreted as unsupervised multi-view learning from multiple data tensors, and\ncan be generalized to relax the usual trilinear tensor factorization\nassumptions. The result is a factorization of the set of tensors into factors\nshared by any subsets of the tensors, and factors private to individual\ntensors. We demonstrate the performance against existing baselines in multiple\ntensor factorization tasks in structural toxicogenomics and functional\nneuroimaging.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 17:10:55 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2015 13:21:17 GMT"}, {"version": "v3", "created": "Sun, 20 Mar 2016 20:48:06 GMT"}, {"version": "v4", "created": "Tue, 9 Aug 2016 13:32:55 GMT"}, {"version": "v5", "created": "Wed, 12 Oct 2016 11:59:49 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Khan", "Suleiman A.", ""], ["Lepp\u00e4aho", "Eemeli", ""], ["Kaski", "Samuel", ""]]}, {"id": "1412.4736", "submitter": "Phil Long", "authors": "David P. Helmbold and Philip M. Long", "title": "On the Inductive Bias of Dropout", "comments": null, "journal-ref": "Journal of Machine Learning Research, 16, 3403-3454 (2015). (See\n  http://jmlr.org/papers/volume16/helmbold15a/helmbold15a.pdf.)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout is a simple but effective technique for learning in neural networks\nand other settings. A sound theoretical understanding of dropout is needed to\ndetermine when dropout should be applied and how to use it most effectively. In\nthis paper we continue the exploration of dropout as a regularizer pioneered by\nWager, et.al. We focus on linear classification where a convex proxy to the\nmisclassification loss (i.e. the logistic loss used in logistic regression) is\nminimized. We show: (a) when the dropout-regularized criterion has a unique\nminimizer, (b) when the dropout-regularization penalty goes to infinity with\nthe weights, and when it remains bounded, (c) that the dropout regularization\ncan be non-monotonic as individual weights increase from 0, and (d) that the\ndropout regularization penalty may not be convex. This last point is\nparticularly surprising because the combination of dropout regularization with\nany convex loss proxy is always a convex function.\n  In order to contrast dropout regularization with $L_2$ regularization, we\nformalize the notion of when different sources are more compatible with\ndifferent regularizers. We then exhibit distributions that are provably more\ncompatible with dropout regularization than $L_2$ regularization, and vice\nversa. These sources provide additional insight into how the inductive biases\nof dropout and $L_2$ regularization differ. We provide some similar results for\n$L_1$ regularization.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 19:40:46 GMT"}, {"version": "v2", "created": "Wed, 17 Dec 2014 02:58:50 GMT"}, {"version": "v3", "created": "Mon, 22 Dec 2014 22:22:30 GMT"}, {"version": "v4", "created": "Tue, 17 Feb 2015 18:59:20 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Helmbold", "David P.", ""], ["Long", "Philip M.", ""]]}, {"id": "1412.4864", "submitter": "Philip Bachman", "authors": "Philip Bachman and Ouais Alsharif and Doina Precup", "title": "Learning with Pseudo-Ensembles", "comments": "To appear in Advances in Neural Information Processing Systems 27\n  (NIPS 2014), Advances in Neural Information Processing Systems 27, Dec. 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formalize the notion of a pseudo-ensemble, a (possibly infinite)\ncollection of child models spawned from a parent model by perturbing it\naccording to some noise process. E.g., dropout (Hinton et. al, 2012) in a deep\nneural network trains a pseudo-ensemble of child subnetworks generated by\nrandomly masking nodes in the parent network. We present a novel regularizer\nbased on making the behavior of a pseudo-ensemble robust with respect to the\nnoise process generating it. In the fully-supervised setting, our regularizer\nmatches the performance of dropout. But, unlike dropout, our regularizer\nnaturally extends to the semi-supervised setting, where it produces\nstate-of-the-art results. We provide a case study in which we transform the\nRecursive Neural Tensor Network of (Socher et. al, 2013) into a\npseudo-ensemble, which significantly improves its performance on a real-world\nsentiment analysis benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 02:55:05 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Bachman", "Philip", ""], ["Alsharif", "Ouais", ""], ["Precup", "Doina", ""]]}, {"id": "1412.4869", "submitter": "Aki Vehtari", "authors": "Aki Vehtari, Andrew Gelman, Tuomas Sivula, Pasi Jyl\\\"anki, Dustin\n  Tran, Swupnil Sahai, Paul Blomstedt, John P. Cunningham, David Schiminovich,\n  Christian Robert", "title": "Expectation propagation as a way of life: A framework for Bayesian\n  inference on partitioned data", "comments": "Minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common divide-and-conquer approach for Bayesian computation with big data\nis to partition the data, perform local inference for each piece separately,\nand combine the results to obtain a global posterior approximation. While being\nconceptually and computationally appealing, this method involves the\nproblematic need to also split the prior for the local inferences; these\nweakened priors may not provide enough regularization for each separate\ncomputation, thus eliminating one of the key advantages of Bayesian methods. To\nresolve this dilemma while still retaining the generalizability of the\nunderlying local inference method, we apply the idea of expectation propagation\n(EP) as a framework for distributed Bayesian inference. The central idea is to\niteratively update approximations to the local likelihoods given the state of\nthe other approximations and the prior. The present paper has two roles: we\nreview the steps that are needed to keep EP algorithms numerically stable, and\nwe suggest a general approach, inspired by EP, for approaching data\npartitioning problems in a way that achieves the computational benefits of\nparallelism while allowing each local update to make use of relevant\ninformation from the other sites. In addition, we demonstrate how the method\ncan be applied in a hierarchical context to make use of partitioning of both\ndata and parameters. The paper describes a general algorithmic framework,\nrather than a specific algorithm, and presents an example implementation for\nit.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 03:47:38 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 13:06:17 GMT"}, {"version": "v3", "created": "Sat, 10 Mar 2018 21:52:41 GMT"}, {"version": "v4", "created": "Tue, 2 Jul 2019 19:33:01 GMT"}, {"version": "v5", "created": "Sat, 30 Nov 2019 14:11:25 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Vehtari", "Aki", ""], ["Gelman", "Andrew", ""], ["Sivula", "Tuomas", ""], ["Jyl\u00e4nki", "Pasi", ""], ["Tran", "Dustin", ""], ["Sahai", "Swupnil", ""], ["Blomstedt", "Paul", ""], ["Cunningham", "John P.", ""], ["Schiminovich", "David", ""], ["Robert", "Christian", ""]]}, {"id": "1412.5059", "submitter": "Hai Shu", "authors": "Hai Shu and Bin Nan", "title": "Estimation of Large Covariance and Precision Matrices from Temporally\n  Dependent Observations", "comments": "The result for banding estimator of covariance matrix is given in the\n  version 2 of this article. See arXiv:1412.5059v2", "journal-ref": "The Annals of Statistics, 2019, 47(3): 1321-1350", "doi": "10.1214/18-AOS1716", "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the estimation of large covariance and precision matrices from\nhigh-dimensional sub-Gaussian or heavier-tailed observations with slowly\ndecaying temporal dependence. The temporal dependence is allowed to be\nlong-range so with longer memory than those considered in the current\nliterature. We show that several commonly used methods for independent\nobservations can be applied to the temporally dependent data. In particular,\nthe rates of convergence are obtained for the generalized thresholding\nestimation of covariance and correlation matrices, and for the constrained\n$\\ell_1$ minimization and the $\\ell_1$ penalized likelihood estimation of\nprecision matrix. Properties of sparsistency and sign-consistency are also\nestablished. A gap-block cross-validation method is proposed for the tuning\nparameter selection, which performs well in simulations. As a motivating\nexample, we study the brain functional connectivity using resting-state fMRI\ntime series data with long-range temporal dependence.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 16:05:07 GMT"}, {"version": "v2", "created": "Sun, 16 Aug 2015 02:29:34 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2015 20:04:52 GMT"}, {"version": "v4", "created": "Tue, 15 Mar 2016 03:36:05 GMT"}, {"version": "v5", "created": "Tue, 18 Jul 2017 17:51:41 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Shu", "Hai", ""], ["Nan", "Bin", ""]]}, {"id": "1412.5083", "submitter": "Qiang Qiu", "authors": "Qiang Qiu, Guillermo Sapiro, Alex Bronstein", "title": "Random Forests Can Hash", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hash codes are a very efficient data representation needed to be able to cope\nwith the ever growing amounts of data. We introduce a random forest semantic\nhashing scheme with information-theoretic code aggregation, showing for the\nfirst time how random forest, a technique that together with deep learning have\nshown spectacular results in classification, can also be extended to\nlarge-scale retrieval. Traditional random forest fails to enforce the\nconsistency of hashes generated from each tree for the same class data, i.e.,\nto preserve the underlying similarity, and it also lacks a principled way for\ncode aggregation across trees. We start with a simple hashing scheme, where\nindependently trained random trees in a forest are acting as hashing functions.\nWe the propose a subspace model as the splitting function, and show that it\nenforces the hash consistency in a tree for data from the same class. We also\nintroduce an information-theoretic approach for aggregating codes of individual\ntrees into a single hash code, producing a near-optimal unique hash for each\nclass. Experiments on large-scale public datasets are presented, showing that\nthe proposed approach significantly outperforms state-of-the-art hashing\nmethods for retrieval tasks.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 17:02:18 GMT"}, {"version": "v2", "created": "Tue, 24 Feb 2015 18:26:12 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2015 01:00:24 GMT"}], "update_date": "2015-04-20", "authors_parsed": [["Qiu", "Qiang", ""], ["Sapiro", "Guillermo", ""], ["Bronstein", "Alex", ""]]}, {"id": "1412.5158", "submitter": "Han Liu", "authors": "Ethan X. Fang, Yang Ning, Han Liu", "title": "Testing and Confidence Intervals for High Dimensional Proportional\n  Hazards Model", "comments": "42 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a decorrelation-based approach to test hypotheses and\nconstruct confidence intervals for the low dimensional component of high\ndimensional proportional hazards models. Motivated by the geometric projection\nprinciple, we propose new decorrelated score, Wald and partial likelihood ratio\nstatistics. Without assuming model selection consistency, we prove the\nasymptotic normality of these test statistics, establish their semiparametric\noptimality. We also develop new procedures for constructing pointwise\nconfidence intervals for the baseline hazard function and baseline survival\nfunction. Thorough numerical results are provided to back up our theory.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 20:36:26 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Fang", "Ethan X.", ""], ["Ning", "Yang", ""], ["Liu", "Han", ""]]}, {"id": "1412.5218", "submitter": "David Duvenaud", "authors": "Roger B. Grosse and David K. Duvenaud", "title": "Testing MCMC code", "comments": "Presented at the 2014 NIPS workshop on Software Engineering for\n  Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo (MCMC) algorithms are a workhorse of probabilistic\nmodeling and inference, but are difficult to debug, and are prone to silent\nfailure if implemented naively. We outline several strategies for testing the\ncorrectness of MCMC algorithms. Specifically, we advocate writing code in a\nmodular way, where conditional probability calculations are kept separate from\nthe logic of the sampler. We discuss strategies for both unit testing and\nintegration testing. As a running example, we show how a Python implementation\nof Gibbs sampling for a mixture of Gaussians model can be tested.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 22:37:20 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Grosse", "Roger B.", ""], ["Duvenaud", "David K.", ""]]}, {"id": "1412.5236", "submitter": "Andrew Dai", "authors": "Andrew M. Dai, Amos J. Storkey", "title": "The supervised hierarchical Dirichlet process", "comments": "14 pages", "journal-ref": null, "doi": "10.1109/TPAMI.2014.2315802", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the supervised hierarchical Dirichlet process (sHDP), a\nnonparametric generative model for the joint distribution of a group of\nobservations and a response variable directly associated with that whole group.\nWe compare the sHDP with another leading method for regression on grouped data,\nthe supervised latent Dirichlet allocation (sLDA) model. We evaluate our method\non two real-world classification problems and two real-world regression\nproblems. Bayesian nonparametric regression models based on the Dirichlet\nprocess, such as the Dirichlet process-generalised linear models (DP-GLM) have\npreviously been explored; these models allow flexibility in modelling nonlinear\nrelationships. However, until now, Hierarchical Dirichlet Process (HDP)\nmixtures have not seen significant use in supervised problems with grouped data\nsince a straightforward application of the HDP on the grouped data results in\nlearnt clusters that are not predictive of the responses. The sHDP solves this\nproblem by allowing for clusters to be learnt jointly from the group structure\nand from the label assigned to each group.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 01:16:31 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Dai", "Andrew M.", ""], ["Storkey", "Amos J.", ""]]}, {"id": "1412.5244", "submitter": "Yujia Li", "authors": "Yujia Li, Kevin Swersky, Richard Zemel", "title": "Learning unbiased features", "comments": "Published in NIPS 2014 Workshop on Transfer and Multitask Learning,\n  see http://nips.cc/Conferences/2014/Program/event.php?ID=4282", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key element in transfer learning is representation learning; if\nrepresentations can be developed that expose the relevant factors underlying\nthe data, then new tasks and domains can be learned readily based on mappings\nof these salient factors. We propose that an important aim for these\nrepresentations are to be unbiased. Different forms of representation learning\ncan be derived from alternative definitions of unwanted bias, e.g., bias to\nparticular tasks, domains, or irrelevant underlying data dimensions. One very\nuseful approach to estimating the amount of bias in a representation comes from\nmaximum mean discrepancy (MMD) [5], a measure of distance between probability\ndistributions. We are not the first to suggest that MMD can be a useful\ncriterion in developing representations that apply across multiple domains or\ntasks [1]. However, in this paper we describe a number of novel applications of\nthis criterion that we have devised, all based on the idea of developing\nunbiased representations. These formulations include: a standard domain\nadaptation framework; a method of learning invariant representations; an\napproach based on noise-insensitive autoencoders; and a novel form of\ngenerative model.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 02:47:22 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Li", "Yujia", ""], ["Swersky", "Kevin", ""], ["Zemel", "Richard", ""]]}, {"id": "1412.5250", "submitter": "Ines Wilms", "authors": "William B. Nicholson, Ines Wilms, Jacob Bien, David S. Matteson", "title": "High Dimensional Forecasting via Interpretable Vector Autoregression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector autoregression (VAR) is a fundamental tool for modeling multivariate\ntime series. However, as the number of component series is increased, the VAR\nmodel becomes overparameterized. Several authors have addressed this issue by\nincorporating regularized approaches, such as the lasso in VAR estimation.\nTraditional approaches address overparameterization by selecting a low lag\norder, based on the assumption of short range dependence, assuming that a\nuniversal lag order applies to all components. Such an approach constrains the\nrelationship between the components and impedes forecast performance. The\nlasso-based approaches work much better in high-dimensional situations but do\nnot incorporate the notion of lag order selection.\n  We propose a new class of hierarchical lag structures (HLag) that embed the\nnotion of lag selection into a convex regularizer. The key modeling tool is a\ngroup lasso with nested groups which guarantees that the sparsity pattern of\nlag coefficients honors the VAR's ordered structure. The HLag framework offers\nthree structures, which allow for varying levels of flexibility. A simulation\nstudy demonstrates improved performance in forecasting and lag order selection\nover previous approaches, and a macroeconomic application further highlights\nforecasting improvements as well as HLag's convenient, interpretable output.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 03:36:06 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2016 16:55:02 GMT"}, {"version": "v3", "created": "Sat, 8 Sep 2018 17:08:29 GMT"}, {"version": "v4", "created": "Mon, 7 Sep 2020 18:18:55 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Nicholson", "William B.", ""], ["Wilms", "Ines", ""], ["Bien", "Jacob", ""], ["Matteson", "David S.", ""]]}, {"id": "1412.5272", "submitter": "Qiang Wu", "authors": "Jun Fan and Ting Hu and Qiang Wu and Ding-Xuan Zhou", "title": "Consistency Analysis of an Empirical Minimum Error Entropy Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the consistency of an empirical minimum error entropy\n(MEE) algorithm in a regression setting. We introduce two types of consistency.\nThe error entropy consistency, which requires the error entropy of the learned\nfunction to approximate the minimum error entropy, is shown to be always true\nif the bandwidth parameter tends to 0 at an appropriate rate. The regression\nconsistency, which requires the learned function to approximate the regression\nfunction, however, is a complicated issue. We prove that the error entropy\nconsistency implies the regression consistency for homoskedastic models where\nthe noise is independent of the input variable. But for heteroskedastic models,\na counterexample is used to show that the two types of consistency do not\ncoincide. A surprising result is that the regression consistency is always\ntrue, provided that the bandwidth parameter tends to infinity at an appropriate\nrate. Regression consistency of two classes of special models is shown to hold\nwith fixed bandwidth parameter, which further illustrates the complexity of\nregression consistency of MEE. Fourier transform plays crucial roles in our\nanalysis.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 07:11:01 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Fan", "Jun", ""], ["Hu", "Ting", ""], ["Wu", "Qiang", ""], ["Zhou", "Ding-Xuan", ""]]}, {"id": "1412.5632", "submitter": "Po-Ling Loh", "authors": "Po-Ling Loh and Martin J. Wainwright", "title": "Support recovery without incoherence: A case for nonconvex\n  regularization", "comments": "51 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that the primal-dual witness proof method may be used to\nestablish variable selection consistency and $\\ell_\\infty$-bounds for sparse\nregression problems, even when the loss function and/or regularizer are\nnonconvex. Using this method, we derive two theorems concerning support\nrecovery and $\\ell_\\infty$-guarantees for the regression estimator in a general\nsetting. Our results provide rigorous theoretical justification for the use of\nnonconvex regularization: For certain nonconvex regularizers with vanishing\nderivative away from the origin, support recovery consistency may be guaranteed\nwithout requiring the typical incoherence conditions present in $\\ell_1$-based\nmethods. We then derive several corollaries that illustrate the wide\napplicability of our method to analyzing composite objective functions\ninvolving losses such as least squares, nonconvex modified least squares for\nerrors-in variables linear regression, the negative log likelihood for\ngeneralized linear models, and the graphical Lasso. We conclude with empirical\nstudies to corroborate our theoretical predictions.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 21:39:35 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Loh", "Po-Ling", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1412.5675", "submitter": "Ali Heydari", "authors": "Ali Heydari", "title": "Stabilizing Value Iteration with and without Approximation Errors", "comments": "In this revision the proof of Lemma 5 is updated. Initial submission\n  date: 12/17/2014. (This study has overlaps on Theorem 6 and Lemma 5 with\n  another work of the author available at arXiv:1412.6095)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive optimal control using value iteration (VI) initiated from a\nstabilizing policy is theoretically analyzed in various aspects including the\ncontinuity of the result, the stability of the system operated using any\nsingle/constant resulting control policy, the stability of the system operated\nusing the evolving/time-varying control policy, the convergence of the\nalgorithm, and the optimality of the limit function. Afterwards, the effect of\npresence of approximation errors in the involved function approximation\nprocesses is incorporated and another set of results for boundedness of the\napproximate VI as well as stability of the system operated under the results\nfor both cases of applying a single policy or an evolving policy are derived. A\nfeature of the presented results is providing estimations of the region of\nattraction so that if the initial condition is within the region, the whole\ntrajectory will remain inside it and hence, the function approximation results\nwill be reliable.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 23:34:47 GMT"}, {"version": "v2", "created": "Fri, 15 May 2015 18:43:56 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Heydari", "Ali", ""]]}, {"id": "1412.5676", "submitter": "Ali Heydari", "authors": "Ali Heydari", "title": "Optimal Triggering of Networked Control Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of resource allocation of nonlinear networked control systems is\ninvestigated, where, unlike the well discussed case of triggering for\nstability, the objective is optimal triggering. An approximate dynamic\nprogramming approach is developed for solving problems with fixed final times\ninitially and then it is extended to infinite horizon problems. Different cases\nincluding Zero-Order-Hold, Generalized Zero-Order-Hold, and stochastic networks\nare investigated. Afterwards, the developments are extended to the case of\nproblems with unknown dynamics and a model-free scheme is presented for\nlearning the (approximate) optimal solution. After detailed analyses of\nconvergence, optimality, and stability of the results, the performance of the\nmethod is demonstrated through different numerical examples.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 23:36:21 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Heydari", "Ali", ""]]}, {"id": "1412.5744", "submitter": "Richard Golden Professor", "authors": "Richard M. Golden", "title": "Stochastic Descent Analysis of Representation Learning Algorithms", "comments": "Version: April 27, 2015. This paper has been withdrawn by the author\n  because of a minor problem with the proof which has since been corrected. The\n  revised manuscript will eventually be published", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although stochastic approximation learning methods have been widely used in\nthe machine learning literature for over 50 years, formal theoretical analyses\nof specific machine learning algorithms are less common because stochastic\napproximation theorems typically possess assumptions which are difficult to\ncommunicate and verify. This paper presents a new stochastic approximation\ntheorem for state-dependent noise with easily verifiable assumptions applicable\nto the analysis and design of important deep learning algorithms including:\nadaptive learning, contrastive divergence learning, stochastic descent\nexpectation maximization, and active learning.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 07:51:24 GMT"}, {"version": "v2", "created": "Fri, 9 Jan 2015 20:05:38 GMT"}, {"version": "v3", "created": "Sun, 22 Feb 2015 00:12:42 GMT"}, {"version": "v4", "created": "Tue, 24 Mar 2015 14:39:33 GMT"}, {"version": "v5", "created": "Tue, 7 Apr 2015 14:43:31 GMT"}, {"version": "v6", "created": "Mon, 27 Apr 2015 15:56:42 GMT"}, {"version": "v7", "created": "Wed, 19 Apr 2017 21:17:10 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Golden", "Richard M.", ""]]}, {"id": "1412.5896", "submitter": "Raja Giryes", "authors": "Raja Giryes and Guillermo Sapiro and Alex M. Bronstein", "title": "On the Stability of Deep Networks", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG cs.NE math.IT math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study the properties of deep neural networks (DNN) with\nrandom weights. We formally prove that these networks perform a\ndistance-preserving embedding of the data. Based on this we then draw\nconclusions on the size of the training data and the networks' structure. A\nlonger version of this paper with more results and details can be found in\n(Giryes et al., 2015). In particular, we formally prove in the longer version\nthat DNN with random Gaussian weights perform a distance-preserving embedding\nof the data, with a special treatment for in-class and out-of-class data.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 15:40:03 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2015 20:52:41 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2015 14:38:57 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Giryes", "Raja", ""], ["Sapiro", "Guillermo", ""], ["Bronstein", "Alex M.", ""]]}, {"id": "1412.5967", "submitter": "Andrew Lan", "authors": "Andrew S. Lan, Christoph Studer, Andrew E. Waters, Richard G. Baraniuk", "title": "Tag-Aware Ordinal Sparse Factor Analysis for Learning and Content\n  Analytics", "comments": null, "journal-ref": "In Proc. 6th Intl. Conf. on Educational Data Mining, pages 90-97,\n  July 2013", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning offers novel ways and means to design personalized learning\nsystems wherein each student's educational experience is customized in real\ntime depending on their background, learning goals, and performance to date.\nSPARse Factor Analysis (SPARFA) is a novel framework for machine learning-based\nlearning analytics, which estimates a learner's knowledge of the concepts\nunderlying a domain, and content analytics, which estimates the relationships\namong a collection of questions and those concepts. SPARFA jointly learns the\nassociations among the questions and the concepts, learner concept knowledge\nprofiles, and the underlying question difficulties, solely based on the\ncorrect/incorrect graded responses of a population of learners to a collection\nof questions. In this paper, we extend the SPARFA framework significantly to\nenable: (i) the analysis of graded responses on an ordinal scale (partial\ncredit) rather than a binary scale (correct/incorrect); (ii) the exploitation\nof tags/labels for questions that partially describe the question{concept\nassociations. The resulting Ordinal SPARFA-Tag framework greatly enhances the\ninterpretability of the estimated concepts. We demonstrate using real\neducational data that Ordinal SPARFA-Tag outperforms both SPARFA and existing\ncollaborative filtering techniques in predicting missing learner responses.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 17:46:41 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Lan", "Andrew S.", ""], ["Studer", "Christoph", ""], ["Waters", "Andrew E.", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1412.5968", "submitter": "Andrew Lan", "authors": "Andrew S. Lan, Christoph Studer, Richard G. Baraniuk", "title": "Quantized Matrix Completion for Personalized Learning", "comments": null, "journal-ref": "In Proc. 7th Intl. Conf. on Educational Data Mining, pages\n  280-283, July 2014", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed SPARse Factor Analysis (SPARFA) framework for\npersonalized learning performs factor analysis on ordinal or binary-valued\n(e.g., correct/incorrect) graded learner responses to questions. The underlying\nfactors are termed \"concepts\" (or knowledge components) and are used for\nlearning analytics (LA), the estimation of learner concept-knowledge profiles,\nand for content analytics (CA), the estimation of question-concept associations\nand question difficulties. While SPARFA is a powerful tool for LA and CA, it\nrequires a number of algorithm parameters (including the number of concepts),\nwhich are difficult to determine in practice. In this paper, we propose\nSPARFA-Lite, a convex optimization-based method for LA that builds on matrix\ncompletion, which only requires a single algorithm parameter and enables us to\nautomatically identify the required number of concepts. Using a variety of\neducational datasets, we demonstrate that SPARFALite (i) achieves comparable\nperformance in predicting unobserved learner responses to existing methods,\nincluding item response theory (IRT) and SPARFA, and (ii) is computationally\nmore efficient.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 17:48:17 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Lan", "Andrew S.", ""], ["Studer", "Christoph", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1412.6039", "submitter": "Xin Yuan", "authors": "Yunchen Pu, Xin Yuan and Lawrence Carin", "title": "Generative Deep Deconvolutional Learning", "comments": "21 pages, 9 figures, revised version for ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generative Bayesian model is developed for deep (multi-layer) convolutional\ndictionary learning. A novel probabilistic pooling operation is integrated into\nthe deep model, yielding efficient bottom-up and top-down probabilistic\nlearning. After learning the deep convolutional dictionary, testing is\nimplemented via deconvolutional inference. To speed up this inference, a new\nstatistical approach is proposed to project the top-layer dictionary elements\nto the data level. Following this, only one layer of deconvolution is required\nduring testing. Experimental results demonstrate powerful capabilities of the\nmodel to learn multi-layer features from images. Excellent classification\nresults are obtained on both the MNIST and Caltech 101 datasets.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 20:01:38 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 17:21:36 GMT"}, {"version": "v3", "created": "Sun, 22 Feb 2015 18:13:29 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Pu", "Yunchen", ""], ["Yuan", "Xin", ""], ["Carin", "Lawrence", ""]]}, {"id": "1412.6095", "submitter": "Ali Heydari", "authors": "Ali Heydari", "title": "Theoretical and Numerical Analysis of Approximate Dynamic Programming\n  with Approximation Errors", "comments": "This study is the counterpart of another work of the author\n  (arXiv:1412.5675) which was for value iterations with initial stabilizing\n  guess (with overlaps on Theorem 1 and Lemma 1). As for the revision on this\n  work, some steps of proofs are updated and an explanation about the\n  approximation error is included. Initial submission date: 12/18/2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study is aimed at answering the famous question of how the approximation\nerrors at each iteration of Approximate Dynamic Programming (ADP) affect the\nquality of the final results considering the fact that errors at each iteration\naffect the next iteration. To this goal, convergence of Value Iteration scheme\nof ADP for deterministic nonlinear optimal control problems with undiscounted\ncost functions is investigated while considering the errors existing in\napproximating respective functions. The boundedness of the results around the\noptimal solution is obtained based on quantities which are known in a general\noptimal control problem and assumptions which are verifiable. Moreover, since\nthe presence of the approximation errors leads to the deviation of the results\nfrom optimality, sufficient conditions for stability of the system operated by\nthe result obtained after a finite number of value iterations, along with an\nestimation of its region of attraction, are derived in terms of a calculable\nupper bound of the control approximation error. Finally, the process of\nimplementation of the method on an orbital maneuver problem is investigated\nthrough which the assumptions made in the theoretical developments are verified\nand the sufficient conditions are applied for guaranteeing stability and near\noptimality.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 16:38:10 GMT"}, {"version": "v2", "created": "Tue, 23 Dec 2014 22:09:05 GMT"}, {"version": "v3", "created": "Fri, 15 May 2015 18:41:08 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Heydari", "Ali", ""]]}, {"id": "1412.6134", "submitter": "Qiang Qiu", "authors": "Qiang Qiu, Andrew Thompson, Robert Calderbank, Guillermo Sapiro", "title": "Data Representation using the Weyl Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Weyl transform is introduced as a rich framework for data representation.\nTransform coefficients are connected to the Walsh-Hadamard transform of\nmultiscale autocorrelations, and different forms of dyadic periodicity in a\nsignal are shown to appear as different features in its Weyl coefficients. The\nWeyl transform has a high degree of symmetry with respect to a large group of\nmultiscale transformations, which allows compact yet discriminative\nrepresentations to be obtained by pooling coefficients. The effectiveness of\nthe Weyl transform is demonstrated through the example of textured image\nclassification.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 21:34:40 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2015 18:19:32 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2015 01:03:43 GMT"}, {"version": "v4", "created": "Mon, 20 Jul 2015 14:11:54 GMT"}, {"version": "v5", "created": "Tue, 21 Jul 2015 14:12:37 GMT"}], "update_date": "2015-07-22", "authors_parsed": [["Qiu", "Qiang", ""], ["Thompson", "Andrew", ""], ["Calderbank", "Robert", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1412.6156", "submitter": "Jiaming Xu", "authors": "Bruce Hajek and Yihong Wu and Jiaming Xu", "title": "Achieving Exact Cluster Recovery Threshold via Semidefinite Programming", "comments": "This paper was accepted to IEEE Transactions on Information Theory on\n  January 3, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The binary symmetric stochastic block model deals with a random graph of $n$\nvertices partitioned into two equal-sized clusters, such that each pair of\nvertices is connected independently with probability $p$ within clusters and\n$q$ across clusters. In the asymptotic regime of $p=a \\log n/n$ and $q=b \\log\nn/n$ for fixed $a,b$ and $n \\to \\infty$, we show that the semidefinite\nprogramming relaxation of the maximum likelihood estimator achieves the optimal\nthreshold for exactly recovering the partition from the graph with probability\ntending to one, resolving a conjecture of Abbe et al. \\cite{Abbe14}.\nFurthermore, we show that the semidefinite programming relaxation also achieves\nthe optimal recovery threshold in the planted dense subgraph model containing a\nsingle cluster of size proportional to $n$.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 05:19:56 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2016 23:47:57 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Hajek", "Bruce", ""], ["Wu", "Yihong", ""], ["Xu", "Jiaming", ""]]}, {"id": "1412.6177", "submitter": "Tomoki Tsuchida", "authors": "Tomoki Tsuchida and Garrison W. Cottrell", "title": "Example Selection For Dictionary Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In unsupervised learning, an unbiased uniform sampling strategy is typically\nused, in order that the learned features faithfully encode the statistical\nstructure of the training data. In this work, we explore whether active example\nselection strategies - algorithms that select which examples to use, based on\nthe current estimate of the features - can accelerate learning. Specifically,\nwe investigate effects of heuristic and saliency-inspired selection algorithms\non the dictionary learning task with sparse activations. We show that some\nselection algorithms do improve the speed of learning, and we speculate on why\nthey might work.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 23:25:22 GMT"}, {"version": "v2", "created": "Mon, 29 Dec 2014 23:28:16 GMT"}, {"version": "v3", "created": "Tue, 31 Mar 2015 19:22:18 GMT"}], "update_date": "2015-04-01", "authors_parsed": [["Tsuchida", "Tomoki", ""], ["Cottrell", "Garrison W.", ""]]}, {"id": "1412.6219", "submitter": "Kevin H. Knuth", "authors": "Kevin H. Knuth, Deniz Gen\\c{c}a\\u{g}a, William B. Rossow", "title": "Information-Theoretic Methods for Identifying Relationships among\n  Climate Variables", "comments": "Presented at the Earth-Sun System Technology Conference (ESTC 2008),\n  Adelphi, MD. http://esto.nasa.gov/conferences/estc2008/ 3 pages, 3 figures.\n  Appears in the Proceedings of the Earth-Sun System Technology Conference\n  (ESTC 2008), Adelphi, MD", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an physics.ao-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information-theoretic quantities, such as entropy, are used to quantify the\namount of information a given variable provides. Entropies can be used together\nto compute the mutual information, which quantifies the amount of information\ntwo variables share. However, accurately estimating these quantities from data\nis extremely challenging. We have developed a set of computational techniques\nthat allow one to accurately compute marginal and joint entropies. These\nalgorithms are probabilistic in nature and thus provide information on the\nuncertainty in our estimates, which enable us to establish statistical\nsignificance of our findings. We demonstrate these methods by identifying\nrelations between cloud data from the International Satellite Cloud Climatology\nProject (ISCCP) and data from other sources, such as equatorial pacific sea\nsurface temperatures (SST).\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 05:22:07 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Knuth", "Kevin H.", ""], ["Gen\u00e7a\u011fa", "Deniz", ""], ["Rossow", "William B.", ""]]}, {"id": "1412.6285", "submitter": "Gianluca Bontempi", "authors": "Gianluca Bontempi and Maxime Flauder", "title": "From dependency to causality: a machine learning approach", "comments": "submitted to JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relationship between statistical dependency and causality lies at the\nheart of all statistical approaches to causal inference. Recent results in the\nChaLearn cause-effect pair challenge have shown that causal directionality can\nbe inferred with good accuracy also in Markov indistinguishable configurations\nthanks to data driven approaches. This paper proposes a supervised machine\nlearning approach to infer the existence of a directed causal link between two\nvariables in multivariate settings with $n>2$ variables. The approach relies on\nthe asymmetry of some conditional (in)dependence relations between the members\nof the Markov blankets of two variables causally connected. Our results show\nthat supervised learning methods may be successfully used to extract causal\ninformation on the basis of asymmetric statistical descriptors also for $n>2$\nvariate distributions.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 10:50:14 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Bontempi", "Gianluca", ""], ["Flauder", "Maxime", ""]]}, {"id": "1412.6286", "submitter": "Wendelin B\\\"ohmer", "authors": "Wendelin B\\\"ohmer and Klaus Obermayer", "title": "Regression with Linear Factored Functions", "comments": "Under review as conference paper at ECML/PKDD 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications that use empirically estimated functions face a curse of\ndimensionality, because the integrals over most function classes must be\napproximated by sampling. This paper introduces a novel regression-algorithm\nthat learns linear factored functions (LFF). This class of functions has\nstructural properties that allow to analytically solve certain integrals and to\ncalculate point-wise products. Applications like belief propagation and\nreinforcement learning can exploit these properties to break the curse and\nspeed up computation. We derive a regularized greedy optimization scheme, that\nlearns factored basis functions during training. The novel regression algorithm\nperforms competitively to Gaussian processes on benchmark tasks, and the\nlearned LFF functions are with 4-9 factored basis functions on average very\ncompact.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 11:01:21 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 14:53:35 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2015 15:14:20 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["B\u00f6hmer", "Wendelin", ""], ["Obermayer", "Klaus", ""]]}, {"id": "1412.6388", "submitter": "Ozan \\.Irsoy", "authors": "Ozan \\.Irsoy, Ethem Alpayd{\\i}n", "title": "Distributed Decision Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently proposed budding tree is a decision tree algorithm in which every\nnode is part internal node and part leaf. This allows representing every\ndecision tree in a continuous parameter space, and therefore a budding tree can\nbe jointly trained with backpropagation, like a neural network. Even though\nthis continuity allows it to be used in hierarchical representation learning,\nthe learned representations are local: Activation makes a soft selection among\nall root-to-leaf paths in a tree. In this work we extend the budding tree and\npropose the distributed tree where the children use different and independent\nsplits and hence multiple paths in a tree can be traversed at the same time.\nThis ability to combine multiple paths gives the power of a distributed\nrepresentation, as in a traditional perceptron layer. We show that distributed\ntrees perform comparably or better than budding and traditional hard trees on\nclassification and regression tasks.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 15:44:02 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["\u0130rsoy", "Ozan", ""], ["Alpayd\u0131n", "Ethem", ""]]}, {"id": "1412.6418", "submitter": "Ehsan Khoddam Mohammadi", "authors": "Ivan Titov and Ehsan Khoddam", "title": "Inducing Semantic Representation from Text by Jointly Predicting and\n  Factorizing Relations", "comments": "Accepted as a workshop contribution at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a new method to integrate two recent lines of work:\nunsupervised induction of shallow semantics (e.g., semantic roles) and\nfactorization of relations in text and knowledge bases. Our model consists of\ntwo components: (1) an encoding component: a semantic role labeling model which\npredicts roles given a rich set of syntactic and lexical features; (2) a\nreconstruction component: a tensor factorization model which relies on roles to\npredict argument fillers. When the components are estimated jointly to minimize\nerrors in argument reconstruction, the induced roles largely correspond to\nroles defined in annotated resources. Our method performs on par with most\naccurate role induction methods on English, even though, unlike these previous\napproaches, we do not incorporate any prior linguistic knowledge about the\nlanguage.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 16:30:33 GMT"}, {"version": "v2", "created": "Thu, 26 Feb 2015 12:16:56 GMT"}, {"version": "v3", "created": "Thu, 16 Apr 2015 10:24:27 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Titov", "Ivan", ""], ["Khoddam", "Ehsan", ""]]}, {"id": "1412.6493", "submitter": "Zichao Yang", "authors": "Zichao Yang and Alexander J. Smola and Le Song and Andrew Gordon\n  Wilson", "title": "A la Carte - Learning Fast Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel methods have great promise for learning rich statistical\nrepresentations of large modern datasets. However, compared to neural networks,\nkernel methods have been perceived as lacking in scalability and flexibility.\nWe introduce a family of fast, flexible, lightly parametrized and general\npurpose kernel learning methods, derived from Fastfood basis function\nexpansions. We provide mechanisms to learn the properties of groups of spectral\nfrequencies in these expansions, which require only O(mlogd) time and O(m)\nmemory, for m basis functions and d input dimensions. We show that the proposed\nmethods can learn a wide class of kernels, outperforming the alternatives in\naccuracy, speed, and memory consumption.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 19:27:21 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Yang", "Zichao", ""], ["Smola", "Alexander J.", ""], ["Song", "Le", ""], ["Wilson", "Andrew Gordon", ""]]}, {"id": "1412.6506", "submitter": "Pengtao Xie", "authors": "Pengtao Xie and Eric Xing", "title": "Cauchy Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal Component Analysis (PCA) has wide applications in machine learning,\ntext mining and computer vision. Classical PCA based on a Gaussian noise model\nis fragile to noise of large magnitude. Laplace noise assumption based PCA\nmethods cannot deal with dense noise effectively. In this paper, we propose\nCauchy Principal Component Analysis (Cauchy PCA), a very simple yet effective\nPCA method which is robust to various types of noise. We utilize Cauchy\ndistribution to model noise and derive Cauchy PCA under the maximum likelihood\nestimation (MLE) framework with low rank constraint. Our method can robustly\nestimate the low rank matrix regardless of whether noise is large or small,\ndense or sparse. We analyze the robustness of Cauchy PCA from a robust\nstatistics view and present an efficient singular value projection optimization\nmethod. Experimental results on both simulated data and real applications\ndemonstrate the robustness of Cauchy PCA to various noise patterns.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 20:06:02 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Xie", "Pengtao", ""], ["Xing", "Eric", ""]]}, {"id": "1412.6514", "submitter": "Majid Janzamin", "authors": "Majid Janzamin and Hanie Sedghi and Anima Anandkumar", "title": "Score Function Features for Discriminative Learning", "comments": "Accepted as a workshop contribution at ICLR 2015. A longer version of\n  this work is also available on arXiv: http://arxiv.org/abs/1412.2863", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature learning forms the cornerstone for tackling challenging learning\nproblems in domains such as speech, computer vision and natural language\nprocessing. In this paper, we consider a novel class of matrix and\ntensor-valued features, which can be pre-trained using unlabeled samples. We\npresent efficient algorithms for extracting discriminative information, given\nthese pre-trained features and labeled samples for any related task. Our class\nof features are based on higher-order score functions, which capture local\nvariations in the probability density function of the input. We establish a\ntheoretical framework to characterize the nature of discriminative information\nthat can be extracted from score-function features, when used in conjunction\nwith labeled samples. We employ efficient spectral decomposition algorithms (on\nmatrices and tensors) for extracting discriminative components. The advantage\nof employing tensor-valued features is that we can extract richer\ndiscriminative information in the form of an overcomplete representations.\nThus, we present a novel framework for employing generative models of the input\nfor discriminative learning.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 20:18:36 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2015 18:46:19 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Janzamin", "Majid", ""], ["Sedghi", "Hanie", ""], ["Anandkumar", "Anima", ""]]}, {"id": "1412.6515", "submitter": "Ian Goodfellow", "authors": "Ian J. Goodfellow", "title": "On distinguishability criteria for estimating generative models", "comments": "This version adds a figure that appeared on the poster at ICLR,\n  changes the template to say that the paper was accepted as a workshop\n  contribution (previously it was under a review as a conference submission),\n  and fixes some typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two recently introduced criteria for estimation of generative models are both\nbased on a reduction to binary classification. Noise-contrastive estimation\n(NCE) is an estimation procedure in which a generative model is trained to be\nable to distinguish data samples from noise samples. Generative adversarial\nnetworks (GANs) are pairs of generator and discriminator networks, with the\ngenerator network learning to generate samples by attempting to fool the\ndiscriminator network into believing its samples are real data. Both estimation\nprocedures use the same function to drive learning, which naturally raises\nquestions about how they are related to each other, as well as whether this\nfunction is related to maximum likelihood estimation (MLE). NCE corresponds to\ntraining an internal data model belonging to the {\\em discriminator} network\nbut using a fixed generator network. We show that a variant of NCE, with a\ndynamic generator network, is equivalent to maximum likelihood estimation.\nSince pairing a learned discriminator with an appropriate dynamically selected\ngenerator recovers MLE, one might expect the reverse to hold for pairing a\nlearned generator with a certain discriminator. However, we show that\nrecovering MLE for a learned generator requires departing from the\ndistinguishability game. Specifically:\n  (i) The expected gradient of the NCE discriminator can be made to match the\nexpected gradient of\n  MLE, if one is allowed to use a non-stationary noise distribution for NCE,\n  (ii) No choice of discriminator network can make the expected gradient for\nthe GAN generator match that of MLE, and\n  (iii) The existing theory does not guarantee that GANs will converge in the\nnon-convex case.\n  This suggests that the key next step in GAN research is to determine whether\nGANs converge, and if not, to modify their training algorithm to force\nconvergence.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 20:28:41 GMT"}, {"version": "v2", "created": "Wed, 25 Feb 2015 01:56:15 GMT"}, {"version": "v3", "created": "Fri, 20 Mar 2015 20:23:47 GMT"}, {"version": "v4", "created": "Thu, 21 May 2015 15:52:10 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Goodfellow", "Ian J.", ""]]}, {"id": "1412.6534", "submitter": "Alan Wisler", "authors": "Visar Berisha, Alan Wisler, Alfred O. Hero, and Andreas Spanias", "title": "Empirically Estimable Classification Bounds Based on a New Divergence\n  Measure", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information divergence functions play a critical role in statistics and\ninformation theory. In this paper we show that a non-parametric f-divergence\nmeasure can be used to provide improved bounds on the minimum binary\nclassification probability of error for the case when the training and test\ndata are drawn from the same distribution and for the case where there exists\nsome mismatch between training and test distributions. We confirm the\ntheoretical results by designing feature selection algorithms using the\ncriteria from these bounds and by evaluating the algorithms on a series of\npathological speech classification tasks.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 21:01:57 GMT"}, {"version": "v2", "created": "Tue, 10 Feb 2015 19:23:18 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Berisha", "Visar", ""], ["Wisler", "Alan", ""], ["Hero", "Alfred O.", ""], ["Spanias", "Andreas", ""]]}, {"id": "1412.6544", "submitter": "Ian Goodfellow", "authors": "Ian J. Goodfellow, Oriol Vinyals, and Andrew M. Saxe", "title": "Qualitatively characterizing neural network optimization problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training neural networks involves solving large-scale non-convex optimization\nproblems. This task has long been believed to be extremely difficult, with fear\nof local minima and other obstacles motivating a variety of schemes to improve\noptimization, such as unsupervised pretraining. However, modern neural networks\nare able to achieve negligible training error on complex tasks, using only\ndirect training with stochastic gradient descent. We introduce a simple\nanalysis technique to look for evidence that such networks are overcoming local\noptima. We find that, in fact, on a straight path from initialization to\nsolution, a variety of state of the art neural networks never encounter any\nsignificant obstacles.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 21:55:01 GMT"}, {"version": "v2", "created": "Tue, 23 Dec 2014 23:59:32 GMT"}, {"version": "v3", "created": "Sat, 28 Feb 2015 05:25:15 GMT"}, {"version": "v4", "created": "Mon, 9 Mar 2015 18:28:17 GMT"}, {"version": "v5", "created": "Fri, 20 Mar 2015 20:08:54 GMT"}, {"version": "v6", "created": "Thu, 21 May 2015 21:44:31 GMT"}], "update_date": "2015-05-25", "authors_parsed": [["Goodfellow", "Ian J.", ""], ["Vinyals", "Oriol", ""], ["Saxe", "Andrew M.", ""]]}, {"id": "1412.6558", "submitter": "David Sussillo", "authors": "David Sussillo, L.F. Abbott", "title": "Random Walk Initialization for Training Very Deep Feedforward Networks", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training very deep networks is an important open problem in machine learning.\nOne of many difficulties is that the norm of the back-propagated error gradient\ncan grow or decay exponentially. Here we show that training very deep\nfeed-forward networks (FFNs) is not as difficult as previously thought. Unlike\nwhen back-propagation is applied to a recurrent network, application to an FFN\namounts to multiplying the error gradient by a different random matrix at each\nlayer. We show that the successive application of correctly scaled random\nmatrices to an initial vector results in a random walk of the log of the norm\nof the resulting vectors, and we compute the scaling that makes this walk\nunbiased. The variance of the random walk grows only linearly with network\ndepth and is inversely proportional to the size of each layer. Practically,\nthis implies a gradient whose log-norm scales with the square root of the\nnetwork depth and shows that the vanishing gradient problem can be mitigated by\nincreasing the width of the layers. Mathematical analyses and experimental\nresults using stochastic gradient descent to optimize tasks related to the\nMNIST and TIMIT datasets are provided to support these claims. Equations for\nthe optimal matrix scaling are provided for the linear and ReLU cases.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 23:24:53 GMT"}, {"version": "v2", "created": "Wed, 14 Jan 2015 21:28:29 GMT"}, {"version": "v3", "created": "Fri, 27 Feb 2015 22:28:32 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Sussillo", "David", ""], ["Abbott", "L. F.", ""]]}, {"id": "1412.6563", "submitter": "David Warde-Farley", "authors": "David Warde-Farley, Andrew Rabinovich, Dragomir Anguelov", "title": "Self-informed neural network structure learning", "comments": "Updated with accepted workshop contribution header", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of large scale, multi-label visual recognition with a\nlarge number of possible classes. We propose a method for augmenting a trained\nneural network classifier with auxiliary capacity in a manner designed to\nsignificantly improve upon an already well-performing model, while minimally\nimpacting its computational footprint. Using the predictions of the network\nitself as a descriptor for assessing visual similarity, we define a\npartitioning of the label space into groups of visually similar entities. We\nthen augment the network with auxilliary hidden layer pathways with\nconnectivity only to these groups of label units. We report a significant\nimprovement in mean average precision on a large-scale object recognition task\nwith the augmented model, while increasing the number of multiply-adds by less\nthan 3%.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 00:05:57 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2015 21:35:29 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Warde-Farley", "David", ""], ["Rabinovich", "Andrew", ""], ["Anguelov", "Dragomir", ""]]}, {"id": "1412.6565", "submitter": "Panayotis Mertikopoulos", "authors": "Mario Bravo and Panayotis Mertikopoulos", "title": "On the robustness of learning in games with stochastically perturbed\n  payoff observations", "comments": "36 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.GT math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the scarcity of accurate payoff feedback in practical\napplications of game theory, we examine a class of learning dynamics where\nplayers adjust their choices based on past payoff observations that are subject\nto noise and random disturbances. First, in the single-player case\n(corresponding to an agent trying to adapt to an arbitrarily changing\nenvironment), we show that the stochastic dynamics under study lead to no\nregret almost surely, irrespective of the noise level in the player's\nobservations. In the multi-player case, we find that dominated strategies\nbecome extinct and we show that strict Nash equilibria are stochastically\nstable and attracting; conversely, if a state is stable or attracting with\npositive probability, then it is a Nash equilibrium. Finally, we provide an\naveraging principle for 2-player games, and we show that in zero-sum games with\nan interior equilibrium, time averages converge to Nash equilibrium for any\nnoise level.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 00:42:59 GMT"}, {"version": "v2", "created": "Thu, 2 Jun 2016 17:21:22 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Bravo", "Mario", ""], ["Mertikopoulos", "Panayotis", ""]]}, {"id": "1412.6572", "submitter": "Ian Goodfellow", "authors": "Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy", "title": "Explaining and Harnessing Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several machine learning models, including neural networks, consistently\nmisclassify adversarial examples---inputs formed by applying small but\nintentionally worst-case perturbations to examples from the dataset, such that\nthe perturbed input results in the model outputting an incorrect answer with\nhigh confidence. Early attempts at explaining this phenomenon focused on\nnonlinearity and overfitting. We argue instead that the primary cause of neural\nnetworks' vulnerability to adversarial perturbation is their linear nature.\nThis explanation is supported by new quantitative results while giving the\nfirst explanation of the most intriguing fact about them: their generalization\nacross architectures and training sets. Moreover, this view yields a simple and\nfast method of generating adversarial examples. Using this approach to provide\nexamples for adversarial training, we reduce the test set error of a maxout\nnetwork on the MNIST dataset.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 01:17:12 GMT"}, {"version": "v2", "created": "Wed, 25 Feb 2015 17:25:05 GMT"}, {"version": "v3", "created": "Fri, 20 Mar 2015 20:19:16 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Goodfellow", "Ian J.", ""], ["Shlens", "Jonathon", ""], ["Szegedy", "Christian", ""]]}, {"id": "1412.6577", "submitter": "Ozan \\.Irsoy", "authors": "Ozan \\.Irsoy, Claire Cardie", "title": "Modeling Compositionality with Multiplicative Recurrent Neural Networks", "comments": "10 pages, 2 figures, published at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the multiplicative recurrent neural network as a general model for\ncompositional meaning in language, and evaluate it on the task of fine-grained\nsentiment analysis. We establish a connection to the previously investigated\nmatrix-space models for compositionality, and show they are special cases of\nthe multiplicative recurrent net. Our experiments show that these models\nperform comparably or better than Elman-type additive recurrent neural networks\nand outperform matrix-space models on a standard fine-grained sentiment\nanalysis corpus. Furthermore, they yield comparable results to structural deep\nmodels on the recently published Stanford Sentiment Treebank without the need\nfor generating parse trees.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 01:53:22 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2015 08:35:22 GMT"}, {"version": "v3", "created": "Sat, 2 May 2015 20:22:32 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["\u0130rsoy", "Ozan", ""], ["Cardie", "Claire", ""]]}, {"id": "1412.6581", "submitter": "Joost van Amersfoort B.Sc.", "authors": "Otto Fabius, Joost R. van Amersfoort", "title": "Variational Recurrent Auto-Encoders", "comments": "Accepted at ICLR workshop track", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a model that combines the strengths of RNNs and\nSGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used\nfor efficient, large scale unsupervised learning on time series data, mapping\nthe time series data to a latent vector representation. The model is\ngenerative, such that data can be generated from samples of the latent space.\nAn important contribution of this work is that the model can make use of\nunlabeled data in order to facilitate supervised training of RNNs by\ninitialising the weights and network state.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 02:07:07 GMT"}, {"version": "v2", "created": "Tue, 23 Dec 2014 15:04:29 GMT"}, {"version": "v3", "created": "Wed, 4 Feb 2015 11:21:39 GMT"}, {"version": "v4", "created": "Wed, 18 Feb 2015 15:42:44 GMT"}, {"version": "v5", "created": "Wed, 15 Apr 2015 14:01:18 GMT"}, {"version": "v6", "created": "Mon, 15 Jun 2015 12:35:11 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Fabius", "Otto", ""], ["van Amersfoort", "Joost R.", ""]]}, {"id": "1412.6586", "submitter": "Alexander Wong", "authors": "Alexander Wong, Mohammad Javad Shafiee, Parthipan Siva, and Xiao Yu\n  Wang", "title": "A deep-structured fully-connected random field model for structured\n  inference", "comments": "Accepted, 13 pages", "journal-ref": "IEEE Access Journal, vol. 3, pp. 469-477, 2015", "doi": "10.1109/ACCESS.2015.2425304", "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been significant interest in the use of fully-connected graphical\nmodels and deep-structured graphical models for the purpose of structured\ninference. However, fully-connected and deep-structured graphical models have\nbeen largely explored independently, leaving the unification of these two\nconcepts ripe for exploration. A fundamental challenge with unifying these two\ntypes of models is in dealing with computational complexity. In this study, we\ninvestigate the feasibility of unifying fully-connected and deep-structured\nmodels in a computationally tractable manner for the purpose of structured\ninference. To accomplish this, we introduce a deep-structured fully-connected\nrandom field (DFRF) model that integrates a series of intermediate sparse\nauto-encoding layers placed between state layers to significantly reduce\ncomputational complexity. The problem of image segmentation was used to\nillustrate the feasibility of using the DFRF for structured inference in a\ncomputationally tractable manner. Results in this study show that it is\nfeasible to unify fully-connected and deep-structured models in a\ncomputationally tractable manner for solving structured inference problems such\nas image segmentation.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 03:02:32 GMT"}, {"version": "v2", "created": "Mon, 12 Jan 2015 21:34:22 GMT"}, {"version": "v3", "created": "Wed, 27 May 2015 23:05:49 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Wong", "Alexander", ""], ["Shafiee", "Mohammad Javad", ""], ["Siva", "Parthipan", ""], ["Wang", "Xiao Yu", ""]]}, {"id": "1412.6602", "submitter": "Richard Golden Professor", "authors": "Shaurabh Nandy and Richard M. Golden", "title": "Generative Modeling of Hidden Functional Brain Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional connectivity refers to the temporal statistical relationship\nbetween spatially distinct brain regions and is usually inferred from the time\nseries coherence/correlation in brain activity between regions of interest. In\nhuman functional brain networks, the network structure is often inferred from\nfunctional magnetic resonance imaging (fMRI) blood oxygen level dependent\n(BOLD) signal. Since the BOLD signal is a proxy for neuronal activity, it is of\ninterest to learn the latent functional network structure. Additionally,\ndespite a core set of observations about functional networks such as\nsmall-worldness, modularity, exponentially truncated degree distributions, and\npresence of various types of hubs, very little is known about the computational\nprinciples which can give rise to these observations. This paper introduces a\nHidden Markov Random Field framework for the purpose of representing,\nestimating, and evaluating latent neuronal functional relationships between\ndifferent brain regions using fMRI data.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 04:52:54 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 15:12:30 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Nandy", "Shaurabh", ""], ["Golden", "Richard M.", ""]]}, {"id": "1412.6606", "submitter": "Roy Frostig", "authors": "Roy Frostig, Rong Ge, Sham M. Kakade, Aaron Sidford", "title": "Competing with the Empirical Risk Minimizer in a Single Pass", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many estimation problems, e.g. linear and logistic regression, we wish to\nminimize an unknown objective given only unbiased samples of the objective\nfunction. Furthermore, we aim to achieve this using as few samples as possible.\nIn the absence of computational constraints, the minimizer of a sample average\nof observed data -- commonly referred to as either the empirical risk minimizer\n(ERM) or the $M$-estimator -- is widely regarded as the estimation strategy of\nchoice due to its desirable statistical convergence properties. Our goal in\nthis work is to perform as well as the ERM, on every problem, while minimizing\nthe use of computational resources such as running time and space usage.\n  We provide a simple streaming algorithm which, under standard regularity\nassumptions on the underlying problem, enjoys the following properties:\n  * The algorithm can be implemented in linear time with a single pass of the\nobserved data, using space linear in the size of a single sample.\n  * The algorithm achieves the same statistical rate of convergence as the\nempirical risk minimizer on every problem, even considering constant factors.\n  * The algorithm's performance depends on the initial error at a rate that\ndecreases super-polynomially.\n  * The algorithm is easily parallelizable.\n  Moreover, we quantify the (finite-sample) rate at which the algorithm becomes\ncompetitive with the ERM.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 05:14:13 GMT"}, {"version": "v2", "created": "Wed, 25 Feb 2015 19:13:05 GMT"}], "update_date": "2015-02-26", "authors_parsed": [["Frostig", "Roy", ""], ["Ge", "Rong", ""], ["Kakade", "Sham M.", ""], ["Sidford", "Aaron", ""]]}, {"id": "1412.6612", "submitter": "Christian Despres", "authors": "Christian J. J. Despres", "title": "The Vapnik-Chervonenkis dimension of cubes in $\\mathbb{R}^d$", "comments": "Derived from Fall 2014 Honours research project done under the\n  supervision of Dr. Vladimir Pestov at the University of Ottawa; 4 pages;\n  significantly simplified the constructions, removed the final two sections\n  (which did not fit well with the rest)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO math.MG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Vapnik-Chervonenkis (VC) dimension of a collection of subsets of a set is\nan important combinatorial concept in settings such as discrete geometry and\nmachine learning. In this paper we prove that the VC dimension of the family of\n$d$-dimensional cubes in $\\mathbb R^d$ is $\\lfloor(3d+1)/2\\rfloor$.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 06:05:12 GMT"}, {"version": "v2", "created": "Thu, 8 Jan 2015 22:01:05 GMT"}, {"version": "v3", "created": "Sun, 26 Nov 2017 01:23:14 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Despres", "Christian J. J.", ""]]}, {"id": "1412.6614", "submitter": "Behnam Neyshabur", "authors": "Behnam Neyshabur, Ryota Tomioka, Nathan Srebro", "title": "In Search of the Real Inductive Bias: On the Role of Implicit\n  Regularization in Deep Learning", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present experiments demonstrating that some other form of capacity\ncontrol, different from network size, plays a central role in learning\nmultilayer feed-forward networks. We argue, partially through analogy to matrix\nfactorization, that this is an inductive bias that can help shed light on deep\nlearning.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 06:52:25 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2015 21:00:09 GMT"}, {"version": "v3", "created": "Fri, 6 Mar 2015 18:51:37 GMT"}, {"version": "v4", "created": "Thu, 16 Apr 2015 18:48:31 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Neyshabur", "Behnam", ""], ["Tomioka", "Ryota", ""], ["Srebro", "Nathan", ""]]}, {"id": "1412.6615", "submitter": "Levent Sagun", "authors": "Levent Sagun, V. Ugur Guney, Gerard Ben Arous, Yann LeCun", "title": "Explorations on high dimensional landscapes", "comments": "11 pages, 8 figures, workshop contribution at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding minima of a real valued non-convex function over a high dimensional\nspace is a major challenge in science. We provide evidence that some such\nfunctions that are defined on high dimensional domains have a narrow band of\nvalues whose pre-image contains the bulk of its critical points. This is in\ncontrast with the low dimensional picture in which this band is wide. Our\nsimulations agree with the previous theoretical work on spin glasses that\nproves the existence of such a band when the dimension of the domain tends to\ninfinity. Furthermore our experiments on teacher-student networks with the\nMNIST dataset establish a similar phenomenon in deep networks. We finally\nobserve that both the gradient descent and the stochastic gradient descent\nmethods can reach this level within the same number of steps.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 06:57:12 GMT"}, {"version": "v2", "created": "Thu, 25 Dec 2014 01:29:56 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2015 10:08:16 GMT"}, {"version": "v4", "created": "Mon, 6 Apr 2015 21:47:50 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Sagun", "Levent", ""], ["Guney", "V. Ugur", ""], ["Arous", "Gerard Ben", ""], ["LeCun", "Yann", ""]]}, {"id": "1412.6621", "submitter": "Arnab Paul", "authors": "Arnab Paul, Suresh Venkatasubramanian", "title": "Why does Deep Learning work? - A perspective from Group Theory", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Why does Deep Learning work? What representations does it capture? How do\nhigher-order representations emerge? We study these questions from the\nperspective of group theory, thereby opening a new approach towards a theory of\nDeep learning.\n  One factor behind the recent resurgence of the subject is a key algorithmic\nstep called pre-training: first search for a good generative model for the\ninput samples, and repeat the process one layer at a time. We show deeper\nimplications of this simple principle, by establishing a connection with the\ninterplay of orbits and stabilizers of group actions. Although the neural\nnetworks themselves may not form groups, we show the existence of {\\em shadow}\ngroups whose elements serve as close approximations.\n  Over the shadow groups, the pre-training step, originally introduced as a\nmechanism to better initialize a network, becomes equivalent to a search for\nfeatures with minimal orbits. Intuitively, these features are in a way the {\\em\nsimplest}. Which explains why a deep learning network learns simple features\nfirst. Next, we show how the same principle, when repeated in the deeper\nlayers, can capture higher order representations, and why representation\ncomplexity increases as the layers get deeper.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 07:28:46 GMT"}, {"version": "v2", "created": "Wed, 24 Dec 2014 02:22:01 GMT"}, {"version": "v3", "created": "Sat, 28 Feb 2015 07:19:35 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Paul", "Arnab", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "1412.6622", "submitter": "Elad Hoffer", "authors": "Elad Hoffer, Nir Ailon", "title": "Deep metric learning using Triplet network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has proven itself as a successful set of models for learning\nuseful semantic representations of data. These, however, are mostly implicitly\nlearned as part of a classification task. In this paper we propose the triplet\nnetwork model, which aims to learn useful representations by distance\ncomparisons. A similar model was defined by Wang et al. (2014), tailor made for\nlearning a ranking for image information retrieval. Here we demonstrate using\nvarious datasets that our model learns a better representation than that of its\nimmediate competitor, the Siamese network. We also discuss future possible\nusage as a framework for unsupervised learning.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 07:34:50 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2015 17:28:52 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2015 09:00:25 GMT"}, {"version": "v4", "created": "Tue, 4 Dec 2018 15:35:35 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Hoffer", "Elad", ""], ["Ailon", "Nir", ""]]}, {"id": "1412.6630", "submitter": "Jan Rudy", "authors": "Jan Rudy, Weiguang Ding, Daniel Jiwoong Im, Graham W. Taylor", "title": "Neural Network Regularization via Robust Weight Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization is essential when training large neural networks. As deep\nneural networks can be mathematically interpreted as universal function\napproximators, they are effective at memorizing sampling noise in the training\ndata. This results in poor generalization to unseen data. Therefore, it is no\nsurprise that a new regularization technique, Dropout, was partially\nresponsible for the now-ubiquitous winning entry to ImageNet 2012 by the\nUniversity of Toronto. Currently, Dropout (and related methods such as\nDropConnect) are the most effective means of regularizing large neural\nnetworks. These amount to efficiently visiting a large number of related models\nat training time, while aggregating them to a single predictor at test time.\nThe proposed FaMe model aims to apply a similar strategy, yet learns a\nfactorization of each weight matrix such that the factors are robust to noise.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 07:59:14 GMT"}, {"version": "v2", "created": "Mon, 5 Jan 2015 13:28:46 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Rudy", "Jan", ""], ["Ding", "Weiguang", ""], ["Im", "Daniel Jiwoong", ""], ["Taylor", "Graham W.", ""]]}, {"id": "1412.6651", "submitter": "Sixin Zhang Sixin Zhang", "authors": "Sixin Zhang, Anna Choromanska, Yann LeCun", "title": "Deep learning with Elastic Averaging SGD", "comments": "NIPS2015 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of stochastic optimization for deep learning in the\nparallel computing environment under communication constraints. A new algorithm\nis proposed in this setting where the communication and coordination of work\namong concurrent processes (local workers), is based on an elastic force which\nlinks the parameters they compute with a center variable stored by the\nparameter server (master). The algorithm enables the local workers to perform\nmore exploration, i.e. the algorithm allows the local variables to fluctuate\nfurther from the center variable by reducing the amount of communication\nbetween local workers and the master. We empirically demonstrate that in the\ndeep learning setting, due to the existence of many local optima, allowing more\nexploration can lead to the improved performance. We propose synchronous and\nasynchronous variants of the new algorithm. We provide the stability analysis\nof the asynchronous variant in the round-robin scheme and compare it with the\nmore common parallelized method ADMM. We show that the stability of EASGD is\nguaranteed when a simple stability condition is satisfied, which is not the\ncase for ADMM. We additionally propose the momentum-based version of our\nalgorithm that can be applied in both synchronous and asynchronous settings.\nAsynchronous variant of the algorithm is applied to train convolutional neural\nnetworks for image classification on the CIFAR and ImageNet datasets.\nExperiments demonstrate that the new algorithm accelerates the training of deep\narchitectures compared to DOWNPOUR and other common baseline approaches and\nfurthermore is very communication efficient.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 13:22:23 GMT"}, {"version": "v2", "created": "Mon, 29 Dec 2014 20:50:02 GMT"}, {"version": "v3", "created": "Mon, 5 Jan 2015 01:18:40 GMT"}, {"version": "v4", "created": "Wed, 25 Feb 2015 19:00:29 GMT"}, {"version": "v5", "created": "Wed, 29 Apr 2015 11:56:24 GMT"}, {"version": "v6", "created": "Sat, 6 Jun 2015 00:20:58 GMT"}, {"version": "v7", "created": "Sat, 8 Aug 2015 02:52:48 GMT"}, {"version": "v8", "created": "Sun, 25 Oct 2015 12:12:52 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Zhang", "Sixin", ""], ["Choromanska", "Anna", ""], ["LeCun", "Yann", ""]]}, {"id": "1412.6734", "submitter": "Aviv Tamar", "authors": "Aviv Tamar, Panos Toulis, Shie Mannor, Edoardo M. Airoldi", "title": "Implicit Temporal Differences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In reinforcement learning, the TD($\\lambda$) algorithm is a fundamental\npolicy evaluation method with an efficient online implementation that is\nsuitable for large-scale problems. One practical drawback of TD($\\lambda$) is\nits sensitivity to the choice of the step-size. It is an empirically well-known\nfact that a large step-size leads to fast convergence, at the cost of higher\nvariance and risk of instability. In this work, we introduce the implicit\nTD($\\lambda$) algorithm which has the same function and computational cost as\nTD($\\lambda$), but is significantly more stable. We provide a theoretical\nexplanation of this stability and an empirical evaluation of implicit\nTD($\\lambda$) on typical benchmark tasks. Our results show that implicit\nTD($\\lambda$) outperforms standard TD($\\lambda$) and a state-of-the-art method\nthat automatically tunes the step-size, and thus shows promise for wide\napplicability.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 06:53:15 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Tamar", "Aviv", ""], ["Toulis", "Panos", ""], ["Mannor", "Shie", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "1412.6741", "submitter": "Cheuk Ting Li", "authors": "Kim-Hung Li and Cheuk Ting Li", "title": "Locally Weighted Learning for Naive Bayes Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a consequence of the strong and usually violated conditional independence\nassumption (CIA) of naive Bayes (NB) classifier, the performance of NB becomes\nless and less favorable compared to sophisticated classifiers when the sample\nsize increases. We learn from this phenomenon that when the size of the\ntraining data is large, we should either relax the assumption or apply NB to a\n\"reduced\" data set, say for example use NB as a local model. The latter\napproach trades the ignored information for the robustness to the model\nassumption. In this paper, we consider using NB as a model for locally weighted\ndata. A special weighting function is designed so that if CIA holds for the\nunweighted data, it also holds for the weighted data. The new method is\nintuitive and capable of handling class imbalance. It is theoretically more\nsound than the locally weighted learners of naive Bayes that base\nclassification only on the $k$ nearest neighbors. Empirical study shows that\nthe new method with appropriate choice of parameter outperforms seven existing\nclassifiers of similar nature.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 08:30:57 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Li", "Kim-Hung", ""], ["Li", "Cheuk Ting", ""]]}, {"id": "1412.6749", "submitter": "Abdulrahman Ibraheem", "authors": "Abdulrahman Oladipupo Ibraheem", "title": "SENNS: Sparse Extraction Neural NetworkS for Feature Extraction", "comments": "Eighteen pages in all, but much of the central ideas are covered in\n  the first five and a half pages; most of the remaining pages are devoted to\n  straightforward mathematical derivations, and the presentation of three\n  algorithms. Manuscript contains no figures at this time", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By drawing on ideas from optimisation theory, artificial neural networks\n(ANN), graph embeddings and sparse representations, I develop a novel\ntechnique, termed SENNS (Sparse Extraction Neural NetworkS), aimed at\naddressing the feature extraction problem. The proposed method uses (preferably\ndeep) ANNs for projecting input attribute vectors to an output space wherein\npairwise distances are maximized for vectors belonging to different classes,\nbut minimized for those belonging to the same class, while simultaneously\nenforcing sparsity on the ANN outputs. The vectors that result from the\nprojection can then be used as features in any classifier of choice.\nMathematically, I formulate the proposed method as the minimisation of an\nobjective function which can be interpreted, in the ANN output space, as a\nnegative factor of the sum of the squares of the pair-wise distances between\noutput vectors belonging to different classes, added to a positive factor of\nthe sum of squares of the pair-wise distances between output vectors belonging\nto the same classes, plus sparsity and weight decay terms. To derive an\nalgorithm for minimizing the objective function via gradient descent, I use the\nmulti-variate version of the chain rule to obtain the partial derivatives of\nthe function with respect to ANN weights and biases, and find that each of the\nrequired partial derivatives can be expressed as a sum of six terms. As it\nturns out, four of those six terms can be computed using the standard back\npropagation algorithm; the fifth can be computed via a slight modification of\nthe standard backpropagation algorithm; while the sixth one can be computed via\nsimple arithmetic. Finally, I propose experiments on the ARABASE Arabic corpora\nof digits and letters, the CMU PIE database of faces, the MNIST digits\ndatabase, and other standard machine learning databases.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 09:28:05 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Ibraheem", "Abdulrahman Oladipupo", ""]]}, {"id": "1412.6752", "submitter": "Abdulrahman Ibraheem", "authors": "Abdulrahman Oladipupo Ibraheem", "title": "Correlation of Data Reconstruction Error and Shrinkages in Pair-wise\n  Distances under Principal Component Analysis (PCA)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this on-going work, I explore certain theoretical and empirical\nimplications of data transformations under the PCA. In particular, I state and\nprove three theorems about PCA, which I paraphrase as follows: 1). PCA without\ndiscarding eigenvector rows is injective, but looses this injectivity when\neigenvector rows are discarded 2). PCA without discarding eigen- vector rows\npreserves pair-wise distances, but tends to cause pair-wise distances to shrink\nwhen eigenvector rows are discarded. 3). For any pair of points, the shrinkage\nin pair-wise distance is bounded above by an L1 norm reconstruction error\nassociated with the points. Clearly, 3). suggests that there might exist some\ncorrelation between shrinkages in pair-wise distances and mean square\nreconstruction error which is defined as the sum of those eigenvalues\nassociated with the discarded eigenvectors. I therefore decided to perform\nnumerical experiments to obtain the corre- lation between the sum of those\neigenvalues and shrinkages in pair-wise distances. In addition, I have also\nperformed some experiments to check respectively the effect of the sum of those\neigenvalues and the effect of the shrinkages on classification accuracies under\nthe PCA map. So far, I have obtained the following results on some publicly\navailable data from the UCI Machine Learning Repository: 1). There seems to be\na strong cor- relation between the sum of those eigenvalues associated with\ndiscarded eigenvectors and shrinkages in pair-wise distances. 2). Neither the\nsum of those eigenvalues nor pair-wise distances have any strong correlations\nwith classification accuracies. 1\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 09:50:32 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Ibraheem", "Abdulrahman Oladipupo", ""]]}, {"id": "1412.6785", "submitter": "Sotetsu Koyamada", "authors": "Sotetsu Koyamada and Masanori Koyama and Ken Nakae and Shin Ishii", "title": "Principal Sensitivity Analysis", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-18038-0_48", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel algorithm (Principal Sensitivity Analysis; PSA) to analyze\nthe knowledge of the classifier obtained from supervised machine learning\ntechniques. In particular, we define principal sensitivity map (PSM) as the\ndirection on the input space to which the trained classifier is most sensitive,\nand use analogously defined k-th PSM to define a basis for the input space. We\ntrain neural networks with artificial data and real data, and apply the\nalgorithm to the obtained supervised classifiers. We then visualize the PSMs to\ndemonstrate the PSA's ability to decompose the knowledge acquired by the\ntrained classifiers.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 13:40:29 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2015 16:18:47 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Koyamada", "Sotetsu", ""], ["Koyama", "Masanori", ""], ["Nakae", "Ken", ""], ["Ishii", "Shin", ""]]}, {"id": "1412.6808", "submitter": "Waheed Bajwa", "authors": "Tong Wu and Waheed U. Bajwa", "title": "Learning the nonlinear geometry of high-dimensional data: Models and\n  algorithms", "comments": "Extended version of the journal paper accepted for publication in\n  IEEE Trans. Signal Processing (20 pages, 7 figures, 4 tables)", "journal-ref": "IEEE Trans. Signal Processing, vol. 63, no. 23, pp. 6229-6244,\n  Dec. 2015", "doi": "10.1109/TSP.2015.2469637", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern information processing relies on the axiom that high-dimensional data\nlie near low-dimensional geometric structures. This paper revisits the problem\nof data-driven learning of these geometric structures and puts forth two new\nnonlinear geometric models for data describing \"related\" objects/phenomena. The\nfirst one of these models straddles the two extremes of the subspace model and\nthe union-of-subspaces model, and is termed the metric-constrained\nunion-of-subspaces (MC-UoS) model. The second one of these models---suited for\ndata drawn from a mixture of nonlinear manifolds---generalizes the kernel\nsubspace model, and is termed the metric-constrained kernel union-of-subspaces\n(MC-KUoS) model. The main contributions of this paper in this regard include\nthe following. First, it motivates and formalizes the problems of MC-UoS and\nMC-KUoS learning. Second, it presents algorithms that efficiently learn an\nMC-UoS or an MC-KUoS underlying data of interest. Third, it extends these\nalgorithms to the case when parts of the data are missing. Last, but not least,\nit reports the outcomes of a series of numerical experiments involving both\nsynthetic and real data that demonstrate the superiority of the proposed\ngeometric models and learning algorithms over existing approaches in the\nliterature. These experiments also help clarify the connections between this\nwork and the literature on (subspace and kernel k-means) clustering.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 16:40:31 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2015 02:12:06 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Wu", "Tong", ""], ["Bajwa", "Waheed U.", ""]]}, {"id": "1412.6821", "submitter": "Roland Kwitt", "authors": "Jan Reininghaus, Stefan Huber, Ulrich Bauer, Roland Kwitt", "title": "A Stable Multi-Scale Kernel for Topological Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological data analysis offers a rich source of valuable information to\nstudy vision problems. Yet, so far we lack a theoretically sound connection to\npopular kernel-based learning techniques, such as kernel SVMs or kernel PCA. In\nthis work, we establish such a connection by designing a multi-scale kernel for\npersistence diagrams, a stable summary representation of topological features\nin data. We show that this kernel is positive definite and prove its stability\nwith respect to the 1-Wasserstein distance. Experiments on two benchmark\ndatasets for 3D shape classification/retrieval and texture recognition show\nconsiderable performance gains of the proposed method compared to an\nalternative approach that is based on the recently introduced persistence\nlandscapes.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 19:17:08 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Reininghaus", "Jan", ""], ["Huber", "Stefan", ""], ["Bauer", "Ulrich", ""], ["Kwitt", "Roland", ""]]}, {"id": "1412.6830", "submitter": "Forest Agostinelli", "authors": "Forest Agostinelli, Matthew Hoffman, Peter Sadowski, Pierre Baldi", "title": "Learning Activation Functions to Improve Deep Neural Networks", "comments": "Accepted as a workshop paper contribution at the International\n  Conference on Learning Representations (ICLR) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks typically have a fixed, non-linear activation\nfunction at each neuron. We have designed a novel form of piecewise linear\nactivation function that is learned independently for each neuron using\ngradient descent. With this adaptive activation function, we are able to\nimprove upon deep neural network architectures composed of static rectified\nlinear units, achieving state-of-the-art performance on CIFAR-10 (7.51%),\nCIFAR-100 (30.83%), and a benchmark from high-energy physics involving Higgs\nboson decay modes.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 20:20:21 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 21:44:41 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2015 08:05:02 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Agostinelli", "Forest", ""], ["Hoffman", "Matthew", ""], ["Sadowski", "Peter", ""], ["Baldi", "Pierre", ""]]}, {"id": "1412.6881", "submitter": "Jinseok Nam", "authors": "Jinseok Nam and Johannes F\\\"urnkranz", "title": "On Learning Vector Representations in Hierarchical Label Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important problem in multi-label classification is to capture label\npatterns or underlying structures that have an impact on such patterns. This\npaper addresses one such problem, namely how to exploit hierarchical structures\nover labels. We present a novel method to learn vector representations of a\nlabel space given a hierarchy of labels and label co-occurrence patterns. Our\nexperimental results demonstrate qualitatively that the proposed method is able\nto learn regularities among labels by exploiting a label hierarchy as well as\nlabel co-occurrences. It highlights the importance of the hierarchical\ninformation in order to obtain regularities which facilitate analogical\nreasoning over a label space. We also experimentally illustrate the dependency\nof the learned representations on the label hierarchy.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 06:12:06 GMT"}, {"version": "v2", "created": "Tue, 13 Jan 2015 17:12:04 GMT"}, {"version": "v3", "created": "Thu, 16 Apr 2015 19:23:23 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Nam", "Jinseok", ""], ["F\u00fcrnkranz", "Johannes", ""]]}, {"id": "1412.7003", "submitter": "Shin-ichi Maeda", "authors": "Shin-ichi Maeda", "title": "A Bayesian encourages dropout", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout is one of the key techniques to prevent the learning from\noverfitting. It is explained that dropout works as a kind of modified L2\nregularization. Here, we shed light on the dropout from Bayesian standpoint.\nBayesian interpretation enables us to optimize the dropout rate, which is\nbeneficial for learning of weight parameters and prediction after learning. The\nexperiment result also encourages the optimization of the dropout.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 14:46:26 GMT"}, {"version": "v2", "created": "Mon, 29 Dec 2014 06:17:16 GMT"}, {"version": "v3", "created": "Tue, 30 Dec 2014 06:32:47 GMT"}], "update_date": "2014-12-31", "authors_parsed": [["Maeda", "Shin-ichi", ""]]}, {"id": "1412.7012", "submitter": "Tomoyuki Obuchi", "authors": "Tomoyuki Obuchi, Hirokazu Koma, and Muneki Yasuda", "title": "Boltzmann-Machine Learning of Prior Distributions of Binarized Natural\n  Images", "comments": "32 pages, 33 figures", "journal-ref": "J. Phys. Soc. Jpn. 85 (2016) 114803", "doi": "10.7566/JPSJ.85.114803", "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior distributions of binarized natural images are learned by using a\nBoltzmann machine. According the results of this study, there emerges a\nstructure with two sublattices in the interactions, and the nearest-neighbor\nand next-nearest-neighbor interactions correspondingly take two discriminative\nvalues, which reflects the individual characteristics of the three sets of\npictures that we process. Meanwhile, in a longer spatial scale, a longer-range,\nalthough still rapidly decaying, ferromagnetic interaction commonly appears in\nall cases. The characteristic length scale of the interactions is universally\nup to approximately four lattice spacings $\\xi \\approx 4$. These results are\nderived by using the mean-field method, which effectively reduces the\ncomputational time required in a Boltzmann machine. An improved mean-field\nmethod called the Bethe approximation also gives the same results, as well as\nthe Monte Carlo method does for small size images. These reinforce the validity\nof our analysis and findings. Relations to criticality, frustration, and\nsimple-cell receptive fields are also discussed.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 04:41:09 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2015 14:03:11 GMT"}, {"version": "v3", "created": "Fri, 7 Oct 2016 02:07:27 GMT"}, {"version": "v4", "created": "Mon, 24 Oct 2016 03:20:38 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Obuchi", "Tomoyuki", ""], ["Koma", "Hirokazu", ""], ["Yasuda", "Muneki", ""]]}, {"id": "1412.7056", "submitter": "Shuchin Aeron", "authors": "Eric Kernfeld and Shuchin Aeron and Misha Kilmer", "title": "Clustering multi-way data: a novel algebraic approach", "comments": "20 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a method for unsupervised clustering of two-way\n(matrix) data by combining two recent innovations from different fields: the\nSparse Subspace Clustering (SSC) algorithm [10], which groups points coming\nfrom a union of subspaces into their respective subspaces, and the t-product\n[18], which was introduced to provide a matrix-like multiplication for third\norder tensors. Our algorithm is analogous to SSC in that an \"affinity\" between\ndifferent data points is built using a sparse self-representation of the data.\nUnlike SSC, we employ the t-product in the self-representation. This allows us\nmore flexibility in modeling; infact, SSC is a special case of our method. When\nusing the t-product, three-way arrays are treated as matrices whose elements\n(scalars) are n-tuples or tubes. Convolutions take the place of scalar\nmultiplication. This framework allows us to embed the 2-D data into a\nvector-space-like structure called a free module over a commutative ring. These\nfree modules retain many properties of complex inner-product spaces, and we\nleverage that to provide theoretical guarantees on our algorithm. We show that\ncompared to vector-space counterparts, SSmC achieves higher accuracy and better\nable to cluster data with less preprocessing in some image clustering problems.\nIn particular we show the performance of the proposed method on Weizmann face\ndatabase, the Extended Yale B Face database and the MNIST handwritten digits\ndatabase.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 17:06:44 GMT"}, {"version": "v2", "created": "Sun, 22 Feb 2015 02:19:29 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Kernfeld", "Eric", ""], ["Aeron", "Shuchin", ""], ["Kilmer", "Misha", ""]]}, {"id": "1412.7149", "submitter": "Zichao Yang", "authors": "Zichao Yang, Marcin Moczulski, Misha Denil, Nando de Freitas, Alex\n  Smola, Le Song, Ziyu Wang", "title": "Deep Fried Convnets", "comments": "svd experiments included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fully connected layers of a deep convolutional neural network typically\ncontain over 90% of the network parameters, and consume the majority of the\nmemory required to store the network parameters. Reducing the number of\nparameters while preserving essentially the same predictive performance is\ncritically important for operating deep neural networks in memory constrained\nenvironments such as GPUs or embedded devices.\n  In this paper we show how kernel methods, in particular a single Fastfood\nlayer, can be used to replace all fully connected layers in a deep\nconvolutional neural network. This novel Fastfood layer is also end-to-end\ntrainable in conjunction with convolutional layers, allowing us to combine them\ninto a new architecture, named deep fried convolutional networks, which\nsubstantially reduces the memory footprint of convolutional networks trained on\nMNIST and ImageNet with no drop in predictive performance.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 20:53:30 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 20:17:24 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2015 21:30:55 GMT"}, {"version": "v4", "created": "Fri, 17 Jul 2015 20:17:26 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Yang", "Zichao", ""], ["Moczulski", "Marcin", ""], ["Denil", "Misha", ""], ["de Freitas", "Nando", ""], ["Smola", "Alex", ""], ["Song", "Le", ""], ["Wang", "Ziyu", ""]]}, {"id": "1412.7210", "submitter": "Antti Rasmus", "authors": "Antti Rasmus, Tapani Raiko, Harri Valpola", "title": "Denoising autoencoder with modulated lateral connections learns\n  invariant representations of natural images", "comments": "Presentation at ICLR 2015 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suitable lateral connections between encoder and decoder are shown to allow\nhigher layers of a denoising autoencoder (dAE) to focus on invariant\nrepresentations. In regular autoencoders, detailed information needs to be\ncarried through the highest layers but lateral connections from encoder to\ndecoder relieve this pressure. It is shown that abstract invariant features can\nbe translated to detailed reconstructions when invariant features are allowed\nto modulate the strength of the lateral connection. Three dAE structures with\nmodulated and additive lateral connections, and without lateral connections\nwere compared in experiments using real-world images. The experiments verify\nthat adding modulated lateral connections to the model 1) improves the accuracy\nof the probability model for inputs, as measured by denoising performance; 2)\nresults in representations whose degree of invariance grows faster towards the\nhigher layers; and 3) supports the formation of diverse invariant poolings.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 23:36:15 GMT"}, {"version": "v2", "created": "Wed, 31 Dec 2014 12:17:51 GMT"}, {"version": "v3", "created": "Wed, 25 Feb 2015 20:56:43 GMT"}, {"version": "v4", "created": "Tue, 31 Mar 2015 15:49:16 GMT"}], "update_date": "2015-04-01", "authors_parsed": [["Rasmus", "Antti", ""], ["Raiko", "Tapani", ""], ["Valpola", "Harri", ""]]}, {"id": "1412.7216", "submitter": "Mathieu Rosenbaum", "authors": "Alexandre Belloni, Mathieu Rosenbaum and Alexandre B. Tsybakov", "title": "An $\\{l_1,l_2,l_{\\infty}\\}$-Regularization Approach to High-Dimensional\n  Errors-in-variables Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several new estimation methods have been recently proposed for the linear\nregression model with observation error in the design. Different assumptions on\nthe data generating process have motivated different estimators and analysis.\nIn particular, the literature considered (1) observation errors in the design\nuniformly bounded by some $\\bar \\delta$, and (2) zero mean independent\nobservation errors. Under the first assumption, the rates of convergence of the\nproposed estimators depend explicitly on $\\bar \\delta$, while the second\nassumption has been applied when an estimator for the second moment of the\nobservational error is available. This work proposes and studies two new\nestimators which, compared to other procedures for regression models with\nerrors in the design, exploit an additional $l_{\\infty}$-norm regularization.\nThe first estimator is applicable when both (1) and (2) hold but does not\nrequire an estimator for the second moment of the observational error. The\nsecond estimator is applicable under (2) and requires an estimator for the\nsecond moment of the observation error. Importantly, we impose no assumption on\nthe accuracy of this pilot estimator, in contrast to the previously known\nprocedures. As the recent proposals, we allow the number of covariates to be\nmuch larger than the sample size. We establish the rates of convergence of the\nestimators and compare them with the bounds obtained for related estimators in\nthe literature. These comparisons show interesting insights on the interplay of\nthe assumptions and the achievable rates of convergence.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 23:57:56 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Belloni", "Alexandre", ""], ["Rosenbaum", "Mathieu", ""], ["Tsybakov", "Alexandre B.", ""]]}, {"id": "1412.7260", "submitter": "Ehsan Elhamifar", "authors": "Ehsan Elhamifar, Mahdi Soltanolkotabi and Shankar Sastry", "title": "Approximate Subspace-Sparse Recovery with Corrupted Data via Constrained\n  $\\ell_1$-Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional data often lie in low-dimensional subspaces corresponding to\ndifferent classes they belong to. Finding sparse representations of data points\nin a dictionary built using the collection of data helps to uncover\nlow-dimensional subspaces and address problems such as clustering,\nclassification, subset selection and more. In this paper, we address the\nproblem of recovering sparse representations for noisy data points in a\ndictionary whose columns correspond to corrupted data lying close to a union of\nsubspaces. We consider a constrained $\\ell_1$-minimization and study conditions\nunder which the solution of the proposed optimization satisfies the approximate\nsubspace-sparse recovery condition. More specifically, we show that each noisy\ndata point, perturbed from a subspace by a noise of the magnitude of\n$\\varepsilon$, will be reconstructed using data points from the same subspace\nwith a small error of the order of $O(\\varepsilon)$ and that the coefficients\ncorresponding to data points in other subspaces will be sufficiently small,\n\\ie, of the order of $O(\\varepsilon)$. We do not impose any randomness\nassumption on the arrangement of subspaces or distribution of data points in\neach subspace. Our framework is based on a novel generalization of the\nnull-space property to the setting where data lie in multiple subspaces, the\nnumber of data points in each subspace exceeds the dimension of the subspace,\nand all data points are corrupted by noise. Moreover, assuming a random\ndistribution for data points, we further show that coefficients from the\ndesired support not only reconstruct a given point with high accuracy, but also\nhave sufficiently large values, \\ie, of the order of $O(1)$.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 06:10:34 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2016 23:05:32 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Elhamifar", "Ehsan", ""], ["Soltanolkotabi", "Mahdi", ""], ["Sastry", "Shankar", ""]]}, {"id": "1412.7299", "submitter": "Christopher Nemeth", "authors": "Christopher Nemeth, Chris Sherlock and Paul Fearnhead", "title": "Particle Metropolis-adjusted Langevin algorithms", "comments": "Accepted to Biometrika. Main text: 22 pages and 3 figures.\n  Supplementary material: 18 pages and 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new sampling scheme based on Langevin dynamics that is\napplicable within pseudo-marginal and particle Markov chain Monte Carlo\nalgorithms. We investigate this algorithm's theoretical properties under\nstandard asymptotics, which correspond to an increasing dimension of the\nparameters, $n$. Our results show that the behaviour of the algorithm depends\ncrucially on how accurately one can estimate the gradient of the log target\ndensity. If the error in the estimate of the gradient is not sufficiently\ncontrolled as dimension increases, then asymptotically there will be no\nadvantage over the simpler random-walk algorithm. However, if the error is\nsufficiently well-behaved, then the optimal scaling of this algorithm will be\n$O(n^{-1/6})$ compared to $O(n^{-1/2})$ for the random walk. Our theory also\ngives guidelines on how to tune the number of Monte Carlo samples in the\nlikelihood estimate and the proposal step-size.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 09:53:09 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2015 16:29:52 GMT"}, {"version": "v3", "created": "Fri, 27 May 2016 12:52:55 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Nemeth", "Christopher", ""], ["Sherlock", "Chris", ""], ["Fearnhead", "Paul", ""]]}, {"id": "1412.7392", "submitter": "Arnak Dalalyan S.", "authors": "Arnak S. Dalalyan", "title": "Theoretical guarantees for approximate sampling from smooth and\n  log-concave densities", "comments": "To appear in JRSS B", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling from various kinds of distributions is an issue of paramount\nimportance in statistics since it is often the key ingredient for constructing\nestimators, test procedures or confidence intervals. In many situations, the\nexact sampling from a given distribution is impossible or computationally\nexpensive and, therefore, one needs to resort to approximate sampling\nstrategies. However, there is no well-developed theory providing meaningful\nnonasymptotic guarantees for the approximate sampling procedures, especially in\nthe high-dimensional problems. This paper makes some progress in this direction\nby considering the problem of sampling from a distribution having a smooth and\nlog-concave density defined on \\(\\RR^p\\), for some integer \\(p>0\\). We\nestablish nonasymptotic bounds for the error of approximating the target\ndistribution by the one obtained by the Langevin Monte Carlo method and its\nvariants. We illustrate the effectiveness of the established guarantees with\nvarious experiments. Underlying our analysis are insights from the theory of\ncontinuous-time diffusion processes, which may be of interest beyond the\nframework of log-concave densities considered in the present work.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 15:00:57 GMT"}, {"version": "v2", "created": "Wed, 31 Dec 2014 03:15:50 GMT"}, {"version": "v3", "created": "Thu, 8 Jan 2015 03:29:23 GMT"}, {"version": "v4", "created": "Thu, 17 Sep 2015 16:02:53 GMT"}, {"version": "v5", "created": "Fri, 19 Feb 2016 23:19:39 GMT"}, {"version": "v6", "created": "Sat, 3 Dec 2016 08:41:19 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Dalalyan", "Arnak S.", ""]]}, {"id": "1412.7419", "submitter": "\\c{C}a\\u{g}lar G\\\"ul\\c{c}ehre", "authors": "Caglar Gulcehre, Marcin Moczulski and Yoshua Bengio", "title": "ADASECANT: Robust Adaptive Secant Method for Stochastic Gradient", "comments": "8 pages, 3 figures, ICLR workshop submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Stochastic gradient algorithms have been the main focus of large-scale\nlearning problems and they led to important successes in machine learning. The\nconvergence of SGD depends on the careful choice of learning rate and the\namount of the noise in stochastic estimates of the gradients. In this paper, we\npropose a new adaptive learning rate algorithm, which utilizes curvature\ninformation for automatically tuning the learning rates. The information about\nthe element-wise curvature of the loss function is estimated from the local\nstatistics of the stochastic first order gradients. We further propose a new\nvariance reduction technique to speed up the convergence. In our preliminary\nexperiments with deep neural networks, we obtained better performance compared\nto the popular stochastic gradient algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 15:55:08 GMT"}, {"version": "v2", "created": "Mon, 5 Jan 2015 19:15:01 GMT"}, {"version": "v3", "created": "Wed, 28 Jan 2015 00:46:01 GMT"}, {"version": "v4", "created": "Tue, 10 Mar 2015 18:52:17 GMT"}, {"version": "v5", "created": "Sun, 1 Nov 2015 03:05:18 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Gulcehre", "Caglar", ""], ["Moczulski", "Marcin", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1412.7449", "submitter": "Oriol Vinyals", "authors": "Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever,\n  Geoffrey Hinton", "title": "Grammar as a Foreign Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Syntactic constituency parsing is a fundamental problem in natural language\nprocessing and has been the subject of intensive research and engineering for\ndecades. As a result, the most accurate parsers are domain specific, complex,\nand inefficient. In this paper we show that the domain agnostic\nattention-enhanced sequence-to-sequence model achieves state-of-the-art results\non the most widely used syntactic constituency parsing dataset, when trained on\na large synthetic corpus that was annotated using existing parsers. It also\nmatches the performance of standard parsers when trained only on a small\nhuman-annotated dataset, which shows that this model is highly data-efficient,\nin contrast to sequence-to-sequence models without the attention mechanism. Our\nparser is also fast, processing over a hundred sentences per second with an\nunoptimized CPU implementation.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 17:16:24 GMT"}, {"version": "v2", "created": "Sat, 28 Feb 2015 03:16:54 GMT"}, {"version": "v3", "created": "Tue, 9 Jun 2015 22:41:07 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Vinyals", "Oriol", ""], ["Kaiser", "Lukasz", ""], ["Koo", "Terry", ""], ["Petrov", "Slav", ""], ["Sutskever", "Ilya", ""], ["Hinton", "Geoffrey", ""]]}, {"id": "1412.7461", "submitter": "Aki Vehtari", "authors": "Aki Vehtari, Tommi Mononen, Ville Tolvanen, Tuomas Sivula and Ole\n  Winther", "title": "Bayesian leave-one-out cross-validation approximations for Gaussian\n  latent variable models", "comments": null, "journal-ref": "Journal of Machine Learning Research, 17(103):1-38, 2016", "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The future predictive performance of a Bayesian model can be estimated using\nBayesian cross-validation. In this article, we consider Gaussian latent\nvariable models where the integration over the latent values is approximated\nusing the Laplace method or expectation propagation (EP). We study the\nproperties of several Bayesian leave-one-out (LOO) cross-validation\napproximations that in most cases can be computed with a small additional cost\nafter forming the posterior approximation given the full data. Our main\nobjective is to assess the accuracy of the approximative LOO cross-validation\nestimators. That is, for each method (Laplace and EP) we compare the\napproximate fast computation with the exact brute force LOO computation.\nSecondarily, we evaluate the accuracy of the Laplace and EP approximations\nthemselves against a ground truth established through extensive Markov chain\nMonte Carlo simulation. Our empirical results show that the approach based upon\na Gaussian approximation to the LOO marginal distribution (the so-called cavity\ndistribution) gives the most accurate and reliable results among the fast\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 18:25:57 GMT"}, {"version": "v2", "created": "Mon, 4 Apr 2016 11:27:21 GMT"}, {"version": "v3", "created": "Mon, 23 May 2016 20:19:39 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Vehtari", "Aki", ""], ["Mononen", "Tommi", ""], ["Tolvanen", "Ville", ""], ["Sivula", "Tuomas", ""], ["Winther", "Ole", ""]]}, {"id": "1412.7468", "submitter": "Yang Feng", "authors": "Pallavi Basu, Yang Feng and Jinchi Lv", "title": "Model Selection in High-Dimensional Misspecified Models", "comments": "43 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model selection is indispensable to high-dimensional sparse modeling in\nselecting the best set of covariates among a sequence of candidate models. Most\nexisting work assumes implicitly that the model is correctly specified or of\nfixed dimensions. Yet model misspecification and high dimensionality are common\nin real applications. In this paper, we investigate two classical\nKullback-Leibler divergence and Bayesian principles of model selection in the\nsetting of high-dimensional misspecified models. Asymptotic expansions of these\nprinciples reveal that the effect of model misspecification is crucial and\nshould be taken into account, leading to the generalized AIC and generalized\nBIC in high dimensions. With a natural choice of prior probabilities, we\nsuggest the generalized BIC with prior probability which involves a logarithmic\nfactor of the dimensionality in penalizing model complexity. We further\nestablish the consistency of the covariance contrast matrix estimator in a\ngeneral setting. Our results and new method are supported by numerical studies.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 18:49:19 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Basu", "Pallavi", ""], ["Feng", "Yang", ""], ["Lv", "Jinchi", ""]]}, {"id": "1412.7477", "submitter": "Tuo Zhao", "authors": "Tuo Zhao, Han Liu, Tong Zhang", "title": "Pathwise Coordinate Optimization for Sparse Learning: Algorithm and\n  Theory", "comments": "Accepted by the Annals of Statistics, 2016+", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pathwise coordinate optimization is one of the most important\ncomputational frameworks for high dimensional convex and nonconvex sparse\nlearning problems. It differs from the classical coordinate optimization\nalgorithms in three salient features: {\\it warm start initialization}, {\\it\nactive set updating}, and {\\it strong rule for coordinate preselection}. Such a\ncomplex algorithmic structure grants superior empirical performance, but also\nposes significant challenge to theoretical analysis. To tackle this long\nlasting problem, we develop a new theory showing that these three features play\npivotal roles in guaranteeing the outstanding statistical and computational\nperformance of the pathwise coordinate optimization framework. Particularly, we\nanalyze the existing pathwise coordinate optimization algorithms and provide\nnew theoretical insights into them. The obtained insights further motivate the\ndevelopment of several modifications to improve the pathwise coordinate\noptimization framework, which guarantees linear convergence to a unique sparse\nlocal optimum with optimal statistical properties in parameter estimation and\nsupport recovery. This is the first result on the computational and statistical\nguarantees of the pathwise coordinate optimization framework in high\ndimensions. Thorough numerical experiments are provided to support our theory.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 19:12:24 GMT"}, {"version": "v2", "created": "Fri, 20 Feb 2015 21:48:23 GMT"}, {"version": "v3", "created": "Mon, 14 Sep 2015 21:57:51 GMT"}, {"version": "v4", "created": "Wed, 3 Aug 2016 17:00:55 GMT"}, {"version": "v5", "created": "Mon, 2 Jan 2017 18:36:23 GMT"}, {"version": "v6", "created": "Wed, 1 Feb 2017 20:41:32 GMT"}, {"version": "v7", "created": "Thu, 9 Feb 2017 18:19:38 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Zhao", "Tuo", ""], ["Liu", "Han", ""], ["Zhang", "Tong", ""]]}, {"id": "1412.7489", "submitter": "Yongxin Yang", "authors": "Yongxin Yang and Timothy M. Hospedales", "title": "A Unified Perspective on Multi-Domain and Multi-Task Learning", "comments": "9 pages, Accepted to ICLR 2015 Conference Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide a new neural-network based perspective on\nmulti-task learning (MTL) and multi-domain learning (MDL). By introducing the\nconcept of a semantic descriptor, this framework unifies MDL and MTL as well as\nencompassing various classic and recent MTL/MDL algorithms by interpreting them\nas different ways of constructing semantic descriptors. Our interpretation\nprovides an alternative pipeline for zero-shot learning (ZSL), where a model\nfor a novel class can be constructed without training data. Moreover, it leads\nto a new and practically relevant problem setting of zero-shot domain\nadaptation (ZSDA), which is the analogous to ZSL but for novel domains: A model\nfor an unseen domain can be generated by its semantic descriptor. Experiments\nacross this range of problems demonstrate that our framework outperforms a\nvariety of alternatives.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 19:50:21 GMT"}, {"version": "v2", "created": "Sun, 22 Feb 2015 15:10:46 GMT"}, {"version": "v3", "created": "Thu, 26 Mar 2015 15:29:50 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Yang", "Yongxin", ""], ["Hospedales", "Timothy M.", ""]]}, {"id": "1412.7625", "submitter": "Teng Qiu", "authors": "Teng Qiu, Yongjie Li", "title": "An Effective Semi-supervised Divisive Clustering Algorithm", "comments": "8 pages, 4 figures, a new (6th) member of the in-tree clustering\n  family", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Nowadays, data are generated massively and rapidly from scientific fields as\nbioinformatics, neuroscience and astronomy to business and engineering fields.\nCluster analysis, as one of the major data analysis tools, is therefore more\nsignificant than ever. We propose in this work an effective Semi-supervised\nDivisive Clustering algorithm (SDC). Data points are first organized by a\nminimal spanning tree. Next, this tree structure is transitioned to the in-tree\nstructure, and then divided into sub-trees under the supervision of the labeled\ndata, and in the end, all points in the sub-trees are directly associated with\nspecific cluster centers. SDC is fully automatic, non-iterative, involving no\nfree parameter, insensitive to noise, able to detect irregularly shaped cluster\nstructures, applicable to the data sets of high dimensionality and different\nattributes. The power of SDC is demonstrated on several datasets.\n", "versions": [{"version": "v1", "created": "Wed, 24 Dec 2014 08:55:50 GMT"}, {"version": "v2", "created": "Tue, 6 Jan 2015 09:35:39 GMT"}], "update_date": "2015-01-07", "authors_parsed": [["Qiu", "Teng", ""], ["Li", "Yongjie", ""]]}, {"id": "1412.7638", "submitter": "Mladen Kolar", "authors": "Jialei Wang and Mladen Kolar", "title": "Inference for Sparse Conditional Precision Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given $n$ i.i.d. observations of a random vector $(X,Z)$, where $X$ is a\nhigh-dimensional vector and $Z$ is a low-dimensional index variable, we study\nthe problem of estimating the conditional inverse covariance matrix $\\Omega(z)\n= (E[(X-E[X \\mid Z])(X-E[X \\mid Z])^T \\mid Z=z])^{-1}$ under the assumption\nthat the set of non-zero elements is small and does not depend on the index\nvariable. We develop a novel procedure that combines the ideas of the local\nconstant smoothing and the group Lasso for estimating the conditional inverse\ncovariance matrix. A proximal iterative smoothing algorithm is used to solve\nthe corresponding convex optimization problems. We prove that our procedure\nrecovers the conditional independence assumptions of the distribution $X \\mid\nZ$ with high probability. This result is established by developing a uniform\ndeviation bound for the high-dimensional conditional covariance matrix from its\npopulation counterpart, which may be of independent interest. Furthermore, we\ndevelop point-wise confidence intervals for individual elements of the\nconditional inverse covariance matrix. We perform extensive simulation studies,\nin which we demonstrate that our proposed procedure outperforms sensible\ncompetitors. We illustrate our proposal on a S&P 500 stock price data set.\n", "versions": [{"version": "v1", "created": "Wed, 24 Dec 2014 10:32:02 GMT"}], "update_date": "2014-12-25", "authors_parsed": [["Wang", "Jialei", ""], ["Kolar", "Mladen", ""]]}, {"id": "1412.7705", "submitter": "Emmanuel Bacry", "authors": "Emmanuel Bacry and St\\'ephane Ga\\\"iffas and Jean-Fran\\c{c}ois Muzy", "title": "Concentration for matrix martingales in continuous time and microscopic\n  activity of social networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper gives new concentration inequalities for the spectral norm of a\nwide class of matrix martingales in continuous time. These results extend\npreviously established Freedman and Bernstein inequalities for series of random\nmatrices to the class of continuous time processes. Our analysis relies on a\nnew supermartingale property of the trace exponential proved within the\nframework of stochastic calculus. We provide also several examples that\nillustrate the fact that our results allow us to recover easily several\nformerly obtained sharp bounds for discrete time matrix martingales.\n", "versions": [{"version": "v1", "created": "Wed, 24 Dec 2014 16:28:07 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 07:53:35 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Bacry", "Emmanuel", ""], ["Ga\u00efffas", "St\u00e9phane", ""], ["Muzy", "Jean-Fran\u00e7ois", ""]]}, {"id": "1412.7839", "submitter": "Waheed Bajwa", "authors": "Haroon Raja and Waheed U. Bajwa", "title": "Cloud K-SVD: A Collaborative Dictionary Learning Algorithm for Big,\n  Distributed Data", "comments": "Accepted for Publication in IEEE Trans. Signal Processing (2015); 16\n  pages, 3 figures", "journal-ref": "IEEE Trans. Signal Processing, vol. 64, no. 1, pp. 173-188, Jan.\n  2016", "doi": "10.1109/TSP.2015.2472372", "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of data-adaptive representations for big,\ndistributed data. It is assumed that a number of geographically-distributed,\ninterconnected sites have massive local data and they are interested in\ncollaboratively learning a low-dimensional geometric structure underlying these\ndata. In contrast to previous works on subspace-based data representations,\nthis paper focuses on the geometric structure of a union of subspaces (UoS). In\nthis regard, it proposes a distributed algorithm---termed cloud K-SVD---for\ncollaborative learning of a UoS structure underlying distributed data of\ninterest. The goal of cloud K-SVD is to learn a common overcomplete dictionary\nat each individual site such that every sample in the distributed data can be\nrepresented through a small number of atoms of the learned dictionary. Cloud\nK-SVD accomplishes this goal without requiring exchange of individual samples\nbetween sites. This makes it suitable for applications where sharing of raw\ndata is discouraged due to either privacy concerns or large volumes of data.\nThis paper also provides an analysis of cloud K-SVD that gives insights into\nits properties as well as deviations of the dictionaries learned at individual\nsites from a centralized solution in terms of different measures of\nlocal/global data and topology of interconnections. Finally, the paper\nnumerically illustrates the efficacy of cloud K-SVD on real and synthetic\ndistributed data.\n", "versions": [{"version": "v1", "created": "Thu, 25 Dec 2014 17:01:52 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2015 21:27:03 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Raja", "Haroon", ""], ["Bajwa", "Waheed U.", ""]]}, {"id": "1412.7868", "submitter": "P.K. Srijith", "authors": "P. K. Srijith, P. Balamurugan and Shirish Shevade", "title": "Gaussian Process Pseudo-Likelihood Models for Sequence Labeling", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several machine learning problems arising in natural language processing can\nbe modeled as a sequence labeling problem. We provide Gaussian process models\nbased on pseudo-likelihood approximation to perform sequence labeling. Gaussian\nprocesses (GPs) provide a Bayesian approach to learning in a kernel based\nframework. The pseudo-likelihood model enables one to capture long range\ndependencies among the output components of the sequence without becoming\ncomputationally intractable. We use an efficient variational Gaussian\napproximation method to perform inference in the proposed model. We also\nprovide an iterative algorithm which can effectively make use of the\ninformation from the neighboring labels to perform prediction. The ability to\ncapture long range dependencies makes the proposed approach useful for a wide\nrange of sequence labeling problems. Numerical experiments on some sequence\nlabeling data sets demonstrate the usefulness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 25 Dec 2014 21:59:46 GMT"}, {"version": "v2", "created": "Wed, 21 Sep 2016 05:41:08 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Srijith", "P. K.", ""], ["Balamurugan", "P.", ""], ["Shevade", "Shirish", ""]]}, {"id": "1412.7938", "submitter": "Shusen Wang", "authors": "Shusen Wang, Tong Zhang, Zhihua Zhang", "title": "Adjusting Leverage Scores by Row Weighting: A Practical Approach to\n  Coherent Matrix Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank matrix completion is an important problem with extensive real-world\napplications. When observations are uniformly sampled from the underlying\nmatrix entries, existing methods all require the matrix to be incoherent. This\npaper provides the first working method for coherent matrix completion under\nthe standard uniform sampling model. Our approach is based on the weighted\nnuclear norm minimization idea proposed in several recent work, and our key\ncontribution is a practical method to compute the weighting matrices so that\nthe leverage scores become more uniform after weighting. Under suitable\nconditions, we are able to derive theoretical results, showing the\neffectiveness of our approach. Experiments on synthetic data show that our\napproach recovers highly coherent matrices with high precision, whereas the\nstandard unweighted method fails even on noise-free data.\n", "versions": [{"version": "v1", "created": "Fri, 26 Dec 2014 13:56:02 GMT"}, {"version": "v2", "created": "Tue, 10 Feb 2015 12:28:34 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Wang", "Shusen", ""], ["Zhang", "Tong", ""], ["Zhang", "Zhihua", ""]]}, {"id": "1412.7955", "submitter": "Santosh Vempala", "authors": "Christos H. Papadimitriou and Santosh S. Vempala", "title": "Unsupervised Learning through Prediction in a Model of Cortex", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DS q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a primitive called PJOIN, for \"predictive join,\" which combines\nand extends the operations JOIN and LINK, which Valiant proposed as the basis\nof a computational theory of cortex. We show that PJOIN can be implemented in\nValiant's model. We also show that, using PJOIN, certain reasonably complex\nlearning and pattern matching tasks can be performed, in a way that involves\nphenomena which have been observed in cognition and the brain, namely\nmemory-based prediction and downward traffic in the cortical hierarchy.\n", "versions": [{"version": "v1", "created": "Fri, 26 Dec 2014 16:41:04 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Papadimitriou", "Christos H.", ""], ["Vempala", "Santosh S.", ""]]}, {"id": "1412.7983", "submitter": "Dong Xia", "authors": "Dong Xia", "title": "Exploring Sparsity in Multi-class Linear Discriminant Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies in the literature have paid much attention to the sparsity in\nlinear classification tasks. One motivation of imposing sparsity assumption on\nthe linear discriminant direction is to rule out the noninformative features,\nmaking hardly contribution to the classification problem. Most of those work\nwere focused on the scenarios of binary classification. In the presence of\nmulti-class data, preceding researches recommended individually pairwise sparse\nlinear discriminant analysis(LDA). However, further sparsity should be\nexplored. In this paper, an estimator of grouped LASSO type is proposed to take\nadvantage of sparsity for multi-class data. It enjoys appealing non-asymptotic\nproperties which allows insignificant correlations among features. This\nestimator exhibits superior capability on both simulated and real data.\n", "versions": [{"version": "v1", "created": "Fri, 26 Dec 2014 20:01:21 GMT"}, {"version": "v2", "created": "Fri, 9 Jan 2015 23:54:38 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Xia", "Dong", ""]]}, {"id": "1412.8285", "submitter": "Piotr Zwiernik", "authors": "Mathias Drton, Shaowei Lin, Luca Weihs and Piotr Zwiernik", "title": "Marginal likelihood and model selection for Gaussian latent tree and\n  forest models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian latent tree models, or more generally, Gaussian latent forest models\nhave Fisher-information matrices that become singular along interesting\nsubmodels, namely, models that correspond to subforests. For these\nsingularities, we compute the real log-canonical thresholds (also known as\nstochastic complexities or learning coefficients) that quantify the\nlarge-sample behavior of the marginal likelihood in Bayesian inference. This\nprovides the information needed for a recently introduced generalization of the\nBayesian information criterion. Our mathematical developments treat the general\nsetting of Laplace integrals whose phase functions are sums of squared\ndifferences between monomials and constants. We clarify how in this case real\nlog-canonical thresholds can be computed using polyhedral geometry, and we show\nhow to apply the general theory to the Laplace integrals associated with\nGaussian latent tree and forest models. In simulations and a data example, we\ndemonstrate how the mathematical knowledge can be applied in model selection.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 09:10:17 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2015 02:19:35 GMT"}], "update_date": "2015-12-24", "authors_parsed": [["Drton", "Mathias", ""], ["Lin", "Shaowei", ""], ["Weihs", "Luca", ""], ["Zwiernik", "Piotr", ""]]}, {"id": "1412.8293", "submitter": "Jiyan Yang", "authors": "Haim Avron, Vikas Sindhwani, Jiyan Yang, Michael Mahoney", "title": "Quasi-Monte Carlo Feature Maps for Shift-Invariant Kernels", "comments": "A short version of this paper has been presented in ICML 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of improving the efficiency of randomized Fourier\nfeature maps to accelerate training and testing speed of kernel methods on\nlarge datasets. These approximate feature maps arise as Monte Carlo\napproximations to integral representations of shift-invariant kernel functions\n(e.g., Gaussian kernel). In this paper, we propose to use Quasi-Monte Carlo\n(QMC) approximations instead, where the relevant integrands are evaluated on a\nlow-discrepancy sequence of points as opposed to random point sets as in the\nMonte Carlo approach. We derive a new discrepancy measure called box\ndiscrepancy based on theoretical characterizations of the integration error\nwith respect to a given sequence. We then propose to learn QMC sequences\nadapted to our setting based on explicit box discrepancy minimization. Our\ntheoretical analyses are complemented with empirical results that demonstrate\nthe effectiveness of classical and adaptive QMC techniques for this problem.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 10:00:39 GMT"}, {"version": "v2", "created": "Sun, 9 Aug 2015 07:20:00 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Avron", "Haim", ""], ["Sindhwani", "Vikas", ""], ["Yang", "Jiyan", ""], ["Mahoney", "Michael", ""]]}, {"id": "1412.8380", "submitter": "Hidetoshi Shimodaira", "authors": "Hidetoshi Shimodaira", "title": "A simple coding for cross-domain matching with dimension reduction via\n  spectral graph embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data vectors are obtained from multiple domains. They are feature vectors of\nimages or vector representations of words. Domains may have different numbers\nof data vectors with different dimensions. These data vectors from multiple\ndomains are projected to a common space by linear transformations in order to\nsearch closely related vectors across domains. We would like to find projection\nmatrices to minimize distances between closely related data vectors. This\nformulation of cross-domain matching is regarded as an extension of the\nspectral graph embedding to multi-domain setting, and it includes several\nmultivariate analysis methods of statistics such as multiset canonical\ncorrelation analysis, correspondence analysis, and principal component\nanalysis. Similar approaches are very popular recently in pattern recognition\nand vision. In this paper, instead of proposing a novel method, we will\nintroduce an embarrassingly simple idea of coding the data vectors for\nexplaining all the above mentioned approaches. A data vector is concatenated\nwith zero vectors from all other domains to make an augmented vector. The\ncross-domain matching is solved by applying the single-domain version of\nspectral graph embedding to these augmented vectors of all the domains. An\ninteresting connection to the classical associative memory model of neural\nnetworks is also discussed by noticing a coding for association. A\ncross-validation method for choosing the dimension of the common space and a\nregularization parameter will be discussed in an illustrative numerical\nexample.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 16:08:27 GMT"}, {"version": "v2", "created": "Sun, 29 Mar 2015 18:38:26 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Shimodaira", "Hidetoshi", ""]]}, {"id": "1412.8464", "submitter": "Yan Kaganovsky Dr.", "authors": "Yan Kaganovsky, Shaobo Han, Soysal Degirmenci, David G. Politte, David\n  J. Brady, Joseph A. O'Sullivan and Lawrence Carin", "title": "Alternating Minimization Algorithm with Automatic Relevance\n  Determination for Transmission Tomography under Poisson Noise", "comments": "Changes relative to the previous version: (1) Minor changes in\n  abstract; (2) The results for simulated data contain another comparison to a\n  previous method; (3) Corrected Eq. (6.3); (4) Additional discussions in\n  Secs.1,7,8", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a globally convergent alternating minimization (AM) algorithm for\nimage reconstruction in transmission tomography, which extends automatic\nrelevance determination (ARD) to Poisson noise models with Beer's law. The\nalgorithm promotes solutions that are sparse in the pixel/voxel-differences\ndomain by introducing additional latent variables, one for each pixel/voxel,\nand then learning these variables from the data using a hierarchical Bayesian\nmodel. Importantly, the proposed AM algorithm is free of any tuning parameters\nwith image quality comparable to standard penalized likelihood methods. Our\nalgorithm exploits optimization transfer principles which reduce the problem\ninto parallel 1D optimization tasks (one for each pixel/voxel), making the\nalgorithm feasible for large-scale problems. This approach considerably reduces\nthe computational bottleneck of ARD associated with the posterior variances.\nPositivity constraints inherent in transmission tomography problems are also\nenforced. We demonstrate the performance of the proposed algorithm for x-ray\ncomputed tomography using synthetic and real-world datasets. The algorithm is\nshown to have much better performance than prior ARD algorithms based on\napproximate Gaussian noise models, even for high photon flux.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 20:56:36 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2015 16:16:12 GMT"}], "update_date": "2015-08-12", "authors_parsed": [["Kaganovsky", "Yan", ""], ["Han", "Shaobo", ""], ["Degirmenci", "Soysal", ""], ["Politte", "David G.", ""], ["Brady", "David J.", ""], ["O'Sullivan", "Joseph A.", ""], ["Carin", "Lawrence", ""]]}, {"id": "1412.8493", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "title": "An ADMM algorithm for solving a proximal bound-constrained quadratic\n  program", "comments": "5 pages, 1 figure. arXiv admin note: text overlap with\n  arXiv:1405.5960", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a proximal operator given by a quadratic function subject to\nbound constraints and give an optimization algorithm using the alternating\ndirection method of multipliers (ADMM). The algorithm is particularly efficient\nto solve a collection of proximal operators that share the same quadratic form,\nor if the quadratic program is the relaxation of a binary quadratic problem.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 21:35:19 GMT"}], "update_date": "2014-12-31", "authors_parsed": [["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""]]}, {"id": "1412.8566", "submitter": "Roger Grosse", "authors": "Yuri Burda and Roger B. Grosse and Ruslan Salakhutdinov", "title": "Accurate and Conservative Estimates of MRF Log-likelihood using Reverse\n  Annealing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov random fields (MRFs) are difficult to evaluate as generative models\nbecause computing the test log-probabilities requires the intractable partition\nfunction. Annealed importance sampling (AIS) is widely used to estimate MRF\npartition functions, and often yields quite accurate results. However, AIS is\nprone to overestimate the log-likelihood with little indication that anything\nis wrong. We present the Reverse AIS Estimator (RAISE), a stochastic lower\nbound on the log-likelihood of an approximation to the original MRF model.\nRAISE requires only the same MCMC transition operators as standard AIS.\nExperimental results indicate that RAISE agrees closely with AIS\nlog-probability estimates for RBMs, DBMs, and DBNs, but typically errs on the\nside of underestimating, rather than overestimating, the log-likelihood.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 06:13:10 GMT"}], "update_date": "2014-12-31", "authors_parsed": [["Burda", "Yuri", ""], ["Grosse", "Roger B.", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1412.8697", "submitter": "Han Liu", "authors": "Zhuoran Yang, Yang Ning, Han Liu", "title": "On Semiparametric Exponential Family Graphical Models", "comments": "51 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new class of semiparametric exponential family graphical models\nfor the analysis of high dimensional mixed data. Different from the existing\nmixed graphical models, we allow the nodewise conditional distributions to be\nsemiparametric generalized linear models with unspecified base measure\nfunctions. Thus, one advantage of our method is that it is unnecessary to\nspecify the type of each node and the method is more convenient to apply in\npractice. Under the proposed model, we consider both problems of parameter\nestimation and hypothesis testing in high dimensions. In particular, we propose\na symmetric pairwise score test for the presence of a single edge in the graph.\nCompared to the existing methods for hypothesis tests, our approach takes into\naccount of the symmetry of the parameters, such that the inferential results\nare invariant with respect to the different parametrizations of the same edge.\nThorough numerical simulations and a real data example are provided to back up\nour results.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 17:39:48 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2015 05:29:36 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Yang", "Zhuoran", ""], ["Ning", "Yang", ""], ["Liu", "Han", ""]]}, {"id": "1412.8724", "submitter": "Tianqi Zhao", "authors": "Tianqi Zhao, Mladen Kolar, Han Liu", "title": "A General Framework for Robust Testing and Confidence Regions in\n  High-Dimensional Quantile Regression", "comments": "70 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a robust inferential procedure for assessing uncertainties of\nparameter estimation in high-dimensional linear models, where the dimension $p$\ncan grow exponentially fast with the sample size $n$. Our method combines the\nde-biasing technique with the composite quantile function to construct an\nestimator that is asymptotically normal. Hence it can be used to construct\nvalid confidence intervals and conduct hypothesis tests. Our estimator is\nrobust and does not require the existence of first or second moment of the\nnoise distribution. It also preserves efficiency in the sense that the worst\ncase efficiency loss is less than 30\\% compared to the square-loss-based\nde-biased Lasso estimator. In many cases our estimator is close to or better\nthan the latter, especially when the noise is heavy-tailed. Our de-biasing\nprocedure does not require solving the $L_1$-penalized composite quantile\nregression. Instead, it allows for any first-stage estimator with desired\nconvergence rate and empirical sparsity. The paper also provides new proof\ntechniques for developing theoretical guarantees of inferential procedures with\nnon-smooth loss functions. To establish the main results, we exploit the local\ncurvature of the conditional expectation of composite quantile loss and apply\nempirical process theories to control the difference between empirical\nquantities and their conditional expectations. Our results are established\nunder weaker assumptions compared to existing work on inference for\nhigh-dimensional quantile regression. Furthermore, we consider a\nhigh-dimensional simultaneous test for the regression parameters by applying\nthe Gaussian approximation and multiplier bootstrap theories. We also study\ndistributed learning and exploit the divide-and-conquer estimator to reduce\ncomputation complexity when the sample size is massive. Finally, we provide\nempirical results to verify the theory.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 18:44:04 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2015 03:37:40 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Zhao", "Tianqi", ""], ["Kolar", "Mladen", ""], ["Liu", "Han", ""]]}, {"id": "1412.8729", "submitter": "Han Liu", "authors": "Zhaoran Wang, Quanquan Gu, Yang Ning, Han Liu", "title": "High Dimensional Expectation-Maximization Algorithm: Statistical\n  Optimization and Asymptotic Normality", "comments": "84 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a general theory of the expectation-maximization (EM) algorithm\nfor inferring high dimensional latent variable models. In particular, we make\ntwo contributions: (i) For parameter estimation, we propose a novel high\ndimensional EM algorithm which naturally incorporates sparsity structure into\nparameter estimation. With an appropriate initialization, this algorithm\nconverges at a geometric rate and attains an estimator with the (near-)optimal\nstatistical rate of convergence. (ii) Based on the obtained estimator, we\npropose new inferential procedures for testing hypotheses and constructing\nconfidence intervals for low dimensional components of high dimensional\nparameters. For a broad family of statistical models, our framework establishes\nthe first computationally feasible approach for optimal estimation and\nasymptotic inference in high dimensions. Our theory is supported by thorough\nnumerical results.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 18:53:22 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 03:52:55 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Wang", "Zhaoran", ""], ["Gu", "Quanquan", ""], ["Ning", "Yang", ""], ["Liu", "Han", ""]]}, {"id": "1412.8765", "submitter": "Han Liu", "authors": "Yang Ning, Han Liu", "title": "A General Theory of Hypothesis Tests and Confidence Regions for Sparse\n  High Dimensional Models", "comments": "80 pages, 1 figure, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of uncertainty assessment for low dimensional\ncomponents in high dimensional models. Specifically, we propose a decorrelated\nscore function to handle the impact of high dimensional nuisance parameters. We\nconsider both hypothesis tests and confidence regions for generic penalized\nM-estimators. Unlike most existing inferential methods which are tailored for\nindividual models, our approach provides a general framework for high\ndimensional inference and is applicable to a wide range of applications. From\nthe testing perspective, we develop general theorems to characterize the\nlimiting distributions of the decorrelated score test statistic under both null\nhypothesis and local alternatives. These results provide asymptotic guarantees\non the type I errors and local powers of the proposed test. Furthermore, we\nshow that the decorrelated score function can be used to construct point and\nconfidence region estimators that are semiparametrically efficient. We also\ngeneralize this framework to broaden its applications. First, we extend it to\nhandle high dimensional null hypothesis, where the number of parameters of\ninterest can increase exponentially fast with the sample size. Second, we\nestablish the theory for model misspecification. Third, we go beyond the\nlikelihood framework, by introducing the generalized score test based on\ngeneral loss functions. Thorough numerical studies are conducted to back up the\ndeveloped theoretical results.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 20:54:27 GMT"}, {"version": "v2", "created": "Wed, 21 Jan 2015 06:50:07 GMT"}], "update_date": "2015-01-22", "authors_parsed": [["Ning", "Yang", ""], ["Liu", "Han", ""]]}]