[{"id": "1508.00085", "submitter": "Ikko Yamane", "authors": "Ikko Yamane, Hiroaki Sasaki, Masashi Sugiyama", "title": "Regularized Multi-Task Learning for Multi-Dimensional Log-Density\n  Gradient Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Log-density gradient estimation is a fundamental statistical problem and\npossesses various practical applications such as clustering and measuring\nnon-Gaussianity. A naive two-step approach of first estimating the density and\nthen taking its log-gradient is unreliable because an accurate density estimate\ndoes not necessarily lead to an accurate log-density gradient estimate. To cope\nwith this problem, a method to directly estimate the log-density gradient\nwithout density estimation has been explored, and demonstrated to work much\nbetter than the two-step method. The objective of this paper is to further\nimprove the performance of this direct method in multi-dimensional cases. Our\nidea is to regard the problem of log-density gradient estimation in each\ndimension as a task, and apply regularized multi-task learning to the direct\nlog-density gradient estimator. We experimentally demonstrate the usefulness of\nthe proposed multi-task method in log-density gradient estimation and\nmode-seeking clustering.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2015 06:43:02 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Yamane", "Ikko", ""], ["Sasaki", "Hiroaki", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1508.00299", "submitter": "Pin-Yu Chen", "authors": "Pin-Yu Chen, Shin-Ming Cheng, Pai-Shun Ting, Chia-Wei Lien, Fu-Jen Chu", "title": "When Crowdsourcing Meets Mobile Sensing: A Social Network Perspective", "comments": "To appear in Oct. IEEE Communications Magazine, feature topic on\n  \"Social Networks Meet Next Generation Mobile Multimedia Internet\"", "journal-ref": null, "doi": "10.1109/MCOM.2015.7295478", "report-no": null, "categories": "cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile sensing is an emerging technology that utilizes agent-participatory\ndata for decision making or state estimation, including multimedia\napplications. This article investigates the structure of mobile sensing schemes\nand introduces crowdsourcing methods for mobile sensing. Inspired by social\nnetwork, one can establish trust among participatory agents to leverage the\nwisdom of crowds for mobile sensing. A prototype of social network inspired\nmobile multimedia and sensing application is presented for illustrative\npurpose. Numerical experiments on real-world datasets show improved performance\nof mobile sensing via crowdsourcing. Challenges for mobile sensing with respect\nto Internet layers are discussed.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 02:01:06 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Chen", "Pin-Yu", ""], ["Cheng", "Shin-Ming", ""], ["Ting", "Pai-Shun", ""], ["Lien", "Chia-Wei", ""], ["Chu", "Fu-Jen", ""]]}, {"id": "1508.00317", "submitter": "Roni Mittelman Roni Mittelman", "authors": "Roni Mittelman", "title": "Time-series modeling with undecimated fully convolutional neural\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new convolutional neural network-based time-series model.\nTypical convolutional neural network (CNN) architectures rely on the use of\nmax-pooling operators in between layers, which leads to reduced resolution at\nthe top layers. Instead, in this work we consider a fully convolutional network\n(FCN) architecture that uses causal filtering operations, and allows for the\nrate of the output signal to be the same as that of the input signal. We\nfurthermore propose an undecimated version of the FCN, which we refer to as the\nundecimated fully convolutional neural network (UFCNN), and is motivated by the\nundecimated wavelet transform. Our experimental results verify that using the\nundecimated version of the FCN is necessary in order to allow for effective\ntime-series modeling. The UFCNN has several advantages compared to other\ntime-series models such as the recurrent neural network (RNN) and long\nshort-term memory (LSTM), since it does not suffer from either the vanishing or\nexploding gradients problems, and is therefore easier to train. Convolution\noperations can also be implemented more efficiently compared to the recursion\nthat is involved in RNN-based models. We evaluate the performance of our model\nin a synthetic target tracking task using bearing only measurements generated\nfrom a state-space model, a probabilistic modeling of polyphonic music\nsequences problem, and a high frequency trading task using a time-series of\nask/bid quotes and their corresponding volumes. Our experimental results using\nsynthetic and real datasets verify the significant advantages of the UFCNN\ncompared to the RNN and LSTM baselines.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 05:58:52 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Mittelman", "Roni", ""]]}, {"id": "1508.00451", "submitter": "Rein Houthooft", "authors": "Rein Houthooft, Filip De Turck", "title": "Integrated Inference and Learning of Neural Factors in Structural\n  Support Vector Machines", "comments": null, "journal-ref": null, "doi": "10.1016/j.patcog.2016.03.014", "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tackling pattern recognition problems in areas such as computer vision,\nbioinformatics, speech or text recognition is often done best by taking into\naccount task-specific statistical relations between output variables. In\nstructured prediction, this internal structure is used to predict multiple\noutputs simultaneously, leading to more accurate and coherent predictions.\nStructural support vector machines (SSVMs) are nonprobabilistic models that\noptimize a joint input-output function through margin-based learning. Because\nSSVMs generally disregard the interplay between unary and interaction factors\nduring the training phase, final parameters are suboptimal. Moreover, its\nfactors are often restricted to linear combinations of input features, limiting\nits generalization power. To improve prediction accuracy, this paper proposes:\n(i) Joint inference and learning by integration of back-propagation and\nloss-augmented inference in SSVM subgradient descent; (ii) Extending SSVM\nfactors to neural networks that form highly nonlinear functions of input\nfeatures. Image segmentation benchmark results demonstrate improvements over\nconventional SSVM training methods in terms of accuracy, highlighting the\nfeasibility of end-to-end SSVM training with neural factors.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 15:29:57 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2015 12:41:52 GMT"}, {"version": "v3", "created": "Wed, 12 Aug 2015 16:02:00 GMT"}, {"version": "v4", "created": "Thu, 3 Mar 2016 22:46:17 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Houthooft", "Rein", ""], ["De Turck", "Filip", ""]]}, {"id": "1508.00459", "submitter": "Ka-Chun Wong", "authors": "Ka-Chun Wong, Yue Li, Zhaolei Zhang", "title": "Unsupervised Learning in Genome Informatics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN q-bio.QM stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With different genomes available, unsupervised learning algorithms are\nessential in learning genome-wide biological insights. Especially, the\nfunctional characterization of different genomes is essential for us to\nunderstand lives. In this book chapter, we review the state-of-the-art\nunsupervised learning algorithms for genome informatics from DNA to MicroRNA.\n  DNA (DeoxyriboNucleic Acid) is the basic component of genomes. A significant\nfraction of DNA regions (transcription factor binding sites) are bound by\nproteins (transcription factors) to regulate gene expression at different\ndevelopment stages in different tissues. To fully understand genetics, it is\nnecessary of us to apply unsupervised learning algorithms to learn and infer\nthose DNA regions. Here we review several unsupervised learning methods for\ndeciphering the genome-wide patterns of those DNA regions.\n  MicroRNA (miRNA), a class of small endogenous non-coding RNA (RiboNucleic\nacid) species, regulate gene expression post-transcriptionally by forming\nimperfect base-pair with the target sites primarily at the 3$'$ untranslated\nregions of the messenger RNAs. Since the 1993 discovery of the first miRNA\n\\emph{let-7} in worms, a vast amount of studies have been dedicated to\nfunctionally characterizing the functional impacts of miRNA in a network\ncontext to understand complex diseases such as cancer. Here we review several\nrepresentative unsupervised learning frameworks on inferring miRNA regulatory\nnetwork by exploiting the static sequence-based information pertinent to the\nprior knowledge of miRNA targeting and the dynamic information of miRNA\nactivities implicated by the recently available large data compendia, which\ninterrogate genome-wide expression profiles of miRNAs and/or mRNAs across\nvarious cell conditions.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 15:52:38 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Wong", "Ka-Chun", ""], ["Li", "Yue", ""], ["Zhang", "Zhaolei", ""]]}, {"id": "1508.00625", "submitter": "Megasthenis Asteris", "authors": "Megasthenis Asteris, Dimitris Papailiopoulos, Anastasios Kyrillidis,\n  Alexandros G. Dimakis", "title": "Sparse PCA via Bipartite Matchings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following multi-component sparse PCA problem: given a set of\ndata points, we seek to extract a small number of sparse components with\ndisjoint supports that jointly capture the maximum possible variance. These\ncomponents can be computed one by one, repeatedly solving the single-component\nproblem and deflating the input data matrix, but as we show this greedy\nprocedure is suboptimal. We present a novel algorithm for sparse PCA that\njointly optimizes multiple disjoint components. The extracted features capture\nvariance that lies within a multiplicative factor arbitrarily close to 1 from\nthe optimal. Our algorithm is combinatorial and computes the desired components\nby solving multiple instances of the bipartite maximum weight matching problem.\nIts complexity grows as a low order polynomial in the ambient dimension of the\ninput data matrix, but exponentially in its rank. However, it can be\neffectively applied on a low-dimensional sketch of the data; this allows us to\nobtain polynomial-time approximation guarantees via spectral bounds. We\nevaluate our algorithm on real data-sets and empirically demonstrate that in\nmany cases it outperforms existing, deflation-based approaches.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 00:12:35 GMT"}], "update_date": "2015-08-05", "authors_parsed": [["Asteris", "Megasthenis", ""], ["Papailiopoulos", "Dimitris", ""], ["Kyrillidis", "Anastasios", ""], ["Dimakis", "Alexandros G.", ""]]}, {"id": "1508.00635", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi", "title": "Bayesian mixtures of spatial spline regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work relates the framework of model-based clustering for spatial\nfunctional data where the data are surfaces. We first introduce a Bayesian\nspatial spline regression model with mixed-effects (BSSR) for modeling spatial\nfunction data. The BSSR model is based on Nodal basis functions for spatial\nregression and accommodates both common mean behavior for the data through a\nfixed-effects part, and variability inter-individuals thanks to a\nrandom-effects part. Then, in order to model populations of spatial functional\ndata issued from heterogeneous groups, we integrate the BSSR model into a\nmixture framework. The resulting model is a Bayesian mixture of spatial spline\nregressions with mixed-effects (BMSSR) used for density estimation and\nmodel-based surface clustering. The models, through their Bayesian formulation,\nallow to integrate possible prior knowledge on the data structure and\nconstitute a good alternative to recent mixture of spatial spline regressions\nmodel estimated in a maximum likelihood framework via the\nexpectation-maximization (EM) algorithm. The Bayesian model inference is\nperformed by Markov Chain Monte Carlo (MCMC) sampling. We derive two Gibbs\nsampler to infer the BSSR and the BMSSR models and apply them on simulated\nsurfaces and a real problem of handwritten digit recognition using the MNIST\ndata set. The obtained results highlight the potential benefit of the proposed\nBayesian approaches for modeling surfaces possibly dispersed in particular in\nclusters.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 01:29:49 GMT"}], "update_date": "2015-08-05", "authors_parsed": [["Chamroukhi", "Faicel", ""]]}, {"id": "1508.00641", "submitter": "Cem Tekin", "authors": "Cem Tekin and Mihaela van der Schaar", "title": "Episodic Multi-armed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new class of reinforcement learning methods referred to as\n{\\em episodic multi-armed bandits} (eMAB). In eMAB the learner proceeds in {\\em\nepisodes}, each composed of several {\\em steps}, in which it chooses an action\nand observes a feedback signal. Moreover, in each step, it can take a special\naction, called the $stop$ action, that ends the current episode. After the\n$stop$ action is taken, the learner collects a terminal reward, and observes\nthe costs and terminal rewards associated with each step of the episode. The\ngoal of the learner is to maximize its cumulative gain (i.e., the terminal\nreward minus costs) over all episodes by learning to choose the best sequence\nof actions based on the feedback. First, we define an {\\em oracle} benchmark,\nwhich sequentially selects the actions that maximize the expected immediate\ngain. Then, we propose our online learning algorithm, named {\\em FeedBack\nAdaptive Learning} (FeedBAL), and prove that its regret with respect to the\nbenchmark is bounded with high probability and increases logarithmically in\nexpectation. Moreover, the regret only has polynomial dependence on the number\nof steps, actions and states. eMAB can be used to model applications that\ninvolve humans in the loop, ranging from personalized medical screening to\npersonalized web-based education, where sequences of actions are taken in each\nepisode, and optimal behavior requires adapting the chosen actions based on the\nfeedback.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 01:52:42 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2015 02:53:20 GMT"}, {"version": "v3", "created": "Thu, 4 May 2017 18:16:26 GMT"}, {"version": "v4", "created": "Sun, 11 Mar 2018 20:17:39 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Tekin", "Cem", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1508.00655", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas, Sashank J. Reddi, Barnabas Poczos, Aarti Singh, Larry\n  Wasserman", "title": "Adaptivity and Computation-Statistics Tradeoffs for Kernel and Distance\n  based High Dimensional Two Sample Testing", "comments": "35 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric two sample testing is a decision theoretic problem that\ninvolves identifying differences between two random variables without making\nparametric assumptions about their underlying distributions. We refer to the\nmost common settings as mean difference alternatives (MDA), for testing\ndifferences only in first moments, and general difference alternatives (GDA),\nwhich is about testing for any difference in distributions. A large number of\ntest statistics have been proposed for both these settings. This paper connects\nthree classes of statistics - high dimensional variants of Hotelling's t-test,\nstatistics based on Reproducing Kernel Hilbert Spaces, and energy statistics\nbased on pairwise distances. We ask the question: how much statistical power do\npopular kernel and distance based tests for GDA have when the unknown\ndistributions differ in their means, compared to specialized tests for MDA?\n  We formally characterize the power of popular tests for GDA like the Maximum\nMean Discrepancy with the Gaussian kernel (gMMD) and bandwidth-dependent\nvariants of the Energy Distance with the Euclidean norm (eED) in the\nhigh-dimensional MDA regime. Some practically important properties include (a)\neED and gMMD have asymptotically equal power; furthermore they enjoy a free\nlunch because, while they are additionally consistent for GDA, they also have\nthe same power as specialized high-dimensional t-test variants for MDA. All\nthese tests are asymptotically optimal (including matching constants) under MDA\nfor spherical covariances, according to simple lower bounds, (b) The power of\ngMMD is independent of the kernel bandwidth, as long as it is larger than the\nchoice made by the median heuristic, (c) There is a clear and smooth\ncomputation-statistics tradeoff for linear-time, subquadratic-time and\nquadratic-time versions of these tests, with more computation resulting in\nhigher power.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 04:10:05 GMT"}], "update_date": "2015-08-05", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Reddi", "Sashank J.", ""], ["Poczos", "Barnabas", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1508.00842", "submitter": "Sougata Chaudhuri", "authors": "Sougata Chaudhuri and Ambuj Tewari", "title": "Perceptron like Algorithms for Online Learning to Rank", "comments": "Under review in Journal of Artificial Intelligence Research (JAIR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perceptron is a classic online algorithm for learning a classification\nfunction. In this paper, we provide a novel extension of the perceptron\nalgorithm to the learning to rank problem in information retrieval. We consider\npopular listwise performance measures such as Normalized Discounted Cumulative\nGain (NDCG) and Average Precision (AP). A modern perspective on perceptron for\nclassification is that it is simply an instance of online gradient descent\n(OGD), during mistake rounds, using the hinge loss function. Motivated by this\ninterpretation, we propose a novel family of listwise, large margin ranking\nsurrogates. Members of this family can be thought of as analogs of the hinge\nloss. Exploiting a certain self-bounding property of the proposed family, we\nprovide a guarantee on the cumulative NDCG (or AP) induced loss incurred by our\nperceptron-like algorithm. We show that, if there exists a perfect oracle\nranker which can correctly rank each instance in an online sequence of ranking\ndata, with some margin, the cumulative loss of perceptron algorithm on that\nsequence is bounded by a constant, irrespective of the length of the sequence.\nThis result is reminiscent of Novikoff's convergence theorem for the\nclassification perceptron. Moreover, we prove a lower bound on the cumulative\nloss achievable by any deterministic algorithm, under the assumption of\nexistence of perfect oracle ranker. The lower bound shows that our perceptron\nbound is not tight, and we propose another, \\emph{purely online}, algorithm\nwhich achieves the lower bound. We provide empirical results on simulated and\nlarge commercial datasets to corroborate our theoretical results.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 17:23:46 GMT"}, {"version": "v2", "created": "Sun, 6 Mar 2016 20:32:04 GMT"}, {"version": "v3", "created": "Sun, 27 Mar 2016 00:29:55 GMT"}, {"version": "v4", "created": "Tue, 23 Aug 2016 06:52:04 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Chaudhuri", "Sougata", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1508.00882", "submitter": "John Duchi", "authors": "John C. Duchi and Sorathan Chaturapruek and Christopher R\\'e", "title": "Asynchronous stochastic convex optimization", "comments": "38 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that asymptotically, completely asynchronous stochastic gradient\nprocedures achieve optimal (even to constant factors) convergence rates for the\nsolution of convex optimization problems under nearly the same conditions\nrequired for asymptotic optimality of standard stochastic gradient procedures.\nRoughly, the noise inherent to the stochastic approximation scheme dominates\nany noise from asynchrony. We also give empirical evidence demonstrating the\nstrong performance of asynchronous, parallel stochastic optimization schemes,\ndemonstrating that the robustness inherent to stochastic approximation problems\nallows substantially faster parallel and asynchronous solution methods.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 19:35:49 GMT"}], "update_date": "2015-08-05", "authors_parsed": [["Duchi", "John C.", ""], ["Chaturapruek", "Sorathan", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1508.00945", "submitter": "Jean Honorio", "authors": "Jean Honorio, Tommi Jaakkola", "title": "Structured Prediction: From Gaussian Perturbations to Linear-Time\n  Principled Algorithms", "comments": "Uncertainty in Artificial Intelligence (UAI) 2016", "journal-ref": "Uncertainty in Artificial Intelligence (UAI), 2016", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Margin-based structured prediction commonly uses a maximum loss over all\npossible structured outputs \\cite{Altun03,Collins04b,Taskar03}. In natural\nlanguage processing, recent work \\cite{Zhang14,Zhang15} has proposed the use of\nthe maximum loss over random structured outputs sampled independently from some\nproposal distribution. This method is linear-time in the number of random\nstructured outputs and trivially parallelizable. We study this family of loss\nfunctions in the PAC-Bayes framework under Gaussian perturbations\n\\cite{McAllester07}. Under some technical conditions and up to statistical\naccuracy, we show that this family of loss functions produces a tighter upper\nbound of the Gibbs decoder distortion than commonly used methods. Thus, using\nthe maximum loss over random structured outputs is a principled way of learning\nthe parameter of structured prediction models. Besides explaining the\nexperimental success of \\cite{Zhang14,Zhang15}, our theoretical results show\nthat more general techniques are possible.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 00:22:39 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2015 16:00:05 GMT"}, {"version": "v3", "created": "Wed, 2 Mar 2016 13:36:25 GMT"}, {"version": "v4", "created": "Sat, 4 Jun 2016 00:19:13 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Honorio", "Jean", ""], ["Jaakkola", "Tommi", ""]]}, {"id": "1508.00973", "submitter": "Peixian Chen", "authors": "Peixian Chen, Nevin L. Zhang, Leonard K.M. Poon, Zhourong Chen", "title": "Progressive EM for Latent Tree Models and Hierarchical Topic Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical latent tree analysis (HLTA) is recently proposed as a new method\nfor topic detection. It differs fundamentally from the LDA-based methods in\nterms of topic definition, topic-document relationship, and learning method. It\nhas been shown to discover significantly more coherent topics and better topic\nhierarchies. However, HLTA relies on the Expectation-Maximization (EM)\nalgorithm for parameter estimation and hence is not efficient enough to deal\nwith large datasets. In this paper, we propose a method to drastically speed up\nHLTA using a technique inspired by recent advances in the moments method.\nEmpirical experiments show that our method greatly improves the efficiency of\nHLTA. It is as efficient as the state-of-the-art LDA-based method for\nhierarchical topic detection and finds substantially better topics and topic\nhierarchies.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 05:00:32 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Chen", "Peixian", ""], ["Zhang", "Nevin L.", ""], ["Poon", "Leonard K. M.", ""], ["Chen", "Zhourong", ""]]}, {"id": "1508.01019", "submitter": "Voot Tangkaratt", "authors": "Voot Tangkaratt, Hiroaki Sasaki, and Masashi Sugiyama", "title": "Direct Estimation of the Derivative of Quadratic Mutual Information with\n  Application in Supervised Dimension Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A typical goal of supervised dimension reduction is to find a low-dimensional\nsubspace of the input space such that the projected input variables preserve\nmaximal information about the output variables. The dependence maximization\napproach solves the supervised dimension reduction problem through maximizing a\nstatistical dependence between projected input variables and output variables.\nA well-known statistical dependence measure is mutual information (MI) which is\nbased on the Kullback-Leibler (KL) divergence. However, it is known that the KL\ndivergence is sensitive to outliers. On the other hand, quadratic MI (QMI) is a\nvariant of MI based on the $L_2$ distance which is more robust against outliers\nthan the KL divergence, and a computationally efficient method to estimate QMI\nfrom data, called least-squares QMI (LSQMI), has been proposed recently. For\nthese reasons, developing a supervised dimension reduction method based on\nLSQMI seems promising. However, not QMI itself, but the derivative of QMI is\nneeded for subspace search in supervised dimension reduction, and the\nderivative of an accurate QMI estimator is not necessarily a good estimator of\nthe derivative of QMI. In this paper, we propose to directly estimate the\nderivative of QMI without estimating QMI itself. We show that the direct\nestimation of the derivative of QMI is more accurate than the derivative of the\nestimated QMI. Finally, we develop a supervised dimension reduction algorithm\nwhich efficiently uses the proposed derivative estimator, and demonstrate\nthrough experiments that the proposed method is more robust against outliers\nthan existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 09:44:32 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Tangkaratt", "Voot", ""], ["Sasaki", "Hiroaki", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1508.01071", "submitter": "Rodrigo Carvajal", "authors": "Rodrigo Carvajal, Juan C. Ag\\\"uero, Boris I. Godoy and Dimitrios\n  Katselis", "title": "A MAP approach for $\\ell_q$-norm regularized sparse parameter estimation\n  using the EM algorithm", "comments": "Accepted to IEEE Machine Learning for Signal Processing Conference\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, Bayesian parameter estimation through the consideration of the\nMaximum A Posteriori (MAP) criterion is revisited under the prism of the\nExpectation-Maximization (EM) algorithm. By incorporating a sparsity-promoting\npenalty term in the cost function of the estimation problem through the use of\nan appropriate prior distribution, we show how the EM algorithm can be used to\nefficiently solve the corresponding optimization problem. To this end, we rely\non variance-mean Gaussian mixtures (VMGM) to describe the prior distribution,\nwhile we incorporate many nice features of these mixtures to our estimation\nproblem. The corresponding MAP estimation problem is completely expressed in\nterms of the EM algorithm, which allows for handling nonlinearities and hidden\nvariables that cannot be easily handled with traditional methods. For\ncomparison purposes, we also develop a Coordinate Descent algorithm for the\n$\\ell_q$-norm penalized problem and present the performance results via\nsimulations.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 13:24:15 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Carvajal", "Rodrigo", ""], ["Ag\u00fcero", "Juan C.", ""], ["Godoy", "Boris I.", ""], ["Katselis", "Dimitrios", ""]]}, {"id": "1508.01211", "submitter": "Navdeep Jaitly", "authors": "William Chan and Navdeep Jaitly and Quoc V. Le and Oriol Vinyals", "title": "Listen, Attend and Spell", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Listen, Attend and Spell (LAS), a neural network that learns to\ntranscribe speech utterances to characters. Unlike traditional DNN-HMM models,\nthis model learns all the components of a speech recognizer jointly. Our system\nhas two components: a listener and a speller. The listener is a pyramidal\nrecurrent network encoder that accepts filter bank spectra as inputs. The\nspeller is an attention-based recurrent network decoder that emits characters\nas outputs. The network produces character sequences without making any\nindependence assumptions between the characters. This is the key improvement of\nLAS over previous end-to-end CTC models. On a subset of the Google voice search\ntask, LAS achieves a word error rate (WER) of 14.1% without a dictionary or a\nlanguage model, and 10.3% with language model rescoring over the top 32 beams.\nBy comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0%.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 20:17:58 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2015 00:38:43 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Chan", "William", ""], ["Jaitly", "Navdeep", ""], ["Le", "Quoc V.", ""], ["Vinyals", "Oriol", ""]]}, {"id": "1508.01217", "submitter": "Lorin Crawford", "authors": "Lorin Crawford, Kris C. Wood, Xiang Zhou, and Sayan Mukherjee", "title": "Bayesian Approximate Kernel Regression with Variable Selection", "comments": "22 pages, 3 figures, 3 tables; theory added; new simulations\n  presented; references added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear kernel regression models are often used in statistics and machine\nlearning because they are more accurate than linear models. Variable selection\nfor kernel regression models is a challenge partly because, unlike the linear\nregression setting, there is no clear concept of an effect size for regression\ncoefficients. In this paper, we propose a novel framework that provides an\neffect size analog of each explanatory variable for Bayesian kernel regression\nmodels when the kernel is shift-invariant --- for example, the Gaussian kernel.\nWe use function analytic properties of shift-invariant reproducing kernel\nHilbert spaces (RKHS) to define a linear vector space that: (i) captures\nnonlinear structure, and (ii) can be projected onto the original explanatory\nvariables. The projection onto the original explanatory variables serves as an\nanalog of effect sizes. The specific function analytic property we use is that\nshift-invariant kernel functions can be approximated via random Fourier bases.\nBased on the random Fourier expansion we propose a computationally efficient\nclass of Bayesian approximate kernel regression (BAKR) models for both\nnonlinear regression and binary classification for which one can compute an\nanalog of effect sizes. We illustrate the utility of BAKR by examining two\nimportant problems in statistical genetics: genomic selection (i.e. phenotypic\nprediction) and association mapping (i.e. inference of significant variants or\nloci). State-of-the-art methods for genomic selection and association mapping\nare based on kernel regression and linear models, respectively. BAKR is the\nfirst method that is competitive in both settings.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 20:40:11 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2016 14:37:03 GMT"}, {"version": "v3", "created": "Tue, 16 May 2017 18:22:14 GMT"}, {"version": "v4", "created": "Sat, 10 Jun 2017 00:57:28 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Crawford", "Lorin", ""], ["Wood", "Kris C.", ""], ["Zhou", "Xiang", ""], ["Mukherjee", "Sayan", ""]]}, {"id": "1508.01235", "submitter": "Arash Pourhabib", "authors": "Arash Pourhabib", "title": "Empirical Similarity for Absent Data Generation in Imbalanced\n  Classification", "comments": null, "journal-ref": "Advances in Information and Communication. FICC 2019. 69 (2020)\n  1010-1030", "doi": "10.1007/978-3-030-12388-8_70", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the training data in a two-class classification problem is overwhelmed\nby one class, most classification techniques fail to correctly identify the\ndata points belonging to the underrepresented class. We propose\nSimilarity-based Imbalanced Classification (SBIC) that learns patterns in the\ntraining data based on an empirical similarity function. To take the imbalanced\nstructure of the training data into account, SBIC utilizes the concept of\nabsent data, i.e. data from the minority class which can help better find the\nboundary between the two classes. SBIC simultaneously optimizes the weights of\nthe empirical similarity function and finds the locations of absent data\npoints. As such, SBIC uses an embedded mechanism for synthetic data generation\nwhich does not modify the training dataset, but alters the algorithm to suit\nimbalanced datasets. Therefore, SBIC uses the ideas of both major schools of\nthoughts in imbalanced classification: Like cost-sensitive approaches SBIC\noperates on an algorithm level to handle imbalanced structures; and similar to\nsynthetic data generation approaches, it utilizes the properties of unobserved\ndata points from the minority class. The application of SBIC to imbalanced\ndatasets suggests it is comparable to, and in some cases outperforms, other\ncommonly used classification techniques for imbalanced datasets.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 21:43:32 GMT"}, {"version": "v2", "created": "Thu, 12 May 2016 18:37:10 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Pourhabib", "Arash", ""]]}, {"id": "1508.01240", "submitter": "Arash Pourhabib", "authors": "Babak Farmanesh, Arash Pourhabib, Balabhaskar Balasundaram, Austin\n  Buchanan", "title": "A Bayesian framework for functional calibration of expensive\n  computational models through non-isometric matching", "comments": "39 pages; added analysis of residuals", "journal-ref": "IISE Transactions, 53 (2021) 352-364", "doi": "10.1080/24725854.2020.1774688", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study statistical calibration, i.e., adjusting features of a computational\nmodel that are not observable or controllable in its associated physical\nsystem. We focus on functional calibration, which arises in many manufacturing\nprocesses where the unobservable features, called calibration variables, are a\nfunction of the input variables. A major challenge in many applications is that\ncomputational models are expensive and can only be evaluated a limited number\nof times. Furthermore, without making strong assumptions, the calibration\nvariables are not identifiable. We propose Bayesian non-isometric matching\ncalibration (BNMC) that allows calibration of expensive computational models\nwith only a limited number of samples taken from a computational model and its\nassociated physical system. BNMC replaces the computational model with a\ndynamic Gaussian process (GP) whose parameters are trained in the calibration\nprocedure. To resolve the identifiability issue, we present the calibration\nproblem from a geometric perspective of non-isometric curve to surface\nmatching, which enables us to take advantage of combinatorial optimization\ntechniques to extract necessary information for constructing prior\ndistributions. Our numerical experiments demonstrate that in terms of\nprediction accuracy BNMC outperforms, or is comparable to, other existing\ncalibration frameworks.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 22:17:56 GMT"}, {"version": "v2", "created": "Sat, 22 Sep 2018 02:02:29 GMT"}, {"version": "v3", "created": "Tue, 9 Apr 2019 04:25:15 GMT"}, {"version": "v4", "created": "Fri, 22 May 2020 21:34:23 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Farmanesh", "Babak", ""], ["Pourhabib", "Arash", ""], ["Balasundaram", "Balabhaskar", ""], ["Buchanan", "Austin", ""]]}, {"id": "1508.01248", "submitter": "Arash Pourhabib", "authors": "Babak Farmanesh and Arash Pourhabib", "title": "Sparse Pseudo-input Local Kriging for Large Spatial Datasets with\n  Exogenous Variables", "comments": "50 pages, 9 figures, one table; added a simulated dataset", "journal-ref": "IISE Transactions, 52:3 (2020), 334-348", "doi": "10.1080/24725854.2019.1624926", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study large-scale spatial systems that contain exogenous variables, e.g.\nenvironmental factors that are significant predictors in spatial processes.\nBuilding predictive models for such processes is challenging because the large\nnumbers of observations present makes it inefficient to apply full Kriging. In\norder to reduce computational complexity, this paper proposes Sparse\nPseudo-input Local Kriging (SPLK), which utilizes hyperplanes to partition a\ndomain into smaller subdomains and then applies a sparse approximation of the\nfull Kriging to each subdomain. We also develop an optimization procedure to\nfind the desired hyperplanes. To alleviate the problem of discontinuity in the\nglobal predictor, we impose continuity constraints on the boundaries of the\nneighboring subdomains. Furthermore, partitioning the domain into smaller\nsubdomains makes it possible to use different parameter values for the\ncovariance function in each region and, therefore, the heterogeneity in the\ndata structure can be effectively captured. Numerical experiments demonstrate\nthat SPLK outperforms, or is comparable to, the algorithms commonly applied to\nspatial datasets.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 23:44:56 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 22:12:43 GMT"}, {"version": "v3", "created": "Tue, 27 Nov 2018 06:32:55 GMT"}, {"version": "v4", "created": "Mon, 20 May 2019 05:29:35 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Farmanesh", "Babak", ""], ["Pourhabib", "Arash", ""]]}, {"id": "1508.01340", "submitter": "Marc Boull\\'e", "authors": "Marc Boull\\'e", "title": "Universal Approximation of Edge Density in Large Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel way to summarize the structure of large\ngraphs, based on non-parametric estimation of edge density in directed\nmultigraphs. Following coclustering approach, we use a clustering of the\nvertices, with a piecewise constant estimation of the density of the edges\nacross the clusters, and address the problem of automatically and reliably\ninferring the number of clusters, which is the granularity of the coclustering.\nWe use a model selection technique with data-dependent prior and obtain an\nexact evaluation criterion for the posterior probability of edge density\nestimation models. We demonstrate, both theoretically and empirically, that our\ndata-dependent modeling technique is consistent, resilient to noise, valid non\nasymptotically and asymptotically behaves as an universal approximator of the\ntrue edge density in directed multigraphs. We evaluate our method using\nartificial graphs and present its practical interest on real world graphs. The\nmethod is both robust and scalable. It is able to extract insightful patterns\nin the unsupervised learning setting and to provide state of the art accuracy\nwhen used as a preparation step for supervised learning.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 09:40:28 GMT"}], "update_date": "2015-08-07", "authors_parsed": [["Boull\u00e9", "Marc", ""]]}, {"id": "1508.01551", "submitter": "Yan Li", "authors": "Yan Li, Kristofer G. Reyes, Jorge Vazquez-Anderson, Yingfei Wang,\n  Lydia M. Contreras, Warren B. Powell", "title": "A Knowledge Gradient Policy for Sequencing Experiments to Identify the\n  Structure of RNA Molecules Using a Sparse Additive Belief Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a sparse knowledge gradient (SpKG) algorithm for adaptively\nselecting the targeted regions within a large RNA molecule to identify which\nregions are most amenable to interactions with other molecules. Experimentally,\nsuch regions can be inferred from fluorescence measurements obtained by binding\na complementary probe with fluorescence markers to the targeted regions. We use\na biophysical model which shows that the fluorescence ratio under the log scale\nhas a sparse linear relationship with the coefficients describing the\naccessibility of each nucleotide, since not all sites are accessible (due to\nthe folding of the molecule). The SpKG algorithm uniquely combines the Bayesian\nranking and selection problem with the frequentist $\\ell_1$ regularized\nregression approach Lasso. We use this algorithm to identify the sparsity\npattern of the linear model as well as sequentially decide the best regions to\ntest before experimental budget is exhausted. Besides, we also develop two\nother new algorithms: batch SpKG algorithm, which generates more suggestions\nsequentially to run parallel experiments; and batch SpKG with a procedure which\nwe call length mutagenesis. It dynamically adds in new alternatives, in the\nform of types of probes, are created by inserting, deleting or mutating\nnucleotides within existing probes. In simulation, we demonstrate these\nalgorithms on the Group I intron (a mid-size RNA molecule), showing that they\nefficiently learn the correct sparsity pattern, identify the most accessible\nregion, and outperform several other policies.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 22:03:34 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Li", "Yan", ""], ["Reyes", "Kristofer G.", ""], ["Vazquez-Anderson", "Jorge", ""], ["Wang", "Yingfei", ""], ["Contreras", "Lydia M.", ""], ["Powell", "Warren B.", ""]]}, {"id": "1508.01596", "submitter": "Pushpendre Rastogi", "authors": "Pushpendre Rastogi and Benjamin Van Durme", "title": "Sublinear Partition Estimation", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The output scores of a neural network classifier are converted to\nprobabilities via normalizing over the scores of all competing categories.\nComputing this partition function, $Z$, is then linear in the number of\ncategories, which is problematic as real-world problem sets continue to grow in\ncategorical types, such as in visual object recognition or discriminative\nlanguage modeling. We propose three approaches for sublinear estimation of the\npartition function, based on approximate nearest neighbor search and kernel\nfeature maps and compare the performance of the proposed approaches\nempirically.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 03:45:21 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Rastogi", "Pushpendre", ""], ["Van Durme", "Benjamin", ""]]}, {"id": "1508.01609", "submitter": "Golan Bel", "authors": "Ehud Strobach and Golan Bel", "title": "The Contribution of Internal and Model Variabilities to the Uncertainty\n  in CMIP5 Decadal Climate Predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decadal climate predictions, which are initialized with observed conditions,\nare characterized by two main sources of uncertainties--internal and model\nvariabilities. Using an ensemble of climate model simulations from the CMIP5\ndecadal experiments, we quantified the total uncertainty associated with these\npredictions and the relative importance of each source. Annual and monthly\naverages of the surface temperature and wind components were considered. We\nshow that different definitions of the anomaly results in different conclusions\nregarding the variance of the ensemble members. However, some features of the\nuncertainty are common to all the measures we considered. We found that over\ndecadal time scales, there is no considerable increase in the uncertainty with\ntime. The model variability is more sensitive to the annual cycle than the\ninternal variability. This, in turn, results in a maximal uncertainty during\nthe winter in the northern hemisphere. The uncertainty of the surface\ntemperature prediction is dominated by the model variability, whereas the\nuncertainty of the wind components is determined by both sources. Analysis of\nthe spatial distribution of the uncertainty reveals that the surface\ntemperature has higher variability over land and in high latitudes, whereas the\nsurface zonal wind has higher variability over the ocean. The relative\nimportance of the internal and model variabilities depends on the averaging\nperiod, the definition of the anomaly, and the location. These findings suggest\nthat several methods should be combined in order to assess future climate\nprediction uncertainties and that weighting schemes of the ensemble members may\nreduce the uncertainties.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 05:36:04 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Strobach", "Ehud", ""], ["Bel", "Golan", ""]]}, {"id": "1508.01713", "submitter": "Luca Scrucca", "authors": "Luca Scrucca", "title": "Dimension reduction for model-based clustering", "comments": null, "journal-ref": "Statistics and Computing 20/4 (2010) 471-484", "doi": "10.1007/s11222-009-9138-7", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a dimension reduction method for visualizing the clustering\nstructure obtained from a finite mixture of Gaussian densities. Information on\nthe dimension reduction subspace is obtained from the variation on group means\nand, depending on the estimated mixture model, on the variation on group\ncovariances. The proposed method aims at reducing the dimensionality by\nidentifying a set of linear combinations, ordered by importance as quantified\nby the associated eigenvalues, of the original features which capture most of\nthe cluster structure contained in the data. Observations may then be projected\nonto such a reduced subspace, thus providing summary plots which help to\nvisualize the clustering structure. These plots can be particularly appealing\nin the case of high-dimensional data and noisy structure. The new constructed\nvariables capture most of the clustering information available in the data, and\nthey can be further reduced to improve clustering performance. We illustrate\nthe approach on both simulated and real data sets.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 14:54:03 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Scrucca", "Luca", ""]]}, {"id": "1508.01717", "submitter": "Christopher Nowzohour", "authors": "Christopher Nowzohour, Marloes H. Maathuis, Robin J. Evans, Peter\n  B\\\"uhlmann", "title": "Distributional Equivalence and Structure Learning for Bow-free Acyclic\n  Path Diagrams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of structure learning for bow-free acyclic path\ndiagrams (BAPs). BAPs can be viewed as a generalization of linear Gaussian DAG\nmodels that allow for certain hidden variables. We present a first method for\nthis problem using a greedy score-based search algorithm. We also prove some\nnecessary and some sufficient conditions for distributional equivalence of BAPs\nwhich are used in an algorithmic ap- proach to compute (nearly) equivalent\nmodel structures. This allows us to infer lower bounds of causal effects. We\nalso present applications to real and simulated datasets using our publicly\navailable R-package.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 15:06:04 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2015 14:18:07 GMT"}, {"version": "v3", "created": "Tue, 24 Jan 2017 13:34:34 GMT"}, {"version": "v4", "created": "Sat, 2 Dec 2017 13:00:51 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Nowzohour", "Christopher", ""], ["Maathuis", "Marloes H.", ""], ["Evans", "Robin J.", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "1508.01720", "submitter": "Jure Sokolic", "authors": "Jure Sokolic, Francesco Renna, Robert Calderbank, Miguel R. D.\n  Rodrigues", "title": "Mismatch in the Classification of Linear Subspaces: Sufficient\n  Conditions for Reliable Classification", "comments": "17 pages, 7 figures, submitted to IEEE Transactions on Signal\n  Processing", "journal-ref": null, "doi": "10.1109/TSP.2016.2537272", "report-no": null, "categories": "cs.IT cs.CV math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the classification of linear subspaces with mismatched\nclassifiers. In particular, we assume a model where one observes signals in the\npresence of isotropic Gaussian noise and the distribution of the signals\nconditioned on a given class is Gaussian with a zero mean and a low-rank\ncovariance matrix. We also assume that the classifier knows only a mismatched\nversion of the parameters of input distribution in lieu of the true parameters.\nBy constructing an asymptotic low-noise expansion of an upper bound to the\nerror probability of such a mismatched classifier, we provide sufficient\nconditions for reliable classification in the low-noise regime that are able to\nsharply predict the absence of a classification error floor. Such conditions\nare a function of the geometry of the true signal distribution, the geometry of\nthe mismatched signal distributions as well as the interplay between such\ngeometries, namely, the principal angles and the overlap between the true and\nthe mismatched signal subspaces. Numerical results demonstrate that our\nconditions for reliable classification can sharply predict the behavior of a\nmismatched classifier both with synthetic data and in a motion segmentation and\na hand-written digit classification applications.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 15:16:39 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2016 18:59:48 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Sokolic", "Jure", ""], ["Renna", "Francesco", ""], ["Calderbank", "Robert", ""], ["Rodrigues", "Miguel R. D.", ""]]}, {"id": "1508.01746", "submitter": "Alan Godoy", "authors": "Alan Godoy, Fl\\'avio Sim\\~oes, Jos\\'e Augusto Stuchi, Marcus de Assis\n  Angeloni, M\\'ario Uliani, Ricardo Violato", "title": "Using Deep Learning for Detecting Spoofing Attacks on Speech Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that speaker verification systems are subject to spoofing\nattacks. The Automatic Speaker Verification Spoofing and Countermeasures\nChallenge -- ASVSpoof2015 -- provides a standard spoofing database, containing\nattacks based on synthetic speech, along with a protocol for experiments. This\npaper describes CPqD's systems submitted to the ASVSpoof2015 Challenge, based\non deep neural networks, working both as a classifier and as a feature\nextraction module for a GMM and a SVM classifier. Results show the validity of\nthis approach, achieving less than 0.5\\% EER for known attacks.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 16:20:52 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2016 16:27:49 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Godoy", "Alan", ""], ["Sim\u00f5es", "Fl\u00e1vio", ""], ["Stuchi", "Jos\u00e9 Augusto", ""], ["Angeloni", "Marcus de Assis", ""], ["Uliani", "M\u00e1rio", ""], ["Violato", "Ricardo", ""]]}, {"id": "1508.01774", "submitter": "Siddharth Sigtia", "authors": "Siddharth Sigtia, Emmanouil Benetos, Simon Dixon", "title": "An End-to-End Neural Network for Polyphonic Piano Music Transcription", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a supervised neural network model for polyphonic piano music\ntranscription. The architecture of the proposed model is analogous to speech\nrecognition systems and comprises an acoustic model and a music language model.\nThe acoustic model is a neural network used for estimating the probabilities of\npitches in a frame of audio. The language model is a recurrent neural network\nthat models the correlations between pitch combinations over time. The proposed\nmodel is general and can be used to transcribe polyphonic music without\nimposing any constraints on the polyphony. The acoustic and language model\npredictions are combined using a probabilistic graphical model. Inference over\nthe output variables is performed using the beam search algorithm. We perform\ntwo sets of experiments. We investigate various neural network architectures\nfor the acoustic models and also investigate the effect of combining acoustic\nand music language model predictions using the proposed architecture. We\ncompare performance of the neural network based acoustic models with two\npopular unsupervised acoustic models. Results show that convolutional neural\nnetwork acoustic models yields the best performance across all evaluation\nmetrics. We also observe improved performance with the application of the music\nlanguage models. Finally, we present an efficient variant of beam search that\nimproves performance and reduces run-times by an order of magnitude, making the\nmodel suitable for real-time applications.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 18:16:32 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2016 12:59:35 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Sigtia", "Siddharth", ""], ["Benetos", "Emmanouil", ""], ["Dixon", "Simon", ""]]}, {"id": "1508.01819", "submitter": "Sharmodeep Bhattacharyya", "authors": "Sharmodeep Bhattacharyya and Peter J. Bickel", "title": "Spectral Clustering and Block Models: A Review And A New Algorithm", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SI stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on spectral clustering of unlabeled graphs and review some results\non clustering methods which achieve weak or strong consistent identification in\ndata generated by such models. We also present a new algorithm which appears to\nperform optimally both theoretically using asymptotic theory and empirically.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 21:11:41 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Bhattacharyya", "Sharmodeep", ""], ["Bickel", "Peter J.", ""]]}, {"id": "1508.01903", "submitter": "Badong Chen", "authors": "Wentao Ma, Badong Chen, Jiandong Duan, Haiquan Zhao", "title": "Diffusion Maximum Correntropy Criterion Algorithms for Robust\n  Distributed Estimation", "comments": "17 pages,10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust diffusion adaptive estimation algorithms based on the maximum\ncorrentropy criterion (MCC), including adaptation to combination MCC and\ncombination to adaptation MCC, are developed to deal with the distributed\nestimation over network in impulsive (long-tailed) noise environments. The cost\nfunctions used in distributed estimation are in general based on the mean\nsquare error (MSE) criterion, which is desirable when the measurement noise is\nGaussian. In non-Gaussian situations, such as the impulsive-noise case, MCC\nbased methods may achieve much better performance than the MSE methods as they\ntake into account higher order statistics of error distribution. The proposed\nmethods can also outperform the robust diffusion least mean p-power(DLMP) and\ndiffusion minimum error entropy (DMEE) algorithms. The mean and mean square\nconvergence analysis of the new algorithms are also carried out.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2015 13:38:41 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2016 12:30:28 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Ma", "Wentao", ""], ["Chen", "Badong", ""], ["Duan", "Jiandong", ""], ["Zhao", "Haiquan", ""]]}, {"id": "1508.01922", "submitter": "Rahul Mazumder", "authors": "Rahul Mazumder and Peter Radchenko", "title": "The Discrete Dantzig Selector: Estimating Sparse Linear Models via Mixed\n  Integer Linear Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel high-dimensional linear regression estimator: the Discrete\nDantzig Selector, which minimizes the number of nonzero regression coefficients\nsubject to a budget on the maximal absolute correlation between the features\nand residuals. Motivated by the significant advances in integer optimization\nover the past 10-15 years, we present a Mixed Integer Linear Optimization\n(MILO) approach to obtain certifiably optimal global solutions to this\nnonconvex optimization problem. The current state of algorithmics in integer\noptimization makes our proposal substantially more computationally attractive\nthan the least squares subset selection framework based on integer quadratic\noptimization, recently proposed in [8] and the continuous nonconvex quadratic\noptimization framework of [33]. We propose new discrete first-order methods,\nwhich when paired with state-of-the-art MILO solvers, lead to good solutions\nfor the Discrete Dantzig Selector problem for a given computational budget. We\nillustrate that our integrated approach provides globally optimal solutions in\nsignificantly shorter computation times, when compared to off-the-shelf MILO\nsolvers. We demonstrate both theoretically and empirically that in a wide range\nof regimes the statistical properties of the Discrete Dantzig Selector are\nsuperior to those of popular $\\ell_{1}$-based approaches. We illustrate that\nour approach can handle problem instances with p = 10,000 features with\ncertifiable optimality making it a highly scalable combinatorial variable\nselection approach in sparse linear modeling.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2015 16:13:07 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 00:52:56 GMT"}, {"version": "v3", "created": "Thu, 19 Jan 2017 05:48:12 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Mazumder", "Rahul", ""], ["Radchenko", "Peter", ""]]}, {"id": "1508.01928", "submitter": "Dejan Slep\\v{c}ev", "authors": "Nicol\\'as Garc\\'ia Trillos and Dejan Slep\\v{c}ev", "title": "A variational approach to the consistency of spectral clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes the consistency of spectral approaches to data\nclustering. We consider clustering of point clouds obtained as samples of a\nground-truth measure. A graph representing the point cloud is obtained by\nassigning weights to edges based on the distance between the points they\nconnect. We investigate the spectral convergence of both unnormalized and\nnormalized graph Laplacians towards the appropriate operators in the continuum\ndomain. We obtain sharp conditions on how the connectivity radius can be scaled\nwith respect to the number of sample points for the spectral convergence to\nhold.\n  We also show that the discrete clusters obtained via spectral clustering\nconverge towards a continuum partition of the ground truth measure. Such\ncontinuum partition minimizes a functional describing the continuum analogue of\nthe graph-based spectral partitioning. Our approach, based on variational\nconvergence, is general and flexible.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2015 17:14:51 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Trillos", "Nicol\u00e1s Garc\u00eda", ""], ["Slep\u010dev", "Dejan", ""]]}, {"id": "1508.01939", "submitter": "Xi Luo", "authors": "Florentina Bunea, Christophe Giraud, Xi Luo, Martin Royer, Nicolas\n  Verzelen", "title": "Model Assisted Variable Clustering: Minimax-optimal Recovery and\n  Algorithms", "comments": "Maintext: 38 pages; supplementary information: 37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based clustering defines population level clusters relative to a model\nthat embeds notions of similarity. Algorithms tailored to such models yield\nestimated clusters with a clear statistical interpretation. We take this view\nhere and introduce the class of G-block covariance models as a background model\nfor variable clustering. In such models, two variables in a cluster are deemed\nsimilar if they have similar associations will all other variables. This can\narise, for instance, when groups of variables are noise corrupted versions of\nthe same latent factor. We quantify the difficulty of clustering data generated\nfrom a G-block covariance model in terms of cluster proximity, measured with\nrespect to two related, but different, cluster separation metrics. We derive\nminimax cluster separation thresholds, which are the metric values below which\nno algorithm can recover the model-defined clusters exactly, and show that they\nare different for the two metrics. We therefore develop two algorithms, COD and\nPECOK, tailored to G-block covariance models, and study their\nminimax-optimality with respect to each metric. Of independent interest is the\nfact that the analysis of the PECOK algorithm, which is based on a corrected\nconvex relaxation of the popular K-means algorithm, provides the first\nstatistical analysis of such algorithms for variable clustering. Additionally,\nwe contrast our methods with another popular clustering method, spectral\nclustering, specialized to variable clustering, and show that ensuring exact\ncluster recovery via this method requires clusters to have a higher separation,\nrelative to the minimax threshold. Extensive simulation studies, as well as our\ndata analyses, confirm the applicability of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2015 18:25:16 GMT"}, {"version": "v2", "created": "Fri, 1 Apr 2016 18:30:48 GMT"}, {"version": "v3", "created": "Fri, 29 Sep 2017 03:13:09 GMT"}, {"version": "v4", "created": "Mon, 16 Apr 2018 03:32:02 GMT"}, {"version": "v5", "created": "Thu, 13 Dec 2018 03:33:17 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Bunea", "Florentina", ""], ["Giraud", "Christophe", ""], ["Luo", "Xi", ""], ["Royer", "Martin", ""], ["Verzelen", "Nicolas", ""]]}, {"id": "1508.01993", "submitter": "Stefan Feuerriegel", "authors": "Stefan Feuerriegel and Ralph Fehrer", "title": "Improving Decision Analytics with Deep Learning: The Case of Financial\n  Disclosures", "comments": null, "journal-ref": "Twenty-Fourth European Conference on Information Systems (ECIS\n  2016), Istanbul, Turkey, 2016", "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision analytics commonly focuses on the text mining of financial news\nsources in order to provide managerial decision support and to predict stock\nmarket movements. Existing predictive frameworks almost exclusively apply\ntraditional machine learning methods, whereas recent research indicates that\ntraditional machine learning methods are not sufficiently capable of extracting\nsuitable features and capturing the non-linear nature of complex tasks. As a\nremedy, novel deep learning models aim to overcome this issue by extending\ntraditional neural network models with additional hidden layers. Indeed, deep\nlearning has been shown to outperform traditional methods in terms of\npredictive performance. In this paper, we adapt the novel deep learning\ntechnique to financial decision support. In this instance, we aim to predict\nthe direction of stock movements following financial disclosures. As a result,\nwe show how deep learning can outperform the accuracy of random forests as a\nbenchmark for machine learning by 5.66%.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2015 07:39:24 GMT"}, {"version": "v2", "created": "Wed, 4 Jul 2018 09:32:57 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Feuerriegel", "Stefan", ""], ["Fehrer", "Ralph", ""]]}, {"id": "1508.02087", "submitter": "Philipp Moritz", "authors": "Philipp Moritz, Robert Nishihara, Michael I. Jordan", "title": "A Linearly-Convergent Stochastic L-BFGS Algorithm", "comments": "10 pages, 3 figures in International Conference on Artificial\n  Intelligence and Statistics, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new stochastic L-BFGS algorithm and prove a linear convergence\nrate for strongly convex and smooth functions. Our algorithm draws heavily from\na recent stochastic variant of L-BFGS proposed in Byrd et al. (2014) as well as\na recent approach to variance reduction for stochastic gradient descent from\nJohnson and Zhang (2013). We demonstrate experimentally that our algorithm\nperforms well on large-scale convex and non-convex optimization problems,\nexhibiting linear convergence and rapidly solving the optimization problems to\nhigh levels of precision. Furthermore, we show that our algorithm performs well\nfor a wide-range of step sizes, often differing by several orders of magnitude.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2015 21:40:33 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2016 23:36:06 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Moritz", "Philipp", ""], ["Nishihara", "Robert", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1508.02171", "submitter": "L\\'aszl\\'o Gyarmati", "authors": "Laszlo Gyarmati and Xavier Anguera", "title": "Automatic Extraction of the Passing Strategies of Soccer Teams", "comments": "2015 KDD Workshop on Large-Scale Sports Analytics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technology offers new ways to measure the locations of the players and of the\nball in sports. This translates to the trajectories the ball takes on the field\nas a result of the tactics the team applies. The challenge professionals in\nsoccer are facing is to take the reverse path: given the trajectories of the\nball is it possible to infer the underlying strategy/tactic of a team? We\npropose a method based on Dynamic Time Warping to reveal the tactics of a team\nthrough the analysis of repeating series of events. Based on the analysis of an\nentire season, we derive insights such as passing strategies for maintaining\nball possession or counter attacks, and passing styles with a focus on the team\nor on the capabilities of the individual players.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 09:00:33 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Gyarmati", "Laszlo", ""], ["Anguera", "Xavier", ""]]}, {"id": "1508.02186", "submitter": "Luca Scrucca", "authors": "Luca Scrucca", "title": "Model-based SIR for dimension reduction", "comments": null, "journal-ref": "Computational Statistics and Data Analysis 55/11 (2011) pp.\n  3010-3026", "doi": "10.1016/j.csda.2011.05.006", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new dimension reduction method based on Gaussian finite mixtures is\nproposed as an extension to sliced inverse regression (SIR). The model-based\nSIR (MSIR) approach allows the main limitation of SIR to be overcome, i.e.,\nfailure in the presence of regression symmetric relationships, without the need\nto impose further assumptions. Extensive numerical studies are presented to\ncompare the new method with some of most popular dimension reduction methods,\nsuch as SIR, sliced average variance estimation, principal Hessian direction,\nand directional regression. MSIR appears sufficiently flexible to accommodate\nvarious regression functions, and its performance is comparable with or better,\nparticularly as sample size grows, than other available methods. Lastly, MSIR\nis illustrated with two real data examples about ozone concentration\nregression, and hand-written digit classification.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 09:31:01 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Scrucca", "Luca", ""]]}, {"id": "1508.02324", "submitter": "Shuchin Aeron", "authors": "Xiao-Yang Liu, Shuchin Aeron, Vaneet Aggarwal, Xiaodong Wang, Min-You\n  Wu", "title": "Adaptive Sampling of RF Fingerprints for Fine-grained Indoor\n  Localization", "comments": "To appear in IEEE Transactions on Mobile Computing", "journal-ref": "IEEE Transactions on Mobile Computing, vol. 15, no. 10, pp.\n  2411-2423 (2016)", "doi": "10.1109/TMC.2015.2505729", "report-no": null, "categories": "cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indoor localization is a supporting technology for a broadening range of\npervasive wireless applications. One promis- ing approach is to locate users\nwith radio frequency fingerprints. However, its wide adoption in real-world\nsystems is challenged by the time- and manpower-consuming site survey process,\nwhich builds a fingerprint database a priori for localization. To address this\nproblem, we visualize the 3-D RF fingerprint data as a function of locations\n(x-y) and indices of access points (fingerprint), as a tensor and use tensor\nalgebraic methods for an adaptive tubal-sampling of this fingerprint space. In\nparticular using a recently proposed tensor algebraic framework in [1] we\ncapture the complexity of the fingerprint space as a low-dimensional\ntensor-column space. In this formulation the proposed scheme exploits\nadaptivity to identify reference points which are highly informative for\nlearning this low-dimensional space. Further, under certain incoherency\nconditions we prove that the proposed scheme achieves bounded recovery error\nand near-optimal sampling complexity. In contrast to several existing work that\nrely on random sampling, this paper shows that adaptivity in sampling can lead\nto significant improvements in localization accuracy. The approach is validated\non both data generated by the ray-tracing indoor model which accounts for the\nfloor plan and the impact of walls and the real world data. Simulation results\nshow that, while maintaining the same localization accuracy of existing\napproaches, the amount of samples can be cut down by 71% for the high SNR case\nand 55% for the low SNR case.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 16:57:54 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2015 22:43:52 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Liu", "Xiao-Yang", ""], ["Aeron", "Shuchin", ""], ["Aggarwal", "Vaneet", ""], ["Wang", "Xiaodong", ""], ["Wu", "Min-You", ""]]}, {"id": "1508.02344", "submitter": "Jiaming Xu", "authors": "Elchanan Mossel and Jiaming Xu", "title": "Local Algorithms for Block Models with Side Information", "comments": "Due to the limitation \"The abstract field cannot be longer than 1,920\n  characters\", the abstract here is shorter than that in the PDF file", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CC cs.DC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a recent interest in understanding the power of local\nalgorithms for optimization and inference problems on sparse graphs. Gamarnik\nand Sudan (2014) showed that local algorithms are weaker than global algorithms\nfor finding large independent sets in sparse random regular graphs. Montanari\n(2015) showed that local algorithms are suboptimal for finding a community with\nhigh connectivity in the sparse Erd\\H{o}s-R\\'enyi random graphs. For the\nsymmetric planted partition problem (also named community detection for the\nblock models) on sparse graphs, a simple observation is that local algorithms\ncannot have non-trivial performance.\n  In this work we consider the effect of side information on local algorithms\nfor community detection under the binary symmetric stochastic block model. In\nthe block model with side information each of the $n$ vertices is labeled $+$\nor $-$ independently and uniformly at random; each pair of vertices is\nconnected independently with probability $a/n$ if both of them have the same\nlabel or $b/n$ otherwise. The goal is to estimate the underlying vertex\nlabeling given 1) the graph structure and 2) side information in the form of a\nvertex labeling positively correlated with the true one. Assuming that the\nratio between in and out degree $a/b$ is $\\Theta(1)$ and the average degree $\n(a+b) / 2 = n^{o(1)}$, we characterize three different regimes under which a\nlocal algorithm, namely, belief propagation run on the local neighborhoods,\nmaximizes the expected fraction of vertices labeled correctly. Thus, in\ncontrast to the case of symmetric block models without side information, we\nshow that local algorithms can achieve optimal performance for the block model\nwith side information.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 18:23:27 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Mossel", "Elchanan", ""], ["Xu", "Jiaming", ""]]}, {"id": "1508.02473", "submitter": "Jie Ding", "authors": "Jie Ding, Vahid Tarokh, Yuhong Yang", "title": "Bridging AIC and BIC: a new criterion for autoregression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-fin.EC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new criterion to determine the order of an autoregressive\nmodel fitted to time series data. It has the benefits of the two well-known\nmodel selection techniques, the Akaike information criterion and the Bayesian\ninformation criterion. When the data is generated from a finite order\nautoregression, the Bayesian information criterion is known to be consistent,\nand so is the new criterion. When the true order is infinity or suitably high\nwith respect to the sample size, the Akaike information criterion is known to\nbe efficient in the sense that its prediction performance is asymptotically\nequivalent to the best offered by the candidate models; in this case, the new\ncriterion behaves in a similar manner. Different from the two classical\ncriteria, the proposed criterion adaptively achieves either consistency or\nefficiency depending on the underlying true model. In practice where the\nobserved time series is given without any prior information about the model\nspecification, the proposed order selection criterion is more flexible and\nrobust compared with classical approaches. Numerical results are presented\ndemonstrating the adaptivity of the proposed technique when applied to various\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 02:49:45 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2015 03:19:05 GMT"}, {"version": "v3", "created": "Sun, 14 Aug 2016 14:50:08 GMT"}, {"version": "v4", "created": "Wed, 24 Aug 2016 16:10:33 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Ding", "Jie", ""], ["Tarokh", "Vahid", ""], ["Yang", "Yuhong", ""]]}, {"id": "1508.02757", "submitter": "Andrea Montanari", "authors": "Adel Javanmard and Andrea Montanari", "title": "De-biasing the Lasso: Optimal Sample Size for Gaussian Designs", "comments": "57 pages, 4 pdf figures (v3 contains stronger statement for unknown\n  covariance, and several new applications)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing statistical inference in high-dimension is an outstanding\nchallenge. A major source of difficulty is the absence of precise information\non the distribution of high-dimensional estimators. Here, we consider linear\nregression in the high-dimensional regime $p\\gg n$. In this context, we would\nlike to perform inference on a high-dimensional parameters vector\n$\\theta^*\\in{\\mathbb R}^p$. Important progress has been achieved in computing\nconfidence intervals for single coordinates $\\theta^*_i$. A key role in these\nnew methods is played by a certain debiased estimator $\\hat{\\theta}^{\\rm d}$\nthat is constructed from the Lasso. Earlier work establishes that, under\nsuitable assumptions on the design matrix, the coordinates of\n$\\hat{\\theta}^{\\rm d}$ are asymptotically Gaussian provided $\\theta^*$ is\n$s_0$-sparse with $s_0 = o(\\sqrt{n}/\\log p )$. The condition $s_0 = o(\\sqrt{n}/\n\\log p )$ is stronger than the one for consistent estimation, namely $s_0 =\no(n/ \\log p)$. We study Gaussian designs with known or unknown population\ncovariance. When the covariance is known, we prove that the debiased estimator\nis asymptotically Gaussian under the nearly optimal condition $s_0 = o(n/ (\\log\np)^2)$. Note that earlier work was limited to $s_0 = o(\\sqrt{n}/\\log p)$ even\nfor perfectly known covariance. The same conclusion holds if the population\ncovariance is unknown but can be estimated sufficiently well, e.g. under the\nsame sparsity conditions on the inverse covariance as assumed by earlier work.\nFor intermediate regimes, we describe the trade-off between sparsity in the\ncoefficients and in the inverse covariance of the design. We further discuss\nseveral applications of our results to high-dimensional inference. In\nparticular, we propose a new estimator that is minimax optimal up to a factor\n$1+o_n(1)$ for i.i.d. Gaussian designs.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 21:38:13 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2015 00:24:06 GMT"}, {"version": "v3", "created": "Mon, 13 Jun 2016 20:07:22 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Javanmard", "Adel", ""], ["Montanari", "Andrea", ""]]}, {"id": "1508.02765", "submitter": "Mostafa El Gamal", "authors": "Mostafa El Gamal and Lifeng Lai", "title": "Are Slepian-Wolf Rates Necessary for Distributed Parameter Estimation?", "comments": "Accepted in Allerton 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a distributed parameter estimation problem, in which multiple\nterminals send messages related to their local observations using limited rates\nto a fusion center who will obtain an estimate of a parameter related to\nobservations of all terminals. It is well known that if the transmission rates\nare in the Slepian-Wolf region, the fusion center can fully recover all\nobservations and hence can construct an estimator having the same performance\nas that of the centralized case. One natural question is whether Slepian-Wolf\nrates are necessary to achieve the same estimation performance as that of the\ncentralized case. In this paper, we show that the answer to this question is\nnegative. We establish our result by explicitly constructing an asymptotically\nminimum variance unbiased estimator (MVUE) that has the same performance as\nthat of the optimal estimator in the centralized case while requiring\ninformation rates less than the conditions required in the Slepian-Wolf rate\nregion.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 22:29:23 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2015 20:12:35 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Gamal", "Mostafa El", ""], ["Lai", "Lifeng", ""]]}, {"id": "1508.02809", "submitter": "Kelum Gajamannage", "authors": "Kelum Gajamannage, Sachit Butail, Maurizio Porfiri, Erik M. Bollt", "title": "Identifying manifolds underlying group motion in Vicsek agents", "comments": "12 pages, 6 figures, journal article", "journal-ref": null, "doi": "10.1140/epjst/e2015-50088-2", "report-no": null, "categories": "math.DS cs.MA math-ph math.GT math.MP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collective motion of animal groups often undergoes changes due to\nperturbations. In a topological sense, we describe these changes as switching\nbetween low-dimensional embedding manifolds underlying a group of evolving\nagents. To characterize such manifolds, first we introduce a simple mapping of\nagents between time-steps. Then, we construct a novel metric which is\nsusceptible to variations in the collective motion, thus revealing distinct\nunderlying manifolds. The method is validated through three sample scenarios\nsimulated using a Vicsek model, namely switching of speed, coordination, and\nstructure of a group. Combined with a dimensionality reduction technique that\nis used to infer the dimensionality of the embedding manifold, this approach\nprovides an effective model-free framework for the analysis of collective\nbehavior across animal species.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 04:51:59 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Gajamannage", "Kelum", ""], ["Butail", "Sachit", ""], ["Porfiri", "Maurizio", ""], ["Bollt", "Erik M.", ""]]}, {"id": "1508.02810", "submitter": "Murat A. Erdogdu", "authors": "Murat A. Erdogdu and Andrea Montanari", "title": "Convergence rates of sub-sampled Newton methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of minimizing a sum of $n$ functions over a convex\nparameter set $\\mathcal{C} \\subset \\mathbb{R}^p$ where $n\\gg p\\gg 1$. In this\nregime, algorithms which utilize sub-sampling techniques are known to be\neffective. In this paper, we use sub-sampling techniques together with low-rank\napproximation to design a new randomized batch algorithm which possesses\ncomparable convergence rate to Newton's method, yet has much smaller\nper-iteration cost. The proposed algorithm is robust in terms of starting point\nand step size, and enjoys a composite convergence rate, namely, quadratic\nconvergence at start and linear convergence when the iterate is close to the\nminimizer. We develop its theoretical analysis which also allows us to select\nnear-optimal algorithm parameters. Our theoretical results can be used to\nobtain convergence rates of previously proposed sub-sampling based algorithms\nas well. We demonstrate how our results apply to well-known machine learning\nproblems. Lastly, we evaluate the performance of our algorithm on several\ndatasets under various scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 04:52:58 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2015 02:16:02 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Erdogdu", "Murat A.", ""], ["Montanari", "Andrea", ""]]}, {"id": "1508.02865", "submitter": "Giulia Prando", "authors": "Giulia Prando, Gianluigi Pillonetto, Alessandro Chiuso", "title": "Maximum Entropy Vector Kernels for MIMO system identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent contributions have framed linear system identification as a\nnonparametric regularized inverse problem. Relying on $\\ell_2$-type\nregularization which accounts for the stability and smoothness of the impulse\nresponse to be estimated, these approaches have been shown to be competitive\nw.r.t classical parametric methods. In this paper, adopting Maximum Entropy\narguments, we derive a new $\\ell_2$ penalty deriving from a vector-valued\nkernel; to do so we exploit the structure of the Hankel matrix, thus\ncontrolling at the same time complexity, measured by the McMillan degree,\nstability and smoothness of the identified models. As a special case we recover\nthe nuclear norm penalty on the squared block Hankel matrix. In contrast with\nprevious literature on reweighted nuclear norm penalties, our kernel is\ndescribed by a small number of hyper-parameters, which are iteratively updated\nthrough marginal likelihood maximization; constraining the structure of the\nkernel acts as a (hyper)regularizer which helps controlling the effective\ndegrees of freedom of our estimator. To optimize the marginal likelihood we\nadapt a Scaled Gradient Projection (SGP) algorithm which is proved to be\nsignificantly computationally cheaper than other first and second order\noff-the-shelf optimization methods. The paper also contains an extensive\ncomparison with many state-of-the-art methods on several Monte-Carlo studies,\nwhich confirms the effectiveness of our procedure.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 09:59:09 GMT"}, {"version": "v2", "created": "Thu, 29 Sep 2016 18:29:09 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Prando", "Giulia", ""], ["Pillonetto", "Gianluigi", ""], ["Chiuso", "Alessandro", ""]]}, {"id": "1508.02884", "submitter": "Ernesto Diaz-Aviles", "authors": "Ernesto Diaz-Aviles (1), Fabio Pinelli (1), Karol Lynch (1), Zubair\n  Nabi (1), Yiannis Gkoufas (1), Eric Bouillet (1), Francesco Calabrese (1),\n  Eoin Coughlan (2), Peter Holland (2), Jason Salzwedel (2) ((1) IBM Research\n  -- Ireland, (2) IBM Now Factory -- Ireland, (3) Vodacom -- South Africa)", "title": "Towards Real-time Customer Experience Prediction for Telecommunication\n  Operators", "comments": "IEEE 2015 BigData Conference (to appear). Keywords: Telecom\n  operators; Customer Care; Big Data; Predictive Analytics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Telecommunications operators (telcos) traditional sources of income, voice\nand SMS, are shrinking due to customers using over-the-top (OTT) applications\nsuch as WhatsApp or Viber. In this challenging environment it is critical for\ntelcos to maintain or grow their market share, by providing users with as good\nan experience as possible on their network.\n  But the task of extracting customer insights from the vast amounts of data\ncollected by telcos is growing in complexity and scale everey day. How can we\nmeasure and predict the quality of a user's experience on a telco network in\nreal-time? That is the problem that we address in this paper.\n  We present an approach to capture, in (near) real-time, the mobile customer\nexperience in order to assess which conditions lead the user to place a call to\na telco's customer care center. To this end, we follow a supervised learning\napproach for prediction and train our 'Restricted Random Forest' model using,\nas a proxy for bad experience, the observed customer transactions in the telco\ndata feed before the user places a call to a customer care center.\n  We evaluate our approach using a rich dataset provided by a major African\ntelecommunication's company and a novel big data architecture for both the\ntraining and scoring of predictive models. Our empirical study shows our\nsolution to be effective at predicting user experience by inferring if a\ncustomer will place a call based on his current context.\n  These promising results open new possibilities for improved customer service,\nwhich will help telcos to reduce churn rates and improve customer experience,\nboth factors that directly impact their revenue growth.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 11:43:11 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2015 15:26:48 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Diaz-Aviles", "Ernesto", ""], ["Pinelli", "Fabio", ""], ["Lynch", "Karol", ""], ["Nabi", "Zubair", ""], ["Gkoufas", "Yiannis", ""], ["Bouillet", "Eric", ""], ["Calabrese", "Francesco", ""], ["Coughlan", "Eoin", ""], ["Holland", "Peter", ""], ["Salzwedel", "Jason", ""]]}, {"id": "1508.02905", "submitter": "Tue Herlau Mr", "authors": "Tue Herlau, Morten M{\\o}rup, Mikkel N. Schmidt", "title": "Bayesian Dropout", "comments": "21 pages, 3 figures. Manuscript prepared 2014 and awaiting submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout has recently emerged as a powerful and simple method for training\nneural networks preventing co-adaptation by stochastically omitting neurons.\nDropout is currently not grounded in explicit modelling assumptions which so\nfar has precluded its adoption in Bayesian modelling. Using Bayesian entropic\nreasoning we show that dropout can be interpreted as optimal inference under\nconstraints. We demonstrate this on an analytically tractable regression model\nproviding a Bayesian interpretation of its mechanism for regularizing and\npreventing co-adaptation as well as its connection to other Bayesian\ntechniques. We also discuss two general approximate techniques for applying\nBayesian dropout for general models, one based on an analytical approximation\nand the other on stochastic variational techniques. These techniques are then\napplied to a Baysian logistic regression problem and are shown to improve\nperformance as the model become more misspecified. Our framework roots dropout\nas a theoretically justified and practical tool for statistical modelling\nallowing Bayesians to tap into the benefits of dropout training.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 13:09:19 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Herlau", "Tue", ""], ["M\u00f8rup", "Morten", ""], ["Schmidt", "Mikkel N.", ""]]}, {"id": "1508.02925", "submitter": "Hao Han", "authors": "Hao Han, Wei Zhu", "title": "RCR: Robust Compound Regression for Robust Estimation of\n  Errors-in-Variables Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The errors-in-variables (EIV) regression model, being more realistic by\naccounting for measurement errors in both the dependent and the independent\nvariables, is widely adopted in applied sciences. The traditional EIV model\nestimators, however, can be highly biased by outliers and other departures from\nthe underlying assumptions. In this paper, we develop a novel nonparametric\nregression approach - the robust compound regression (RCR) analysis method for\nthe robust estimation of EIV models. We first introduce a robust and efficient\nestimator called least sine squares (LSS). Taking full advantage of both the\nnew LSS method and the compound regression analysis method developed in our own\ngroup, we subsequently propose the RCR approach as a generalization of those\ntwo, which provides a robust counterpart of the entire class of the maximum\nlikelihood estimation (MLE) solutions of the EIV model, in a 1-1 mapping.\nTechnically, our approach gives users the flexibility to select from a class of\nRCR estimates the optimal one with a predefined regression efficiency criterion\nsatisfied. Simulation studies and real-life examples are provided to illustrate\nthe effectiveness of the RCR approach.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 14:19:24 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Han", "Hao", ""], ["Zhu", "Wei", ""]]}, {"id": "1508.02933", "submitter": "Robert Nishihara", "authors": "Robert Nishihara, David Lopez-Paz, L\\'eon Bottou", "title": "No Regret Bound for Extreme Bandits", "comments": "11 pages, International Conference on Artificial Intelligence and\n  Statistics, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms for hyperparameter optimization abound, all of which work well\nunder different and often unverifiable assumptions. Motivated by the general\nchallenge of sequentially choosing which algorithm to use, we study the more\nspecific task of choosing among distributions to use for random hyperparameter\noptimization. This work is naturally framed in the extreme bandit setting,\nwhich deals with sequentially choosing which distribution from a collection to\nsample in order to minimize (maximize) the single best cost (reward). Whereas\nthe distributions in the standard bandit setting are primarily characterized by\ntheir means, a number of subtleties arise when we care about the minimal cost\nas opposed to the average cost. For example, there may not be a well-defined\n\"best\" distribution as there is in the standard bandit setting. The best\ndistribution depends on the rewards that have been obtained and on the\nremaining time horizon. Whereas in the standard bandit setting, it is sensible\nto compare policies with an oracle which plays the single best arm, in the\nextreme bandit setting, there are multiple sensible oracle models. We define a\nsensible notion of \"extreme regret\" in the extreme bandit setting, which\nparallels the concept of regret in the standard bandit setting. We then prove\nthat no policy can asymptotically achieve no extreme regret.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 14:31:49 GMT"}, {"version": "v2", "created": "Sun, 23 Aug 2015 21:45:51 GMT"}, {"version": "v3", "created": "Mon, 11 Apr 2016 18:20:50 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Nishihara", "Robert", ""], ["Lopez-Paz", "David", ""], ["Bottou", "L\u00e9on", ""]]}, {"id": "1508.03106", "submitter": "Anqi Zhao", "authors": "Anqi Zhao, Yang Feng, Lie Wang, Xin Tong", "title": "Neyman-Pearson Classification under High-Dimensional Settings", "comments": "33 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing binary classification methods target on the optimization of the\noverall classification risk and may fail to serve some real-world applications\nsuch as cancer diagnosis, where users are more concerned with the risk of\nmisclassifying one specific class than the other. Neyman-Pearson (NP) paradigm\nwas introduced in this context as a novel statistical framework for handling\nasymmetric type I/II error priorities. It seeks classifiers with a minimal type\nII error and a constrained type I error under a user specified level. This\narticle is the first attempt to construct classifiers with guaranteed\ntheoretical performance under the NP paradigm in high-dimensional settings.\nBased on the fundamental Neyman-Pearson Lemma, we used a plug-in approach to\nconstruct NP-type classifiers for Naive Bayes models. The proposed classifiers\nsatisfy the NP oracle inequalities, which are natural NP paradigm counterparts\nof the oracle inequalities in classical binary classification. Besides their\ndesirable theoretical properties, we also demonstrated their numerical\nadvantages in prioritized error control via both simulation and real data\nstudies.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 02:47:53 GMT"}, {"version": "v2", "created": "Sat, 15 Aug 2015 02:21:54 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Zhao", "Anqi", ""], ["Feng", "Yang", ""], ["Wang", "Lie", ""], ["Tong", "Xin", ""]]}, {"id": "1508.03332", "submitter": "Kelum Gajamannage", "authors": "Kelum Gajamannage, Sachit Butail, Maurizio Porfiri, Erik M. Bollt", "title": "Dimensionality Reduction of Collective Motion by Principal Manifolds", "comments": "19 pages, 13 figures, journal article", "journal-ref": "Physica-D : Nonlinear Phenomena, Volume 291, 15 January 2015,\n  Pages 62-73", "doi": "10.1016/j.physd.2014.09.009", "report-no": null, "categories": "math.NA cs.LG cs.MA math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the existence of low-dimensional embedding manifolds has been shown in\npatterns of collective motion, the current battery of nonlinear dimensionality\nreduction methods are not amenable to the analysis of such manifolds. This is\nmainly due to the necessary spectral decomposition step, which limits control\nover the mapping from the original high-dimensional space to the embedding\nspace. Here, we propose an alternative approach that demands a two-dimensional\nembedding which topologically summarizes the high-dimensional data. In this\nsense, our approach is closely related to the construction of one-dimensional\nprincipal curves that minimize orthogonal error to data points subject to\nsmoothness constraints. Specifically, we construct a two-dimensional principal\nmanifold directly in the high-dimensional space using cubic smoothing splines,\nand define the embedding coordinates in terms of geodesic distances. Thus, the\nmapping from the high-dimensional data to the manifold is defined in terms of\nlocal coordinates. Through representative examples, we show that compared to\nexisting nonlinear dimensionality reduction methods, the principal manifold\nretains the original structure even in noisy and sparse datasets. The principal\nmanifold finding algorithm is applied to configurations obtained from a\ndynamical system of multiple agents simulating a complex maneuver called\npredator mobbing, and the resulting two-dimensional embedding is compared with\nthat of a well-established nonlinear dimensionality reduction method.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 21:07:51 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Gajamannage", "Kelum", ""], ["Butail", "Sachit", ""], ["Porfiri", "Maurizio", ""], ["Bollt", "Erik M.", ""]]}, {"id": "1508.03337", "submitter": "Kimon Fountoulakis", "authors": "Kimon Fountoulakis, Abhisek Kundu, Eugenia-Maria Kontopoulou and\n  Petros Drineas", "title": "A Randomized Rounding Algorithm for Sparse PCA", "comments": "28 pages, 11 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and analyze a simple, two-step algorithm to approximate the\noptimal solution of the sparse PCA problem. Our approach first solves a L1\npenalized version of the NP-hard sparse PCA optimization problem and then uses\na randomized rounding strategy to sparsify the resulting dense solution. Our\nmain theoretical result guarantees an additive error approximation and provides\na tradeoff between sparsity and accuracy. Our experimental evaluation indicates\nthat our approach is competitive in practice, even compared to state-of-the-art\ntoolboxes such as Spasm.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 20:06:59 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2016 08:38:17 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2016 01:34:47 GMT"}, {"version": "v4", "created": "Sat, 26 Mar 2016 21:09:19 GMT"}, {"version": "v5", "created": "Tue, 22 Nov 2016 20:05:10 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Fountoulakis", "Kimon", ""], ["Kundu", "Abhisek", ""], ["Kontopoulou", "Eugenia-Maria", ""], ["Drineas", "Petros", ""]]}, {"id": "1508.03390", "submitter": "Adams Wei Yu", "authors": "Adams Wei Yu, Qihang Lin, Tianbao Yang", "title": "Doubly Stochastic Primal-Dual Coordinate Method for Bilinear\n  Saddle-Point Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a doubly stochastic primal-dual coordinate optimization algorithm\nfor empirical risk minimization, which can be formulated as a bilinear\nsaddle-point problem. In each iteration, our method randomly samples a block of\ncoordinates of the primal and dual solutions to update. The linear convergence\nof our method could be established in terms of 1) the distance from the current\niterate to the optimal solution and 2) the primal-dual objective gap. We show\nthat the proposed method has a lower overall complexity than existing\ncoordinate methods when either the data matrix has a factorized structure or\nthe proximal mapping on each block is computationally expensive, e.g.,\ninvolving an eigenvalue decomposition. The efficiency of the proposed method is\nconfirmed by empirical studies on several real applications, such as the\nmulti-task large margin nearest neighbor problem.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2015 00:22:45 GMT"}, {"version": "v2", "created": "Mon, 11 Apr 2016 16:24:20 GMT"}, {"version": "v3", "created": "Wed, 12 Apr 2017 05:24:19 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Yu", "Adams Wei", ""], ["Lin", "Qihang", ""], ["Yang", "Tianbao", ""]]}, {"id": "1508.03395", "submitter": "Vaneet Aggarwal", "authors": "Vaneet Aggarwal and Shuchin Aeron", "title": "Information-theoretic Bounds on Matrix Completion under Union of\n  Subspaces Model", "comments": null, "journal-ref": "54th Annual Allerton Conference on Communication, Control, and\n  Computing (Allerton), Monticello, IL, pp. 281-283 (2016)", "doi": "10.1109/ALLERTON.2016.7852241", "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short note we extend some of the recent results on matrix completion\nunder the assumption that the columns of the matrix can be grouped (clustered)\ninto subspaces (not necessarily disjoint or independent). This model deviates\nfrom the typical assumption prevalent in the literature dealing with\ncompression and recovery for big-data applications. The results have a direct\nbearing on the problem of subspace clustering under missing or incomplete\ninformation.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2015 01:09:00 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Aggarwal", "Vaneet", ""], ["Aeron", "Shuchin", ""]]}, {"id": "1508.03411", "submitter": "Assaf Hallak", "authors": "Assaf Hallak, Aviv Tamar and Shie Mannor", "title": "Emphatic TD Bellman Operator is a Contraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, \\citet{SuttonMW15} introduced the emphatic temporal differences\n(ETD) algorithm for off-policy evaluation in Markov decision processes. In this\nshort note, we show that the projected fixed-point equation that underlies ETD\ninvolves a contraction operator, with a $\\sqrt{\\gamma}$-contraction modulus\n(where $\\gamma$ is the discount factor). This allows us to provide error bounds\non the approximation error of ETD. To our knowledge, these are the first error\nbounds for an off-policy evaluation algorithm under general target and behavior\npolicies.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2015 03:34:10 GMT"}, {"version": "v2", "created": "Sun, 23 Aug 2015 13:07:35 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Hallak", "Assaf", ""], ["Tamar", "Aviv", ""], ["Mannor", "Shie", ""]]}, {"id": "1508.03666", "submitter": "Bobak Shahriari", "authors": "Bobak Shahriari and Alexandre Bouchard-C\\^ot\\'e and Nando de Freitas", "title": "Unbounded Bayesian Optimization via Regularization", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization has recently emerged as a popular and efficient tool\nfor global optimization and hyperparameter tuning. Currently, the established\nBayesian optimization practice requires a user-defined bounding box which is\nassumed to contain the optimizer. However, when little is known about the\nprobed objective function, it can be difficult to prescribe such bounds. In\nthis work we modify the standard Bayesian optimization framework in a\nprincipled way to allow automatic resizing of the search space. We introduce\ntwo alternative methods and compare them on two common synthetic benchmarking\ntest functions as well as the tasks of tuning the stochastic gradient descent\noptimizer of a multi-layered perceptron and a convolutional neural network on\nMNIST.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2015 21:10:46 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Shahriari", "Bobak", ""], ["Bouchard-C\u00f4t\u00e9", "Alexandre", ""], ["de Freitas", "Nando", ""]]}, {"id": "1508.03712", "submitter": "Ingo Steinwart", "authors": "Philipp Thomann, Ingo Steinwart, Nico Schmid", "title": "Towards an Axiomatic Approach to Hierarchical Clustering of Measures", "comments": null, "journal-ref": "Journal of Machine Learning Research. 16(Sep):1949-2002, 2015", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose some axioms for hierarchical clustering of probability measures\nand investigate their ramifications. The basic idea is to let the user\nstipulate the clusters for some elementary measures. This is done without the\nneed of any notion of metric, similarity or dissimilarity. Our main results\nthen show that for each suitable choice of user-defined clustering on\nelementary measures we obtain a unique notion of clustering on a large set of\ndistributions satisfying a set of additivity and continuity axioms. We\nillustrate the developed theory by numerous examples including some with and\nsome without a density.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2015 09:07:01 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Thomann", "Philipp", ""], ["Steinwart", "Ingo", ""], ["Schmid", "Nico", ""]]}, {"id": "1508.03826", "submitter": "Shaohua Li", "authors": "Shaohua Li, Jun Zhu, Chunyan Miao", "title": "A Generative Word Embedding Model and its Low Rank Positive Semidefinite\n  Solution", "comments": "Proceedings of the Conference on Empirical Methods in Natural\n  Language Processing (EMNLP) 2015 2015, 11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing word embedding methods can be categorized into Neural Embedding\nModels and Matrix Factorization (MF)-based methods. However some models are\nopaque to probabilistic interpretation, and MF-based methods, typically solved\nusing Singular Value Decomposition (SVD), may incur loss of corpus information.\nIn addition, it is desirable to incorporate global latent factors, such as\ntopics, sentiments or writing styles, into the word embedding model. Since\ngenerative models provide a principled way to incorporate latent factors, we\npropose a generative word embedding model, which is easy to interpret, and can\nserve as a basis of more sophisticated latent factor models. The model\ninference reduces to a low rank weighted positive semidefinite approximation\nproblem. Its optimization is approached by eigendecomposition on a submatrix,\nfollowed by online blockwise regression, which is scalable and avoids the\ninformation loss in SVD. In experiments on 7 common benchmark datasets, our\nvectors are competitive to word2vec, and better than other MF-based methods.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2015 14:12:17 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Li", "Shaohua", ""], ["Zhu", "Jun", ""], ["Miao", "Chunyan", ""]]}, {"id": "1508.04035", "submitter": "Emmanuel Osegi", "authors": "Emmanuel N. Osegi", "title": "A Generative Model for Multi-Dialect Representation", "comments": "19 pages, 3 figures, 2 tables, Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the era of deep learning several unsupervised models have been developed\nto capture the key features in unlabeled handwritten data. Popular among them\nis the Restricted Boltzmann Machines RBM. However, due to the novelty in\nhandwritten multidialect data, the RBM may fail to generate an efficient\nrepresentation. In this paper we propose a generative model, the Mode\nSynthesizing Machine MSM for on-line representation of real life handwritten\nmultidialect language data. The MSM takes advantage of the hierarchical\nrepresentation of the modes of a data distribution using a two-point error\nupdate to learn a sequence of representative multidialects in a generative way.\nExperiments were performed to evaluate the performance of the MSM over the RBM\nwith the former attaining much lower error values than the latter on both\nindependent and mixed data set.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 14:05:44 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Osegi", "Emmanuel N.", ""]]}, {"id": "1508.04065", "submitter": "Ali Mousavi", "authors": "Ali Mousavi, Ankit B. Patel, Richard G. Baraniuk", "title": "A Deep Learning Approach to Structured Signal Recovery", "comments": null, "journal-ref": "In Proceeding of 2015 53rd Annual Allerton Conference on\n  Communication, Control, and Computing (Allerton)", "doi": "10.1109/ALLERTON.2015.7447163", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a new framework for sensing and recovering\nstructured signals. In contrast to compressive sensing (CS) systems that employ\nlinear measurements, sparse representations, and computationally complex\nconvex/greedy algorithms, we introduce a deep learning framework that supports\nboth linear and mildly nonlinear measurements, that learns a structured\nrepresentation from training data, and that efficiently computes a signal\nestimate. In particular, we apply a stacked denoising autoencoder (SDA), as an\nunsupervised feature learner. SDA enables us to capture statistical\ndependencies between the different elements of certain signals and improve\nsignal recovery performance as compared to the CS approach.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 15:46:09 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Mousavi", "Ali", ""], ["Patel", "Ankit B.", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1508.04210", "submitter": "Changwei Hu", "authors": "Changwei Hu, Piyush Rai, Lawrence Carin", "title": "Zero-Truncated Poisson Tensor Factorization for Massive Binary Tensors", "comments": "UAI (Uncertainty in Artificial Intelligence) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a scalable Bayesian model for low-rank factorization of massive\ntensors with binary observations. The proposed model has the following key\nproperties: (1) in contrast to the models based on the logistic or probit\nlikelihood, using a zero-truncated Poisson likelihood for binary data allows\nour model to scale up in the number of \\emph{ones} in the tensor, which is\nespecially appealing for massive but sparse binary tensors; (2)\nside-information in form of binary pairwise relationships (e.g., an adjacency\nnetwork) between objects in any tensor mode can also be leveraged, which can be\nespecially useful in \"cold-start\" settings; and (3) the model admits simple\nBayesian inference via batch, as well as \\emph{online} MCMC; the latter allows\nscaling up even for \\emph{dense} binary data (i.e., when the number of ones in\nthe tensor/network is also massive). In addition, non-negative factor matrices\nin our model provide easy interpretability, and the tensor rank can be inferred\nfrom the data. We evaluate our model on several large-scale real-world binary\ntensors, achieving excellent computational scalability, and also demonstrate\nits usefulness in leveraging side-information provided in form of\nmode-network(s).\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 04:24:24 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Hu", "Changwei", ""], ["Rai", "Piyush", ""], ["Carin", "Lawrence", ""]]}, {"id": "1508.04211", "submitter": "Changwei Hu", "authors": "Changwei Hu, Piyush Rai, Changyou Chen, Matthew Harding, Lawrence\n  Carin", "title": "Scalable Bayesian Non-Negative Tensor Factorization for Massive Count\n  Data", "comments": "ECML PKDD 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Bayesian non-negative tensor factorization model for\ncount-valued tensor data, and develop scalable inference algorithms (both batch\nand online) for dealing with massive tensors. Our generative model can handle\noverdispersed counts as well as infer the rank of the decomposition. Moreover,\nleveraging a reparameterization of the Poisson distribution as a multinomial\nfacilitates conjugacy in the model and enables simple and efficient Gibbs\nsampling and variational Bayes (VB) inference updates, with a computational\ncost that only depends on the number of nonzeros in the tensor. The model also\nprovides a nice interpretability for the factors; in our model, each factor\ncorresponds to a \"topic\". We develop a set of online inference algorithms that\nallow further scaling up the model to massive tensors, for which batch\ninference methods may be infeasible. We apply our framework on diverse\nreal-world applications, such as \\emph{multiway} topic modeling on a scientific\npublications database, analyzing a political science data set, and analyzing a\nmassive household transactions data set.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 04:28:56 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Hu", "Changwei", ""], ["Rai", "Piyush", ""], ["Chen", "Changyou", ""], ["Harding", "Matthew", ""], ["Carin", "Lawrence", ""]]}, {"id": "1508.04306", "submitter": "John Hershey", "authors": "John R. Hershey, Zhuo Chen, Jonathan Le Roux, Shinji Watanabe", "title": "Deep clustering: Discriminative embeddings for segmentation and\n  separation", "comments": "Originally submitted on June 5, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of acoustic source separation in a deep learning\nframework we call \"deep clustering.\" Rather than directly estimating signals or\nmasking functions, we train a deep network to produce spectrogram embeddings\nthat are discriminative for partition labels given in training data. Previous\ndeep network approaches provide great advantages in terms of learning power and\nspeed, but previously it has been unclear how to use them to separate signals\nin a class-independent way. In contrast, spectral clustering approaches are\nflexible with respect to the classes and number of items to be segmented, but\nit has been unclear how to leverage the learning power and speed of deep\nnetworks. To obtain the best of both worlds, we use an objective function that\nto train embeddings that yield a low-rank approximation to an ideal pairwise\naffinity matrix, in a class-independent way. This avoids the high cost of\nspectral factorization and instead produces compact clusters that are amenable\nto simple clustering methods. The segmentations are therefore implicitly\nencoded in the embeddings, and can be \"decoded\" by clustering. Preliminary\nexperiments show that the proposed method can separate speech: when trained on\nspectrogram features containing mixtures of two speakers, and tested on\nmixtures of a held-out set of speakers, it can infer masking functions that\nimprove signal quality by around 6dB. We show that the model can generalize to\nthree-speaker mixtures despite training only on two-speaker mixtures. The\nframework can be used without class labels, and therefore has the potential to\nbe trained on a diverse set of sound types, and to generalize to novel sources.\nWe hope that future work will lead to segmentation of arbitrary sounds, with\nextensions to microphone array methods as well as image segmentation and other\ndomains.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 13:12:34 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Hershey", "John R.", ""], ["Chen", "Zhuo", ""], ["Roux", "Jonathan Le", ""], ["Watanabe", "Shinji", ""]]}, {"id": "1508.04319", "submitter": "Markus Heinonen", "authors": "Markus Heinonen, Henrik Mannerstr\\\"om, Juho Rousu, Samuel Kaski, Harri\n  L\\\"ahdesm\\\"aki", "title": "Non-Stationary Gaussian Process Regression with Hamiltonian Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for fully non-stationary Gaussian process\nregression (GPR), where all three key parameters -- noise variance, signal\nvariance and lengthscale -- can be simultaneously input-dependent. We develop\ngradient-based inference methods to learn the unknown function and the\nnon-stationary model parameters, without requiring any model approximations. We\npropose to infer full parameter posterior with Hamiltonian Monte Carlo (HMC),\nwhich conveniently extends the analytical gradient-based GPR learning by\nguiding the sampling with model gradients. We also learn the MAP solution from\nthe posterior by gradient ascent. In experiments on several synthetic datasets\nand in modelling of temporal gene expression, the nonstationary GPR is shown to\nbe necessary for modeling realistic input-dependent dynamics, while it performs\ncomparably to conventional stationary or previous non-stationary GPR models\notherwise.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 13:48:02 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Heinonen", "Markus", ""], ["Mannerstr\u00f6m", "Henrik", ""], ["Rousu", "Juho", ""], ["Kaski", "Samuel", ""], ["L\u00e4hdesm\u00e4ki", "Harri", ""]]}, {"id": "1508.04409", "submitter": "Marvin N Wright", "authors": "Marvin N. Wright and Andreas Ziegler", "title": "ranger: A Fast Implementation of Random Forests for High Dimensional\n  Data in C++ and R", "comments": null, "journal-ref": "Wright, M. N. & Ziegler, A. (2017). ranger: A fast implementation\n  of random forests for high dimensional data in C++ and R. Journal of\n  Statistical Software 77:1-17", "doi": "10.18637/jss.v077.i01", "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the C++ application and R package ranger. The software is a fast\nimplementation of random forests for high dimensional data. Ensembles of\nclassification, regression and survival trees are supported. We describe the\nimplementation, provide examples, validate the package with a reference\nimplementation, and compare runtime and memory usage with other\nimplementations. The new software proves to scale best with the number of\nfeatures, samples, trees, and features tried for splitting. Finally, we show\nthat ranger is the fastest and most memory efficient implementation of random\nforests to analyze data on the scale of a genome-wide association study.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 18:47:10 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 09:04:34 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Wright", "Marvin N.", ""], ["Ziegler", "Andreas", ""]]}, {"id": "1508.04422", "submitter": "Vince Lyzinski", "authors": "Aren Jansen, Gregory Sell, Vince Lyzinski", "title": "Scalable Out-of-Sample Extension of Graph Embeddings Using Deep Neural\n  Networks", "comments": "10 pages, 2 figures, 1 table, this paper is under consideration for\n  publication in Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several popular graph embedding techniques for representation learning and\ndimensionality reduction rely on performing computationally expensive\neigendecompositions to derive a nonlinear transformation of the input data\nspace. The resulting eigenvectors encode the embedding coordinates for the\ntraining samples only, and so the embedding of novel data samples requires\nfurther costly computation. In this paper, we present a method for the\nout-of-sample extension of graph embeddings using deep neural networks (DNN) to\nparametrically approximate these nonlinear maps. Compared with traditional\nnonparametric out-of-sample extension methods, we demonstrate that the DNNs can\ngeneralize with equal or better fidelity and require orders of magnitude less\ncomputation at test time. Moreover, we find that unsupervised pretraining of\nthe DNNs improves optimization for larger network sizes, thus removing\nsensitivity to model selection.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 19:47:31 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 16:07:53 GMT"}, {"version": "v3", "created": "Tue, 14 Jun 2016 15:50:41 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Jansen", "Aren", ""], ["Sell", "Gregory", ""], ["Lyzinski", "Vince", ""]]}, {"id": "1508.04467", "submitter": "Zhao Kang", "authors": "Zhao Kang, Chong Peng, Qiang Cheng", "title": "Robust Subspace Clustering via Smoothed Rank Approximation", "comments": "Journal, code is available", "journal-ref": "IEEE Signal Processing Letters, 22(2015)2088-2092", "doi": "10.1109/LSP.2015.2460737", "report-no": null, "categories": "cs.CV cs.IT cs.LG cs.NA math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix rank minimizing subject to affine constraints arises in many\napplication areas, ranging from signal processing to machine learning. Nuclear\nnorm is a convex relaxation for this problem which can recover the rank exactly\nunder some restricted and theoretically interesting conditions. However, for\nmany real-world applications, nuclear norm approximation to the rank function\ncan only produce a result far from the optimum. To seek a solution of higher\naccuracy than the nuclear norm, in this paper, we propose a rank approximation\nbased on Logarithm-Determinant. We consider using this rank approximation for\nsubspace clustering application. Our framework can model different kinds of\nerrors and noise. Effective optimization strategy is developed with theoretical\nguarantee to converge to a stationary point. The proposed method gives\npromising results on face clustering and motion segmentation tasks compared to\nthe state-of-the-art subspace clustering algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 21:54:03 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Kang", "Zhao", ""], ["Peng", "Chong", ""], ["Cheng", "Qiang", ""]]}, {"id": "1508.04486", "submitter": "Cem Subakan", "authors": "Y. Cem Subakan, Johannes Traa, Paris Smaragdis, Noah Stein", "title": "A Dictionary Learning Approach for Factorial Gaussian Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a parameter estimation method for factorially\nparametrized models such as Factorial Gaussian Mixture Model and Factorial\nHidden Markov Model. Our contributions are two-fold. First, we show that the\nemission matrix of the standard Factorial Model is unidentifiable even if the\ntrue assignment matrix is known. Secondly, we address the issue of\nidentifiability by making a one component sharing assumption and derive a\nparameter learning algorithm for this case. Our approach is based on a\ndictionary learning problem of the form $X = O R$, where the goal is to learn\nthe dictionary $O$ given the data matrix $X$. We argue that due to the specific\nstructure of the activation matrix $R$ in the shared component factorial\nmixture model, and an incoherence assumption on the shared component, it is\npossible to extract the columns of the $O$ matrix without the need for\nalternating between the estimation of $O$ and $R$.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 23:47:28 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Subakan", "Y. Cem", ""], ["Traa", "Johannes", ""], ["Smaragdis", "Paris", ""], ["Stein", "Noah", ""]]}, {"id": "1508.04554", "submitter": "Bokai Cao", "authors": "Bokai Cao, Xiangnan Kong, Jingyuan Zhang, Philip S. Yu and Ann B.\n  Ragin", "title": "Mining Brain Networks using Multiple Side Views for Neurological\n  Disorder Identification", "comments": "in Proceedings of IEEE International Conference on Data Mining (ICDM)\n  2015", "journal-ref": null, "doi": "10.1109/ICDM.2015.50", "report-no": null, "categories": "cs.LG cs.CV cs.CY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining discriminative subgraph patterns from graph data has attracted great\ninterest in recent years. It has a wide variety of applications in disease\ndiagnosis, neuroimaging, etc. Most research on subgraph mining focuses on the\ngraph representation alone. However, in many real-world applications, the side\ninformation is available along with the graph data. For example, for\nneurological disorder identification, in addition to the brain networks derived\nfrom neuroimaging data, hundreds of clinical, immunologic, serologic and\ncognitive measures may also be documented for each subject. These measures\ncompose multiple side views encoding a tremendous amount of supplemental\ninformation for diagnostic purposes, yet are often ignored. In this paper, we\nstudy the problem of discriminative subgraph selection using multiple side\nviews and propose a novel solution to find an optimal set of subgraph features\nfor graph classification by exploring a plurality of side views. We derive a\nfeature evaluation criterion, named gSide, to estimate the usefulness of\nsubgraph patterns based upon side views. Then we develop a branch-and-bound\nalgorithm, called gMSV, to efficiently search for optimal subgraph features by\nintegrating the subgraph mining process and the procedure of discriminative\nfeature selection. Empirical studies on graph classification tasks for\nneurological disorders using brain networks demonstrate that subgraph patterns\nselected by the multi-side-view guided subgraph selection approach can\neffectively boost graph classification performances and are relevant to disease\ndiagnosis.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 07:51:14 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Cao", "Bokai", ""], ["Kong", "Xiangnan", ""], ["Zhang", "Jingyuan", ""], ["Yu", "Philip S.", ""], ["Ragin", "Ann B.", ""]]}, {"id": "1508.04556", "submitter": "Michael Riis Andersen", "authors": "Michael Riis Andersen, Ole Winther, Lars Kai Hansen", "title": "Spatio-temporal Spike and Slab Priors for Multiple Measurement Vector\n  Problems", "comments": "6 pages, 6 figures, accepted for presentation at SPARS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in solving the multiple measurement vector (MMV) problem\nfor instances, where the underlying sparsity pattern exhibit spatio-temporal\nstructure motivated by the electroencephalogram (EEG) source localization\nproblem. We propose a probabilistic model that takes this structure into\naccount by generalizing the structured spike and slab prior and the associated\nExpectation Propagation inference scheme. Based on numerical experiments, we\ndemonstrate the viability of the model and the approximate inference scheme.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 08:09:06 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Andersen", "Michael Riis", ""], ["Winther", "Ole", ""], ["Hansen", "Lars Kai", ""]]}, {"id": "1508.04559", "submitter": "Przemys{\\l}aw Spurek", "authors": "Jacek Tabor, Przemys{\\l}aw Spurek, Konrad Kamieniecki, Marek \\'Smieja,\n  Krzysztof Misztal", "title": "Introduction to Cross-Entropy Clustering The R Package CEC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R Package CEC performs clustering based on the cross-entropy clustering\n(CEC) method, which was recently developed with the use of information theory.\nThe main advantage of CEC is that it combines the speed and simplicity of\n$k$-means with the ability to use various Gaussian mixture models and reduce\nunnecessary clusters. In this work we present a practical tutorial to CEC based\non the R Package CEC. Functions are provided to encompass the whole process of\nclustering.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 08:13:56 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Tabor", "Jacek", ""], ["Spurek", "Przemys\u0142aw", ""], ["Kamieniecki", "Konrad", ""], ["\u015amieja", "Marek", ""], ["Misztal", "Krzysztof", ""]]}, {"id": "1508.04757", "submitter": "Leonardo Ferreira", "authors": "Leonardo N. Ferreira and Liang Zhao", "title": "Time Series Clustering via Community Detection in Networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.ins.2015.07.046", "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a technique for time series clustering using\ncommunity detection in complex networks. Firstly, we present a method to\ntransform a set of time series into a network using different distance\nfunctions, where each time series is represented by a vertex and the most\nsimilar ones are connected. Then, we apply community detection algorithms to\nidentify groups of strongly connected vertices (called a community) and,\nconsequently, identify time series clusters. Still in this paper, we make a\ncomprehensive analysis on the influence of various combinations of time series\ndistance functions, network generation methods and community detection\ntechniques on clustering results. Experimental study shows that the proposed\nnetwork-based approach achieves better results than various classic or\nup-to-date clustering techniques under consideration. Statistical tests confirm\nthat the proposed method outperforms some classic clustering algorithms, such\nas $k$-medoids, diana, median-linkage and centroid-linkage in various data\nsets. Interestingly, the proposed method can effectively detect shape patterns\npresented in time series due to the topological structure of the underlying\nnetwork constructed in the clustering process. At the same time, other\ntechniques fail to identify such patterns. Moreover, the proposed method is\nrobust enough to group time series presenting similar pattern but with time\nshifts and/or amplitude variations. In summary, the main point of the proposed\nmethod is the transformation of time series from time-space domain to\ntopological domain. Therefore, we hope that our approach contributes not only\nfor time series clustering, but also for general time series analysis tasks.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 19:55:08 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Ferreira", "Leonardo N.", ""], ["Zhao", "Liang", ""]]}, {"id": "1508.04887", "submitter": "Ko-Jen Hsiao", "authors": "Ko-Jen Hsiao, Kevin S. Xu, Jeff Calder and Alfred O. Hero III", "title": "Multi-criteria Similarity-based Anomaly Detection using Pareto Depth\n  Analysis", "comments": "The work is submitted to IEEE TNNLS Special Issue on Learning in\n  Non-(geo)metric Spaces for review on October 28, 2013, revised on July 26,\n  2015 and accepted on July 30, 2015. A preliminary version of this work is\n  reported in the conference Advances in Neural Information Processing Systems\n  (NIPS) 2012", "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems 27\n  (2016) 1307-1321", "doi": "10.1109/TNNLS.2015.2466686", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of identifying patterns in a data set that exhibit\nanomalous behavior, often referred to as anomaly detection. Similarity-based\nanomaly detection algorithms detect abnormally large amounts of similarity or\ndissimilarity, e.g.~as measured by nearest neighbor Euclidean distances between\na test sample and the training samples. In many application domains there may\nnot exist a single dissimilarity measure that captures all possible anomalous\npatterns. In such cases, multiple dissimilarity measures can be defined,\nincluding non-metric measures, and one can test for anomalies by scalarizing\nusing a non-negative linear combination of them. If the relative importance of\nthe different dissimilarity measures are not known in advance, as in many\nanomaly detection applications, the anomaly detection algorithm may need to be\nexecuted multiple times with different choices of weights in the linear\ncombination. In this paper, we propose a method for similarity-based anomaly\ndetection using a novel multi-criteria dissimilarity measure, the Pareto depth.\nThe proposed Pareto depth analysis (PDA) anomaly detection algorithm uses the\nconcept of Pareto optimality to detect anomalies under multiple criteria\nwithout having to run an algorithm multiple times with different choices of\nweights. The proposed PDA approach is provably better than using linear\ncombinations of the criteria and shows superior performance on experiments with\nsynthetic and real data sets.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 06:25:52 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Hsiao", "Ko-Jen", ""], ["Xu", "Kevin S.", ""], ["Calder", "Jeff", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1508.04904", "submitter": "Brendan Guillouet", "authors": "Philippe Besse (INSA Toulouse, IMT), Brendan Guillouet (IMT),\n  Jean-Michel Loubes, Royer Fran\\c{c}ois", "title": "Review and Perspective for Distance Based Trajectory Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we tackle the issue of clustering trajectories of geolocalized\nobservations. Using clustering technics based on the choice of a distance\nbetween the observations, we first provide a comprehensive review of the\ndifferent distances used in the literature to compare trajectories. Then based\non the limitations of these methods, we introduce a new distance : Symmetrized\nSegment-Path Distance (SSPD). We finally compare this new distance to the\nothers according to their corresponding clustering results obtained using both\nhierarchical clustering and affinity propagation methods.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 07:46:15 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Besse", "Philippe", "", "INSA Toulouse, IMT"], ["Guillouet", "Brendan", "", "IMT"], ["Loubes", "Jean-Michel", ""], ["Fran\u00e7ois", "Royer", ""]]}, {"id": "1508.04912", "submitter": "Rocco De Rosa rd", "authors": "Rocco De Rosa, Francesco Orabona, Nicol\\`o Cesa-Bianchi", "title": "The ABACOC Algorithm: a Novel Approach for Nonparametric Classification\n  of Data Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stream mining poses unique challenges to machine learning: predictive models\nare required to be scalable, incrementally trainable, must remain bounded in\nsize (even when the data stream is arbitrarily long), and be nonparametric in\norder to achieve high accuracy even in complex and dynamic environments.\nMoreover, the learning system must be parameterless ---traditional tuning\nmethods are problematic in streaming settings--- and avoid requiring prior\nknowledge of the number of distinct class labels occurring in the stream. In\nthis paper, we introduce a new algorithmic approach for nonparametric learning\nin data streams. Our approach addresses all above mentioned challenges by\nlearning a model that covers the input space using simple local classifiers.\nThe distribution of these classifiers dynamically adapts to the local (unknown)\ncomplexity of the classification problem, thus achieving a good balance between\nmodel complexity and predictive accuracy. We design four variants of our\napproach of increasing adaptivity. By means of an extensive empirical\nevaluation against standard nonparametric baselines, we show state-of-the-art\nresults in terms of accuracy versus model size. For the variant that imposes a\nstrict bound on the model size, we show better performance against all other\nmethods measured at the same model size value. Our empirical analysis is\ncomplemented by a theoretical performance guarantee which does not rely on any\nstochastic assumption on the source generating the stream.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 08:15:08 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["De Rosa", "Rocco", ""], ["Orabona", "Francesco", ""], ["Cesa-Bianchi", "Nicol\u00f2", ""]]}, {"id": "1508.04945", "submitter": "Lianwen Jin", "authors": "Weixin Yang, Lianwen Jin, Manfei Liu", "title": "DeepWriterID: An End-to-end Online Text-independent Writer\n  Identification System", "comments": "7 pages5 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Owing to the rapid growth of touchscreen mobile terminals and pen-based\ninterfaces, handwriting-based writer identification systems are attracting\nincreasing attention for personal authentication, digital forensics, and other\napplications. However, most studies on writer identification have not been\nsatisfying because of the insufficiency of data and difficulty of designing\ngood features under various conditions of handwritings. Hence, we introduce an\nend-to-end system, namely DeepWriterID, employed a deep convolutional neural\nnetwork (CNN) to address these problems. A key feature of DeepWriterID is a new\nmethod we are proposing, called DropSegment. It designs to achieve data\naugmentation and improve the generalized applicability of CNN. For sufficient\nfeature representation, we further introduce path signature feature maps to\nimprove performance. Experiments were conducted on the NLPR handwriting\ndatabase. Even though we only use pen-position information in the pen-down\nstate of the given handwriting samples, we achieved new state-of-the-art\nidentification rates of 95.72% for Chinese text and 98.51% for English text.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 10:39:19 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2015 14:05:48 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Yang", "Weixin", ""], ["Jin", "Lianwen", ""], ["Liu", "Manfei", ""]]}, {"id": "1508.04999", "submitter": "Juhan Nam", "authors": "Juhan Nam, Jorge Herrera, Kyogu Lee", "title": "A Deep Bag-of-Features Model for Music Auto-Tagging", "comments": "We resubmit a new version to revive the paper and record it as a\n  technical report. We did not add any incremental work to the previous work\n  but removed out some sections (criticized by a review process) and polished\n  sentences accordingly", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature learning and deep learning have drawn great attention in recent years\nas a way of transforming input data into more effective representations using\nlearning algorithms. Such interest has grown in the area of music information\nretrieval (MIR) as well, particularly in music audio classification tasks such\nas auto-tagging. In this paper, we present a two-stage learning model to\neffectively predict multiple labels from music audio. The first stage learns to\nproject local spectral patterns of an audio track onto a high-dimensional\nsparse space in an unsupervised manner and summarizes the audio track as a\nbag-of-features. The second stage successively performs the unsupervised\nlearning on the bag-of-features in a layer-by-layer manner to initialize a deep\nneural network and finally fine-tunes it with the tag labels. Through the\nexperiment, we rigorously examine training choices and tuning parameters, and\nshow that the model achieves high performance on Magnatagatune, a popularly\nused dataset in music auto-tagging.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 14:38:56 GMT"}, {"version": "v2", "created": "Sat, 18 Jun 2016 02:45:04 GMT"}, {"version": "v3", "created": "Sun, 16 Oct 2016 13:03:20 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Nam", "Juhan", ""], ["Herrera", "Jorge", ""], ["Lee", "Kyogu", ""]]}, {"id": "1508.05003", "submitter": "Suvrit Sra", "authors": "Suvrit Sra, Adams Wei Yu, Mu Li, Alexander J. Smola", "title": "AdaDelay: Delay Adaptive Distributed Stochastic Convex Optimization", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed stochastic convex optimization under the delayed\ngradient model where the server nodes perform parameter updates, while the\nworker nodes compute stochastic gradients. We discuss, analyze, and experiment\nwith a setup motivated by the behavior of real-world distributed computation\nnetworks, where the machines are differently slow at different time. Therefore,\nwe allow the parameter updates to be sensitive to the actual delays\nexperienced, rather than to worst-case bounds on the maximum delay. This\nsensitivity leads to larger stepsizes, that can help gain rapid initial\nconvergence without having to wait too long for slower machines, while\nmaintaining the same asymptotic complexity. We obtain encouraging improvements\nto overall convergence for distributed experiments on real datasets with up to\nbillions of examples and features.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 15:11:11 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Sra", "Suvrit", ""], ["Yu", "Adams Wei", ""], ["Li", "Mu", ""], ["Smola", "Alexander J.", ""]]}, {"id": "1508.05170", "submitter": "Dylan Foster", "authors": "Dylan J. Foster, Alexander Rakhlin, Karthik Sridharan", "title": "Adaptive Online Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework for studying adaptive regret bounds in the\nonline learning framework, including model selection bounds and data-dependent\nbounds. Given a data- or model-dependent bound we ask, \"Does there exist some\nalgorithm achieving this bound?\" We show that modifications to recently\nintroduced sequential complexity measures can be used to answer this question\nby providing sufficient conditions under which adaptive rates can be achieved.\nIn particular each adaptive rate induces a set of so-called offset complexity\nmeasures, and obtaining small upper bounds on these quantities is sufficient to\ndemonstrate achievability. A cornerstone of our analysis technique is the use\nof one-sided tail inequalities to bound suprema of offset random processes.\n  Our framework recovers and improves a wide variety of adaptive bounds\nincluding quantile bounds, second-order data-dependent bounds, and small loss\nbounds. In addition we derive a new type of adaptive bound for online linear\noptimization based on the spectral norm, as well as a new online PAC-Bayes\ntheorem that holds for countably infinite sets.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2015 03:44:43 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 02:49:33 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Foster", "Dylan J.", ""], ["Rakhlin", "Alexander", ""], ["Sridharan", "Karthik", ""]]}, {"id": "1508.05243", "submitter": "Mario Lucic", "authors": "Mario Lucic, Olivier Bachem, Andreas Krause", "title": "Strong Coresets for Hard and Soft Bregman Clustering with Applications\n  to Exponential Family Mixtures", "comments": "14 pages, camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coresets are efficient representations of data sets such that models trained\non the coreset are provably competitive with models trained on the original\ndata set. As such, they have been successfully used to scale up clustering\nmodels such as K-Means and Gaussian mixture models to massive data sets.\nHowever, until now, the algorithms and the corresponding theory were usually\nspecific to each clustering problem.\n  We propose a single, practical algorithm to construct strong coresets for a\nlarge class of hard and soft clustering problems based on Bregman divergences.\nThis class includes hard clustering with popular distortion measures such as\nthe Squared Euclidean distance, the Mahalanobis distance, KL-divergence and\nItakura-Saito distance. The corresponding soft clustering problems are directly\nrelated to popular mixture models due to a dual relationship between Bregman\ndivergences and Exponential family distributions. Our theoretical results\nfurther imply a randomized polynomial-time approximation scheme for hard\nclustering. We demonstrate the practicality of the proposed algorithm in an\nempirical evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2015 11:31:04 GMT"}, {"version": "v2", "created": "Mon, 2 May 2016 15:11:23 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Lucic", "Mario", ""], ["Bachem", "Olivier", ""], ["Krause", "Andreas", ""]]}, {"id": "1508.05249", "submitter": "Ingo Steinwart", "authors": "Ingo Steinwart", "title": "Representation of Quasi-Monotone Functionals by Families of Separating\n  Hyperplanes", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.FA math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterize when the level sets of a continuous quasi-monotone functional\ndefined on a suitable convex subset of a normed space can be uniquely\nrepresented by a family of bounded continuous functionals. Furthermore, we\ninvestigate how regularly these functionals depend on the parameterizing level.\nFinally, we show how this question relates to the recent problem of property\nelicitation that simultaneously attracted interest in machine learning,\nstatistical evaluation of forecasts, and finance.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2015 11:58:42 GMT"}], "update_date": "2015-08-24", "authors_parsed": [["Steinwart", "Ingo", ""]]}, {"id": "1508.05383", "submitter": "Ni  Ding Miss", "authors": "Ni Ding, Parastoo Sadeghi and Rodney A. Kennedy", "title": "On Monotonicity of the Optimal Transmission Policy in Cross-layer\n  Adaptive m-QAM Modulation", "comments": "27 pages (single column), 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a cross-layer adaptive modulation system that is modeled\nas a Markov decision process (MDP). We study how to utilize the monotonicity of\nthe optimal transmission policy to relieve the computational complexity of\ndynamic programming (DP). In this system, a scheduler controls the bit rate of\nthe m-quadrature amplitude modulation (m-QAM) in order to minimize the\nlong-term losses incurred by the queue overflow in the data link layer and the\ntransmission power consumption in the physical layer. The work is done in two\nsteps. Firstly, we observe the L-natural-convexity and submodularity of DP to\nprove that the optimal policy is always nondecreasing in queue occupancy/state\nand derive the sufficient condition for it to be nondecreasing in both queue\nand channel states. We also show that, due to the L-natural-convexity of DP,\nthe variation of the optimal policy in queue state is restricted by a bounded\nmarginal effect: The increment of the optimal policy between adjacent queue\nstates is no greater than one. Secondly, we use the monotonicity results to\npresent two low complexity algorithms: monotonic policy iteration (MPI) based\non L-natural-convexity and discrete simultaneous perturbation stochastic\napproximation (DSPSA). We run experiments to show that the time complexity of\nMPI based on L-natural-convexity is much lower than that of DP and the\nconventional MPI that is based on submodularity and DSPSA is able to adaptively\ntrack the optimal policy when the system parameters change.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2015 03:23:21 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Ding", "Ni", ""], ["Sadeghi", "Parastoo", ""], ["Kennedy", "Rodney A.", ""]]}, {"id": "1508.05495", "submitter": "Mehdi Korki", "authors": "Mehdi Korki, Hadi Zayyani, and Jingxin Zhang", "title": "Bayesian Hypothesis Testing for Block Sparse Signal Recovery", "comments": "5 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:1412.2316", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter presents a novel Block Bayesian Hypothesis Testing Algorithm\n(Block-BHTA) for reconstructing block sparse signals with unknown block\nstructures. The Block-BHTA comprises the detection and recovery of the\nsupports, and the estimation of the amplitudes of the block sparse signal. The\nsupport detection and recovery is performed using a Bayesian hypothesis\ntesting. Then, based on the detected and reconstructed supports, the nonzero\namplitudes are estimated by linear MMSE. The effectiveness of Block-BHTA is\ndemonstrated by numerical experiments.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2015 10:58:25 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Korki", "Mehdi", ""], ["Zayyani", "Hadi", ""], ["Zhang", "Jingxin", ""]]}, {"id": "1508.05514", "submitter": "Tohid Ardeshiri", "authors": "Tohid Ardeshiri, Umut Orguner, Emre \\\"Ozkan", "title": "Gaussian Mixture Reduction Using Reverse Kullback-Leibler Divergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a greedy mixture reduction algorithm which is capable of pruning\nmixture components as well as merging them based on the Kullback-Leibler\ndivergence (KLD). The algorithm is distinct from the well-known Runnalls' KLD\nbased method since it is not restricted to merging operations. The capability\nof pruning (in addition to merging) gives the algorithm the ability of\npreserving the peaks of the original mixture during the reduction. Analytical\napproximations are derived to circumvent the computational intractability of\nthe KLD which results in a computationally efficient method. The proposed\nalgorithm is compared with Runnalls' and Williams' methods in two numerical\nexamples, using both simulated and real world data. The results indicate that\nthe performance and computational complexity of the proposed approach make it\nan efficient alternative to existing mixture reduction methods.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2015 13:41:17 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Ardeshiri", "Tohid", ""], ["Orguner", "Umut", ""], ["\u00d6zkan", "Emre", ""]]}, {"id": "1508.05550", "submitter": "Ofir Lindenbaum", "authors": "Ofir Lindenbaum, Arie Yeredor, Moshe Salhov, Amir Averbuch", "title": "MultiView Diffusion Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we address the challenging task of achieving multi-view\ndimensionality reduction. The goal is to effectively use the availability of\nmultiple views for extracting a coherent low-dimensional representation of the\ndata. The proposed method exploits the intrinsic relation within each view, as\nwell as the mutual relations between views. The multi-view dimensionality\nreduction is achieved by defining a cross-view model in which an implied random\nwalk process is restrained to hop between objects in the different views. The\nmethod is robust to scaling and insensitive to small structural changes in the\ndata. We define new diffusion distances and analyze the spectra of the proposed\nkernel. We show that the proposed framework is useful for various machine\nlearning applications such as clustering, classification, and manifold\nlearning. Finally, by fusing multi-sensor seismic data we present a method for\nautomatic identification of seismic events.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2015 00:20:17 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2015 16:48:36 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2016 08:41:28 GMT"}, {"version": "v4", "created": "Mon, 22 Feb 2016 09:01:14 GMT"}, {"version": "v5", "created": "Mon, 11 Jul 2016 16:20:04 GMT"}, {"version": "v6", "created": "Mon, 6 Mar 2017 23:28:34 GMT"}, {"version": "v7", "created": "Wed, 5 Jun 2019 02:19:15 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Lindenbaum", "Ofir", ""], ["Yeredor", "Arie", ""], ["Salhov", "Moshe", ""], ["Averbuch", "Amir", ""]]}, {"id": "1508.05565", "submitter": "Weicong Ding", "authors": "Weicong Ding, Prakash Ishwar, Venkatesh Saligrama", "title": "Necessary and Sufficient Conditions and a Provably Efficient Algorithm\n  for Separable Topic Discovery", "comments": "Typo corrected; Revised argument in Lemma 3 and 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop necessary and sufficient conditions and a novel provably\nconsistent and efficient algorithm for discovering topics (latent factors) from\nobservations (documents) that are realized from a probabilistic mixture of\nshared latent factors that have certain properties. Our focus is on the class\nof topic models in which each shared latent factor contains a novel word that\nis unique to that factor, a property that has come to be known as separability.\nOur algorithm is based on the key insight that the novel words correspond to\nthe extreme points of the convex hull formed by the row-vectors of a suitably\nnormalized word co-occurrence matrix. We leverage this geometric insight to\nestablish polynomial computation and sample complexity bounds based on a few\nisotropic random projections of the rows of the normalized word co-occurrence\nmatrix. Our proposed random-projections-based algorithm is naturally amenable\nto an efficient distributed implementation and is attractive for modern\nweb-scale distributed data mining applications.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2015 03:44:26 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2015 18:26:33 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Ding", "Weicong", ""], ["Ishwar", "Prakash", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1508.05608", "submitter": "Yahel David", "authors": "Yahel David and Nahum Shimkin", "title": "The Max $K$-Armed Bandit: A PAC Lower Bound and tighter Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Max $K$-Armed Bandit problem, where a learning agent is faced\nwith several sources (arms) of items (rewards), and interested in finding the\nbest item overall. At each time step the agent chooses an arm, and obtains a\nrandom real valued reward. The rewards of each arm are assumed to be i.i.d.,\nwith an unknown probability distribution that generally differs among the arms.\nUnder the PAC framework, we provide lower bounds on the sample complexity of\nany $(\\epsilon,\\delta)$-correct algorithm, and propose algorithms that attain\nthis bound up to logarithmic factors. We compare the performance of this\nmulti-arm algorithms to the variant in which the arms are not distinguishable\nby the agent and are chosen randomly at each stage. Interestingly, when the\nmaximal rewards of the arms happen to be similar, the latter approach may\nprovide better performance.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2015 13:38:15 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["David", "Yahel", ""], ["Shimkin", "Nahum", ""]]}, {"id": "1508.05711", "submitter": "Zhao Shen-Yi", "authors": "Shen-Yi Zhao and Wu-Jun Li", "title": "Fast Asynchronous Parallel Stochastic Gradient Decent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent~(SGD) and its variants have become more and more\npopular in machine learning due to their efficiency and effectiveness. To\nhandle large-scale problems, researchers have recently proposed several\nparallel SGD methods for multicore systems. However, existing parallel SGD\nmethods cannot achieve satisfactory performance in real applications. In this\npaper, we propose a fast asynchronous parallel SGD method, called AsySVRG, by\ndesigning an asynchronous strategy to parallelize the recently proposed SGD\nvariant called stochastic variance reduced gradient~(SVRG). Both theoretical\nand empirical results show that AsySVRG can outperform existing\nstate-of-the-art parallel SGD methods like Hogwild! in terms of convergence\nrate and computation cost.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 07:51:09 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Zhao", "Shen-Yi", ""], ["Li", "Wu-Jun", ""]]}, {"id": "1508.05803", "submitter": "Felipe Llinares", "authors": "Felipe Llinares-Lopez, Laetitia Papaxanthos, Dean Bodenham, Karsten\n  Borgwardt", "title": "Searching for significant patterns in stratified data", "comments": "18 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant pattern mining, the problem of finding itemsets that are\nsignificantly enriched in one class of objects, is statistically challenging,\nas the large space of candidate patterns leads to an enormous multiple testing\nproblem. Recently, the concept of testability was proposed as one approach to\ncorrect for multiple testing in pattern mining while retaining statistical\npower. Still, these strategies based on testability do not allow one to\ncondition the test of significance on the observed covariates, which severely\nlimits its utility in biomedical applications. Here we propose a strategy and\nan efficient algorithm to perform significant pattern mining in the presence of\ncategorical covariates with K states.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 13:53:06 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Llinares-Lopez", "Felipe", ""], ["Papaxanthos", "Laetitia", ""], ["Bodenham", "Dean", ""], ["Borgwardt", "Karsten", ""]]}, {"id": "1508.05913", "submitter": "Boxiang Wang", "authors": "Boxiang Wang, Hui Zou", "title": "Another Look at DWD: Thrifty Algorithm and Bayes Risk Consistency in\n  RKHS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance weighted discrimination (DWD) is a margin-based classifier with an\ninteresting geometric motivation. DWD was originally proposed as a superior\nalternative to the support vector machine (SVM), however DWD is yet to be\npopular compared with the SVM. The main reasons are twofold. First, the\nstate-of-the-art algorithm for solving DWD is based on the second-order-cone\nprogramming (SOCP), while the SVM is a quadratic programming problem which is\nmuch more efficient to solve. Second, the current statistical theory of DWD\nmainly focuses on the linear DWD for the high-dimension-low-sample-size setting\nand data-piling, while the learning theory for the SVM mainly focuses on the\nBayes risk consistency of the kernel SVM. In fact, the Bayes risk consistency\nof DWD is presented as an open problem in the original DWD paper. In this work,\nwe advance the current understanding of DWD from both computational and\ntheoretical perspectives. We propose a novel efficient algorithm for solving\nDWD, and our algorithm can be several hundred times faster than the existing\nstate-of-the-art algorithm based on the SOCP. In addition, our algorithm can\nhandle the generalized DWD, while the SOCP algorithm only works well for a\nspecial DWD but not the generalized DWD. Furthermore, we consider a natural\nkernel DWD in a reproducing kernel Hilbert space and then establish the Bayes\nrisk consistency of the kernel DWD. We compare DWD and the SVM on several\nbenchmark data sets and show that the two have comparable classification\naccuracy, but DWD equipped with our new algorithm can be much faster to compute\nthan the SVM.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 18:59:03 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Wang", "Boxiang", ""], ["Zou", "Hui", ""]]}, {"id": "1508.06091", "submitter": "Charanpal Dhanjal", "authors": "Charanpal Dhanjal (LTCI), Romaric Gaudel (SEQUEL), Stephan Clemencon\n  (LTCI)", "title": "AUC Optimisation and Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recommendation systems, one is interested in the ranking of the predicted\nitems as opposed to other losses such as the mean squared error. Although a\nvariety of ways to evaluate rankings exist in the literature, here we focus on\nthe Area Under the ROC Curve (AUC) as it widely used and has a strong\ntheoretical underpinning. In practical recommendation, only items at the top of\nthe ranked list are presented to the users. With this in mind, we propose a\nclass of objective functions over matrix factorisations which primarily\nrepresent a smooth surrogate for the real AUC, and in a special case we show\nhow to prioritise the top of the list. The objectives are differentiable and\noptimised through a carefully designed stochastic gradient-descent-based\nalgorithm which scales linearly with the size of the data. In the special case\nof square loss we show how to improve computational complexity by leveraging\npreviously computed measures. To understand theoretically the underlying matrix\nfactorisation approaches we study both the consistency of the loss functions\nwith respect to AUC, and generalisation using Rademacher theory. The resulting\ngeneralisation analysis gives strong motivation for the optimisation under\nstudy. Finally, we provide computation results as to the efficacy of the\nproposed method using synthetic and real data.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 09:46:09 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Dhanjal", "Charanpal", "", "LTCI"], ["Gaudel", "Romaric", "", "SEQUEL"], ["Clemencon", "Stephan", "", "LTCI"]]}, {"id": "1508.06095", "submitter": "Rossella Cancelliere", "authors": "Rossella Cancelliere, Mario Gai, Patrick Gallinari, Luca Rubini", "title": "OCReP: An Optimally Conditioned Regularization for Pseudoinversion Based\n  Neural Training", "comments": "Published on Neural Networks", "journal-ref": null, "doi": "10.1016/j.neunet.2015.07.015", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the training of single hidden layer neural networks\nby pseudoinversion, which, in spite of its popularity, is sometimes affected by\nnumerical instability issues. Regularization is known to be effective in such\ncases, so that we introduce, in the framework of Tikhonov regularization, a\nmatricial reformulation of the problem which allows us to use the condition\nnumber as a diagnostic tool for identification of instability. By imposing\nwell-conditioning requirements on the relevant matrices, our theoretical\nanalysis allows the identification of an optimal value for the regularization\nparameter from the standpoint of stability. We compare with the value derived\nby cross-validation for overfitting control and optimisation of the\ngeneralization performance. We test our method for both regression and\nclassification tasks. The proposed method is quite effective in terms of\npredictivity, often with some improvement on performance with respect to the\nreference cases considered. This approach, due to analytical determination of\nthe regularization parameter, dramatically reduces the computational load\nrequired by many other techniques.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 10:09:31 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Cancelliere", "Rossella", ""], ["Gai", "Mario", ""], ["Gallinari", "Patrick", ""], ["Rubini", "Luca", ""]]}, {"id": "1508.06235", "submitter": "Daniel Khashabi Mr.", "authors": "Daniel Khashabi, John Wieting, Jeffrey Yufei Liu, Feng Liang", "title": "Clustering With Side Information: From a Probabilistic Model to a\n  Deterministic Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a model-based clustering method (TVClust) that\nrobustly incorporates noisy side information as soft-constraints and aims to\nseek a consensus between side information and the observed data. Our method is\nbased on a nonparametric Bayesian hierarchical model that combines the\nprobabilistic model for the data instance and the one for the side-information.\nAn efficient Gibbs sampling algorithm is proposed for posterior inference.\nUsing the small-variance asymptotics of our probabilistic model, we then derive\na new deterministic clustering algorithm (RDP-means). It can be viewed as an\nextension of K-means that allows for the inclusion of side information and has\nthe additional property that the number of clusters does not need to be\nspecified a priori. Empirical studies have been carried out to compare our work\nwith many constrained clustering algorithms from the literature on both a\nvariety of data sets and under a variety of conditions such as using noisy side\ninformation and erroneous k values. The results of our experiments show strong\nresults for our probabilistic and deterministic approaches under these\nconditions when compared to other algorithms in the literature.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 18:13:27 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2015 17:46:36 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2015 17:48:54 GMT"}, {"version": "v4", "created": "Sat, 31 Oct 2015 05:38:15 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Khashabi", "Daniel", ""], ["Wieting", "John", ""], ["Liu", "Jeffrey Yufei", ""], ["Liang", "Feng", ""]]}, {"id": "1508.06388", "submitter": "Mu Qiao", "authors": "Mu Qiao and Jia Li", "title": "Gaussian Mixture Models with Component Means Constrained in Pre-selected\n  Subspaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a Gaussian mixture model (GMM) with component means\nconstrained in a pre-selected subspace. Applications to classification and\nclustering are explored. An EM-type estimation algorithm is derived. We prove\nthat the subspace containing the component means of a GMM with a common\ncovariance matrix also contains the modes of the density and the class means.\nThis motivates us to find a subspace by applying weighted principal component\nanalysis to the modes of a kernel density and the class means. To circumvent\nthe difficulty of deciding the kernel bandwidth, we acquire multiple subspaces\nfrom the kernel densities based on a sequence of bandwidths. The GMM\nconstrained by each subspace is estimated; and the model yielding the maximum\nlikelihood is chosen. A dimension reduction property is proved in the sense of\nbeing informative for classification or clustering. Experiments on real and\nsimulated data sets are conducted to examine several ways of determining the\nsubspace and to compare with the reduced rank mixture discriminant analysis\n(MDA). Our new method with the simple technique of spanning the subspace only\nby class means often outperforms the reduced rank MDA when the subspace\ndimension is very low, making it particularly appealing for visualization.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 07:25:22 GMT"}], "update_date": "2015-08-27", "authors_parsed": [["Qiao", "Mu", ""], ["Li", "Jia", ""]]}, {"id": "1508.06446", "submitter": "Lavanya Sita Tekumalla", "authors": "Lavanya Sita Tekumalla, Priyanka Agrawal, Indrajit Bhattacharya", "title": "Nested Hierarchical Dirichlet Processes for Multi-Level Non-Parametric\n  Admixture Modeling", "comments": "Proceedings of European Conference of Machine Learning (ECML) 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dirichlet Process(DP) is a Bayesian non-parametric prior for infinite mixture\nmodeling, where the number of mixture components grows with the number of data\nitems. The Hierarchical Dirichlet Process (HDP), is an extension of DP for\ngrouped data, often used for non-parametric topic modeling, where each group is\na mixture over shared mixture densities. The Nested Dirichlet Process (nDP), on\nthe other hand, is an extension of the DP for learning group level\ndistributions from data, simultaneously clustering the groups. It allows group\nlevel distributions to be shared across groups in a non-parametric setting,\nleading to a non-parametric mixture of mixtures. The nCRF extends the nDP for\nmultilevel non-parametric mixture modeling, enabling modeling topic\nhierarchies. However, the nDP and nCRF do not allow sharing of distributions as\nrequired in many applications, motivating the need for multi-level\nnon-parametric admixture modeling. We address this gap by proposing multi-level\nnested HDPs (nHDP) where the base distribution of the HDP is itself a HDP at\neach level thereby leading to admixtures of admixtures at each level. Because\nof couplings between various HDP levels, scaling up is naturally a challenge\nduring inference. We propose a multi-level nested Chinese Restaurant Franchise\n(nCRF) representation for the nested HDP, with which we outline an inference\nalgorithm based on Gibbs Sampling. We evaluate our model with the two level\nnHDP for non-parametric entity topic modeling where an inner HDP creates a\ncountably infinite set of topic mixtures and associates them with author\nentities, while an outer HDP associates documents with these author entities.\nIn our experiments on two real world research corpora, the nHDP is able to\ngeneralize significantly better than existing models and detect missing author\nentities with a reasonable level of accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 11:24:36 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2015 08:14:45 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Tekumalla", "Lavanya Sita", ""], ["Agrawal", "Priyanka", ""], ["Bhattacharya", "Indrajit", ""]]}, {"id": "1508.06574", "submitter": "Louis Aslett", "authors": "Louis J. M. Aslett, Pedro M. Esperan\\c{c}a, Chris C. Holmes", "title": "A review of homomorphic encryption and software tools for encrypted\n  statistical machine learning", "comments": "21 pages, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in cryptography promise to enable secure statistical\ncomputation on encrypted data, whereby a limited set of operations can be\ncarried out without the need to first decrypt. We review these homomorphic\nencryption schemes in a manner accessible to statisticians and machine\nlearners, focusing on pertinent limitations inherent in the current state of\nthe art. These limitations restrict the kind of statistics and machine learning\nalgorithms which can be implemented and we review those which have been\nsuccessfully applied in the literature. Finally, we document a high performance\nR package implementing a recent homomorphic scheme in a general framework.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 17:11:12 GMT"}], "update_date": "2015-08-27", "authors_parsed": [["Aslett", "Louis J. M.", ""], ["Esperan\u00e7a", "Pedro M.", ""], ["Holmes", "Chris C.", ""]]}, {"id": "1508.06615", "submitter": "Yoon Kim", "authors": "Yoon Kim, Yacine Jernite, David Sontag, Alexander M. Rush", "title": "Character-Aware Neural Language Models", "comments": "AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a simple neural language model that relies only on\ncharacter-level inputs. Predictions are still made at the word-level. Our model\nemploys a convolutional neural network (CNN) and a highway network over\ncharacters, whose output is given to a long short-term memory (LSTM) recurrent\nneural network language model (RNN-LM). On the English Penn Treebank the model\nis on par with the existing state-of-the-art despite having 60% fewer\nparameters. On languages with rich morphology (Arabic, Czech, French, German,\nSpanish, Russian), the model outperforms word-level/morpheme-level LSTM\nbaselines, again with fewer parameters. The results suggest that on many\nlanguages, character inputs are sufficient for language modeling. Analysis of\nword representations obtained from the character composition part of the model\nreveals that the model is able to encode, from characters only, both semantic\nand orthographic information.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 19:25:34 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2015 23:18:00 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2015 03:18:13 GMT"}, {"version": "v4", "created": "Tue, 1 Dec 2015 22:59:24 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Kim", "Yoon", ""], ["Jernite", "Yacine", ""], ["Sontag", "David", ""], ["Rush", "Alexander M.", ""]]}, {"id": "1508.06845", "submitter": "Louis Aslett", "authors": "Louis J. M. Aslett, Pedro M. Esperan\\c{c}a, Chris C. Holmes", "title": "Encrypted statistical machine learning: new privacy preserving methods", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two new statistical machine learning methods designed to learn on\nfully homomorphic encrypted (FHE) data. The introduction of FHE schemes\nfollowing Gentry (2009) opens up the prospect of privacy preserving statistical\nmachine learning analysis and modelling of encrypted data without compromising\nsecurity constraints. We propose tailored algorithms for applying extremely\nrandom forests, involving a new cryptographic stochastic fraction estimator,\nand na\\\"{i}ve Bayes, involving a semi-parametric model for the class decision\nboundary, and show how they can be used to learn and predict from encrypted\ndata. We demonstrate that these techniques perform competitively on a variety\nof classification data sets and provide detailed information about the\ncomputational practicalities of these and other FHE methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 13:06:55 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Aslett", "Louis J. M.", ""], ["Esperan\u00e7a", "Pedro M.", ""], ["Holmes", "Chris C.", ""]]}, {"id": "1508.06901", "submitter": "Xin Yuan", "authors": "Xin Yuan, Hong Jiang, Gang Huang, Paul A. Wilford", "title": "Compressive Sensing via Low-Rank Gaussian Mixture Models", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new compressive sensing (CS) inversion algorithm by utilizing\nthe Gaussian mixture model (GMM). While the compressive sensing is performed\nglobally on the entire image as implemented in our lensless camera, a low-rank\nGMM is imposed on the local image patches. This low-rank GMM is derived via\neigenvalue thresholding of the GMM trained on the projection of the measurement\ndata, thus learned {\\em in situ}. The GMM and the projection of the measurement\ndata are updated iteratively during the reconstruction. Our GMM algorithm\ndegrades to the piecewise linear estimator (PLE) if each patch is represented\nby a single Gaussian model. Inspired by this, a low-rank PLE algorithm is also\ndeveloped for CS inversion, constituting an additional contribution of this\npaper. Extensive results on both simulation data and real data captured by the\nlensless camera demonstrate the efficacy of the proposed algorithm.\nFurthermore, we compare the CS reconstruction results using our algorithm with\nthe JPEG compression. Simulation results demonstrate that when limited\nbandwidth is available (a small number of measurements), our algorithm can\nachieve comparable results as JPEG.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 15:35:11 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Yuan", "Xin", ""], ["Jiang", "Hong", ""], ["Huang", "Gang", ""], ["Wilford", "Paul A.", ""]]}, {"id": "1508.06916", "submitter": "Vladimir Teif", "authors": "Vladimir B. Teif", "title": "Nucleosome positioning: resources and tools online", "comments": "To appear in Briefings in Bioinformatics", "journal-ref": "Briefings in Bioinformatics (2016) 17, 745-757", "doi": "10.1093/bib/bbv086", "report-no": null, "categories": "q-bio.GN physics.bio-ph q-bio.BM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nucleosome positioning is an important process required for proper genome\npacking and its accessibility to execute the genetic program in a\ncell-specific, timely manner. In the recent years hundreds of papers have been\ndevoted to the bioinformatics, physics and biology of nucleosome positioning.\nThe purpose of this review is to cover a practical aspect of this field, namely\nto provide a guide to the multitude of nucleosome positioning resources\navailable online. These include almost 300 experimental datasets of genome-wide\nnucleosome occupancy profiles determined in different cell types and more than\n40 computational tools for the analysis of experimental nucleosome positioning\ndata and prediction of intrinsic nucleosome formation probabilities from the\nDNA sequence. A manually curated, up to date list of these resources will be\nmaintained at http://generegulation.info.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 16:09:30 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2015 23:30:38 GMT"}, {"version": "v3", "created": "Wed, 2 Sep 2015 22:01:29 GMT"}, {"version": "v4", "created": "Tue, 8 Sep 2015 19:46:54 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Teif", "Vladimir B.", ""]]}, {"id": "1508.07096", "submitter": "Yanping Huang", "authors": "Yanping Huang, Sai Zhang", "title": "Partitioning Large Scale Deep Belief Networks Using Dropout", "comments": "arXiv admin note: text overlap with arXiv:1207.0580 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods have shown great promise in many practical\napplications, ranging from speech recognition, visual object recognition, to\ntext processing. However, most of the current deep learning methods suffer from\nscalability problems for large-scale applications, forcing researchers or users\nto focus on small-scale problems with fewer parameters.\n  In this paper, we consider a well-known machine learning model, deep belief\nnetworks (DBNs) that have yielded impressive classification performance on a\nlarge number of benchmark machine learning tasks. To scale up DBN, we propose\nan approach that can use the computing clusters in a distributed environment to\ntrain large models, while the dense matrix computations within a single machine\nare sped up using graphics processors (GPU). When training a DBN, each machine\nrandomly drops out a portion of neurons in each hidden layer, for each training\ncase, making the remaining neurons only learn to detect features that are\ngenerally helpful for producing the correct answer. Within our approach, we\nhave developed four methods to combine outcomes from each machine to form a\nunified model. Our preliminary experiment on the mnst handwritten digit\ndatabase demonstrates that our approach outperforms the state of the art test\nerror rate.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2015 05:24:06 GMT"}], "update_date": "2015-08-31", "authors_parsed": [["Huang", "Yanping", ""], ["Zhang", "Sai", ""]]}, {"id": "1508.07103", "submitter": "Songlin Zhao", "authors": "Songlin Zhao", "title": "Regularized Kernel Recursive Least Square Algoirthm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most adaptive signal processing applications, system linearity is assumed\nand adaptive linear filters are thus used. The traditional class of supervised\nadaptive filters rely on error-correction learning for their adaptive\ncapability. The kernel method is a powerful nonparametric modeling tool for\npattern analysis and statistical signal processing. Through a nonlinear\nmapping, kernel methods transform the data into a set of points in a\nReproducing Kernel Hilbert Space. KRLS achieves high accuracy and has fast\nconvergence rate in stationary scenario. However the good performance is\nobtained at a cost of high computation complexity. Sparsification in kernel\nmethods is know to related to less computational complexity and memory\nconsumption.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2015 06:04:37 GMT"}], "update_date": "2015-08-31", "authors_parsed": [["Zhao", "Songlin", ""]]}, {"id": "1508.07192", "submitter": "Niels Landwehr", "authors": "Matthias Bussas, Christoph Sawade, Tobias Scheffer and Niels Landwehr", "title": "Varying-coefficient models with isotropic Gaussian process priors", "comments": "17 pages, 4 Figures, minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study learning problems in which the conditional distribution of the\noutput given the input varies as a function of additional task variables. In\nvarying-coefficient models with Gaussian process priors, a Gaussian process\ngenerates the functional relationship between the task variables and the\nparameters of this conditional. Varying-coefficient models subsume hierarchical\nBayesian multitask models, but also generalizations in which the conditional\nvaries continuously, for instance, in time or space. However, Bayesian\ninference in varying-coefficient models is generally intractable. We show that\ninference for varying-coefficient models with isotropic Gaussian process priors\nresolves to standard inference for a Gaussian process that can be solved\nefficiently. MAP inference in this model resolves to multitask learning using\ntask and instance kernels, and inference for hierarchical Bayesian multitask\nmodels can be carried out efficiently using graph-Laplacian kernels. We report\non experiments for geospatial prediction.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2015 13:13:49 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2015 07:44:56 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Bussas", "Matthias", ""], ["Sawade", "Christoph", ""], ["Scheffer", "Tobias", ""], ["Landwehr", "Niels", ""]]}, {"id": "1508.07384", "submitter": "Guanghui Lan", "authors": "Saeed Ghadimi, Guanghui Lan, Hongchao Zhang", "title": "Generalized Uniformly Optimal Methods for Nonlinear Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a generic framework to extend existing uniformly\noptimal convex programming algorithms to solve more general nonlinear, possibly\nnonconvex, optimization problems. The basic idea is to incorporate a local\nsearch step (gradient descent or Quasi-Newton iteration) into these uniformly\noptimal convex programming methods, and then enforce a monotone decreasing\nproperty of the function values computed along the trajectory. Algorithms of\nthese types will then achieve the best known complexity for nonconvex problems,\nand the optimal complexity for convex ones without requiring any problem\nparameters. As a consequence, we can have a unified treatment for a general\nclass of nonlinear programming problems regardless of their convexity and\nsmoothness level. In particular, we show that the accelerated gradient and\nlevel methods, both originally designed for solving convex optimization\nproblems only, can be used for solving both convex and nonconvex problems\nuniformly. In a similar vein, we show that some well-studied techniques for\nnonlinear programming, e.g., Quasi-Newton iteration, can be embedded into\noptimal convex optimization algorithms to possibly further enhance their\nnumerical performance. Our theoretical and algorithmic developments are\ncomplemented by some promising numerical results obtained for solving a few\nimportant nonconvex and nonlinear data analysis problems in the literature.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2015 01:03:47 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2015 17:45:47 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Ghadimi", "Saeed", ""], ["Lan", "Guanghui", ""], ["Zhang", "Hongchao", ""]]}, {"id": "1508.07535", "submitter": "Albert Thomas", "authors": "Albert Thomas (LTCI), Vincent Feuillard, Alexandre Gramfort (LTCI)", "title": "Calibration of One-Class SVM for MV set estimation", "comments": "IEEE DSAA' 2015, Oct 2015, Paris, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general approach for anomaly detection or novelty detection consists in\nestimating high density regions or Minimum Volume (MV) sets. The One-Class\nSupport Vector Machine (OCSVM) is a state-of-the-art algorithm for estimating\nsuch regions from high dimensional data. Yet it suffers from practical\nlimitations. When applied to a limited number of samples it can lead to poor\nperformance even when picking the best hyperparameters. Moreover the solution\nof OCSVM is very sensitive to the selection of hyperparameters which makes it\nhard to optimize in an unsupervised setting. We present a new approach to\nestimate MV sets using the OCSVM with a different choice of the parameter\ncontrolling the proportion of outliers. The solution function of the OCSVM is\nlearnt on a training set and the desired probability mass is obtained by\nadjusting the offset on a test set to prevent overfitting. Models learnt on\ndifferent train/test splits are then aggregated to reduce the variance induced\nby such random splits. Our approach makes it possible to tune the\nhyperparameters automatically and obtain nested set estimates. Experimental\nresults show that our approach outperforms the standard OCSVM formulation while\nsuffering less from the curse of dimensionality than kernel density estimates.\nResults on actual data sets are also presented.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2015 06:51:31 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Thomas", "Albert", "", "LTCI"], ["Feuillard", "Vincent", "", "LTCI"], ["Gramfort", "Alexandre", "", "LTCI"]]}, {"id": "1508.07630", "submitter": "Vural Aksakalli", "authors": "Vural Aksakalli and Milad Malekipirbazari", "title": "Feature Selection via Binary Simultaneous Perturbation Stochastic\n  Approximation", "comments": "This is the Istanbul Sehir University Technical Report\n  #SHR-ISE-2016.01. A short version of this report has been accepted for\n  publication at Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": "SHR-ISE-2016.01", "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection (FS) has become an indispensable task in dealing with\ntoday's highly complex pattern recognition problems with massive number of\nfeatures. In this study, we propose a new wrapper approach for FS based on\nbinary simultaneous perturbation stochastic approximation (BSPSA). This\npseudo-gradient descent stochastic algorithm starts with an initial feature\nvector and moves toward the optimal feature vector via successive iterations.\nIn each iteration, the current feature vector's individual components are\nperturbed simultaneously by random offsets from a qualified probability\ndistribution. We present computational experiments on datasets with numbers of\nfeatures ranging from a few dozens to thousands using three widely-used\nclassifiers as wrappers: nearest neighbor, decision tree, and linear support\nvector machine. We compare our methodology against the full set of features as\nwell as a binary genetic algorithm and sequential FS methods using\ncross-validated classification error rate and AUC as the performance criteria.\nOur results indicate that features selected by BSPSA compare favorably to\nalternative methods in general and BSPSA can yield superior feature sets for\ndatasets with tens of thousands of features by examining an extremely small\nfraction of the solution space. We are not aware of any other wrapper FS\nmethods that are computationally feasible with good convergence properties for\nsuch large datasets.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2015 20:03:53 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2016 08:02:42 GMT"}, {"version": "v3", "created": "Sat, 5 Mar 2016 19:42:14 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Aksakalli", "Vural", ""], ["Malekipirbazari", "Milad", ""]]}, {"id": "1508.07643", "submitter": "Marc Goessling", "authors": "Marc Goessling and Shan Kang", "title": "Directional Decision Lists", "comments": "IEEE Big Data for Advanced Manufacturing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a novel family of decision lists consisting of\nhighly interpretable models which can be learned efficiently in a greedy\nmanner. The defining property is that all rules are oriented in the same\ndirection. Particular examples of this family are decision lists with\nmonotonically decreasing (or increasing) probabilities. On simulated data we\nempirically confirm that the proposed model family is easier to train than\ngeneral decision lists. We exemplify the practical usability of our approach by\nidentifying problem symptoms in a manufacturing process.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2015 23:00:35 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2015 16:41:16 GMT"}, {"version": "v3", "created": "Sun, 10 Jan 2016 19:38:53 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Goessling", "Marc", ""], ["Kang", "Shan", ""]]}, {"id": "1508.07648", "submitter": "Mehdi Korki", "authors": "Hadi Zayyani, Mehdi Korki, and Farrokh Marvasti", "title": "Dictionary Learning for Blind One Bit Compressed Sensing", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": "10.1109/LSP.2015.2503804", "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter proposes a dictionary learning algorithm for blind one bit\ncompressed sensing. In the blind one bit compressed sensing framework, the\noriginal signal to be reconstructed from one bit linear random measurements is\nsparse in an unknown domain. In this context, the multiplication of measurement\nmatrix $\\Ab$ and sparse domain matrix $\\Phi$, \\ie $\\Db=\\Ab\\Phi$, should be\nlearned. Hence, we use dictionary learning to train this matrix. Towards that\nend, an appropriate continuous convex cost function is suggested for one bit\ncompressed sensing and a simple steepest-descent method is exploited to learn\nthe rows of the matrix $\\Db$. Experimental results show the effectiveness of\nthe proposed algorithm against the case of no dictionary learning, specially\nwith increasing the number of training signals and the number of sign\nmeasurements.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2015 23:48:52 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Zayyani", "Hadi", ""], ["Korki", "Mehdi", ""], ["Marvasti", "Farrokh", ""]]}, {"id": "1508.07678", "submitter": "Shahriar Shariat", "authors": "Shahriar Shariat, Burkay Orten, Ali Dasdan", "title": "Online Model Evaluation in a Large-Scale Computational Advertising\n  Platform", "comments": "Accepted to ICDM2015", "journal-ref": "ICDM (2015) pp. 369 - 378", "doi": "10.1109/ICDM.2015.32", "report-no": null, "categories": "cs.AI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online media provides opportunities for marketers through which they can\ndeliver effective brand messages to a wide range of audiences. Advertising\ntechnology platforms enable advertisers to reach their target audience by\ndelivering ad impressions to online users in real time. In order to identify\nthe best marketing message for a user and to purchase impressions at the right\nprice, we rely heavily on bid prediction and optimization models. Even though\nthe bid prediction models are well studied in the literature, the equally\nimportant subject of model evaluation is usually overlooked. Effective and\nreliable evaluation of an online bidding model is crucial for making faster\nmodel improvements as well as for utilizing the marketing budgets more\nefficiently. In this paper, we present an experimentation framework for bid\nprediction models where our focus is on the practical aspects of model\nevaluation. Specifically, we outline the unique challenges we encounter in our\nplatform due to a variety of factors such as heterogeneous goal definitions,\nvarying budget requirements across different campaigns, high seasonality and\nthe auction-based environment for inventory purchasing. Then, we introduce\nreturn on investment (ROI) as a unified model performance (i.e., success)\nmetric and explain its merits over more traditional metrics such as\nclick-through rate (CTR) or conversion rate (CVR). Most importantly, we discuss\ncommonly used evaluation and metric summarization approaches in detail and\npropose a more accurate method for online evaluation of new experimental models\nagainst the baseline. Our meta-analysis-based approach addresses various\nshortcomings of other methods and yields statistically robust conclusions that\nallow us to conclude experiments more quickly in a reliable manner. We\ndemonstrate the effectiveness of our evaluation strategy on real campaign data\nthrough some experiments.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 04:11:50 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Shariat", "Shahriar", ""], ["Orten", "Burkay", ""], ["Dasdan", "Ali", ""]]}, {"id": "1508.07680", "submitter": "Muhammad Ghifary", "authors": "Muhammad Ghifary and W. Bastiaan Kleijn and Mengjie Zhang and David\n  Balduzzi", "title": "Domain Generalization for Object Recognition with Multi-task\n  Autoencoders", "comments": "accepted in ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of domain generalization is to take knowledge acquired from a\nnumber of related domains where training data is available, and to then\nsuccessfully apply it to previously unseen domains. We propose a new feature\nlearning algorithm, Multi-Task Autoencoder (MTAE), that provides good\ngeneralization performance for cross-domain object recognition.\n  Our algorithm extends the standard denoising autoencoder framework by\nsubstituting artificially induced corruption with naturally occurring\ninter-domain variability in the appearance of objects. Instead of\nreconstructing images from noisy versions, MTAE learns to transform the\noriginal image into analogs in multiple related domains. It thereby learns\nfeatures that are robust to variations across domains. The learnt features are\nthen used as inputs to a classifier.\n  We evaluated the performance of the algorithm on benchmark image recognition\ndatasets, where the task is to learn features from multiple datasets and to\nthen predict the image label from unseen datasets. We found that (denoising)\nMTAE outperforms alternative autoencoder-based models as well as the current\nstate-of-the-art algorithms for domain generalization.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 04:15:31 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Ghifary", "Muhammad", ""], ["Kleijn", "W. Bastiaan", ""], ["Zhang", "Mengjie", ""], ["Balduzzi", "David", ""]]}, {"id": "1508.07709", "submitter": "Simon \\v{S}uster", "authors": "Simon \\v{S}uster and Gertjan van Noord and Ivan Titov", "title": "Word Representations, Tree Models and Syntactic Functions", "comments": "Add github code repository link. Fix equation 4.1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word representations induced from models with discrete latent variables\n(e.g.\\ HMMs) have been shown to be beneficial in many NLP applications. In this\nwork, we exploit labeled syntactic dependency trees and formalize the induction\nproblem as unsupervised learning of tree-structured hidden Markov models.\nSyntactic functions are used as additional observed variables in the model,\ninfluencing both transition and emission components. Such syntactic information\ncan potentially lead to capturing more fine-grain and functional distinctions\nbetween words, which, in turn, may be desirable in many NLP applications. We\nevaluate the word representations on two tasks -- named entity recognition and\nsemantic frame identification. We observe improvements from exploiting\nsyntactic function information in both cases, and the results rivaling those of\nstate-of-the-art representation learning methods. Additionally, we revisit the\nrelationship between sequential and unlabeled-tree models and find that the\nadvantage of the latter is not self-evident.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 07:52:50 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2016 13:26:56 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["\u0160uster", "Simon", ""], ["van Noord", "Gertjan", ""], ["Titov", "Ivan", ""]]}, {"id": "1508.07741", "submitter": "Luk\\'a\\v{s} Bajer", "authors": "Lukas Bajer and Martin Holena", "title": "Model Guided Sampling Optimization for Low-dimensional Problems", "comments": null, "journal-ref": "Bajer, L. & Holena, M. Model Guided Sampling Optimization for\n  Low-dimensional Problems. in ICAART 2015 Proceedings of the International\n  Conference on Agents and Artificial Intelligence, Volume 2 451-456\n  (SCITEPRESS, Lisbon, 2015)", "doi": null, "report-no": null, "categories": "cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization of very expensive black-box functions requires utilization of\nmaximum information gathered by the process of optimization. Model Guided\nSampling Optimization (MGSO) forms a more robust alternative to Jones'\nGaussian-process-based EGO algorithm. Instead of EGO's maximizing expected\nimprovement, the MGSO uses sampling the probability of improvement which is\nshown to be helpful against trapping in local minima. Further, the MGSO can\nreach close-to-optimum solutions faster than standard optimization algorithms\non low dimensional or smooth problems.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 09:42:33 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Bajer", "Lukas", ""], ["Holena", "Martin", ""]]}, {"id": "1508.07744", "submitter": "Gilles Louppe", "authors": "Gilles Louppe, Hussein Al-Natsheh, Mateusz Susik, Eamonn Maguire", "title": "Ethnicity sensitive author disambiguation using semi-supervised learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Author name disambiguation in bibliographic databases is the problem of\ngrouping together scientific publications written by the same person,\naccounting for potential homonyms and/or synonyms. Among solutions to this\nproblem, digital libraries are increasingly offering tools for authors to\nmanually curate their publications and claim those that are theirs. Indirectly,\nthese tools allow for the inexpensive collection of large annotated training\ndata, which can be further leveraged to build a complementary automated\ndisambiguation system capable of inferring patterns for identifying\npublications written by the same person. Building on more than 1 million\npublicly released crowdsourced annotations, we propose an automated author\ndisambiguation solution exploiting this data (i) to learn an accurate\nclassifier for identifying coreferring authors and (ii) to guide the clustering\nof scientific publications by distinct authors in a semi-supervised way. To the\nbest of our knowledge, our analysis is the first to be carried out on data of\nthis size and coverage. With respect to the state of the art, we validate the\ngeneral pipeline used in most existing solutions, and improve by: (i) proposing\nphonetic-based blocking strategies, thereby increasing recall; and (ii) adding\nstrong ethnicity-sensitive features for learning a linkage function, thereby\ntailoring disambiguation to non-Western author names whenever necessary.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 09:49:27 GMT"}, {"version": "v2", "created": "Wed, 4 May 2016 06:35:55 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Louppe", "Gilles", ""], ["Al-Natsheh", "Hussein", ""], ["Susik", "Mateusz", ""], ["Maguire", "Eamonn", ""]]}, {"id": "1508.07753", "submitter": "Pekka Parviainen", "authors": "Pekka Parviainen and Samuel Kaski", "title": "Learning Structures of Bayesian Networks for Variable Groups", "comments": "To appear at the International Journal of Approximate Reasoning. A\n  preliminary version appeared in Proceedings of the Eighth International\n  Conference on Probabilistic Graphical Models", "journal-ref": null, "doi": "10.1016/j.ijar.2017.05.006", "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian networks, and especially their structures, are powerful tools for\nrepresenting conditional independencies and dependencies between random\nvariables. In applications where related variables form a priori known groups,\nchosen to represent different \"views\" to or aspects of the same entities, one\nmay be more interested in modeling dependencies between groups of variables\nrather than between individual variables. Motivated by this, we study prospects\nof representing relationships between variable groups using Bayesian network\nstructures. We show that for dependency structures between groups to be\nexpressible exactly, the data have to satisfy the so-called groupwise\nfaithfulness assumption. We also show that one cannot learn causal relations\nbetween groups using only groupwise conditional independencies, but also\nvariable-wise relations are needed. Additionally, we present algorithms for\nfinding the groupwise dependency structures.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 10:19:41 GMT"}, {"version": "v2", "created": "Fri, 19 Aug 2016 12:59:49 GMT"}, {"version": "v3", "created": "Thu, 1 Jun 2017 11:36:43 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Parviainen", "Pekka", ""], ["Kaski", "Samuel", ""]]}, {"id": "1508.07964", "submitter": "Diyan Teng", "authors": "Diyan Teng and Emre Ertin", "title": "Wald-Kernel: Learning to Aggregate Information for Sequential Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential hypothesis testing is a desirable decision making strategy in any\ntime sensitive scenario. Compared with fixed sample-size testing, sequential\ntesting is capable of achieving identical probability of error requirements\nusing less samples in average. For a binary detection problem, it is well known\nthat for known density functions accumulating the likelihood ratio statistics\nis time optimal under a fixed error rate constraint. This paper considers the\nproblem of learning a binary sequential detector from training samples when\ndensity functions are unavailable. We formulate the problem as a constrained\nlikelihood ratio estimation which can be solved efficiently through convex\noptimization by imposing Reproducing Kernel Hilbert Space (RKHS) structure on\nthe log-likelihood ratio function. In addition, we provide a computationally\nefficient approximated solution for large scale data set. The proposed\nalgorithm, namely Wald-Kernel, is tested on a synthetic data set and two real\nworld data sets, together with previous approaches for likelihood ratio\nestimation. Our empirical results show that the classifier trained through the\nproposed technique achieves smaller average sampling cost than previous\napproaches proposed in the literature for the same error rate.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 19:06:52 GMT"}, {"version": "v2", "created": "Tue, 3 May 2016 11:39:23 GMT"}, {"version": "v3", "created": "Thu, 16 Nov 2017 02:03:54 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Teng", "Diyan", ""], ["Ertin", "Emre", ""]]}]