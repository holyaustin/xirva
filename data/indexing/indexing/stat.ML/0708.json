[{"id": "0708.1838", "submitter": "Ingo Steinwart", "authors": "Ingo Steinwart, Clint Scovel", "title": "Fast rates for support vector machines using Gaussian kernels", "comments": "Published at http://dx.doi.org/10.1214/009053606000001226 in the\n  Annals of Statistics (http://www.imstat.org/aos/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2007, Vol. 35, No. 2, 575-607", "doi": "10.1214/009053606000001226", "report-no": "IMS-AOS-AOS0237", "categories": "math.ST stat.ML stat.TH", "license": null, "abstract": "  For binary classification we establish learning rates up to the order of\n$n^{-1}$ for support vector machines (SVMs) with hinge loss and Gaussian RBF\nkernels. These rates are in terms of two assumptions on the considered\ndistributions: Tsybakov's noise assumption to establish a small estimation\nerror, and a new geometric noise condition which is used to bound the\napproximation error. Unlike previously proposed concepts for bounding the\napproximation error, the geometric noise assumption does not employ any\nsmoothness assumption.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2007 09:04:15 GMT"}], "update_date": "2007-08-22", "authors_parsed": [["Steinwart", "Ingo", ""], ["Scovel", "Clint", ""]]}, {"id": "0708.2197", "submitter": "Ji Zhu", "authors": "Saharon Rosset, Ji Zhu", "title": "Piecewise linear regularized solution paths", "comments": "Published at http://dx.doi.org/10.1214/009053606000001370 in the\n  Annals of Statistics (http://www.imstat.org/aos/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2007, Vol. 35, No. 3, 1012-1030", "doi": "10.1214/009053606000001370", "report-no": "IMS-AOS-AOS0192", "categories": "math.ST stat.ML stat.TH", "license": null, "abstract": "  We consider the generic regularized optimization problem\n$\\hat{\\mathsf{\\beta}}(\\lambda)=\\arg\n\\min_{\\beta}L({\\sf{y}},X{\\sf{\\beta}})+\\lambda J({\\sf{\\beta}})$. Efron, Hastie,\nJohnstone and Tibshirani [Ann. Statist. 32 (2004) 407--499] have shown that for\nthe LASSO--that is, if $L$ is squared error loss and $J(\\beta)=\\|\\beta\\|_1$ is\nthe $\\ell_1$ norm of $\\beta$--the optimal coefficient path is piecewise linear,\nthat is, $\\partial \\hat{\\beta}(\\lambda)/\\partial \\lambda$ is piecewise\nconstant. We derive a general characterization of the properties of (loss $L$,\npenalty $J$) pairs which give piecewise linear coefficient paths. Such pairs\nallow for efficient generation of the full regularized coefficient paths. We\ninvestigate the nature of efficient path following algorithms which arise. We\nuse our results to suggest robust versions of the LASSO for regression and\nclassification, and to develop new, efficient algorithms for existing problems\nin the literature, including Mammen and van de Geer's locally adaptive\nregression splines.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2007 13:23:47 GMT"}], "update_date": "2007-08-22", "authors_parsed": [["Rosset", "Saharon", ""], ["Zhu", "Ji", ""]]}, {"id": "0708.2377", "submitter": "Roberto Alamino", "authors": "Roberto C. Alamino, Nestor Caticha", "title": "Online Learning in Discrete Hidden Markov Models", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": "10.1063/1.2423274", "report-no": null, "categories": "stat.ML", "license": null, "abstract": "  We present and analyse three online algorithms for learning in discrete\nHidden Markov Models (HMMs) and compare them with the Baldi-Chauvin Algorithm.\nUsing the Kullback-Leibler divergence as a measure of generalisation error we\ndraw learning curves in simplified situations. The performance for learning\ndrifting concepts of one of the presented algorithms is analysed and compared\nwith the Baldi-Chauvin algorithm in the same situations. A brief discussion\nabout learning and symmetry breaking based on our results is also presented.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2007 14:41:58 GMT"}], "update_date": "2007-08-20", "authors_parsed": [["Alamino", "Roberto C.", ""], ["Caticha", "Nestor", ""]]}]