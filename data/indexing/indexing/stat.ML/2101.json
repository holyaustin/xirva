[{"id": "2101.00009", "submitter": "Vasilis Syrgkanis", "authors": "Victor Chernozhukov, Whitney Newey, Rahul Singh, Vasilis Syrgkanis", "title": "Adversarial Estimation of Riesz Representers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide an adversarial approach to estimating Riesz representers of linear\nfunctionals within arbitrary function spaces. We prove oracle inequalities\nbased on the localized Rademacher complexity of the function space used to\napproximate the Riesz representer and the approximation error. These\ninequalities imply fast finite sample mean-squared-error rates for many\nfunction spaces of interest, such as high-dimensional sparse linear functions,\nneural networks and reproducing kernel Hilbert spaces. Our approach offers a\nnew way of estimating Riesz representers with a plethora of recently introduced\nmachine learning techniques. We show how our estimator can be used in the\ncontext of de-biasing structural/causal parameters in semi-parametric models,\nfor automated orthogonalization of moment equations and for estimating the\nstochastic discount factor in the context of asset pricing.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 19:46:57 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Newey", "Whitney", ""], ["Singh", "Rahul", ""], ["Syrgkanis", "Vasilis", ""]]}, {"id": "2101.00029", "submitter": "Maciej Skorski", "authors": "Maciej Skorski", "title": "Random Embeddings with Optimal Accuracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work constructs Jonson-Lindenstrauss embeddings with best accuracy, as\nmeasured by variance, mean-squared error and exponential concentration of the\nlength distortion. Lower bounds for any data and embedding dimensions are\ndetermined, and accompanied by matching and efficiently samplable constructions\n(built on orthogonal matrices). Novel techniques: a unit sphere\nparametrization, the use of singular-value latent variables and Schur-convexity\nare of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 19:00:31 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Skorski", "Maciej", ""]]}, {"id": "2101.00072", "submitter": "Qianli Liao", "authors": "Tomaso Poggio and Qianli Liao", "title": "Explicit regularization and implicit bias in deep network classifiers\n  trained with the square loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep ReLU networks trained with the square loss have been observed to perform\nwell in classification tasks. We provide here a theoretical justification based\non analysis of the associated gradient flow. We show that convergence to a\nsolution with the absolute minimum norm is expected when normalization\ntechniques such as Batch Normalization (BN) or Weight Normalization (WN) are\nused together with Weight Decay (WD). The main property of the minimizers that\nbounds their expected error is the norm: we prove that among all the\nclose-to-interpolating solutions, the ones associated with smaller Frobenius\nnorms of the unnormalized weight matrices have better margin and better bounds\non the expected classification error. With BN but in the absence of WD, the\ndynamical system is singular. Implicit dynamical regularization -- that is\nzero-initial conditions biasing the dynamics towards high margin solutions --\nis also possible in the no-BN and no-WD case. The theory yields several\npredictions, including the role of BN and weight decay, aspects of Papyan, Han\nand Donoho's Neural Collapse and the constraints induced by BN on the network\nweights.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 21:07:56 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Poggio", "Tomaso", ""], ["Liao", "Qianli", ""]]}, {"id": "2101.00079", "submitter": "Kimberly Stachenfeld", "authors": "Kimberly Stachenfeld, Jonathan Godwin, Peter Battaglia", "title": "Graph Networks with Spectral Message Passing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph Neural Networks (GNNs) are the subject of intense focus by the machine\nlearning community for problems involving relational reasoning. GNNs can be\nbroadly divided into spatial and spectral approaches. Spatial approaches use a\nform of learned message-passing, in which interactions among vertices are\ncomputed locally, and information propagates over longer distances on the graph\nwith greater numbers of message-passing steps. Spectral approaches use\neigendecompositions of the graph Laplacian to produce a generalization of\nspatial convolutions to graph structured data which access information over\nshort and long time scales simultaneously. Here we introduce the Spectral Graph\nNetwork, which applies message passing to both the spatial and spectral\ndomains. Our model projects vertices of the spatial graph onto the Laplacian\neigenvectors, which are each represented as vertices in a fully connected\n\"spectral graph\", and then applies learned message passing to them. We apply\nthis model to various benchmark tasks including a graph-based variant of MNIST\nclassification, molecular property prediction on MoleculeNet and QM9, and\nshortest path problems on random graphs. Our results show that the Spectral GN\npromotes efficient training, reaching high performance with fewer training\niterations despite having more parameters. The model also provides robustness\nto edge dropout and outperforms baselines for the classification tasks. We also\nexplore how these performance benefits depend on properties of the dataset.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 21:33:17 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Stachenfeld", "Kimberly", ""], ["Godwin", "Jonathan", ""], ["Battaglia", "Peter", ""]]}, {"id": "2101.00157", "submitter": "Jing Lin", "authors": "Jing Lin, Ryan Luley, and Kaiqi Xiong", "title": "Active Learning Under Malicious Mislabeling and Poisoning Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep neural networks usually require large labeled datasets for training to\nachieve the start-of-the-art performance in many tasks, such as image\nclassification and natural language processing. Though a lot of data is created\neach day by active Internet users through various distributed systems across\nthe world, most of these data are unlabeled and are vulnerable to data\npoisoning attacks. In this paper, we develop an efficient active learning\nmethod that requires fewer labeled instances and incorporates the technique of\nadversarial retraining in which additional labeled artificial data are\ngenerated without increasing the labeling budget. The generated adversarial\nexamples also provide a way to measure the vulnerability of the model. To check\nthe performance of the proposed method under an adversarial setting, i.e.,\nmalicious mislabeling and data poisoning attacks, we perform an extensive\nevaluation on the reduced CIFAR-10 dataset, which contains only two classes:\n'airplane' and 'frog' by using the private cloud on campus. Our experimental\nresults demonstrate that the proposed active learning method is efficient for\ndefending against malicious mislabeling and data poisoning attacks.\nSpecifically, whereas the baseline active learning method based on the random\nsampling strategy performs poorly (about 50%) under a malicious mislabeling\nattack, the proposed active learning method can achieve the desired accuracy of\n89% using only one-third of the dataset on average.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 03:43:36 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 01:07:29 GMT"}, {"version": "v3", "created": "Sun, 16 May 2021 20:06:13 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Lin", "Jing", ""], ["Luley", "Ryan", ""], ["Xiong", "Kaiqi", ""]]}, {"id": "2101.00218", "submitter": "Yingshi Chen", "authors": "Yingshi Chen", "title": "An iterative K-FAC algorithm for Deep Learning", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Kronecker-factored Approximate Curvature (K-FAC) method is a high efficiency\nsecond order optimizer for the deep learning. Its training time is less than\nSGD(or other first-order method) with same accuracy in many large-scale\nproblems. The key of K-FAC is to approximates Fisher information matrix (FIM)\nas a block-diagonal matrix where each block is an inverse of tiny Kronecker\nfactors. In this short note, we present CG-FAC -- an new iterative K-FAC\nalgorithm. It uses conjugate gradient method to approximate the nature\ngradient. This CG-FAC method is matrix-free, that is, no need to generate the\nFIM matrix, also no need to generate the Kronecker factors A and G. We prove\nthat the time and memory complexity of iterative CG-FAC is much less than that\nof standard K-FAC algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 12:04:01 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Chen", "Yingshi", ""]]}, {"id": "2101.00245", "submitter": "Er-Dong Guo", "authors": "Erdong Guo and David Draper", "title": "The Bayesian Method of Tensor Networks", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian learning is a powerful learning framework which combines the\nexternal information of the data (background information) with the internal\ninformation (training data) in a logically consistent way in inference and\nprediction. By Bayes rule, the external information (prior distribution) and\nthe internal information (training data likelihood) are combined coherently,\nand the posterior distribution and the posterior predictive (marginal)\ndistribution obtained by Bayes rule summarize the total information needed in\nthe inference and prediction, respectively. In this paper, we study the\nBayesian framework of the Tensor Network from two perspective. First, we\nintroduce the prior distribution to the weights in the Tensor Network and\npredict the labels of the new observations by the posterior predictive\n(marginal) distribution. Since the intractability of the parameter integral in\nthe normalization constant computation, we approximate the posterior predictive\ndistribution by Laplace approximation and obtain the out-product approximation\nof the hessian matrix of the posterior distribution of the Tensor Network\nmodel. Second, to estimate the parameters of the stationary mode, we propose a\nstable initialization trick to accelerate the inference process by which the\nTensor Network can converge to the stationary path more efficiently and stably\nwith gradient descent method. We verify our work on the MNIST, Phishing Website\nand Breast Cancer data set. We study the Bayesian properties of the Bayesian\nTensor Network by visualizing the parameters of the model and the decision\nboundaries in the two dimensional synthetic data set. For a application\npurpose, our work can reduce the overfitting and improve the performance of\nnormal Tensor Network model.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 14:59:15 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Guo", "Erdong", ""], ["Draper", "David", ""]]}, {"id": "2101.00300", "submitter": "Dhruv Malik", "authors": "Dhruv Malik, Yuanzhi Li, Pradeep Ravikumar", "title": "When Is Generalizable Reinforcement Learning Tractable?", "comments": "v2 extends results to function approximation setting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agents trained by reinforcement learning (RL) often fail to generalize beyond\nthe environment they were trained in, even when presented with new scenarios\nthat seem similar to the training environment. We study the query complexity\nrequired to train RL agents that generalize to multiple environments.\nIntuitively, tractable generalization is only possible when the environments\nare similar or close in some sense. To capture this, we introduce Weak\nProximity, a natural structural condition that requires the environments to\nhave highly similar transition and reward functions and share a policy\nproviding optimal value. Despite such shared structure, we prove that tractable\ngeneralization is impossible in the worst case. This holds even when each\nindividual environment can be efficiently solved to obtain an optimal linear\npolicy, and when the agent possesses a generative model. Our lower bound\napplies to the more complex task of representation learning for the purpose of\nefficient generalization to multiple environments. On the positive side, we\nintroduce Strong Proximity, a strengthened condition which we prove is\nsufficient for efficient generalization.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 19:08:24 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 01:35:52 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Malik", "Dhruv", ""], ["Li", "Yuanzhi", ""], ["Ravikumar", "Pradeep", ""]]}, {"id": "2101.00323", "submitter": "Chengrun Yang", "authors": "Chengrun Yang, Lijun Ding, Ziyang Wu, Madeleine Udell", "title": "TenIPS: Inverse Propensity Sampling for Tensor Completion", "comments": "AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Tensors are widely used to represent multiway arrays of data. The recovery of\nmissing entries in a tensor has been extensively studied, generally under the\nassumption that entries are missing completely at random (MCAR). However, in\nmost practical settings, observations are missing not at random (MNAR): the\nprobability that a given entry is observed (also called the propensity) may\ndepend on other entries in the tensor or even on the value of the missing\nentry. In this paper, we study the problem of completing a partially observed\ntensor with MNAR observations, without prior information about the\npropensities. To complete the tensor, we assume that both the original tensor\nand the tensor of propensities have low multilinear rank. The algorithm first\nestimates the propensities using a convex relaxation and then predicts missing\nvalues using a higher-order SVD approach, reweighting the observed tensor by\nthe inverse propensities. We provide finite-sample error bounds on the\nresulting complete tensor. Numerical experiments demonstrate the effectiveness\nof our approach.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 22:13:19 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 23:34:26 GMT"}, {"version": "v3", "created": "Thu, 22 Apr 2021 15:41:38 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Yang", "Chengrun", ""], ["Ding", "Lijun", ""], ["Wu", "Ziyang", ""], ["Udell", "Madeleine", ""]]}, {"id": "2101.00336", "submitter": "Hanxun Huang", "authors": "Hanxun Huang, Xingjun Ma, Sarah M. Erfani, James Bailey", "title": "Neural Architecture Search via Combinatorial Multi-Armed Bandit", "comments": "10 pages, 7 figures", "journal-ref": "International Joint Conference on Neural Networks (IJCNN) 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Neural Architecture Search (NAS) has gained significant popularity as an\neffective tool for designing high performance deep neural networks (DNNs). NAS\ncan be performed via policy gradient, evolutionary algorithms, differentiable\narchitecture search or tree-search methods. While significant progress has been\nmade for both policy gradient and differentiable architecture search,\ntree-search methods have so far failed to achieve comparable accuracy or search\nefficiency. In this paper, we formulate NAS as a Combinatorial Multi-Armed\nBandit (CMAB) problem (CMAB-NAS). This allows the decomposition of a large\nsearch space into smaller blocks where tree-search methods can be applied more\neffectively and efficiently. We further leverage a tree-based method called\nNested Monte-Carlo Search to tackle the CMAB-NAS problem. On CIFAR-10, our\napproach discovers a cell structure that achieves a low error rate that is\ncomparable to the state-of-the-art, using only 0.58 GPU days, which is 20 times\nfaster than current tree-search methods. Moreover, the discovered structure\ntransfers well to large-scale datasets such as ImageNet.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 23:29:33 GMT"}, {"version": "v2", "created": "Sat, 24 Apr 2021 14:13:15 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Huang", "Hanxun", ""], ["Ma", "Xingjun", ""], ["Erfani", "Sarah M.", ""], ["Bailey", "James", ""]]}, {"id": "2101.00352", "submitter": "Amanda Coston", "authors": "Amanda Coston and Ashesh Rambachan and Alexandra Chouldechova", "title": "Characterizing Fairness Over the Set of Good Models Under Selective\n  Labels", "comments": "Added comparison methods to the empirical lending analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic risk assessments are used to inform decisions in a wide variety\nof high-stakes settings. Often multiple predictive models deliver similar\noverall performance but differ markedly in their predictions for individual\ncases, an empirical phenomenon known as the \"Rashomon Effect.\" These models may\nhave different properties over various groups, and therefore have different\npredictive fairness properties. We develop a framework for characterizing\npredictive fairness properties over the set of models that deliver similar\noverall performance, or \"the set of good models.\" Our framework addresses the\nempirically relevant challenge of selectively labelled data in the setting\nwhere the selection decision and outcome are unconfounded given the observed\ndata features. Our framework can be used to 1) replace an existing model with\none that has better fairness properties; or 2) audit for predictive bias. We\nillustrate these uses cases on a real-world credit-scoring task and a\nrecidivism prediction task.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 02:11:37 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 00:25:36 GMT"}, {"version": "v3", "created": "Sat, 1 May 2021 00:21:59 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Coston", "Amanda", ""], ["Rambachan", "Ashesh", ""], ["Chouldechova", "Alexandra", ""]]}, {"id": "2101.00354", "submitter": "Mehmet Kolcu", "authors": "Mehmet Kolcu, Alper E. Murat", "title": "Integrated Optimization of Predictive and Prescriptive Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In traditional machine learning techniques, the degree of closeness between\ntrue and predicted values generally measures the quality of predictions.\nHowever, these learning algorithms do not consider prescription problems where\nthe predicted values will be used as input to decision problems. In this paper,\nwe efficiently leverage feature variables, and we propose a new framework\ndirectly integrating predictive tasks under prescriptive tasks in order to\nprescribe consistent decisions. We train the parameters of predictive algorithm\nwithin a prescription problem via bilevel optimization techniques. We present\nthe structure of our method and demonstrate its performance using synthetic\ndata compared to classical methods like point-estimate-based, stochastic\noptimization and recently developed machine learning based optimization\nmethods. In addition, we control generalization error using different penalty\napproaches and optimize the integration over validation data set.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 02:43:10 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Kolcu", "Mehmet", ""], ["Murat", "Alper E.", ""]]}, {"id": "2101.00362", "submitter": "Xi Yang", "authors": "Xi Yang, Jan Hannig, J.S. Marron", "title": "Visual High Dimensional Hypothesis Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In exploratory data analysis of known classes of high dimensional data, a\ncentral question is how distinct are the classes? The Direction Projection\nPermutation (DiProPerm) hypothesis test provides an answer to this that is\ndirectly connected to a visual analysis of the data. In this paper, we propose\nan improved DiProPerm test that solves 3 major challenges of the original\nversion. First, we implement only balanced permutations to increase the test\npower for data with strong signals. Second, our mathematical analysis leads to\nan adjustment to correct the null behavior of both balanced and the\nconventional all permutations. Third, new confidence intervals (reflecting\npermutation variation) for test significance are also proposed for comparison\nof results across different contexts. This improvement of DiProPerm inference\nis illustrated in the context of comparing cancer types in examples from The\nCancer Genome Atlas.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 03:34:16 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Yang", "Xi", ""], ["Hannig", "Jan", ""], ["Marron", "J. S.", ""]]}, {"id": "2101.00400", "submitter": "Paul Liu", "authors": "Aram Ebtekar and Paul Liu", "title": "An Elo-like System for Massive Multiplayer Competitions", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.GT cs.LG cs.SI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rating systems play an important role in competitive sports and games. They\nprovide a measure of player skill, which incentivizes competitive performances\nand enables balanced match-ups. In this paper, we present a novel Bayesian\nrating system for contests with many participants. It is widely applicable to\ncompetition formats with discrete ranked matches, such as online programming\ncompetitions, obstacle courses races, and some video games. The simplicity of\nour system allows us to prove theoretical bounds on robustness and runtime. In\naddition, we show that the system aligns incentives: that is, a player who\nseeks to maximize their rating will never want to underperform. Experimentally,\nthe rating system rivals or surpasses existing systems in prediction accuracy,\nand computes faster than existing systems by up to an order of magnitude.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 08:14:31 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Ebtekar", "Aram", ""], ["Liu", "Paul", ""]]}, {"id": "2101.00407", "submitter": "Liyuan Wang", "authors": "Liyuan Wang, Kuo Yang, Chongxuan Li, Lanqing Hong, Zhenguo Li, Jun Zhu", "title": "ORDisCo: Effective and Efficient Usage of Incremental Unlabeled Data for\n  Semi-supervised Continual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning usually assumes the incoming data are fully labeled, which\nmight not be applicable in real applications. In this work, we consider\nsemi-supervised continual learning (SSCL) that incrementally learns from\npartially labeled data. Observing that existing continual learning methods lack\nthe ability to continually exploit the unlabeled data, we propose deep Online\nReplay with Discriminator Consistency (ORDisCo) to interdependently learn a\nclassifier with a conditional generative adversarial network (GAN), which\ncontinually passes the learned data distribution to the classifier. In\nparticular, ORDisCo replays data sampled from the conditional generator to the\nclassifier in an online manner, exploiting unlabeled data in a time- and\nstorage-efficient way. Further, to explicitly overcome the catastrophic\nforgetting of unlabeled data, we selectively stabilize parameters of the\ndiscriminator that are important for discriminating the pairs of old unlabeled\ndata and their pseudo-labels predicted by the classifier. We extensively\nevaluate ORDisCo on various semi-supervised learning benchmark datasets for\nSSCL, and show that ORDisCo achieves significant performance improvement on\nSVHN, CIFAR10 and Tiny-ImageNet, compared to strong baselines.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 09:04:14 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 01:57:03 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Wang", "Liyuan", ""], ["Yang", "Kuo", ""], ["Li", "Chongxuan", ""], ["Hong", "Lanqing", ""], ["Li", "Zhenguo", ""], ["Zhu", "Jun", ""]]}, {"id": "2101.00494", "submitter": "Lin Yang", "authors": "Minbo Gao, Tianle Xie, Simon S. Du, Lin F. Yang", "title": "A Provably Efficient Algorithm for Linear Markov Decision Process with\n  Low Switching Cost", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world applications, such as those in medical domains,\nrecommendation systems, etc, can be formulated as large state space\nreinforcement learning problems with only a small budget of the number of\npolicy changes, i.e., low switching cost. This paper focuses on the linear\nMarkov Decision Process (MDP) recently studied in [Yang et al 2019, Jin et al\n2020] where the linear function approximation is used for generalization on the\nlarge state space. We present the first algorithm for linear MDP with a low\nswitching cost. Our algorithm achieves an\n$\\widetilde{O}\\left(\\sqrt{d^3H^4K}\\right)$ regret bound with a near-optimal\n$O\\left(d H\\log K\\right)$ global switching cost where $d$ is the feature\ndimension, $H$ is the planning horizon and $K$ is the number of episodes the\nagent plays. Our regret bound matches the best existing polynomial algorithm by\n[Jin et al 2020] and our switching cost is exponentially smaller than theirs.\nWhen specialized to tabular MDP, our switching cost bound improves those in\n[Bai et al 2019, Zhang et al 20020]. We complement our positive result with an\n$\\Omega\\left(dH/\\log d\\right)$ global switching cost lower bound for any\nno-regret algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 18:41:27 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Gao", "Minbo", ""], ["Xie", "Tianle", ""], ["Du", "Simon S.", ""], ["Yang", "Lin F.", ""]]}, {"id": "2101.00563", "submitter": "Sahil Sidheekh", "authors": "Sahil Sidheekh", "title": "Learning Neural Networks on SVD Boosted Latent Spaces for Semantic\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of large amounts of data and compelling computation power\nhave made deep learning models much popular for text classification and\nsentiment analysis. Deep neural networks have achieved competitive performance\non the above tasks when trained on naive text representations such as word\ncount, term frequency, and binary matrix embeddings. However, many of the above\nrepresentations result in the input space having a dimension of the order of\nthe vocabulary size, which is enormous. This leads to a blow-up in the number\nof parameters to be learned, and the computational cost becomes infeasible when\nscaling to domains that require retaining a colossal vocabulary. This work\nproposes using singular value decomposition to transform the high dimensional\ninput space to a lower-dimensional latent space. We show that neural networks\ntrained on this lower-dimensional space are not only able to retain performance\nwhile savoring significant reduction in the computational complexity but, in\nmany situations, also outperforms the classical neural networks trained on the\nnative input space.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 05:30:37 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Sidheekh", "Sahil", ""]]}, {"id": "2101.00574", "submitter": "Amir Zadeh", "authors": "Amir Zadeh, Santiago Benoit, Louis-Philippe Morency", "title": "StarNet: Gradient-free Training of Deep Generative Models using\n  Determined System of Linear Equations", "comments": "Work in progress at CMU", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we present an approach for training deep generative models\nsolely based on solving determined systems of linear equations. A network that\nuses this approach, called a StarNet, has the following desirable properties:\n1) training requires no gradient as solution to the system of linear equations\nis not stochastic, 2) is highly scalable when solving the system of linear\nequations w.r.t the latent codes, and similarly for the parameters of the\nmodel, and 3) it gives desirable least-square bounds for the estimation of\nlatent codes and network parameters within each layer.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 08:06:42 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Zadeh", "Amir", ""], ["Benoit", "Santiago", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "2101.00598", "submitter": "Sanket Kamthe", "authors": "Sanket Kamthe, Samuel Assefa, Marc Deisenroth", "title": "Copula Flows for Synthetic Data Generation", "comments": "Working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The ability to generate high-fidelity synthetic data is crucial when\navailable (real) data is limited or where privacy and data protection standards\nallow only for limited use of the given data, e.g., in medical and financial\ndata-sets. Current state-of-the-art methods for synthetic data generation are\nbased on generative models, such as Generative Adversarial Networks (GANs).\nEven though GANs have achieved remarkable results in synthetic data generation,\nthey are often challenging to interpret.Furthermore, GAN-based methods can\nsuffer when used with mixed real and categorical variables.Moreover, loss\nfunction (discriminator loss) design itself is problem specific, i.e., the\ngenerative model may not be useful for tasks it was not explicitly trained for.\nIn this paper, we propose to use a probabilistic model as a synthetic data\ngenerator. Learning the probabilistic model for the data is equivalent to\nestimating the density of the data. Based on the copula theory, we divide the\ndensity estimation task into two parts, i.e., estimating univariate marginals\nand estimating the multivariate copula density over the univariate marginals.\nWe use normalising flows to learn both the copula density and univariate\nmarginals. We benchmark our method on both simulated and real data-sets in\nterms of density estimation as well as the ability to generate high-fidelity\nsynthetic data\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 10:06:23 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Kamthe", "Sanket", ""], ["Assefa", "Samuel", ""], ["Deisenroth", "Marc", ""]]}, {"id": "2101.00650", "submitter": "Songting Shi", "authors": "Songting Shi", "title": "A Tutorial on the Mathematical Model of Single Cell Variational\n  Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.OT cs.LG q-bio.GN stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the large amount of sequencing data accumulated in past decades and it is\nstill accumulating, we need to handle the more and more sequencing data. As the\nfast development of the computing technologies, we now can handle a large\namount of data by a reasonable of time using the neural network based model.\nThis tutorial will introduce the the mathematical model of the single cell\nvariational inference (scVI), which use the variational auto-encoder (building\non the neural networks) to learn the distribution of the data to gain insights.\nIt was written for beginners in the simple and intuitive way with many\ndeduction details to encourage more researchers into this field.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 16:02:36 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Shi", "Songting", ""]]}, {"id": "2101.00672", "submitter": "Ozan Kayaalp", "authors": "Ozan Kaan Kayaalp", "title": "Learning optimal Bayesian prior probabilities from data", "comments": "25 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Noninformative uniform priors are staples of Bayesian inference, especially\nin Bayesian machine learning. This study challenges the assumption that they\nare optimal and their use in Bayesian inference yields optimal outcomes.\nInstead of using arbitrary noninformative uniform priors, we propose a machine\nlearning based alternative method, learning optimal priors from data by\nmaximizing a target function of interest. Applying na\\\"ive Bayes text\nclassification methodology and a search algorithm developed for this study, our\nsystem learned priors from data using the positive predictive value metric as\nthe target function. The task was to find Wikipedia articles that had not (but\nshould have) been categorized under certain Wikipedia categories. We conducted\nfive sets of experiments using separate Wikipedia categories. While the\nbaseline models used the popular Bayes-Laplace priors, the study models learned\nthe optimal priors for each set of experiments separately before using them.\nThe results showed that the study models consistently outperformed the baseline\nmodels with a wide margin of statistical significance (p < 0.001). The measured\nperformance improvement of the study model over the baseline was as high as\n443% with the mean value of 193% over five Wikipedia categories.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 17:38:08 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Kayaalp", "Ozan Kaan", ""]]}, {"id": "2101.00698", "submitter": "Omer Bobrowski", "authors": "Yohai Reani, Omer Bobrowski", "title": "Cycle Registration in Persistent Homology with Applications in\n  Topological Bootstrap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG math.AT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we propose a novel approach for comparing the persistent\nhomology representations of two spaces (filtrations). Commonly used methods are\nbased on numerical summaries such as persistence diagrams and persistence\nlandscapes, along with suitable metrics (e.g. Wasserstein). These summaries are\nuseful for computational purposes, but they are merely a marginal of the actual\ntopological information that persistent homology can provide. Instead, our\napproach compares between two topological representations directly in the data\nspace. We do so by defining a correspondence relation between individual\npersistent cycles of two different spaces, and devising a method for computing\nthis correspondence. Our matching of cycles is based on both the persistence\nintervals and the spatial placement of each feature. We demonstrate our new\nframework in the context of topological inference, where we use statistical\nbootstrap methods in order to differentiate between real features and noise in\npoint cloud data.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 20:12:00 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Reani", "Yohai", ""], ["Bobrowski", "Omer", ""]]}, {"id": "2101.00734", "submitter": "Benyamin Ghojogh", "authors": "Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley", "title": "Factor Analysis, Probabilistic Principal Component Analysis, Variational\n  Inference, and Variational Autoencoder: Tutorial and Survey", "comments": "To appear as a part of an upcoming textbook on dimensionality\n  reduction and manifold learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a tutorial and survey paper on factor analysis, probabilistic\nPrincipal Component Analysis (PCA), variational inference, and Variational\nAutoencoder (VAE). These methods, which are tightly related, are dimensionality\nreduction and generative models. They asssume that every data point is\ngenerated from or caused by a low-dimensional latent factor. By learning the\nparameters of distribution of latent space, the corresponding low-dimensional\nfactors are found for the sake of dimensionality reduction. For their\nstochastic and generative behaviour, these models can also be used for\ngeneration of new data points in the data space. In this paper, we first start\nwith variational inference where we derive the Evidence Lower Bound (ELBO) and\nExpectation Maximization (EM) for learning the parameters. Then, we introduce\nfactor analysis, derive its joint and marginal distributions, and work out its\nEM steps. Probabilistic PCA is then explained, as a special case of factor\nanalysis, and its closed-form solutions are derived. Finally, VAE is explained\nwhere the encoder, decoder and sampling from the latent space are introduced.\nTraining VAE using both EM and backpropagation are explained.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 01:29:09 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Ghojogh", "Benyamin", ""], ["Ghodsi", "Ali", ""], ["Karray", "Fakhri", ""], ["Crowley", "Mark", ""]]}, {"id": "2101.00772", "submitter": "Mohammadhossein Toutiaee", "authors": "Mohammadhossein Toutiaee, John Miller", "title": "Gaussian Function On Response Surface Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for 2-D interpreting (features and samples)\nblack-box machine learning models via a metamodeling technique, by which we\nstudy the output and input relationships of the underlying machine learning\nmodel. The metamodel can be estimated from data generated via a trained complex\nmodel by running the computer experiment on samples of data in the region of\ninterest. We utilize a Gaussian process as a surrogate to capture the response\nsurface of a complex model, in which we incorporate two parts in the process:\ninterpolated values that are modeled by a stationary Gaussian process Z\ngoverned by a prior covariance function, and a mean function mu that captures\nthe known trends in the underlying model. The optimization procedure for the\nvariable importance parameter theta is to maximize the likelihood function.\nThis theta corresponds to the correlation of individual variables with the\ntarget response. There is no need for any pre-assumed models since it depends\non empirical observations. Experiments demonstrate the potential of the\ninterpretable model through quantitative assessment of the predicted samples.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 04:47:00 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Toutiaee", "Mohammadhossein", ""], ["Miller", "John", ""]]}, {"id": "2101.00853", "submitter": "Rahul Bhadani", "authors": "Rahul Bhadani", "title": "AutoEncoder for Interpolation", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In physical science, sensor data are collected over time to produce\ntimeseries data. However, depending on the real-world condition and underlying\nphysics of the sensor, data might be noisy. Besides, the limitation of\nsample-time on sensors may not allow collecting data over all the timepoints,\nmay require some form of interpolation. Interpolation may not be smooth enough,\nfail to denoise data, and derivative operation on noisy sensor data may be poor\nthat do not reveal any high order dynamics. In this article, we propose to use\nAutoEncoder to perform interpolation that also denoise data simultaneously. A\nbrief example using a real-world is also provided.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 09:51:58 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 01:42:59 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Bhadani", "Rahul", ""]]}, {"id": "2101.00947", "submitter": "Thiago Ritto", "authors": "Luan S Prado and Thiago G Ritto", "title": "Data driven Dirichlet sampling on manifolds", "comments": "19 pages, 10 figures, 38 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article presents a novel method to sampling on manifolds based on the\nDirichlet distribution. The proposed strategy allows to completely respect the\nunderlying manifold around which data is observed, and to do massive samplings\nwith low computational effort. This can be very helpful, for instance, in\nneural networks training process, as well as in uncertainty analysis and\nstochastic optimization. Due to its simplicity and efficiency, we believe that\nthe new method has great potential. Three manifolds (two dimensional ring,\nMobius strip and spider geometry) are considered to test the proposed\nmethodology, and then it is employed to an engineering application, related to\ngas seal coefficients.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 11:19:45 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Prado", "Luan S", ""], ["Ritto", "Thiago G", ""]]}, {"id": "2101.01121", "submitter": "Konstantinos P. Panousis", "authors": "Konstantinos P. Panousis and Sotirios Chatzis and Antonios Alexos and\n  Sergios Theodoridis", "title": "Local Competition and Stochasticity for Adversarial Robustness in Deep\n  Learning", "comments": "Accepted AISTATS 2021. arXiv admin note: text overlap with\n  arXiv:2006.10620", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work addresses adversarial robustness in deep learning by considering\ndeep networks with stochastic local winner-takes-all (LWTA) activations. This\ntype of network units result in sparse representations from each model layer,\nas the units are organized in blocks where only one unit generates a non-zero\noutput. The main operating principle of the introduced units lies on stochastic\narguments, as the network performs posterior sampling over competing units to\nselect the winner. We combine these LWTA arguments with tools from the field of\nBayesian non-parametrics, specifically the stick-breaking construction of the\nIndian Buffet Process, to allow for inferring the sub-part of each layer that\nis essential for modeling the data at hand. Then, inference is performed by\nmeans of stochastic variational Bayes. We perform a thorough experimental\nevaluation of our model using benchmark datasets. As we show, our method\nachieves high robustness to adversarial perturbations, with state-of-the-art\nperformance in powerful adversarial attack schemes.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 17:40:52 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 12:35:02 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Panousis", "Konstantinos P.", ""], ["Chatzis", "Sotirios", ""], ["Alexos", "Antonios", ""], ["Theodoridis", "Sergios", ""]]}, {"id": "2101.01134", "submitter": "Danica J. Sutherland", "authors": "Pritish Kamath and Akilesh Tangella and Danica J. Sutherland and\n  Nathan Srebro", "title": "Does Invariant Risk Minimization Capture Invariance?", "comments": "Code is available in the arXiv ancillary files, linked from this page", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the Invariant Risk Minimization (IRM) formulation of Arjovsky et\nal. (2019) can fail to capture \"natural\" invariances, at least when used in its\npractical \"linear\" form, and even on very simple problems which directly follow\nthe motivating examples for IRM. This can lead to worse generalization on new\nenvironments, even when compared to unconstrained ERM. The issue stems from a\nsignificant gap between the linear variant (as in their concrete method IRMv1)\nand the full non-linear IRM formulation. Additionally, even when capturing the\n\"right\" invariances, we show that it is possible for IRM to learn a sub-optimal\npredictor, due to the loss function not being invariant across environments.\nThe issues arise even when measuring invariance on the population\ndistributions, but are exacerbated by the fact that IRM is extremely fragile to\nsampling.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 18:02:45 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 23:21:48 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Kamath", "Pritish", ""], ["Tangella", "Akilesh", ""], ["Sutherland", "Danica J.", ""], ["Srebro", "Nathan", ""]]}, {"id": "2101.01152", "submitter": "Quanquan Gu", "authors": "Spencer Frei and Yuan Cao and Quanquan Gu", "title": "Provable Generalization of SGD-trained Neural Networks of Any Width in\n  the Presence of Adversarial Label Noise", "comments": "30 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a one-hidden-layer leaky ReLU network of arbitrary width trained\nby stochastic gradient descent (SGD) following an arbitrary initialization. We\nprove that SGD produces neural networks that have classification accuracy\ncompetitive with that of the best halfspace over the distribution for a broad\nclass of distributions that includes log-concave isotropic and hard margin\ndistributions. Equivalently, such networks can generalize when the data\ndistribution is linearly separable but corrupted with adversarial label noise,\ndespite the capacity to overfit. To the best of our knowledge, this is the\nfirst work to show that overparameterized neural networks trained by SGD can\ngeneralize when the data is corrupted with adversarial label noise.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 18:32:49 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 18:57:11 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2021 18:57:47 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Frei", "Spencer", ""], ["Cao", "Yuan", ""], ["Gu", "Quanquan", ""]]}, {"id": "2101.01163", "submitter": "Xiaohan Chen", "authors": "Xiaohan Chen, Yang Zhao, Yue Wang, Pengfei Xu, Haoran You, Chaojian\n  Li, Yonggan Fu, Yingyan Lin, Zhangyang Wang", "title": "SmartDeal: Re-Modeling Deep Network Weights for Efficient Inference and\n  Training", "comments": "arXiv admin note: substantial text overlap with arXiv:2005.03403", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The record-breaking performance of deep neural networks (DNNs) comes with\nheavy parameterization, leading to external dynamic random-access memory (DRAM)\nfor storage. The prohibitive energy of DRAM accesses makes it non-trivial to\ndeploy DNN on resource-constrained devices, calling for minimizing the weight\nand data movements to improve the energy efficiency. We present SmartDeal (SD),\nan algorithm framework to trade higher-cost memory storage/access for\nlower-cost computation, in order to aggressively boost the storage and energy\nefficiency, for both inference and training. The core of SD is a novel weight\ndecomposition with structural constraints, carefully crafted to unleash the\nhardware efficiency potential. Specifically, we decompose each weight tensor as\nthe product of a small basis matrix and a large structurally sparse coefficient\nmatrix whose non-zeros are quantized to power-of-2. The resulting sparse and\nquantized DNNs enjoy greatly reduced energy for data movement and weight\nstorage, incurring minimal overhead to recover the original weights thanks to\nthe sparse bit-operations and cost-favorable computations. Beyond inference, we\ntake another leap to embrace energy-efficient training, introducing innovative\ntechniques to address the unique roadblocks arising in training while\npreserving the SD structures. We also design a dedicated hardware accelerator\nto fully utilize the SD structure to improve the real energy efficiency and\nlatency. We conduct experiments on both multiple tasks, models and datasets in\ndifferent settings. Results show that: 1) applied to inference, SD achieves up\nto 2.44x energy efficiency as evaluated via real hardware implementations; 2)\napplied to training, SD leads to 10.56x and 4.48x reduction in the storage and\ntraining energy, with negligible accuracy loss compared to state-of-the-art\ntraining baselines. Our source codes are available online.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 18:54:07 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Chen", "Xiaohan", ""], ["Zhao", "Yang", ""], ["Wang", "Yue", ""], ["Xu", "Pengfei", ""], ["You", "Haoran", ""], ["Li", "Chaojian", ""], ["Fu", "Yonggan", ""], ["Lin", "Yingyan", ""], ["Wang", "Zhangyang", ""]]}, {"id": "2101.01214", "submitter": "Eric Guzman", "authors": "Eric Guzman and Joel Meyers", "title": "Reconstructing Patchy Reionization with Deep Learning", "comments": "14 pages, 9 figures. Code available from\n  https://github.com/EEmGuzman/resunet-cmb", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.CO cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The precision anticipated from next-generation cosmic microwave background\n(CMB) surveys will create opportunities for characteristically new insights\ninto cosmology. Secondary anisotropies of the CMB will have an increased\nimportance in forthcoming surveys, due both to the cosmological information\nthey encode and the role they play in obscuring our view of the primary\nfluctuations. Quadratic estimators have become the standard tools for\nreconstructing the fields that distort the primary CMB and produce secondary\nanisotropies. While successful for lensing reconstruction with current data,\nquadratic estimators will be sub-optimal for the reconstruction of lensing and\nother effects at the expected sensitivity of the upcoming CMB surveys. In this\npaper we describe a convolutional neural network, ResUNet-CMB, that is capable\nof the simultaneous reconstruction of two sources of secondary CMB\nanisotropies, gravitational lensing and patchy reionization. We show that the\nResUNet-CMB network significantly outperforms the quadratic estimator at low\nnoise levels and is not subject to the lensing-induced bias on the patchy\nreionization reconstruction that would be present with a straightforward\napplication of the quadratic estimator.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 19:58:28 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Guzman", "Eric", ""], ["Meyers", "Joel", ""]]}, {"id": "2101.01300", "submitter": "Waheed Bajwa", "authors": "Arpita Gang and Waheed U. Bajwa", "title": "A Linearly Convergent Algorithm for Distributed Principal Component\n  Analysis", "comments": "33 pages; 15 figures; preprint of a journal paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.MA eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal Component Analysis (PCA) is the workhorse tool for dimensionality\nreduction in this era of big data. While often overlooked, the purpose of PCA\nis not only to reduce data dimensionality, but also to yield features that are\nuncorrelated. Furthermore, the ever-increasing volume of data in the modern\nworld often requires storage of data samples across multiple machines, which\nprecludes the use of centralized PCA algorithms. This paper focuses on the dual\nobjective of PCA, namely, dimensionality reduction and decorrelation of\nfeatures, but in a distributed setting. This requires estimating the\neigenvectors of the data covariance matrix, as opposed to only estimating the\nsubspace spanned by the eigenvectors, when data is distributed across a network\nof machines. Although a few distributed solutions to the PCA problem have been\nproposed recently, convergence guarantees and/or communications overhead of\nthese solutions remain a concern. With an eye towards communications\nefficiency, this paper introduces a feedforward neural network-based one\ntime-scale distributed PCA algorithm termed Distributed Sanger's Algorithm\n(DSA) that estimates the eigenvectors of the data covariance matrix when data\nis distributed across an undirected and arbitrarily connected network of\nmachines. Furthermore, the proposed algorithm is shown to converge linearly to\na neighborhood of the true solution. Numerical results are also provided to\ndemonstrate the efficacy of the proposed solution.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 00:51:14 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 20:04:10 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Gang", "Arpita", ""], ["Bajwa", "Waheed U.", ""]]}, {"id": "2101.01301", "submitter": "Yining Wang", "authors": "Xi Chen and Yanjun Han and Yining Wang", "title": "Adversarial Combinatorial Bandits with General Non-linear Reward\n  Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we study the adversarial combinatorial bandit with a known\nnon-linear reward function, extending existing work on adversarial linear\ncombinatorial bandit. {The adversarial combinatorial bandit with general\nnon-linear reward is an important open problem in bandit literature, and it is\nstill unclear whether there is a significant gap from the case of linear\nreward, stochastic bandit, or semi-bandit feedback.} We show that, with $N$\narms and subsets of $K$ arms being chosen at each of $T$ time periods, the\nminimax optimal regret is $\\widetilde\\Theta_{d}(\\sqrt{N^d T})$ if the reward\nfunction is a $d$-degree polynomial with $d< K$, and $\\Theta_K(\\sqrt{N^K T})$\nif the reward function is not a low-degree polynomial. {Both bounds are\nsignificantly different from the bound $O(\\sqrt{\\mathrm{poly}(N,K)T})$ for the\nlinear case, which suggests that there is a fundamental gap between the linear\nand non-linear reward structures.} Our result also finds applications to\nadversarial assortment optimization problem in online recommendation. We show\nthat in the worst-case of adversarial assortment problem, the optimal algorithm\nmust treat each individual $\\binom{N}{K}$ assortment as independent.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 00:56:27 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Chen", "Xi", ""], ["Han", "Yanjun", ""], ["Wang", "Yining", ""]]}, {"id": "2101.01366", "submitter": "Nontawat Charoenphakdee", "authors": "Nontawat Charoenphakdee, Jongyeong Lee, Masashi Sugiyama", "title": "A Symmetric Loss Perspective of Reliable Machine Learning", "comments": "Preprint of an Invited Review Article", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When minimizing the empirical risk in binary classification, it is a common\npractice to replace the zero-one loss with a surrogate loss to make the\nlearning objective feasible to optimize. Examples of well-known surrogate\nlosses for binary classification include the logistic loss, hinge loss, and\nsigmoid loss. It is known that the choice of a surrogate loss can highly\ninfluence the performance of the trained classifier and therefore it should be\ncarefully chosen. Recently, surrogate losses that satisfy a certain symmetric\ncondition (aka., symmetric losses) have demonstrated their usefulness in\nlearning from corrupted labels. In this article, we provide an overview of\nsymmetric losses and their applications. First, we review how a symmetric loss\ncan yield robust classification from corrupted labels in balanced error rate\n(BER) minimization and area under the receiver operating characteristic curve\n(AUC) maximization. Then, we demonstrate how the robust AUC maximization method\ncan benefit natural language processing in the problem where we want to learn\nonly from relevant keywords and unlabeled documents. Finally, we conclude this\narticle by discussing future directions, including potential applications of\nsymmetric losses for reliable machine learning and the design of non-symmetric\nlosses that can benefit from the symmetric condition.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 06:25:47 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Charoenphakdee", "Nontawat", ""], ["Lee", "Jongyeong", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "2101.01407", "submitter": "Wouter Verbeke", "authors": "Diego Olaya, Wouter Verbeke, Jente Van Belle, Marie-Anne Guerry", "title": "To do or not to do: cost-sensitive causal decision-making", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Causal classification models are adopted across a variety of operational\nbusiness processes to predict the effect of a treatment on a categorical\nbusiness outcome of interest depending on the process instance characteristics.\nThis allows optimizing operational decision-making and selecting the optimal\ntreatment to apply in each specific instance, with the aim of maximizing the\npositive outcome rate. While various powerful approaches have been presented in\nthe literature for learning causal classification models, no formal framework\nhas been elaborated for optimal decision-making based on the estimated\nindividual treatment effects, given the cost of the various treatments and the\nbenefit of the potential outcomes.\n  In this article, we therefore extend upon the expected value framework and\nformally introduce a cost-sensitive decision boundary for double binary causal\nclassification, which is a linear function of the estimated individual\ntreatment effect, the positive outcome probability and the cost and benefit\nparameters of the problem setting. The boundary allows causally classifying\ninstances in the positive and negative treatment class to maximize the expected\ncausal profit, which is introduced as the objective at hand in cost-sensitive\ncausal classification. We introduce the expected causal profit ranker which\nranks instances for maximizing the expected causal profit at each possible\nthreshold for causally classifying instances and differs from the conventional\nranking approach based on the individual treatment effect. The proposed ranking\napproach is experimentally evaluated on synthetic and marketing campaign data\nsets. The results indicate that the presented ranking method effectively\noutperforms the cost-insensitive ranking approach and allows boosting\nprofitability.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 08:36:01 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Olaya", "Diego", ""], ["Verbeke", "Wouter", ""], ["Van Belle", "Jente", ""], ["Guerry", "Marie-Anne", ""]]}, {"id": "2101.01429", "submitter": "Minh Ha Quang", "authors": "Minh Ha Quang", "title": "Convergence and finite sample approximations of entropic regularized\n  Wasserstein distances in Gaussian and RKHS settings", "comments": "51 pages, minor revision, references added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the convergence and finite sample approximations of\nentropic regularized Wasserstein distances in the Hilbert space setting. Our\nfirst main result is that for Gaussian measures on an infinite-dimensional\nHilbert space, convergence in the 2-Sinkhorn divergence is {\\it strictly\nweaker} than convergence in the exact 2-Wasserstein distance. Specifically, a\nsequence of centered Gaussian measures converges in the 2-Sinkhorn divergence\nif the corresponding covariance operators converge in the Hilbert-Schmidt norm.\nThis is in contrast to the previous known result that a sequence of centered\nGaussian measures converges in the exact 2-Wasserstein distance if and only if\nthe covariance operators converge in the trace class norm. In the reproducing\nkernel Hilbert space (RKHS) setting, the {\\it kernel Gaussian-Sinkhorn\ndivergence}, which is the Sinkhorn divergence between Gaussian measures defined\non an RKHS, defines a semi-metric on the set of Borel probability measures on a\nPolish space, given a characteristic kernel on that space. With the\nHilbert-Schmidt norm convergence, we obtain {\\it dimension-independent}\nconvergence rates for finite sample approximations of the kernel\nGaussian-Sinkhorn divergence, with the same order as the Maximum Mean\nDiscrepancy. These convergence rates apply in particular to Sinkhorn divergence\nbetween Gaussian measures on Euclidean and infinite-dimensional Hilbert spaces.\nThe sample complexity for the 2-Wasserstein distance between Gaussian measures\non Euclidean space, while dimension-dependent and larger than that of the\nSinkhorn divergence, is exponentially faster than the worst case scenario in\nthe literature.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 09:46:58 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 09:44:14 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Quang", "Minh Ha", ""]]}, {"id": "2101.01441", "submitter": "Sangkyun Lee", "authors": "Hyeongmin Cho, Sangkyun Lee", "title": "Data Quality Measures and Efficient Evaluation Algorithms for\n  Large-Scale High-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning has been proven to be effective in various application\nareas, such as object and speech recognition on mobile systems. Since a\ncritical key to machine learning success is the availability of large training\ndata, many datasets are being disclosed and published online. From a data\nconsumer or manager point of view, measuring data quality is an important first\nstep in the learning process. We need to determine which datasets to use,\nupdate, and maintain. However, not many practical ways to measure data quality\nare available today, especially when it comes to large-scale high-dimensional\ndata, such as images and videos. This paper proposes two data quality measures\nthat can compute class separability and in-class variability, the two important\naspects of data quality, for a given dataset. Classical data quality measures\ntend to focus only on class separability; however, we suggest that in-class\nvariability is another important data quality factor. We provide efficient\nalgorithms to compute our quality measures based on random projections and\nbootstrapping with statistical benefits on large-scale high-dimensional data.\nIn experiments, we show that our measures are compatible with classical\nmeasures on small-scale data and can be computed much more efficiently on\nlarge-scale high-dimensional datasets.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 10:23:08 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Cho", "Hyeongmin", ""], ["Lee", "Sangkyun", ""]]}, {"id": "2101.01442", "submitter": "Yannick Deville", "authors": "Yannick Deville, Alain Deville", "title": "Single-preparation unsupervised quantum machine learning: concepts and\n  applications", "comments": "7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The term \"machine learning\" especially refers to algorithms that derive\nmappings, i.e. intput/output transforms, by using numerical data that provide\ninformation about considered transforms. These transforms appear in many\nproblems, related to classification/clustering, regression, system\nidentification, system inversion and input signal restoration/separation. We\nhere first analyze the connections between all these problems, in the classical\nand quantum frameworks. We then focus on their most challenging versions,\ninvolving quantum data and/or quantum processing means, and unsupervised, i.e.\nblind, learning. Moreover, we propose the quite general concept of\nSIngle-Preparation Quantum Information Processing (SIPQIP). The resulting\nmethods only require a single instance of each state, whereas usual methods\nhave to very accurately create many copies of each fixed state. We apply our\nSIPQIP concept to various tasks, related to system identification (blind\nquantum process tomography or BQPT, blind Hamiltonian parameter estimation or\nBHPE, blind quantum channel identification/estimation, blind phase estimation),\nsystem inversion and state estimation (blind quantum source separation or BQSS,\nblind quantum entangled state restoration or BQSR, blind quantum channel\nequalization) and classification. Numerical tests show that our framework\nmoreover yields much more accurate estimation than the standard\nmultiple-preparation approach. Our methods are especially useful in a quantum\ncomputer, that we propose to more briefly call a \"quamputer\": BQPT and BHPE\nsimplify the characterization of the gates of quamputers; BQSS and BQSR allow\none to design quantum gates that may be used to compensate for the\nnon-idealities that alter states stored in quantum registers, and they open the\nway to the much more general concept of self-adaptive quantum gates (see longer\nversion of abstract in paper).\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 10:31:05 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Deville", "Yannick", ""], ["Deville", "Alain", ""]]}, {"id": "2101.01494", "submitter": "Wouter Verbeke", "authors": "Jakob Raymaekers, Wouter Verbeke, Tim Verdonck", "title": "Weight-of-evidence 2.0 with shrinkage and spline-binning", "comments": "New version: duplicate paragraph omitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many practical applications, such as fraud detection, credit risk modeling\nor medical decision making, classification models for assigning instances to a\npredefined set of classes are required to be both precise as well as\ninterpretable. Linear modeling methods such as logistic regression are often\nadopted, since they offer an acceptable balance between precision and\ninterpretability. Linear methods, however, are not well equipped to handle\ncategorical predictors with high-cardinality or to exploit non-linear relations\nin the data. As a solution, data preprocessing methods such as\nweight-of-evidence are typically used for transforming the predictors. The\nbinning procedure that underlies the weight-of-evidence approach, however, has\nbeen little researched and typically relies on ad-hoc or expert driven\nprocedures. The objective in this paper, therefore, is to propose a formalized,\ndata-driven and powerful method.\n  To this end, we explore the discretization of continuous variables through\nthe binning of spline functions, which allows for capturing non-linear effects\nin the predictor variables and yields highly interpretable predictors taking\nonly a small number of discrete values. Moreover, we extend upon the\nweight-of-evidence approach and propose to estimate the proportions using\nshrinkage estimators. Together, this offers an improved ability to exploit both\nnon-linear and categorical predictors for achieving increased classification\nprecision, while maintaining interpretability of the resulting model and\ndecreasing the risk of overfitting.\n  We present the results of a series of experiments in a fraud detection\nsetting, which illustrate the effectiveness of the presented approach. We\nfacilitate reproduction of the presented results and adoption of the proposed\napproaches by providing both the dataset and the code for implementing the\nexperiments and the presented approach.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 13:13:16 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 08:02:49 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Raymaekers", "Jakob", ""], ["Verbeke", "Wouter", ""], ["Verdonck", "Tim", ""]]}, {"id": "2101.01505", "submitter": "Xiang Li", "authors": "Xiang Li, Zhihua Zhang", "title": "Delayed Projection Techniques for Linearly Constrained Problems:\n  Convergence Rates, Acceleration, and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study a novel class of projection-based algorithms for\nlinearly constrained problems (LCPs) which have a lot of applications in\nstatistics, optimization, and machine learning. Conventional primal\ngradient-based methods for LCPs call a projection after each (stochastic)\ngradient descent, resulting in that the required number of projections equals\nthat of gradient descents (or total iterations). Motivated by the recent\nprogress in distributed optimization, we propose the delayed projection\ntechnique that calls a projection once for a while, lowering the projection\nfrequency and improving the projection efficiency. Accordingly, we devise a\nseries of stochastic methods for LCPs using the technique, including a variance\nreduced method and an accelerated one. We theoretically show that it is\nfeasible to improve projection efficiency in both strongly convex and generally\nconvex cases. Our analysis is simple and unified and can be easily extended to\nother methods using delayed projections. When applying our new algorithms to\nfederated optimization, a newfangled and privacy-preserving subfield in\ndistributed optimization, we obtain not only a variance reduced federated\nalgorithm with convergence rates better than previous works, but also the first\naccelerated method able to handle data heterogeneity inherent in federated\noptimization.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 13:42:41 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Li", "Xiang", ""], ["Zhang", "Zhihua", ""]]}, {"id": "2101.01506", "submitter": "Marcus Haywood-Alexander", "authors": "Marcus Haywood-Alexander, Nikolaos Dervilis, Keith Worden, Elizabeth\n  J. Cross, Robin S. Mills, Timothy J. Rogers", "title": "Structured Machine Learning Tools for Modelling Characteristics of\n  Guided Waves", "comments": "33 pages, 11 figures", "journal-ref": null, "doi": "10.1016/j.ymssp.2021.107628", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The use of ultrasonic guided waves to probe the materials/structures for\ndamage continues to increase in popularity for non-destructive evaluation (NDE)\nand structural health monitoring (SHM). The use of high-frequency waves such as\nthese offers an advantage over low-frequency methods from their ability to\ndetect damage on a smaller scale. However, in order to assess damage in a\nstructure, and implement any NDE or SHM tool, knowledge of the behaviour of a\nguided wave throughout the material/structure is important (especially when\ndesigning sensor placement for SHM systems). Determining this behaviour is\nextremely diffcult in complex materials, such as fibre-matrix composites, where\nunique phenomena such as continuous mode conversion takes place. This paper\nintroduces a novel method for modelling the feature-space of guided waves in a\ncomposite material. This technique is based on a data-driven model, where prior\nphysical knowledge can be used to create structured machine learning tools;\nwhere constraints are applied to provide said structure. The method shown makes\nuse of Gaussian processes, a full Bayesian analysis tool, and in this paper it\nis shown how physical knowledge of the guided waves can be utilised in\nmodelling using an ML tool. This paper shows that through careful consideration\nwhen applying machine learning techniques, more robust models can be generated\nwhich offer advantages such as extrapolation ability and physical\ninterpretation.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 13:42:50 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Haywood-Alexander", "Marcus", ""], ["Dervilis", "Nikolaos", ""], ["Worden", "Keith", ""], ["Cross", "Elizabeth J.", ""], ["Mills", "Robin S.", ""], ["Rogers", "Timothy J.", ""]]}, {"id": "2101.01509", "submitter": "Stefan Tiegel", "authors": "David Steurer, Stefan Tiegel", "title": "SoS Degree Reduction with Applications to Clustering and Robust Moment\n  Estimation", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a general framework to significantly reduce the degree of\nsum-of-squares proofs by introducing new variables. To illustrate the power of\nthis framework, we use it to speed up previous algorithms based on\nsum-of-squares for two important estimation problems, clustering and robust\nmoment estimation. The resulting algorithms offer the same statistical\nguarantees as the previous best algorithms but have significantly faster\nrunning times. Roughly speaking, given a sample of $n$ points in dimension $d$,\nour algorithms can exploit order-$\\ell$ moments in time $d^{O(\\ell)}\\cdot\nn^{O(1)}$, whereas a naive implementation requires time $(d\\cdot n)^{O(\\ell)}$.\nSince for the aforementioned applications, the typical sample size is\n$d^{\\Theta(\\ell)}$, our framework improves running times from $d^{O(\\ell^2)}$\nto $d^{O(\\ell)}$.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 13:49:59 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Steurer", "David", ""], ["Tiegel", "Stefan", ""]]}, {"id": "2101.01519", "submitter": "Zoltan Szabo", "authors": "Pierre-Cyril Aubin-Frankowski, Zoltan Szabo", "title": "Handling Hard Affine SDP Shape Constraints in RKHSs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape constraints, such as non-negativity, monotonicity, convexity or\nsupermodularity, play a key role in various applications of machine learning\nand statistics. However, incorporating this side information into predictive\nmodels in a hard way (for example at all points of an interval) for rich\nfunction classes is a notoriously challenging problem. We propose a unified and\nmodular convex optimization framework, relying on second-order cone (SOC)\ntightening, to encode hard affine SDP constraints on function derivatives, for\nmodels belonging to vector-valued reproducing kernel Hilbert spaces (vRKHSs).\nThe modular nature of the proposed approach allows to simultaneously handle\nmultiple shape constraints, and to tighten an infinite number of constraints\ninto finitely many. We prove the consistency of the proposed scheme and that of\nits adaptive variant, leveraging geometric properties of vRKHSs. The efficiency\nof the approach is illustrated in the context of shape optimization,\nsafety-critical control and econometrics.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 14:08:58 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Aubin-Frankowski", "Pierre-Cyril", ""], ["Szabo", "Zoltan", ""]]}, {"id": "2101.01570", "submitter": "Zaccharie Ramzi", "authors": "Zaccharie Ramzi, Jean-Luc Starck, Philippe Ciuciu", "title": "Density Compensated Unrolled Networks for Non-Cartesian MRI\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.med-ph stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks have recently been thoroughly investigated as a powerful\ntool for MRI reconstruction. There is a lack of research, however, regarding\ntheir use for a specific setting of MRI, namely non-Cartesian acquisitions. In\nthis work, we introduce a novel kind of deep neural networks to tackle this\nproblem, namely density compensated unrolled neural networks, which rely on\nDensity Compensation to correct the uneven weighting of the k-space. We assess\ntheir efficiency on the publicly available fastMRI dataset, and perform a small\nablation study. Our results show that the density-compensated unrolled neural\nnetworks outperform the different baselines, and that all parts of the design\nare needed. We also open source our code, in particular a Non-Uniform Fast\nFourier transform for TensorFlow.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 15:03:38 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 10:52:24 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Ramzi", "Zaccharie", ""], ["Starck", "Jean-Luc", ""], ["Ciuciu", "Philippe", ""]]}, {"id": "2101.01572", "submitter": "Anshuka Rangi", "authors": "Anshuka Rangi, Massimo Franceschetti and Long Tran-Thanh", "title": "Sequential Choice Bandits with Feedback for Personalizing users'\n  experience", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study sequential choice bandits with feedback. We propose\nbandit algorithms for a platform that personalizes users' experience to\nmaximize its rewards. For each action directed to a given user, the platform is\ngiven a positive reward, which is a non-decreasing function of the action, if\nthis action is below the user's threshold. Users are equipped with a patience\nbudget, and actions that are above the threshold decrease the user's patience.\nWhen all patience is lost, the user abandons the platform. The platform\nattempts to learn the thresholds of the users in order to maximize its rewards,\nbased on two different feedback models describing the information pattern\navailable to the platform at each action. We define a notion of regret by\ndetermining the best action to be taken when the platform knows that the user's\nthreshold is in a given interval. We then propose bandit algorithms for the two\nfeedback models and show that upper and lower bounds on the regret are of the\norder of $\\tilde{O}(N^{2/3})$ and $\\tilde\\Omega(N^{2/3})$, respectively, where\n$N$ is the total number of users. Finally, we show that the waiting time of any\nuser before receiving a personalized experience is uniform in $N$.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 15:04:10 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Rangi", "Anshuka", ""], ["Franceschetti", "Massimo", ""], ["Tran-Thanh", "Long", ""]]}, {"id": "2101.01708", "submitter": "Yulong Lu", "authors": "Jianfeng Lu, Yulong Lu, Min Wang", "title": "A Priori Generalization Analysis of the Deep Ritz Method for Solving\n  High Dimensional Elliptic Equations", "comments": "Revised the definition of Barron space and updated the proofs induced\n  by the changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA math.AP math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns the a priori generalization analysis of the Deep Ritz\nMethod (DRM) [W. E and B. Yu, 2017], a popular neural-network-based method for\nsolving high dimensional partial differential equations. We derive the\ngeneralization error bounds of two-layer neural networks in the framework of\nthe DRM for solving two prototype elliptic PDEs: Poisson equation and static\nSchr\\\"odinger equation on the $d$-dimensional unit hypercube. Specifically, we\nprove that the convergence rates of generalization errors are independent of\nthe dimension $d$, under the a priori assumption that the exact solutions of\nthe PDEs lie in a suitable low-complexity space called spectral Barron space.\nMoreover, we give sufficient conditions on the forcing term and the potential\nfunction which guarantee that the solutions are spectral Barron functions. We\nachieve this by developing a new solution theory for the PDEs on the spectral\nBarron space, which can be viewed as an analog of the classical Sobolev\nregularity theory for PDEs.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 18:50:59 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 14:58:28 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Lu", "Jianfeng", ""], ["Lu", "Yulong", ""], ["Wang", "Min", ""]]}, {"id": "2101.01765", "submitter": "Cemre Zor", "authors": "Cemre Zor and Terry Windeatt", "title": "A unifying approach on bias and variance analysis for classification", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Standard bias and variance (B&V) terminologies were originally defined for\nthe regression setting and their extensions to classification have led to\nseveral different models / definitions in the literature. In this paper, we aim\nto provide the link between the commonly used frameworks of Tumer & Ghosh (T&G)\nand James. By unifying the two approaches, we relate the B&V defined for the\n0/1 loss to the standard B&V of the boundary distributions given for the\nsquared error loss. The closed form relationships provide a deeper\nunderstanding of classification performance, and their use is demonstrated in\ntwo case studies.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 20:00:12 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 19:28:01 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Zor", "Cemre", ""], ["Windeatt", "Terry", ""]]}, {"id": "2101.01792", "submitter": "Kilian Fatras", "authors": "Kilian Fatras, Younes Zine, Szymon Majewski, R\\'emi Flamary, R\\'emi\n  Gribonval, Nicolas Courty", "title": "Minibatch optimal transport distances; analysis and applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optimal transport distances have become a classic tool to compare probability\ndistributions and have found many applications in machine learning. Yet,\ndespite recent algorithmic developments, their complexity prevents their direct\nuse on large scale datasets. To overcome this challenge, a common workaround is\nto compute these distances on minibatches i.e. to average the outcome of\nseveral smaller optimal transport problems. We propose in this paper an\nextended analysis of this practice, which effects were previously studied in\nrestricted cases. We first consider a large variety of Optimal Transport\nkernels. We notably argue that the minibatch strategy comes with appealing\nproperties such as unbiased estimators, gradients and a concentration bound\naround the expectation, but also with limits: the minibatch OT is not a\ndistance. To recover some of the lost distance axioms, we introduce a debiased\nminibatch OT function and study its statistical and optimisation properties.\nAlong with this theoretical analysis, we also conduct empirical experiments on\ngradient flows, generative adversarial networks (GANs) or color transfer that\nhighlight the practical interest of this strategy.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 21:29:31 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Fatras", "Kilian", ""], ["Zine", "Younes", ""], ["Majewski", "Szymon", ""], ["Flamary", "R\u00e9mi", ""], ["Gribonval", "R\u00e9mi", ""], ["Courty", "Nicolas", ""]]}, {"id": "2101.01876", "submitter": "Chaopeng Shen", "authors": "Kuai Fang, Daniel Kifer, Kathryn Lawson, Dapeng Feng, Chaopeng Shen", "title": "The data synergy effects of time-series deep learning models in\n  hydrology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When fitting statistical models to variables in geoscientific disciplines\nsuch as hydrology, it is a customary practice to regionalize - to divide a\nlarge spatial domain into multiple regions and study each region separately -\ninstead of fitting a single model on the entire data (also known as\nunification). Traditional wisdom in these fields suggests that models built for\neach region separately will have higher performance because of homogeneity\nwithin each region. However, by partitioning the training data, each model has\naccess to fewer data points and cannot learn from commonalities between\nregions. Here, through two hydrologic examples (soil moisture and streamflow),\nwe argue that unification can often significantly outperform regionalization in\nthe era of big data and deep learning (DL). Common DL architectures, even\nwithout bespoke customization, can automatically build models that benefit from\nregional commonality while accurately learning region-specific differences. We\nhighlight an effect we call data synergy, where the results of the DL models\nimproved when data were pooled together from characteristically different\nregions. In fact, the performance of the DL models benefited from more diverse\nrather than more homogeneous training data. We hypothesize that DL models\nautomatically adjust their internal representations to identify commonalities\nwhile also providing sufficient discriminatory information to the model. The\nresults here advocate for pooling together larger datasets, and suggest the\nacademic community should place greater emphasis on data sharing and\ncompilation.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 05:24:45 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Fang", "Kuai", ""], ["Kifer", "Daniel", ""], ["Lawson", "Kathryn", ""], ["Feng", "Dapeng", ""], ["Shen", "Chaopeng", ""]]}, {"id": "2101.01918", "submitter": "Oussama Dhifallah", "authors": "Oussama Dhifallah and Yue M. Lu", "title": "Phase Transitions in Transfer Learning for High-Dimensional Perceptrons", "comments": null, "journal-ref": null, "doi": "10.3390/e23040400", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transfer learning seeks to improve the generalization performance of a target\ntask by exploiting the knowledge learned from a related source task. Central\nquestions include deciding what information one should transfer and when\ntransfer can be beneficial. The latter question is related to the so-called\nnegative transfer phenomenon, where the transferred source information actually\nreduces the generalization performance of the target task. This happens when\nthe two tasks are sufficiently dissimilar. In this paper, we present a\ntheoretical analysis of transfer learning by studying a pair of related\nperceptron learning tasks. Despite the simplicity of our model, it reproduces\nseveral key phenomena observed in practice. Specifically, our asymptotic\nanalysis reveals a phase transition from negative transfer to positive transfer\nas the similarity of the two tasks moves past a well-defined threshold.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 08:29:22 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Dhifallah", "Oussama", ""], ["Lu", "Yue M.", ""]]}, {"id": "2101.01990", "submitter": "Ansgar Steland", "authors": "Ansgar Steland and Bart E. Pieters", "title": "Cross-Validation and Uncertainty Determination for Randomized Neural\n  Networks with Applications to Mobile Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Randomized artificial neural networks such as extreme learning machines\nprovide an attractive and efficient method for supervised learning under\nlimited computing ressources and green machine learning. This especially\napplies when equipping mobile devices (sensors) with weak artificial\nintelligence. Results are discussed about supervised learning with such\nnetworks and regression methods in terms of consistency and bounds for the\ngeneralization and prediction error. Especially, some recent results are\nreviewed addressing learning with data sampled by moving sensors leading to\nnon-stationary and dependent samples.\n  As randomized networks lead to random out-of-sample performance measures, we\nstudy a cross-validation approach to handle the randomness and make use of it\nto improve out-of-sample performance. Additionally, a computationally efficient\napproach to determine the resulting uncertainty in terms of a confidence\ninterval for the mean out-of-sample prediction error is discussed based on\ntwo-stage estimation. The approach is applied to a prediction problem arising\nin vehicle integrated photovoltaics.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 12:28:06 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Steland", "Ansgar", ""], ["Pieters", "Bart E.", ""]]}, {"id": "2101.02028", "submitter": "Ye Tian", "authors": "Ye Tian", "title": "A Multilayer Correlated Topic Model", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.CO stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We proposed a novel multilayer correlated topic model (MCTM) to analyze how\nthe main ideas inherit and vary between a document and its different segments,\nwhich helps understand an article's structure. The variational\nexpectation-maximization (EM) algorithm was derived to estimate the posterior\nand parameters in MCTM. We introduced two potential applications of MCTM,\nincluding the paragraph-level document analysis and market basket data\nanalysis. The effectiveness of MCTM in understanding the document structure has\nbeen verified by the great predictive performance on held-out documents and\nintuitive visualization. We also showed that MCTM could successfully capture\ncustomers' popular shopping patterns in the market basket analysis.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 21:50:36 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Tian", "Ye", ""]]}, {"id": "2101.02083", "submitter": "Hiroaki Sasaki", "authors": "Hiroaki Sasaki and Takashi Takenouchi", "title": "A unified view for unsupervised representation learning with density\n  ratio estimation: Maximization of mutual information, nonlinear ICA and\n  nonlinear subspace estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised representation learning is one of the most important problems in\nmachine learning. Recent promising methods are based on contrastive learning.\nHowever, contrastive learning often relies on heuristic ideas, and therefore it\nis not easy to understand what contrastive learning is doing. This paper\nemphasizes that density ratio estimation is a promising goal for unsupervised\nrepresentation learning, and promotes understanding to contrastive learning.\nOur primal contribution is to theoretically show that density ratio estimation\nunifies three frameworks for unsupervised representation learning: Maximization\nof mutual information (MI), nonlinear independent component analysis (ICA) and\na novel framework for estimation of a lower-dimensional nonlinear subspace\nproposed in this paper. This unified view clarifies under what conditions\ncontrastive learning can be regarded as maximizing MI, performing nonlinear ICA\nor estimating the lower-dimensional nonlinear subspace in the proposed\nframework. Furthermore, we also make theoretical contributions in each of the\nthree frameworks: We show that MI can be maximized through density ratio\nestimation under certain conditions, while our analysis for nonlinear ICA\nreveals a novel insight for recovery of the latent source components, which is\nclearly supported by numerical experiments. In addition, some theoretical\nconditions are also established to estimate a nonlinear subspace in the\nproposed framework. Based on the unified view, we propose two practical methods\nfor unsupervised representation learning through density ratio estimation: The\nfirst method is an outlier-robust method for representation learning, while the\nsecond one is a sample-efficient nonlinear ICA method. Finally, we numerically\ndemonstrate usefulness of the proposed methods in nonlinear ICA and through\napplication to a downstream task for classification.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 15:08:54 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Sasaki", "Hiroaki", ""], ["Takenouchi", "Takashi", ""]]}, {"id": "2101.02084", "submitter": "Silvia Chiappa", "authors": "Silvia Chiappa and Aldo Pacchiano", "title": "Fairness with Continuous Optimal Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whilst optimal transport (OT) is increasingly being recognized as a powerful\nand flexible approach for dealing with fairness issues, current OT fairness\nmethods are confined to the use of discrete OT. In this paper, we leverage\nrecent advances from the OT literature to introduce a stochastic-gradient\nfairness method based on a dual formulation of continuous OT. We show that this\nmethod gives superior performance to discrete OT methods when little data is\navailable to solve the OT problem, and similar performance otherwise. We also\nshow that both continuous and discrete OT methods are able to continually\nadjust the model parameters to adapt to different levels of unfairness that\nmight occur in real-world applications of ML systems.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 15:10:10 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Chiappa", "Silvia", ""], ["Pacchiano", "Aldo", ""]]}, {"id": "2101.02113", "submitter": "Francesca Tang", "authors": "Francesca Tang, Yang Feng, Hamza Chiheb, Jianqing Fan", "title": "The Interplay of Demographic Variables and Social Distancing Scores in\n  Deep Prediction of U.S. COVID-19 Cases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the severity of the COVID-19 outbreak, we characterize the nature of the\ngrowth trajectories of counties in the United States using a novel combination\nof spectral clustering and the correlation matrix. As the U.S. and the rest of\nthe world are experiencing a severe second wave of infections, the importance\nof assigning growth membership to counties and understanding the determinants\nof the growth are increasingly evident. Subsequently, we select the demographic\nfeatures that are most statistically significant in distinguishing the\ncommunities. Lastly, we effectively predict the future growth of a given county\nwith an LSTM using three social distancing scores. This comprehensive study\ncaptures the nature of counties' growth in cases at a very micro-level using\ngrowth communities, demographic factors, and social distancing performance to\nhelp government agencies utilize known information to make appropriate\ndecisions regarding which potential counties to target resources and funding\nto.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 16:12:29 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Tang", "Francesca", ""], ["Feng", "Yang", ""], ["Chiheb", "Hamza", ""], ["Fan", "Jianqing", ""]]}, {"id": "2101.02118", "submitter": "Daniela Thyssens", "authors": "Shereen Elsayed, Daniela Thyssens, Ahmed Rashed, Lars Schmidt-Thieme\n  and Hadi Samer Jomaa", "title": "Do We Really Need Deep Learning Models for Time Series Forecasting?", "comments": "14 pages with appendix, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series forecasting is a crucial task in machine learning, as it has a\nwide range of applications including but not limited to forecasting electricity\nconsumption, traffic, and air quality. Traditional forecasting models relied on\nrolling averages, vector auto-regression and auto-regressive integrated moving\naverages. On the other hand, deep learning and matrix factorization models have\nbeen recently proposed to tackle the same problem with more competitive\nperformance. However, one major drawback of such models is that they tend to be\noverly complex in comparison to traditional techniques. In this paper, we try\nto answer whether these highly complex deep learning models are without\nalternative. We aim to enrich the pool of simple but powerful baselines by\nrevisiting the gradient boosting regression trees for time series forecasting.\nSpecifically, we reconfigure the way time series data is handled by Gradient\nTree Boosting models in a windowed fashion that is similar to the deep learning\nmodels. For each training window, the target values are concatenated with\nexternal features, and then flattened to form one input instance for a\nmulti-output gradient boosting regression tree model. We conducted a\ncomparative study on nine datasets for eight state-of-the-art deep-learning\nmodels that were presented at top-level conferences in the last years. The\nresults demonstrated that the proposed approach outperforms all of the\nstate-of-the-art models.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 16:18:04 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Elsayed", "Shereen", ""], ["Thyssens", "Daniela", ""], ["Rashed", "Ahmed", ""], ["Schmidt-Thieme", "Lars", ""], ["Jomaa", "Hadi Samer", ""]]}, {"id": "2101.02138", "submitter": "Zoe Holmes", "authors": "Zo\\\"e Holmes, Kunal Sharma, M. Cerezo, Patrick J. Coles", "title": "Connecting ansatz expressibility to gradient magnitudes and barren\n  plateaus", "comments": "Main text: 10 pages, 4 figures. Appendices: 10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": "LA-UR-21-20034", "categories": "quant-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameterized quantum circuits serve as ans\\\"{a}tze for solving variational\nproblems and provide a flexible paradigm for programming near-term quantum\ncomputers. Ideally, such ans\\\"{a}tze should be highly expressive so that a\nclose approximation of the desired solution can be accessed. On the other hand,\nthe ansatz must also have sufficiently large gradients to allow for training.\nHere, we derive a fundamental relationship between these two essential\nproperties: expressibility and trainability. This is done by extending the well\nestablished barren plateau phenomenon, which holds for ans\\\"{a}tze that form\nexact 2-designs, to arbitrary ans\\\"{a}tze. Specifically, we calculate the\nvariance in the cost gradient in terms of the expressibility of the ansatz, as\nmeasured by its distance from being a 2-design. Our resulting bounds indicate\nthat highly expressive ans\\\"{a}tze exhibit flatter cost landscapes and\ntherefore will be harder to train. Furthermore, we provide numerics\nillustrating the effect of expressiblity on gradient scalings, and we discuss\nthe implications for designing strategies to avoid barren plateaus.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 17:09:37 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Holmes", "Zo\u00eb", ""], ["Sharma", "Kunal", ""], ["Cerezo", "M.", ""], ["Coles", "Patrick J.", ""]]}, {"id": "2101.02180", "submitter": "David Wu", "authors": "David Wu, David R. Palmer, Daryl R. Deford", "title": "Bayesian Inference of Random Dot Product Graphs via Conic Programming", "comments": "submitted for publication in SIAM Journal on Optimization (SIOPT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a convex cone program to infer the latent probability matrix of a\nrandom dot product graph (RDPG). The optimization problem maximizes the\nBernoulli maximum likelihood function with an added nuclear norm regularization\nterm. The dual problem has a particularly nice form, related to the well-known\nsemidefinite program relaxation of the MaxCut problem. Using the primal-dual\noptimality conditions, we bound the entries and rank of the primal and dual\nsolutions. Furthermore, we bound the optimal objective value and prove\nasymptotic consistency of the probability estimates of a slightly modified\nmodel under mild technical assumptions. Our experiments on synthetic RDPGs not\nonly recover natural clusters, but also reveal the underlying low-dimensional\ngeometry of the original data. We also demonstrate that the method recovers\nlatent structure in the Karate Club Graph and synthetic U.S. Senate vote graphs\nand is scalable to graphs with up to a few hundred nodes.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 18:29:37 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Wu", "David", ""], ["Palmer", "David R.", ""], ["Deford", "Daryl R.", ""]]}, {"id": "2101.02195", "submitter": "Quanquan Gu", "authors": "Tianhao Wang and Dongruo Zhou and Quanquan Gu", "title": "Provably Efficient Reinforcement Learning with Linear Function\n  Approximation Under Adaptivity Constraints", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study reinforcement learning (RL) with linear function approximation under\nthe adaptivity constraint. We consider two popular limited adaptivity models:\nbatch learning model and rare policy switch model, and propose two efficient\nonline RL algorithms for linear Markov decision processes. In specific, for the\nbatch learning model, our proposed LSVI-UCB-Batch algorithm achieves an $\\tilde\nO(\\sqrt{d^3H^3T} + dHT/B)$ regret, where $d$ is the dimension of the feature\nmapping, $H$ is the episode length, $T$ is the number of interactions and $B$\nis the number of batches. Our result suggests that it suffices to use only\n$\\sqrt{T/dH}$ batches to obtain $\\tilde O(\\sqrt{d^3H^3T})$ regret. For the rare\npolicy switch model, our proposed LSVI-UCB-RareSwitch algorithm enjoys an\n$\\tilde O(\\sqrt{d^3H^3T[1+T/(dH)]^{dH/B}})$ regret, which implies that $dH\\log\nT$ policy switches suffice to obtain the $\\tilde O(\\sqrt{d^3H^3T})$ regret. Our\nalgorithms achieve the same regret as the LSVI-UCB algorithm (Jin et al.,\n2019), yet with a substantially smaller amount of adaptivity.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 18:56:07 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Wang", "Tianhao", ""], ["Zhou", "Dongruo", ""], ["Gu", "Quanquan", ""]]}, {"id": "2101.02289", "submitter": "Joaquin Vanschoren", "authors": "Jeroen van Hoof, Joaquin Vanschoren", "title": "Hyperboost: Hyperparameter Optimization by Gradient Boosting surrogate\n  models", "comments": "ECMLPKDD 2019 Workshop on Automating Data Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian Optimization is a popular tool for tuning algorithms in automatic\nmachine learning (AutoML) systems. Current state-of-the-art methods leverage\nRandom Forests or Gaussian processes to build a surrogate model that predicts\nalgorithm performance given a certain set of hyperparameter settings. In this\npaper, we propose a new surrogate model based on gradient boosting, where we\nuse quantile regression to provide optimistic estimates of the performance of\nan unobserved hyperparameter setting, and combine this with a distance metric\nbetween unobserved and observed hyperparameter settings to help regulate\nexploration. We demonstrate empirically that the new method is able to\noutperform some state-of-the art techniques across a reasonable sized set of\nclassification problems.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 22:07:19 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["van Hoof", "Jeroen", ""], ["Vanschoren", "Joaquin", ""]]}, {"id": "2101.02305", "submitter": "Maryam Motamedi", "authors": "Maryam Motamedi, Na Li, Douglas G. Down and Nancy M. Heddle", "title": "Demand Forecasting for Platelet Usage: from Univariate Time Series to\n  Multivariate Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Platelet products are both expensive and have very short shelf lives. As\nusage rates for platelets are highly variable, the effective management of\nplatelet demand and supply is very important yet challenging. The primary goal\nof this paper is to present an efficient forecasting model for platelet demand\nat Canadian Blood Services (CBS). To accomplish this goal, four different\ndemand forecasting methods, ARIMA (Auto Regressive Moving Average), Prophet,\nlasso regression (least absolute shrinkage and selection operator) and LSTM\n(Long Short-Term Memory) networks are utilized and evaluated. We use a large\nclinical dataset for a centralized blood distribution centre for four hospitals\nin Hamilton, Ontario, spanning from 2010 to 2018 and consisting of daily\nplatelet transfusions along with information such as the product\nspecifications, the recipients' characteristics, and the recipients' laboratory\ntest results. This study is the first to utilize different methods from\nstatistical time series models to data-driven regression and a machine learning\ntechnique for platelet transfusion using clinical predictors and with different\namounts of data. We find that the multivariate approaches have the highest\naccuracy in general, however, if sufficient data are available, a simpler time\nseries approach such as ARIMA appears to be sufficient. We also comment on the\napproach to choose clinical indicators (inputs) for the multivariate models.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 23:54:10 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Motamedi", "Maryam", ""], ["Li", "Na", ""], ["Down", "Douglas G.", ""], ["Heddle", "Nancy M.", ""]]}, {"id": "2101.02307", "submitter": "Huan Qing", "authors": "Huan Qing and Jingli Wang", "title": "Bipartite mixed membership stochastic blockmodel", "comments": "24 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed membership problem for undirected network has been well studied in\nnetwork analysis recent years. However, the more general case of mixed\nmembership for directed network remains a challenge. Here, we propose an\ninterpretable model: bipartite mixed membership stochastic blockmodel (BiMMSB\nfor short) for directed mixed membership networks. BiMMSB allows that row nodes\nand column nodes of the adjacency matrix can be different and these nodes may\nhave distinct community structure in a directed network. We also develop an\nefficient spectral algorithm called BiMPCA to estimate the mixed memberships\nfor both row nodes and column nodes in a directed network. We show that the\napproach is asymptotically consistent under BiMMSB. We demonstrate the\nadvantages of BiMMSB with applications to a small-scale simulation study, the\ndirected Political blogs network and the Papers Citations network.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 00:21:50 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Qing", "Huan", ""], ["Wang", "Jingli", ""]]}, {"id": "2101.02330", "submitter": "Matthew Davidow", "authors": "Matthew Davidow, David Matteson", "title": "Copula Quadrant Similarity for Anomaly Scores", "comments": "17 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Practical anomaly detection requires applying numerous approaches due to the\ninherent difficulty of unsupervised learning. Direct comparison between complex\nor opaque anomaly detection algorithms is intractable; we instead propose a\nframework for associating the scores of multiple methods. Our aim is to answer\nthe question: how should one measure the similarity between anomaly scores\ngenerated by different methods? The scoring crux is the extremes, which\nidentify the most anomalous observations. A pair of algorithms are defined here\nto be similar if they assign their highest scores to roughly the same small\nfraction of observations. To formalize this, we propose a measure based on\nextremal similarity in scoring distributions through a novel upper quadrant\nmodeling approach, and contrast it with tail and other dependence measures. We\nillustrate our method with simulated and real experiments, applying spectral\nmethods to cluster multiple anomaly detection methods and to contrast our\nsimilarity measure with others. We demonstrate that our method is able to\ndetect the clusters of anomaly detection algorithms to achieve an accurate and\nrobust ensemble algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 02:19:36 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Davidow", "Matthew", ""], ["Matteson", "David", ""]]}, {"id": "2101.02332", "submitter": "Boris Hayete", "authors": "Boris Hayete, Fred Gruber, Anna Decker, Raymond Yan", "title": "Identification of Latent Variables From Graphical Model Residuals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph-based causal discovery methods aim to capture conditional\nindependencies consistent with the observed data and differentiate causal\nrelationships from indirect or induced ones. Successful construction of\ngraphical models of data depends on the assumption of causal sufficiency: that\nis, that all confounding variables are measured. When this assumption is not\nmet, learned graphical structures may become arbitrarily incorrect and effects\nimplied by such models may be wrongly attributed, carry the wrong magnitude, or\nmis-represent direction of correlation. Wide application of graphical models to\nincreasingly less curated \"big data\" draws renewed attention to the unobserved\nconfounder problem.\n  We present a novel method that aims to control for the latent space when\nestimating a DAG by iteratively deriving proxies for the latent space from the\nresiduals of the inferred model. Under mild assumptions, our method improves\nstructural inference of Gaussian graphical models and enhances identifiability\nof the causal effect. In addition, when the model is being used to predict\noutcomes, it un-confounds the coefficients on the parents of the outcomes and\nleads to improved predictive performance when out-of-sample regime is very\ndifferent from the training data. We show that any improvement of prediction of\nan outcome is intrinsically capped and cannot rise beyond a certain limit as\ncompared to the confounded model. We extend our methodology beyond GGMs to\nordinal variables and nonlinear cases. Our R package provides both PCA and\nautoencoder implementations of the methodology, suitable for GGMs with some\nguarantees and for better performance in general cases but without such\nguarantees.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 02:28:49 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Hayete", "Boris", ""], ["Gruber", "Fred", ""], ["Decker", "Anna", ""], ["Yan", "Raymond", ""]]}, {"id": "2101.02333", "submitter": "Er-Dong Guo", "authors": "Erdong Guo and David Draper", "title": "Infinitely Wide Tensor Networks as Gaussian Process", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Process is a non-parametric prior which can be understood as a\ndistribution on the function space intuitively. It is known that by introducing\nappropriate prior to the weights of the neural networks, Gaussian Process can\nbe obtained by taking the infinite-width limit of the Bayesian neural networks\nfrom a Bayesian perspective. In this paper, we explore the infinitely wide\nTensor Networks and show the equivalence of the infinitely wide Tensor Networks\nand the Gaussian Process. We study the pure Tensor Network and another two\nextended Tensor Network structures: Neural Kernel Tensor Network and Tensor\nNetwork hidden layer Neural Network and prove that each one will converge to\nthe Gaussian Process as the width of each model goes to infinity. (We note here\nthat Gaussian Process can also be obtained by taking the infinite limit of at\nleast one of the bond dimensions $\\alpha_{i}$ in the product of tensor nodes,\nand the proofs can be done with the same ideas in the proofs of the\ninfinite-width cases.) We calculate the mean function (mean vector) and the\ncovariance function (covariance matrix) of the finite dimensional distribution\nof the induced Gaussian Process by the infinite-width tensor network with a\ngeneral set-up. We study the properties of the covariance function and derive\nthe approximation of the covariance function when the integral in the\nexpectation operator is intractable. In the numerical experiments, we implement\nthe Gaussian Process corresponding to the infinite limit tensor networks and\nplot the sample paths of these models. We study the hyperparameters and plot\nthe sample path families in the induced Gaussian Process by varying the\nstandard deviations of the prior distributions. As expected, the parameters in\nthe prior distribution namely the hyper-parameters in the induced Gaussian\nProcess controls the characteristic lengthscales of the Gaussian Process.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 02:29:15 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Guo", "Erdong", ""], ["Draper", "David", ""]]}, {"id": "2101.02347", "submitter": "Chao Gao", "authors": "Chao Gao and Anderson Y. Zhang", "title": "SDP Achieves Exact Minimax Optimality in Phase Synchronization", "comments": "arXiv admin note: text overlap with arXiv:2010.04345", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the phase synchronization problem with noisy measurements\n$Y=z^*z^{*H}+\\sigma W\\in\\mathbb{C}^{n\\times n}$, where $z^*$ is an\n$n$-dimensional complex unit-modulus vector and $W$ is a complex-valued\nGaussian random matrix. It is assumed that each entry $Y_{jk}$ is observed with\nprobability $p$. We prove that an SDP relaxation of the MLE achieves the error\nbound $(1+o(1))\\frac{\\sigma^2}{2np}$ under a normalized squared $\\ell_2$ loss.\nThis result matches the minimax lower bound of the problem, and even the\nleading constant is sharp. The analysis of the SDP is based on an equivalent\nnon-convex programming whose solution can be characterized as a fixed point of\nthe generalized power iteration lifted to a higher dimensional space. This\nviewpoint unifies the proofs of the statistical optimality of three different\nmethods: MLE, SDP, and generalized power method. The technique is also applied\nto the analysis of the SDP for $\\mathbb{Z}_2$ synchronization, and we achieve\nthe minimax optimal error $\\exp\\left(-(1-o(1))\\frac{np}{2\\sigma^2}\\right)$ with\na sharp constant in the exponent.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 03:14:05 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Gao", "Chao", ""], ["Zhang", "Anderson Y.", ""]]}, {"id": "2101.02358", "submitter": "SungKwon An", "authors": "Sungkwon An, Jeonghoon Kim, Myungjoo Kang, Shahbaz Razaei and Xin Liu", "title": "OAAE: Adversarial Autoencoders for Novelty Detection in Multi-modal\n  Normality Case via Orthogonalized Latent Space", "comments": "Accepted to AAAI 2021 Workshop: Towards Robust, Secure and Efficient\n  Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novelty detection using deep generative models such as autoencoder,\ngenerative adversarial networks mostly takes image reconstruction error as\nnovelty score function. However, image data, high dimensional as it is,\ncontains a lot of different features other than class information which makes\nmodels hard to detect novelty data. The problem gets harder in multi-modal\nnormality case. To address this challenge, we propose a new way of measuring\nnovelty score in multi-modal normality cases using orthogonalized latent space.\nSpecifically, we employ orthogonal low-rank embedding in the latent space to\ndisentangle the features in the latent space using mutual class information.\nWith the orthogonalized latent space, novelty score is defined by the change of\neach latent vector. Proposed algorithm was compared to state-of-the-art novelty\ndetection algorithms using GAN such as RaPP and OCGAN, and experimental results\nshow that ours outperforms those algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 03:59:47 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["An", "Sungkwon", ""], ["Kim", "Jeonghoon", ""], ["Kang", "Myungjoo", ""], ["Razaei", "Shahbaz", ""], ["Liu", "Xin", ""]]}, {"id": "2101.02404", "submitter": "Mitchell Krock", "authors": "Mitchell Krock, William Kleiber, Dorit Hammerling, and Stephen Becker", "title": "Modeling massive highly-multivariate nonstationary spatial data with the\n  basis graphical lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new modeling framework for highly-multivariate spatial processes\nthat synthesizes ideas from recent multiscale and spectral approaches with\ngraphical models. The basis graphical lasso writes a univariate Gaussian\nprocess as a linear combination of basis functions weighted with entries of a\nGaussian graphical vector whose graph is estimated from optimizing an $\\ell_1$\npenalized likelihood. This paper extends the setting to a multivariate Gaussian\nprocess where the basis functions are weighted with Gaussian graphical vectors.\nWe motivate a model where the basis functions represent different levels of\nresolution and the graphical vectors for each level are assumed to be\nindependent. Using an orthogonal basis grants linear complexity and memory\nusage in the number of spatial locations, the number of basis functions, and\nthe number of realizations. An additional fusion penalty encourages a\nparsimonious conditional independence structure in the multilevel graphical\nmodel. We illustrate our method on a large climate ensemble from the National\nCenter for Atmospheric Research's Community Atmosphere Model that involves 40\nspatial processes.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 07:01:54 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 05:15:53 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Krock", "Mitchell", ""], ["Kleiber", "William", ""], ["Hammerling", "Dorit", ""], ["Becker", "Stephen", ""]]}, {"id": "2101.02429", "submitter": "Burak Bartan", "authors": "Burak Bartan, Mert Pilanci", "title": "Neural Spectrahedra and Semidefinite Lifts: Global Convex Optimization\n  of Polynomial Activation Neural Networks in Fully Polynomial-Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The training of two-layer neural networks with nonlinear activation functions\nis an important non-convex optimization problem with numerous applications and\npromising performance in layerwise deep learning. In this paper, we develop\nexact convex optimization formulations for two-layer neural networks with\nsecond degree polynomial activations based on semidefinite programming.\nRemarkably, we show that semidefinite lifting is always exact and therefore\ncomputational complexity for global optimization is polynomial in the input\ndimension and sample size for all input data. The developed convex formulations\nare proven to achieve the same global optimal solution set as their non-convex\ncounterparts. More specifically, the globally optimal two-layer neural network\nwith polynomial activations can be found by solving a semidefinite program\n(SDP) and decomposing the solution using a procedure we call Neural\nDecomposition. Moreover, the choice of regularizers plays a crucial role in the\ncomputational tractability of neural network training. We show that the\nstandard weight decay regularization formulation is NP-hard, whereas other\nsimple convex penalties render the problem tractable in polynomial time via\nconvex programming. We extend the results beyond the fully connected\narchitecture to different neural network architectures including networks with\nvector outputs and convolutional architectures with pooling. We provide\nextensive numerical simulations showing that the standard backpropagation\napproach often fails to achieve the global optimum of the training loss. The\nproposed approach is significantly faster to obtain better test accuracy\ncompared to the standard backpropagation procedure.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 08:43:01 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Bartan", "Burak", ""], ["Pilanci", "Mert", ""]]}, {"id": "2101.02452", "submitter": "Valentin Thorey", "authors": "Antoine Guillot and Valentin Thorey", "title": "RobustSleepNet: Transfer learning for automated sleep staging at scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sleep disorder diagnosis relies on the analysis of polysomnography (PSG)\nrecords. As a preliminary step of this examination, sleep stages are\nsystematically determined. In practice, sleep stage classification relies on\nthe visual inspection of 30-second epochs of polysomnography signals. Numerous\nautomatic approaches have been developed to replace this tedious and expensive\ntask. Although these methods demonstrated better performance than human sleep\nexperts on specific datasets, they remain largely unused in sleep clinics. The\nmain reason is that each sleep clinic uses a specific PSG montage that most\nautomatic approaches cannot handle out-of-the-box. Moreover, even when the PSG\nmontage is compatible, publications have shown that automatic approaches\nperform poorly on unseen data with different demographics. To address these\nissues, we introduce RobustSleepNet, a deep learning model for automatic sleep\nstage classification able to handle arbitrary PSG montages. We trained and\nevaluated this model in a leave-one-out-dataset fashion on a large corpus of 8\nheterogeneous sleep staging datasets to make it robust to demographic changes.\nWhen evaluated on an unseen dataset, RobustSleepNet reaches 97% of the F1 of a\nmodel explicitly trained on this dataset. Hence, RobustSleepNet unlocks the\npossibility to perform high-quality out-of-the-box automatic sleep staging with\nany clinical setup. We further show that finetuning RobustSleepNet, using a\npart of the unseen dataset, increases the F1 by 2% when compared to a model\ntrained specifically for this dataset. Therefore, finetuning might be used to\nreach a state-of-the-art level of performance on a specific population.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 09:39:08 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 12:58:36 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Guillot", "Antoine", ""], ["Thorey", "Valentin", ""]]}, {"id": "2101.02481", "submitter": "Marcello D'Orazio", "authors": "Marcello D'Orazio", "title": "Distances with mixed type variables some modified Gower's coefficients", "comments": "17 pages without figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Nearest neighbor methods have become popular in official statistics, mainly\nin imputation or in statistical matching problems; they play a key role in\nmachine learning too, where a high number of variants have been proposed. The\nchoice of the distance function depends mainly on the type of the selected\nvariables. Unfortunately, relatively few options permit to handle mixed type\nvariables, a situation frequently encountered in official statistics. The most\npopular distance for mixed type variables is derived as the complement of the\nGower's similarity coefficient; it is appealing because ranges between 0 and 1\nand allows to handle missing values. Unfortunately, the unweighted standard\nsetting the contribution of the single variables to the overall Gower's\ndistance is unbalanced because of the different nature of the variables\nthemselves. This article tries to address the main drawbacks that affect the\noverall unweighted Gower's distance by suggesting some modifications in\ncalculating the distance on the interval and ratio scaled variables. Simple\nmodifications try to attenuate the impact of outliers on the scaled Manhattan\ndistance; other modifications, relying on the kernel density estimation methods\nattempt to reduce the unbalanced contribution of the different types of\nvariables. The performance of the proposals is evaluated in simulations\nmimicking the imputation of missing values through nearest neighbor distance\nhotdeck method.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 11:00:57 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["D'Orazio", "Marcello", ""]]}, {"id": "2101.02510", "submitter": "Tiago Peixoto", "authors": "Tiago P. Peixoto", "title": "Disentangling homophily, community structure and triadic closure in\n  networks", "comments": "21 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.data-an physics.soc-ph stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Network homophily, the tendency of similar nodes to be connected, and\ntransitivity, the tendency of two nodes being connected if they share a common\nneighbor, are conflated properties in network analysis, since one mechanism can\ndrive the other. Here we present a generative model and corresponding inference\nprocedure that is capable of distinguishing between both mechanisms. Our\napproach is based on a variation of the stochastic block model (SBM) with the\naddition of triadic closure edges, and its inference can identify the most\nplausible mechanism responsible for the existence of every edge in the network,\nin addition to the underlying community structure itself. We show how the\nmethod can evade the detection of spurious communities caused solely by the\nformation of triangles in the network, and how it can improve the performance\nof link prediction when compared to the pure version of the SBM without triadic\nclosure.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 12:11:23 GMT"}, {"version": "v2", "created": "Sun, 10 Jan 2021 20:53:45 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Peixoto", "Tiago P.", ""]]}, {"id": "2101.02530", "submitter": "Alexander Neergaard Olesen", "authors": "Alexander Neergaard Olesen, Poul Jennum, Emmanuel Mignot and Helge B.\n  D. Sorensen", "title": "MSED: a multi-modal sleep event detection model for clinical sleep\n  analysis", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.SP stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Study objective: Clinical sleep analysis require manual analysis of sleep\npatterns for correct diagnosis of sleep disorders. Several studies show\nsignificant variability in scoring discrete sleep events. We wished to\ninvestigate, whether an automatic method could be used for detection of\narousals (Ar), leg movements (LM) and sleep disordered breathing (SDB) events,\nand if the joint detection of these events performed better than having three\nseparate models.\n  Methods: We designed a single deep neural network architecture to jointly\ndetect sleep events in a polysomnogram. We trained the model on 1653 recordings\nof individuals, and tested the optimized model on 1000 separate recordings. The\nperformance of the model was quantified by F1, precision, and recall scores,\nand by correlating index values to clinical values using Pearson's correlation\ncoefficient.\n  Results: F1 scores for the optimized model was 0.70, 0.63, and 0.62 for Ar,\nLM, and SDB, respectively. The performance was higher, when detecting events\njointly compared to corresponding single-event models. Index values computed\nfrom detected events correlated well with manual annotations ($r^2$ = 0.73,\n$r^2$ = 0.77, $r^2$ = 0.78, respectively).\n  Conclusion: Detecting arousals, leg movements and sleep disordered breathing\nevents jointly is possible, and the computed index values correlates well with\nhuman annotations.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 13:08:44 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Olesen", "Alexander Neergaard", ""], ["Jennum", "Poul", ""], ["Mignot", "Emmanuel", ""], ["Sorensen", "Helge B. D.", ""]]}, {"id": "2101.02553", "submitter": "Nikos Vlassis", "authors": "Nikos Vlassis, Fernando Amat Gil, Ashok Chandrashekar", "title": "Off-Policy Evaluation of Slate Policies under Bayes Risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of off-policy evaluation for slate bandits, for the\ntypical case in which the logging policy factorizes over the slots of the\nslate. We slightly depart from the existing literature by taking Bayes risk as\nthe criterion by which to evaluate estimators, and we analyze the family of\n'additive' estimators that includes the pseudoinverse (PI) estimator of\nSwaminathan et al.\\ (2017; arXiv:1605.04812). Using a control variate approach,\nwe identify a new estimator in this family that is guaranteed to have lower\nrisk than PI in the above class of problems. In particular, we show that the\nrisk improvement over PI grows linearly with the number of slots, and linearly\nwith the gap between the arithmetic and the harmonic mean of a set of\nslot-level divergences between the logging and the target policy. In the\ntypical case of a uniform logging policy and a deterministic target policy,\neach divergence corresponds to slot size, showing that maximal gains can be\nobtained for slate problems with diverse numbers of actions per slot.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 20:07:56 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Vlassis", "Nikos", ""], ["Gil", "Fernando Amat", ""], ["Chandrashekar", "Ashok", ""]]}, {"id": "2101.02561", "submitter": "Yiming Xu", "authors": "Yiming Xu, Diego Klabjan", "title": "Open Set Domain Adaptation by Extreme Value Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common domain adaptation techniques assume that the source domain and the\ntarget domain share an identical label space, which is problematic since when\ntarget samples are unlabeled we have no knowledge on whether the two domains\nshare the same label space. When this is not the case, the existing methods\nfail to perform well because the additional unknown classes are also matched\nwith the source domain during adaptation. In this paper, we tackle the open set\ndomain adaptation problem under the assumption that the source and the target\nlabel spaces only partially overlap, and the task becomes when the unknown\nclasses exist, how to detect the target unknown classes and avoid aligning them\nwith the source domain. We propose to utilize an instance-level reweighting\nstrategy for domain adaptation where the weights indicate the likelihood of a\nsample belonging to known classes and to model the tail of the entropy\ndistribution with Extreme Value Theory for unknown class detection. Experiments\non conventional domain adaptation datasets show that the proposed method\noutperforms the state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 19:31:32 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Xu", "Yiming", ""], ["Klabjan", "Diego", ""]]}, {"id": "2101.02609", "submitter": "Louis Falissard", "authors": "Louis Falissard, Karim Bounebache, Gr\\'egoire Rey", "title": "Learning a binary search with a recurrent neural network. A novel\n  approach to ordinal regression analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are a family of computational models that are naturally\nsuited to the analysis of hierarchical data such as, for instance, sequential\ndata with the use of recurrent neural networks. In the other hand, ordinal\nregression is a well-known predictive modelling problem used in fields as\ndiverse as psychometry to deep neural network based voice modelling. Their\nspecificity lies in the properties of their outcome variable, typically\nconsidered as a categorical variable with natural ordering properties,\ntypically allowing comparisons between different states (\"a little\" is less\nthan \"somewhat\" which is itself less than \"a lot\", with transitivity allowed).\nThis article investigates the application of sequence-to-sequence learning\nmethods provided by the deep learning framework in ordinal regression, by\nformulating the ordinal regression problem as a sequential binary search. A\nmethod for visualizing the model's explanatory variables according to the\nordinal target variable is proposed, that bears some similarities to linear\ndiscriminant analysis. The method is compared to traditional ordinal regression\nmethods on a number of benchmark dataset, and is shown to have comparable or\nsignificantly better predictive power.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 16:16:43 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Falissard", "Louis", ""], ["Bounebache", "Karim", ""], ["Rey", "Gr\u00e9goire", ""]]}, {"id": "2101.02689", "submitter": "Arno Blaas", "authors": "Arno Blaas, Stephen J. Roberts", "title": "The Effect of Prior Lipschitz Continuity on the Adversarial Robustness\n  of Bayesian Neural Networks", "comments": "4 pages, 2 tables, AAAI 2021 Workshop Towards Robust, Secure and\n  Efficient Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is desirable, and often a necessity, for machine learning models to be\nrobust against adversarial attacks. This is particularly true for Bayesian\nmodels, as they are well-suited for safety-critical applications, in which\nadversarial attacks can have catastrophic outcomes. In this work, we take a\ndeeper look at the adversarial robustness of Bayesian Neural Networks (BNNs).\nIn particular, we consider whether the adversarial robustness of a BNN can be\nincreased by model choices, particularly the Lipschitz continuity induced by\nthe prior. Conducting in-depth analysis on the case of i.i.d., zero-mean\nGaussian priors and posteriors approximated via mean-field variational\ninference, we find evidence that adversarial robustness is indeed sensitive to\nthe prior variance.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 18:51:05 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Blaas", "Arno", ""], ["Roberts", "Stephen J.", ""]]}, {"id": "2101.02696", "submitter": "Karan Chadha", "authors": "Karan Chadha, Gary Cheng, John C. Duchi", "title": "Accelerated, Optimal, and Parallel: Some Results on Model-Based\n  Stochastic Optimization", "comments": "24 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the Approximate-Proximal Point (aProx) family of model-based\nmethods for solving stochastic convex optimization problems, including\nstochastic subgradient, proximal point, and bundle methods, to the minibatch\nand accelerated setting. To do so, we propose specific model-based algorithms\nand an acceleration scheme for which we provide non-asymptotic convergence\nguarantees, which are order-optimal in all problem-dependent constants and\nprovide linear speedup in minibatch size, while maintaining the desirable\nrobustness traits (e.g. to stepsize) of the aProx family. Additionally, we show\nimproved convergence rates and matching lower bounds identifying new\nfundamental constants for \"interpolation\" problems, whose importance in\nstatistical machine learning is growing; this, for example, gives a\nparallelization strategy for alternating projections. We corroborate our\ntheoretical results with empirical testing to demonstrate the gains accurate\nmodeling, acceleration, and minibatching provide.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 18:58:39 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Chadha", "Karan", ""], ["Cheng", "Gary", ""], ["Duchi", "John C.", ""]]}, {"id": "2101.02703", "submitter": "Anastasios Angelopoulos", "authors": "Stephen Bates and Anastasios Angelopoulos and Lihua Lei and Jitendra\n  Malik and Michael I. Jordan", "title": "Distribution-Free, Risk-Controlling Prediction Sets", "comments": "Project website available at\n  http://www.angelopoulos.ai/blog/posts/rcps/ and codebase available at\n  https://github.com/aangelopoulos/rcps", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While improving prediction accuracy has been the focus of machine learning in\nrecent years, this alone does not suffice for reliable decision-making.\nDeploying learning systems in consequential settings also requires calibrating\nand communicating the uncertainty of predictions. To convey instance-wise\nuncertainty for prediction tasks, we show how to generate set-valued\npredictions from a black-box predictor that control the expected loss on future\ntest points at a user-specified level. Our approach provides explicit\nfinite-sample guarantees for any dataset by using a holdout set to calibrate\nthe size of the prediction sets. This framework enables simple,\ndistribution-free, rigorous error control for many tasks, and we demonstrate it\nin five large-scale machine learning problems: (1) classification problems\nwhere some mistakes are more costly than others; (2) multi-label\nclassification, where each observation has multiple associated labels; (3)\nclassification problems where the labels have a hierarchical structure; (4)\nimage segmentation, where we wish to predict a set of pixels containing an\nobject of interest; and (5) protein structure prediction. Lastly, we discuss\nextensions to uncertainty quantification for ranking, metric learning and\ndistributionally robust learning.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 18:59:33 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 03:48:34 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Bates", "Stephen", ""], ["Angelopoulos", "Anastasios", ""], ["Lei", "Lihua", ""], ["Malik", "Jitendra", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2101.02726", "submitter": "Joachim Sicking", "authors": "Joachim Sicking, Maram Akila, Maximilian Pintz, Tim Wirtz, Asja\n  Fischer, Stefan Wrobel", "title": "A Novel Regression Loss for Non-Parametric Uncertainty Optimization", "comments": "Accepted at the 3rd Symposium on Advances in Approximate Bayesian\n  Inference (AABI), code is available on:\n  https://github.com/fraunhofer-iais/second-moment-loss. arXiv admin note:\n  substantial text overlap with arXiv:2012.12687", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantification of uncertainty is one of the most promising approaches to\nestablish safe machine learning. Despite its importance, it is far from being\ngenerally solved, especially for neural networks. One of the most commonly used\napproaches so far is Monte Carlo dropout, which is computationally cheap and\neasy to apply in practice. However, it can underestimate the uncertainty. We\npropose a new objective, referred to as second-moment loss (SML), to address\nthis issue. While the full network is encouraged to model the mean, the dropout\nnetworks are explicitly used to optimize the model variance. We intensively\nstudy the performance of the new objective on various UCI regression datasets.\nComparing to the state-of-the-art of deep ensembles, SML leads to comparable\nprediction accuracies and uncertainty estimates while only requiring a single\nmodel. Under distribution shift, we observe moderate improvements. As a side\nresult, we introduce an intuitive Wasserstein distance-based uncertainty\nmeasure that is non-saturating and thus allows to resolve quality differences\nbetween any two uncertainty estimates.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 19:12:06 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Sicking", "Joachim", ""], ["Akila", "Maram", ""], ["Pintz", "Maximilian", ""], ["Wirtz", "Tim", ""], ["Fischer", "Asja", ""], ["Wrobel", "Stefan", ""]]}, {"id": "2101.02874", "submitter": "Jose-Luis Blanco-Claraco", "authors": "Jos\\'e-Luis Blanco-Claraco, Antonio Leanza, Giulio Reina", "title": "A general framework for modeling and dynamic simulation of multibody\n  systems using factor graphs", "comments": "23 pages", "journal-ref": "Nonlinear Dynamics, 2021", "doi": "10.1007/s11071-021-06731-6", "report-no": null, "categories": "cs.RO cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present a novel general framework grounded in the factor\ngraph theory to solve kinematic and dynamic problems for multi-body systems.\nAlthough the motion of multi-body systems is considered to be a well-studied\nproblem and various methods have been proposed for its solution, a unified\napproach providing an intuitive interpretation is still pursued. We describe\nhow to build factor graphs to model and simulate multibody systems using both,\nindependent and dependent coordinates. Then, batch optimization or a\nfixed-lag-smoother can be applied to solve the underlying optimization problem\nthat results in a highly-sparse nonlinear minimization problem. The proposed\nframework has been tested in extensive simulations and validated against a\ncommercial multibody software. We release a reference implementation as an\nopen-source C++ library, based on the GTSAM framework, a well-known estimation\nlibrary. Simulations of forward and inverse dynamics are presented, showing\ncomparable accuracy with classical approaches. The proposed factor graph-based\nframework has the potential to be integrated into applications related with\nmotion estimation and parameter identification of complex mechanical systems,\nranging from mechanisms to vehicles, or robot manipulators.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 06:45:45 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Blanco-Claraco", "Jos\u00e9-Luis", ""], ["Leanza", "Antonio", ""], ["Reina", "Giulio", ""]]}, {"id": "2101.02908", "submitter": "Weijun Li", "authors": "Liang Xu, Liying Zheng, Weijun Li, Zhenbo Chen, Weishun Song, Yue\n  Deng, Yongzhe Chang, Jing Xiao, Bo Yuan", "title": "NVAE-GAN Based Approach for Unsupervised Time Series Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent studies, Lots of work has been done to solve time series anomaly\ndetection by applying Variational Auto-Encoders (VAEs). Time series anomaly\ndetection is a very common but challenging task in many industries, which plays\nan important role in network monitoring, facility maintenance, information\nsecurity, and so on. However, it is very difficult to detect anomalies in time\nseries with high accuracy, due to noisy data collected from real world, and\ncomplicated abnormal patterns. From recent studies, we are inspired by Nouveau\nVAE (NVAE) and propose our anomaly detection model: Time series to Image VAE\n(T2IVAE), an unsupervised model based on NVAE for univariate series,\ntransforming 1D time series to 2D image as input, and adopting the\nreconstruction error to detect anomalies. Besides, we also apply the Generative\nAdversarial Networks based techniques to T2IVAE training strategy, aiming to\nreduce the overfitting. We evaluate our model performance on three datasets,\nand compare it with other several popular models using F1 score. T2IVAE\nachieves 0.639 on Numenta Anomaly Benchmark, 0.651 on public dataset from NASA,\nand 0.504 on our dataset collected from real-world scenario, outperforms other\ncomparison models.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 08:35:15 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Xu", "Liang", ""], ["Zheng", "Liying", ""], ["Li", "Weijun", ""], ["Chen", "Zhenbo", ""], ["Song", "Weishun", ""], ["Deng", "Yue", ""], ["Chang", "Yongzhe", ""], ["Xiao", "Jing", ""], ["Yuan", "Bo", ""]]}, {"id": "2101.02972", "submitter": "Sven Burger", "authors": "Xavier Garcia-Santiago, Sven Burger, Carsten Rockstuhl,\n  Philipp-Immanuel Schneider", "title": "Bayesian optimization with improved scalability and derivative\n  information for efficient design of nanophotonic structures", "comments": null, "journal-ref": "J. Light. Technol. 39, 167 (2021)", "doi": "10.1109/JLT.2020.3023450", "report-no": null, "categories": "physics.comp-ph physics.optics stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the combination of forward shape derivatives and the use of an\niterative inversion scheme for Bayesian optimization to find optimal designs of\nnanophotonic devices. This approach widens the range of applicability of\nBayesian optmization to situations where a larger number of iterations is\nrequired and where derivative information is available. This was previously\nimpractical because the computational efforts required to identify the next\nevaluation point in the parameter space became much larger than the actual\nevaluation of the objective function. We demonstrate an implementation of the\nmethod by optimizing a waveguide edge coupler.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 11:46:11 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Garcia-Santiago", "Xavier", ""], ["Burger", "Sven", ""], ["Rockstuhl", "Carsten", ""], ["Schneider", "Philipp-Immanuel", ""]]}, {"id": "2101.02974", "submitter": "Joachim Sicking", "authors": "Joachim Sicking, Alexander Kister, Matthias Fahrland, Stefan Eickeler,\n  Fabian H\\\"uger, Stefan R\\\"uping, Peter Schlicht, Tim Wirtz", "title": "Approaching Neural Network Uncertainty Realism", "comments": "Accepted at the NeurIPS 2019 Workshop on Machine Learning for\n  Autonomous Driving (ML4AD)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical models are inherently uncertain. Quantifying or at least\nupper-bounding their uncertainties is vital for safety-critical systems such as\nautonomous vehicles. While standard neural networks do not report this\ninformation, several approaches exist to integrate uncertainty estimates into\nthem. Assessing the quality of these uncertainty estimates is not\nstraightforward, as no direct ground truth labels are available. Instead,\nimplicit statistical assessments are required. For regression, we propose to\nevaluate uncertainty realism -- a strict quality criterion -- with a\nMahalanobis distance-based statistical test. An empirical evaluation reveals\nthe need for uncertainty measures that are appropriate to upper-bound\nheavy-tailed empirical errors. Alongside, we transfer the variational U-Net\nclassification architecture to standard supervised image-to-image tasks. We\nadopt it to the automotive domain and show that it significantly improves\nuncertainty realism compared to a plain encoder-decoder model.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 11:56:12 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Sicking", "Joachim", ""], ["Kister", "Alexander", ""], ["Fahrland", "Matthias", ""], ["Eickeler", "Stefan", ""], ["H\u00fcger", "Fabian", ""], ["R\u00fcping", "Stefan", ""], ["Schlicht", "Peter", ""], ["Wirtz", "Tim", ""]]}, {"id": "2101.02997", "submitter": "Constance Beguier", "authors": "Constance Beguier, Jean Ogier du Terrail, Iqraa Meah, Mathieu Andreux,\n  Eric W. Tramel", "title": "Differentially Private Federated Learning for Cancer Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since 2014, the NIH funded iDASH (integrating Data for Analysis,\nAnonymization, SHaring) National Center for Biomedical Computing has hosted\nyearly competitions on the topic of private computing for genomic data. For one\ntrack of the 2020 iteration of this competition, participants were challenged\nto produce an approach to federated learning (FL) training of genomic cancer\nprediction models using differential privacy (DP), with submissions ranked\naccording to held-out test accuracy for a given set of DP budgets. More\nprecisely, in this track, we are tasked with training a supervised model for\nthe prediction of breast cancer occurrence from genomic data split between two\nvirtual centers while ensuring data privacy with respect to model transfer via\nDP. In this article, we present our 3rd place submission to this competition.\nDuring the competition, we encountered two main challenges discussed in this\narticle: i) ensuring correctness of the privacy budget evaluation and ii)\nachieving an acceptable trade-off between prediction performance and privacy\nbudget.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 13:12:40 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 12:47:01 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Beguier", "Constance", ""], ["Terrail", "Jean Ogier du", ""], ["Meah", "Iqraa", ""], ["Andreux", "Mathieu", ""], ["Tramel", "Eric W.", ""]]}, {"id": "2101.03037", "submitter": "Bobak Kiani", "authors": "Bobak Toussi Kiani, Giacomo De Palma, Milad Marvian, Zi-Wen Liu, Seth\n  Lloyd", "title": "Quantum Earth Mover's Distance: A New Approach to Learning Quantum Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying how far the output of a learning algorithm is from its target is\nan essential task in machine learning. However, in quantum settings, the loss\nlandscapes of commonly used distance metrics often produce undesirable outcomes\nsuch as poor local minima and exponentially decaying gradients. As a new\napproach, we consider here the quantum earth mover's (EM) or Wasserstein-1\ndistance, recently proposed in [De Palma et al., arXiv:2009.04469] as a quantum\nanalog to the classical EM distance. We show that the quantum EM distance\npossesses unique properties, not found in other commonly used quantum distance\nmetrics, that make quantum learning more stable and efficient. We propose a\nquantum Wasserstein generative adversarial network (qWGAN) which takes\nadvantage of the quantum EM distance and provides an efficient means of\nperforming learning on quantum data. Our qWGAN requires resources polynomial in\nthe number of qubits, and our numerical experiments demonstrate that it is\ncapable of learning a diverse set of quantum data.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 14:33:19 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Kiani", "Bobak Toussi", ""], ["De Palma", "Giacomo", ""], ["Marvian", "Milad", ""], ["Liu", "Zi-Wen", ""], ["Lloyd", "Seth", ""]]}, {"id": "2101.03093", "submitter": "Ricardo Baptista", "authors": "Ricardo Baptista, Youssef Marzouk, Rebecca E. Morrison, Olivier Zahm", "title": "Learning non-Gaussian graphical models via Hessian scores and triangular\n  transport", "comments": "40 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Undirected probabilistic graphical models represent the conditional\ndependencies, or Markov properties, of a collection of random variables.\nKnowing the sparsity of such a graphical model is valuable for modeling\nmultivariate distributions and for efficiently performing inference. While the\nproblem of learning graph structure from data has been studied extensively for\ncertain parametric families of distributions, most existing methods fail to\nconsistently recover the graph structure for non-Gaussian data. Here we propose\nan algorithm for learning the Markov structure of continuous and non-Gaussian\ndistributions. To characterize conditional independence, we introduce a score\nbased on integrated Hessian information from the joint log-density, and we\nprove that this score upper bounds the conditional mutual information for a\ngeneral class of distributions. To compute the score, our algorithm SING\nestimates the density using a deterministic coupling, induced by a triangular\ntransport map, and iteratively exploits sparse structure in the map to reveal\nsparsity in the graph. For certain non-Gaussian datasets, we show that our\nalgorithm recovers the graph structure even with a biased approximation to the\ndensity. Among other examples, we apply sing to learn the dependencies between\nthe states of a chaotic dynamical system with local interactions.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 16:42:42 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Baptista", "Ricardo", ""], ["Marzouk", "Youssef", ""], ["Morrison", "Rebecca E.", ""], ["Zahm", "Olivier", ""]]}, {"id": "2101.03108", "submitter": "David Ginsbourger", "authors": "David Ginsbourger and Cedric Sch\\\"arer", "title": "Fast calculation of Gaussian Process multiple-fold cross-validation\n  residuals and their covariances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize fast Gaussian process leave-one-out formulae to multiple-fold\ncross-validation, highlighting in turn in broad settings the covariance\nstructure of cross-validation residuals. The employed approach, that relies on\nblock matrix inversion via Schur complements, is applied to both Simple and\nUniversal Kriging frameworks. We illustrate how resulting covariances affect\nmodel diagnostics and how to properly transform residuals in the first place.\nBeyond that, we examine how accounting for dependency between such residuals\naffect cross-validation-based estimation of the scale parameter. It is found in\ntwo distinct cases, namely in scale estimation and in broader covariance\nparameter estimation via pseudo-likelihood, that correcting for covariances\nbetween cross-validation residuals leads back to maximum likelihood estimation\nor to an original variation thereof. The proposed fast calculation of Gaussian\nProcess multiple-fold cross-validation residuals is implemented and benchmarked\nagainst a naive implementation, all in R language. Numerical experiments\nhighlight the accuracy of our approach as well as the substantial speed-ups\nthat it enables. It is noticeable however, as supported by a discussion on the\nmain drivers of computational costs and by a dedicated numerical benchmark,\nthat speed-ups steeply decline as the number of folds (say, all sharing the\nsame size) decreases. Overall, our results enable fast multiple-fold\ncross-validation, have direct consequences in GP model diagnostics, and pave\nthe way to future work on hyperparameter fitting as well as on the promising\nfield of goal-oriented fold design.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 17:02:37 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Ginsbourger", "David", ""], ["Sch\u00e4rer", "Cedric", ""]]}, {"id": "2101.03139", "submitter": "Rohit Kannan", "authors": "Rohit Kannan and G\\\"uzin Bayraksan and James Luedtke", "title": "Heteroscedasticity-aware residuals-based contextual stochastic\n  optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore generalizations of some integrated learning and optimization\nframeworks for data-driven contextual stochastic optimization that can adapt to\nheteroscedasticity. We identify conditions on the stochastic program, data\ngeneration process, and the prediction setup under which these generalizations\npossess asymptotic and finite sample guarantees for a class of stochastic\nprograms, including two-stage stochastic mixed-integer programs with continuous\nrecourse. We verify that our assumptions hold for popular parametric and\nnonparametric regression methods.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 18:11:21 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Kannan", "Rohit", ""], ["Bayraksan", "G\u00fczin", ""], ["Luedtke", "James", ""]]}, {"id": "2101.03288", "submitter": "Yang Song", "authors": "Yang Song and Diederik P. Kingma", "title": "How to Train Your Energy-Based Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy-Based Models (EBMs), also known as non-normalized probabilistic\nmodels, specify probability density or mass functions up to an unknown\nnormalizing constant. Unlike most other probabilistic models, EBMs do not place\na restriction on the tractability of the normalizing constant, thus are more\nflexible to parameterize and can model a more expressive family of probability\ndistributions. However, the unknown normalizing constant of EBMs makes training\nparticularly difficult. Our goal is to provide a friendly introduction to\nmodern approaches for EBM training. We start by explaining maximum likelihood\ntraining with Markov chain Monte Carlo (MCMC), and proceed to elaborate on\nMCMC-free approaches, including Score Matching (SM) and Noise Constrastive\nEstimation (NCE). We highlight theoretical connections among these three\napproaches, and end with a brief survey on alternative training methods, which\nare still under active research. Our tutorial is targeted at an audience with\nbasic understanding of generative models who want to apply EBMs or start a\nresearch project in this direction.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 04:51:31 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 19:20:09 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Song", "Yang", ""], ["Kingma", "Diederik P.", ""]]}, {"id": "2101.03419", "submitter": "Shiyu Duan", "authors": "Shiyu Duan and Jose C. Principe", "title": "Training Deep Architectures Without End-to-End Backpropagation: A Brief\n  Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This tutorial paper surveys training alternatives to end-to-end\nbackpropagation (E2EBP) -- the de facto standard for training deep\narchitectures. Modular training refers to strictly local training without both\nthe forward and the backward pass, i.e., dividing a deep architecture into\nseveral nonoverlapping modules and training them separately without any\nend-to-end operation. Between the fully global E2EBP and the strictly local\nmodular training, there are \"weakly modular\" hybrids performing training\nwithout the backward pass only. These alternatives can match or surpass the\nperformance of E2EBP on challenging datasets such as ImageNet, and are gaining\nincreased attention primarily because they offer practical advantages over\nE2EBP, which will be enumerated herein. In particular, they allow for greater\nmodularity and transparency in deep learning workflows, aligning deep learning\nwith the mainstream computer science engineering that heavily exploits\nmodularization for scalability. Modular training has also revealed novel\ninsights about learning and has further implications on other important\nresearch domains. Specifically, it induces natural and effective solutions to\nsome important practical problems such as data efficiency and transferability\nestimation.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 19:56:22 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 03:36:00 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Duan", "Shiyu", ""], ["Principe", "Jose C.", ""]]}, {"id": "2101.03499", "submitter": "Adrian Prochaska", "authors": "Adrian Prochaska, Julien Pillas and Bernard B\\\"aker", "title": "Improved active output selection strategy for noisy environments", "comments": "This work has been submitted to IFAC for possible publication at\n  SysID 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The test bench time needed for model-based calibration can be reduced with\nactive learning methods for test design. This paper presents an improved\nstrategy for active output selection. This is the task of learning multiple\nmodels in the same input dimensions and suits the needs of calibration tasks.\nCompared to an existing strategy, we take into account the noise estimate,\nwhich is inherent to Gaussian processes. The method is validated on three\ndifferent toy examples. The performance compared to the existing best strategy\nis the same or better in each example. In a best case scenario, the new\nstrategy needs at least 10% less measurements compared to all other active or\npassive strategies. Further efforts will evaluate the strategy on a real-world\napplication. Moreover, the implementation of more sophisticated active-learning\nstrategies for the query placement will be realized.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 08:27:30 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Prochaska", "Adrian", ""], ["Pillas", "Julien", ""], ["B\u00e4ker", "Bernard", ""]]}, {"id": "2101.03501", "submitter": "Spencer Compton", "authors": "Spencer Compton, Murat Kocaoglu, Kristjan Greenewald, Dmitriy Katz", "title": "Entropic Causal Inference: Identifiability and Finite Sample Results", "comments": "In Proceedings of NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entropic causal inference is a framework for inferring the causal direction\nbetween two categorical variables from observational data. The central\nassumption is that the amount of unobserved randomness in the system is not too\nlarge. This unobserved randomness is measured by the entropy of the exogenous\nvariable in the underlying structural causal model, which governs the causal\nrelation between the observed variables. Kocaoglu et al. conjectured that the\ncausal direction is identifiable when the entropy of the exogenous variable is\nnot too large. In this paper, we prove a variant of their conjecture. Namely,\nwe show that for almost all causal models where the exogenous variable has\nentropy that does not scale with the number of states of the observed\nvariables, the causal direction is identifiable from observational data. We\nalso consider the minimum entropy coupling-based algorithmic approach presented\nby Kocaoglu et al., and for the first time demonstrate algorithmic\nidentifiability guarantees using a finite number of samples. We conduct\nextensive experiments to evaluate the robustness of the method to relaxing some\nof the assumptions in our theory and demonstrate that both the constant-entropy\nexogenous variable and the no latent confounder assumptions can be relaxed in\npractice. We also empirically characterize the number of observational samples\nneeded for causal identification. Finally, we apply the algorithm on Tuebingen\ncause-effect pairs dataset.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 08:37:54 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Compton", "Spencer", ""], ["Kocaoglu", "Murat", ""], ["Greenewald", "Kristjan", ""], ["Katz", "Dmitriy", ""]]}, {"id": "2101.03606", "submitter": "Wessel Bruinsma", "authors": "Wessel P. Bruinsma and James Requeima and Andrew Y. K. Foong and\n  Jonathan Gordon and Richard E. Turner", "title": "The Gaussian Neural Process", "comments": "34 pages; includes supplementary material; to appear in AABI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Processes (NPs; Garnelo et al., 2018a,b) are a rich class of models\nfor meta-learning that map data sets directly to predictive stochastic\nprocesses. We provide a rigorous analysis of the standard maximum-likelihood\nobjective used to train conditional NPs. Moreover, we propose a new member to\nthe Neural Process family called the Gaussian Neural Process (GNP), which\nmodels predictive correlations, incorporates translation equivariance, provides\nuniversal approximation guarantees, and demonstrates encouraging performance.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 19:15:27 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Bruinsma", "Wessel P.", ""], ["Requeima", "James", ""], ["Foong", "Andrew Y. K.", ""], ["Gordon", "Jonathan", ""], ["Turner", "Richard E.", ""]]}, {"id": "2101.03663", "submitter": "Alvin Lim", "authors": "Hsin-Chan Huang and Jiefeng Xu and Alvin Lim", "title": "Marketing Mix Optimization with Practical Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address a variant of the marketing mix optimization (MMO)\nproblem which is commonly encountered in many industries, e.g., retail and\nconsumer packaged goods (CPG) industries. This problem requires the spend for\neach marketing activity, if adjusted, be changed by a non-negligible degree\n(minimum change) and also the total number of activities with spend change be\nlimited (maximum number of changes). With these two additional practical\nrequirements, the original resource allocation problem is formulated as a mixed\ninteger nonlinear program (MINLP). Given the size of a realistic problem in the\nindustrial setting, the state-of-the-art integer programming solvers may not be\nable to solve the problem to optimality in a straightforward way within a\nreasonable amount of time. Hence, we propose a systematic reformulation to ease\nthe computational burden. Computational tests show significant improvements in\nthe solution process.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 02:10:19 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Huang", "Hsin-Chan", ""], ["Xu", "Jiefeng", ""], ["Lim", "Alvin", ""]]}, {"id": "2101.03709", "submitter": "Ali Siahkoohi", "authors": "Ali Siahkoohi and Gabrio Rizzuti and Mathias Louboutin and Philipp A.\n  Witte and Felix J. Herrmann", "title": "Preconditioned training of normalizing flows for variational inference\n  in inverse problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.geo-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Obtaining samples from the posterior distribution of inverse problems with\nexpensive forward operators is challenging especially when the unknowns involve\nthe strongly heterogeneous Earth. To meet these challenges, we propose a\npreconditioning scheme involving a conditional normalizing flow (NF) capable of\nsampling from a low-fidelity posterior distribution directly. This conditional\nNF is used to speed up the training of the high-fidelity objective involving\nminimization of the Kullback-Leibler divergence between the predicted and the\ndesired high-fidelity posterior density for indirect measurements at hand. To\nminimize costs associated with the forward operator, we initialize the\nhigh-fidelity NF with the weights of the pretrained low-fidelity NF, which is\ntrained beforehand on available model and data pairs. Our numerical\nexperiments, including a 2D toy and a seismic compressed sensing example,\ndemonstrate that thanks to the preconditioning considerable speed-ups are\nachievable compared to training NFs from scratch.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 05:35:36 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Siahkoohi", "Ali", ""], ["Rizzuti", "Gabrio", ""], ["Louboutin", "Mathias", ""], ["Witte", "Philipp A.", ""], ["Herrmann", "Felix J.", ""]]}, {"id": "2101.03735", "submitter": "Wei Xie", "authors": "Bo Wang, Wei Xie, Tugce Martagan, Alp Akcay, Bram van Ravenstein", "title": "Optimizing Biomanufacturing Harvesting Decisions under Limited\n  Historical Data", "comments": "37 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In biopharmaceutical manufacturing, fermentation processes play a critical\nrole on productivity and profit. A fermentation process uses living cells with\ncomplex biological mechanisms, and this leads to high variability in the\nprocess outputs. By building on the biological mechanisms of protein and\nimpurity growth, we introduce a stochastic model to characterize the\naccumulation of the protein and impurity levels in the fermentation process.\nHowever, a common challenge in industry is the availability of only very\nlimited amount of data especially in the development and early stage of\nproduction. This adds an additional layer of uncertainty, referred to as model\nrisk, due to the difficulty of estimating the model parameters with limited\ndata. In this paper, we study the harvesting decision for a fermentation\nprocess under model risk. In particular, we adopt a Bayesian approach to update\nthe unknown parameters of the growth-rate distributions, and use the resulting\nposterior distributions to characterize the impact of model risk on\nfermentation output variability. The harvesting problem is formulated as a\nMarkov decision process model with knowledge states that summarize the\nposterior distributions and hence incorporate the model risk in\ndecision-making. The resulting model is solved by using a reinforcement\nlearning algorithm based on Bayesian sparse sampling. We provide analytical\nresults on the structure of the optimal policy and its objective function, and\nexplicitly study the impact of model risk on harvesting decisions. Our case\nstudies at MSD Animal Health demonstrate that the proposed model and solution\napproach improve the harvesting decisions in real life by achieving\nsubstantially higher average output from a fermentation batch along with lower\nbatch-to-batch variability.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 07:47:25 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 21:35:42 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 19:09:42 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Wang", "Bo", ""], ["Xie", "Wei", ""], ["Martagan", "Tugce", ""], ["Akcay", "Alp", ""], ["van Ravenstein", "Bram", ""]]}, {"id": "2101.03843", "submitter": "Simone Riggi", "authors": "S. Riggi, G. Umana, C. Trigilio, F. Cavallaro, A. Ingallinera, P.\n  Leto, F. Bufano, R.P. Norris, A.M. Hopkins, M.D. Filipovi\\'c, H. Andernach,\n  J.Th. van Loon, M.J. Micha{\\l}owski, C. Bordiu, T. An, C. Buemi, E. Carretti,\n  J.D. Collier, T. Joseph, B.S. Koribalski, R. Kothes, S. Loru, D. McConnell,\n  M. Pommier, E. Sciacca, F. Schillir\\'o, F. Vitello, K. Warhurst, M. Whiting", "title": "Evolutionary Map of the Universe (EMU):Compact radio sources in the\n  SCORPIO field towards the Galactic plane", "comments": "31 pages, 15 figures, 15 tables", "journal-ref": null, "doi": "10.1093/mnras/stab028", "report-no": null, "categories": "astro-ph.GA astro-ph.IM astro-ph.SR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present observations of a region of the Galactic plane taken during the\nEarly Science Program of the Australian Square Kilometre Array Pathfinder\n(ASKAP). In this context, we observed the SCORPIO field at 912 MHz with an\nuncompleted array consisting of 15 commissioned antennas. The resulting map\ncovers a square region of ~40 deg^2, centred on (l, b)=(343.5{\\deg},\n0.75{\\deg}), with a synthesized beam of 24\"x21\" and a background rms noise of\n150-200 {\\mu}Jy/beam, increasing to 500-600 {\\mu}Jy/beam close to the Galactic\nplane. A total of 3963 radio sources were detected and characterized in the\nfield using the CAESAR source finder. We obtained differential source counts in\nagreement with previously published data after correction for source extraction\nand characterization uncertainties, estimated from simulated data. The ASKAP\npositional and flux density scale accuracy were also investigated through\ncomparison with previous surveys (MGPS, NVSS) and additional observations of\nthe SCORPIO field, carried out with ATCA at 2.1 GHz and 10\" spatial resolution.\nThese allowed us to obtain a measurement of the spectral index for a subset of\nthe catalogued sources and an estimated fraction of (at least) 8% of resolved\nsources in the reported catalogue. We cross-matched our catalogued sources with\ndifferent astronomical databases to search for possible counterparts, finding\n~150 associations to known Galactic objects. Finally, we explored a\nmultiparametric approach for classifying previously unreported Galactic sources\nbased on their radio-infrared colors.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 12:25:32 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Riggi", "S.", ""], ["Umana", "G.", ""], ["Trigilio", "C.", ""], ["Cavallaro", "F.", ""], ["Ingallinera", "A.", ""], ["Leto", "P.", ""], ["Bufano", "F.", ""], ["Norris", "R. P.", ""], ["Hopkins", "A. M.", ""], ["Filipovi\u0107", "M. D.", ""], ["Andernach", "H.", ""], ["van Loon", "J. Th.", ""], ["Micha\u0142owski", "M. J.", ""], ["Bordiu", "C.", ""], ["An", "T.", ""], ["Buemi", "C.", ""], ["Carretti", "E.", ""], ["Collier", "J. D.", ""], ["Joseph", "T.", ""], ["Koribalski", "B. S.", ""], ["Kothes", "R.", ""], ["Loru", "S.", ""], ["McConnell", "D.", ""], ["Pommier", "M.", ""], ["Sciacca", "E.", ""], ["Schillir\u00f3", "F.", ""], ["Vitello", "F.", ""], ["Warhurst", "K.", ""], ["Whiting", "M.", ""]]}, {"id": "2101.03875", "submitter": "Mirrelijn van Nee", "authors": "Mirrelijn M. van Nee, Tim van de Brug, Mark A. van de Wiel", "title": "Fast marginal likelihood estimation of penalties for group-adaptive\n  elastic net", "comments": "16 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, clinical research routinely uses omics data, such as gene\nexpression, for predicting clinical outcomes or selecting markers.\nAdditionally, so-called co-data are often available, providing complementary\ninformation on the covariates, like p-values from previously published studies\nor groups of genes corresponding to pathways. Elastic net penalisation is\nwidely used for prediction and covariate selection. Group-adaptive elastic net\npenalisation learns from co-data to improve the prediction and covariate\nselection, by penalising important groups of covariates less than other groups.\nExisting methods are, however, computationally expensive. Here we present a\nfast method for marginal likelihood estimation of group-adaptive elastic net\npenalties for generalised linear models. We first derive a low-dimensional\nrepresentation of the Taylor approximation of the marginal likelihood and its\nfirst derivative for group-adaptive ridge penalties, to efficiently estimate\nthese penalties. Then we show by using asymptotic normality of the linear\npredictors that the marginal likelihood for elastic net models may be\napproximated well by the marginal likelihood for ridge models. The ridge group\npenalties are then transformed to elastic net group penalties by using the\nvariance function. The method allows for overlapping groups and unpenalised\nvariables. We demonstrate the method in a model-based simulation study and an\napplication to cancer genomics. The method substantially decreases computation\ntime and outperforms or matches other methods by learning from co-data.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 13:30:24 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["van Nee", "Mirrelijn M.", ""], ["van de Brug", "Tim", ""], ["van de Wiel", "Mark A.", ""]]}, {"id": "2101.03906", "submitter": "Shiwei Lan", "authors": "Shiwei Lan, Shuyi Li, Babak Shahbaba", "title": "Scaling Up Bayesian Uncertainty Quantification for Inverse Problems\n  using Deep Neural Networks", "comments": "40 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the importance of uncertainty quantification (UQ), Bayesian approach\nto inverse problems has recently gained popularity in applied mathematics,\nphysics, and engineering. However, traditional Bayesian inference methods based\non Markov Chain Monte Carlo (MCMC) tend to be computationally intensive and\ninefficient for such high dimensional problems. To address this issue, several\nmethods based on surrogate models have been proposed to speed up the inference\nprocess. More specifically, the calibration-emulation-sampling (CES) scheme has\nbeen proven to be successful in large dimensional UQ problems. In this work, we\npropose a novel CES approach for Bayesian inference based on deep neural\nnetwork (DNN) models for the emulation phase. The resulting algorithm is not\nonly computationally more efficient, but also less sensitive to the training\nset. Further, by using an Autoencoder (AE) for dimension reduction, we have\nbeen able to speed up our Bayesian inference method up to three orders of\nmagnitude. Overall, our method, henceforth called \\emph{Dimension-Reduced\nEmulative Autoencoder Monte Carlo (DREAM)} algorithm, is able to scale Bayesian\nUQ up to thousands of dimensions in physics-constrained inverse problems. Using\ntwo low-dimensional (linear and nonlinear) inverse problems we illustrate the\nvalidity this approach. Next, we apply our method to two high-dimensional\nnumerical examples (elliptic and advection-diffussion) to demonstrate its\ncomputational advantage over existing algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 14:18:38 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Lan", "Shiwei", ""], ["Li", "Shuyi", ""], ["Shahbaba", "Babak", ""]]}, {"id": "2101.03996", "submitter": "Baichuan Mo", "authors": "Baichuan Mo, Zhan Zhao, Haris N. Koutsopoulos, Jinhua Zhao", "title": "Individual Mobility Prediction: An Interpretable Activity-based Hidden\n  Markov Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individual mobility is driven by demand for activities with diverse\nspatiotemporal patterns, but existing methods for mobility prediction often\noverlook the underlying activity patterns. To address this issue, this study\ndevelops an activity-based modeling framework for individual mobility\nprediction. Specifically, an input-output hidden Markov model (IOHMM) framework\nis proposed to simultaneously predict the (continuous) time and (discrete)\nlocation of an individual's next trip using transit smart card data. The\nprediction task can be transformed into predicting the hidden activity duration\nand end location. Based on a case study of Hong Kong's metro system, we show\nthat the proposed model can achieve similar prediction performance as the\nstate-of-the-art long short-term memory (LSTM) model. Unlike LSTM, the proposed\nIOHMM model can also be used to analyze hidden activity patterns, which\nprovides meaningful behavioral interpretation for why an individual makes a\ncertain trip. Therefore, the activity-based prediction framework offers a way\nto preserve the predictive power of advanced machine learning methods while\nenhancing our ability to generate insightful behavioral explanations, which is\nuseful for enhancing situational awareness in user-centric transportation\napplications such as personalized traveler information.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 16:11:27 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Mo", "Baichuan", ""], ["Zhao", "Zhan", ""], ["Koutsopoulos", "Haris N.", ""], ["Zhao", "Jinhua", ""]]}, {"id": "2101.04025", "submitter": "Malte S. Kurz", "authors": "Malte S. Kurz", "title": "Distributed Double Machine Learning with a Serverless Architecture", "comments": null, "journal-ref": "In Companion of the ACM/SPEC International Conference on\n  Performance Engineering (ICPE '21), 2021, Association for Computing\n  Machinery, New York, NY, USA, 27-33", "doi": "10.1145/3447545.3451181", "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores serverless cloud computing for double machine learning.\nBeing based on repeated cross-fitting, double machine learning is particularly\nwell suited to exploit the high level of parallelism achievable with serverless\ncomputing. It allows to get fast on-demand estimations without additional cloud\nmaintenance effort. We provide a prototype Python implementation\n\\texttt{DoubleML-Serverless} for the estimation of double machine learning\nmodels with the serverless computing platform AWS Lambda and demonstrate its\nutility with a case study analyzing estimation times and costs.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 16:58:30 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 12:13:03 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Kurz", "Malte S.", ""]]}, {"id": "2101.04039", "submitter": "Sloan Nietert", "authors": "Sloan Nietert, Ziv Goldfeld, Kengo Kato", "title": "From Smooth Wasserstein Distance to Dual Sobolev Norm: Empirical\n  Approximation and Statistical Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical distances, i.e., discrepancy measures between probability\ndistributions, are ubiquitous in probability theory, statistics and machine\nlearning. To combat the curse of dimensionality when estimating these distances\nfrom data, recent work has proposed smoothing out local irregularities in the\nmeasured distributions via convolution with a Gaussian kernel. Motivated by the\nscalability of the smooth framework to high dimensions, we conduct an in-depth\nstudy of the structural and statistical behavior of the Gaussian-smoothed\n$p$-Wasserstein distance $\\mathsf{W}_p^{(\\sigma)}$, for arbitrary $p\\geq 1$. We\nstart by showing that $\\mathsf{W}_p^{(\\sigma)}$ admits a metric structure that\nis topologically equivalent to classic $\\mathsf{W}_p$ and is stable with\nrespect to perturbations in $\\sigma$. Moving to statistical questions, we\nexplore the asymptotic properties of\n$\\mathsf{W}_p^{(\\sigma)}(\\hat{\\mu}_n,\\mu)$, where $\\hat{\\mu}_n$ is the\nempirical distribution of $n$ i.i.d. samples from $\\mu$. To that end, we prove\nthat $\\mathsf{W}_p^{(\\sigma)}$ is controlled by a $p$th order smooth dual\nSobolev norm $\\mathsf{d}_p^{(\\sigma)}$. Since\n$\\mathsf{d}_p^{(\\sigma)}(\\hat{\\mu}_n,\\mu)$ coincides with the supremum of an\nempirical process indexed by Gaussian-smoothed Sobolev functions, it lends\nitself well to analysis via empirical process theory. We derive the limit\ndistribution of $\\sqrt{n}\\mathsf{d}_p^{(\\sigma)}(\\hat{\\mu}_n,\\mu)$ in all\ndimensions $d$, when $\\mu$ is sub-Gaussian. Through the aforementioned bound,\nthis implies a parametric empirical convergence rate of $n^{-1/2}$ for\n$\\mathsf{W}_p^{(\\sigma)}$, contrasting the $n^{-1/d}$ rate for unsmoothed\n$\\mathsf{W}_p$ when $d \\geq 3$. As applications, we provide asymptotic\nguarantees for two-sample testing and minimum distance estimation. When $p=2$,\nwe further show that $\\mathsf{d}_2^{(\\sigma)}$ can be expressed as a maximum\nmean discrepancy.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 17:23:24 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 18:39:00 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Nietert", "Sloan", ""], ["Goldfeld", "Ziv", ""], ["Kato", "Kengo", ""]]}, {"id": "2101.04097", "submitter": "Adri\\`a Garriga-Alonso", "authors": "Adri\\`a Garriga-Alonso, Mark van der Wilk", "title": "Correlated Weights in Infinite Limits of Deep Convolutional Neural\n  Networks", "comments": "Accepted for the 37th Conference on Uncertainty in Artificial\n  Intelligence (UAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Infinite width limits of deep neural networks often have tractable forms.\nThey have been used to analyse the behaviour of finite networks, as well as\nbeing useful methods in their own right. When investigating infinitely wide\nconvolutional neural networks (CNNs), it was observed that the correlations\narising from spatial weight sharing disappear in the infinite limit. This is\nundesirable, as spatial correlation is the main motivation behind CNNs. We show\nthat the loss of this property is not a consequence of the infinite limit, but\nrather of choosing an independent weight prior. Correlating the weights\nmaintains the correlations in the activations. Varying the amount of\ncorrelation interpolates between independent-weight limits and mean-pooling.\nEmpirical evaluation of the infinitely wide network shows that optimal\nperformance is achieved between the extremes, indicating that correlations can\nbe useful.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 18:43:40 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 12:16:59 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Garriga-Alonso", "Adri\u00e0", ""], ["van der Wilk", "Mark", ""]]}, {"id": "2101.04108", "submitter": "Umang Gupta", "authors": "Umang Gupta and Aaron M Ferber and Bistra Dilkina and Greg Ver Steeg", "title": "Controllable Guarantees for Fair Outcomes via Contrastive Information\n  Estimation", "comments": "This version fixes an error in Theorem 2 of the original manuscript\n  that appeared at the Proceedings of the 35th AAAI Conference on Artificial\n  Intelligence (AAAI-21). Code is available at\n  https://github.com/umgupta/fairness-via-contrastive-estimation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controlling bias in training datasets is vital for ensuring equal treatment,\nor parity, between different groups in downstream applications. A naive\nsolution is to transform the data so that it is statistically independent of\ngroup membership, but this may throw away too much information when a\nreasonable compromise between fairness and accuracy is desired. Another common\napproach is to limit the ability of a particular adversary who seeks to\nmaximize parity. Unfortunately, representations produced by adversarial\napproaches may still retain biases as their efficacy is tied to the complexity\nof the adversary used during training. To this end, we theoretically establish\nthat by limiting the mutual information between representations and protected\nattributes, we can assuredly control the parity of any downstream classifier.\nWe demonstrate an effective method for controlling parity through mutual\ninformation based on contrastive information estimators and show that they\noutperform approaches that rely on variational bounds based on complex\ngenerative models. We test our approach on UCI Adult and Heritage Health\ndatasets and demonstrate that our approach provides more informative\nrepresentations across a range of desired parity thresholds while providing\nstrong theoretical guarantees on the parity of any downstream algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 18:57:33 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 17:54:43 GMT"}, {"version": "v3", "created": "Thu, 3 Jun 2021 17:21:06 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Gupta", "Umang", ""], ["Ferber", "Aaron M", ""], ["Dilkina", "Bistra", ""], ["Steeg", "Greg Ver", ""]]}, {"id": "2101.04117", "submitter": "Miles Cranmer", "authors": "Miles Cranmer, Daniel Tamayo, Hanno Rein, Peter Battaglia, Samuel\n  Hadden, Philip J. Armitage, Shirley Ho, David N. Spergel", "title": "A Bayesian neural network predicts the dissolution of compact planetary\n  systems", "comments": "8 content pages, 7 appendix and references. 8 figures. Source code\n  at: https://github.com/MilesCranmer/bnn_chaos_model inference code at\n  https://github.com/dtamayo/spock", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.EP astro-ph.IM cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite over three hundred years of effort, no solutions exist for predicting\nwhen a general planetary configuration will become unstable. We introduce a\ndeep learning architecture to push forward this problem for compact systems.\nWhile current machine learning algorithms in this area rely on\nscientist-derived instability metrics, our new technique learns its own metrics\nfrom scratch, enabled by a novel internal structure inspired from dynamics\ntheory. Our Bayesian neural network model can accurately predict not only if,\nbut also when a compact planetary system with three or more planets will go\nunstable. Our model, trained directly from short N-body time series of raw\norbital elements, is more than two orders of magnitude more accurate at\npredicting instability times than analytical estimators, while also reducing\nthe bias of existing machine learning algorithms by nearly a factor of three.\nDespite being trained on compact resonant and near-resonant three-planet\nconfigurations, the model demonstrates robust generalization to both\nnon-resonant and higher multiplicity configurations, in the latter case\noutperforming models fit to that specific set of integrations. The model\ncomputes instability estimates up to five orders of magnitude faster than a\nnumerical integrator, and unlike previous efforts provides confidence intervals\non its predictions. Our inference model is publicly available in the SPOCK\npackage, with training code open-sourced.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 19:00:00 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Cranmer", "Miles", ""], ["Tamayo", "Daniel", ""], ["Rein", "Hanno", ""], ["Battaglia", "Peter", ""], ["Hadden", "Samuel", ""], ["Armitage", "Philip J.", ""], ["Ho", "Shirley", ""], ["Spergel", "David N.", ""]]}, {"id": "2101.04210", "submitter": "Kare Kamila", "authors": "Kare Kamila", "title": "General Hannan and Quinn Criterion for Common Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper aims to study data driven model selection criteria for a large\nclass of time series, which includes ARMA or AR($\\infty$) processes, as well as\nGARCH or ARCH($\\infty$), APARCH and many others processes. We tackled the\nchallenging issue of designing adaptive criteria which enjoys the strong\nconsistency property. When the observations are generated from one of the\naforementioned models, the new criteria, select the true model almost surely\nasymptotically. The proposed criteria are based on the minimization of a\npenalized contrast akin to the Hannan and Quinn's criterion and then involved a\nterm which is known for most classical time series models and for more complex\nmodels, this term can be data driven calibrated. Monte-Carlo experiments and an\nillustrative example on the CAC 40 index are performed to highlight the\nobtained results.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 22:03:02 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Kamila", "Kare", ""]]}, {"id": "2101.04243", "submitter": "Asaf Noy", "authors": "Asaf Noy, Yi Xu, Yonathan Aflalo, Lihi Zelnik-Manor, Rong Jin", "title": "A Convergence Theory Towards Practical Over-parameterized Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks' remarkable ability to correctly fit training data when\noptimized by gradient-based algorithms is yet to be fully understood. Recent\ntheoretical results explain the convergence for ReLU networks that are wider\nthan those used in practice by orders of magnitude. In this work, we take a\nstep towards closing the gap between theory and practice by significantly\nimproving the known theoretical bounds on both the network width and the\nconvergence time. We show that convergence to a global minimum is guaranteed\nfor networks with widths quadratic in the sample size and linear in their depth\nat a time logarithmic in both. Our analysis and convergence bounds are derived\nvia the construction of a surrogate network with fixed activation patterns that\ncan be transformed at any time to an equivalent ReLU network of a reasonable\nsize. This construction can be viewed as a novel technique to accelerate\ntraining, while its tight finite-width equivalence to Neural Tangent Kernel\n(NTK) suggests it can be utilized to study generalization as well.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 00:40:45 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 11:38:39 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Noy", "Asaf", ""], ["Xu", "Yi", ""], ["Aflalo", "Yonathan", ""], ["Zelnik-Manor", "Lihi", ""], ["Jin", "Rong", ""]]}, {"id": "2101.04306", "submitter": "Andrew McDonald", "authors": "Lai Wei, Andrew McDonald, Vaibhav Srivastava", "title": "Multi-Robot Gaussian Process Estimation and Coverage: A Deterministic\n  Sequencing Algorithm and Regret Analysis", "comments": "7 pages, 2 figures, accepted to IEEE ICRA'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.SY eess.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of distributed multi-robot coverage over an unknown,\nnonuniform sensory field. Modeling the sensory field as a realization of a\nGaussian Process and using Bayesian techniques, we devise a policy which aims\nto balance the tradeoff between learning the sensory function and covering the\nenvironment. We propose an adaptive coverage algorithm called Deterministic\nSequencing of Learning and Coverage (DSLC) that schedules learning and coverage\nepochs such that its emphasis gradually shifts from exploration to exploitation\nwhile never fully ceasing to learn. Using a novel definition of coverage regret\nwhich characterizes overall coverage performance of a multi-robot team over a\ntime horizon $T$, we analyze DSLC to provide an upper bound on expected\ncumulative coverage regret. Finally, we illustrate the empirical performance of\nthe algorithm through simulations of the coverage task over an unknown\ndistribution of wildfires.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 05:46:13 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 05:08:18 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 00:01:32 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Wei", "Lai", ""], ["McDonald", "Andrew", ""], ["Srivastava", "Vaibhav", ""]]}, {"id": "2101.04388", "submitter": "Akshayaa Magesh", "authors": "Meghana Bande, Akshayaa Magesh, Venugopal V. Veeravalli", "title": "Dynamic Spectrum Access using Stochastic Multi-User Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A stochastic multi-user multi-armed bandit framework is used to develop\nalgorithms for uncoordinated spectrum access. In contrast to prior work, it is\nassumed that rewards can be non-zero even under collisions, thus allowing for\nthe number of users to be greater than the number of channels. The proposed\nalgorithm consists of an estimation phase and an allocation phase. It is shown\nthat if every user adopts the algorithm, the system wide regret is\norder-optimal of order $O(\\log T)$ over a time-horizon of duration $T$. The\nregret guarantees hold for both the cases where the number of users is greater\nthan or less than the number of channels. The algorithm is extended to the\ndynamic case where the number of users in the system evolves over time, and is\nshown to lead to sub-linear regret.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 10:29:57 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Bande", "Meghana", ""], ["Magesh", "Akshayaa", ""], ["Veeravalli", "Venugopal V.", ""]]}, {"id": "2101.04430", "submitter": "Stefano Carrazza", "authors": "L. Berta, C. De Mattia, F. Rizzetto, S. Carrazza, P.E. Colombo, R.\n  Fumagalli, T. Langer, D. Lizio, A. Vanzulli, A. Torresin", "title": "A patient-specific approach for quantitative and automatic analysis of\n  computed tomography images in lung disease: application to COVID-19 patients", "comments": "31 pages, 7 figures, accepted in EJMP", "journal-ref": null, "doi": null, "report-no": "TIF-UNIMI-2020-26", "categories": "physics.med-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative metrics in lung computed tomography (CT) images have been widely\nused, often without a clear connection with physiology. This work proposes a\npatient-independent model for the estimation of well-aerated volume of lungs in\nCT images (WAVE). A Gaussian fit, with mean (Mu.f) and width (Sigma.f) values,\nwas applied to the lower CT histogram data points of the lung to provide the\nestimation of the well-aerated lung volume (WAVE.f). Independence from CT\nreconstruction parameters and respiratory cycle was analysed using healthy lung\nCT images and 4DCT acquisitions. The Gaussian metrics and first order radiomic\nfeatures calculated for a third cohort of COVID-19 patients were compared with\nthose relative to healthy lungs. Each lung was further segmented in 24\nsubregions and a new biomarker derived from Gaussian fit parameter Mu.f was\nproposed to represent the local density changes. WAVE.f resulted independent\nfrom the respiratory motion in 80% of the cases. Differences of 1%, 2% and up\nto 14% resulted comparing a moderate iterative strength and FBP algorithm, 1\nand 3 mm of slice thickness and different reconstruction kernel. Healthy\nsubjects were significantly different from COVID-19 patients for all the\nmetrics calculated. Graphical representation of the local biomarker provides\nspatial and quantitative information in a single 2D picture. Unlike other\nmetrics based on fixed histogram thresholds, this model is able to consider the\ninter-and intra-subject variability. In addition, it defines a local biomarker\nto quantify the severity of the disease, independently of the observer.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 12:02:01 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Berta", "L.", ""], ["De Mattia", "C.", ""], ["Rizzetto", "F.", ""], ["Carrazza", "S.", ""], ["Colombo", "P. E.", ""], ["Fumagalli", "R.", ""], ["Langer", "T.", ""], ["Lizio", "D.", ""], ["Vanzulli", "A.", ""], ["Torresin", "A.", ""]]}, {"id": "2101.04530", "submitter": "Thomas Daniel", "authors": "Thomas Daniel, Fabien Casenave, Nissrine Akkari, David Ryckelynck", "title": "Data augmentation and feature selection for automatic model\n  recommendation in computational physics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification algorithms have recently found applications in computational\nphysics for the selection of numerical methods or models adapted to the\nenvironment and the state of the physical system. For such classification\ntasks, labeled training data come from numerical simulations and generally\ncorrespond to physical fields discretized on a mesh. Three challenging\ndifficulties arise: the lack of training data, their high dimensionality, and\nthe non-applicability of common data augmentation techniques to physics data.\nThis article introduces two algorithms to address these issues, one for\ndimensionality reduction via feature selection, and one for data augmentation.\nThese algorithms are combined with a wide variety of classifiers for their\nevaluation. When combined with a stacking ensemble made of six multilayer\nperceptrons and a ridge logistic regression, they enable reaching an accuracy\nof 90% on our classification problem for nonlinear structural mechanics.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 15:09:11 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Daniel", "Thomas", ""], ["Casenave", "Fabien", ""], ["Akkari", "Nissrine", ""], ["Ryckelynck", "David", ""]]}, {"id": "2101.04584", "submitter": "Mingao Yuan", "authors": "Mingao Yuan and Zuofeng Shang", "title": "Sharp detection boundaries on testing dense subhypergraph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of testing the existence of a dense subhypergraph. The\nnull hypothesis is an Erdos-Renyi uniform random hypergraph and the alternative\nhypothesis is a uniform random hypergraph that contains a dense subhypergraph.\nWe establish sharp detection boundaries in both scenarios: (1) the edge\nprobabilities are known; (2) the edge probabilities are unknown. In both\nscenarios, sharp detectable boundaries are characterized by the appropriate\nmodel parameters. Asymptotically powerful tests are provided when the model\nparameters fall in the detectable regions. Our results indicate that the\ndetectable regions for general hypergraph models are dramatically different\nfrom their graph counterparts.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 16:31:47 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Yuan", "Mingao", ""], ["Shang", "Zuofeng", ""]]}, {"id": "2101.04653", "submitter": "Jan-Matthis Lueckmann", "authors": "Jan-Matthis Lueckmann, Jan Boelts, David S. Greenberg, Pedro J.\n  Gon\\c{c}alves, Jakob H. Macke", "title": "Benchmarking Simulation-Based Inference", "comments": "In AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in probabilistic modelling have led to a large number of\nsimulation-based inference algorithms which do not require numerical evaluation\nof likelihoods. However, a public benchmark with appropriate performance\nmetrics for such 'likelihood-free' algorithms has been lacking. This has made\nit difficult to compare algorithms and identify their strengths and weaknesses.\nWe set out to fill this gap: We provide a benchmark with inference tasks and\nsuitable performance metrics, with an initial selection of algorithms including\nrecent approaches employing neural networks and classical Approximate Bayesian\nComputation methods. We found that the choice of performance metric is\ncritical, that even state-of-the-art algorithms have substantial room for\nimprovement, and that sequential estimation improves sample efficiency. Neural\nnetwork-based approaches generally exhibit better performance, but there is no\nuniformly best algorithm. We provide practical advice and highlight the\npotential of the benchmark to diagnose problems and improve algorithms. The\nresults can be explored interactively on a companion website. All code is open\nsource, making it possible to contribute further benchmark tasks and inference\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 18:31:22 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 15:07:54 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Lueckmann", "Jan-Matthis", ""], ["Boelts", "Jan", ""], ["Greenberg", "David S.", ""], ["Gon\u00e7alves", "Pedro J.", ""], ["Macke", "Jakob H.", ""]]}, {"id": "2101.04671", "submitter": "Omar Rivasplata", "authors": "Omar Rivasplata", "title": "A note on a confidence bound of Kuzborskij and Szepesv\\'ari", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an interesting recent work, Kuzborskij and Szepesv\\'ari derived a\nconfidence bound for functions of independent random variables, which is based\non an inequality that relates concentration to squared perturbations of the\nchosen function. Kuzborskij and Szepesv\\'ari also established the\nPAC-Bayes-ification of their confidence bound. Two important aspects of their\nwork are that the random variables could be of unbounded range, and not\nnecessarily of an identical distribution. The purpose of this note is to\nadvertise/discuss these interesting results, with streamlined proofs. This\nexpository note is written for persons who, metaphorically speaking, enjoy the\n\"featured movie\" but prefer to skip the preview sequence.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 18:57:23 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Rivasplata", "Omar", ""]]}, {"id": "2101.04726", "submitter": "Nir Shlezinger", "authors": "Nir Shlezinger, Nariman Farsad, Yonina C. Eldar, and Andrea J.\n  Goldsmith", "title": "Model-Based Machine Learning for Communications", "comments": "arXiv admin note: text overlap with arXiv:2002.07806", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an introduction to model-based machine learning for communication\nsystems. We begin by reviewing existing strategies for combining model-based\nalgorithms and machine learning from a high level perspective, and compare them\nto the conventional deep learning approach which utilizes established deep\nneural network (DNN) architectures trained in an end-to-end manner. Then, we\nfocus on symbol detection, which is one of the fundamental tasks of\ncommunication receivers. We show how the different strategies of conventional\ndeep architectures, deep unfolding, and DNN-aided hybrid algorithms, can be\napplied to this problem. The last two approaches constitute a middle ground\nbetween purely model-based and solely DNN-based receivers. By focusing on this\nspecific task, we highlight the advantages and drawbacks of each strategy, and\npresent guidelines to facilitate the design of future model-based deep learning\nsystems for communications.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 19:55:34 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Shlezinger", "Nir", ""], ["Farsad", "Nariman", ""], ["Eldar", "Yonina C.", ""], ["Goldsmith", "Andrea J.", ""]]}, {"id": "2101.04750", "submitter": "Matt Peng", "authors": "Matt Peng, Banghua Zhu, Jiantao Jiao", "title": "Linear Representation Meta-Reinforcement Learning for Instant Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Fast Linearized Adaptive Policy (FLAP), a new\nmeta-reinforcement learning (meta-RL) method that is able to extrapolate well\nto out-of-distribution tasks without the need to reuse data from training, and\nadapt almost instantaneously with the need of only a few samples during\ntesting. FLAP builds upon the idea of learning a shared linear representation\nof the policy so that when adapting to a new task, it suffices to predict a set\nof linear weights. A separate adapter network is trained simultaneously with\nthe policy such that during adaptation, we can directly use the adapter network\nto predict these linear weights instead of updating a meta-policy via gradient\ndescent, such as in prior meta-RL methods like MAML, to obtain the new policy.\nThe application of the separate feed-forward network not only speeds up the\nadaptation run-time significantly, but also generalizes extremely well to very\ndifferent tasks that prior Meta-RL methods fail to generalize to. Experiments\non standard continuous-control meta-RL benchmarks show FLAP presenting\nsignificantly stronger performance on out-of-distribution tasks with up to\ndouble the average return and up to 8X faster adaptation run-time speeds when\ncompared to prior methods.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 20:56:34 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Peng", "Matt", ""], ["Zhu", "Banghua", ""], ["Jiao", "Jiantao", ""]]}, {"id": "2101.04789", "submitter": "Mounia Hamidouche", "authors": "Mounia Hamidouche, Carlos Lassance, Yuqing Hu, Lucas Drumetz, Bastien\n  Pasdeloup, Vincent Gripon", "title": "Improving Classification Accuracy with Graph Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning, classifiers are typically susceptible to noise in the\ntraining data. In this work, we aim at reducing intra-class noise with the help\nof graph filtering to improve the classification performance. Considered graphs\nare obtained by connecting samples of the training set that belong to a same\nclass depending on the similarity of their representation in a latent space. We\nshow that the proposed graph filtering methodology has the effect of\nasymptotically reducing intra-class variance, while maintaining the mean. While\nour approach applies to all classification problems in general, it is\nparticularly useful in few-shot settings, where intra-class noise can have a\nhuge impact due to the small sample selection. Using standardized benchmarks in\nthe field of vision, we empirically demonstrate the ability of the proposed\nmethod to slightly improve state-of-the-art results in both cases of few-shot\nand standard classification.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 22:51:55 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 18:24:06 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Hamidouche", "Mounia", ""], ["Lassance", "Carlos", ""], ["Hu", "Yuqing", ""], ["Drumetz", "Lucas", ""], ["Pasdeloup", "Bastien", ""], ["Gripon", "Vincent", ""]]}, {"id": "2101.04898", "submitter": "Hanxun Huang", "authors": "Hanxun Huang, Xingjun Ma, Sarah Monazam Erfani, James Bailey, Yisen\n  Wang", "title": "Unlearnable Examples: Making Personal Data Unexploitable", "comments": "ICLR2021, In International Conference on Learning Representations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The volume of \"free\" data on the internet has been key to the current success\nof deep learning. However, it also raises privacy concerns about the\nunauthorized exploitation of personal data for training commercial models. It\nis thus crucial to develop methods to prevent unauthorized data exploitation.\nThis paper raises the question: \\emph{can data be made unlearnable for deep\nlearning models?} We present a type of \\emph{error-minimizing} noise that can\nindeed make training examples unlearnable. Error-minimizing noise is\nintentionally generated to reduce the error of one or more of the training\nexample(s) close to zero, which can trick the model into believing there is\n\"nothing\" to learn from these example(s). The noise is restricted to be\nimperceptible to human eyes, and thus does not affect normal data utility. We\nempirically verify the effectiveness of error-minimizing noise in both\nsample-wise and class-wise forms. We also demonstrate its flexibility under\nextensive experimental settings and practicability in a case study of face\nrecognition. Our work establishes an important first step towards making\npersonal data unexploitable to deep learning models.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 06:15:56 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 22:53:31 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Huang", "Hanxun", ""], ["Ma", "Xingjun", ""], ["Erfani", "Sarah Monazam", ""], ["Bailey", "James", ""], ["Wang", "Yisen", ""]]}, {"id": "2101.04968", "submitter": "Dominic Richards", "authors": "Dominic Richards, Mike Rabbat", "title": "Learning with Gradient Descent and Weakly Convex Losses", "comments": "Updated References", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We study the learning performance of gradient descent when the empirical risk\nis weakly convex, namely, the smallest negative eigenvalue of the empirical\nrisk's Hessian is bounded in magnitude. By showing that this eigenvalue can\ncontrol the stability of gradient descent, generalisation error bounds are\nproven that hold under a wider range of step sizes compared to previous work.\nOut of sample guarantees are then achieved by decomposing the test error into\ngeneralisation, optimisation and approximation errors, each of which can be\nbounded and traded off with respect to algorithmic parameters, sample size and\nmagnitude of this eigenvalue. In the case of a two layer neural network, we\ndemonstrate that the empirical risk can satisfy a notion of local weak\nconvexity, specifically, the Hessian's smallest eigenvalue during training can\nbe controlled by the normalisation of the layers, i.e., network scaling. This\nallows test error guarantees to then be achieved when the population risk\nminimiser satisfies a complexity assumption. By trading off the network\ncomplexity and scaling, insights are gained into the implicit bias of neural\nnetwork scaling, which are further supported by experimental findings.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 09:58:06 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 14:43:44 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Richards", "Dominic", ""], ["Rabbat", "Mike", ""]]}, {"id": "2101.05036", "submitter": "Ali Harakeh", "authors": "Ali Harakeh and Steven L. Waslander", "title": "Estimating and Evaluating Regression Predictive Uncertainty in Deep\n  Object Detectors", "comments": "Published as a conference paper at ICLR 2021. Link:\n  https://openreview.net/forum?id=YLewtnvKgR7. This is the final camera-ready\n  version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Predictive uncertainty estimation is an essential next step for the reliable\ndeployment of deep object detectors in safety-critical tasks. In this work, we\nfocus on estimating predictive distributions for bounding box regression output\nwith variance networks. We show that in the context of object detection,\ntraining variance networks with negative log likelihood (NLL) can lead to high\nentropy predictive distributions regardless of the correctness of the output\nmean. We propose to use the energy score as a non-local proper scoring rule and\nfind that when used for training, the energy score leads to better calibrated\nand lower entropy predictive distributions than NLL. We also address the\nwidespread use of non-proper scoring metrics for evaluating predictive\ndistributions from deep object detectors by proposing an alternate evaluation\napproach founded on proper scoring rules. Using the proposed evaluation tools,\nwe show that although variance networks can be used to produce high quality\npredictive distributions, ad-hoc approaches used by seminal object detectors\nfor choosing regression targets during training do not provide wide enough data\nsupport for reliable variance learning. We hope that our work helps shift\nevaluation in probabilistic object detection to better align with predictive\nuncertainty evaluation in other machine learning domains. Code for all models,\nevaluation, and datasets is available at:\nhttps://github.com/asharakeh/probdet.git.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 12:53:54 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 13:33:53 GMT"}, {"version": "v3", "created": "Fri, 12 Mar 2021 18:16:36 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Harakeh", "Ali", ""], ["Waslander", "Steven L.", ""]]}, {"id": "2101.05108", "submitter": "Thea Aarrestad", "authors": "Thea Aarrestad, Vladimir Loncar, Nicol\\`o Ghielmetti, Maurizio\n  Pierini, Sioni Summers, Jennifer Ngadiuba, Christoffer Petersson, Hampus\n  Linander, Yutaro Iiyama, Giuseppe Di Guglielmo, Javier Duarte, Philip Harris,\n  Dylan Rankin, Sergo Jindariani, Kevin Pedro, Nhan Tran, Mia Liu, Edward\n  Kreinar, Zhenbin Wu, and Duc Hoang", "title": "Fast convolutional neural networks on FPGAs with hls4ml", "comments": "18 pages, 18 figures, 4 tables", "journal-ref": "Mach. Learn.: Sci. Technol. 2 045015 (2021)", "doi": "10.1088/2632-2153/ac0ea1", "report-no": null, "categories": "cs.LG cs.CV hep-ex physics.ins-det stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an automated tool for deploying ultra low-latency, low-power\ndeep neural networks with convolutional layers on FPGAs. By extending the\nhls4ml library, we demonstrate an inference latency of $5\\,\\mu$s using\nconvolutional architectures, targeting microsecond latency applications like\nthose at the CERN Large Hadron Collider. Considering benchmark models trained\non the Street View House Numbers Dataset, we demonstrate various methods for\nmodel compression in order to fit the computational constraints of a typical\nFPGA device used in trigger and data acquisition systems of particle detectors.\nIn particular, we discuss pruning and quantization-aware training, and\ndemonstrate how resource utilization can be significantly reduced with little\nto no loss in model accuracy. We show that the FPGA critical resource\nconsumption can be reduced by 97% with zero loss in model accuracy, and by 99%\nwhen tolerating a 6% accuracy degradation.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 14:47:11 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 11:30:02 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Aarrestad", "Thea", ""], ["Loncar", "Vladimir", ""], ["Ghielmetti", "Nicol\u00f2", ""], ["Pierini", "Maurizio", ""], ["Summers", "Sioni", ""], ["Ngadiuba", "Jennifer", ""], ["Petersson", "Christoffer", ""], ["Linander", "Hampus", ""], ["Iiyama", "Yutaro", ""], ["Di Guglielmo", "Giuseppe", ""], ["Duarte", "Javier", ""], ["Harris", "Philip", ""], ["Rankin", "Dylan", ""], ["Jindariani", "Sergo", ""], ["Pedro", "Kevin", ""], ["Tran", "Nhan", ""], ["Liu", "Mia", ""], ["Kreinar", "Edward", ""], ["Wu", "Zhenbin", ""], ["Hoang", "Duc", ""]]}, {"id": "2101.05113", "submitter": "Yuejie Chi", "authors": "Cong Ma, Yuanxin Li, Yuejie Chi", "title": "Beyond Procrustes: Balancing-Free Gradient Descent for Asymmetric\n  Low-Rank Matrix Sensing", "comments": "To appear on IEEE Trans. on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2021.3051425", "report-no": null, "categories": "eess.SP cs.IT math.IT stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Low-rank matrix estimation plays a central role in various applications\nacross science and engineering. Recently, nonconvex formulations based on\nmatrix factorization are provably solved by simple gradient descent algorithms\nwith strong computational and statistical guarantees. However, when the\nlow-rank matrices are asymmetric, existing approaches rely on adding a\nregularization term to balance the scale of the two matrix factors which in\npractice can be removed safely without hurting the performance when initialized\nvia the spectral method. In this paper, we provide a theoretical justification\nto this for the matrix sensing problem, which aims to recover a low-rank matrix\nfrom a small number of linear measurements. As long as the measurement ensemble\nsatisfies the restricted isometry property, gradient descent -- in conjunction\nwith spectral initialization -- converges linearly without the need of\nexplicitly promoting balancedness of the factors; in fact, the factors stay\nbalanced automatically throughout the execution of the algorithm. Our analysis\nis based on analyzing the evolution of a new distance metric that directly\naccounts for the ambiguity due to invertible transforms, and might be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 15:03:52 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Ma", "Cong", ""], ["Li", "Yuanxin", ""], ["Chi", "Yuejie", ""]]}, {"id": "2101.05119", "submitter": "Stefano Vigogna", "authors": "Wenjing Liao, Mauro Maggioni and Stefano Vigogna", "title": "Multiscale regression on unknown manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the regression problem of estimating functions on $\\mathbb{R}^D$\nbut supported on a $d$-dimensional manifold $ \\mathcal{M} \\subset \\mathbb{R}^D\n$ with $ d \\ll D $. Drawing ideas from multi-resolution analysis and nonlinear\napproximation, we construct low-dimensional coordinates on $\\mathcal{M}$ at\nmultiple scales, and perform multiscale regression by local polynomial fitting.\nWe propose a data-driven wavelet thresholding scheme that automatically adapts\nto the unknown regularity of the function, allowing for efficient estimation of\nfunctions exhibiting nonuniform regularity at different locations and scales.\nWe analyze the generalization error of our method by proving finite sample\nbounds in high probability on rich classes of priors. Our estimator attains\noptimal learning rates (up to logarithmic factors) as if the function was\ndefined on a known Euclidean domain of dimension $d$, instead of an unknown\nmanifold embedded in $\\mathbb{R}^D$. The implemented algorithm has quasilinear\ncomplexity in the sample size, with constants linear in $D$ and exponential in\n$d$. Our work therefore establishes a new framework for regression on\nlow-dimensional sets embedded in high dimensions, with fast implementation and\nstrong theoretical guarantees.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 15:14:31 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Liao", "Wenjing", ""], ["Maggioni", "Mauro", ""], ["Vigogna", "Stefano", ""]]}, {"id": "2101.05147", "submitter": "Niv Nayman", "authors": "Jian Tan, Niv Nayman, Mengchang Wang, Feifei Li, Rong Jin", "title": "CobBO: Coordinate Backoff Bayesian Optimization", "comments": "Jian Tan and Niv Nayman contributed equally. An implementation of\n  CobBO is available at: https://github.com/Alibaba-MIIL/CobBO", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization is a popular method for optimizing expensive black-box\nfunctions. The objective functions of hard real world problems are oftentimes\ncharacterized by a fluctuated landscape of many local optima. Bayesian\noptimization risks in over-exploiting such traps, remaining with insufficient\nquery budget for exploring the global landscape. We introduce Coordinate\nBackoff Bayesian Optimization (CobBO) to alleviate those challenges. CobBO\ncaptures a smooth approximation of the global landscape by interpolating the\nvalues of queried points projected to randomly selected promising subspaces.\nThus also a smaller query budget is required for the Gaussian process\nregressions applied over the lower dimensional subspaces. This approach can be\nviewed as a variant of coordinate ascent, tailored for Bayesian optimization,\nusing a stopping rule for backing off from a certain subspace and switching to\nanother coordinate subset. Extensive evaluations show that CobBO finds\nsolutions comparable to or better than other state-of-the-art methods for\ndimensions ranging from tens to hundreds, while reducing the trial complexity.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 15:39:32 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 10:18:57 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Tan", "Jian", ""], ["Nayman", "Niv", ""], ["Wang", "Mengchang", ""], ["Li", "Feifei", ""], ["Jin", "Rong", ""]]}, {"id": "2101.05186", "submitter": "Pieter-Jan Hoedt", "authors": "Pieter-Jan Hoedt, Frederik Kratzert, Daniel Klotz, Christina Halmich,\n  Markus Holzleitner, Grey Nearing, Sepp Hochreiter and G\\\"unter Klambauer", "title": "MC-LSTM: Mass-Conserving LSTM", "comments": "13 pages (8.5 without references) + 17 pages appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of Convolutional Neural Networks (CNNs) in computer vision is\nmainly driven by their strong inductive bias, which is strong enough to allow\nCNNs to solve vision-related tasks with random weights, meaning without\nlearning. Similarly, Long Short-Term Memory (LSTM) has a strong inductive bias\ntowards storing information over time. However, many real-world systems are\ngoverned by conservation laws, which lead to the redistribution of particular\nquantities -- e.g. in physical and economical systems. Our novel\nMass-Conserving LSTM (MC-LSTM) adheres to these conservation laws by extending\nthe inductive bias of LSTM to model the redistribution of those stored\nquantities. MC-LSTMs set a new state-of-the-art for neural arithmetic units at\nlearning arithmetic operations, such as addition tasks, which have a strong\nconservation law, as the sum is constant over time. Further, MC-LSTM is applied\nto traffic forecasting, modelling a pendulum, and a large benchmark dataset in\nhydrology, where it sets a new state-of-the-art for predicting peak flows. In\nthe hydrology example, we show that MC-LSTM states correlate with real-world\nprocesses and are therefore interpretable.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 16:40:48 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 14:24:37 GMT"}, {"version": "v3", "created": "Thu, 10 Jun 2021 15:33:23 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Hoedt", "Pieter-Jan", ""], ["Kratzert", "Frederik", ""], ["Klotz", "Daniel", ""], ["Halmich", "Christina", ""], ["Holzleitner", "Markus", ""], ["Nearing", "Grey", ""], ["Hochreiter", "Sepp", ""], ["Klambauer", "G\u00fcnter", ""]]}, {"id": "2101.05201", "submitter": "Ka Man Yim", "authors": "Ka Man Yim, Jacob Leygonie", "title": "Optimisation of Spectral Wavelets for Persistence-based Graph\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A graph's spectral wavelet signature determines a filtration, and\nconsequently an associated set of extended persistence diagrams. We propose a\nframework that optimises the choice of wavelet for a dataset of graphs, such\nthat their associated persistence diagrams capture features of the graphs that\nare best suited to a given data science problem. Since the spectral wavelet\nsignature of a graph is derived from its Laplacian, our framework encodes\ngeometric properties of graphs in their associated persistence diagrams and can\nbe applied to graphs without a priori node attributes. We apply our framework\nto graph classification problems and obtain performances competitive with other\npersistence-based architectures. To provide the underlying theoretical\nfoundations, we extend the differentiability result for ordinary persistent\nhomology to extended persistent homology.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 18:34:27 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 16:43:12 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Yim", "Ka Man", ""], ["Leygonie", "Jacob", ""]]}, {"id": "2101.05234", "submitter": "Annie Marsden", "authors": "Annie Marsden, John Duchi, Gregory Valiant", "title": "On Misspecification in Prediction Problems and Robustness via Improper\n  Learning", "comments": "28 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study probabilistic prediction games when the underlying model is\nmisspecified, investigating the consequences of predicting using an incorrect\nparametric model. We show that for a broad class of loss functions and\nparametric families of distributions, the regret of playing a \"proper\"\npredictor -- one from the putative model class -- relative to the best\npredictor in the same model class has lower bound scaling at least as\n$\\sqrt{\\gamma n}$, where $\\gamma$ is a measure of the model misspecification to\nthe true distribution in terms of total variation distance. In contrast, using\nan aggregation-based (improper) learner, one can obtain regret $d \\log n$ for\nany underlying generating distribution, where $d$ is the dimension of the\nparameter; we exhibit instances in which this is unimprovable even over the\nfamily of all learners that may play distributions in the convex hull of the\nparametric family. These results suggest that simple strategies for aggregating\nmultiple learners together should be more robust, and several experiments\nconform to this hypothesis.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 17:54:08 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 21:34:04 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Marsden", "Annie", ""], ["Duchi", "John", ""], ["Valiant", "Gregory", ""]]}, {"id": "2101.05239", "submitter": "Yermek Kapushev", "authors": "Tsimboy Olga, Yermek Kapushev, Evgeny Burnaev, Ivan Oseledets", "title": "Denoising Score Matching with Random Fourier Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The density estimation is one of the core problems in statistics. Despite\nthis, existing techniques like maximum likelihood estimation are\ncomputationally inefficient due to the intractability of the normalizing\nconstant. For this reason an interest to score matching has increased being\nindependent on the normalizing constant. However, such estimator is consistent\nonly for distributions with the full space support. One of the approaches to\nmake it consistent is to add noise to the input data which is called Denoising\nScore Matching. In this work we derive analytical expression for the Denoising\nScore matching using the Kernel Exponential Family as a model distribution. The\nusage of the kernel exponential family is motivated by the richness of this\nclass of densities. To tackle the computational complexity we use Random\nFourier Features based approximation of the kernel function. The analytical\nexpression allows to drop additional regularization terms based on the\nhigher-order derivatives as they are already implicitly included. Moreover, the\nobtained expression explicitly depends on the noise variance, so the validation\nloss can be straightforwardly used to tune the noise level. Along with\nbenchmark experiments, the model was tested on various synthetic distributions\nto study the behaviour of the model in different cases. The empirical study\nshows comparable quality to the competing approaches, while the proposed method\nbeing computationally faster. The latter one enables scaling up to complex\nhigh-dimensional data.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 18:02:39 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Olga", "Tsimboy", ""], ["Kapushev", "Yermek", ""], ["Burnaev", "Evgeny", ""], ["Oseledets", "Ivan", ""]]}, {"id": "2101.05248", "submitter": "Emmanouil Vasileios Vlatakis Gkaragkounis", "authors": "Lampros Flokas, Emmanouil-Vasileios Vlatakis-Gkaragkounis, Georgios\n  Piliouras", "title": "Solving Min-Max Optimization with Hidden Structure via Gradient Descent\n  Ascent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.GT cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many recent AI architectures are inspired by zero-sum games, however, the\nbehavior of their dynamics is still not well understood. Inspired by this, we\nstudy standard gradient descent ascent (GDA) dynamics in a specific class of\nnon-convex non-concave zero-sum games, that we call hidden zero-sum games. In\nthis class, players control the inputs of smooth but possibly non-linear\nfunctions whose outputs are being applied as inputs to a convex-concave game.\nUnlike general zero-sum games, these games have a well-defined notion of\nsolution; outcomes that implement the von-Neumann equilibrium of the \"hidden\"\nconvex-concave game. We prove that if the hidden game is strictly\nconvex-concave then vanilla GDA converges not merely to local Nash, but\ntypically to the von-Neumann solution. If the game lacks strict convexity\nproperties, GDA may fail to converge to any equilibrium, however, by applying\nstandard regularization techniques we can prove convergence to a von-Neumann\nsolution of a slightly perturbed zero-sum game. Our convergence guarantees are\nnon-local, which as far as we know is a first-of-its-kind type of result in\nnon-convex non-concave games. Finally, we discuss connections of our framework\nwith generative adversarial networks.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 18:13:49 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Flokas", "Lampros", ""], ["Vlatakis-Gkaragkounis", "Emmanouil-Vasileios", ""], ["Piliouras", "Georgios", ""]]}, {"id": "2101.05265", "submitter": "Rishabh Agarwal", "authors": "Rishabh Agarwal, Marlos C. Machado, Pablo Samuel Castro, Marc G.\n  Bellemare", "title": "Contrastive Behavioral Similarity Embeddings for Generalization in\n  Reinforcement Learning", "comments": "ICLR 2021 (Spotlight). Website: https://agarwl.github.io/pse", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reinforcement learning methods trained on few environments rarely learn\npolicies that generalize to unseen environments. To improve generalization, we\nincorporate the inherent sequential structure in reinforcement learning into\nthe representation learning process. This approach is orthogonal to recent\napproaches, which rarely exploit this structure explicitly. Specifically, we\nintroduce a theoretically motivated policy similarity metric (PSM) for\nmeasuring behavioral similarity between states. PSM assigns high similarity to\nstates for which the optimal policies in those states as well as in future\nstates are similar. We also present a contrastive representation learning\nprocedure to embed any state similarity metric, which we instantiate with PSM\nto obtain policy similarity embeddings (PSEs). We demonstrate that PSEs improve\ngeneralization on diverse benchmarks, including LQR with spurious correlations,\na jumping task from pixels, and Distracting DM Control Suite.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 18:55:43 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 13:58:01 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Agarwal", "Rishabh", ""], ["Machado", "Marlos C.", ""], ["Castro", "Pablo Samuel", ""], ["Bellemare", "Marc G.", ""]]}, {"id": "2101.05328", "submitter": "Armin Lederer", "authors": "Armin Lederer, Jonas Umlauft, Sandra Hirche", "title": "Uniform Error and Posterior Variance Bounds for Gaussian Process\n  Regression with Application to Safe Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In application areas where data generation is expensive, Gaussian processes\nare a preferred supervised learning model due to their high data-efficiency.\nParticularly in model-based control, Gaussian processes allow the derivation of\nperformance guarantees using probabilistic model error bounds. To make these\napproaches applicable in practice, two open challenges must be solved i)\nExisting error bounds rely on prior knowledge, which might not be available for\nmany real-world tasks. (ii) The relationship between training data and the\nposterior variance, which mainly drives the error bound, is not well understood\nand prevents the asymptotic analysis. This article addresses these issues by\npresenting a novel uniform error bound using Lipschitz continuity and an\nanalysis of the posterior variance function for a large class of kernels.\nAdditionally, we show how these results can be used to guarantee safe control\nof an unknown dynamical system and provide numerical illustration examples.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 20:06:30 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Lederer", "Armin", ""], ["Umlauft", "Jonas", ""], ["Hirche", "Sandra", ""]]}, {"id": "2101.05346", "submitter": "Xintian Han", "authors": "Mark Goldstein, Xintian Han, Aahlad Puli, Adler J. Perotte and Rajesh\n  Ranganath", "title": "X-CAL: Explicit Calibration for Survival Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Survival analysis models the distribution of time until an event of interest,\nsuch as discharge from the hospital or admission to the ICU. When a model's\npredicted number of events within any time interval is similar to the observed\nnumber, it is called well-calibrated. A survival model's calibration can be\nmeasured using, for instance, distributional calibration (D-CALIBRATION)\n[Haider et al., 2020] which computes the squared difference between the\nobserved and predicted number of events within different time intervals.\nClassically, calibration is addressed in post-training analysis. We develop\nexplicit calibration (X-CAL), which turns D-CALIBRATION into a differentiable\nobjective that can be used in survival modeling alongside maximum likelihood\nestimation and other objectives. X-CAL allows practitioners to directly\noptimize calibration and strike a desired balance between predictive power and\ncalibration. In our experiments, we fit a variety of shallow and deep models on\nsimulated data, a survival dataset based on MNIST, on length-of-stay prediction\nusing MIMIC-III data, and on brain cancer data from The Cancer Genome Atlas. We\nshow that the models we study can be miscalibrated. We give experimental\nevidence on these datasets that X-CAL improves D-CALIBRATION without a large\ndecrease in concordance or likelihood.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 21:00:23 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Goldstein", "Mark", ""], ["Han", "Xintian", ""], ["Puli", "Aahlad", ""], ["Perotte", "Adler J.", ""], ["Ranganath", "Rajesh", ""]]}, {"id": "2101.05397", "submitter": "Xixin Wu", "authors": "Xixin Wu and Mark Gales", "title": "Should Ensemble Members Be Calibrated?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Underlying the use of statistical approaches for a wide range of applications\nis the assumption that the probabilities obtained from a statistical model are\nrepresentative of the \"true\" probability that event, or outcome, will occur.\nUnfortunately, for modern deep neural networks this is not the case, they are\noften observed to be poorly calibrated. Additionally, these deep learning\napproaches make use of large numbers of model parameters, motivating the use of\nBayesian, or ensemble approximation, approaches to handle issues with parameter\nestimation. This paper explores the application of calibration schemes to deep\nensembles from both a theoretical perspective and empirically on a standard\nimage classification task, CIFAR-100. The underlying theoretical requirements\nfor calibration, and associated calibration criteria, are first described. It\nis shown that well calibrated ensemble members will not necessarily yield a\nwell calibrated ensemble prediction, and if the ensemble prediction is well\ncalibrated its performance cannot exceed that of the average performance of the\ncalibrated ensemble members. On CIFAR-100 the impact of calibration for\nensemble prediction, and associated calibration is evaluated. Additionally the\nsituation where multiple different topologies are combined together is\ndiscussed.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 23:59:00 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Wu", "Xixin", ""], ["Gales", "Mark", ""]]}, {"id": "2101.05402", "submitter": "Anderson Ye Zhang", "authors": "Xin Chen, Anderson Y. Zhang", "title": "Optimal Clustering in Anisotropic Gaussian Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the clustering task under anisotropic Gaussian Mixture Models where\nthe covariance matrices from different clusters are unknown and are not\nnecessarily the identical matrix. We characterize the dependence of\nsignal-to-noise ratios on the cluster centers and covariance matrices and\nobtain the minimax lower bound for the clustering problem. In addition, we\npropose a computationally feasible procedure and prove it achieves the optimal\nrate within a few iterations. The proposed procedure is a hard EM type\nalgorithm, and it can also be seen as a variant of the Lloyd's algorithm that\nis adjusted to the anisotropic covariance matrices.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 00:31:52 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 04:24:38 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Chen", "Xin", ""], ["Zhang", "Anderson Y.", ""]]}, {"id": "2101.05467", "submitter": "Qizhou Wang", "authors": "Qizhou Wang, Bo Han, Tongliang Liu, Gang Niu, Jian Yang, Chen Gong", "title": "Tackling Instance-Dependent Label Noise via a Universal Probabilistic\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The drastic increase of data quantity often brings the severe decrease of\ndata quality, such as incorrect label annotations, which poses a great\nchallenge for robustly training Deep Neural Networks (DNNs). Existing learning\n\\mbox{methods} with label noise either employ ad-hoc heuristics or restrict to\nspecific noise assumptions. However, more general situations, such as\ninstance-dependent label noise, have not been fully explored, as scarce studies\nfocus on their label corruption process. By categorizing instances into\nconfusing and unconfusing instances, this paper proposes a simple yet universal\nprobabilistic model, which explicitly relates noisy labels to their instances.\nThe resultant model can be realized by DNNs, where the training procedure is\naccomplished by employing an alternating optimization algorithm. Experiments on\ndatasets with both synthetic and real-world label noise verify that the\nproposed method yields significant improvements on robustness over\nstate-of-the-art counterparts.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 05:43:51 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Wang", "Qizhou", ""], ["Han", "Bo", ""], ["Liu", "Tongliang", ""], ["Niu", "Gang", ""], ["Yang", "Jian", ""], ["Gong", "Chen", ""]]}, {"id": "2101.05490", "submitter": "Fengxiang He", "authors": "Fengxiang He, Shiye Lei, Jianmin Ji, Dacheng Tao", "title": "Neural networks behave as hash encoders: An empirical study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The input space of a neural network with ReLU-like activations is partitioned\ninto multiple linear regions, each corresponding to a specific activation\npattern of the included ReLU-like activations. We demonstrate that this\npartition exhibits the following encoding properties across a variety of deep\nlearning models: (1) {\\it determinism}: almost every linear region contains at\nmost one training example. We can therefore represent almost every training\nexample by a unique activation pattern, which is parameterized by a {\\it neural\ncode}; and (2) {\\it categorization}: according to the neural code, simple\nalgorithms, such as $K$-Means, $K$-NN, and logistic regression, can achieve\nfairly good performance on both training and test data. These encoding\nproperties surprisingly suggest that {\\it normal neural networks well-trained\nfor classification behave as hash encoders without any extra efforts.} In\naddition, the encoding properties exhibit variability in different scenarios.\n{Further experiments demonstrate that {\\it model size}, {\\it training time},\n{\\it training sample size}, {\\it regularization}, and {\\it label noise}\ncontribute in shaping the encoding properties, while the impacts of the first\nthree are dominant.} We then define an {\\it activation hash phase chart} to\nrepresent the space expanded by {model size}, training time, training sample\nsize, and the encoding properties, which is divided into three canonical\nregions: {\\it under-expressive regime}, {\\it critically-expressive regime}, and\n{\\it sufficiently-expressive regime}. The source code package is available at\n\\url{https://github.com/LeavesLei/activation-code}.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 07:50:40 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["He", "Fengxiang", ""], ["Lei", "Shiye", ""], ["Ji", "Jianmin", ""], ["Tao", "Dacheng", ""]]}, {"id": "2101.05510", "submitter": "Michael Schaub", "authors": "Michael T. Schaub and Yu Zhu and Jean-Baptiste Seby and T. Mitchell\n  Roddenberry and Santiago Segarra", "title": "Signal Processing on Higher-Order Networks: Livin' on the Edge ... and\n  Beyond", "comments": "41 pages; 8 figures", "journal-ref": "Signal Processing, Volume 187, 2021, 108149, ISSN 0165-1684", "doi": "10.1016/j.sigpro.2021.108149", "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this tutorial, we provide a didactic treatment of the emerging topic of\nsignal processing on higher-order networks. Drawing analogies from discrete and\ngraph signal processing, we introduce the building blocks for processing data\non simplicial complexes and hypergraphs, two common higher-order network\nabstractions that can incorporate polyadic relationships. We provide brief\nintroductions to simplicial complexes and hypergraphs, with a special emphasis\non the concepts needed for the processing of signals supported on these\nstructures. Specifically, we discuss Fourier analysis, signal denoising, signal\ninterpolation, node embeddings, and nonlinear processing through neural\nnetworks, using these two higher-order network models. In the context of\nsimplicial complexes, we specifically focus on signal processing using the\nHodge Laplacian matrix, a multi-relational operator that leverages the special\nstructure of simplicial complexes and generalizes desirable properties of the\nLaplacian matrix in graph signal processing. For hypergraphs, we present both\nmatrix and tensor representations, and discuss the trade-offs in adopting one\nor the other. We also highlight limitations and potential research avenues,\nboth to inform practitioners and to motivate the contribution of new\nresearchers to the area.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 09:08:26 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 17:24:25 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 10:16:00 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Schaub", "Michael T.", ""], ["Zhu", "Yu", ""], ["Seby", "Jean-Baptiste", ""], ["Roddenberry", "T. Mitchell", ""], ["Segarra", "Santiago", ""]]}, {"id": "2101.05514", "submitter": "Riikka Huusari", "authors": "Riikka Huusari, Hachem Kadri", "title": "Entangled Kernels -- Beyond Separability", "comments": null, "journal-ref": "Journal of Machine Learning Research 22 (2021) 1-40", "doi": null, "report-no": null, "categories": "cs.LG quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of operator-valued kernel learning and investigate\nthe possibility of going beyond the well-known separable kernels. Borrowing\ntools and concepts from the field of quantum computing, such as partial trace\nand entanglement, we propose a new view on operator-valued kernels and define a\ngeneral family of kernels that encompasses previously known operator-valued\nkernels, including separable and transformable kernels. Within this framework,\nwe introduce another novel class of operator-valued kernels called entangled\nkernels that are not separable. We propose an efficient two-step algorithm for\nthis framework, where the entangled kernel is learned based on a novel\nextension of kernel alignment to operator-valued kernels. We illustrate our\nalgorithm with an application to supervised dimensionality reduction, and\ndemonstrate its effectiveness with both artificial and real data for\nmulti-output regression.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 09:18:02 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Huusari", "Riikka", ""], ["Kadri", "Hachem", ""]]}, {"id": "2101.05640", "submitter": "Junya Ikemoto", "authors": "Junya Ikemoto and Toshimitsu Ushio", "title": "Continuous Deep Q-Learning with Simulator for Stabilization of Uncertain\n  Discrete-Time Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications of reinforcement learning (RL) to stabilization problems of real\nsystems are restricted since an agent needs many experiences to learn an\noptimal policy and may determine dangerous actions during its exploration. If\nwe know a mathematical model of a real system, a simulator is useful because it\npredicates behaviors of the real system using the mathematical model with a\ngiven system parameter vector. We can collect many experiences more efficiently\nthan interactions with the real system. However, it is difficult to identify\nthe system parameter vector accurately. If we have an identification error,\nexperiences obtained by the simulator may degrade the performance of the\nlearned policy. Thus, we propose a practical RL algorithm that consists of two\nstages. At the first stage, we choose multiple system parameter vectors. Then,\nwe have a mathematical model for each system parameter vector, which is called\na virtual system. We obtain optimal Q-functions for multiple virtual systems\nusing the continuous deep Q-learning algorithm. At the second stage, we\nrepresent a Q-function for the real system by a linear approximated function\nwhose basis functions are optimal Q-functions learned at the first stage. The\nagent learns the Q-function through interactions with the real system online.\nBy numerical simulations, we show the usefulness of our proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 10:21:12 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 04:08:45 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ikemoto", "Junya", ""], ["Ushio", "Toshimitsu", ""]]}, {"id": "2101.05657", "submitter": "Ankur Moitra", "authors": "Linus Hamilton, Ankur Moitra", "title": "No-go Theorem for Acceleration in the Hyperbolic Plane", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there has been significant effort to adapt the key tools and\nideas in convex optimization to the Riemannian setting. One key challenge has\nremained: Is there a Nesterov-like accelerated gradient method for geodesically\nconvex functions on a Riemannian manifold? Recent work has given partial\nanswers and the hope was that this ought to be possible.\n  Here we dash these hopes. We prove that in a noisy setting, there is no\nanalogue of accelerated gradient descent for geodesically convex functions on\nthe hyperbolic plane. Our results apply even when the noise is exponentially\nsmall. The key intuition behind our proof is short and simple: In negatively\ncurved spaces, the volume of a ball grows so fast that information about the\npast gradients is not useful in the future.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 15:11:24 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 03:01:27 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Hamilton", "Linus", ""], ["Moitra", "Ankur", ""]]}, {"id": "2101.05679", "submitter": "Aratrika Mustafi", "authors": "Aratrika Mustafi", "title": "Convex Smoothed Autoencoder-Optimal Transport model", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative modelling is a key tool in unsupervised machine learning which has\nachieved stellar success in recent years. Despite this huge success, even the\nbest generative models such as Generative Adversarial Networks (GANs) and\nVariational Autoencoders (VAEs) come with their own shortcomings, mode collapse\nand mode mixture being the two most prominent problems. In this paper we\ndevelop a new generative model capable of generating samples which resemble the\nobserved data, and is free from mode collapse and mode mixture. Our model is\ninspired by the recently proposed Autoencoder-Optimal Transport (AE-OT) model\nand tries to improve on it by addressing the problems faced by the AE-OT model\nitself, specifically with respect to the sample generation algorithm.\nTheoretical results concerning the bound on the error in approximating the\nnon-smooth Brenier potential by its smoothed estimate, and approximating the\ndiscontinuous optimal transport map by a smoothed optimal transport map\nestimate have also been established in this paper.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 15:55:20 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Mustafi", "Aratrika", ""]]}, {"id": "2101.05774", "submitter": "Nicolas Apfel", "authors": "Nicolas Apfel and Xiaoran Liang", "title": "Agglomerative Hierarchical Clustering for Selecting Valid Instrumental\n  Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an instrumental variable (IV) selection procedure which combines\nthe agglomerative hierarchical clustering method and the Hansen-Sargan\noveridentification test for selecting valid instruments for IV estimation from\na large set of candidate instruments. Some of the instruments may be invalid in\nthe sense that they may fail the exclusion restriction. We show that under the\nplurality rule, our method can achieve oracle selection and estimation results.\nCompared to the previous IV selection methods, our method has the advantages\nthat it can deal with the weak instruments problem effectively, and can be\neasily extended to settings where there are multiple endogenous regressors and\nheterogenous treatment effects. We conduct Monte Carlo simulations to examine\nthe performance of our method, and compare it with two existing methods, the\nHard Thresholding method (HT) and the Confidence Interval method (CIM). The\nsimulation results show that our method achieves oracle selection and\nestimation results in both single and multiple endogenous regressors settings\nin large samples when all the instruments are strong. Also, our method works\nwell when some of the candidate instruments are weak, outperforming HT and CIM.\nWe apply our method to the estimation of the effect of immigration on wages in\nthe US.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 18:25:26 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Apfel", "Nicolas", ""], ["Liang", "Xiaoran", ""]]}, {"id": "2101.05834", "submitter": "Sebastian Kaltenbach", "authors": "Sebastian Kaltenbach, Phaedon-Stelios Koutsourelakis", "title": "Physics-aware, probabilistic model order reduction with guaranteed\n  stability", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given (small amounts of) time-series' data from a high-dimensional,\nfine-grained, multiscale dynamical system, we propose a generative framework\nfor learning an effective, lower-dimensional, coarse-grained dynamical model\nthat is predictive of the fine-grained system's long-term evolution but also of\nits behavior under different initial conditions. We target fine-grained models\nas they arise in physical applications (e.g. molecular dynamics, agent-based\nmodels), the dynamics of which are strongly non-stationary but their transition\nto equilibrium is governed by unknown slow processes which are largely\ninaccessible by brute-force simulations. Approaches based on domain knowledge\nheavily rely on physical insight in identifying temporally slow features and\nfail to enforce the long-term stability of the learned dynamics. On the other\nhand, purely statistical frameworks lack interpretability and rely on large\namounts of expensive simulation data (long and multiple trajectories) as they\ncannot infuse domain knowledge. The generative framework proposed achieves the\naforementioned desiderata by employing a flexible prior on the complex plane\nfor the latent, slow processes, and an intermediate layer of physics-motivated\nlatent variables that reduces reliance on data and imbues inductive bias. In\ncontrast to existing schemes, it does not require the a priori definition of\nprojection operators from the fine-grained description and addresses\nsimultaneously the tasks of dimensionality reduction and model estimation. We\ndemonstrate its efficacy and accuracy in multiscale physical systems of\nparticle dynamics where probabilistic, long-term predictions of phenomena not\ncontained in the training data are produced.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 19:16:51 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Kaltenbach", "Sebastian", ""], ["Koutsourelakis", "Phaedon-Stelios", ""]]}, {"id": "2101.05928", "submitter": "Mingao Yuan", "authors": "Mingao Yuan and Qian Wen", "title": "A practical test for a planted community in heterogeneous networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the fundamental task in graph data mining is to find a planted\ncommunity(dense subgraph), which has wide application in biology, finance, spam\ndetection and so on. For a real network data, the existence of a dense subgraph\nis generally unknown. Statistical tests have been devised to testing the\nexistence of dense subgraph in a homogeneous random graph. However, many\nnetworks present extreme heterogeneity, that is, the degrees of nodes or\nvertexes don't concentrate on a typical value. The existing tests designed for\nhomogeneous random graph are not straightforwardly applicable to the\nheterogeneous case. Recently, scan test was proposed for detecting a dense\nsubgraph in heterogeneous(inhomogeneous) graph(\\cite{BCHV19}). However, the\ncomputational complexity of the scan test is generally not polynomial in the\ngraph size, which makes the test impractical for large or moderate networks. In\nthis paper, we propose a polynomial-time test that has the standard normal\ndistribution as the null limiting distribution. The power of the test is\ntheoretically investigated and we evaluate the performance of the test by\nsimulation and real data example.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 01:34:14 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Yuan", "Mingao", ""], ["Wen", "Qian", ""]]}, {"id": "2101.05974", "submitter": "Yanbang Wang", "authors": "Yanbang Wang, Yen-Yu Chang, Yunyu Liu, Jure Leskovec, Pan Li", "title": "Inductive Representation Learning in Temporal Networks via Causal\n  Anonymous Walks", "comments": "Accepted at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Temporal networks serve as abstractions of many real-world dynamic systems.\nThese networks typically evolve according to certain laws, such as the law of\ntriadic closure, which is universal in social networks. Inductive\nrepresentation learning of temporal networks should be able to capture such\nlaws and further be applied to systems that follow the same laws but have not\nbeen unseen during the training stage. Previous works in this area depend on\neither network node identities or rich edge attributes and typically fail to\nextract these laws. Here, we propose Causal Anonymous Walks (CAWs) to\ninductively represent a temporal network. CAWs are extracted by temporal random\nwalks and work as automatic retrieval of temporal network motifs to represent\nnetwork dynamics while avoiding the time-consuming selection and counting of\nthose motifs. CAWs adopt a novel anonymization strategy that replaces node\nidentities with the hitting counts of the nodes based on a set of sampled walks\nto keep the method inductive, and simultaneously establish the correlation\nbetween motifs. We further propose a neural-network model CAW-N to encode CAWs,\nand pair it with a CAW sampling strategy with constant memory and time cost to\nsupport online training and inference. CAW-N is evaluated to predict links over\n6 real temporal networks and uniformly outperforms previous SOTA methods by\naveraged 10% AUC gain in the inductive setting. CAW-N also outperforms previous\nmethods in 4 out of the 6 networks in the transductive setting.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 05:47:26 GMT"}, {"version": "v2", "created": "Sat, 20 Feb 2021 05:29:12 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 03:38:47 GMT"}, {"version": "v4", "created": "Mon, 26 Apr 2021 04:26:19 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Wang", "Yanbang", ""], ["Chang", "Yen-Yu", ""], ["Liu", "Yunyu", ""], ["Leskovec", "Jure", ""], ["Li", "Pan", ""]]}, {"id": "2101.06061", "submitter": "Bogdan Georgiev", "authors": "Bogdan Georgiev, Lukas Franken, Mayukh Mukherjee", "title": "Heating up decision boundaries: isocapacitory saturation, adversarial\n  scenarios and generalization bounds", "comments": "Accepted as conference paper at ICLR 2021. 36 pages, 16 figures,\n  comments welcome!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.MG math.PR math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the present work we study classifiers' decision boundaries via Brownian\nmotion processes in ambient data space and associated probabilistic techniques.\nIntuitively, our ideas correspond to placing a heat source at the decision\nboundary and observing how effectively the sample points warm up. We are\nlargely motivated by the search for a soft measure that sheds further light on\nthe decision boundary's geometry. En route, we bridge aspects of potential\ntheory and geometric analysis (Mazya, 2011, Grigoryan-Saloff-Coste, 2002) with\nactive fields of ML research such as adversarial examples and generalization\nbounds. First, we focus on the geometric behavior of decision boundaries in the\nlight of adversarial attack/defense mechanisms. Experimentally, we observe a\ncertain capacitory trend over different adversarial defense strategies:\ndecision boundaries locally become flatter as measured by isoperimetric\ninequalities (Ford et al, 2019); however, our more sensitive heat-diffusion\nmetrics extend this analysis and further reveal that some non-trivial geometry\ninvisible to plain distance-based methods is still preserved. Intuitively, we\nprovide evidence that the decision boundaries nevertheless retain many\npersistent \"wiggly and fuzzy\" regions on a finer scale. Second, we show how\nBrownian hitting probabilities translate to soft generalization bounds which\nare in turn connected to compression and noise stability (Arora et al, 2018),\nand these bounds are significantly stronger if the decision boundary has\ncontrolled geometric features.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 11:15:51 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Georgiev", "Bogdan", ""], ["Franken", "Lukas", ""], ["Mukherjee", "Mayukh", ""]]}, {"id": "2101.06069", "submitter": "Gaurav Kumar Nayak", "authors": "Gaurav Kumar Nayak, Konda Reddy Mopuri, Saksham Jain, Anirban\n  Chakraborty", "title": "Mining Data Impressions from Deep Models as Substitute for the\n  Unavailable Training Data", "comments": "PAMI Submission (Under Review). arXiv admin note: text overlap with\n  arXiv:1905.08114", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretrained deep models hold their learnt knowledge in the form of model\nparameters. These parameters act as \"memory\" for the trained models and help\nthem generalize well on unseen data. However, in absence of training data, the\nutility of a trained model is merely limited to either inference or better\ninitialization towards a target task. In this paper, we go further and extract\nsynthetic data by leveraging the learnt model parameters. We dub them \"Data\nImpressions\", which act as proxy to the training data and can be used to\nrealize a variety of tasks. These are useful in scenarios where only the\npretrained models are available and the training data is not shared (e.g., due\nto privacy or sensitivity concerns). We show the applicability of data\nimpressions in solving several computer vision tasks such as unsupervised\ndomain adaptation, continual learning as well as knowledge distillation. We\nalso study the adversarial robustness of lightweight models trained via\nknowledge distillation using these data impressions. Further, we demonstrate\nthe efficacy of data impressions in generating data-free Universal Adversarial\nPerturbations (UAPs) with better fooling rates. Extensive experiments performed\non benchmark datasets demonstrate competitive performance achieved using data\nimpressions in absence of original training data.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 11:37:29 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 13:41:36 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Nayak", "Gaurav Kumar", ""], ["Mopuri", "Konda Reddy", ""], ["Jain", "Saksham", ""], ["Chakraborty", "Anirban", ""]]}, {"id": "2101.06078", "submitter": "Edvard Bakhitov", "authors": "Edvard Bakhitov and Amandeep Singh", "title": "Causal Gradient Boosting: Boosted Instrumental Variable Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in the literature have demonstrated that standard supervised\nlearning algorithms are ill-suited for problems with endogenous explanatory\nvariables. To correct for the endogeneity bias, many variants of nonparameteric\ninstrumental variable regression methods have been developed. In this paper, we\npropose an alternative algorithm called boostIV that builds on the traditional\ngradient boosting algorithm and corrects for the endogeneity bias. The\nalgorithm is very intuitive and resembles an iterative version of the standard\n2SLS estimator. Moreover, our approach is data driven, meaning that the\nresearcher does not have to make a stance on neither the form of the target\nfunction approximation nor the choice of instruments. We demonstrate that our\nestimator is consistent under mild conditions. We carry out extensive Monte\nCarlo simulations to demonstrate the finite sample performance of our algorithm\ncompared to other recently developed methods. We show that boostIV is at worst\non par with the existing methods and on average significantly outperforms them.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 11:54:25 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Bakhitov", "Edvard", ""], ["Singh", "Amandeep", ""]]}, {"id": "2101.06154", "submitter": "Kaifeng Bu", "authors": "Kaifeng Bu, Dax Enshan Koh, Lu Li, Qingxian Luo, Yaobo Zhang", "title": "On the statistical complexity of quantum circuits", "comments": "6+19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In theoretical machine learning, the statistical complexity is a notion that\nmeasures the richness of a hypothesis space. In this work, we apply a\nparticular measure of statistical complexity, namely the Rademacher complexity,\nto the quantum circuit model in quantum computation and study how the\nstatistical complexity depends on various quantum circuit parameters. In\nparticular, we investigate the dependence of the statistical complexity on the\nresources, depth, width, and the number of input and output registers of a\nquantum circuit. To study how the statistical complexity scales with resources\nin the circuit, we introduce a resource measure of magic based on the $(p,q)$\ngroup norm, which quantifies the amount of magic in the quantum channels\nassociated with the circuit. These dependencies are investigated in the\nfollowing two settings: (i) where the entire quantum circuit is treated as a\nsingle quantum channel, and (ii) where each layer of the quantum circuit is\ntreated as a separate quantum channel. The bounds we obtain can be used to\nconstrain the capacity of quantum neural networks in terms of their depths and\nwidths as well as the resources in the network.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 14:55:55 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Bu", "Kaifeng", ""], ["Koh", "Dax Enshan", ""], ["Li", "Lu", ""], ["Luo", "Qingxian", ""], ["Zhang", "Yaobo", ""]]}, {"id": "2101.06233", "submitter": "Tomoya Sakai", "authors": "Tomoya Sakai, Naoto Ohsaka", "title": "Predictive Optimization with Zero-Shot Domain Adaptation", "comments": "SDM2021. Full version including appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction in a new domain without any training sample, called zero-shot\ndomain adaptation (ZSDA), is an important task in domain adaptation. While\nprediction in a new domain has gained much attention in recent years, in this\npaper, we investigate another potential of ZSDA. Specifically, instead of\npredicting responses in a new domain, we find a description of a new domain\ngiven a prediction. The task is regarded as predictive optimization, but\nexisting predictive optimization methods have not been extended to handling\nmultiple domains. We propose a simple framework for predictive optimization\nwith ZSDA and analyze the condition in which the optimization problem becomes\nconvex optimization. We also discuss how to handle the interaction of\ncharacteristics of a domain in predictive optimization. Through numerical\nexperiments, we demonstrate the potential usefulness of our proposed framework.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 17:35:12 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Sakai", "Tomoya", ""], ["Ohsaka", "Naoto", ""]]}, {"id": "2101.06255", "submitter": "Daniel Moyer", "authors": "Daniel Moyer and Polina Golland", "title": "Harmonization and the Worst Scanner Syndrome", "comments": "Med-NeurIPS 2020 Workshop Paper, updated 4/2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that for a wide class of harmonization/domain-invariance schemes\nseveral undesirable properties are unavoidable. If a predictive machine is made\ninvariant to a set of domains, the accuracy of the output predictions (as\nmeasured by mutual information) is limited by the domain with the least amount\nof information to begin with. If a real label value is highly informative about\nthe source domain, it cannot be accurately predicted by an invariant predictor.\nThese results are simple and intuitive, but we believe that it is beneficial to\nstate them for medical imaging harmonization.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 18:41:41 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 17:16:42 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Moyer", "Daniel", ""], ["Golland", "Polina", ""]]}, {"id": "2101.06296", "submitter": "Nathan Wycoff", "authors": "Nathan Wycoff, Micka\\\"el Binois, Robert B. Gramacy", "title": "Sensitivity Prewarping for Local Surrogate Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the continual effort to improve product quality and decrease operations\ncosts, computational modeling is increasingly being deployed to determine\nfeasibility of product designs or configurations. Surrogate modeling of these\ncomputer experiments via local models, which induce sparsity by only\nconsidering short range interactions, can tackle huge analyses of complicated\ninput-output relationships. However, narrowing focus to local scale means that\nglobal trends must be re-learned over and over again. In this article, we\npropose a framework for incorporating information from a global sensitivity\nanalysis into the surrogate model as an input rotation and rescaling\npreprocessing step. We discuss the relationship between several sensitivity\nanalysis methods based on kernel regression before describing how they give\nrise to a transformation of the input variables. Specifically, we perform an\ninput warping such that the \"warped simulator\" is equally sensitive to all\ninput directions, freeing local models to focus on local dynamics. Numerical\nexperiments on observational data and benchmark test functions, including a\nhigh-dimensional computer simulator from the automotive industry, provide\nempirical validation.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 20:42:32 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Wycoff", "Nathan", ""], ["Binois", "Micka\u00ebl", ""], ["Gramacy", "Robert B.", ""]]}, {"id": "2101.06309", "submitter": "Adel Javanmard", "authors": "Mohammad Mehrabi, Adel Javanmard, Ryan A. Rossi, Anup Rao and Tung Mai", "title": "Fundamental Tradeoffs in Distributionally Adversarial Training", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training is among the most effective techniques to improve the\nrobustness of models against adversarial perturbations. However, the full\neffect of this approach on models is not well understood. For example, while\nadversarial training can reduce the adversarial risk (prediction error against\nan adversary), it sometimes increase standard risk (generalization error when\nthere is no adversary). Even more, such behavior is impacted by various\nelements of the learning problem, including the size and quality of training\ndata, specific forms of adversarial perturbations in the input, model\noverparameterization, and adversary's power, among others. In this paper, we\nfocus on \\emph{distribution perturbing} adversary framework wherein the\nadversary can change the test distribution within a neighborhood of the\ntraining data distribution. The neighborhood is defined via Wasserstein\ndistance between distributions and the radius of the neighborhood is a measure\nof adversary's manipulative power. We study the tradeoff between standard risk\nand adversarial risk and derive the Pareto-optimal tradeoff, achievable over\nspecific classes of models, in the infinite data limit with features dimension\nkept fixed. We consider three learning settings: 1) Regression with the class\nof linear models; 2) Binary classification under the Gaussian mixtures data\nmodel, with the class of linear classifiers; 3) Regression with the class of\nrandom features model (which can be equivalently represented as two-layer\nneural network with random first-layer weights). We show that a tradeoff\nbetween standard and adversarial risk is manifested in all three settings. We\nfurther characterize the Pareto-optimal tradeoff curves and discuss how a\nvariety of factors, such as features correlation, adversary's power or the\nwidth of two-layer neural network would affect this tradeoff.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 21:59:18 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Mehrabi", "Mohammad", ""], ["Javanmard", "Adel", ""], ["Rossi", "Ryan A.", ""], ["Rao", "Anup", ""], ["Mai", "Tung", ""]]}, {"id": "2101.06348", "submitter": "Marcos Carreira", "authors": "Marcos Costa Santos Carreira", "title": "Exponential Kernels with Latency in Hawkes Processes: Applications in\n  Finance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Tick library allows researchers in market microstructure to simulate and\nlearn Hawkes process in high-frequency data, with optimized parametric and\nnon-parametric learners. But one challenge is to take into account the correct\ncausality of order book events considering latency: the only way one order book\nevent can influence another is if the time difference between them (by the\ncentral order book timestamps) is greater than the minimum amount of time for\nan event to be (i) published in the order book, (ii) reach the trader\nresponsible for the second event, (iii) influence the decision (processing time\nat the trader) and (iv) the 2nd event reach the order book and be processed.\nFor this we can use exponential kernels shifted to the right by the latency\namount. We derive the expression for the log-likelihood to be minimized for the\n1-D and the multidimensional cases, and test this method with simulated data\nand real data. On real data we find that, although not all decays are the same,\nthe latency itself will determine most of the decays. We also show how the\ndecays are related to the latency. Code is available on GitHub at\nhttps://github.com/MarcosCarreira/Hawkes-With-Latency.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 01:57:59 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Carreira", "Marcos Costa Santos", ""]]}, {"id": "2101.06388", "submitter": "Ruizhong Miao", "authors": "Ruizhong Miao and Tianxi Li", "title": "Informative core identification in complex networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In network analysis, the core structure of modeling interest is usually\nhidden in a larger network in which most structures are not informative. The\nnoise and bias introduced by the non-informative component in networks can\nobscure the salient structure and limit many network modeling procedures'\neffectiveness. This paper introduces a novel core-periphery model for the\nnon-informative periphery structure of networks without imposing a specific\nform for the informative core structure. We propose spectral algorithms for\ncore identification as a data preprocessing step for general downstream network\nanalysis tasks based on the model. The algorithm enjoys a strong theoretical\nguarantee of accuracy and is scalable for large networks. We evaluate the\nproposed method by extensive simulation studies demonstrating various\nadvantages over many traditional core-periphery methods. The method is applied\nto extract the informative core structure from a citation network and give more\ninformative results in the downstream hierarchical community detection.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 07:19:21 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Miao", "Ruizhong", ""], ["Li", "Tianxi", ""]]}, {"id": "2101.06417", "submitter": "Shaopeng Fu", "authors": "Shaopeng Fu, Fengxiang He, Yue Xu, Dacheng Tao", "title": "Bayesian Inference Forgetting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The right to be forgotten has been legislated in many countries but the\nenforcement in machine learning would cause unbearable costs: companies may\nneed to delete whole models learned from massive resources due to single\nindividual requests. Existing works propose to remove the knowledge learned\nfrom the requested data via its influence function which is no longer naturally\nwell-defined in Bayesian inference. This paper proposes a {\\it Bayesian\ninference forgetting} (BIF) framework to realize the right to be forgotten in\nBayesian inference. In the BIF framework, we develop forgetting algorithms for\nvariational inference and Markov chain Monte Carlo. We show that our algorithms\ncan provably remove the influence of single datums on the learned models.\nTheoretical analysis demonstrates that our algorithms have guaranteed\ngeneralizability. Experiments of Gaussian mixture models on the synthetic\ndataset and Bayesian neural networks on the real-world data verify the\nfeasibility of our methods. The source code package is available at\n\\url{https://github.com/fshp971/BIF}.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 09:52:51 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 09:05:14 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Fu", "Shaopeng", ""], ["He", "Fengxiang", ""], ["Xu", "Yue", ""], ["Tao", "Dacheng", ""]]}, {"id": "2101.06453", "submitter": "Anand Jerry George", "authors": "Anand Jerry George, Navin Kashyap", "title": "An MCMC Method to Sample from Lattice Distributions", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Markov Chain Monte Carlo (MCMC) algorithm to generate samples\nfrom probability distributions supported on a $d$-dimensional lattice $\\Lambda\n= \\mathbf{B}\\mathbb{Z}^d$, where $\\mathbf{B}$ is a full-rank matrix.\nSpecifically, we consider lattice distributions $P_\\Lambda$ in which the\nprobability at a lattice point is proportional to a given probability density\nfunction, $f$, evaluated at that point. To generate samples from $P_\\Lambda$,\nit suffices to draw samples from a pull-back measure $P_{\\mathbb{Z}^d}$ defined\non the integer lattice. The probability of an integer lattice point under\n$P_{\\mathbb{Z}^d}$ is proportional to the density function $\\pi =\n|\\det(\\mathbf{B})|f\\circ \\mathbf{B}$. The algorithm we present in this paper\nfor sampling from $P_{\\mathbb{Z}^d}$ is based on the Metropolis-Hastings\nframework. In particular, we use $\\pi$ as the proposal distribution and\ncalculate the Metropolis-Hastings acceptance ratio for a well-chosen target\ndistribution. We can use any method, denoted by ALG, that ideally draws samples\nfrom the probability density $\\pi$, to generate a proposed state. The target\ndistribution is a piecewise sigmoidal distribution, chosen such that the\ncoordinate-wise rounding of a sample drawn from the target distribution gives a\nsample from $P_{\\mathbb{Z}^d}$. When ALG is ideal, we show that our algorithm\nis uniformly ergodic if $-\\log(\\pi)$ satisfies a gradient Lipschitz condition.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 15:01:53 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 12:23:37 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["George", "Anand Jerry", ""], ["Kashyap", "Navin", ""]]}, {"id": "2101.06482", "submitter": "Federica Ferretti", "authors": "Federica Ferretti, Victor Chard\\`es, Thierry Mora, Aleksandra M\n  Walczak, Irene Giardina", "title": "A Renormalization Group Approach to Connect Discrete- and\n  Continuous-Time Descriptions of Gaussian Processes", "comments": "5 pages, 2 figures; 14 pages - Supplemental Material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.stat-mech cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying correct discretization schemes of continuous stochastic processes\nis an important task, which is needed to infer model parameters from\nexperimental observations. Motivated by the observation that consistent\ndiscretizations of continuous models should be invariant under temporal coarse\ngraining, we derive an explicit Renormalization Group transformation on linear\nstochastic time series and show that the Renormalization Group fixed points\ncorrespond to discretizations of naturally occuring physical dynamics. Our\nfixed point analysis explains why standard embedding procedures do not allow\nfor reconstructing hidden Markov dynamics, and why the Euler-Maruyama scheme\napplied to underdamped Langevin equations works for numerical integration, but\nnot to derive the likelihood of a partially observed process in the context of\nparametric inference.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 17:11:02 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 11:10:06 GMT"}, {"version": "v3", "created": "Fri, 28 May 2021 10:34:15 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Ferretti", "Federica", ""], ["Chard\u00e8s", "Victor", ""], ["Mora", "Thierry", ""], ["Walczak", "Aleksandra M", ""], ["Giardina", "Irene", ""]]}, {"id": "2101.06536", "submitter": "Chirag Nagpal", "authors": "Chirag Nagpal, Steve Yadlowsky, Negar Rostamzadeh and Katherine Heller", "title": "Deep Cox Mixtures for Survival Regression", "comments": "Machine Learning for Healthcare Conference, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Survival analysis is a challenging variation of regression modeling because\nof the presence of censoring, where the outcome measurement is only partially\nknown, due to, for example, loss to follow up. Such problems come up frequently\nin medical applications, making survival analysis a key endeavor in\nbiostatistics and machine learning for healthcare, with Cox regression models\nbeing amongst the most commonly employed models. We describe a new approach for\nsurvival analysis regression models, based on learning mixtures of Cox\nregressions to model individual survival distributions. We propose an\napproximation to the Expectation Maximization algorithm for this model that\ndoes hard assignments to mixture groups to make optimization efficient. In each\ngroup assignment, we fit the hazard ratios within each group using deep neural\nnetworks, and the baseline hazard for each mixture component\nnon-parametrically.\n  We perform experiments on multiple real world datasets, and look at the\nmortality rates of patients across ethnicity and gender. We emphasize the\nimportance of calibration in healthcare settings and demonstrate that our\napproach outperforms classical and modern survival analysis baselines, both in\nterms of discriminative performance and calibration, with large gains in\nperformance on the minority demographics.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 22:41:22 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 15:28:29 GMT"}, {"version": "v3", "created": "Thu, 22 Jul 2021 11:11:32 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Nagpal", "Chirag", ""], ["Yadlowsky", "Steve", ""], ["Rostamzadeh", "Negar", ""], ["Heller", "Katherine", ""]]}, {"id": "2101.06590", "submitter": "Jingkang Wang", "authors": "Jingkang Wang, Mengye Ren, Ilija Bogunovic, Yuwen Xiong, Raquel\n  Urtasun", "title": "Cost-Efficient Online Hyperparameter Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on hyperparameters optimization (HPO) has shown the possibility\nof training certain hyperparameters together with regular parameters. However,\nthese online HPO algorithms still require running evaluation on a set of\nvalidation examples at each training step, steeply increasing the training\ncost. To decide when to query the validation loss, we model online HPO as a\ntime-varying Bayesian optimization problem, on top of which we propose a novel\n\\textit{costly feedback} setting to capture the concept of the query cost.\nUnder this setting, standard algorithms are cost-inefficient as they evaluate\non the validation set at every round. In contrast, the cost-efficient GP-UCB\nalgorithm proposed in this paper queries the unknown function only when the\nmodel is less confident about current decisions. We evaluate our proposed\nalgorithm by tuning hyperparameters online for VGG and ResNet on CIFAR-10 and\nImageNet100. Our proposed online HPO algorithm reaches human expert-level\nperformance within a single run of the experiment, while incurring only modest\ncomputational overhead compared to regular training.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 04:55:30 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Wang", "Jingkang", ""], ["Ren", "Mengye", ""], ["Bogunovic", "Ilija", ""], ["Xiong", "Yuwen", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.06640", "submitter": "Hrayr Harutyunyan", "authors": "Hrayr Harutyunyan, Alessandro Achille, Giovanni Paolini, Orchid\n  Majumder, Avinash Ravichandran, Rahul Bhotika, Stefano Soatto", "title": "Estimating informativeness of samples with Smooth Unique Information", "comments": "ICLR 2021, 22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a notion of information that an individual sample provides to the\ntraining of a neural network, and we specialize it to measure both how much a\nsample informs the final weights and how much it informs the function computed\nby the weights. Though related, we show that these quantities have a\nqualitatively different behavior. We give efficient approximations of these\nquantities using a linearized network and demonstrate empirically that the\napproximation is accurate for real-world architectures, such as pre-trained\nResNets. We apply these measures to several problems, such as dataset\nsummarization, analysis of under-sampled classes, comparison of informativeness\nof different data sources, and detection of adversarial and corrupted examples.\nOur work generalizes existing frameworks but enjoys better computational\nproperties for heavily over-parametrized models, which makes it possible to\napply it to real-world networks.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 10:29:29 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 08:24:40 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Harutyunyan", "Hrayr", ""], ["Achille", "Alessandro", ""], ["Paolini", "Giovanni", ""], ["Majumder", "Orchid", ""], ["Ravichandran", "Avinash", ""], ["Bhotika", "Rahul", ""], ["Soatto", "Stefano", ""]]}, {"id": "2101.06662", "submitter": "Pengzhou (Abel) Wu", "authors": "Pengzhou Wu and Kenji Fukumizu", "title": "Intact-VAE: Estimating Treatment Effects under Unobserved Confounding", "comments": "A major update of the ICLR submission. About 80% of the paper is\n  rewritten, and the theoretical part is totally new. For detailed notes on the\n  update, see https://openreview.net/forum?id=D3TNqCspFpM", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an important problem of causal inference, we discuss the identification\nand estimation of treatment effects under unobserved confounding. Representing\nthe confounder as a latent variable, we propose Intact-VAE, a new variant of\nvariational autoencoder (VAE), motivated by the prognostic score that is\nsufficient for identifying treatment effects. We theoretically show that, under\ncertain settings, treatment effects are identified by our model, and further,\nbased on the identifiability of our model (i.e., determinacy of\nrepresentation), our VAE is a consistent estimator with representation balanced\nfor treatment groups. Experiments on (semi-)synthetic datasets show\nstate-of-the-art performance under diverse settings.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 13:03:44 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 01:12:31 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Wu", "Pengzhou", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "2101.06742", "submitter": "Simon Suo", "authors": "Shenlong Wang, Simon Suo, Wei-Chiu Ma, Andrei Pokrovsky, Raquel\n  Urtasun", "title": "Deep Parametric Continuous Convolutional Neural Networks", "comments": "Accepted by CVPR 2018", "journal-ref": null, "doi": "10.1109/CVPR.2018.00274", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Standard convolutional neural networks assume a grid structured input is\navailable and exploit discrete convolutions as their fundamental building\nblocks. This limits their applicability to many real-world applications. In\nthis paper we propose Parametric Continuous Convolution, a new learnable\noperator that operates over non-grid structured data. The key idea is to\nexploit parameterized kernel functions that span the full continuous vector\nspace. This generalization allows us to learn over arbitrary data structures as\nlong as their support relationship is computable. Our experiments show\nsignificant improvement over the state-of-the-art in point cloud segmentation\nof indoor and outdoor scenes, and lidar motion estimation of driving scenes.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 18:28:23 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Wang", "Shenlong", ""], ["Suo", "Simon", ""], ["Ma", "Wei-Chiu", ""], ["Pokrovsky", "Andrei", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.06763", "submitter": "Theodoulos Rodosthenous", "authors": "Theodoulos Rodosthenous, Vahid Shahrezaei and Marina Evangelou", "title": "Multi-view Data Visualisation via Manifold Learning", "comments": "27 pages, 12 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Non-linear dimensionality reduction can be performed by \\textit{manifold\nlearning} approaches, such as Stochastic Neighbour Embedding (SNE), Locally\nLinear Embedding (LLE) and Isometric Feature Mapping (ISOMAP). These methods\naim to produce two or three latent embeddings, primarily to visualise the data\nin intelligible representations. This manuscript proposes extensions of\nStudent's t-distributed SNE (t-SNE), LLE and ISOMAP, for dimensionality\nreduction and visualisation of multi-view data. Multi-view data refers to\nmultiple types of data generated from the same samples. The proposed multi-view\napproaches provide more comprehensible projections of the samples compared to\nthe ones obtained by visualising each data-view separately. Commonly\nvisualisation is used for identifying underlying patterns within the samples.\nBy incorporating the obtained low-dimensional embeddings from the multi-view\nmanifold approaches into the K-means clustering algorithm, it is shown that\nclusters of the samples are accurately identified. Through the analysis of real\nand synthetic data the proposed multi-SNE approach is found to have the best\nperformance. We further illustrate the applicability of the multi-SNE approach\nfor the analysis of multi-omics single-cell data, where the aim is to visualise\nand identify cell heterogeneity and cell types in biological tissues relevant\nto health and disease.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 19:54:36 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 11:22:41 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Rodosthenous", "Theodoulos", ""], ["Shahrezaei", "Vahid", ""], ["Evangelou", "Marina", ""]]}, {"id": "2101.06808", "submitter": "Youssef Diouane", "authors": "Youssef Diouane and Victor Picheny and Rodolphe Le Riche and Alexandre\n  Scotto Di Perrotolo", "title": "TREGO: a Trust-Region Framework for Efficient Global Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Efficient Global Optimization (EGO) is the canonical form of Bayesian\noptimization that has been successfully applied to solve global optimization of\nexpensive-to-evaluate black-box problems. However, EGO struggles to scale with\ndimension, and offers limited theoretical guarantees. In this work, we propose\nand analyze a trust-region-like EGO method (TREGO). TREGO alternates between\nregular EGO steps and local steps within a trust region. By following a\nclassical scheme for the trust region (based on a sufficient decrease\ncondition), we demonstrate that our algorithm enjoys strong global convergence\nproperties, while departing from EGO only for a subset of optimization steps.\nUsing extensive numerical experiments based on the well-known COCO benchmark,\nwe first analyze the sensitivity of TREGO to its own parameters, then show that\nthe resulting algorithm is consistently outperforming EGO and getting\ncompetitive with other state-of-the-art global optimization methods. The method\nis available both in the R package DiceOptim\n(https://cran.r-project.org/package=DiceOptim) and Python library trieste\n(https://secondmind-labs.github.io/trieste/).\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 00:14:40 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 10:26:21 GMT"}, {"version": "v3", "created": "Tue, 2 Feb 2021 12:05:29 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Diouane", "Youssef", ""], ["Picheny", "Victor", ""], ["Riche", "Rodolphe Le", ""], ["Di Perrotolo", "Alexandre Scotto", ""]]}, {"id": "2101.06811", "submitter": "Farhad Farokhi", "authors": "Farhad Farokhi", "title": "Optimal Pre-Processing to Achieve Fairness and Its Relationship with\n  Total Variation Barycenter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use disparate impact, i.e., the extent that the probability of observing\nan output depends on protected attributes such as race and gender, to measure\nfairness. We prove that disparate impact is upper bounded by the total\nvariation distance between the distribution of the inputs given the protected\nattributes. We then use pre-processing, also known as data repair, to enforce\nfairness. We show that utility degradation, i.e., the extent that the success\nof a forecasting model changes by pre-processing the data, is upper bounded by\nthe total variation distance between the distribution of the data before and\nafter pre-processing. Hence, the problem of finding the optimal pre-processing\nregiment for enforcing fairness can be cast as minimizing total variations\ndistance between the distribution of the data before and after pre-processing\nsubject to a constraint on the total variation distance between the\ndistribution of the inputs given protected attributes. This problem is a linear\nprogram that can be efficiently solved. We show that this problem is intimately\nrelated to finding the barycenter (i.e., center of mass) of two distributions\nwhen distances in the probability space are measured by total variation\ndistance. We also investigate the effect of differential privacy on fairness\nusing the proposed the total variation distances. We demonstrate the results\nusing numerical experimentation with a practice dataset.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 00:20:32 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Farokhi", "Farhad", ""]]}, {"id": "2101.06823", "submitter": "Yizhen Xu", "authors": "Yizhen Xu, Joseph W. Hogan, Michael J. Daniels, Rami Kantor, Ann\n  Mwangi", "title": "Inference for BART with Multinomial Outcomes", "comments": "23 pages, 12 tables, 6 figures, with appendix, 49 pages total", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The multinomial probit Bayesian additive regression trees (MPBART) framework\nwas proposed by Kindo et al. (KD), approximating the latent utilities in the\nmultinomial probit (MNP) model with BART (Chipman et al. 2010). Compared to\nmultinomial logistic models, MNP does not assume independent alternatives and\nthe correlation structure among alternatives can be specified through\nmultivariate Gaussian distributed latent utilities. We introduce two new\nalgorithms for fitting the MPBART and show that the theoretical mixing rates of\nour proposals are equal or superior to the existing algorithm in KD. Through\nsimulations, we explore the robustness of the methods to the choice of\nreference level, imbalance in outcome frequencies, and the specifications of\nprior hyperparameters for the utility error term. The work is motivated by the\napplication of generating posterior predictive distributions for mortality and\nengagement in care among HIV-positive patients based on electronic health\nrecords (EHRs) from the Academic Model Providing Access to Healthcare (AMPATH)\nin Kenya. In both the application and simulations, we observe better\nperformance using our proposals as compared to KD in terms of MCMC convergence\nrate and posterior predictive accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 00:59:03 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Xu", "Yizhen", ""], ["Hogan", "Joseph W.", ""], ["Daniels", "Michael J.", ""], ["Kantor", "Rami", ""], ["Mwangi", "Ann", ""]]}, {"id": "2101.06861", "submitter": "Jie Chen", "authors": "Chao Shang, Jie Chen, Jinbo Bi", "title": "Discrete Graph Structure Learning for Forecasting Multiple Time Series", "comments": "ICLR 2021. Code is available at https://github.com/chaoshangcs/GTS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series forecasting is an extensively studied subject in statistics,\neconomics, and computer science. Exploration of the correlation and causation\namong the variables in a multivariate time series shows promise in enhancing\nthe performance of a time series model. When using deep neural networks as\nforecasting models, we hypothesize that exploiting the pairwise information\namong multiple (multivariate) time series also improves their forecast. If an\nexplicit graph structure is known, graph neural networks (GNNs) have been\ndemonstrated as powerful tools to exploit the structure. In this work, we\npropose learning the structure simultaneously with the GNN if the graph is\nunknown. We cast the problem as learning a probabilistic graph model through\noptimizing the mean performance over the graph distribution. The distribution\nis parameterized by a neural network so that discrete graphs can be sampled\ndifferentiably through reparameterization. Empirical evaluations show that our\nmethod is simpler, more efficient, and better performing than a recently\nproposed bilevel learning approach for graph structure learning, as well as a\nbroad array of forecasting models, either deep or non-deep learning based, and\ngraph or non-graph based.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 03:36:33 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 20:21:28 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 03:42:14 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Shang", "Chao", ""], ["Chen", "Jie", ""], ["Bi", "Jinbo", ""]]}, {"id": "2101.06887", "submitter": "Dmitry Krotov", "authors": "Yuchen Liang, Chaitanya K. Ryali, Benjamin Hoover, Leopold Grinberg,\n  Saket Navlakha, Mohammed J. Zaki, Dmitry Krotov", "title": "Can a Fruit Fly Learn Word Embeddings?", "comments": "Accepted for publication at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mushroom body of the fruit fly brain is one of the best studied systems\nin neuroscience. At its core it consists of a population of Kenyon cells, which\nreceive inputs from multiple sensory modalities. These cells are inhibited by\nthe anterior paired lateral neuron, thus creating a sparse high dimensional\nrepresentation of the inputs. In this work we study a mathematical\nformalization of this network motif and apply it to learning the correlational\nstructure between words and their context in a corpus of unstructured text, a\ncommon natural language processing (NLP) task. We show that this network can\nlearn semantic representations of words and can generate both static and\ncontext-dependent word embeddings. Unlike conventional methods (e.g., BERT,\nGloVe) that use dense representations for word embedding, our algorithm encodes\nsemantic meaning of words and their context in the form of sparse binary hash\ncodes. The quality of the learned representations is evaluated on word\nsimilarity analysis, word-sense disambiguation, and document classification. It\nis shown that not only can the fruit fly network motif achieve performance\ncomparable to existing methods in NLP, but, additionally, it uses only a\nfraction of the computational resources (shorter training time and smaller\nmemory footprint).\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 05:41:50 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 19:50:25 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Liang", "Yuchen", ""], ["Ryali", "Chaitanya K.", ""], ["Hoover", "Benjamin", ""], ["Grinberg", "Leopold", ""], ["Navlakha", "Saket", ""], ["Zaki", "Mohammed J.", ""], ["Krotov", "Dmitry", ""]]}, {"id": "2101.06967", "submitter": "Alexandre Thiery", "authors": "Atin Ghosh and Alexandre H. Thiery", "title": "On Data-Augmentation and Consistency-Based Semi-Supervised Learning", "comments": null, "journal-ref": "ICLR 2021", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently proposed consistency-based Semi-Supervised Learning (SSL) methods\nsuch as the $\\Pi$-model, temporal ensembling, the mean teacher, or the virtual\nadversarial training, have advanced the state of the art in several SSL tasks.\nThese methods can typically reach performances that are comparable to their\nfully supervised counterparts while using only a fraction of labelled examples.\nDespite these methodological advances, the understanding of these methods is\nstill relatively limited. In this text, we analyse (variations of) the\n$\\Pi$-model in settings where analytically tractable results can be obtained.\nWe establish links with Manifold Tangent Classifiers and demonstrate that the\nquality of the perturbations is key to obtaining reasonable SSL performances.\nImportantly, we propose a simple extension of the Hidden Manifold Model that\nnaturally incorporates data-augmentation schemes and offers a framework for\nunderstanding and experimenting with SSL methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 10:12:31 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Ghosh", "Atin", ""], ["Thiery", "Alexandre H.", ""]]}, {"id": "2101.06986", "submitter": "Katarina Domijan PhD", "authors": "Catherine B. Hurley, Mark O'Connell, Katarina Domijan", "title": "Interactive slice visualization for exploring machine learning models", "comments": "35 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning models fit complex algorithms to arbitrarily large datasets.\nThese algorithms are well-known to be high on performance and low on\ninterpretability. We use interactive visualization of slices of predictor space\nto address the interpretability deficit; in effect opening up the black-box of\nmachine learning algorithms, for the purpose of interrogating, explaining,\nvalidating and comparing model fits. Slices are specified directly through\ninteraction, or using various touring algorithms designed to visit\nhigh-occupancy sections or regions where the model fits have interesting\nproperties. The methods presented here are implemented in the R package\n\\pkg{condvis2}.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 10:47:53 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Hurley", "Catherine B.", ""], ["O'Connell", "Mark", ""], ["Domijan", "Katarina", ""]]}, {"id": "2101.07012", "submitter": "Hisham Husain", "authors": "Hisham Husain and Kamil Ciosek and Ryota Tomioka", "title": "Regularized Policies are Reward Robust", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entropic regularization of policies in Reinforcement Learning (RL) is a\ncommonly used heuristic to ensure that the learned policy explores the\nstate-space sufficiently before overfitting to a local optimal policy. The\nprimary motivation for using entropy is for exploration and disambiguating\noptimal policies; however, the theoretical effects are not entirely understood.\nIn this work, we study the more general regularized RL objective and using\nFenchel duality; we derive the dual problem which takes the form of an\nadversarial reward problem. In particular, we find that the optimal policy\nfound by a regularized objective is precisely an optimal policy of a\nreinforcement learning problem under a worst-case adversarial reward. Our\nresult allows us to reinterpret the popular entropic regularization scheme as a\nform of robustification. Furthermore, due to the generality of our results, we\napply to other existing regularization schemes. Our results thus give insights\ninto the effects of regularization of policies and deepen our understanding of\nexploration through robust rewards at large.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 11:38:47 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Husain", "Hisham", ""], ["Ciosek", "Kamil", ""], ["Tomioka", "Ryota", ""]]}, {"id": "2101.07023", "submitter": "Laura Scarabosio", "authors": "Laura Scarabosio", "title": "Deep neural network surrogates for non-smooth quantities of interest in\n  shape uncertainty quantification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the point evaluation of the solution to interface problems with\ngeometric uncertainties, where the uncertainty in the obstacle is described by\na high-dimensional parameter $\\boldsymbol{y}\\in[-1,1]^d$, $d\\in\\mathbb{N}$. We\nfocus in particular on an elliptic interface problem and a Helmholtz\ntransmission problem. Point values of the solution in the physical domain\ndepend in general non-smoothly on the high-dimensional parameter, posing a\nchallenge when one is interested in building surrogates. Indeed, high-order\nmethods show poor convergence rates, while methods which are able to track\ndiscontinuities usually suffer from the so-called curse of dimensionality. For\nthis reason, in this work we propose to build surrogates for point evaluation\nusing deep neural networks. We provide a theoretical justification for why we\nexpect neural networks to provide good surrogates. Furthermore, we present\nextensive numerical experiments showing their good performance in practice. We\nobserve in particular that neural networks do not suffer from the curse of\ndimensionality, and we study the dependence of the error on the number of point\nevaluations (that is, the number of discontinuities in the parameter space), as\nwell as on several modeling parameters, such as the contrast between the two\nmaterials and, for the Helmholtz transmission problem, the wavenumber.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 12:02:57 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Scarabosio", "Laura", ""]]}, {"id": "2101.07046", "submitter": "Maximilian Soelch", "authors": "Justin Bayer, Maximilian Soelch, Atanas Mirchev, Baris Kayalibay,\n  Patrick van der Smagt", "title": "Mind the Gap when Conditioning Amortised Inference in Sequential\n  Latent-Variable Models", "comments": "Published as a conference paper at ICLR 2021 (Poster)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Amortised inference enables scalable learning of sequential latent-variable\nmodels (LVMs) with the evidence lower bound (ELBO). In this setting,\nvariational posteriors are often only partially conditioned. While the true\nposteriors depend, e.g., on the entire sequence of observations, approximate\nposteriors are only informed by past observations. This mimics the Bayesian\nfilter -- a mixture of smoothing posteriors. Yet, we show that the ELBO\nobjective forces partially-conditioned amortised posteriors to approximate\nproducts of smoothing posteriors instead. Consequently, the learned generative\nmodel is compromised. We demonstrate these theoretical findings in three\nscenarios: traffic flow, handwritten digits, and aerial vehicle dynamics. Using\nfully-conditioned approximate posteriors, performance improves in terms of\ngenerative modelling and multi-step prediction.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 12:53:39 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 16:52:01 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Bayer", "Justin", ""], ["Soelch", "Maximilian", ""], ["Mirchev", "Atanas", ""], ["Kayalibay", "Baris", ""], ["van der Smagt", "Patrick", ""]]}, {"id": "2101.07235", "submitter": "Jean-Francois Rajotte", "authors": "Jean-Francois Rajotte, Sumit Mukherjee, Caleb Robinson, Anthony Ortiz,\n  Christopher West, Juan Lavista Ferres, Raymond T Ng", "title": "Reducing bias and increasing utility by federated generative modeling of\n  medical images using a centralized adversary", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce FELICIA (FEderated LearnIng with a CentralIzed Adversary) a\ngenerative mechanism enabling collaborative learning. In particular, we show\nhow a data owner with limited and biased data could benefit from other data\nowners while keeping data from all the sources private. This is a common\nscenario in medical image analysis where privacy legislation prevents data from\nbeing shared outside local premises. FELICIA works for a large family of\nGenerative Adversarial Networks (GAN) architectures including vanilla and\nconditional GANs as demonstrated in this work. We show that by using the\nFELICIA mechanism, a data owner with limited image samples can generate\nhigh-quality synthetic images with high utility while neither data owners has\nto provide access to its data. The sharing happens solely through a central\ndiscriminator that has access limited to synthetic data. Here, utility is\ndefined as classification performance on a real test set. We demonstrate these\nbenefits on several realistic healthcare scenarions using benchmark image\ndatasets (MNIST, CIFAR-10) as well as on medical images for the task of skin\nlesion classification. With multiple experiments, we show that even in the\nworst cases, combining FELICIA with real data gracefully achieves performance\non par with real data while most results significantly improves the utility.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 18:40:46 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Rajotte", "Jean-Francois", ""], ["Mukherjee", "Sumit", ""], ["Robinson", "Caleb", ""], ["Ortiz", "Anthony", ""], ["West", "Christopher", ""], ["Ferres", "Juan Lavista", ""], ["Ng", "Raymond T", ""]]}, {"id": "2101.07263", "submitter": "Benjamin Nachman", "authors": "Benjamin Nachman and Jesse Thaler", "title": "E Pluribus Unum Ex Machina: Learning from Many Collider Events at Once", "comments": "17 pages, 10 figures, 1 table; v2: added footnote about GAN training\n  and added exponential example in appendix; v3: minor updates to match journal\n  version", "journal-ref": "Phys. Rev. D 103, 116013 (2021)", "doi": "10.1103/PhysRevD.103.116013", "report-no": "MIT-CTP 5271", "categories": "physics.data-an hep-ex hep-ph stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There have been a number of recent proposals to enhance the performance of\nmachine learning strategies for collider physics by combining many distinct\nevents into a single ensemble feature. To evaluate the efficacy of these\nproposals, we study the connection between single-event classifiers and\nmulti-event classifiers under the assumption that collider events are\nindependent and identically distributed (IID). We show how one can build\noptimal multi-event classifiers from single-event classifiers, and we also show\nhow to construct multi-event classifiers such that they produce optimal\nsingle-event classifiers. This is illustrated for a Gaussian example as well as\nfor classification tasks relevant for searches and measurements at the Large\nHadron Collider. We extend our discussion to regression tasks by showing how\nthey can be phrased in terms of parametrized classifiers. Empirically, we find\nthat training a single-event (per-instance) classifier is more effective than\ntraining a multi-event (per-ensemble) classifier, as least for the cases we\nstudied, and we relate this fact to properties of the loss function gradient in\nthe two cases. While we did not identify a clear benefit from using multi-event\nclassifiers in the collider context, we speculate on the potential value of\nthese methods in cases involving only approximate independence, as relevant for\njet substructure studies.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 19:00:00 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2021 05:09:40 GMT"}, {"version": "v3", "created": "Sun, 20 Jun 2021 17:42:10 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Nachman", "Benjamin", ""], ["Thaler", "Jesse", ""]]}, {"id": "2101.07354", "submitter": "Yichi Zhang", "authors": "Yichi Zhang, Minh Tang", "title": "Consistency of random-walk based network embedding algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random-walk based network embedding algorithms like node2vec and DeepWalk are\nwidely used to obtain Euclidean representation of the nodes in a network prior\nto performing down-stream network inference tasks. Nevertheless, despite their\nimpressive empirical performance, there is a lack of theoretical results\nexplaining their behavior. In this paper we studied the node2vec and DeepWalk\nalgorithms through the perspective of matrix factorization. We analyze these\nalgorithms in the setting of community detection for stochastic blockmodel\ngraphs; in particular we established large-sample error bounds and prove\nconsistent community recovery of node2vec/DeepWalk embedding followed by\nk-means clustering. Our theoretical results indicate a subtle interplay between\nthe sparsity of the observed networks, the window sizes of the random walks,\nand the convergence rates of the node2vec/DeepWalk embedding toward the\nembedding of the true but unknown edge probabilities matrix. More specifically,\nas the network becomes sparser, our results suggest using larger window sizes,\nor equivalently, taking longer random walks, in order to attain better\nconvergence rate for the resulting embeddings. The paper includes numerical\nexperiments corroborating these observations.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 22:49:22 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Zhang", "Yichi", ""], ["Tang", "Minh", ""]]}, {"id": "2101.07464", "submitter": "Yue Lu", "authors": "Yue M. Lu", "title": "Householder Dice: A Matrix-Free Algorithm for Simulating Dynamics on\n  Gaussian and Random Orthogonal Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT physics.data-an stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a new algorithm, named Householder Dice (HD), for\nsimulating dynamics on dense random matrix ensembles with translation-invariant\nproperties. Examples include the Gaussian ensemble, the Haar-distributed random\northogonal ensemble, and their complex-valued counterparts. A \"direct\" approach\nto the simulation, where one first generates a dense $n \\times n$ matrix from\nthe ensemble, requires at least $\\mathcal{O}(n^2)$ resource in space and time.\nThe HD algorithm overcomes this $\\mathcal{O}(n^2)$ bottleneck by using the\nprinciple of deferred decisions: rather than fixing the entire random matrix in\nadvance, it lets the randomness unfold with the dynamics. At the heart of this\nmatrix-free algorithm is an adaptive and recursive construction of (random)\nHouseholder reflectors. These orthogonal transformations exploit the group\nsymmetry of the matrix ensembles, while simultaneously maintaining the\nstatistical correlations induced by the dynamics. The memory and computation\ncosts of the HD algorithm are $\\mathcal{O}(nT)$ and $\\mathcal{O}(nT^2)$,\nrespectively, with $T$ being the number of iterations. When $T \\ll n$, which is\nnearly always the case in practice, the new algorithm leads to significant\nreductions in runtime and memory footprint. Numerical results demonstrate the\npromise of the HD algorithm as a new computational tool in the study of\nhigh-dimensional random systems.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 04:50:53 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 00:15:14 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Lu", "Yue M.", ""]]}, {"id": "2101.07561", "submitter": "Paul Novello", "authors": "Paul Novello (CEA, X, Inria), Ga\\\"el Po\\\"ette (CEA), David Lugato\n  (CEA), Pietro Congedo (X, Inria)", "title": "Variance Based Samples Weighting for Supervised Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of supervised learning of a function by a Neural Network (NN),\nwe claim and empirically justify that a NN yields better results when the\ndistribution of the data set focuses on regions where the function to learn is\nsteeper. We first traduce this assumption in a mathematically workable way\nusing Taylor expansion. Then, theoretical derivations allow to construct a\nmethodology that we call Variance Based Samples Weighting (VBSW). VBSW uses\nlocal variance of the labels to weight the training points. This methodology is\ngeneral, scalable, cost effective, and significantly increases the performances\nof a large class of NNs for various classification and regression tasks on\nimage, text and multivariate data. We highlight its benefits with experiments\ninvolving NNs from shallow linear NN to Resnet or Bert.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 11:08:40 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 12:50:28 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Novello", "Paul", "", "CEA, X, Inria"], ["Po\u00ebtte", "Ga\u00ebl", "", "CEA"], ["Lugato", "David", "", "CEA"], ["Congedo", "Pietro", "", "X, Inria"]]}, {"id": "2101.07564", "submitter": "Luc Pronzato", "authors": "Luc Pronzato", "title": "Performance analysis of greedy algorithms for minimising a Maximum Mean\n  Discrepancy", "comments": "34 pages, 7 figures, preprint submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We analyse the performance of several iterative algorithms for the\nquantisation of a probability measure $\\mu$, based on the minimisation of a\nMaximum Mean Discrepancy (MMD). Our analysis includes kernel herding, greedy\nMMD minimisation and Sequential Bayesian Quadrature (SBQ). We show that the\nfinite-sample-size approximation error, measured by the MMD, decreases as $1/n$\nfor SBQ and also for kernel herding and greedy MMD minimisation when using a\nsuitable step-size sequence. The upper bound on the approximation error is\nslightly better for SBQ, but the other methods are significantly faster, with a\ncomputational cost that increases only linearly with the number of points\nselected. This is illustrated by two numerical examples, with the target\nmeasure $\\mu$ being uniform (a space-filling design application) and with $\\mu$\na Gaussian mixture.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 11:18:51 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Pronzato", "Luc", ""]]}, {"id": "2101.07598", "submitter": "Sergei Koltcov", "authors": "Sergei Koltcov, Vera Ignatenko, Maxim Terpilovskii, Paolo Rosso", "title": "Analysis and tuning of hierarchical topic models based on Renyi entropy\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical topic modeling is a potentially powerful instrument for\ndetermining the topical structure of text collections that allows constructing\na topical hierarchy representing levels of topical abstraction. However, tuning\nof parameters of hierarchical models, including the number of topics on each\nhierarchical level, remains a challenging task and an open issue. In this\npaper, we propose a Renyi entropy-based approach for a partial solution to the\nabove problem. First, we propose a Renyi entropy-based metric of quality for\nhierarchical models. Second, we propose a practical concept of hierarchical\ntopic model tuning tested on datasets with human mark-up. In the numerical\nexperiments, we consider three different hierarchical models, namely,\nhierarchical latent Dirichlet allocation (hLDA) model, hierarchical Pachinko\nallocation model (hPAM), and hierarchical additive regularization of topic\nmodels (hARTM). We demonstrate that hLDA model possesses a significant level of\ninstability and, moreover, the derived numbers of topics are far away from the\ntrue numbers for labeled datasets. For hPAM model, the Renyi entropy approach\nallows us to determine only one level of the data structure. For hARTM model,\nthe proposed approach allows us to estimate the number of topics for two\nhierarchical levels.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 12:54:47 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Koltcov", "Sergei", ""], ["Ignatenko", "Vera", ""], ["Terpilovskii", "Maxim", ""], ["Rosso", "Paolo", ""]]}, {"id": "2101.07600", "submitter": "Ricards Marcinkevics", "authors": "Ri\\v{c}ards Marcinkevi\\v{c}s, Julia E. Vogt", "title": "Interpretable Models for Granger Causality Using Self-explaining Neural\n  Networks", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Exploratory analysis of time series data can yield a better understanding of\ncomplex dynamical systems. Granger causality is a practical framework for\nanalysing interactions in sequential data, applied in a wide range of domains.\nIn this paper, we propose a novel framework for inferring multivariate Granger\ncausality under nonlinear dynamics based on an extension of self-explaining\nneural networks. This framework is more interpretable than other\nneural-network-based techniques for inferring Granger causality, since in\naddition to relational inference, it also allows detecting signs of\nGranger-causal effects and inspecting their variability over time. In\ncomprehensive experiments on simulated data, we show that our framework\nperforms on par with several powerful baseline methods at inferring Granger\ncausality and that it achieves better performance at inferring interaction\nsigns. The results suggest that our framework is a viable and more\ninterpretable alternative to sparse-input neural networks for inferring Granger\ncausality.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 12:59:00 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Marcinkevi\u010ds", "Ri\u010dards", ""], ["Vogt", "Julia E.", ""]]}, {"id": "2101.07661", "submitter": "Martin Huber", "authors": "Simon Berset, Martin Huber, Mark Schelker", "title": "The fiscal response to revenue shocks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the impact of fiscal revenue shocks on local fiscal policy. We focus\non the very volatile revenues from the immovable property gains tax in the\ncanton of Zurich, Switzerland, and analyze fiscal behavior following large and\nrare positive and negative revenue shocks. We apply causal machine learning\nstrategies and implement the post-double-selection LASSO estimator to identify\nthe causal effect of revenue shocks on public finances. We show that local\npolicymakers overall predominantly smooth fiscal shocks. However, we also find\nsome patterns consistent with fiscal conservatism, where positive shocks are\nsmoothed, while negative ones are mitigated by spending cuts.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 14:51:28 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Berset", "Simon", ""], ["Huber", "Martin", ""], ["Schelker", "Mark", ""]]}, {"id": "2101.07683", "submitter": "Kui Yang", "authors": "Kui Yang, Wenjing Zhao, Constantinos Antoniou", "title": "Utilizing Import Vector Machines to Identify Dangerous Pro-active\n  Traffic Conditions", "comments": "6 pages, 3 figures, 2020 IEEE 23rd International Conference on\n  Intelligent Transportation Systems (ITSC)", "journal-ref": "In 2020 IEEE 23rd International Conference on Intelligent\n  Transportation Systems (ITSC) (pp. 1-6). IEEE", "doi": "10.1109/ITSC45102.2020.9294284", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic accidents have been a severe issue in metropolises with the\ndevelopment of traffic flow. This paper explores the theory and application of\na recently developed machine learning technique, namely Import Vector Machines\n(IVMs), in real-time crash risk analysis, which is a hot topic to reduce\ntraffic accidents. Historical crash data and corresponding traffic data from\nShanghai Urban Expressway System were employed and matched. Traffic conditions\nare labelled as dangerous (i.e. probably leading to a crash) and safe (i.e. a\nnormal traffic condition) based on 5-minute measurements of average speed,\nvolume and occupancy. The IVM algorithm is trained to build the classifier and\nits performance is compared to the popular and successfully applied technique\nof Support Vector Machines (SVMs). The main findings indicate that IVMs could\nsuccessfully be employed in real-time identification of dangerous pro-active\ntraffic conditions. Furthermore, similar to the \"support points\" of the SVM,\nthe IVM model uses only a fraction of the training data to index kernel basis\nfunctions, typically a much smaller fraction than the SVM, and its\nclassification rates are similar to those of SVMs. This gives the IVM a\ncomputational advantage over the SVM, especially when the size of the training\ndata set is large.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 15:22:23 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Yang", "Kui", ""], ["Zhao", "Wenjing", ""], ["Antoniou", "Constantinos", ""]]}, {"id": "2101.07766", "submitter": "Louis Raynal", "authors": "Louis Raynal and Jukka-Pekka Onnela", "title": "Selection of Summary Statistics for Network Model Choice with\n  Approximate Bayesian Computation", "comments": "30 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian Computation (ABC) now serves as one of the major\nstrategies to perform model choice and parameter inference on models with\nintractable likelihoods. An essential component of ABC involves comparing a\nlarge amount of simulated data with the observed data through summary\nstatistics. To avoid the curse of dimensionality, summary statistic selection\nis of prime importance, and becomes even more critical when applying ABC to\nmechanistic network models. Indeed, while many summary statistics can be used\nto encode network structures, their computational complexity can be highly\nvariable. For large networks, computation of summary statistics can quickly\ncreate a bottleneck, making the use of ABC difficult. To reduce this\ncomputational burden and make the analysis of mechanistic network models more\npractical, we investigated two questions in a model choice framework. First, we\nstudied the utility of cost-based filter selection methods to account for\ndifferent summary costs during the selection process. Second, we performed\nselection using networks generated with a smaller number of nodes to reduce the\ntime required for the selection step. Our findings show that computationally\ninexpensive summary statistics can be efficiently selected with minimal impact\non classification accuracy. Furthermore, we found that networks with a smaller\nnumber of nodes can only be employed to eliminate a moderate number of\nsummaries. While this latter finding is network specific, the former is general\nand can be adapted to any ABC application.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 18:21:06 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Raynal", "Louis", ""], ["Onnela", "Jukka-Pekka", ""]]}, {"id": "2101.07773", "submitter": "Balasubramaniam Srinivasan", "authors": "Balasubramaniam Srinivasan, Da Zheng, George Karypis", "title": "Learning over Families of Sets -- Hypergraph Representation Learning for\n  Higher Order Tasks", "comments": "Published as a conference paper at SIAM International Conference on\n  Data Mining(SDM 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph representation learning has made major strides over the past decade.\nHowever, in many relational domains, the input data are not suited for simple\ngraph representations as the relationships between entities go beyond pairwise\ninteractions. In such cases, the relationships in the data are better\nrepresented as hyperedges (set of entities) of a non-uniform hypergraph. While\nthere have been works on principled methods for learning representations of\nnodes of a hypergraph, these approaches are limited in their applicability to\ntasks on non-uniform hypergraphs (hyperedges with different cardinalities). In\nthis work, we exploit the incidence structure to develop a hypergraph neural\nnetwork to learn provably expressive representations of variable sized\nhyperedges which preserve local-isomorphism in the line graph of the\nhypergraph, while also being invariant to permutations of its constituent\nvertices. Specifically, for a given vertex set, we propose frameworks for (1)\nhyperedge classification and (2) variable sized expansion of partially observed\nhyperedges which captures the higher order interactions among vertices and\nhyperedges. We evaluate performance on multiple real-world hypergraph datasets\nand demonstrate consistent, significant improvement in accuracy, over\nstate-of-the-art models.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 18:37:50 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Srinivasan", "Balasubramaniam", ""], ["Zheng", "Da", ""], ["Karypis", "George", ""]]}, {"id": "2101.07781", "submitter": "Cong Ma", "authors": "Cong Ma, Banghua Zhu, Jiantao Jiao, Martin J. Wainwright", "title": "Minimax Off-Policy Evaluation for Multi-Armed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of off-policy evaluation in the multi-armed bandit model\nwith bounded rewards, and develop minimax rate-optimal procedures under three\nsettings. First, when the behavior policy is known, we show that the Switch\nestimator, a method that alternates between the plug-in and importance sampling\nestimators, is minimax rate-optimal for all sample sizes. Second, when the\nbehavior policy is unknown, we analyze performance in terms of the competitive\nratio, thereby revealing a fundamental gap between the settings of known and\nunknown behavior policies. When the behavior policy is unknown, any estimator\nmust have mean-squared error larger -- relative to the oracle estimator\nequipped with the knowledge of the behavior policy -- by a multiplicative\nfactor proportional to the support size of the target policy. Moreover, we\ndemonstrate that the plug-in approach achieves this worst-case competitive\nratio up to a logarithmic factor. Third, we initiate the study of the partial\nknowledge setting in which it is assumed that the minimum probability taken by\nthe behavior policy is known. We show that the plug-in estimator is optimal for\nrelatively large values of the minimum probability, but is sub-optimal when the\nminimum probability is low. In order to remedy this gap, we propose a new\nestimator based on approximation by Chebyshev polynomials that provably\nachieves the optimal estimation error. Numerical experiments on both simulated\nand real data corroborate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 18:55:29 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Ma", "Cong", ""], ["Zhu", "Banghua", ""], ["Jiao", "Jiantao", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "2101.07833", "submitter": "Melikasadat Emami", "authors": "Melikasadat Emami, Mojtaba Sahraee-Ardakan, Parthe Pandit, Sundeep\n  Rangan, Alyson K. Fletcher", "title": "Implicit Bias of Linear RNNs", "comments": "30 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary wisdom based on empirical studies suggests that standard\nrecurrent neural networks (RNNs) do not perform well on tasks requiring\nlong-term memory. However, precise reasoning for this behavior is still\nunknown. This paper provides a rigorous explanation of this property in the\nspecial case of linear RNNs. Although this work is limited to linear RNNs, even\nthese systems have traditionally been difficult to analyze due to their\nnon-linear parameterization. Using recently-developed kernel regime analysis,\nour main result shows that linear RNNs learned from random initializations are\nfunctionally equivalent to a certain weighted 1D-convolutional network.\nImportantly, the weightings in the equivalent model cause an implicit bias to\nelements with smaller time lags in the convolution and hence, shorter memory.\nThe degree of this bias depends on the variance of the transition kernel matrix\nat initialization and is related to the classic exploding and vanishing\ngradients problem. The theory is validated in both synthetic and real data\nexperiments.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 19:39:28 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Emami", "Melikasadat", ""], ["Sahraee-Ardakan", "Mojtaba", ""], ["Pandit", "Parthe", ""], ["Rangan", "Sundeep", ""], ["Fletcher", "Alyson K.", ""]]}, {"id": "2101.07957", "submitter": "Kei Takemura", "authors": "Kei Takemura, Shinji Ito, Daisuke Hatano, Hanna Sumita, Takuro\n  Fukunaga, Naonori Kakimura, Ken-ichi Kawarabayashi", "title": "Near-Optimal Regret Bounds for Contextual Combinatorial Semi-Bandits\n  with Linear Payoff Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The contextual combinatorial semi-bandit problem with linear payoff functions\nis a decision-making problem in which a learner chooses a set of arms with the\nfeature vectors in each round under given constraints so as to maximize the sum\nof rewards of arms. Several existing algorithms have regret bounds that are\noptimal with respect to the number of rounds $T$. However, there is a gap of\n$\\tilde{O}(\\max(\\sqrt{d}, \\sqrt{k}))$ between the current best upper and lower\nbounds, where $d$ is the dimension of the feature vectors, $k$ is the number of\nthe chosen arms in a round, and $\\tilde{O}(\\cdot)$ ignores the logarithmic\nfactors. The dependence of $k$ and $d$ is of practical importance because $k$\nmay be larger than $T$ in real-world applications such as recommender systems.\nIn this paper, we fill the gap by improving the upper and lower bounds. More\nprecisely, we show that the C${}^2$UCB algorithm proposed by Qin, Chen, and Zhu\n(2014) has the optimal regret bound $\\tilde{O}(d\\sqrt{kT} + dk)$ for the\npartition matroid constraints. For general constraints, we propose an algorithm\nthat modifies the reward estimates of arms in the C${}^2$UCB algorithm and\ndemonstrate that it enjoys the optimal regret bound for a more general problem\nthat can take into account other objectives simultaneously. We also show that\nour technique would be applicable to related problems. Numerical experiments\nsupport our theoretical results and considerations.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 04:29:18 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 11:22:30 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Takemura", "Kei", ""], ["Ito", "Shinji", ""], ["Hatano", "Daisuke", ""], ["Sumita", "Hanna", ""], ["Fukunaga", "Takuro", ""], ["Kakimura", "Naonori", ""], ["Kawarabayashi", "Ken-ichi", ""]]}, {"id": "2101.07969", "submitter": "Zheng Liu", "authors": "Zheng Liu, Po-Ling Loh", "title": "Robust W-GAN-Based Estimation Under Wasserstein Contamination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust estimation is an important problem in statistics which aims at\nproviding a reasonable estimator when the data-generating distribution lies\nwithin an appropriately defined ball around an uncontaminated distribution.\nAlthough minimax rates of estimation have been established in recent years,\nmany existing robust estimators with provably optimal convergence rates are\nalso computationally intractable. In this paper, we study several estimation\nproblems under a Wasserstein contamination model and present computationally\ntractable estimators motivated by generative adversarial networks (GANs).\nSpecifically, we analyze properties of Wasserstein GAN-based estimators for\nlocation estimation, covariance matrix estimation, and linear regression and\nshow that our proposed estimators are minimax optimal in many scenarios.\nFinally, we present numerical results which demonstrate the effectiveness of\nour estimators.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 05:15:16 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Liu", "Zheng", ""], ["Loh", "Po-Ling", ""]]}, {"id": "2101.07997", "submitter": "Zhanlin Liu", "authors": "Zhanlin Liu and Youngjun Choe", "title": "Data-driven sparse polynomial chaos expansion for models with dependent\n  inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.SY stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polynomial chaos expansions (PCEs) have been used in many real-world\nengineering applications to quantify how the uncertainty of an output is\npropagated from inputs. PCEs for models with independent inputs have been\nextensively explored in the literature. Recently, different approaches have\nbeen proposed for models with dependent inputs to expand the use of PCEs to\nmore real-world applications. Typical approaches include building PCEs based on\nthe Gram-Schmidt algorithm or transforming the dependent inputs into\nindependent inputs. However, the two approaches have their limitations\nregarding computational efficiency and additional assumptions about the input\ndistributions, respectively. In this paper, we propose a data-driven approach\nto build sparse PCEs for models with dependent inputs. The proposed algorithm\nrecursively constructs orthonormal polynomials using a set of monomials based\non their correlations with the output. The proposed algorithm on building\nsparse PCEs not only reduces the number of minimally required observations but\nalso improves the numerical stability and computational efficiency. Four\nnumerical examples are implemented to validate the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 07:06:50 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2021 01:44:40 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Liu", "Zhanlin", ""], ["Choe", "Youngjun", ""]]}, {"id": "2101.08007", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "On the Non-Monotonicity of a Non-Differentially Mismeasured Binary\n  Confounder", "comments": "arXiv admin note: text overlap with arXiv:2005.13245", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that we are interested in the average causal effect of a binary\ntreatment on an outcome when this relationship is confounded by a binary\nconfounder. Suppose that the confounder is unobserved but a non-differential\nbinary proxy of it is observed. We identify conditions under which adjusting\nfor the proxy comes closer to the incomputable true average causal effect than\nnot adjusting at all. Unlike other works, we do not assume that the average\ncausal effect of the confounder on the outcome is in the same direction among\ntreated and untreated.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 07:42:54 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2021 16:04:50 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2021 09:02:05 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}, {"id": "2101.08049", "submitter": "Luka \\v{Z}nidari\\v{c}", "authors": "Luka \\v{Z}nidari\\v{c}, Gjorgji Nusev, Bertrand Morel, Julie Mougin,\n  {\\DJ}ani Juri\\v{c}i\\'c and Pavle Bo\\v{s}koski", "title": "Evaluating uncertainties in electrochemical impedance spectra of solid\n  oxide fuel cells", "comments": "28 pages, 18 figures. Submitted to: Applied Energy", "journal-ref": null, "doi": "10.1016/j.apenergy.2021.117101", "report-no": null, "categories": "stat.CO cs.LG stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Electrochemical impedance spectroscopy (EIS) is a widely used tool for\ncharacterization of fuel cells and other electrochemical conversion systems.\nWhen applied to the on-line monitoring in the context of in-field applications,\nthe disturbances, drifts and sensor noise may cause severe distortions in the\nevaluated spectra, especially in the low-frequency part. Failure to ignore the\nrandom effects can result in misinterpreted spectra and, consequently, in\nmisleading diagnostic reasoning. This fact has not been often addressed in the\nresearch so far. In this paper, we propose an approach to the quantification of\nthe spectral uncertainty, which relies on evaluating the uncertainty of the\nequivalent circuit model (ECM). We apply the computationally efficient\nvariational Bayes (VB) method and compare the quality of the results with those\nobtained with the Markov chain Monte Carlo (MCMC) algorithm. Namely, MCMC\nalgorithm returns accurate distributions of the estimated model parameters,\nwhile VB approach provides the approximate distributions. By using simulated\nand real data we show that approximate results provided by VB approach,\nalthough slightly over-optimistic, are still close to the more realistic MCMC\nestimates. A great advantage of the VB method for online monitoring is low\ncomputational load, which is several orders of magnitude lower compared to\nMCMC. The performance of VB algorithm is demonstrated on a case of ECM\nparameters estimation in a 6 cell solid oxide fuel cell (SOFC) stack. The\ncomplete numerical implementation for recreating the results can be found at\nhttps://repo.ijs.si/lznidaric/variational-bayes-supplementary-material.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 10:07:32 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 12:22:32 GMT"}, {"version": "v3", "created": "Wed, 5 May 2021 11:35:23 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["\u017dnidari\u010d", "Luka", ""], ["Nusev", "Gjorgji", ""], ["Morel", "Bertrand", ""], ["Mougin", "Julie", ""], ["Juri\u010di\u0107", "\u0110ani", ""], ["Bo\u0161koski", "Pavle", ""]]}, {"id": "2101.08061", "submitter": "Eduardo C\\'esar Garrido-Merch\\'an", "authors": "Lucia Asencio-Mart\\'in, Eduardo C. Garrido-Merch\\'an", "title": "A Similarity Measure of Gaussian Process Predictive Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some scenarios require the computation of a predictive distribution of a new\nvalue evaluated on an objective function conditioned on previous observations.\nWe are interested on using a model that makes valid assumptions on the\nobjective function whose values we are trying to predict. Some of these\nassumptions may be smoothness or stationarity. Gaussian process (GPs) are\nprobabilistic models that can be interpreted as flexible distributions over\nfunctions. They encode the assumptions through covariance functions, making\nhypotheses about new data through a predictive distribution by being fitted to\nold observations. We can face the case where several GPs are used to model\ndifferent objective functions. GPs are non-parametric models whose complexity\nis cubic on the number of observations. A measure that represents how similar\nis one GP predictive distribution with respect to another would be useful to\nstop using one GP when they are modelling functions of the same input space. We\nare really inferring that two objective functions are correlated, so one GP is\nenough to model both of them by performing a transformation of the prediction\nof the other function in case of inverse correlation. We show empirical\nevidence in a set of synthetic and benchmark experiments that GPs predictive\ndistributions can be compared and that one of them is enough to predict two\ncorrelated functions in the same input space. This similarity metric could be\nextremely useful used to discard objectives in Bayesian many-objective\noptimization.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 10:52:48 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Asencio-Mart\u00edn", "Lucia", ""], ["Garrido-Merch\u00e1n", "Eduardo C.", ""]]}, {"id": "2101.08303", "submitter": "Gal Vardi", "authors": "Amit Daniely and Gal Vardi", "title": "From Local Pseudorandom Generators to Hardness of Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove hardness-of-learning results under a well-studied assumption on the\nexistence of local pseudorandom generators. As we show, this assumption allows\nus to surpass the current state of the art, and prove hardness of various basic\nproblems, with no hardness results to date.\n  Our results include: hardness of learning shallow ReLU neural networks under\nthe Gaussian distribution and other distributions; hardness of learning\nintersections of $\\omega(1)$ halfspaces, DNF formulas with $\\omega(1)$ terms,\nand ReLU networks with $\\omega(1)$ hidden neurons; hardness of weakly learning\ndeterministic finite automata under the uniform distribution; hardness of\nweakly learning depth-$3$ Boolean circuits under the uniform distribution, as\nwell as distribution-specific hardness results for learning DNF formulas and\nintersections of halfspaces. We also establish lower bounds on the complexity\nof learning intersections of a constant number of halfspaces, and ReLU networks\nwith a constant number of hidden neurons. Moreover, our results imply the\nhardness of virtually all improper PAC-learning problems (both\ndistribution-free and distribution-specific) that were previously shown hard\nunder other assumptions.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 20:07:47 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 11:49:29 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Daniely", "Amit", ""], ["Vardi", "Gal", ""]]}, {"id": "2101.08354", "submitter": "Xun Gao", "authors": "Xun Gao, Eric R. Anschuetz, Sheng-Tao Wang, J. Ignacio Cirac and\n  Mikhail D. Lukin", "title": "Enhancing Generative Models via Quantum Correlations", "comments": "25 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cond-mat.stat-mech cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative modeling using samples drawn from the probability distribution\nconstitutes a powerful approach for unsupervised machine learning. Quantum\nmechanical systems can produce probability distributions that exhibit quantum\ncorrelations which are difficult to capture using classical models. We show\ntheoretically that such quantum correlations provide a powerful resource for\ngenerative modeling. In particular, we provide an unconditional proof of\nseparation in expressive power between a class of widely-used generative\nmodels, known as Bayesian networks, and its minimal quantum extension. We show\nthat this expressivity advantage is associated with quantum nonlocality and\nquantum contextuality. Furthermore, we numerically test this separation on\nstandard machine learning data sets and show that it holds for practical\nproblems. The possibility of quantum advantage demonstrated in this work not\nonly sheds light on the design of useful quantum machine learning protocols but\nalso provides inspiration to draw on ideas from quantum foundations to improve\npurely classical algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 22:57:22 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Gao", "Xun", ""], ["Anschuetz", "Eric R.", ""], ["Wang", "Sheng-Tao", ""], ["Cirac", "J. Ignacio", ""], ["Lukin", "Mikhail D.", ""]]}, {"id": "2101.08367", "submitter": "Naoyuki Terashita", "authors": "Naoyuki Terashita, Hiroki Ohashi, Yuichi Nonaka, Takashi Kanemaru", "title": "Influence Estimation for Generative Adversarial Networks", "comments": "Accepted as a conference paper at ICLR 2021 (Spotlight). This is the\n  peer-reviewed version and not ready for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying harmful instances, whose absence in a training dataset improves\nmodel performance, is important for building better machine learning models.\nAlthough previous studies have succeeded in estimating harmful instances under\nsupervised settings, they cannot be trivially extended to generative\nadversarial networks (GANs). This is because previous approaches require that\n(1) the absence of a training instance directly affects the loss value and that\n(2) the change in the loss directly measures the harmfulness of the instance\nfor the performance of a model. In GAN training, however, neither of the\nrequirements is satisfied. This is because, (1) the generator's loss is not\ndirectly affected by the training instances as they are not part of the\ngenerator's training steps, and (2) the values of GAN's losses normally do not\ncapture the generative performance of a model. To this end, (1) we propose an\ninfluence estimation method that uses the Jacobian of the gradient of the\ngenerator's loss with respect to the discriminator's parameters (and vice\nversa) to trace how the absence of an instance in the discriminator's training\naffects the generator's parameters, and (2) we propose a novel evaluation\nscheme, in which we assess harmfulness of each training instance on the basis\nof how GAN evaluation metric (e.g., inception score) is expect to change due to\nthe removal of the instance. We experimentally verified that our influence\nestimation method correctly inferred the changes in GAN evaluation metrics.\nFurther, we demonstrated that the removal of the identified harmful instances\neffectively improved the model's generative performance with respect to various\nGAN evaluation metrics.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 23:55:54 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Terashita", "Naoyuki", ""], ["Ohashi", "Hiroki", ""], ["Nonaka", "Yuichi", ""], ["Kanemaru", "Takashi", ""]]}, {"id": "2101.08421", "submitter": "Chao Gao", "authors": "Pinhan Chen, Chao Gao, Anderson Y. Zhang", "title": "Optimal Full Ranking from Pairwise Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of ranking $n$ players from partial pairwise\ncomparison data under the Bradley-Terry-Luce model. For the first time in the\nliterature, the minimax rate of this ranking problem is derived with respect to\nthe Kendall's tau distance that measures the difference between two rank\nvectors by counting the number of inversions. The minimax rate of ranking\nexhibits a transition between an exponential rate and a polynomial rate\ndepending on the magnitude of the signal-to-noise ratio of the problem. To the\nbest of our knowledge, this phenomenon is unique to full ranking and has not\nbeen seen in any other statistical estimation problem. To achieve the minimax\nrate, we propose a divide-and-conquer ranking algorithm that first divides the\n$n$ players into groups of similar skills and then computes local MLE within\neach group. The optimality of the proposed algorithm is established by a\ncareful approximate independence argument between the two steps.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 03:34:44 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Chen", "Pinhan", ""], ["Gao", "Chao", ""], ["Zhang", "Anderson Y.", ""]]}, {"id": "2101.08452", "submitter": "Huan Zhang", "authors": "Huan Zhang, Hongge Chen, Duane Boning, Cho-Jui Hsieh", "title": "Robust Reinforcement Learning on State Observations with Learned Optimal\n  Adversary", "comments": "Accepted by ICLR 2021. Huan Zhang and Hongge Chen contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the robustness of reinforcement learning (RL) with adversarially\nperturbed state observations, which aligns with the setting of many adversarial\nattacks to deep reinforcement learning (DRL) and is also important for rolling\nout real-world RL agent under unpredictable sensing noise. With a fixed agent\npolicy, we demonstrate that an optimal adversary to perturb state observations\ncan be found, which is guaranteed to obtain the worst case agent reward. For\nDRL settings, this leads to a novel empirical adversarial attack to RL agents\nvia a learned adversary that is much stronger than previous ones. To enhance\nthe robustness of an agent, we propose a framework of alternating training with\nlearned adversaries (ATLA), which trains an adversary online together with the\nagent using policy gradient following the optimal adversarial attack framework.\nAdditionally, inspired by the analysis of state-adversarial Markov decision\nprocess (SA-MDP), we show that past states and actions (history) can be useful\nfor learning a robust agent, and we empirically find a LSTM based policy can be\nmore robust under adversaries. Empirical evaluations on a few continuous\ncontrol environments show that ATLA achieves state-of-the-art performance under\nstrong adversaries. Our code is available at\nhttps://github.com/huanzhang12/ATLA_robust_RL.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 05:38:52 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Zhang", "Huan", ""], ["Chen", "Hongge", ""], ["Boning", "Duane", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "2101.08490", "submitter": "Tobias Hatt", "authors": "Tobias Hatt, Stefan Feuerriegel", "title": "Estimating Average Treatment Effects via Orthogonal Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision-making often requires accurate estimation of treatment effects from\nobservational data. This is challenging as outcomes of alternative decisions\nare not observed and have to be estimated. Previous methods estimate outcomes\nbased on unconfoundedness but neglect any constraints that unconfoundedness\nimposes on the outcomes. In this paper, we propose a novel regularization\nframework for estimating average treatment effects that exploits\nunconfoundedness. To this end, we formalize unconfoundedness as an\northogonality constraint, which ensures that the outcomes are orthogonal to the\ntreatment assignment. This orthogonality constraint is then included in the\nloss function via a regularization. Based on our regularization framework, we\ndevelop deep orthogonal networks for unconfounded treatments (DONUT), which\nlearn outcomes that are orthogonal to the treatment assignment. Using a variety\nof benchmark datasets for estimating average treatment effects, we demonstrate\nthat DONUT outperforms the state-of-the-art substantially.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 08:05:35 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Hatt", "Tobias", ""], ["Feuerriegel", "Stefan", ""]]}, {"id": "2101.08505", "submitter": "YunPeng Li", "authors": "YunPeng Li, ZhaoHui Ye", "title": "Boosting in Univariate Nonparametric Maximum Likelihood Estimation", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2021.3065881", "report-no": null, "categories": "stat.ML cs.LG eess.SP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric maximum likelihood estimation is intended to infer the unknown\ndensity distribution while making as few assumptions as possible. To alleviate\nthe over parameterization in nonparametric data fitting, smoothing assumptions\nare usually merged into the estimation. In this paper a novel boosting-based\nmethod is introduced to the nonparametric estimation in univariate cases. We\ndeduce the boosting algorithm by the second-order approximation of\nnonparametric log-likelihood. Gaussian kernel and smooth spline are chosen as\nweak learners in boosting to satisfy the smoothing assumptions. Simulations and\nreal data experiments demonstrate the efficacy of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 08:46:33 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Li", "YunPeng", ""], ["Ye", "ZhaoHui", ""]]}, {"id": "2101.08521", "submitter": "Chuan-Long Xie", "authors": "Haotian Ye, Chuanlong Xie, Yue Liu, Zhenguo Li", "title": "Out-of-Distribution Generalization Analysis via Influence Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The mismatch between training and target data is one major challenge for\ncurrent machine learning systems. When training data is collected from multiple\ndomains and the target domains include all training domains and other new\ndomains, we are facing an Out-of-Distribution (OOD) generalization problem that\naims to find a model with the best OOD accuracy. One of the definitions of OOD\naccuracy is worst-domain accuracy. In general, the set of target domains is\nunknown, and the worst over target domains may be unseen when the number of\nobserved domains is limited. In this paper, we show that the worst accuracy\nover the observed domains may dramatically fail to identify the OOD accuracy.\nTo this end, we introduce Influence Function, a classical tool from robust\nstatistics, into the OOD generalization problem and suggest the variance of\ninfluence function to monitor the stability of a model on training domains. We\nshow that the accuracy on test domains and the proposed index together can help\nus discern whether OOD algorithms are needed and whether a model achieves good\nOOD generalization.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 09:59:55 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Ye", "Haotian", ""], ["Xie", "Chuanlong", ""], ["Liu", "Yue", ""], ["Li", "Zhenguo", ""]]}, {"id": "2101.08534", "submitter": "Marc Jourdan", "authors": "Marc Jourdan, Mojm\\'ir Mutn\\'y, Johannes Kirschner, Andreas Krause", "title": "Efficient Pure Exploration for Combinatorial Bandits with Semi-Bandit\n  Feedback", "comments": "45 pages. 3 tables. Appendices: from A to I. Figures: 1(a), 1(b),\n  2(a), 2(b), 3(a), 3(b), 3(c), 4(a), 4(b), 5(a), 5(b), 5(c), 5(d), 6(a), 6(b).\n  To be published in the 32nd International Conference on Algorithmic Learning\n  Theory and the Proceedings of Machine Learning Research vol 132:1-45, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combinatorial bandits with semi-bandit feedback generalize multi-armed\nbandits, where the agent chooses sets of arms and observes a noisy reward for\neach arm contained in the chosen set. The action set satisfies a given\nstructure such as forming a base of a matroid or a path in a graph. We focus on\nthe pure-exploration problem of identifying the best arm with fixed confidence,\nas well as a more general setting, where the structure of the answer set\ndiffers from the one of the action set. Using the recently popularized game\nframework, we interpret this problem as a sequential zero-sum game and develop\na CombGame meta-algorithm whose instances are asymptotically optimal algorithms\nwith finite time guarantees. In addition to comparing two families of learners\nto instantiate our meta-algorithm, the main contribution of our work is a\nspecific oracle efficient instance for best-arm identification with\ncombinatorial actions. Based on a projection-free online learning algorithm for\nconvex polytopes, it is the first computationally efficient algorithm which is\nasymptotically optimal and has competitive empirical performance.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 10:35:09 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Jourdan", "Marc", ""], ["Mutn\u00fd", "Mojm\u00edr", ""], ["Kirschner", "Johannes", ""], ["Krause", "Andreas", ""]]}, {"id": "2101.08539", "submitter": "Sikai Zhang", "authors": "Sikai Zhang, Zi-Qiang Lang", "title": "Orthogonal Least Squares Based Fast Feature Selection for Linear\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  An Orthogonal Least Squares (OLS) based feature selection method is proposed\nfor both binomial and multinomial classification. The novel Squared Orthogonal\nCorrelation Coefficient (SOCC) is defined based on Error Reduction Ratio (ERR)\nin OLS and used as the feature ranking criterion. The equivalence between the\ncanonical correlation coefficient, Fisher's criterion, and the sum of the SOCCs\nis revealed, which unveils the statistical implication of ERR in OLS for the\nfirst time. It is also shown that the OLS based feature selection method has\nspeed advantages when applied for greedy search. The proposed method is\ncomprehensively compared with the mutual information based feature selection\nmethods in 2 synthetic and 7 real world datasets. The results show that the\nproposed method is always in the top 5 among the 10 candidate methods. Besides,\nthe proposed method can be directly applied to continuous features without\ndiscretisation, which is another significant advantage over mutual information\nbased methods.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 10:42:06 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 10:49:53 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Zhang", "Sikai", ""], ["Lang", "Zi-Qiang", ""]]}, {"id": "2101.08576", "submitter": "Quynh Nguyen", "authors": "Quynh Nguyen", "title": "A Note on Connectivity of Sublevel Sets in Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is shown that for deep neural networks, a single wide layer of width $N+1$\n($N$ being the number of training samples) suffices to prove the connectivity\nof sublevel sets of the training loss function. In the two-layer setting, the\nsame property may not hold even if one has just one neuron less (i.e. width $N$\ncan lead to disconnected sublevel sets).\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 12:43:26 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Nguyen", "Quynh", ""]]}, {"id": "2101.08578", "submitter": "Joosep Pata", "authors": "Joosep Pata, Javier Duarte, Jean-Roch Vlimant, Maurizio Pierini, Maria\n  Spiropulu", "title": "MLPF: Efficient machine-learned particle-flow reconstruction using graph\n  neural networks", "comments": "15 pages, 10 figures", "journal-ref": "Eur. Phys. J. C (2021) 81: 381", "doi": "10.1140/epjc/s10052-021-09158-w", "report-no": null, "categories": "physics.data-an cs.LG hep-ex physics.ins-det stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In general-purpose particle detectors, the particle-flow algorithm may be\nused to reconstruct a comprehensive particle-level view of the event by\ncombining information from the calorimeters and the trackers, significantly\nimproving the detector resolution for jets and the missing transverse momentum.\nIn view of the planned high-luminosity upgrade of the CERN Large Hadron\nCollider (LHC), it is necessary to revisit existing reconstruction algorithms\nand ensure that both the physics and computational performance are sufficient\nin an environment with many simultaneous proton-proton interactions (pileup).\nMachine learning may offer a prospect for computationally efficient event\nreconstruction that is well-suited to heterogeneous computing platforms, while\nsignificantly improving the reconstruction quality over rule-based algorithms\nfor granular detectors. We introduce MLPF, a novel, end-to-end trainable,\nmachine-learned particle-flow algorithm based on parallelizable,\ncomputationally efficient, and scalable graph neural networks optimized using a\nmulti-task objective on simulated events. We report the physics and\ncomputational performance of the MLPF algorithm on a Monte Carlo dataset of top\nquark-antiquark pairs produced in proton-proton collisions in conditions\nsimilar to those expected for the high-luminosity LHC. The MLPF algorithm\nimproves the physics response with respect to a rule-based benchmark algorithm\nand demonstrates computationally scalable particle-flow reconstruction in a\nhigh-pileup environment.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 12:47:54 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 15:18:59 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 06:52:41 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Pata", "Joosep", ""], ["Duarte", "Javier", ""], ["Vlimant", "Jean-Roch", ""], ["Pierini", "Maurizio", ""], ["Spiropulu", "Maria", ""]]}, {"id": "2101.08656", "submitter": "Jinxiong Zhang", "authors": "Jinxiong Zhang", "title": "Dive into Decision Trees and Forests: A Theoretical Demonstration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Based on decision trees, many fields have arguably made tremendous progress\nin recent years. In simple words, decision trees use the strategy of\n\"divide-and-conquer\" to divide the complex problem on the dependency between\ninput features and labels into smaller ones. While decision trees have a long\nhistory, recent advances have greatly improved their performance in\ncomputational advertising, recommender system, information retrieval, etc. We\nintroduce common tree-based models (e.g., Bayesian CART, Bayesian regression\nsplines) and training techniques (e.g., mixed integer programming, alternating\noptimization, gradient descent). Along the way, we highlight probabilistic\ncharacteristics of tree-based models and explain their practical and\ntheoretical benefits. Except machine learning and data mining, we try to show\ntheoretical advances on tree-based models from other fields such as statistics\nand operation research. We list the reproducible resource at the end of each\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 16:47:59 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Zhang", "Jinxiong", ""]]}, {"id": "2101.08685", "submitter": "Thomas Pfeil", "authors": "Thomas Pfeil", "title": "ItNet: iterative neural networks with small graphs for accurate and\n  efficient anytime prediction", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have usually to be compressed and accelerated for their\nusage in low-power, e.g. mobile, devices. Recently, massively-parallel hardware\naccelerators were developed that offer high throughput and low latency at low\npower by utilizing in-memory computation. However, to exploit these benefits\nthe computational graph of a neural network has to fit into the in-computation\nmemory of these hardware systems that is usually rather limited in size. In\nthis study, we introduce a class of network models that have a small memory\nfootprint in terms of their computational graphs. To this end, the graph is\ndesigned to contain loops by iteratively executing a single network building\nblock. Furthermore, the trade-off between accuracy and latency of these\nso-called iterative neural networks is improved by adding multiple intermediate\noutputs both during training and inference. We show state-of-the-art results\nfor semantic segmentation on the CamVid and Cityscapes datasets that are\nespecially demanding in terms of computational resources. In ablation studies,\nthe improvement of network training by intermediate network outputs as well as\nthe trade-off between weight sharing over iterations and the network size are\ninvestigated.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 15:56:29 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 14:25:35 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Pfeil", "Thomas", ""]]}, {"id": "2101.08688", "submitter": "Yingdong Lu", "authors": "Soumyadip Ghosh, Yingdong Lu, Tomasz Nowicki", "title": "HMC, an example of Functional Analysis applied to Algorithms in Data\n  Mining. The convergence in $L^p$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CA cs.DS cs.LG math.FA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a proof of convergence of the Hamiltonian Monte Carlo algorithm in\nterms of Functional Analysis. We represent the algorithm as an operator on the\ndensity functions, and prove the convergence of iterations of this operator in\n$L^p$, for $1<p<\\infty$, and strong convergence for $2\\le p<\\infty$.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 15:59:32 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Ghosh", "Soumyadip", ""], ["Lu", "Yingdong", ""], ["Nowicki", "Tomasz", ""]]}, {"id": "2101.08692", "submitter": "Andrew Brock", "authors": "Andrew Brock, Soham De, Samuel L. Smith", "title": "Characterizing signal propagation to close the performance gap in\n  unnormalized ResNets", "comments": "Published as a conference paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch Normalization is a key component in almost all state-of-the-art image\nclassifiers, but it also introduces practical challenges: it breaks the\nindependence between training examples within a batch, can incur compute and\nmemory overhead, and often results in unexpected bugs. Building on recent\ntheoretical analyses of deep ResNets at initialization, we propose a simple set\nof analysis tools to characterize signal propagation on the forward pass, and\nleverage these tools to design highly performant ResNets without activation\nnormalization layers. Crucial to our success is an adapted version of the\nrecently proposed Weight Standardization. Our analysis tools show how this\ntechnique preserves the signal in networks with ReLU or Swish activation\nfunctions by ensuring that the per-channel activation means do not grow with\ndepth. Across a range of FLOP budgets, our networks attain performance\ncompetitive with the state-of-the-art EfficientNets on ImageNet.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 16:07:06 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 11:28:41 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Brock", "Andrew", ""], ["De", "Soham", ""], ["Smith", "Samuel L.", ""]]}, {"id": "2101.08696", "submitter": "Naifu Zhang", "authors": "Naifu Zhang, Meixia Tao and Jia Wang", "title": "Sum-Rate-Distortion Function for Indirect Multiterminal Source Coding in\n  Federated Learning", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main focus in federated learning (FL) is the communication\nefficiency since a large number of participating edge devices send their\nupdates to the edge server at each round of the model training. Existing works\nreconstruct each model update from edge devices and implicitly assume that the\nlocal model updates are independent over edge devices. In FL, however, the\nmodel update is an indirect multi-terminal source coding problem, also called\nas the CEO problem where each edge device cannot observe directly the gradient\nthat is to be reconstructed at the decoder, but is rather provided only with a\nnoisy version. The existing works do not leverage the redundancy in the\ninformation transmitted by different edges. This paper studies the rate region\nfor the indirect multiterminal source coding problem in FL. The goal is to\nobtain the minimum achievable rate at a particular upper bound of gradient\nvariance. We obtain the rate region for the quadratic vector Gaussian CEO\nproblem under unbiased estimator and derive an explicit formula of the\nsum-rate-distortion function in the special case where gradient are identical\nover edge device and dimension. Finally, we analyse communication efficiency of\nconvex Minibatched SGD and non-convex Minibatched SGD based on the\nsum-rate-distortion function, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 16:17:39 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 13:24:24 GMT"}, {"version": "v3", "created": "Thu, 13 May 2021 13:53:10 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Zhang", "Naifu", ""], ["Tao", "Meixia", ""], ["Wang", "Jia", ""]]}, {"id": "2101.08743", "submitter": "Wenjie Chen", "authors": "Wenjie Chen, Shengcai Liu, and Ke Tang", "title": "A New Knowledge Gradient-based Method for Constrained Bayesian\n  Optimization", "comments": "14 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Black-box problems are common in real life like structural design, drug\nexperiments, and machine learning. When optimizing black-box systems,\ndecision-makers always consider multiple performances and give the final\ndecision by comprehensive evaluations. Motivated by such practical needs, we\nfocus on constrained black-box problems where the objective and constraints\nlack known special structure, and evaluations are expensive and even with\nnoise. We develop a novel constrained Bayesian optimization approach based on\nthe knowledge gradient method ($c-\\rm{KG}$). A new acquisition function is\nproposed to determine the next batch of samples considering optimality and\nfeasibility. An unbiased estimator of the gradient of the new acquisition\nfunction is derived to implement the $c-\\rm{KG}$ approach.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 05:00:38 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Chen", "Wenjie", ""], ["Liu", "Shengcai", ""], ["Tang", "Ke", ""]]}, {"id": "2101.08917", "submitter": "Vincent Tan", "authors": "Anshoo Tandon, Aldric H. J. Yuan, Vincent Y. F. Tan", "title": "SGA: A Robust Algorithm for Partial Recovery of Tree-Structured\n  Graphical Models with Noisy Samples", "comments": "23 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider learning Ising tree models when the observations from the nodes\nare corrupted by independent but non-identically distributed noise with unknown\nstatistics. Katiyar et al. (2020) showed that although the exact tree structure\ncannot be recovered, one can recover a partial tree structure; that is, a\nstructure belonging to the equivalence class containing the true tree. This\npaper presents a systematic improvement of Katiyar et al. (2020). First, we\npresent a novel impossibility result by deriving a bound on the necessary\nnumber of samples for partial recovery. Second, we derive a significantly\nimproved sample complexity result in which the dependence on the minimum\ncorrelation $\\rho_{\\min}$ is $\\rho_{\\min}^{-8}$ instead of $\\rho_{\\min}^{-24}$.\nFinally, we propose Symmetrized Geometric Averaging (SGA), a more statistically\nrobust algorithm for partial tree recovery. We provide error exponent analyses\nand extensive numerical results on a variety of trees to show that the sample\ncomplexity of SGA is significantly better than the algorithm of Katiyar et al.\n(2020). SGA can be readily extended to Gaussian models and is shown via\nnumerical experiments to be similarly superior.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 01:57:35 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Tandon", "Anshoo", ""], ["Yuan", "Aldric H. J.", ""], ["Tan", "Vincent Y. F.", ""]]}, {"id": "2101.08925", "submitter": "Puyu Wang", "authors": "Puyu Wang, Yunwen Lei, Yiming Ying, Hai Zhang", "title": "Differentially Private SGD with Non-Smooth Losses", "comments": "29 pages, 1 table, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we are concerned with differentially private {stochastic\ngradient descent (SGD)} algorithms in the setting of stochastic convex\noptimization (SCO). Most of the existing work requires the loss to be Lipschitz\ncontinuous and strongly smooth, and the model parameter to be uniformly\nbounded. However, these assumptions are restrictive as many popular losses\nviolate these conditions including the hinge loss for SVM, the absolute loss in\nrobust regression, and even the least square loss in an unbounded domain. We\nsignificantly relax these restrictive assumptions and establish privacy and\ngeneralization (utility) guarantees for private SGD algorithms using output and\ngradient perturbations associated with non-smooth convex losses. Specifically,\nthe loss function is relaxed to have an $\\alpha$-H\\\"{o}lder continuous gradient\n(referred to as $\\alpha$-H\\\"{o}lder smoothness) which instantiates the\nLipschitz continuity ($\\alpha=0$) and the strong smoothness ($\\alpha=1$). We\nprove that noisy SGD with $\\alpha$-H\\\"older smooth losses using gradient\nperturbation can guarantee $(\\epsilon,\\delta)$-differential privacy (DP) and\nattain optimal excess population risk\n$\\mathcal{O}\\Big(\\frac{\\sqrt{d\\log(1/\\delta)}}{n\\epsilon}+\\frac{1}{\\sqrt{n}}\\Big)$,\nup to logarithmic terms, with the gradient complexity $ \\mathcal{O}(\nn^{2-\\alpha\\over 1+\\alpha}+ n).$ This shows an important trade-off between\n$\\alpha$-H\\\"older smoothness of the loss and the computational complexity for\nprivate SGD with statistically optimal performance. In particular, our results\nindicate that $\\alpha$-H\\\"older smoothness with $\\alpha\\ge {1/2}$ is sufficient\nto guarantee $(\\epsilon,\\delta)$-DP of noisy SGD algorithms while achieving\noptimal excess risk with the linear gradient complexity $\\mathcal{O}(n).$\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 03:19:06 GMT"}, {"version": "v2", "created": "Sat, 26 Jun 2021 10:34:48 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Wang", "Puyu", ""], ["Lei", "Yunwen", ""], ["Ying", "Yiming", ""], ["Zhang", "Hai", ""]]}, {"id": "2101.08954", "submitter": "Yuling Yao", "authors": "Yuling Yao, Gregor Pir\\v{s}, Aki Vehtari, Andrew Gelman", "title": "Bayesian hierarchical stacking: Some models are (somewhere) useful", "comments": "minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stacking is a widely used model averaging technique that asymptotically\nyields optimal predictions among linear averages. We show that stacking is most\neffective when model predictive performance is heterogeneous in inputs, and we\ncan further improve the stacked mixture with a hierarchical model. We\ngeneralize stacking to Bayesian hierarchical stacking. The model weights are\nvarying as a function of data, partially-pooled, and inferred using Bayesian\ninference. We further incorporate discrete and continuous inputs, other\nstructured priors, and time series and longitudinal data. To verify the\nperformance gain of the proposed method, we derive theory bounds, and\ndemonstrate on several applied problems.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 05:19:49 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 22:14:30 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Yao", "Yuling", ""], ["Pir\u0161", "Gregor", ""], ["Vehtari", "Aki", ""], ["Gelman", "Andrew", ""]]}, {"id": "2101.09001", "submitter": "Martin Hellkvist", "authors": "Martin Hellkvist and Ay\\c{c}a \\\"Oz\\c{c}elikkale and Anders Ahl\\'en", "title": "Linear Regression with Distributed Learning: A Generalization Error\n  Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed learning provides an attractive framework for scaling the\nlearning task by sharing the computational load over multiple nodes in a\nnetwork. Here, we investigate the performance of distributed learning for\nlarge-scale linear regression where the model parameters, i.e., the unknowns,\nare distributed over the network. We adopt a statistical learning approach. In\ncontrast to works that focus on the performance on the training data, we focus\non the generalization error, i.e., the performance on unseen data. We provide\nhigh-probability bounds on the generalization error for both isotropic and\ncorrelated Gaussian data as well as sub-gaussian data. These results reveal the\ndependence of the generalization performance on the partitioning of the model\nover the network. In particular, our results show that the generalization error\nof the distributed solution can be substantially higher than that of the\ncentralized solution even when the error on the training data is at the same\nlevel for both the centralized and distributed approaches. Our numerical\nresults illustrate the performance with both real-world image data as well as\nsynthetic data.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 08:43:28 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 14:16:24 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Hellkvist", "Martin", ""], ["\u00d6z\u00e7elikkale", "Ay\u00e7a", ""], ["Ahl\u00e9n", "Anders", ""]]}, {"id": "2101.09039", "submitter": "Matteo Pegoraro", "authors": "Matteo Pegoraro and Mario Beraha", "title": "Projected Statistical Methods for Distributional Data on the Real Line\n  with the Wasserstein Metric", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel class of projected methods, to perform statistical\nanalysis on a data set of probability distributions on the real line, with the\n2-Wasserstein metric. We focus in particular on Principal Component Analysis\n(PCA) and regression. To define these models, we exploit a representation of\nthe Wasserstein space closely related to its weak Riemannian structure, by\nmapping the data to a suitable linear space and using a metric projection\noperator to constrain the results in the Wasserstein space. By carefully\nchoosing the tangent point, we are able to derive fast empirical methods,\nexploiting a constrained B-spline approximation. As a byproduct of our\napproach, we are also able to derive faster routines for previous work on PCA\nfor distributions. By means of simulation studies, we compare our approaches to\npreviously proposed methods, showing that our projected PCA has similar\nperformance for a fraction of the computational cost and that the projected\nregression is extremely flexible even under misspecification. Several\ntheoretical properties of the models are investigated and asymptotic\nconsistency is proven. Two real world applications to Covid-19 mortality in the\nUS and wind speed forecasting are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 10:24:49 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Pegoraro", "Matteo", ""], ["Beraha", "Mario", ""]]}, {"id": "2101.09054", "submitter": "Yuval Dagan", "authors": "Noga Alon, Omri Ben-Eliezer, Yuval Dagan, Shay Moran, Moni Naor, Eylon\n  Yogev", "title": "Adversarial Laws of Large Numbers and Optimal Regret in Online\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Laws of large numbers guarantee that given a large enough sample from some\npopulation, the measure of any fixed sub-population is well-estimated by its\nfrequency in the sample. We study laws of large numbers in sampling processes\nthat can affect the environment they are acting upon and interact with it.\nSpecifically, we consider the sequential sampling model proposed by Ben-Eliezer\nand Yogev (2020), and characterize the classes which admit a uniform law of\nlarge numbers in this model: these are exactly the classes that are\n\\emph{online learnable}. Our characterization may be interpreted as an online\nanalogue to the equivalence between learnability and uniform convergence in\nstatistical (PAC) learning.\n  The sample-complexity bounds we obtain are tight for many parameter regimes,\nand as an application, we determine the optimal regret bounds in online\nlearning, stated in terms of \\emph{Littlestone's dimension}, thus resolving the\nmain open question from Ben-David, P\\'al, and Shalev-Shwartz (2009), which was\nalso posed by Rakhlin, Sridharan, and Tewari (2015).\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 11:15:19 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Alon", "Noga", ""], ["Ben-Eliezer", "Omri", ""], ["Dagan", "Yuval", ""], ["Moran", "Shay", ""], ["Naor", "Moni", ""], ["Yogev", "Eylon", ""]]}, {"id": "2101.09126", "submitter": "Christopher Irrgang", "authors": "Christopher Irrgang (1), Niklas Boers (2 and 3 and 4), Maike Sonnewald\n  (5 and 6 and 7), Elizabeth A. Barnes (8), Christopher Kadow (9), Joanna\n  Staneva (10), Jan Saynisch-Wagner (1) ((1) Helmholtz Centre Potsdam, German\n  Research Centre for Geosciences GFZ, Potsdam, Germany, (2) Department of\n  Mathematics and Computer Science, Free University of Berlin, Germany, (3)\n  Potsdam Institute for Climate Impact Research, Potsdam, Germany (4)\n  Department of Mathematics and Global Systems Institute, University of Exeter,\n  Exeter, UK (5) Program in Atmospheric and Oceanic Sciences, Princeton\n  University, Princeton, USA (6) NOAA/OAR Geophysical Fluid Dynamics\n  Laboratory, Ocean and Cryosphere Division, Princeton, USA (7) University of\n  Washington, School of Oceanography, Seattle, USA (8) Colorado State\n  University, Fort Collins, USA (9) German Climate Computing Center DKRZ,\n  Hamburg, Germany (10) Helmholtz-Zentrum Geesthacht, Center for Material and\n  Coastal Research HZG, Geesthacht, Germany)", "title": "Will Artificial Intelligence supersede Earth System and Climate Models?", "comments": "Perspective paper submitted to Nature Machine Intelligence, 23 pages,\n  3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.ao-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We outline a perspective of an entirely new research branch in Earth and\nclimate sciences, where deep neural networks and Earth system models are\ndismantled as individual methodological approaches and reassembled as learning,\nself-validating, and interpretable Earth system model-network hybrids.\nFollowing this path, we coin the term \"Neural Earth System Modelling\" (NESYM)\nand highlight the necessity of a transdisciplinary discussion platform,\nbringing together Earth and climate scientists, big data analysts, and AI\nexperts. We examine the concurrent potential and pitfalls of Neural Earth\nSystem Modelling and discuss the open question whether artificial intelligence\nwill not only infuse Earth system modelling, but ultimately render them\nobsolete.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 14:33:24 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Irrgang", "Christopher", "", "2 and 3 and 4"], ["Boers", "Niklas", "", "2 and 3 and 4"], ["Sonnewald", "Maike", "", "5 and 6 and 7"], ["Barnes", "Elizabeth A.", ""], ["Kadow", "Christopher", ""], ["Staneva", "Joanna", ""], ["Saynisch-Wagner", "Jan", ""]]}, {"id": "2101.09174", "submitter": "Anindya Sundar Chakrabarti", "authors": "Arnab Chakrabarti and Anindya S. Chakrabarti", "title": "Sparsistent filtering of comovement networks from high-dimensional data", "comments": "31 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Network filtering is an important form of dimension reduction to isolate the\ncore constituents of large and interconnected complex systems. We introduce a\nnew technique to filter large dimensional networks arising out of dynamical\nbehavior of the constituent nodes, exploiting their spectral properties. As\nopposed to the well known network filters that rely on preserving key\ntopological properties of the realized network, our method treats the spectrum\nas the fundamental object and preserves spectral properties. Applying\nasymptotic theory for high dimensional data for the filter, we show that it can\nbe tuned to interpolate between zero filtering to maximal filtering that\ninduces sparsity and consistency while having the least spectral distance from\na linear shrinkage estimator. We apply our proposed filter to covariance\nnetworks constructed from financial data, to extract the key subnetwork\nembedded in the full sample network.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 15:44:41 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Chakrabarti", "Arnab", ""], ["Chakrabarti", "Anindya S.", ""]]}, {"id": "2101.09258", "submitter": "Yang Song", "authors": "Yang Song and Conor Durkan and Iain Murray and Stefano Ermon", "title": "Maximum Likelihood Training of Score-Based Diffusion Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Score-based diffusion models synthesize samples by reversing a stochastic\nprocess that diffuses data to noise, and are trained by minimizing a weighted\ncombination of score matching losses. The log-likelihood of score-based models\ncan be tractably computed through a connection to continuous normalizing flows,\nbut log-likelihood is not directly optimized by the weighted combination of\nscore matching losses. We show that for a specific weighting scheme, the\nobjective upper bounds the negative log-likelihood, thus enabling approximate\nmaximum likelihood training of score-based models. We empirically observe that\nmaximum likelihood training consistently improves the likelihood of score-based\nmodels across multiple datasets, stochastic processes, and model architectures.\nOur best models achieve negative log-likelihoods of 2.74 and 3.76 bits/dim on\nCIFAR-10 and ImageNet 32x32, outperforming autoregressive models on these\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 18:22:29 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 04:42:19 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 04:48:04 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Song", "Yang", ""], ["Durkan", "Conor", ""], ["Murray", "Iain", ""], ["Ermon", "Stefano", ""]]}, {"id": "2101.09271", "submitter": "Eliana Duarte", "authors": "Eliana Duarte, Liam Solus", "title": "Representation of Context-Specific Causal Models with Observational and\n  Interventional Data", "comments": "26 pages, supplementary material 18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.CO stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of representing causal models that encode\ncontext-specific information for discrete data. To represent such models we use\na proper subclass of staged tree models which we call CStrees. We show that the\ncontext-specific information encoded by a CStree can be equivalently expressed\nvia a collection of DAGs. As not all staged tree models admit this property,\nCStrees are a subclass that provides a transparent, intuitive and compact\nrepresentation of context-specific causal information. Model equivalence for\nCStrees also takes a simpler form than for general staged trees: We provide a\ncharacterization of the complete set of asymmetric conditional independence\nrelations encoded by a CStree. As a consequence, we obtain a global Markov\nproperty for CStrees which leads to a graphical criterion of model equivalence\nfor CStrees generalizing that of Verma and Pearl for DAG models. In addition,\nwe provide a closed-form formula for the maximum likelihood estimator of a\nCStree and use it to show that the Bayesian information criterion is a locally\nconsistent score function for this model class. We also give an analogous\nglobal Markov property and characterization of model equivalence for general\ninterventions in CStrees. As examples, we apply these results to two real data\nsets, and examine how BIC-optimal CStrees for each provide a clear and concise\nrepresentation of the learned context-specific causal structure.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 18:48:29 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 17:10:20 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Duarte", "Eliana", ""], ["Solus", "Liam", ""]]}, {"id": "2101.09315", "submitter": "Borja Rodr\\'iguez G\\'alvez", "authors": "Borja Rodr\\'iguez-G\\'alvez, Germ\\'an Bassi, Ragnar Thobaben, and\n  Mikael Skoglund", "title": "Tighter expected generalization error bounds via Wasserstein distance", "comments": "22 pages: 12 of the main text, 2 of references, and 8 of appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we introduce several expected generalization error bounds based\non the Wasserstein distance. More precisely, we present full-dataset,\nsingle-letter, and random-subset bounds on both the standard setting and the\nrandomized-subsample setting from Steinke and Zakynthinou [2020]. Moreover, we\nshow that, when the loss function is bounded, these bounds recover from below\n(and thus are tighter than) current bounds based on the relative entropy and,\nfor the standard setting, generate new, non-vacuous bounds also based on the\nrelative entropy. Then, we show how similar bounds featuring the backward\nchannel can be derived with the proposed proof techniques. Finally, we show how\nvarious new bounds based on different information measures (e.g., the lautum\ninformation or several $f$-divergences) can be derived from the presented\nbounds.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 20:13:59 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Rodr\u00edguez-G\u00e1lvez", "Borja", ""], ["Bassi", "Germ\u00e1n", ""], ["Thobaben", "Ragnar", ""], ["Skoglund", "Mikael", ""]]}, {"id": "2101.09394", "submitter": "Jaehyuk Choi", "authors": "Jaehyuk Choi, Desheng Ge, Kyu Ho Kang, Sungbin Sohn", "title": "Predicting Recession Probabilities Using Term Spreads: New Evidence from\n  a Machine Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The literature on using yield curves to forecast recessions typically\nmeasures the term spread as the difference between the 10-year and the\nthree-month Treasury rates. Furthermore, using the term spread constrains the\nlong- and short-term interest rates to have the same absolute effect on the\nrecession probability. In this study, we adopt a machine learning method to\ninvestigate whether the predictive ability of interest rates can be improved.\nThe machine learning algorithm identifies the best maturity pair, separating\nthe effects of interest rates from those of the term spread. Our comprehensive\nempirical exercise shows that, despite the likelihood gain, the machine\nlearning approach does not significantly improve the predictive accuracy, owing\nto the estimation error. Our finding supports the conventional use of the\n10-year--three-month Treasury yield spread. This is robust to the forecasting\nhorizon, control variable, sample period, and oversampling of the recession\nobservations.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 01:26:54 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Choi", "Jaehyuk", ""], ["Ge", "Desheng", ""], ["Kang", "Kyu Ho", ""], ["Sohn", "Sungbin", ""]]}, {"id": "2101.09436", "submitter": "Xudong Sun", "authors": "Xudong Sun, Florian Buettner", "title": "Hierarchical Variational Auto-Encoding for Unsupervised Domain\n  Generalization", "comments": "Presented at ICLR 2021 RobustML Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the task of domain generalization, where the goal is to train a\npredictive model such that it is able to generalize to a new, previously unseen\ndomain. We choose a hierarchical generative approach within the framework of\nvariational autoencoders and propose a domain-unsupervised algorithm that is\nable to generalize to new domains without domain supervision. We show that our\nmethod is able to learn representations that disentangle domain-specific\ninformation from class-label specific information even in complex settings\nwhere domain structure is not observed during training. Our interpretable\nmethod outperforms previously proposed generative algorithms for domain\ngeneralization as well as other non-generative state-of-the-art approaches in\nseveral hierarchical domain settings including sequential overlapped near\ncontinuous domain shift. It also achieves competitive performance on the\nstandard domain generalization benchmark dataset PACS compared to\nstate-of-the-art approaches which rely on observing domain-specific information\nduring training, as well as another domain unsupervised method. Additionally,\nwe proposed model selection purely based on Evidence Lower Bound (ELBO) and\nalso proposed weak domain supervision where implicit domain information can be\nadded into the algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 07:09:59 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 19:00:48 GMT"}, {"version": "v3", "created": "Sat, 27 Feb 2021 13:35:03 GMT"}, {"version": "v4", "created": "Thu, 6 May 2021 17:36:04 GMT"}, {"version": "v5", "created": "Fri, 14 May 2021 20:51:15 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Sun", "Xudong", ""], ["Buettner", "Florian", ""]]}, {"id": "2101.09438", "submitter": "Dheeraj Baby", "authors": "Dheeraj Baby and Xuandong Zhao and Yu-Xiang Wang", "title": "An Optimal Reduction of TV-Denoising to Adaptive Online Learning", "comments": "To appear at AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider the problem of estimating a function from $n$ noisy samples whose\ndiscrete Total Variation (TV) is bounded by $C_n$. We reveal a deep connection\nto the seemingly disparate problem of Strongly Adaptive online learning\n(Daniely et al, 2015) and provide an $O(n \\log n)$ time algorithm that attains\nthe near minimax optimal rate of $\\tilde O (n^{1/3}C_n^{2/3})$ under squared\nerror loss. The resulting algorithm runs online and optimally adapts to the\nunknown smoothness parameter $C_n$. This leads to a new and more versatile\nalternative to wavelets-based methods for (1) adaptively estimating TV bounded\nfunctions; (2) online forecasting of TV bounded trends in time series.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 07:13:53 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 06:55:03 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Baby", "Dheeraj", ""], ["Zhao", "Xuandong", ""], ["Wang", "Yu-Xiang", ""]]}, {"id": "2101.09446", "submitter": "Yunzhen Yao", "authors": "Yunzhen Yao, Liangzu Peng and Manolis C. Tsakiris", "title": "Unlabeled Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of principal component analysis from a data matrix\nwhere the entries of each column have undergone some unknown permutation,\ntermed Unlabeled Principal Component Analysis (UPCA). Using algebraic geometry,\nwe establish that for generic enough data, and up to a permutation of the\ncoordinates of the ambient space, there is a unique subspace of minimal\ndimension that explains the data. We show that a permutation-invariant system\nof polynomial equations has finitely many solutions, with each solution\ncorresponding to a row permutation of the ground-truth data matrix. Allowing\nfor missing entries on top of permutations leads to the problem of unlabeled\nmatrix completion, for which we give theoretical results of similar flavor. We\nalso propose a two-stage algorithmic pipeline for UPCA suitable for the\npractically relevant case where only a fraction of the data has been permuted.\nStage-I of this pipeline employs robust-PCA methods to estimate the\nground-truth column-space. Equipped with the column-space, stage-II applies\nmethods for linear regression without correspondences to restore the permuted\ndata. A computational study reveals encouraging findings, including the ability\nof UPCA to handle face images from the Extended Yale-B database with\narbitrarily permuted patches of arbitrary size in $0.3$ seconds on a standard\ndesktop computer.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 07:34:48 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Yao", "Yunzhen", ""], ["Peng", "Liangzu", ""], ["Tsakiris", "Manolis C.", ""]]}, {"id": "2101.09460", "submitter": "Sodiq Adewole", "authors": "Sali Rasoul, Sodiq Adewole, Alphonse Akakpo", "title": "Feature Selection Using Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the decreasing cost of data collection, the space of variables or\nfeatures that can be used to characterize a particular predictor of interest\ncontinues to grow exponentially. Therefore, identifying the most characterizing\nfeatures that minimizes the variance without jeopardizing the bias of our\nmodels is critical to successfully training a machine learning model. In\naddition, identifying such features is critical for interpretability,\nprediction accuracy and optimal computation cost. While statistical methods\nsuch as subset selection, shrinkage, dimensionality reduction have been applied\nin selecting the best set of features, some other approaches in literature have\napproached feature selection task as a search problem where each state in the\nsearch space is a possible feature subset. In this paper, we solved the feature\nselection problem using Reinforcement Learning. Formulating the state space as\na Markov Decision Process (MDP), we used Temporal Difference (TD) algorithm to\nselect the best subset of features. Each state was evaluated using a robust and\nlow cost classifier algorithm which could handle any non-linearities in the\ndataset.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 09:24:37 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Rasoul", "Sali", ""], ["Adewole", "Sodiq", ""], ["Akakpo", "Alphonse", ""]]}, {"id": "2101.09512", "submitter": "Karthigan Sinnathamby", "authors": "Karthigan Sinnathamby, Chang-Yu Hou, Lalitha Venkataramanan,\n  Vasileios-Marios Gkortsas, Fran\\c{c}ois Fleuret", "title": "Unsupervised clustering of series using dynamic programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in clustering parts of a given single multi-variate series\nin an unsupervised manner. We would like to segment and cluster the series such\nthat the resulting blocks present in each cluster are coherent with respect to\na known model (e.g. physics model). Data points are said to be coherent if they\ncan be described using this model with the same parameters. We have designed an\nalgorithm based on dynamic programming with constraints on the number of\nclusters, the number of transitions as well as the minimal size of a block such\nthat the clusters are coherent with this process. We present an use-case:\nclustering of petrophysical series using the Waxman-Smits equation.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 14:35:35 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Sinnathamby", "Karthigan", ""], ["Hou", "Chang-Yu", ""], ["Venkataramanan", "Lalitha", ""], ["Gkortsas", "Vasileios-Marios", ""], ["Fleuret", "Fran\u00e7ois", ""]]}, {"id": "2101.09577", "submitter": "Bla\\v{z} \\v{S}krlj", "authors": "Bla\\v{z} \\v{S}krlj, Sa\\v{s}o D\\v{z}eroski, Nada Lavra\\v{c} and Matej\n  Petkovi\\'c", "title": "ReliefE: Feature Ranking in High-dimensional Spaces via Manifold\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Feature ranking has been widely adopted in machine learning applications such\nas high-throughput biology and social sciences. The approaches of the popular\nRelief family of algorithms assign importances to features by iteratively\naccounting for nearest relevant and irrelevant instances. Despite their high\nutility, these algorithms can be computationally expensive and not-well suited\nfor high-dimensional sparse input spaces. In contrast, recent embedding-based\nmethods learn compact, low-dimensional representations, potentially\nfacilitating down-stream learning capabilities of conventional learners. This\npaper explores how the Relief branch of algorithms can be adapted to benefit\nfrom (Riemannian) manifold-based embeddings of instance and target spaces,\nwhere a given embedding's dimensionality is intrinsic to the dimensionality of\nthe considered data set. The developed ReliefE algorithm is faster and can\nresult in better feature rankings, as shown by our evaluation on 20 real-life\ndata sets for multi-class and multi-label classification tasks. The utility of\nReliefE for high-dimensional data sets is ensured by its implementation that\nutilizes sparse matrix algebraic operations. Finally, the relation of ReliefE\nto other ranking algorithms is studied via the Fuzzy Jaccard Index.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 20:23:31 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["\u0160krlj", "Bla\u017e", ""], ["D\u017eeroski", "Sa\u0161o", ""], ["Lavra\u010d", "Nada", ""], ["Petkovi\u0107", "Matej", ""]]}, {"id": "2101.09611", "submitter": "Philip Chodrow", "authors": "Philip S. Chodrow, Nate Veldt, and Austin R. Benson", "title": "Generative hypergraph clustering: from blockmodels to modularity", "comments": "23 pages + 7 pages of supplementary information, 3 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DM physics.data-an physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypergraphs are a natural modeling paradigm for a wide range of complex\nrelational systems. A standard analysis task is to identify clusters of closely\nrelated or densely interconnected nodes. Many graph algorithms for this task\nare based on variants of the stochastic blockmodel, a random graph with\nflexible cluster structure. However, there are few models and algorithms for\nhypergraph clustering. Here, we propose a Poisson degree-corrected hypergraph\nstochastic blockmodel (DCHSBM), a generative model of clustered hypergraphs\nwith heterogeneous node degrees and edge sizes. Approximate maximum-likelihood\ninference in the DCHSBM naturally leads to a clustering objective that\ngeneralizes the popular modularity objective for graphs. We derive a general\nLouvain-type algorithm for this objective, as well as a a faster, specialized\n\"All-Or-Nothing\" (AON) variant in which edges are expected to lie fully within\nclusters. This special case encompasses a recent proposal for modularity in\nhypergraphs, while also incorporating flexible resolution and edge-size\nparameters. We show that AON hypergraph Louvain is highly scalable, including\nas an example an experiment on a synthetic hypergraph of one million nodes. We\nalso demonstrate through synthetic experiments that the detectability regimes\nfor hypergraph community detection differ from methods based on dyadic graph\nprojections. We use our generative model to analyze different patterns of\nhigher-order structure in school contact networks, U.S. congressional bill\ncosponsorship, U.S. congressional committees, product categories in\nco-purchasing behavior, and hotel locations from web browsing sessions, finding\ninterpretable higher-order structure. We then study the behavior of our AON\nhypergraph Louvain algorithm, finding that it is able to recover ground truth\nclusters in empirical data sets exhibiting the corresponding higher-order\nstructure.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 00:25:22 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 21:26:58 GMT"}, {"version": "v3", "created": "Tue, 27 Apr 2021 00:01:19 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Chodrow", "Philip S.", ""], ["Veldt", "Nate", ""], ["Benson", "Austin R.", ""]]}, {"id": "2101.09612", "submitter": "Quynh Nguyen", "authors": "Quynh Nguyen", "title": "On the Proof of Global Convergence of Gradient Descent for Deep ReLU\n  Networks with Linear Widths", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a simple proof for the global convergence of gradient descent in\ntraining deep ReLU networks with the standard square loss, and show some of its\nimprovements over the state-of-the-art. In particular, while prior works\nrequire all the hidden layers to be wide with width at least $\\Omega(N^8)$ ($N$\nbeing the number of training samples), we require a single wide layer of\nlinear, quadratic or cubic width depending on the type of initialization.\nUnlike many recent proofs based on the Neural Tangent Kernel (NTK), our proof\nneed not track the evolution of the entire NTK matrix, or more generally, any\nquantities related to the changes of activation patterns during training.\nInstead, we only need to track the evolution of the output at the last hidden\nlayer, which can be done much more easily thanks to the Lipschitz property of\nReLU. Some highlights of our setting: (i) all the layers are trained with\nstandard gradient descent, (ii) the network has standard parameterization as\nopposed to the NTK one, and (iii) the network has a single wide layer as\nopposed to having all wide hidden layers as in most of NTK-related results.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 00:29:19 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 21:18:44 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 07:39:59 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Nguyen", "Quynh", ""]]}, {"id": "2101.09645", "submitter": "Zekai Chen", "authors": "Zekai Chen, Jiaze E, Xiao Zhang, Hao Sheng, Xiuzheng Cheng", "title": "Multi-Task Time Series Forecasting With Shared Attention", "comments": "Accepted by ICDMW 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series forecasting is a key component in many industrial and business\ndecision processes and recurrent neural network (RNN) based models have\nachieved impressive progress on various time series forecasting tasks. However,\nmost of the existing methods focus on single-task forecasting problems by\nlearning separately based on limited supervised objectives, which often suffer\nfrom insufficient training instances. As the Transformer architecture and other\nattention-based models have demonstrated its great capability of capturing long\nterm dependency, we propose two self-attention based sharing schemes for\nmulti-task time series forecasting which can train jointly across multiple\ntasks. We augment a sequence of paralleled Transformer encoders with an\nexternal public multi-head attention function, which is updated by all data of\nall tasks. Experiments on a number of real-world multi-task time series\nforecasting tasks show that our proposed architectures can not only outperform\nthe state-of-the-art single-task forecasting baselines but also outperform the\nRNN-based multi-task forecasting method.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 04:25:08 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Chen", "Zekai", ""], ["E", "Jiaze", ""], ["Zhang", "Xiao", ""], ["Sheng", "Hao", ""], ["Cheng", "Xiuzheng", ""]]}, {"id": "2101.09682", "submitter": "John Ery", "authors": "John Ery and Loris Michel", "title": "Solving optimal stopping problems with Deep Q-Learning", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a reinforcement learning (RL) approach to model optimal exercise\nstrategies for option-type products. We pursue the RL avenue in order to learn\nthe optimal action-value function of the underlying stopping problem. In\naddition to retrieving the optimal Q-function at any time step, one can also\nprice the contract at inception. We first discuss the standard setting with one\nexercise right, and later extend this framework to the case of multiple\nstopping opportunities in the presence of constraints. We propose to\napproximate the Q-function with a deep neural network, which does not require\nthe specification of basis functions as in the least-squares Monte Carlo\nframework and is scalable to higher dimensions. We derive a lower bound on the\noption price obtained from the trained neural network and an upper bound from\nthe dual formulation of the stopping problem, which can also be expressed in\nterms of the Q-function. Our methodology is illustrated with examples covering\nthe pricing of swing options.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 10:05:46 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Ery", "John", ""], ["Michel", "Loris", ""]]}, {"id": "2101.09747", "submitter": "Emmanuel Vazquez", "authors": "Subhasish Basak, S\\'ebastien Petit, Julien Bect, Emmanuel Vazquez", "title": "Numerical issues in maximum likelihood parameter estimation for Gaussian\n  process interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This article investigates the origin of numerical issues in maximum\nlikelihood parameter estimation for Gaussian process (GP) interpolation and\ninvestigates simple but effective strategies for improving commonly used\nopen-source software implementations. This work targets a basic problem but a\nhost of studies, particularly in the literature of Bayesian optimization, rely\non off-the-shelf GP implementations. For the conclusions of these studies to be\nreliable and reproducible, robust GP implementations are critical.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 16:30:55 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 19:31:13 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Basak", "Subhasish", ""], ["Petit", "S\u00e9bastien", ""], ["Bect", "Julien", ""], ["Vazquez", "Emmanuel", ""]]}, {"id": "2101.09756", "submitter": "Tam Le", "authors": "Tam Le, Truyen Nguyen", "title": "Entropy Partial Transport with Tree Metrics: Theory and Practice", "comments": "To appear in AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optimal transport (OT) theory provides powerful tools to compare probability\nmeasures. However, OT is limited to nonnegative measures having the same mass,\nand suffers serious drawbacks about its computation and statistics. This leads\nto several proposals of regularized variants of OT in the recent literature. In\nthis work, we consider an \\textit{entropy partial transport} (EPT) problem for\nnonnegative measures on a tree having different masses. The EPT is shown to be\nequivalent to a standard complete OT problem on a one-node extended tree. We\nderive its dual formulation, then leverage this to propose a novel\nregularization for EPT which admits fast computation and negative definiteness.\nTo our knowledge, the proposed regularized EPT is the first approach that\nyields a \\textit{closed-form} solution among available variants of unbalanced\nOT. For practical applications without priori knowledge about the tree\nstructure for measures, we propose tree-sliced variants of the regularized EPT,\ncomputed by averaging the regularized EPT between these measures using random\ntree metrics, built adaptively from support data points. Exploiting the\nnegative definiteness of our regularized EPT, we introduce a positive definite\nkernel, and evaluate it against other baselines on benchmark tasks such as\ndocument classification with word embedding and topological data analysis. In\naddition, we empirically demonstrate that our regularization also provides\neffective approximations.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 17:04:24 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Le", "Tam", ""], ["Nguyen", "Truyen", ""]]}, {"id": "2101.09763", "submitter": "Michael A. Hedderich", "authors": "Michael A. Hedderich, Dawei Zhu, Dietrich Klakow", "title": "Analysing the Noise Model Error for Realistic Noisy Label Data", "comments": "Accepted at AAAI 2021, additional material at\n  https://github.com/uds-lsv/noise-estimation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distant and weak supervision allow to obtain large amounts of labeled\ntraining data quickly and cheaply, but these automatic annotations tend to\ncontain a high amount of errors. A popular technique to overcome the negative\neffects of these noisy labels is noise modelling where the underlying noise\nprocess is modelled. In this work, we study the quality of these estimated\nnoise models from the theoretical side by deriving the expected error of the\nnoise model. Apart from evaluating the theoretical results on commonly used\nsynthetic noise, we also publish NoisyNER, a new noisy label dataset from the\nNLP domain that was obtained through a realistic distant supervision technique.\nIt provides seven sets of labels with differing noise patterns to evaluate\ndifferent noise levels on the same instances. Parallel, clean labels are\navailable making it possible to study scenarios where a small amount of\ngold-standard data can be leveraged. Our theoretical results and the\ncorresponding experiments give insights into the factors that influence the\nnoise model estimation like the noise distribution and the sampling technique.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 17:45:15 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 11:14:54 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Hedderich", "Michael A.", ""], ["Zhu", "Dawei", ""], ["Klakow", "Dietrich", ""]]}, {"id": "2101.09809", "submitter": "Lin Qiu", "authors": "Lin Qiu, Nils Murrugarra-Llerena, V\\'itor Silva, Lin Lin, Vernon M.\n  Chinchilli", "title": "NeurT-FDR: Controlling FDR by Incorporating Feature Hierarchy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controlling false discovery rate (FDR) while leveraging the side information\nof multiple hypothesis testing is an emerging research topic in modern data\nscience. Existing methods rely on the test-level covariates while ignoring\npossible hierarchy among the covariates. This strategy may not be optimal for\ncomplex large-scale problems, where hierarchical information often exists among\nthose test-level covariates. We propose NeurT-FDR which boosts statistical\npower and controls FDR for multiple hypothesis testing while leveraging the\nhierarchy among test-level covariates. Our method parametrizes the test-level\ncovariates as a neural network and adjusts the feature hierarchy through a\nregression framework, which enables flexible handling of high-dimensional\nfeatures as well as efficient end-to-end optimization. We show that NeurT-FDR\nhas strong FDR guarantees and makes substantially more discoveries in synthetic\nand real datasets compared to competitive baselines.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 21:55:10 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Qiu", "Lin", ""], ["Murrugarra-Llerena", "Nils", ""], ["Silva", "V\u00edtor", ""], ["Lin", "Lin", ""], ["Chinchilli", "Vernon M.", ""]]}, {"id": "2101.09844", "submitter": "Stanislav Sobolevsky", "authors": "Shivam Pathak, Mingyi He, Sergey Malinchik, Stanislav Sobolevsky", "title": "Pattern Ensembling for Spatial Trajectory Reconstruction", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital sensing provides an unprecedented opportunity to assess and\nunderstand mobility. However, incompleteness, missing information, possible\ninaccuracies, and temporal heterogeneity in the geolocation data can undermine\nits applicability. As mobility patterns are often repeated, we propose a method\nto use similar trajectory patterns from the local vicinity and\nprobabilistically ensemble them to robustly reconstruct missing or unreliable\nobservations. We evaluate the proposed approach in comparison with traditional\nfunctional trajectory interpolation using a case of sea vessel trajectory data\nprovided by The Automatic Identification System (AIS). By effectively\nleveraging the similarities in real-world trajectories, our pattern ensembling\nmethod helps to reconstruct missing trajectory segments of extended length and\ncomplex geometry. It can be used for locating mobile objects when temporary\nunobserved as well as for creating an evenly sampled trajectory interpolation\nuseful for further trajectory mining.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 01:44:00 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Pathak", "Shivam", ""], ["He", "Mingyi", ""], ["Malinchik", "Sergey", ""], ["Sobolevsky", "Stanislav", ""]]}, {"id": "2101.09875", "submitter": "Xiuyuan Cheng", "authors": "Xiuyuan Cheng, Nan Wu", "title": "Eigen-convergence of Gaussian kernelized graph Laplacian by manifold\n  heat interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This work studies the spectral convergence of graph Laplacian to the\nLaplace-Beltrami operator when the graph affinity matrix is constructed from\n$N$ random samples on a $d$-dimensional manifold embedded in a possibly high\ndimensional space. By analyzing Dirichlet form convergence and constructing\ncandidate approximate eigenfunctions via convolution with manifold heat kernel,\nwe prove that, with Gaussian kernel, one can set the kernel bandwidth parameter\n$\\epsilon \\sim (\\log N/ N)^{1/(d/2+2)}$ such that the eigenvalue convergence\nrate is $N^{-1/(d/2+2)}$ and the eigenvector convergence in 2-norm has rate\n$N^{-1/(d+4)}$; When $\\epsilon \\sim N^{-1/(d/2+3)}$, both eigenvalue and\neigenvector rates are $N^{-1/(d/2+3)}$. These rates are up to a $\\log N$ factor\nand proved for finitely many low-lying eigenvalues. The result holds for\nun-normalized and random-walk graph Laplacians when data are uniformly sampled\non the manifold, as well as the density-corrected graph Laplacian (where the\naffinity matrix is normalized by the degree matrix from both sides) with\nnon-uniformly sampled data. As an intermediate result, we prove new point-wise\nand Dirichlet form convergence rates for the density-corrected graph Laplacian.\nNumerical results are provided to verify the theory.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 03:22:18 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Cheng", "Xiuyuan", ""], ["Wu", "Nan", ""]]}, {"id": "2101.09957", "submitter": "Johannes Lederer", "authors": "Johannes Lederer", "title": "Activation Functions in Artificial Neural Networks: A Systematic\n  Overview", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activation functions shape the outputs of artificial neurons and, therefore,\nare integral parts of neural networks in general and deep learning in\nparticular. Some activation functions, such as logistic and relu, have been\nused for many decades. But with deep learning becoming a mainstream research\ntopic, new activation functions have mushroomed, leading to confusion in both\ntheory and practice. This paper provides an analytic yet up-to-date overview of\npopular activation functions and their properties, which makes it a timely\nresource for anyone who studies or applies neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 08:55:26 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Lederer", "Johannes", ""]]}, {"id": "2101.09973", "submitter": "Manuj Mukherjee", "authors": "Manuj Mukherjee and Aslan Tchamkerten and Mansoor Yousefi", "title": "Approximating Probability Distributions by ReLU Networks", "comments": "Longer version of a paper accepted for presentation at the ITW 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How many neurons are needed to approximate a target probability distribution\nusing a neural network with a given input distribution and approximation error?\nThis paper examines this question for the case when the input distribution is\nuniform, and the target distribution belongs to the class of histogram\ndistributions. We obtain a new upper bound on the number of required neurons,\nwhich is strictly better than previously existing upper bounds. The key\ningredient in this improvement is an efficient construction of the neural nets\nrepresenting piecewise linear functions. We also obtain a lower bound on the\nminimum number of neurons needed to approximate the histogram distributions.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 09:31:20 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Mukherjee", "Manuj", ""], ["Tchamkerten", "Aslan", ""], ["Yousefi", "Mansoor", ""]]}, {"id": "2101.10037", "submitter": "Kevin Styp-Rekowski", "authors": "Kevin Styp-Rekowski, Florian Schmidt, Odej Kao", "title": "Optimizing Convergence for Iterative Learning of ARIMA for Stationary\n  Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Forecasting of time series in continuous systems becomes an increasingly\nrelevant task due to recent developments in IoT and 5G. The popular forecasting\nmodel ARIMA is applied to a large variety of applications for decades. An\nonline variant of ARIMA applies the Online Newton Step in order to learn the\nunderlying process of the time series. This optimization method has pitfalls\nconcerning the computational complexity and convergence. Thus, this work\nfocuses on the computational less expensive Online Gradient Descent\noptimization method, which became popular for learning of neural networks in\nrecent years. For the iterative training of such models, we propose a new\napproach combining different Online Gradient Descent learners (such as Adam,\nAMSGrad, Adagrad, Nesterov) to achieve fast convergence. The evaluation on\nsynthetic data and experimental datasets show that the proposed approach\noutperforms the existing methods resulting in an overall lower prediction\nerror.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 12:07:46 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Styp-Rekowski", "Kevin", ""], ["Schmidt", "Florian", ""], ["Kao", "Odej", ""]]}, {"id": "2101.10050", "submitter": "George Dasoulas", "authors": "George Dasoulas, Johannes Lutzeyer, Michalis Vazirgiannis", "title": "Learning Parametrised Graph Shift Operators", "comments": "17 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many domains data is currently represented as graphs and therefore, the\ngraph representation of this data becomes increasingly important in machine\nlearning. Network data is, implicitly or explicitly, always represented using a\ngraph shift operator (GSO) with the most common choices being the adjacency,\nLaplacian matrices and their normalisations. In this paper, a novel\nparametrised GSO (PGSO) is proposed, where specific parameter values result in\nthe most commonly used GSOs and message-passing operators in graph neural\nnetwork (GNN) frameworks. The PGSO is suggested as a replacement of the\nstandard GSOs that are used in state-of-the-art GNN architectures and the\noptimisation of the PGSO parameters is seamlessly included in the model\ntraining. It is proved that the PGSO has real eigenvalues and a set of real\neigenvectors independent of the parameter values and spectral bounds on the\nPGSO are derived. PGSO parameters are shown to adapt to the sparsity of the\ngraph structure in a study on stochastic blockmodel networks, where they are\nfound to automatically replicate the GSO regularisation found in the\nliterature. On several real-world datasets the accuracy of state-of-the-art GNN\narchitectures is improved by the inclusion of the PGSO in both node- and\ngraph-classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 13:01:26 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 12:14:19 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Dasoulas", "George", ""], ["Lutzeyer", "Johannes", ""], ["Vazirgiannis", "Michalis", ""]]}, {"id": "2101.10058", "submitter": "Yikun Zhang", "authors": "Yikun Zhang, Yen-Chi Chen", "title": "The EM Perspective of Directional Mean Shift Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The directional mean shift (DMS) algorithm is a nonparametric method for\npursuing local modes of densities defined by kernel density estimators on the\nunit hypersphere. In this paper, we show that any DMS iteration can be viewed\nas a generalized Expectation-Maximization (EM) algorithm; in particular, when\nthe von Mises kernel is applied, it becomes an exact EM algorithm. Under the\n(generalized) EM framework, we provide a new proof for the ascending property\nof density estimates and demonstrate the global convergence of directional mean\nshift sequences. Finally, we give a new insight into the linear convergence of\nthe DMS algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 13:17:12 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Zhang", "Yikun", ""], ["Chen", "Yen-Chi", ""]]}, {"id": "2101.10102", "submitter": "Renjue Li", "authors": "Renjue Li and Pengfei Yang and Cheng-Chao Huang and Bai Xue and Lijun\n  Zhang", "title": "Probabilistic Robustness Analysis for DNNs based on PAC Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a black box based approach for analysing deep neural\nnetworks (DNNs). We view a DNN as a function $\\boldsymbol{f}$ from inputs to\noutputs, and consider the local robustness property for a given input. Based on\nscenario optimization technique in robust control design, we learn the score\ndifference function $f_i-f_\\ell$ with respect to the target label $\\ell$ and\nattacking label $i$. We use a linear template over the input pixels, and learn\nthe corresponding coefficients of the score difference function, based on a\nreduction to a linear programming (LP) problems. To make it scalable, we\npropose optimizations including components based learning and focused learning.\nThe learned function offers a probably approximately correct (PAC) guarantee\nfor the robustness property. Since the score difference function is an\napproximation of the local behaviour of the DNN, it can be used to generate\npotential adversarial examples, and the original network can be used to check\nwhether they are spurious or not. Finally, we focus on the input pixels with\nlarge absolute coefficients, and use them to explain the attacking scenario. We\nhave implemented our approach in a prototypical tool DeepPAC. Our experimental\nresults show that our framework can handle very large neural networks like\nResNet152 with $6.5$M neurons, and often generates adversarial examples which\nare very close to the decision boundary.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 14:10:52 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Li", "Renjue", ""], ["Yang", "Pengfei", ""], ["Huang", "Cheng-Chao", ""], ["Xue", "Bai", ""], ["Zhang", "Lijun", ""]]}, {"id": "2101.10123", "submitter": "Janis Klaise", "authors": "Arnaud Van Looveren, Janis Klaise, Giovanni Vacanti, Oliver Cobb", "title": "Conditional Generative Models for Counterfactual Explanations", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counterfactual instances offer human-interpretable insight into the local\nbehaviour of machine learning models. We propose a general framework to\ngenerate sparse, in-distribution counterfactual model explanations which match\na desired target prediction with a conditional generative model, allowing\nbatches of counterfactual instances to be generated with a single forward pass.\nThe method is flexible with respect to the type of generative model used as\nwell as the task of the underlying predictive model. This allows\nstraightforward application of the framework to different modalities such as\nimages, time series or tabular data as well as generative model paradigms such\nas GANs or autoencoders and predictive tasks like classification or regression.\nWe illustrate the effectiveness of our method on image (CelebA), time series\n(ECG) and mixed-type tabular (Adult Census) data.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 14:31:13 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Van Looveren", "Arnaud", ""], ["Klaise", "Janis", ""], ["Vacanti", "Giovanni", ""], ["Cobb", "Oliver", ""]]}, {"id": "2101.10160", "submitter": "Shujian Yu", "authors": "Shujian Yu, Francesco Alesiani, Xi Yu, Robert Jenssen, Jose C.\n  Principe", "title": "Measuring Dependence with Matrix-based Entropy Functional", "comments": "Accepted at AAAI-21. An interpretable and differentiable dependence\n  (or independence) measure that can be used to 1) train deep network under\n  covariate shift and non-Gaussian noise; 2) implement a deep deterministic\n  information bottleneck; and 3) understand the dynamics of learning of CNN.\n  Code available at https://bit.ly/AAAI-dependence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring the dependence of data plays a central role in statistics and\nmachine learning. In this work, we summarize and generalize the main idea of\nexisting information-theoretic dependence measures into a higher-level\nperspective by the Shearer's inequality. Based on our generalization, we then\npropose two measures, namely the matrix-based normalized total correlation\n($T_\\alpha^*$) and the matrix-based normalized dual total correlation\n($D_\\alpha^*$), to quantify the dependence of multiple variables in arbitrary\ndimensional space, without explicit estimation of the underlying data\ndistributions. We show that our measures are differentiable and statistically\nmore powerful than prevalent ones. We also show the impact of our measures in\nfour different machine learning problems, namely the gene regulatory network\ninference, the robust machine learning under covariate shift and non-Gaussian\nnoises, the subspace outlier detection, and the understanding of the learning\ndynamics of convolutional neural networks (CNNs), to demonstrate their\nutilities, advantages, as well as implications to those problems. Code of our\ndependence measure is available at: https://bit.ly/AAAI-dependence\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 15:18:16 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Yu", "Shujian", ""], ["Alesiani", "Francesco", ""], ["Yu", "Xi", ""], ["Jenssen", "Robert", ""], ["Principe", "Jose C.", ""]]}, {"id": "2101.10189", "submitter": "Wolfgang Karl H\\\"ardle", "authors": "Kainat Khowaja, Mykhaylo Shcherbatyy, Wolfgang Karl H\\\"ardle", "title": "Surrogate Models for Optimization of Dynamical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Driven by increased complexity of dynamical systems, the solution of system\nof differential equations through numerical simulation in optimization problems\nhas become computationally expensive. This paper provides a smart data driven\nmechanism to construct low dimensional surrogate models. These surrogate models\nreduce the computational time for solution of the complex optimization problems\nby using training instances derived from the evaluations of the true objective\nfunctions. The surrogate models are constructed using combination of proper\northogonal decomposition and radial basis functions and provides system\nresponses by simple matrix multiplication. Using relative maximum absolute\nerror as the measure of accuracy of approximation, it is shown surrogate models\nwith latin hypercube sampling and spline radial basis functions dominate\nvariable order methods in computational time of optimization, while preserving\nthe accuracy. These surrogate models also show robustness in presence of model\nnon-linearities. Therefore, these computational efficient predictive surrogate\nmodels are applicable in various fields, specifically to solve inverse problems\nand optimal control problems, some examples of which are demonstrated in this\npaper.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 14:09:30 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Khowaja", "Kainat", ""], ["Shcherbatyy", "Mykhaylo", ""], ["H\u00e4rdle", "Wolfgang Karl", ""]]}, {"id": "2101.10229", "submitter": "Masato Kimura Dr.", "authors": "Yuto Aizawa and Masato Kimura", "title": "Universal Approximation Properties for ODENet and ResNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NA math.CA math.NA stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We prove a universal approximation property (UAP) for a class of ODENet and a\nclass of ResNet, which are used in many deep learning algorithms. The UAP can\nbe stated as follows. Let $n$ and $m$ be the dimension of input and output\ndata, and assume $m\\leq n$. Then we show that ODENet width $n+m$ with any\nnon-polynomial continuous activation function can approximate any continuous\nfunction on a compact subset on $\\mathbb{R}^n$. We also show that ResNet has\nthe same property as the depth tends to infinity. Furthermore, we derive\nexplicitly the gradient of a loss function with respect to a certain tuning\nvariable. We use this to construct a learning algorithm for ODENet. To\ndemonstrate the usefulness of this algorithm, we apply it to a regression\nproblem, a binary classification, and a multinomial classification in MNIST.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 06:04:09 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 23:21:38 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Aizawa", "Yuto", ""], ["Kimura", "Masato", ""]]}, {"id": "2101.10369", "submitter": "Tze-Yang Tung", "authors": "Tze-Yang Tung, Szymon Kobus, Joan Roig Pujol, Deniz Gunduz", "title": "Effective Communications: A Joint Learning and Communication Framework\n  for Multi-Agent Reinforcement Learning over Noisy Channels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI cs.IT cs.LG math.IT stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel formulation of the \"effectiveness problem\" in\ncommunications, put forth by Shannon and Weaver in their seminal work [2], by\nconsidering multiple agents communicating over a noisy channel in order to\nachieve better coordination and cooperation in a multi-agent reinforcement\nlearning (MARL) framework. Specifically, we consider a multi-agent partially\nobservable Markov decision process (MA-POMDP), in which the agents, in addition\nto interacting with the environment can also communicate with each other over a\nnoisy communication channel. The noisy communication channel is considered\nexplicitly as part of the dynamics of the environment and the message each\nagent sends is part of the action that the agent can take. As a result, the\nagents learn not only to collaborate with each other but also to communicate\n\"effectively\" over a noisy channel. This framework generalizes both the\ntraditional communication problem, where the main goal is to convey a message\nreliably over a noisy channel, and the \"learning to communicate\" framework that\nhas received recent attention in the MARL literature, where the underlying\ncommunication channels are assumed to be error-free. We show via examples that\nthe joint policy learned using the proposed framework is superior to that where\nthe communication is considered separately from the underlying MA-POMDP. This\nis a very powerful framework, which has many real world applications, from\nautonomous vehicle planning to drone swarm control, and opens up the rich\ntoolbox of deep reinforcement learning for the design of multi-user\ncommunication systems.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 10:43:41 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 17:30:45 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Tung", "Tze-Yang", ""], ["Kobus", "Szymon", ""], ["Pujol", "Joan Roig", ""], ["Gunduz", "Deniz", ""]]}, {"id": "2101.10420", "submitter": "Shibo Zhou", "authors": "Shibo Zhou, Yu Pan", "title": "Spectrum Attention Mechanism for Time Series Classification", "comments": "6 pages, 9 figures, China Control Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Time series classification(TSC) has always been an important and challenging\nresearch task. With the wide application of deep learning, more and more\nresearchers use deep learning models to solve TSC problems. Since time series\nalways contains a lot of noise, which has a negative impact on network\ntraining, people usually filter the original data before training the network.\nThe existing schemes are to treat the filtering and training as two stages, and\nthe design of the filter requires expert experience, which increases the design\ndifficulty of the algorithm and is not universal. We note that the essence of\nfiltering is to filter out the insignificant frequency components and highlight\nthe important ones, which is similar to the attention mechanism. In this paper,\nwe propose an attention mechanism that acts on spectrum (SAM). The network can\nassign appropriate weights to each frequency component to achieve adaptive\nfiltering. We use L1 regularization to further enhance the frequency screening\ncapability of SAM. We also propose a segmented-SAM (SSAM) to avoid the loss of\ntime domain information caused by using the spectrum of the whole sequence. In\nwhich, a tumbling window is introduced to segment the original data. Then SAM\nis applied to each segment to generate new features. We propose a heuristic\nstrategy to search for the appropriate number of segments. Experimental results\nshow that SSAM can produce better feature representations, make the network\nconverge faster, and improve the robustness and classification accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 21:14:05 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Zhou", "Shibo", ""], ["Pan", "Yu", ""]]}, {"id": "2101.10427", "submitter": "Nihal Acharya Adde", "authors": "Thilo Moshagen, Nihal Acharya Adde, Ajay Navilarekal Rajgopal", "title": "Finding hidden-feature depending laws inside a data set and classifying\n  it using Neural Network", "comments": "arXiv admin note: substantial text overlap with arXiv:2011.07332", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The logcosh loss function for neural networks has been developed to combine\nthe advantage of the absolute error loss function of not overweighting outliers\nwith the advantage of the mean square error of continuous derivative near the\nmean, which makes the last phase of learning easier. It is clear, and one\nexperiences it soon, that in the case of clustered data, an artificial neural\nnetwork with logcosh loss learns the bigger cluster rather than the mean of the\ntwo. Even more so, the ANN, when used for regression of a set-valued function,\nwill learn a value close to one of the choices, in other words, one branch of\nthe set-valued function, while a mean-square-error NN will learn the value in\nbetween. This work suggests a method that uses artificial neural networks with\nlogcosh loss to find the branches of set-valued mappings in parameter-outcome\nsample sets and classifies the samples according to those branches.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 21:37:37 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Moshagen", "Thilo", ""], ["Adde", "Nihal Acharya", ""], ["Rajgopal", "Ajay Navilarekal", ""]]}, {"id": "2101.10542", "submitter": "Jonathan Libgober", "authors": "In-Koo Cho and Jonathan Libgober", "title": "Iterative Weak Learnability and Multi-Class AdaBoost", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct an efficient recursive ensemble algorithm for the multi-class\nclassification problem, inspired by SAMME (Zhu, Zou, Rosset, and Hastie\n(2009)). We strengthen the weak learnability condition in Zhu, Zou, Rosset, and\nHastie (2009) by requiring that the weak learnability condition holds for any\nsubset of labels with at least two elements. This condition is simpler to check\nthan many proposed alternatives (e.g., Mukherjee and Schapire (2013)). As\nSAMME, our algorithm is reduced to the Adaptive Boosting algorithm (Schapire\nand Freund (2012)) if the number of labels is two, and can be motivated as a\nfunctional version of the steepest descending method to find an optimal\nsolution. In contrast to SAMME, our algorithm's final hypothesis converges to\nthe correct label with probability 1. For any number of labels, the probability\nof misclassification vanishes exponentially as the training period increases.\nThe sum of the training error and an additional term, that depends only on the\nsample size, bounds the generalization error of our algorithm as the Adaptive\nBoosting algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 03:30:30 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Cho", "In-Koo", ""], ["Libgober", "Jonathan", ""]]}, {"id": "2101.10588", "submitter": "Andrea Montanari", "authors": "Song Mei, Theodor Misiakiewicz, Andrea Montanari", "title": "Generalization error of random features and kernel methods:\n  hypercontractivity and kernel matrix concentration", "comments": "77 pages; 1 pdf figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the classical supervised learning problem: we are given data\n$(y_i,{\\boldsymbol x}_i)$, $i\\le n$, with $y_i$ a response and ${\\boldsymbol\nx}_i\\in {\\mathcal X}$ a covariates vector, and try to learn a model\n$f:{\\mathcal X}\\to{\\mathbb R}$ to predict future responses. Random features\nmethods map the covariates vector ${\\boldsymbol x}_i$ to a point ${\\boldsymbol\n\\phi}({\\boldsymbol x}_i)$ in a higher dimensional space ${\\mathbb R}^N$, via a\nrandom featurization map ${\\boldsymbol \\phi}$. We study the use of random\nfeatures methods in conjunction with ridge regression in the feature space\n${\\mathbb R}^N$. This can be viewed as a finite-dimensional approximation of\nkernel ridge regression (KRR), or as a stylized model for neural networks in\nthe so called lazy training regime.\n  We define a class of problems satisfying certain spectral conditions on the\nunderlying kernels, and a hypercontractivity assumption on the associated\neigenfunctions. These conditions are verified by classical high-dimensional\nexamples. Under these conditions, we prove a sharp characterization of the\nerror of random features ridge regression. In particular, we address two\nfundamental questions: $(1)$~What is the generalization error of KRR? $(2)$~How\nbig $N$ should be for the random features approximation to achieve the same\nerror as KRR?\n  In this setting, we prove that KRR is well approximated by a projection onto\nthe top $\\ell$ eigenfunctions of the kernel, where $\\ell$ depends on the sample\nsize $n$. We show that the test error of random features ridge regression is\ndominated by its approximation error and is larger than the error of KRR as\nlong as $N\\le n^{1-\\delta}$ for some $\\delta>0$. We characterize this gap. For\n$N\\ge n^{1+\\delta}$, random features achieve the same error as the\ncorresponding KRR, and further increasing $N$ does not lead to a significant\nchange in test error.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 06:46:41 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Mei", "Song", ""], ["Misiakiewicz", "Theodor", ""], ["Montanari", "Andrea", ""]]}, {"id": "2101.10617", "submitter": "Lingbin Bian", "authors": "Lingbin Bian, Tiangang Cui, B.T. Thomas Yeo, Alex Fornito, Adeel Razi\n  and Jonathan Keith", "title": "Identification of brain states, transitions, and communities using\n  functional MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Brain function relies on a precisely coordinated and dynamic balance between\nthe functional integration and segregation of distinct neural systems.\nCharacterizing the way in which neural systems reconfigure their interactions\nto give rise to distinct but hidden brain states remains an open challenge. In\nthis paper, we propose a Bayesian model-based characterization of latent brain\nstates and showcase a novel method based on posterior predictive discrepancy\nusing the latent block model to detect transitions between latent brain states\nin blood oxygen level-dependent (BOLD) time series. The set of estimated\nparameters in the model includes a latent label vector that assigns network\nnodes to communities, and also block model parameters that reflect the weighted\nconnectivity within and between communities. Besides extensive in-silico model\nevaluation, we also provide empirical validation (and replication) using the\nHuman Connectome Project (HCP) dataset of 100 healthy adults. Our results\nobtained through an analysis of task-fMRI data during working memory\nperformance show appropriate lags between external task demands and\nchange-points between brain states, with distinctive community patterns\ndistinguishing fixation, low-demand and high-demand task conditions.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 08:10:00 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Bian", "Lingbin", ""], ["Cui", "Tiangang", ""], ["Yeo", "B. T. Thomas", ""], ["Fornito", "Alex", ""], ["Razi", "Adeel", ""], ["Keith", "Jonathan", ""]]}, {"id": "2101.10640", "submitter": "Paul Platzer", "authors": "Paul Platzer, Pascal Yiou (ESTIMR), Philippe Naveau (ESTIMR),\n  Jean-Fran\\c{c}ois Filipot, Maxime Thiebaut, Pierre Tandeo (IMT Atlantique -\n  SC)", "title": "Probability distributions for analog-to-target distances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some properties of chaotic dynamical systems can be probed through features\nof recurrences, also called analogs. In practice, analogs are nearest\nneighbours of the state of a system, taken from a large database called the\ncatalog. Analogs have been used in many atmospheric applications including\nforecasts, downscaling, predictability estimation, and attribution of extreme\nevents. The distances of the analogs to the target state condition the\nperformances of analog applications. These distances can be viewed as random\nvariables, and their probability distributions can be related to the catalog\nsize and properties of the system at stake. A few studies have focused on the\nfirst moments of return time statistics for the best analog, fixing an\nobjective of maximum distance from this analog to the target state. However,\nfor practical use and to reduce estimation variance, applications usually\nrequire not just one, but many analogs. In this paper, we evaluate from a\ntheoretical standpoint and with numerical experiments the probability\ndistributions of the $K$-best analog-to-target distances. We show that\ndimensionality plays a role on the size of the catalog needed to find good\nanalogs, and also on the relative means and variances of the $K$-best analogs.\nOur results are based on recently developed tools from dynamical systems\ntheory. These findings are illustrated with numerical simulations of a\nwell-known chaotic dynamical system and on 10m-wind reanalysis data in\nnorth-west France. A practical application of our derivations for the purpose\nof objective-based dimension reduction is shown using the same reanalysis data.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 09:10:12 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Platzer", "Paul", "", "ESTIMR"], ["Yiou", "Pascal", "", "ESTIMR"], ["Naveau", "Philippe", "", "ESTIMR"], ["Filipot", "Jean-Fran\u00e7ois", "", "IMT Atlantique -\n  SC"], ["Thiebaut", "Maxime", "", "IMT Atlantique -\n  SC"], ["Tandeo", "Pierre", "", "IMT Atlantique -\n  SC"]]}, {"id": "2101.10643", "submitter": "Jie Zhu", "authors": "Jie Zhu, Blanca Gallego", "title": "Casual Inference using Deep Bayesian Dynamic Survival Model (CDS)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Causal inference in longitudinal observational health data often requires the\naccurate estimation of treatment effects on time-to-event outcomes in the\npresence of time-varying covariates. To tackle this sequential treatment effect\nestimation problem, we have developed a causal dynamic survival (CDS) model\nthat uses the potential outcomes framework with the recurrent sub-networks with\nrandom seed ensembles to estimate the difference in survival curves of its\nconfidence interval. Using simulated survival datasets, the CDS model has shown\ngood causal effect estimation performance across scenarios of sample dimension,\nevent rate, confounding and overlapping. However, increasing the sample size is\nnot effective to alleviate the adverse impact from high level of confounding.\nIn two large clinical cohort studies, our model identified the expected\nconditional average treatment effect and detected individual effect\nheterogeneity over time and patient subgroups. CDS provides individualised\nabsolute treatment effect estimations to improve clinical decisions.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 09:15:49 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 13:41:07 GMT"}, {"version": "v3", "created": "Wed, 24 Feb 2021 23:06:53 GMT"}, {"version": "v4", "created": "Sun, 28 Feb 2021 12:55:21 GMT"}, {"version": "v5", "created": "Tue, 2 Mar 2021 12:03:38 GMT"}, {"version": "v6", "created": "Wed, 3 Mar 2021 08:23:41 GMT"}, {"version": "v7", "created": "Tue, 13 Jul 2021 09:03:46 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Zhu", "Jie", ""], ["Gallego", "Blanca", ""]]}, {"id": "2101.10719", "submitter": "Pedro Cadahia Delgado", "authors": "Pedro Cadah\\'ia and Jose Manuel Bravo Caro", "title": "Short-term prediction of Time Series based on bounding techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper it is reconsidered the prediction problem in time series\nframework by using a new non-parametric approach. Through this reconsideration,\nthe prediction is obtained by a weighted sum of past observed data. These\nweights are obtained by solving a constrained linear optimization problem that\nminimizes an outer bound of the prediction error. The innovation is to consider\nboth deterministic and stochastic assumptions in order to obtain the upper\nbound of the prediction error, a tuning parameter is used to balance these\ndeterministic-stochastic assumptions in order to improve the predictor\nperformance. A benchmark is included to illustrate that the proposed predictor\ncan obtain suitable results in a prediction scheme, and can be an interesting\nalternative method to the classical non-parametric methods. Besides, it is\nshown how this model can outperform the preexisting ones in a short term\nforecast.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 11:27:36 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Cadah\u00eda", "Pedro", ""], ["Caro", "Jose Manuel Bravo", ""]]}, {"id": "2101.10739", "submitter": "Jie Zhu", "authors": "Jie Zhu, Blanca Gallego", "title": "Dynamic prediction of time to event with survival curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the ever-growing complexity of primary health care system, proactive\npatient failure management is an effective way to enhancing the availability of\nhealth care resource. One key enabler is the dynamic prediction of\ntime-to-event outcomes. Conventional explanatory statistical approach lacks the\ncapability of making precise individual level prediction, while the data\nadaptive binary predictors does not provide nominal survival curves for\nbiologically plausible survival analysis. The purpose of this article is to\nelucidate that the knowledge of explanatory survival analysis can significantly\nenhance the current black-box data adaptive prediction models. We apply our\nrecently developed counterfactual dynamic survival model (CDSM) to static and\nlongitudinal observational data and testify that the inflection point of its\nestimated individual survival curves provides reliable prediction of the\npatient failure time.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 12:17:27 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 04:32:31 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Zhu", "Jie", ""], ["Gallego", "Blanca", ""]]}, {"id": "2101.10790", "submitter": "Simon Meyer Lauritsen", "authors": "Simon Meyer Lauritsen, Bo Thiesson, Marianne Johansson J{\\o}rgensen,\n  Anders Hammerich Riis, Ulrick Skipper Espelund, Jesper Bo Weile and Jeppe\n  Lange", "title": "The Consequences of the Framing of Machine Learning Risk Prediction\n  Models: Evaluation of Sepsis in General Wards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Objectives: To evaluate the consequences of the framing of machine learning\nrisk prediction models. We evaluate how framing affects model performance and\nmodel learning in four different approaches previously applied in published\nartificial-intelligence (AI) models.\n  Setting and participants: We analysed structured secondary healthcare data\nfrom 221,283 citizens from four Danish municipalities who were 18 years of age\nor older.\n  Results: The four models had similar population level performance (a mean\narea under the receiver operating characteristic curve of 0.73 to 0.82), in\ncontrast to the mean average precision, which varied greatly from 0.007 to\n0.385. Correspondingly, the percentage of missing values also varied between\nframing approaches. The on-clinical-demand framing, which involved samples for\neach time the clinicians made an early warning score assessment, showed the\nlowest percentage of missing values among the vital sign parameters, and this\nmodel was also able to learn more temporal dependencies than the others. The\nShapley additive explanations demonstrated opposing interpretations of SpO2 in\nthe prediction of sepsis as a consequence of differentially framed models.\n  Conclusions: The profound consequences of framing mandate attention from\nclinicians and AI developers, as the understanding and reporting of framing are\npivotal to the successful development and clinical implementation of future AI\ntechnology. Model framing must reflect the expected clinical environment. The\nimportance of proper problem framing is by no means exclusive to sepsis\nprediction and applies to most clinical risk prediction models.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 14:00:05 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Lauritsen", "Simon Meyer", ""], ["Thiesson", "Bo", ""], ["J\u00f8rgensen", "Marianne Johansson", ""], ["Riis", "Anders Hammerich", ""], ["Espelund", "Ulrick Skipper", ""], ["Weile", "Jesper Bo", ""], ["Lange", "Jeppe", ""]]}, {"id": "2101.10832", "submitter": "Yulin Wang", "authors": "Yulin Wang, Zanlin Ni, Shiji Song, Le Yang, Gao Huang", "title": "Revisiting Locally Supervised Learning: an Alternative to End-to-end\n  Training", "comments": "Accepted by ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the need to store the intermediate activations for back-propagation,\nend-to-end (E2E) training of deep networks usually suffers from high GPUs\nmemory footprint. This paper aims to address this problem by revisiting the\nlocally supervised learning, where a network is split into gradient-isolated\nmodules and trained with local supervision. We experimentally show that simply\ntraining local modules with E2E loss tends to collapse task-relevant\ninformation at early layers, and hence hurts the performance of the full model.\nTo avoid this issue, we propose an information propagation (InfoPro) loss,\nwhich encourages local modules to preserve as much useful information as\npossible, while progressively discard task-irrelevant information. As InfoPro\nloss is difficult to compute in its original form, we derive a feasible upper\nbound as a surrogate optimization objective, yielding a simple but effective\nalgorithm. In fact, we show that the proposed method boils down to minimizing\nthe combination of a reconstruction loss and a normal cross-entropy/contrastive\nterm. Extensive empirical results on five datasets (i.e., CIFAR, SVHN, STL-10,\nImageNet and Cityscapes) validate that InfoPro is capable of achieving\ncompetitive performance with less than 40% memory footprint compared to E2E\ntraining, while allowing using training data with higher-resolution or larger\nbatch sizes under the same GPU memory constraint. Our method also enables\ntraining local modules asynchronously for potential training acceleration. Code\nis available at: https://github.com/blackfeather-wang/InfoPro-Pytorch.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 15:02:18 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Wang", "Yulin", ""], ["Ni", "Zanlin", ""], ["Song", "Shiji", ""], ["Yang", "Le", ""], ["Huang", "Gao", ""]]}, {"id": "2101.10876", "submitter": "Rasika Karkare", "authors": "Rasika Karkare, Randy Paffenroth and Gunjan Mahindre", "title": "Blind Image Denoising and Inpainting Using Robust Hadamard Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we demonstrate how deep autoencoders can be generalized to the\ncase of inpainting and denoising, even when no clean training data is\navailable. In particular, we show how neural networks can be trained to perform\nall of these tasks simultaneously. While, deep autoencoders implemented by way\nof neural networks have demonstrated potential for denoising and anomaly\ndetection, standard autoencoders have the drawback that they require access to\nclean data for training. However, recent work in Robust Deep Autoencoders\n(RDAEs) shows how autoencoders can be trained to eliminate outliers and noise\nin a dataset without access to any clean training data. Inspired by this work,\nwe extend RDAEs to the case where data are not only noisy and have outliers,\nbut also only partially observed. Moreover, the dataset we train the neural\nnetwork on has the properties that all entries have noise, some entries are\ncorrupted by large mistakes, and many entries are not even known. Given such an\nalgorithm, many standard tasks, such as denoising, image inpainting, and\nunobserved entry imputation can all be accomplished simultaneously within the\nsame framework. Herein we demonstrate these techniques on standard machine\nlearning tasks, such as image inpainting and denoising for the MNIST and\nCIFAR10 datasets. However, these approaches are not only applicable to image\nprocessing problems, but also have wide ranging impacts on datasets arising\nfrom real-world problems, such as manufacturing and network processing, where\nnoisy, partially observed data naturally arise.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 15:33:22 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Karkare", "Rasika", ""], ["Paffenroth", "Randy", ""], ["Mahindre", "Gunjan", ""]]}, {"id": "2101.10880", "submitter": "Richard Samworth", "authors": "Thomas B. Berrett and Richard J. Samworth", "title": "USP: an independence test that improves on Pearson's chi-squared and the\n  $G$-test", "comments": "27 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the $U$-Statistic Permutation (USP) test of independence in the\ncontext of discrete data displayed in a contingency table. Either Pearson's\nchi-squared test of independence, or the $G$-test, are typically used for this\ntask, but we argue that these tests have serious deficiencies, both in terms of\ntheir inability to control the size of the test, and their power properties. By\ncontrast, the USP test is guaranteed to control the size of the test at the\nnominal level for all sample sizes, has no issues with small (or zero) cell\ncounts, and is able to detect distributions that violate independence in only a\nminimal way. The test statistic is derived from a $U$-statistic estimator of a\nnatural population measure of dependence, and we prove that this is the unique\nminimum variance unbiased estimator of this population quantity. The practical\nutility of the USP test is demonstrated on both simulated data, where its power\ncan be dramatically greater than those of Pearson's test and the $G$-test, and\non real data. The USP test is implemented in the R package USP.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 15:42:44 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Berrett", "Thomas B.", ""], ["Samworth", "Richard J.", ""]]}, {"id": "2101.10943", "submitter": "Alicia Curth", "authors": "Alicia Curth and Mihaela van der Schaar", "title": "Nonparametric Estimation of Heterogeneous Treatment Effects: From Theory\n  to Learning Algorithms", "comments": "To appear in the Proceedings of the 24th International Conference on\n  Artificial Intelligence and Statistics (AISTATS) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need to evaluate treatment effectiveness is ubiquitous in most of\nempirical science, and interest in flexibly investigating effect heterogeneity\nis growing rapidly. To do so, a multitude of model-agnostic, nonparametric\nmeta-learners have been proposed in recent years. Such learners decompose the\ntreatment effect estimation problem into separate sub-problems, each solvable\nusing standard supervised learning methods. Choosing between different\nmeta-learners in a data-driven manner is difficult, as it requires access to\ncounterfactual information. Therefore, with the ultimate goal of building\nbetter understanding of the conditions under which some learners can be\nexpected to perform better than others a priori, we theoretically analyze four\nbroad meta-learning strategies which rely on plug-in estimation and\npseudo-outcome regression. We highlight how this theoretical reasoning can be\nused to guide principled algorithm design and translate our analyses into\npractice by considering a variety of neural network architectures as\nbase-learners for the discussed meta-learning strategies. In a simulation\nstudy, we showcase the relative strengths of the learners under different\ndata-generating processes.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 17:11:40 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 13:36:14 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Curth", "Alicia", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "2101.10950", "submitter": "Ali Amiryousefi", "authors": "Ali Amiryousefi", "title": "Asymptotic Supervised Predictive Classifiers under Partition\n  Exchangeability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The convergence of simultaneous and marginal predictive classifiers under\npartition exchangeability in supervised classification is obtained. The result\nshows the asymptotic convergence of these classifiers under infinite amount of\ntraining or test data, such that after observing umpteen amount of data, the\ndifferences between these classifiers would be negligible. This is an important\nresult from the practical perspective as under the presence of sufficiently\nlarge amount of data, one can replace the simpler marginal classifier with\ncomputationally more expensive simultaneous one.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 17:17:40 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Amiryousefi", "Ali", ""]]}, {"id": "2101.10967", "submitter": "Kushal Chakrabarti", "authors": "Kushal Chakrabarti, Nirupam Gupta and Nikhil Chopra", "title": "Robustness of Iteratively Pre-Conditioned Gradient-Descent Method: The\n  Case of Distributed Linear Regression Problem", "comments": "in IEEE Control Systems Letters. Related articles: arXiv:2003.07180v2\n  [math.OC], arXiv:2008.02856v1 [math.OC], and arXiv:2011.07595v2 [math.OC]", "journal-ref": null, "doi": "10.1109/LCSYS.2020.3045533", "report-no": null, "categories": "math.OC cs.LG cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of multi-agent distributed linear regression\nin the presence of system noises. In this problem, the system comprises\nmultiple agents wherein each agent locally observes a set of data points, and\nthe agents' goal is to compute a linear model that best fits the collective\ndata points observed by all the agents. We consider a server-based distributed\narchitecture where the agents interact with a common server to solve the\nproblem; however, the server cannot access the agents' data points. We consider\na practical scenario wherein the system either has observation noise, i.e., the\ndata points observed by the agents are corrupted, or has process noise, i.e.,\nthe computations performed by the server and the agents are corrupted. In\nnoise-free systems, the recently proposed distributed linear regression\nalgorithm, named the Iteratively Pre-conditioned Gradient-descent (IPG) method,\nhas been claimed to converge faster than related methods. In this paper, we\nstudy the robustness of the IPG method, against both the observation noise and\nthe process noise. We empirically show that the robustness of the IPG method\ncompares favorably to the state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 17:51:49 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Chakrabarti", "Kushal", ""], ["Gupta", "Nirupam", ""], ["Chopra", "Nikhil", ""]]}, {"id": "2101.10998", "submitter": "Cong Shen", "authors": "Hyun-Suk Lee, Cong Shen, William Zame, Jang-Won Lee, Mihaela van der\n  Schaar", "title": "SDF-Bayes: Cautious Optimism in Safe Dose-Finding Clinical Trials with\n  Drug Combinations and Heterogeneous Patient Groups", "comments": "Accepted to AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phase I clinical trials are designed to test the safety (non-toxicity) of\ndrugs and find the maximum tolerated dose (MTD). This task becomes\nsignificantly more challenging when multiple-drug dose-combinations (DC) are\ninvolved, due to the inherent conflict between the exponentially increasing DC\ncandidates and the limited patient budget. This paper proposes a novel Bayesian\ndesign, SDF-Bayes, for finding the MTD for drug combinations in the presence of\nsafety constraints. Rather than the conventional principle of escalating or\nde-escalating the current dose of one drug (perhaps alternating between drugs),\nSDF-Bayes proceeds by cautious optimism: it chooses the next DC that, on the\nbasis of current information, is most likely to be the MTD (optimism), subject\nto the constraint that it only chooses DCs that have a high probability of\nbeing safe (caution). We also propose an extension, SDF-Bayes-AR, that accounts\nfor patient heterogeneity and enables heterogeneous patient recruitment.\nExtensive experiments based on both synthetic and real-world datasets\ndemonstrate the advantages of SDF-Bayes over state of the art DC trial designs\nin terms of accuracy and safety.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 18:59:26 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Lee", "Hyun-Suk", ""], ["Shen", "Cong", ""], ["Zame", "William", ""], ["Lee", "Jang-Won", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "2101.11003", "submitter": "Steven Golovkine", "authors": "Steven Golovkine", "title": "FDApy: a Python package for functional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.LG stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the Python package, FDApy, as an implementation of functional\ndata. This package provide modules for the analysis of such data. It includes\nclasses for different dimensional data as well as irregularly sampled\nfunctional data. A simulation toolbox is also provided. It might be used to\nsimulate different clusters of functional data. Some methodologies to handle\nthese data are implemented, such as dimension reduction and clustering. New\nmethods can be easily added. The package is publicly available on the Python\nPackage Index and Github.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 10:07:33 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Golovkine", "Steven", ""]]}, {"id": "2101.11020", "submitter": "Maria Schuld", "authors": "Maria Schuld", "title": "Supervised quantum machine learning models are kernel methods", "comments": "26 pages, 9 figures - Version 2 emphasises focus on supervised\n  learning, adds more references to existing literature, deletes section on\n  state discrimination due to a technical error, and updates the comparison\n  between kernel-based and variational training", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With near-term quantum devices available and the race for fault-tolerant\nquantum computers in full swing, researchers became interested in the question\nof what happens if we replace a supervised machine learning model with a\nquantum circuit. While such \"quantum models\" are sometimes called \"quantum\nneural networks\", it has been repeatedly noted that their mathematical\nstructure is actually much more closely related to kernel methods: they analyse\ndata in high-dimensional Hilbert spaces to which we only have access through\ninner products revealed by measurements. This technical manuscript summarises\nand extends the idea of systematically rephrasing supervised quantum models as\na kernel method. With this, a lot of near-term and fault-tolerant quantum\nmodels can be replaced by a general support vector machine whose kernel\ncomputes distances between data-encoding quantum states. Kernel-based training\nis then guaranteed to find better or equally good quantum models than\nvariational circuit training. Overall, the kernel perspective of quantum\nmachine learning tells us that the way that data is encoded into quantum states\nis the main ingredient that can potentially set quantum models apart from\nclassical machine learning models.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 19:00:04 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 07:29:07 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Schuld", "Maria", ""]]}, {"id": "2101.11037", "submitter": "Oliver Urs Lenz", "authors": "Oliver Urs Lenz, Daniel Peralta, Chris Cornelis", "title": "Average Localised Proximity: A new data descriptor with good default\n  one-class classification performance", "comments": "Accepted manuscript", "journal-ref": "Pattern Recognition 118 (2021) 107991", "doi": "10.1016/j.patcog.2021.107991", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  One-class classification is a challenging subfield of machine learning in\nwhich so-called data descriptors are used to predict membership of a class\nbased solely on positive examples of that class, and no counter-examples. A\nnumber of data descriptors that have been shown to perform well in previous\nstudies of one-class classification, like the Support Vector Machine (SVM),\nrequire setting one or more hyperparameters. There has been no systematic\nattempt to date to determine optimal default values for these hyperparameters,\nwhich limits their ease of use, especially in comparison with\nhyperparameter-free proposals like the Isolation Forest (IF). We address this\nissue by determining optimal default hyperparameter values across a collection\nof 246 one-class classification problems derived from 50 different real-world\ndatasets. In addition, we propose a new data descriptor, Average Localised\nProximity (ALP) to address certain issues with existing approaches based on\nnearest neighbour distances. Finally, we evaluate classification performance\nusing a leave-one-dataset-out procedure, and find strong evidence that ALP\noutperforms IF and a number of other data descriptors, as well as weak evidence\nthat it outperforms SVM, making ALP a good default choice.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 19:14:14 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 12:17:09 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Lenz", "Oliver Urs", ""], ["Peralta", "Daniel", ""], ["Cornelis", "Chris", ""]]}, {"id": "2101.11041", "submitter": "Jelena Diakonikolas", "authors": "Jelena Diakonikolas and Crist\\'obal Guzm\\'an", "title": "Complementary Composite Minimization, Small Gradients in General Norms,\n  and Applications to Regression Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Composite minimization is a powerful framework in large-scale convex\noptimization, based on decoupling of the objective function into terms with\nstructurally different properties and allowing for more flexible algorithmic\ndesign. In this work, we introduce a new algorithmic framework for\ncomplementary composite minimization, where the objective function decouples\ninto a (weakly) smooth and a uniformly convex term. This particular form of\ndecoupling is pervasive in statistics and machine learning, due to its link to\nregularization.\n  The main contributions of our work are summarized as follows. First, we\nintroduce the problem of complementary composite minimization in general normed\nspaces; second, we provide a unified accelerated algorithmic framework to\naddress broad classes of complementary composite minimization problems; and\nthird, we prove that the algorithms resulting from our framework are\nnear-optimal in most of the standard optimization settings. Additionally, we\nshow that our algorithmic framework can be used to address the problem of\nmaking the gradients small in general normed spaces. As a concrete example, we\nobtain a nearly-optimal method for the standard $\\ell_1$ setup (small gradients\nin the $\\ell_\\infty$ norm), essentially matching the bound of Nesterov (2012)\nthat was previously known only for the Euclidean setup. Finally, we show that\nour composite methods are broadly applicable to a number of regression\nproblems, leading to complexity bounds that are either new or match the best\nexisting ones.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 19:21:28 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Diakonikolas", "Jelena", ""], ["Guzm\u00e1n", "Crist\u00f3bal", ""]]}, {"id": "2101.11046", "submitter": "Matthias Bauer", "authors": "Matthias Bauer and Andriy Mnih", "title": "Generalized Doubly Reparameterized Gradient Estimators", "comments": null, "journal-ref": "38th International Conference on Machine Learning (ICML 2021)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient low-variance gradient estimation enabled by the reparameterization\ntrick (RT) has been essential to the success of variational autoencoders.\nDoubly-reparameterized gradients (DReGs) improve on the RT for multi-sample\nvariational bounds by applying reparameterization a second time for an\nadditional reduction in variance. Here, we develop two generalizations of the\nDReGs estimator and show that they can be used to train conditional and\nhierarchical VAEs on image modelling tasks more effectively. First, we extend\nthe estimator to hierarchical models with several stochastic layers by showing\nhow to treat additional score function terms due to the hierarchical\nvariational posterior. We then generalize DReGs to score functions of arbitrary\ndistributions instead of just those of the sampling distribution, which makes\nthe estimator applicable to the parameters of the prior in addition to those of\nthe posterior.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 19:30:00 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 15:52:44 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Bauer", "Matthias", ""], ["Mnih", "Andriy", ""]]}, {"id": "2101.11055", "submitter": "Dhruv Kohli", "authors": "Dhruv Kohli, Alexander Cloninger, Gal Mishne", "title": "LDLE: Low Distortion Local Eigenmaps", "comments": "37 pages, 23 figures, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.SP cs.LG math.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Low Distortion Local Eigenmaps (LDLE), a manifold learning\ntechnique which constructs a set of low distortion local views of a dataset in\nlower dimension and registers them to obtain a global embedding. The local\nviews are constructed using the global eigenvectors of the graph Laplacian and\nare registered using Procrustes analysis. The choice of these eigenvectors may\nvary across the regions. In contrast to existing techniques, LDLE is more\ngeometric and can embed manifolds without boundary as well as non-orientable\nmanifolds into their intrinsic dimension.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 19:55:05 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Kohli", "Dhruv", ""], ["Cloninger", "Alexander", ""], ["Mishne", "Gal", ""]]}, {"id": "2101.11071", "submitter": "William Guss", "authors": "William H. Guss, Mario Ynocente Castro, Sam Devlin, Brandon Houghton,\n  Noboru Sean Kuno, Crissman Loomis, Stephanie Milani, Sharada Mohanty, Keisuke\n  Nakata, Ruslan Salakhutdinov, John Schulman, Shinya Shiroshita, Nicholay\n  Topin, Avinash Ummadisingu, Oriol Vinyals", "title": "The MineRL 2020 Competition on Sample Efficient Reinforcement Learning\n  using Human Priors", "comments": "37 pages, initial submission, accepted at NeurIPS. arXiv admin note:\n  substantial text overlap with arXiv:1904.10079", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although deep reinforcement learning has led to breakthroughs in many\ndifficult domains, these successes have required an ever-increasing number of\nsamples, affording only a shrinking segment of the AI community access to their\ndevelopment. Resolution of these limitations requires new, sample-efficient\nmethods. To facilitate research in this direction, we propose this second\niteration of the MineRL Competition. The primary goal of the competition is to\nfoster the development of algorithms which can efficiently leverage human\ndemonstrations to drastically reduce the number of samples needed to solve\ncomplex, hierarchical, and sparse environments. To that end, participants\ncompete under a limited environment sample-complexity budget to develop systems\nwhich solve the MineRL ObtainDiamond task in Minecraft, a sequential decision\nmaking environment requiring long-term planning, hierarchical control, and\nefficient exploration methods. The competition is structured into two rounds in\nwhich competitors are provided several paired versions of the dataset and\nenvironment with different game textures and shaders. At the end of each round,\ncompetitors submit containerized versions of their learning algorithms to the\nAIcrowd platform where they are trained from scratch on a hold-out\ndataset-environment pair for a total of 4-days on a pre-specified hardware\nplatform. In this follow-up iteration to the NeurIPS 2019 MineRL Competition,\nwe implement new features to expand the scale and reach of the competition. In\nresponse to the feedback of the previous participants, we introduce a second\nminor track focusing on solutions without access to environment interactions of\nany kind except during test-time. Further we aim to prompt domain agnostic\nsubmissions by implementing several novel competition mechanics including\naction-space randomization and desemantization of observations and actions.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 20:32:30 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Guss", "William H.", ""], ["Castro", "Mario Ynocente", ""], ["Devlin", "Sam", ""], ["Houghton", "Brandon", ""], ["Kuno", "Noboru Sean", ""], ["Loomis", "Crissman", ""], ["Milani", "Stephanie", ""], ["Mohanty", "Sharada", ""], ["Nakata", "Keisuke", ""], ["Salakhutdinov", "Ruslan", ""], ["Schulman", "John", ""], ["Shiroshita", "Shinya", ""], ["Topin", "Nicholay", ""], ["Ummadisingu", "Avinash", ""], ["Vinyals", "Oriol", ""]]}, {"id": "2101.11083", "submitter": "Naoki Awaya", "authors": "Naoki Awaya and Li Ma", "title": "Tree boosting for learning probability measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning probability measures based on an i.i.d. sample is a fundamental\ninference task, but is challenging when the sample space is high-dimensional.\nInspired by the success of tree boosting in high-dimensional classification and\nregression, we propose a tree boosting method for learning high-dimensional\nprobability distributions. We formulate concepts of \"addition\" and \"residuals\"\non probability distributions in terms of compositions of a new, more general\nnotion of multivariate cumulative distribution functions (CDFs) than classical\nCDFs. This then gives rise to a simple boosting algorithm based on\nforward-stagewise (FS) fitting of an additive ensemble of measures, which\nsequentially minimizes the entropy loss. The output of the FS algorithm allows\nanalytic computation of the probability density function for the fitted\ndistribution. It also provides an exact simulator for drawing independent Monte\nCarlo samples from the fitted measure. Typical considerations in applying\nboosting--namely choosing the number of trees, setting the appropriate level of\nshrinkage/regularization in the weak learner, and the evaluation of variable\nimportance--can all be accomplished in an analogous fashion to traditional\nboosting in supervised learning. Numerical experiments confirm that boosting\ncan substantially improve the fit to multivariate distributions compared to the\nstate-of-the-art single-tree learner and is computationally efficient. We\nillustrate through an application to a data set from mass cytometry how the\nsimulator can be used to investigate various aspects of the underlying\ndistribution.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 21:03:27 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 19:41:48 GMT"}, {"version": "v3", "created": "Thu, 29 Apr 2021 23:28:20 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Awaya", "Naoki", ""], ["Ma", "Li", ""]]}, {"id": "2101.11095", "submitter": "Pablo Jos\\'e Del Moral Pastor", "authors": "Pablo del Moral, Slawomir Nowaczyk, Anita Sant'Anna, Sepideh Pashami", "title": "Pitfalls of Assessing Extracted Hierarchies for Multi-Class\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Using hierarchies of classes is one of the standard methods to solve\nmulti-class classification problems. In the literature, selecting the right\nhierarchy is considered to play a key role in improving classification\nperformance. Although different methods have been proposed, there is still a\nlack of understanding of what makes one method to extract hierarchies perform\nbetter or worse. To this effect, we analyze and compare some of the most\npopular approaches to extracting hierarchies. We identify some common pitfalls\nthat may lead practitioners to make misleading conclusions about their methods.\nIn addition, to address some of these problems, we demonstrate that using\nrandom hierarchies is an appropriate benchmark to assess how the hierarchy's\nquality affects the classification performance. In particular, we show how the\nhierarchy's quality can become irrelevant depending on the experimental setup:\nwhen using powerful enough classifiers, the final performance is not affected\nby the quality of the hierarchy. We also show how comparing the effect of the\nhierarchies against non-hierarchical approaches might incorrectly indicate\ntheir superiority. Our results confirm that datasets with a high number of\nclasses generally present complex structures in how these classes relate to\neach other. In these datasets, the right hierarchy can dramatically improve\nclassification performance.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 21:50:57 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["del Moral", "Pablo", ""], ["Nowaczyk", "Slawomir", ""], ["Sant'Anna", "Anita", ""], ["Pashami", "Sepideh", ""]]}, {"id": "2101.11105", "submitter": "Paul Novello", "authors": "Paul Novello (CEA, Inria, X), Ga\\\"el Po\\\"ette (CEA), David Lugato\n  (CEA), Pietro Congedo (Inria, X)", "title": "A Taylor Based Sampling Scheme for Machine Learning in Computational\n  Physics", "comments": "Second Workshop on Machine Learning and the Physical Sciences\n  (NeurIPS 2019), Vancouver, Canada. arXiv admin note: substantial text overlap\n  with arXiv:2101.07561", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph math-ph math.MP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) is increasingly used to construct surrogate models for\nphysical simulations. We take advantage of the ability to generate data using\nnumerical simulations programs to train ML models better and achieve accuracy\ngain with no performance cost. We elaborate a new data sampling scheme based on\nTaylor approximation to reduce the error of a Deep Neural Network (DNN) when\nlearning the solution of an ordinary differential equations (ODE) system.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 12:56:09 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 12:48:18 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Novello", "Paul", "", "CEA, Inria, X"], ["Po\u00ebtte", "Ga\u00ebl", "", "CEA"], ["Lugato", "David", "", "CEA"], ["Congedo", "Pietro", "", "Inria, X"]]}, {"id": "2101.11156", "submitter": "Lan Truong", "authors": "Lan V. Truong", "title": "Fundamental limits and algorithms for sparse linear regression with\n  sublinear sparsity", "comments": "45 pages, 2 figures. Under review for publication. Add some auxiliary\n  proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We establish exact asymptotic expressions for the normalized mutual\ninformation and minimum mean-square-error (MMSE) of sparse linear regression in\nthe sub-linear sparsity regime. Our result is achieved by a generalization of\nthe adaptive interpolation method in Bayesian inference for linear regimes to\nsub-linear ones. A modification of the well-known approximate message passing\nalgorithm to approach the MMSE fundamental limit is also proposed, and its\nstate evolution is rigorously analyzed. Our results show that the traditional\nlinear assumption between the signal dimension and number of observations in\nthe replica and adaptive interpolation methods is not necessary for sparse\nsignals. They also show how to modify the existing well-known AMP algorithms\nfor linear regimes to sub-linear ones.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 01:27:03 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 03:53:21 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 20:03:55 GMT"}, {"version": "v4", "created": "Wed, 14 Jul 2021 15:22:05 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Truong", "Lan V.", ""]]}, {"id": "2101.11162", "submitter": "Samuel Otto", "authors": "Samuel E. Otto and Clarence W. Rowley", "title": "Inadequacy of Linear Methods for Minimal Sensor Placement and Feature\n  Selection in Nonlinear Systems; a New Approach Using Secants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.SY eess.SY stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Sensor placement and feature selection are critical steps in engineering,\nmodeling, and data science that share a common mathematical theme: the selected\nmeasurements should enable solution of an inverse problem. Most real-world\nsystems of interest are nonlinear, yet the majority of available techniques for\nfeature selection and sensor placement rely on assumptions of linearity or\nsimple statistical models. We show that when these assumptions are violated,\nstandard techniques can lead to costly over-sensing without guaranteeing that\nthe desired information can be recovered from the measurements. In order to\nremedy these problems, we introduce a novel data-driven approach for sensor\nplacement and feature selection for a general type of nonlinear inverse problem\nbased on the information contained in secant vectors between data points. Using\nthe secant-based approach, we develop three efficient greedy algorithms that\neach provide different types of robust, near-minimal reconstruction guarantees.\nWe demonstrate them on two problems where linear techniques consistently fail:\nsensor placement to reconstruct a fluid flow formed by a complicated\nshock-mixing layer interaction and selecting fundamental manifold learning\ncoordinates on a torus.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 01:57:34 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Otto", "Samuel E.", ""], ["Rowley", "Clarence W.", ""]]}, {"id": "2101.11179", "submitter": "Minghe Zhang", "authors": "Minghe Zhang, Chen Xu, Andy Sun, Feng Qiu, Yao Xie", "title": "Solar Radiation Anomaly Events Modeling Using Spatial-Temporal Mutually\n  Interactive Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling and predicting solar events, in particular, the solar ramping event\nis critical for improving situational awareness for solar power generation\nsystems. Solar ramping events are significantly impacted by weather conditions\nsuch as temperature, humidity, and cloud density. Discovering the correlation\nbetween different locations and times is a highly challenging task since the\nsystem is complex and noisy. We propose a novel method to model and predict\nramping events from spatial-temporal sequential solar radiation data based on a\nspatio-temporal interactive Bernoulli process. We demonstrate the good\nperformance of our approach on real solar radiation datasets.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 03:02:39 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Zhang", "Minghe", ""], ["Xu", "Chen", ""], ["Sun", "Andy", ""], ["Qiu", "Feng", ""], ["Xie", "Yao", ""]]}, {"id": "2101.11201", "submitter": "Cuong Nguyen", "authors": "Cuong Nguyen, Thanh-Toan Do, Gustavo Carneiro", "title": "Similarity of Classification Tasks", "comments": "Accepted at Neurips Meta-learning Workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in meta-learning has led to remarkable performances on\nseveral few-shot learning benchmarks. However, such success often ignores the\nsimilarity between training and testing tasks, resulting in a potential bias\nevaluation. We, therefore, propose a generative approach based on a variant of\nLatent Dirichlet Allocation to analyse task similarity to optimise and better\nunderstand the performance of meta-learning. We demonstrate that the proposed\nmethod can provide an insightful evaluation for meta-learning algorithms on two\nfew-shot classification benchmarks that matches common intuition: the more\nsimilar the higher performance. Based on this similarity measure, we propose a\ntask-selection strategy for meta-learning and show that it can produce more\naccurate classification results than methods that randomly select training\ntasks.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 04:37:34 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Nguyen", "Cuong", ""], ["Do", "Thanh-Toan", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "2101.11256", "submitter": "Mamikon Gulian", "authors": "Kookjin Lee, Nathaniel A. Trask, Ravi G. Patel, Mamikon A. Gulian,\n  Eric C. Cyr", "title": "Partition of unity networks: deep hp-approximation", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximation theorists have established best-in-class optimal approximation\nrates of deep neural networks by utilizing their ability to simultaneously\nemulate partitions of unity and monomials. Motivated by this, we propose\npartition of unity networks (POUnets) which incorporate these elements directly\ninto the architecture. Classification architectures of the type used to learn\nprobability measures are used to build a meshfree partition of space, while\npolynomial spaces with learnable coefficients are associated to each partition.\nThe resulting hp-element-like approximation allows use of a fast least-squares\noptimizer, and the resulting architecture size need not scale exponentially\nwith spatial dimension, breaking the curse of dimensionality. An abstract\napproximation result establishes desirable properties to guide network design.\nNumerical results for two choices of architecture demonstrate that POUnets\nyield hp-convergence for smooth functions and consistently outperform MLPs for\npiecewise polynomial functions with large numbers of discontinuities.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 08:26:11 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Lee", "Kookjin", ""], ["Trask", "Nathaniel A.", ""], ["Patel", "Ravi G.", ""], ["Gulian", "Mamikon A.", ""], ["Cyr", "Eric C.", ""]]}, {"id": "2101.11286", "submitter": "Zhou Lu", "authors": "Zhou Lu", "title": "A Note on the Representation Power of GHHs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this note we prove a sharp lower bound on the necessary number of nestings\nof nested absolute-value functions of generalized hinging hyperplanes (GHH) to\nrepresent arbitrary CPWL functions. Previous upper bound states that $n+1$\nnestings is sufficient for GHH to achieve universal representation power, but\nthe corresponding lower bound was unknown. We prove that $n$ nestings is\nnecessary for universal representation power, which provides an almost tight\nlower bound. We also show that one-hidden-layer neural networks don't have\nuniversal approximation power over the whole domain. The analysis is based on a\nkey lemma showing that any finite sum of periodic functions is either\nnon-integrable or the zero function, which might be of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 09:46:37 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Lu", "Zhou", ""]]}, {"id": "2101.11347", "submitter": "Jinxiong Zhang", "authors": "Jinxiong Zhang", "title": "Decision Machines: Interpreting Decision Tree as a Model Combination\n  Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Based on decision trees, it is efficient to handle tabular data. Conventional\ndecision tree growth methods often result in suboptimal trees because of their\ngreedy nature. Their inherent structure limits the options of hardware to\nimplement decision trees in parallel. Here is a compact representation of\nbinary decision trees to overcome these deficiencies. We explicitly formulate\nthe dependence of prediction on binary tests for binary decision trees and\nconstruct a function to guide the input sample from the root to the appropriate\nleaf node. And based on this formulation we introduce a new interpretation of\nbinary decision trees. Then we approximate this formulation via continuous\nfunctions. Finally, we interpret the decision tree as a model combination\nmethod. And we propose the selection-prediction scheme to unify a few learning\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 12:23:24 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Zhang", "Jinxiong", ""]]}, {"id": "2101.11410", "submitter": "Yuka Hashimoto", "authors": "Yuka Hashimoto, Isao Ishikawa, Masahiro Ikeda, Fuyuta Komura, Takeshi\n  Katsura, and Yoshinobu Kawahara", "title": "Reproducing kernel Hilbert C*-module and kernel mean embeddings", "comments": "merged two unpablished papers arXiv:2003.00738 and 2007.14698 into\n  this paper. arXiv admin note: text overlap with arXiv:2007.14698", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Kernel methods have been among the most popular techniques in machine\nlearning, where learning tasks are solved using the property of reproducing\nkernel Hilbert space (RKHS). In this paper, we propose a novel data analysis\nframework with reproducing kernel Hilbert $C^*$-module (RKHM) and kernel mean\nembedding (KME) in RKHM. Since RKHM contains richer information than RKHS or\nvector-valued RKHS (vv RKHS), analysis with RKHM enables us to capture and\nextract structural properties in multivariate data, functional data and other\nstructured data. We show a branch of theories for RKHM to apply to data\nanalysis, including the representer theorem, and the injectivity and\nuniversality of the proposed KME. We also show RKHM generalizes RKHS and vv\nRKHS. Then, we provide concrete procedures for employing RKHM and the proposed\nKME to data analysis.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 14:02:18 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Hashimoto", "Yuka", ""], ["Ishikawa", "Isao", ""], ["Ikeda", "Masahiro", ""], ["Komura", "Fuyuta", ""], ["Katsura", "Takeshi", ""], ["Kawahara", "Yoshinobu", ""]]}, {"id": "2101.11453", "submitter": "Jan Metzen", "authors": "Jan Hendrik Metzen, Nicole Finnie, Robin Hutmacher", "title": "Meta Adversarial Training against Universal Patches", "comments": "Accepted by the ICML 2021 workshop on \"A Blessing in Disguise: The\n  Prospects and Perils of Adversarial Machine Learning\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently demonstrated physical-world adversarial attacks have exposed\nvulnerabilities in perception systems that pose severe risks for\nsafety-critical applications such as autonomous driving. These attacks place\nadversarial artifacts in the physical world that indirectly cause the addition\nof a universal patch to inputs of a model that can fool it in a variety of\ncontexts. Adversarial training is the most effective defense against\nimage-dependent adversarial attacks. However, tailoring adversarial training to\nuniversal patches is computationally expensive since the optimal universal\npatch depends on the model weights which change during training. We propose\nmeta adversarial training (MAT), a novel combination of adversarial training\nwith meta-learning, which overcomes this challenge by meta-learning universal\npatches along with model training. MAT requires little extra computation while\ncontinuously adapting a large set of patches to the current model. MAT\nconsiderably increases robustness against universal patch attacks on image\nclassification and traffic-light detection.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 14:36:23 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 14:07:54 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Metzen", "Jan Hendrik", ""], ["Finnie", "Nicole", ""], ["Hutmacher", "Robin", ""]]}, {"id": "2101.11482", "submitter": "Zhenhua Zhang", "authors": "Zhenhua Zhang", "title": "Deriving the Traveler Behavior Information from Social Media: A Case\n  Study in Manhattan with Twitter", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.NA math.NA stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social media platforms, such as Twitter, provide a totally new perspective in\ndealing with the traffic problems and is anticipated to complement the\ntraditional methods. The geo-tagged tweets can provide the Twitter users'\nlocation information and is being applied in traveler behavior analysis. This\npaper explores the full potentials of Twitter in deriving travel behavior\ninformation and conducts a case study in Manhattan Area. A systematic method is\nproposed to extract displacement information from Twitter locations. Our study\nshows that Twitter has a unique demographics which combine not only local\nresidents but also the tourists or passengers. For individual user, Twitter can\nuncover his/her travel behavior features including the time-of-day and location\ndistributions on both weekdays and weekends. For all Twitter users, the\naggregated travel behavior results also show that the time-of-day travel\npatterns in Manhattan Island resemble that of the traffic flow; the\nidentification of OD pattern is also promising by comparing with the results of\ntravel survey.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 15:19:50 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Zhang", "Zhenhua", ""]]}, {"id": "2101.11520", "submitter": "Yuki Takezawa", "authors": "Yuki Takezawa, Ryoma Sato, Makoto Yamada", "title": "Supervised Tree-Wasserstein Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To measure the similarity of documents, the Wasserstein distance is a\npowerful tool, but it requires a high computational cost. Recently, for fast\ncomputation of the Wasserstein distance, methods for approximating the\nWasserstein distance using a tree metric have been proposed. These tree-based\nmethods allow fast comparisons of a large number of documents; however, they\nare unsupervised and do not learn task-specific distances. In this work, we\npropose the Supervised Tree-Wasserstein (STW) distance, a fast, supervised\nmetric learning method based on the tree metric. Specifically, we rewrite the\nWasserstein distance on the tree metric by the parent-child relationships of a\ntree and formulate it as a continuous optimization problem using a contrastive\nloss. Experimentally, we show that the STW distance can be computed fast, and\nimproves the accuracy of document classification tasks. Furthermore, the STW\ndistance is formulated by matrix multiplications, runs on a GPU, and is\nsuitable for batch processing. Therefore, we show that the STW distance is\nextremely efficient when comparing a large number of documents.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 16:24:51 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 07:55:07 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Takezawa", "Yuki", ""], ["Sato", "Ryoma", ""], ["Yamada", "Makoto", ""]]}, {"id": "2101.11552", "submitter": "Jun Hu", "authors": "Jun Hu, Shengsheng Qian, Quan Fang, Youze Wang, Quan Zhao, Huaiwen\n  Zhang, Changsheng Xu", "title": "Efficient Graph Deep Learning in TensorFlow with tf_geometric", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce tf_geometric, an efficient and friendly library for graph deep\nlearning, which is compatible with both TensorFlow 1.x and 2.x. tf_geometric\nprovides kernel libraries for building Graph Neural Networks (GNNs) as well as\nimplementations of popular GNNs. The kernel libraries consist of\ninfrastructures for building efficient GNNs, including graph data structures,\ngraph map-reduce framework, graph mini-batch strategy, etc. These\ninfrastructures enable tf_geometric to support single-graph computation,\nmulti-graph computation, graph mini-batch, distributed training, etc.;\ntherefore, tf_geometric can be used for a variety of graph deep learning tasks,\nsuch as transductive node classification, inductive node classification, link\nprediction, and graph classification. Based on the kernel libraries,\ntf_geometric implements a variety of popular GNN models for different tasks. To\nfacilitate the implementation of GNNs, tf_geometric also provides some other\nlibraries for dataset management, graph sampling, etc. Different from existing\npopular GNN libraries, tf_geometric provides not only Object-Oriented\nProgramming (OOP) APIs, but also Functional APIs, which enable tf_geometric to\nhandle advanced graph deep learning tasks such as graph meta-learning. The APIs\nof tf_geometric are friendly, and they are suitable for both beginners and\nexperts. In this paper, we first present an overview of tf_geometric's\nframework. Then, we conduct experiments on some benchmark datasets and report\nthe performance of several popular GNN models implemented by tf_geometric.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 17:16:36 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Hu", "Jun", ""], ["Qian", "Shengsheng", ""], ["Fang", "Quan", ""], ["Wang", "Youze", ""], ["Zhao", "Quan", ""], ["Zhang", "Huaiwen", ""], ["Xu", "Changsheng", ""]]}, {"id": "2101.11665", "submitter": "Sebastian Farquhar", "authors": "Sebastian Farquhar, Yarin Gal, Tom Rainforth", "title": "On Statistical Bias In Active Learning: How and When To Fix It", "comments": "Published at ICLR 2021 (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning is a powerful tool when labelling data is expensive, but it\nintroduces a bias because the training data no longer follows the population\ndistribution. We formalize this bias and investigate the situations in which it\ncan be harmful and sometimes even helpful. We further introduce novel\ncorrective weights to remove bias when doing so is beneficial. Through this,\nour work not only provides a useful mechanism that can improve the active\nlearning approach, but also an explanation of the empirical successes of\nvarious existing approaches which ignore this bias. In particular, we show that\nthis bias can be actively helpful when training overparameterized models --\nlike neural networks -- with relatively little data.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 19:52:24 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 11:46:48 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Farquhar", "Sebastian", ""], ["Gal", "Yarin", ""], ["Rainforth", "Tom", ""]]}, {"id": "2101.11688", "submitter": "Leonard Schulman", "authors": "Spencer L. Gordon, Leonard J. Schulman", "title": "Hadamard Extensions and the Identification of Mixtures of Product\n  Distributions", "comments": "V2: re-titled and slight edits", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Hadamard Extension of a matrix is the matrix consisting of all Hadamard\nproducts of subsets of its rows. This construction arises in the context of\nidentifying a mixture of product distributions on binary random variables: full\ncolumn rank of such extensions is a necessary ingredient of identification\nalgorithms. We provide several results concerning when a Hadamard Extension has\nfull column rank.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 21:07:54 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 20:34:41 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Gordon", "Spencer L.", ""], ["Schulman", "Leonard J.", ""]]}, {"id": "2101.11717", "submitter": "Francois Malgouyres", "authors": "Adrien Gauffriau, Fran\\c{c}ois Malgouyres (IMT), M\\'elanie Ducoffe", "title": "Overestimation learning with guarantees", "comments": null, "journal-ref": "AAAI-21, workshop on safeAI, Feb 2021, Valence (Virtual), Spain", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a complete method that learns a neural network which is\nguaranteed to overestimate a reference function on a given domain. The neural\nnetwork can then be used as a surrogate for the reference function. The method\ninvolves two steps. In the first step, we construct an adaptive set of Majoring\nPoints. In the second step, we optimize a well-chosen neural network to\noverestimate the Majoring Points. In order to extend the guarantee on the\nMajoring Points to the whole domain, we necessarily have to make an assumption\non the reference function. In this study, we assume that the reference function\nis monotonic. We provide experiments on synthetic and real problems. The\nexperiments show that the density of the Majoring Points concentrate where the\nreference function varies. The learned over-estimations are both guaranteed to\noverestimate the reference function and are proven empirically to provide good\napproximations of it. Experiments on real data show that the method makes it\npossible to use the surrogate function in embedded systems for which an\nunderestimation is critical; when computing the reference function requires too\nmany resources.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 09:06:03 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Gauffriau", "Adrien", "", "IMT"], ["Malgouyres", "Fran\u00e7ois", "", "IMT"], ["Ducoffe", "M\u00e9lanie", ""]]}, {"id": "2101.11769", "submitter": "Can Xu", "authors": "Can Xu, Ahmed M. Alaa, Ioana Bica, Brent D. Ershoff, Maxime Cannesson,\n  Mihaela van der Schaar", "title": "Learning Matching Representations for Individualized Organ\n  Transplantation Allocation", "comments": "Accepted to AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Organ transplantation is often the last resort for treating end-stage\nillness, but the probability of a successful transplantation depends greatly on\ncompatibility between donors and recipients. Current medical practice relies on\ncoarse rules for donor-recipient matching, but is short of domain knowledge\nregarding the complex factors underlying organ compatibility. In this paper, we\nformulate the problem of learning data-driven rules for organ matching using\nobservational data for organ allocations and transplant outcomes. This problem\ndeparts from the standard supervised learning setup in that it involves\nmatching the two feature spaces (i.e., donors and recipients), and requires\nestimating transplant outcomes under counterfactual matches not observed in the\ndata. To address these problems, we propose a model based on representation\nlearning to predict donor-recipient compatibility; our model learns\nrepresentations that cluster donor features, and applies donor-invariant\ntransformations to recipient features to predict outcomes for a given\ndonor-recipient feature instance. Experiments on semi-synthetic and real-world\ndatasets show that our model outperforms state-of-art allocation methods and\npolicies executed by human experts.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 01:33:21 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 02:46:43 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Xu", "Can", ""], ["Alaa", "Ahmed M.", ""], ["Bica", "Ioana", ""], ["Ershoff", "Brent D.", ""], ["Cannesson", "Maxime", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "2101.11783", "submitter": "Cheng Mao", "authors": "Cheng Mao, Mark Rudelson, and Konstantin Tikhomirov", "title": "Random Graph Matching with Improved Noise Robustness", "comments": "34 pages. Accepted for presentation at Conference on Learning Theory\n  (COLT) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph matching, also known as network alignment, refers to finding a\nbijection between the vertex sets of two given graphs so as to maximally align\ntheir edges. This fundamental computational problem arises frequently in\nmultiple fields such as computer vision and biology. Recently, there has been a\nplethora of work studying efficient algorithms for graph matching under\nprobabilistic models. In this work, we propose a new algorithm for graph\nmatching: Our algorithm associates each vertex with a signature vector using a\nmultistage procedure and then matches a pair of vertices from the two graphs if\ntheir signature vectors are close to each other. We show that, for two\nErd\\H{o}s--R\\'enyi graphs with edge correlation $1-\\alpha$, our algorithm\nrecovers the underlying matching exactly with high probability when $\\alpha \\le\n1 / (\\log \\log n)^C$, where $n$ is the number of vertices in each graph and $C$\ndenotes a positive universal constant. This improves the condition $\\alpha \\le\n1 / (\\log n)^C$ achieved in previous work.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 02:39:27 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 21:11:26 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Mao", "Cheng", ""], ["Rudelson", "Mark", ""], ["Tikhomirov", "Konstantin", ""]]}, {"id": "2101.11815", "submitter": "Tengyuan Liang", "authors": "Tengyuan Liang, Benjamin Recht", "title": "Interpolating Classifiers Make Few Mistakes", "comments": "23 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides elementary analyses of the regret and generalization of\nminimum-norm interpolating classifiers (MNIC). The MNIC is the function of\nsmallest Reproducing Kernel Hilbert Space norm that perfectly interpolates a\nlabel pattern on a finite data set. We derive a mistake bound for MNIC and a\nregularized variant that holds for all data sets. This bound follows from\nelementary properties of matrix inverses. Under the assumption that the data is\nindependently and identically distributed, the mistake bound implies that MNIC\ngeneralizes at a rate proportional to the norm of the interpolating solution\nand inversely proportional to the number of data points. This rate matches\nsimilar rates derived for margin classifiers and perceptrons. We derive several\nplausible generative models where the norm of the interpolating classifier is\nbounded or grows at a rate sublinear in $n$. We also show that as long as the\npopulation class conditional distributions are sufficiently separable in total\nvariation, then MNIC generalizes with a fast rate.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 04:51:24 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 01:40:30 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Liang", "Tengyuan", ""], ["Recht", "Benjamin", ""]]}, {"id": "2101.11816", "submitter": "Junghyo Jo", "authors": "Sangwon Lee and Vipul Periwal and Junghyo Jo", "title": "Inference of stochastic time series with missing data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring dynamics from time series is an important objective in data\nanalysis. In particular, it is challenging to infer stochastic dynamics given\nincomplete data. We propose an expectation maximization (EM) algorithm that\niterates between alternating two steps: E-step restores missing data points,\nwhile M-step infers an underlying network model of restored data. Using\nsynthetic data generated by a kinetic Ising model, we confirm that the\nalgorithm works for restoring missing data points as well as inferring the\nunderlying model. At the initial iteration of the EM algorithm, the model\ninference shows better model-data consistency with observed data points than\nwith missing data points. As we keep iterating, however, missing data points\nshow better model-data consistency. We find that demanding equal consistency of\nobserved and missing data points provides an effective stopping criterion for\nthe iteration to prevent overshooting the most accurate model inference. Armed\nwith this EM algorithm with this stopping criterion, we infer missing data\npoints and an underlying network from a time-series data of real neuronal\nactivities. Our method recovers collective properties of neuronal activities,\nsuch as time correlations and firing statistics, which have previously never\nbeen optimized to fit.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 04:56:59 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 04:26:26 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Lee", "Sangwon", ""], ["Periwal", "Vipul", ""], ["Jo", "Junghyo", ""]]}, {"id": "2101.11881", "submitter": "Rohitash Chandra", "authors": "Rohitash Chandra, Ayush Jain, Divyanshu Singh Chauhan", "title": "Deep learning via LSTM models for COVID-19 infection forecasting in\n  India", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We have entered an era of a pandemic that has shaken the world with major\nimpact to medical systems, economics and agriculture. Prominent computational\nand mathematical models have been unreliable due to the complexity of the\nspread of infections. Moreover, lack of data collection and reporting makes any\nsuch modelling attempts unreliable. Hence we need to re-look at the situation\nwith the latest data sources and most comprehensive forecasting models. Deep\nlearning models such as recurrent neural networks are well suited for modelling\ntemporal sequences. In this paper, prominent recurrent neural networks, in\nparticular \\textit{long short term memory} (LSTMs) networks, bidirectional\nLSTM, and encoder-decoder LSTM models for multi-step (short-term) forecasting\nthe spread of COVID-infections among selected states in India. We select states\nwith COVID-19 hotpots in terms of the rate of infections and compare with\nstates where infections have been contained or reached their peak and provide\ntwo months ahead forecast that shows that cases will slowly decline. Our\nresults show that long-term forecasts are promising which motivates the\napplication of the method in other countries or areas. We note that although we\nmade some progress in forecasting, the challenges in modelling remain due to\ndata and difficulty in capturing factors such as population density, travel\nlogistics, and social aspects such culture and lifestyle.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 09:19:10 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Chandra", "Rohitash", ""], ["Jain", "Ayush", ""], ["Chauhan", "Divyanshu Singh", ""]]}, {"id": "2101.11885", "submitter": "Tineke Blom", "authors": "Tineke Blom and Joris M. Mooij", "title": "Causality and independence in perfectly adapted dynamical systems", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Perfect adaptation in a dynamical system is the phenomenon that one or more\nvariables have an initial transient response to a persistent change in an\nexternal stimulus but revert to their original value as the system converges to\nequilibrium. The causal ordering algorithm can be used to construct an\nequilibrium causal ordering graph that represents causal relations and a Markov\nordering graph that implies conditional independences from a set of equilibrium\nequations. Based on this, we formulate sufficient graphical conditions to\nidentify perfect adaptation from a set of first-order differential equations.\nFurthermore, we give sufficient conditions to test for the presence of perfect\nadaptation in experimental equilibrium data. We apply our ideas to a simple\nmodel for a protein signalling pathway and test its predictions both in\nsimulations and on real-world protein expression data. We demonstrate that\nperfect adaptation in this model can explain why the presence and orientation\nof edges in the output of causal discovery algorithms does not always appear to\nagree with the direction of edges in biological consensus networks.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 09:28:58 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Blom", "Tineke", ""], ["Mooij", "Joris M.", ""]]}, {"id": "2101.12002", "submitter": "Sebastien Destercke", "authors": "Soundouss Messoudi, S\\'ebastien Destercke, Sylvain Rousseau", "title": "Copula-based conformal prediction for Multi-Target Regression", "comments": "17 pages, 8 figures, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There are relatively few works dealing with conformal prediction for\nmulti-task learning issues, and this is particularly true for multi-target\nregression. This paper focuses on the problem of providing valid (i.e.,\nfrequency calibrated) multi-variate predictions. To do so, we propose to use\ncopula functions applied to deep neural networks for inductive conformal\nprediction. We show that the proposed method ensures efficiency and validity\nfor multi-target regression problems on various data sets.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 14:06:25 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Messoudi", "Soundouss", ""], ["Destercke", "S\u00e9bastien", ""], ["Rousseau", "Sylvain", ""]]}, {"id": "2101.12113", "submitter": "Gil Shamir", "authors": "Gil I. Shamir and Wojciech Szpankowski", "title": "Low Complexity Approximate Bayesian Logistic Regression for Sparse\n  Online Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Theoretical results show that Bayesian methods can achieve lower bounds on\nregret for online logistic regression. In practice, however, such techniques\nmay not be feasible especially for very large feature sets. Various\napproximations that, for huge sparse feature sets, diminish the theoretical\nadvantages, must be used. Often, they apply stochastic gradient methods with\nhyper-parameters that must be tuned on some surrogate loss, defeating\ntheoretical advantages of Bayesian methods. The surrogate loss, defined to\napproximate the mixture, requires techniques as Monte Carlo sampling,\nincreasing computations per example. We propose low complexity analytical\napproximations for sparse online logistic and probit regressions. Unlike\nvariational inference and other methods, our methods use analytical closed\nforms, substantially lowering computations. Unlike dense solutions, as Gaussian\nMixtures, our methods allow for sparse problems with huge feature sets without\nincreasing complexity. With the analytical closed forms, there is also no need\nfor applying stochastic gradient methods on surrogate losses, and for tuning\nand balancing learning and regularization hyper-parameters. Empirical results\ntop the performance of the more computationally involved methods. Like such\nmethods, our methods still reveal per feature and per example uncertainty\nmeasures.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 16:59:31 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Shamir", "Gil I.", ""], ["Szpankowski", "Wojciech", ""]]}, {"id": "2101.12176", "submitter": "Samuel L. Smith", "authors": "Samuel L. Smith, Benoit Dherin, David G. T. Barrett and Soham De", "title": "On the Origin of Implicit Regularization in Stochastic Gradient Descent", "comments": "Accepted as a conference paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For infinitesimal learning rates, stochastic gradient descent (SGD) follows\nthe path of gradient flow on the full batch loss function. However moderately\nlarge learning rates can achieve higher test accuracies, and this\ngeneralization benefit is not explained by convergence bounds, since the\nlearning rate which maximizes test accuracy is often larger than the learning\nrate which minimizes training loss. To interpret this phenomenon we prove that\nfor SGD with random shuffling, the mean SGD iterate also stays close to the\npath of gradient flow if the learning rate is small and finite, but on a\nmodified loss. This modified loss is composed of the original loss function and\nan implicit regularizer, which penalizes the norms of the minibatch gradients.\nUnder mild assumptions, when the batch size is small the scale of the implicit\nregularization term is proportional to the ratio of the learning rate to the\nbatch size. We verify empirically that explicitly including the implicit\nregularizer in the loss can enhance the test accuracy when the learning rate is\nsmall.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 18:32:14 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Smith", "Samuel L.", ""], ["Dherin", "Benoit", ""], ["Barrett", "David G. T.", ""], ["De", "Soham", ""]]}, {"id": "2101.12204", "submitter": "Cong Shen", "authors": "Chengshuai Shi and Cong Shen", "title": "Federated Multi-Armed Bandits", "comments": "AAAI 2021, Camera Ready. Code is available at:\n  https://github.com/ShenGroup/FMAB", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT cs.MA math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated multi-armed bandits (FMAB) is a new bandit paradigm that parallels\nthe federated learning (FL) framework in supervised learning. It is inspired by\npractical applications in cognitive radio and recommender systems, and enjoys\nfeatures that are analogous to FL. This paper proposes a general framework of\nFMAB and then studies two specific federated bandit models. We first study the\napproximate model where the heterogeneous local models are random realizations\nof the global model from an unknown distribution. This model introduces a new\nuncertainty of client sampling, as the global model may not be reliably learned\neven if the finite local models are perfectly known. Furthermore, this\nuncertainty cannot be quantified a priori without knowledge of the\nsuboptimality gap. We solve the approximate model by proposing Federated Double\nUCB (Fed2-UCB), which constructs a novel \"double UCB\" principle accounting for\nuncertainties from both arm and client sampling. We show that gradually\nadmitting new clients is critical in achieving an O(log(T)) regret while\nexplicitly considering the communication cost. The exact model, where the\nglobal bandit model is the exact average of heterogeneous local models, is then\nstudied as a special case. We show that, somewhat surprisingly, the\norder-optimal regret can be achieved independent of the number of clients with\na careful choice of the update periodicity. Experiments using both synthetic\nand real-world datasets corroborate the theoretical analysis and demonstrate\nthe effectiveness and efficiency of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 18:59:19 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 14:36:34 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Shi", "Chengshuai", ""], ["Shen", "Cong", ""]]}, {"id": "2101.12249", "submitter": "Joshua Bassey", "authors": "Joshua Bassey, Lijun Qian, Xianfang Li", "title": "A Survey of Complex-Valued Neural Networks", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artificial neural networks (ANNs) based machine learning models and\nespecially deep learning models have been widely applied in computer vision,\nsignal processing, wireless communications, and many other domains, where\ncomplex numbers occur either naturally or by design. However, most of the\ncurrent implementations of ANNs and machine learning frameworks are using real\nnumbers rather than complex numbers. There are growing interests in building\nANNs using complex numbers, and exploring the potential advantages of the\nso-called complex-valued neural networks (CVNNs) over their real-valued\ncounterparts. In this paper, we discuss the recent development of CVNNs by\nperforming a survey of the works on CVNNs in the literature. Specifically, a\ndetailed review of various CVNNs in terms of activation function, learning and\noptimization, input and output representations, and their applications in tasks\nsuch as signal processing and computer vision are provided, followed by a\ndiscussion on some pertinent challenges and future research directions.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 19:40:50 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Bassey", "Joshua", ""], ["Qian", "Lijun", ""], ["Li", "Xianfang", ""]]}, {"id": "2101.12282", "submitter": "Christoph Breunig", "authors": "Christoph Breunig, Xiaohong Chen", "title": "Adaptive Estimation of Quadratic Functionals in Nonparametric\n  Instrumental Variable Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers adaptive estimation of quadratic functionals in the\nnonparametric instrumental variables (NPIV) models. Minimax estimation of a\nquadratic functional of a NPIV is an important problem in optimal estimation of\na nonlinear functional of an ill-posed inverse regression with an unknown\noperator using one random sample. We first show that a leave-one-out, sieve\nNPIV estimator of the quadratic functional proposed by \\cite{BC2020} attains a\nconvergence rate that coincides with the lower bound previously derived by\n\\cite{ChenChristensen2017}. The minimax rate is achieved by the optimal choice\nof a key tuning parameter (sieve dimension) that depends on unknown NPIV model\nfeatures. We next propose a data driven choice of the tuning parameter based on\nLepski's method. The adaptive estimator attains the minimax optimal rate in the\nseverely ill-posed case and in the regular, mildly ill-posed case, but up to a\nmultiplicative $\\sqrt{\\log n}$ in the irregular, mildly ill-posed case.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 21:14:02 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Breunig", "Christoph", ""], ["Chen", "Xiaohong", ""]]}, {"id": "2101.12288", "submitter": "Elchanan Solomon", "authors": "Elchanan Solomon, Alexander Wagner, Paul Bendich", "title": "From Geometry to Topology: Inverse Theorems for Distributed Persistence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  What is the \"right\" topological invariant of a large point cloud X? Prior\nresearch has focused on estimating the full persistence diagram of X, a\nquantity that is very expensive to compute, unstable to outliers, and far from\na sufficient statistic. We therefore propose that the correct invariant is not\nthe persistence diagram of X, but rather the collection of persistence diagrams\nof many small subsets. This invariant, which we call \"distributed persistence,\"\nis trivially parallelizable, more stable to outliers, and has a rich inverse\ntheory. The map from the space of point clouds (with the quasi-isometry metric)\nto the space of distributed persistence invariants (with the\nHausdorff-Bottleneck distance) is a global quasi-isometry. This is a much\nstronger property than simply being injective, as it implies that the inverse\nof a small neighborhood is a small neighborhood, and is to our knowledge the\nonly result of its kind in the TDA literature. Moreover, the quasi-isometry\nbounds depend on the size of the subsets taken, so that as the size of these\nsubsets goes from small to large, the invariant interpolates between a purely\ngeometric one and a topological one. Lastly, we note that our inverse results\ndo not actually require considering all subsets of a fixed size (an enormous\ncollection), but a relatively small collection satisfying certain covering\nproperties that arise with high probability when randomly sampling subsets.\nThese theoretical results are complemented by two synthetic experiments\ndemonstrating the use of distributed persistence in practice.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 21:36:45 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 22:35:52 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Solomon", "Elchanan", ""], ["Wagner", "Alexander", ""], ["Bendich", "Paul", ""]]}, {"id": "2101.12353", "submitter": "Yunfei Yang", "authors": "Yunfei Yang, Zhen Li, Yang Wang", "title": "On the capacity of deep generative networks for approximating\n  distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the efficacy and efficiency of deep generative networks for\napproximating probability distributions. We prove that neural networks can\ntransform a low-dimensional source distribution to a distribution that is\narbitrarily close to a high-dimensional target distribution, when the closeness\nare measured by Wasserstein distances and maximum mean discrepancy. Upper\nbounds of the approximation error are obtained in terms of the width and depth\nof neural network. Furthermore, it is shown that the approximation error in\nWasserstein distance grows at most linearly on the ambient dimension and that\nthe approximation order only depends on the intrinsic dimension of the target\ndistribution. On the contrary, when $f$-divergences are used as metrics of\ndistributions, the approximation property is different. We show that in order\nto approximate the target distribution in $f$-divergences, the dimension of the\nsource distribution cannot be smaller than the intrinsic dimension of the\ntarget distribution.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 01:45:02 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 07:42:34 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Yang", "Yunfei", ""], ["Li", "Zhen", ""], ["Wang", "Yang", ""]]}, {"id": "2101.12365", "submitter": "Jonathan Siegel", "authors": "Jonathan W. Siegel, Jinchao Xu", "title": "Improved Approximation Properties of Dictionaries and Applications to\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article addresses the problem of approximating a function in a Hilbert\nspace by an expansion over a dictionary $\\mathbb{D}$. We introduce the notion\nof a smoothly parameterized dictionary and give upper bounds on the\napproximation rates, metric entropy and $n$-widths of the absolute convex hull,\nwhich we denote $B_1(\\mathbb{D})$, of such dictionaries. The upper bounds\ndepend upon the order of smoothness of the parameterization, and improve upon\nexisting results in many cases. The main applications of these results is to\nthe dictionaries $\\mathbb{D} = \\{\\sigma(\\omega\\cdot x + b)\\}\\subset L^2$\ncorresponding to shallow neural networks with activation function $\\sigma$, and\nto the dictionary of decaying Fourier modes corresponding to the spectral\nBarron space. This improves upon existing approximation rates for shallow\nneural networks when $\\sigma = \\text{ReLU}^k$ for $k\\geq 2$, sharpens bounds on\nthe metric entropy, and provides the first bounds on the Gelfand $n$-widths of\nthe Barron space and spectral Barron space.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 02:29:48 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 18:14:32 GMT"}, {"version": "v3", "created": "Mon, 5 Apr 2021 16:34:46 GMT"}, {"version": "v4", "created": "Thu, 22 Apr 2021 13:17:28 GMT"}, {"version": "v5", "created": "Thu, 20 May 2021 01:38:30 GMT"}, {"version": "v6", "created": "Mon, 28 Jun 2021 21:54:22 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Siegel", "Jonathan W.", ""], ["Xu", "Jinchao", ""]]}, {"id": "2101.12369", "submitter": "Jiajun Liang", "authors": "Jiajun Liang, Chuyang Ke and Jean Honorio", "title": "Information Theoretic Limits of Exact Recovery in Sub-hypergraph Models\n  for Community Detection", "comments": null, "journal-ref": "IEEE International Symposium on Information Theory (ISIT), 2021", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the information theoretic bounds for exact recovery\nin sub-hypergraph models for community detection. We define a general model\ncalled the $m-$uniform sub-hypergraph stochastic block model ($m-$ShSBM). Under\nthe $m-$ShSBM, we use Fano's inequality to identify the region of model\nparameters where any algorithm fails to exactly recover the planted communities\nwith a large probability. We also identify the region where a Maximum\nLikelihood Estimation (MLE) algorithm succeeds to exactly recover the\ncommunities with high probability. Our bounds are tight and pertain to the\ncommunity detection problems in various models such as the planted hypergraph\nstochastic block model, the planted densest sub-hypergraph model, and the\nplanted multipartite hypergraph model.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 02:50:34 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Liang", "Jiajun", ""], ["Ke", "Chuyang", ""], ["Honorio", "Jean", ""]]}, {"id": "2101.12414", "submitter": "Shane Barratt", "authors": "Shane Barratt, Yining Dong, Stephen Boyd", "title": "Low Rank Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of forecasting multiple values of the future of a\nvector time series, using some past values. This problem, and related ones such\nas one-step-ahead prediction, have a very long history, and there are a number\nof well-known methods for it, including vector auto-regressive models,\nstate-space methods, multi-task regression, and others. Our focus is on low\nrank forecasters, which break forecasting up into two steps: estimating a\nvector that can be interpreted as a latent state, given the past, and then\nestimating the future values of the time series, given the latent state\nestimate. We introduce the concept of forecast consistency, which means that\nthe estimates of the same value made at different times are consistent. We\nformulate the forecasting problem in general form, and focus on linear\nforecasters, for which we propose a formulation that can be solved via convex\noptimization. We describe a number of extensions and variations, including\nnonlinear forecasters, data weighting, the inclusion of auxiliary data, and\nadditional objective terms. We illustrate our methods with several examples.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 05:59:19 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Barratt", "Shane", ""], ["Dong", "Yining", ""], ["Boyd", "Stephen", ""]]}, {"id": "2101.12416", "submitter": "Shane Barratt", "authors": "Shane Barratt and Stephen Boyd", "title": "Covariance Prediction via Convex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of predicting the covariance of a zero mean Gaussian\nvector, based on another feature vector. We describe a covariance predictor\nthat has the form of a generalized linear model, i.e., an affine function of\nthe features followed by an inverse link function that maps vectors to\nsymmetric positive definite matrices. The log-likelihood is a concave function\nof the predictor parameters, so fitting the predictor involves convex\noptimization. Such predictors can be combined with others, or recursively\napplied to improve performance.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 06:06:58 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Barratt", "Shane", ""], ["Boyd", "Stephen", ""]]}, {"id": "2101.12430", "submitter": "Vince Lyzinski", "authors": "Al-Fahad M. Al-Qadhi, Carey E. Priebe, Hayden S. Helm, Vince Lyzinski", "title": "Subgraph nomination: Query by Example Subgraph Retrieval in Networks", "comments": "31 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the subgraph nomination inference task, in which\nexample subgraphs of interest are used to query a network for similarly\ninteresting subgraphs. This type of problem appears time and again in real\nworld problems connected to, for example, user recommendation systems and\nstructural retrieval tasks in social and biological/connectomic networks. We\nformally define the subgraph nomination framework with an emphasis on the\nnotion of a user-in-the-loop in the subgraph nomination pipeline. In this\nsetting, a user can provide additional post-nomination light supervision that\ncan be incorporated into the retrieval task. After introducing and formalizing\nthe retrieval task, we examine the nuanced effect that user-supervision can\nhave on performance, both analytically and across real and simulated data\nexamples.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 06:50:27 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Al-Qadhi", "Al-Fahad M.", ""], ["Priebe", "Carey E.", ""], ["Helm", "Hayden S.", ""], ["Lyzinski", "Vince", ""]]}, {"id": "2101.12503", "submitter": "Ines Wilms", "authors": "Ines Wilms and Jacob Bien", "title": "Tree-based Node Aggregation in Sparse Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High-dimensional graphical models are often estimated using regularization\nthat is aimed at reducing the number of edges in a network. In this work, we\nshow how even simpler networks can be produced by aggregating the nodes of the\ngraphical model. We develop a new convex regularized method, called the\ntree-aggregated graphical lasso or tag-lasso, that estimates graphical models\nthat are both edge-sparse and node-aggregated. The aggregation is performed in\na data-driven fashion by leveraging side information in the form of a tree that\nencodes node similarity and facilitates the interpretation of the resulting\naggregated nodes. We provide an efficient implementation of the tag-lasso by\nusing the locally adaptive alternating direction method of multipliers and\nillustrate our proposal's practical advantages in simulation and in\napplications in finance and biology.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 10:17:31 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Wilms", "Ines", ""], ["Bien", "Jacob", ""]]}, {"id": "2101.12578", "submitter": "Fan-Keng Sun", "authors": "Fan-Keng Sun and Christopher I. Lang and Duane S. Boning", "title": "Adjusting for Autocorrelated Errors in Neural Networks for Time Series\n  Regression and Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many cases, it is difficult to generate highly accurate models for time\nseries data using a known parametric model structure. In response, an\nincreasing body of research focuses on using neural networks to model time\nseries approximately. A common assumption in training neural networks on time\nseries is that the errors at different time steps are uncorrelated. However,\ndue to the temporality of the data, errors are actually autocorrelated in many\ncases, which makes such maximum likelihood estimation inaccurate. In this\npaper, we propose to learn the autocorrelation coefficient jointly with the\nmodel parameters in order to adjust for autocorrelated errors. For time series\nregression, large-scale experiments indicate that our method outperforms the\nPrais-Winsten method, especially when the autocorrelation is strong.\nFurthermore, we broaden our method to time series forecasting and apply it with\nvarious state-of-the-art models. Results across a wide range of real-world\ndatasets show that our method enhances performance in almost all cases.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 04:25:51 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 04:12:58 GMT"}, {"version": "v3", "created": "Wed, 3 Mar 2021 01:01:16 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Sun", "Fan-Keng", ""], ["Lang", "Christopher I.", ""], ["Boning", "Duane S.", ""]]}, {"id": "2101.12678", "submitter": "Hannes K\\\"ohler", "authors": "Hannes K\\\"ohler, Andreas Christmann", "title": "Total Stability of SVMs and Localized SVMs", "comments": "30 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularized kernel-based methods such as support vector machines (SVMs)\ntypically depend on the underlying probability measure $\\mathrm{P}$\n(respectively an empirical measure $\\mathrm{D}_n$ in applications) as well as\non the regularization parameter $\\lambda$ and the kernel $k$. Whereas classical\nstatistical robustness only considers the effect of small perturbations in\n$\\mathrm{P}$, the present paper investigates the influence of simultaneous\nslight variations in the whole triple $(\\mathrm{P},\\lambda,k)$, respectively\n$(\\mathrm{D}_n,\\lambda_n,k)$, on the resulting predictor. Existing results from\nthe literature are considerably generalized and improved. In order to also make\nthem applicable to big data, where regular SVMs suffer from their super-linear\ncomputational requirements, we show how our results can be transferred to the\ncontext of localized learning. Here, the effect of slight variations in the\napplied regionalization, which might for example stem from changes in\n$\\mathrm{P}$ respectively $\\mathrm{D}_n$, is considered as well.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 16:44:14 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["K\u00f6hler", "Hannes", ""], ["Christmann", "Andreas", ""]]}, {"id": "2101.12699", "submitter": "Weijie J. Su", "authors": "Cong Fang, Hangfeng He, Qi Long, Weijie J. Su", "title": "Layer-Peeled Model: Toward Understanding Well-Trained Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce the Layer-Peeled Model, a nonconvex yet\nanalytically tractable optimization program, in a quest to better understand\ndeep neural networks that are trained for a sufficiently long time. As the name\nsuggests, this new model is derived by isolating the topmost layer from the\nremainder of the neural network, followed by imposing certain constraints\nseparately on the two parts. We demonstrate that the Layer-Peeled Model, albeit\nsimple, inherits many characteristics of well-trained neural networks, thereby\noffering an effective tool for explaining and predicting common empirical\npatterns of deep learning training. First, when working on class-balanced\ndatasets, we prove that any solution to this model forms a simplex equiangular\ntight frame, which in part explains the recently discovered phenomenon of\nneural collapse in deep learning training [PHD20]. Moreover, when moving to the\nimbalanced case, our analysis of the Layer-Peeled Model reveals a hitherto\nunknown phenomenon that we term Minority Collapse, which fundamentally limits\nthe performance of deep learning models on the minority classes. In addition,\nwe use the Layer-Peeled Model to gain insights into how to mitigate Minority\nCollapse. Interestingly, this phenomenon is first predicted by the Layer-Peeled\nModel before its confirmation by our computational experiments.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 17:37:17 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 20:31:42 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Fang", "Cong", ""], ["He", "Hangfeng", ""], ["Long", "Qi", ""], ["Su", "Weijie J.", ""]]}]