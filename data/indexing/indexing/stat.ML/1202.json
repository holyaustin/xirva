[{"id": "1202.0302", "submitter": "Danica J. Sutherland", "authors": "Danica J. Sutherland, Liang Xiong, Barnab\\'as P\\'oczos, and Jeff\n  Schneider", "title": "Kernels on Sample Sets via Nonparametric Divergence Estimates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most machine learning algorithms, such as classification or regression, treat\nthe individual data point as the object of interest. Here we consider extending\nmachine learning algorithms to operate on groups of data points. We suggest\ntreating a group of data points as an i.i.d. sample set from an underlying\nfeature distribution for that group. Our approach employs kernel machines with\na kernel on i.i.d. sample sets of vectors. We define certain kernel functions\non pairs of distributions, and then use a nonparametric estimator to\nconsistently estimate those functions based on sample sets. The projection of\nthe estimated Gram matrix to the cone of symmetric positive semi-definite\nmatrices enables us to use kernel machines for classification, regression,\nanomaly detection, and low-dimensional embedding in the space of distributions.\nWe present several numerical experiments both on real and simulated datasets to\ndemonstrate the advantages of our new approach.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2012 21:36:40 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2012 15:43:04 GMT"}, {"version": "v3", "created": "Thu, 14 Jan 2021 06:58:28 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Sutherland", "Danica J.", ""], ["Xiong", "Liang", ""], ["P\u00f3czos", "Barnab\u00e1s", ""], ["Schneider", "Jeff", ""]]}, {"id": "1202.0501", "submitter": "Leo Lahti", "authors": "Leo Lahti, Juha E. A. Knuuttila, Samuel Kaski", "title": "Global modeling of transcriptional responses in interaction networks", "comments": "19 pages, 13 figures", "journal-ref": "Global modeling of transcriptional responses in interaction\n  networks. Leo Lahti, Juha E.A. Knuuttila, and Samuel Kaski. Bioinformatics\n  26(21):2713-2720, 2010", "doi": "10.1093/bioinformatics/btq500", "report-no": null, "categories": "q-bio.MN cs.CE q-bio.QM stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Motivation: Cell-biological processes are regulated through a complex network\nof interactions between genes and their products. The processes, their\nactivating conditions, and the associated transcriptional responses are often\nunknown. Organism-wide modeling of network activation can reveal unique and\nshared mechanisms between physiological conditions, and potentially as yet\nunknown processes. We introduce a novel approach for organism-wide discovery\nand analysis of transcriptional responses in interaction networks. The method\nsearches for local, connected regions in a network that exhibit coordinated\ntranscriptional response in a subset of conditions. Known interactions between\ngenes are used to limit the search space and to guide the analysis. Validation\non a human pathway network reveals physiologically coherent responses,\nfunctional relatedness between physiological conditions, and coordinated,\ncontext-specific regulation of the genes. Availability: Implementation is\nfreely available in R and Matlab at http://netpro.r-forge.r-project.org\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2012 17:40:14 GMT"}], "update_date": "2012-02-03", "authors_parsed": [["Lahti", "Leo", ""], ["Knuuttila", "Juha E. A.", ""], ["Kaski", "Samuel", ""]]}, {"id": "1202.0515", "submitter": "Makoto Yamada", "authors": "Makoto Yamada, Wittawat Jitkrittum, Leonid Sigal, Eric P. Xing,\n  Masashi Sugiyama", "title": "High-Dimensional Feature Selection by Feature-Wise Kernelized Lasso", "comments": "18 pages", "journal-ref": "Neural Computation 2014", "doi": "10.1162/NECO_a_00537", "report-no": null, "categories": "stat.ML cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of supervised feature selection is to find a subset of input\nfeatures that are responsible for predicting output values. The least absolute\nshrinkage and selection operator (Lasso) allows computationally efficient\nfeature selection based on linear dependency between input features and output\nvalues. In this paper, we consider a feature-wise kernelized Lasso for\ncapturing non-linear input-output dependency. We first show that, with\nparticular choices of kernel functions, non-redundant features with strong\nstatistical dependence on output values can be found in terms of kernel-based\nindependence measures. We then show that the globally optimal solution can be\nefficiently computed; this makes the approach scalable to high-dimensional\nproblems. The effectiveness of the proposed method is demonstrated through\nfeature selection experiments with thousands of features.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2012 19:06:02 GMT"}, {"version": "v2", "created": "Sat, 9 Jun 2012 02:26:05 GMT"}, {"version": "v3", "created": "Wed, 21 Aug 2013 05:25:29 GMT"}, {"version": "v4", "created": "Fri, 4 Jan 2019 00:04:52 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Yamada", "Makoto", ""], ["Jitkrittum", "Wittawat", ""], ["Sigal", "Leonid", ""], ["Xing", "Eric P.", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1202.0786", "submitter": "Vincent Vu", "authors": "Vincent Q. Vu and Jing Lei", "title": "Minimax Rates of Estimation for Sparse PCA in High Dimensions", "comments": "To appear in Proceedings of the 15th International Conference on\n  Artificial Intelligence and Statistics (AISTATS) 2012, La Palma, Canary\n  Islands. Volume 22 of JMLR: W&CP 22", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study sparse principal components analysis in the high-dimensional\nsetting, where $p$ (the number of variables) can be much larger than $n$ (the\nnumber of observations). We prove optimal, non-asymptotic lower and upper\nbounds on the minimax estimation error for the leading eigenvector when it\nbelongs to an $\\ell_q$ ball for $q \\in [0,1]$. Our bounds are sharp in $p$ and\n$n$ for all $q \\in [0, 1]$ over a wide class of distributions. The upper bound\nis obtained by analyzing the performance of $\\ell_q$-constrained PCA. In\nparticular, our results provide convergence rates for $\\ell_1$-constrained PCA.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2012 17:44:36 GMT"}, {"version": "v2", "created": "Mon, 6 Feb 2012 01:19:43 GMT"}], "update_date": "2012-02-07", "authors_parsed": [["Vu", "Vincent Q.", ""], ["Lei", "Jing", ""]]}, {"id": "1202.0825", "submitter": "Brian McWilliams", "authors": "Brian McWilliams and Giovanni Montana", "title": "Multi-view predictive partitioning in high dimensions", "comments": "31 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern data mining applications are concerned with the analysis of\ndatasets in which the observations are described by paired high-dimensional\nvectorial representations or \"views\". Some typical examples can be found in web\nmining and genomics applications. In this article we present an algorithm for\ndata clustering with multiple views, Multi-View Predictive Partitioning (MVPP),\nwhich relies on a novel criterion of predictive similarity between data points.\nWe assume that, within each cluster, the dependence between multivariate views\ncan be modelled by using a two-block partial least squares (TB-PLS) regression\nmodel, which performs dimensionality reduction and is particularly suitable for\nhigh-dimensional settings. The proposed MVPP algorithm partitions the data such\nthat the within-cluster predictive ability between views is maximised. The\nproposed objective function depends on a measure of predictive influence of\npoints under the TB-PLS model which has been derived as an extension of the\nPRESS statistic commonly used in ordinary least squares regression. Using\nsimulated data, we compare the performance of MVPP to that of competing\nmulti-view clustering methods which rely upon geometric structures of points,\nbut ignore the predictive relationship between the two views. State-of-art\nresults are obtained on benchmark web mining datasets.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2012 18:22:46 GMT"}], "update_date": "2012-02-07", "authors_parsed": [["McWilliams", "Brian", ""], ["Montana", "Giovanni", ""]]}, {"id": "1202.0840", "submitter": "Ramji Venkataramanan", "authors": "Ramji Venkataramanan, Antony Joseph, Sekhar Tatikonda", "title": "Lossy Compression via Sparse Linear Regression: Performance under\n  Minimum-distance Encoding", "comments": "This version corrects a typo in the statement of Theorem 2 of the\n  published paper", "journal-ref": "IEEE Transactions on Information Theory, vol. 60, no. 6, pp.\n  3254-3264, June 2014", "doi": "10.1109/TIT.2014.2313085", "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a new class of codes for lossy compression with the squared-error\ndistortion criterion, designed using the statistical framework of\nhigh-dimensional linear regression. Codewords are linear combinations of\nsubsets of columns of a design matrix. Called a Sparse Superposition or Sparse\nRegression codebook, this structure is motivated by an analogous construction\nproposed recently by Barron and Joseph for communication over an AWGN channel.\nFor i.i.d Gaussian sources and minimum-distance encoding, we show that such a\ncode can attain the Shannon rate-distortion function with the optimal error\nexponent, for all distortions below a specified value. It is also shown that\nsparse regression codes are robust in the following sense: a codebook designed\nto compress an i.i.d Gaussian source of variance $\\sigma^2$ with\n(squared-error) distortion $D$ can compress any ergodic source of variance less\nthan $\\sigma^2$ to within distortion $D$. Thus the sparse regression ensemble\nretains many of the good covering properties of the i.i.d random Gaussian\nensemble, while having having a compact representation in terms of a matrix\nwhose size is a low-order polynomial in the block-length.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2012 22:49:47 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2012 18:35:46 GMT"}, {"version": "v3", "created": "Fri, 28 Mar 2014 15:33:08 GMT"}, {"version": "v4", "created": "Fri, 18 Dec 2015 19:42:20 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Venkataramanan", "Ramji", ""], ["Joseph", "Antony", ""], ["Tatikonda", "Sekhar", ""]]}, {"id": "1202.0855", "submitter": "Buyue Qian", "authors": "Buyue Qian, Xiang Wang and Ian Davidson", "title": "A Reconstruction Error Formulation for Semi-Supervised Multi-task and\n  Multi-view Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significant challenge to make learning techniques more suitable for general\npurpose use is to move beyond i) complete supervision, ii) low dimensional\ndata, iii) a single task and single view per instance. Solving these challenges\nallows working with \"Big Data\" problems that are typically high dimensional\nwith multiple (but possibly incomplete) labelings and views. While other work\nhas addressed each of these problems separately, in this paper we show how to\naddress them together, namely semi-supervised dimension reduction for\nmulti-task and multi-view learning (SSDR-MML), which performs optimization for\ndimension reduction and label inference in semi-supervised setting. The\nproposed framework is designed to handle both multi-task and multi-view\nlearning settings, and can be easily adapted to many useful applications.\nInformation obtained from all tasks and views is combined via reconstruction\nerrors in a linear fashion that can be efficiently solved using an alternating\noptimization scheme. Our formulation has a number of advantages. We explicitly\nmodel the information combining mechanism as a data structure (a\nweight/nearest-neighbor matrix) which allows investigating fundamental\nquestions in multi-task and multi-view learning. We address one such question\nby presenting a general measure to quantify the success of simultaneous\nlearning of multiple tasks or from multiple views. We show that our SSDR-MML\napproach can outperform many state-of-the-art baseline methods and demonstrate\nthe effectiveness of connecting dimension reduction and learning.\n", "versions": [{"version": "v1", "created": "Sat, 4 Feb 2012 01:41:36 GMT"}], "update_date": "2012-02-07", "authors_parsed": [["Qian", "Buyue", ""], ["Wang", "Xiang", ""], ["Davidson", "Ian", ""]]}, {"id": "1202.1119", "submitter": "Ranjitha Prasad", "authors": "Ranjitha Prasad and Chandra R. Murthy", "title": "Cramer Rao-Type Bounds for Sparse Bayesian Learning", "comments": "Accepted for publication in the IEEE Transactions on Signal\n  Processing, 11 pages, 10 figures", "journal-ref": null, "doi": "10.1109/TSP.2012.2226165", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we derive Hybrid, Bayesian and Marginalized Cram\\'{e}r-Rao\nlower bounds (HCRB, BCRB and MCRB) for the single and multiple measurement\nvector Sparse Bayesian Learning (SBL) problem of estimating compressible\nvectors and their prior distribution parameters. We assume the unknown vector\nto be drawn from a compressible Student-t prior distribution. We derive CRBs\nthat encompass the deterministic or random nature of the unknown parameters of\nthe prior distribution and the regression noise variance. We extend the MCRB to\nthe case where the compressible vector is distributed according to a general\ncompressible prior distribution, of which the generalized Pareto distribution\nis a special case. We use the derived bounds to uncover the relationship\nbetween the compressibility and Mean Square Error (MSE) in the estimates.\nFurther, we illustrate the tightness and utility of the bounds through\nsimulations, by comparing them with the MSE performance of two popular\nSBL-based estimators. It is found that the MCRB is generally the tightest among\nthe bounds derived and that the MSE performance of the Expectation-Maximization\n(EM) algorithm coincides with the MCRB for the compressible vector. Through\nsimulations, we demonstrate the dependence of the MSE performance of SBL based\nestimators on the compressibility of the vector for several values of the\nnumber of observations and at different signal powers.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2012 12:39:37 GMT"}, {"version": "v2", "created": "Thu, 18 Oct 2012 04:17:59 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Prasad", "Ranjitha", ""], ["Murthy", "Chandra R.", ""]]}, {"id": "1202.1121", "submitter": "Miron Kursa", "authors": "Miron B. Kursa", "title": "rFerns: An Implementation of the Random Ferns Method for General-Purpose\n  Machine Learning", "comments": null, "journal-ref": "Journal of Statistical Software, 61(10), 1-13", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper I present an extended implementation of the Random ferns\nalgorithm contained in the R package rFerns. It differs from the original by\nthe ability of consuming categorical and numerical attributes instead of only\nbinary ones. Also, instead of using simple attribute subspace ensemble it\nemploys bagging and thus produce error approximation and variable importance\nmeasure modelled after Random forest algorithm. I also present benchmarks'\nresults which show that although Random ferns' accuracy is mostly smaller than\nachieved by Random forest, its speed and good quality of importance measure it\nprovides make rFerns a reasonable choice for a specific applications.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2012 12:43:12 GMT"}, {"version": "v2", "created": "Fri, 14 Nov 2014 14:39:39 GMT"}], "update_date": "2014-11-17", "authors_parsed": [["Kursa", "Miron B.", ""]]}, {"id": "1202.1467", "submitter": "Mihai-Alin Badiu", "authors": "Mihai-Alin Badiu, Gunvor Elisabeth Kirkelund, Carles Navarro\n  Manch\\'on, Erwin Riegler, Bernard Henri Fleury", "title": "Message-Passing Algorithms for Channel Estimation and Decoding Using\n  Approximate Inference", "comments": "Accepted for publication in the Proceedings of 2012 IEEE\n  International Symposium on Information Theory", "journal-ref": null, "doi": "10.1109/ISIT.2012.6283939", "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design iterative receiver schemes for a generic wireless communication\nsystem by treating channel estimation and information decoding as an inference\nproblem in graphical models. We introduce a recently proposed inference\nframework that combines belief propagation (BP) and the mean field (MF)\napproximation and includes these algorithms as special cases. We also show that\nthe expectation propagation and expectation maximization algorithms can be\nembedded in the BP-MF framework with slight modifications. By applying the\nconsidered inference algorithms to our probabilistic model, we derive four\ndifferent message-passing receiver schemes. Our numerical evaluation\ndemonstrates that the receiver based on the BP-MF framework and its variant\nbased on BP-EM yield the best compromise between performance, computational\ncomplexity and numerical stability among all candidate algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2012 17:14:46 GMT"}, {"version": "v2", "created": "Mon, 23 Jul 2012 16:09:55 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Badiu", "Mihai-Alin", ""], ["Kirkelund", "Gunvor Elisabeth", ""], ["Manch\u00f3n", "Carles Navarro", ""], ["Riegler", "Erwin", ""], ["Fleury", "Bernard Henri", ""]]}, {"id": "1202.1523", "submitter": "Stefano Soatto", "authors": "Zhao Yi, Stefano Soatto, Maneesh Dewan, Yiqiang Zhan", "title": "Information Forests", "comments": "Proceedings of the Information Theory and Applications (ITA)\n  Workshop, 2/7/2012", "journal-ref": null, "doi": null, "report-no": "UCLA CSD TR120002", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe Information Forests, an approach to classification that\ngeneralizes Random Forests by replacing the splitting criterion of non-leaf\nnodes from a discriminative one -- based on the entropy of the label\ndistribution -- to a generative one -- based on maximizing the information\ndivergence between the class-conditional distributions in the resulting\npartitions. The basic idea consists of deferring classification until a measure\nof \"classification confidence\" is sufficiently high, and instead breaking down\nthe data so as to maximize this measure. In an alternative interpretation,\nInformation Forests attempt to partition the data into subsets that are \"as\ninformative as possible\" for the purpose of the task, which is to classify the\ndata. Classification confidence, or informative content of the subsets, is\nquantified by the Information Divergence. Our approach relates to active\nlearning, semi-supervised learning, mixed generative/discriminative learning.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2012 14:54:59 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Yi", "Zhao", ""], ["Soatto", "Stefano", ""], ["Dewan", "Maneesh", ""], ["Zhan", "Yiqiang", ""]]}, {"id": "1202.1595", "submitter": "Chinmay Hegde", "authors": "Chinmay Hegde and Richard G. Baraniuk", "title": "Signal Recovery on Incoherent Manifolds", "comments": "20 pages, 3 figures. Submitted to IEEE Trans. Inform. Theory. Revised\n  version (June 2012) : fixed typos in proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that we observe noisy linear measurements of an unknown signal that\ncan be modeled as the sum of two component signals, each of which arises from a\nnonlinear sub-manifold of a high dimensional ambient space. We introduce SPIN,\na first order projected gradient method to recover the signal components.\nDespite the nonconvex nature of the recovery problem and the possibility of\nunderdetermined measurements, SPIN provably recovers the signal components,\nprovided that the signal manifolds are incoherent and that the measurement\noperator satisfies a certain restricted isometry property. SPIN significantly\nextends the scope of current recovery models and algorithms for low dimensional\nlinear inverse problems and matches (or exceeds) the current state of the art\nin terms of performance.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2012 04:06:54 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2012 15:37:29 GMT"}], "update_date": "2012-06-11", "authors_parsed": [["Hegde", "Chinmay", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1202.1779", "submitter": "Praneeth Netrapalli", "authors": "Praneeth Netrapalli and Sujay Sanghavi", "title": "Finding the Graph of Epidemic Cascades", "comments": "To appear in Proc. ACM SIGMETRICS/Performance 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding the graph on which an epidemic cascade\nspreads, given only the times when each node gets infected. While this is a\nproblem of importance in several contexts -- offline and online social\nnetworks, e-commerce, epidemiology, vulnerabilities in infrastructure networks\n-- there has been very little work, analytical or empirical, on finding the\ngraph. Clearly, it is impossible to do so from just one cascade; our interest\nis in learning the graph from a small number of cascades.\n  For the classic and popular \"independent cascade\" SIR epidemics, we\nanalytically establish the number of cascades required by both the global\nmaximum-likelihood (ML) estimator, and a natural greedy algorithm. Both results\nare based on a key observation: the global graph learning problem decouples\ninto $n$ local problems -- one for each node. For a node of degree $d$, we show\nthat its neighborhood can be reliably found once it has been infected $O(d^2\n\\log n)$ times (for ML on general graphs) or $O(d\\log n)$ times (for greedy on\ntrees). We also provide a corresponding information-theoretic lower bound of\n$\\Omega(d\\log n)$; thus our bounds are essentially tight. Furthermore, if we\nare given side-information in the form of a super-graph of the actual graph (as\nis often the case), then the number of cascade samples required -- in all cases\n-- becomes independent of the network size $n$.\n  Finally, we show that for a very general SIR epidemic cascade model, the\nMarkov graph of infection times is obtained via the moralization of the network\ngraph.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2012 17:46:09 GMT"}], "update_date": "2012-02-09", "authors_parsed": [["Netrapalli", "Praneeth", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1202.1787", "submitter": "Praneeth Netrapalli", "authors": "Praneeth Netrapalli, Siddhartha Banerjee, Sujay Sanghavi and Sanjay\n  Shakkottai", "title": "Greedy Learning of Markov Network Structure", "comments": "Preliminary version appeared in 48th Annual Allerton Conference on\n  Communication, Control, and Computing, 2010. Full version submitted to JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new yet natural algorithm for learning the graph structure of\ngeneral discrete graphical models (a.k.a. Markov random fields) from samples.\nOur algorithm finds the neighborhood of a node by sequentially adding nodes\nthat produce the largest reduction in empirical conditional entropy; it is\ngreedy in the sense that the choice of addition is based only on the reduction\nachieved at that iteration. Its sequential nature gives it a lower\ncomputational complexity as compared to other existing comparison-based\ntechniques, all of which involve exhaustive searches over every node set of a\ncertain size. Our main result characterizes the sample complexity of this\nprocedure, as a function of node degrees, graph size and girth in factor-graph\nrepresentation. We subsequently specialize this result to the case of Ising\nmodels, where we provide a simple transparent characterization of sample\ncomplexity as a function of model and graph parameters.\n  For tree graphs, our algorithm is the same as the classical Chow-Liu\nalgorithm, and in that sense can be considered the extension of the same to\ngraphs with cycles.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2012 18:18:46 GMT"}], "update_date": "2012-02-09", "authors_parsed": [["Netrapalli", "Praneeth", ""], ["Banerjee", "Siddhartha", ""], ["Sanghavi", "Sujay", ""], ["Shakkottai", "Sanjay", ""]]}, {"id": "1202.2143", "submitter": "Il Memming Park", "authors": "Il Memming Park, Marcel Nassar, Mijung Park", "title": "Active Bayesian Optimization: Minimizing Minimizer Entropy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ultimate goal of optimization is to find the minimizer of a target\nfunction.However, typical criteria for active optimization often ignore the\nuncertainty about the minimizer. We propose a novel criterion for global\noptimization and an associated sequential active learning strategy using\nGaussian processes.Our criterion is the reduction of uncertainty in the\nposterior distribution of the function minimizer. It can also flexibly\nincorporate multiple global minimizers. We implement a tractable approximation\nof the criterion and demonstrate that it obtains the global minimizer\naccurately compared to conventional Bayesian optimization criteria.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2012 22:31:01 GMT"}], "update_date": "2012-02-13", "authors_parsed": [["Park", "Il Memming", ""], ["Nassar", "Marcel", ""], ["Park", "Mijung", ""]]}, {"id": "1202.2169", "submitter": "Han Liu", "authors": "Han Liu, Fang Han, Ming Yuan, John Lafferty and Larry Wasserman", "title": "High Dimensional Semiparametric Gaussian Copula Graphical Models", "comments": "34 pages, 10 figures; the Annals of Statistics, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a semiparametric approach, named nonparanormal\nskeptic, for efficiently and robustly estimating high dimensional undirected\ngraphical models. To achieve modeling flexibility, we consider Gaussian Copula\ngraphical models (or the nonparanormal) as proposed by Liu et al. (2009). To\nachieve estimation robustness, we exploit nonparametric rank-based correlation\ncoefficient estimators, including Spearman's rho and Kendall's tau. In high\ndimensional settings, we prove that the nonparanormal skeptic achieves the\noptimal parametric rate of convergence in both graph and parameter estimation.\nThis celebrating result suggests that the Gaussian copula graphical models can\nbe used as a safe replacement of the popular Gaussian graphical models, even\nwhen the data are truly Gaussian. Besides theoretical analysis, we also conduct\nthorough numerical simulations to compare different estimators for their graph\nrecovery performance under both ideal and noisy settings. The proposed methods\nare then applied on a large-scale genomic dataset to illustrate their empirical\nusefulness. The R language software package huge implementing the proposed\nmethods is available on the Comprehensive R Archive Network: http://cran.\nr-project.org/.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2012 03:06:07 GMT"}, {"version": "v2", "created": "Mon, 26 Mar 2012 01:11:09 GMT"}, {"version": "v3", "created": "Fri, 27 Jul 2012 19:53:58 GMT"}], "update_date": "2012-07-30", "authors_parsed": [["Liu", "Han", ""], ["Han", "Fang", ""], ["Yuan", "Ming", ""], ["Lafferty", "John", ""], ["Wasserman", "Larry", ""]]}, {"id": "1202.2194", "submitter": "Peter Mills", "authors": "Peter Mills", "title": "Efficient statistical classification of satellite measurements", "comments": "Corrected formatting errors, corrected equation in appendix", "journal-ref": "International Journal of Remote Sensing, 2011, 32(21): 6109-6132", "doi": "10.1080/01431161.2010.507795", "report-no": null, "categories": "physics.ao-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised statistical classification is a vital tool for satellite image\nprocessing. It is useful not only when a discrete result, such as feature\nextraction or surface type, is required, but also for continuum retrievals by\ndividing the quantity of interest into discrete ranges. Because of the high\nresolution of modern satellite instruments and because of the requirement for\nreal-time processing, any algorithm has to be fast to be useful. Here we\ndescribe an algorithm based on kernel estimation called Adaptive Gaussian\nFiltering that incorporates several innovations to produce superior efficiency\nas compared to three other popular methods: k-nearest-neighbour (KNN), Learning\nVector Quantization (LVQ) and Support Vector Machines (SVM). This efficiency is\ngained with no compromises: accuracy is maintained, while estimates of the\nconditional probabilities are returned. These are useful not only to gauge the\naccuracy of an estimate in the absence of its true value, but also to\nre-calibrate a retrieved image and as a proxy for a discretized continuum\nvariable. The algorithm is demonstrated and compared with the other three on a\npair of synthetic test classes and to map the waterways of the Netherlands.\nSoftware may be found at: http://libagf.sourceforge.net.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2012 06:35:20 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2012 19:47:23 GMT"}, {"version": "v3", "created": "Thu, 5 Feb 2015 21:55:45 GMT"}, {"version": "v4", "created": "Wed, 3 Feb 2016 22:59:26 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Mills", "Peter", ""]]}, {"id": "1202.2476", "submitter": "Genevera Allen", "authors": "Genevera I. Allen", "title": "Regularized Tensor Factorizations and Higher-Order Principal Components\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional tensors or multi-way data are becoming prevalent in areas\nsuch as biomedical imaging, chemometrics, networking and bibliometrics.\nTraditional approaches to finding lower dimensional representations of tensor\ndata include flattening the data and applying matrix factorizations such as\nprincipal components analysis (PCA) or employing tensor decompositions such as\nthe CANDECOMP / PARAFAC (CP) and Tucker decompositions. The former can lose\nimportant structure in the data, while the latter Higher-Order PCA (HOPCA)\nmethods can be problematic in high-dimensions with many irrelevant features. We\nintroduce frameworks for sparse tensor factorizations or Sparse HOPCA based on\nheuristic algorithmic approaches and by solving penalized optimization problems\nrelated to the CP decomposition. Extensions of these approaches lead to methods\nfor general regularized tensor factorizations, multi-way Functional HOPCA and\ngeneralizations of HOPCA for structured data. We illustrate the utility of our\nmethods for dimension reduction, feature selection, and signal recovery on\nsimulated data and multi-dimensional microarrays and functional MRIs.\n", "versions": [{"version": "v1", "created": "Sat, 11 Feb 2012 21:22:41 GMT"}], "update_date": "2012-02-14", "authors_parsed": [["Allen", "Genevera I.", ""]]}, {"id": "1202.2564", "submitter": "Christoforos Anagnostopoulos Dr", "authors": "David J. Hand, Christoforos Anagnostopoulos", "title": "A better Beta for the H measure of classification performance", "comments": "Preprint. Keywords: supervised classification, classifier\n  performance, AUC, ROC curve, H measure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The area under the ROC curve is widely used as a measure of performance of\nclassification rules. However, it has recently been shown that the measure is\nfundamentally incoherent, in the sense that it treats the relative severities\nof misclassifications differently when different classifiers are used. To\novercome this, Hand (2009) proposed the $H$ measure, which allows a given\nresearcher to fix the distribution of relative severities to a\nclassifier-independent setting on a given problem. This note extends the\ndiscussion, and proposes a modified standard distribution for the $H$ measure,\nwhich better matches the requirements of researchers, in particular those faced\nwith heavily unbalanced datasets, the $Beta(\\pi_1+1,\\pi_0+1)$ distribution.\n[Preprint submitted at Pattern Recognition Letters]\n", "versions": [{"version": "v1", "created": "Sun, 12 Feb 2012 20:32:15 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2013 11:44:54 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Hand", "David J.", ""], ["Anagnostopoulos", "Christoforos", ""]]}, {"id": "1202.2723", "submitter": "Tingni Sun", "authors": "Tingni Sun and Cun-Hui Zhang", "title": "Sparse Matrix Inversion with Scaled Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method of learning a sparse nonnegative-definite target\nmatrix. Our primary example of the target matrix is the inverse of a population\ncovariance or correlation matrix. The algorithm first estimates each column of\nthe target matrix by the scaled Lasso and then adjusts the matrix estimator to\nbe symmetric. The penalty level of the scaled Lasso for each column is\ncompletely determined by data via convex minimization, without using\ncross-validation.\n  We prove that this scaled Lasso method guarantees the fastest proven rate of\nconvergence in the spectrum norm under conditions of weaker form than those in\nthe existing analyses of other $\\ell_1$ regularized algorithms, and has faster\nguaranteed rate of convergence when the ratio of the $\\ell_1$ and spectrum\nnorms of the target inverse matrix diverges to infinity. A simulation study\ndemonstrates the computational feasibility and superb performance of the\nproposed method.\n  Our analysis also provides new performance bounds for the Lasso and scaled\nLasso to guarantee higher concentration of the error at a smaller threshold\nlevel than previous analyses, and to allow the use of the union bound in\ncolumn-by-column applications of the scaled Lasso without an adjustment of the\npenalty level. In addition, the least squares estimation after the scaled Lasso\nselection is considered and proven to guarantee performance bounds similar to\nthat of the scaled Lasso.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2012 13:31:45 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2013 19:58:33 GMT"}], "update_date": "2013-10-15", "authors_parsed": [["Sun", "Tingni", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "1202.2892", "submitter": "Dmitry Ignatov", "authors": "Dmitry I. Ignatov, Jonas Poelmans, Vasily Zaharchuk", "title": "Recommender System Based on Algorithm of Bicluster Analysis RecBi", "comments": null, "journal-ref": "CEUR Workshop proceedings Vol-757, CDUD'11 - Concept Discovery in\n  Unstructured Data, pp. 122-126, 2011", "doi": null, "report-no": null, "categories": "cs.AI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose two new algorithms based on biclustering analysis,\nwhich can be used at the basis of a recommender system for educational\norientation of Russian School graduates. The first algorithm was designed to\nhelp students make a choice between different university faculties when some of\ntheir preferences are known. The second algorithm was developed for the special\nsituation when nothing is known about their preferences. The final version of\nthis recommender system will be used by Higher School of Economics.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2012 23:10:08 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Ignatov", "Dmitry I.", ""], ["Poelmans", "Jonas", ""], ["Zaharchuk", "Vasily", ""]]}, {"id": "1202.2895", "submitter": "Dmitry Ignatov", "authors": "Jonas Poelmans, Paul Elzinga, Alexey Neznanov, Stijn Viaene, Sergei O.\n  Kuznetsov, Dmitry Ignatov, Guido Dedene", "title": "Concept Relation Discovery and Innovation Enabling Technology (CORDIET)", "comments": null, "journal-ref": "In CEUR Workshop proceedings Vol-757, CDUD'11 - Concept Discovery\n  in Unstructured Data, pp. 53-62, 2011", "doi": null, "report-no": null, "categories": "cs.AI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concept Relation Discovery and Innovation Enabling Technology (CORDIET), is a\ntoolbox for gaining new knowledge from unstructured text data. At the core of\nCORDIET is the C-K theory which captures the essential elements of innovation.\nThe tool uses Formal Concept Analysis (FCA), Emergent Self Organizing Maps\n(ESOM) and Hidden Markov Models (HMM) as main artifacts in the analysis\nprocess. The user can define temporal, text mining and compound attributes. The\ntext mining attributes are used to analyze the unstructured text in documents,\nthe temporal attributes use these document's timestamps for analysis. The\ncompound attributes are XML rules based on text mining and temporal attributes.\nThe user can cluster objects with object-cluster rules and can chop the data in\npieces with segmentation rules. The artifacts are optimized for efficient data\nanalysis; object labels in the FCA lattice and ESOM map contain an URL on which\nthe user can click to open the selected document.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2012 23:19:51 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Poelmans", "Jonas", ""], ["Elzinga", "Paul", ""], ["Neznanov", "Alexey", ""], ["Viaene", "Stijn", ""], ["Kuznetsov", "Sergei O.", ""], ["Ignatov", "Dmitry", ""], ["Dedene", "Guido", ""]]}, {"id": "1202.3079", "submitter": "Nicol\\`o Cesa-Bianchi", "authors": "S\\'ebastien Bubeck, Nicol\\`o Cesa-Bianchi, Sham M. Kakade", "title": "Towards minimax policies for online linear optimization with bandit\n  feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the online linear optimization problem with bandit feedback. Our\ncontribution is twofold. First, we provide an algorithm (based on exponential\nweights) with a regret of order $\\sqrt{d n \\log N}$ for any finite action set\nwith $N$ actions, under the assumption that the instantaneous loss is bounded\nby 1. This shaves off an extraneous $\\sqrt{d}$ factor compared to previous\nworks, and gives a regret bound of order $d \\sqrt{n \\log n}$ for any compact\nset of actions. Without further assumptions on the action set, this last bound\nis minimax optimal up to a logarithmic factor. Interestingly, our result also\nshows that the minimax regret for bandit linear optimization with expert advice\nin $d$ dimension is the same as for the basic $d$-armed bandit with expert\nadvice. Our second contribution is to show how to use the Mirror Descent\nalgorithm to obtain computationally efficient strategies with minimax optimal\nregret bounds in specific examples. More precisely we study two canonical\naction sets: the hypercube and the Euclidean ball. In the former case, we\nobtain the first computationally efficient algorithm with a $d \\sqrt{n}$\nregret, thus improving by a factor $\\sqrt{d \\log n}$ over the best known result\nfor a computationally efficient algorithm. In the latter case, our approach\ngives the first algorithm with a $\\sqrt{d n \\log n}$ regret, again shaving off\nan extraneous $\\sqrt{d}$ compared to previous works.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:12:09 GMT"}], "update_date": "2012-02-15", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Cesa-Bianchi", "Nicol\u00f2", ""], ["Kakade", "Sham M.", ""]]}, {"id": "1202.3323", "submitter": "Gilles Stoltz", "authors": "Nicol\\`o Cesa-Bianchi, Pierre Gaillard (INRIA Paris - Rocquencourt,\n  DMA), Gabor Lugosi (ICREA), Gilles Stoltz (INRIA Paris - Rocquencourt, DMA,\n  GREGH)", "title": "Mirror Descent Meets Fixed Share (and feels no regret)", "comments": null, "journal-ref": "NIPS 2012, Lake Tahoe : United States (2012)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mirror descent with an entropic regularizer is known to achieve shifting\nregret bounds that are logarithmic in the dimension. This is done using either\na carefully designed projection or by a weight sharing technique. Via a novel\nunified analysis, we show that these two approaches deliver essentially\nequivalent bounds on a notion of regret generalizing shifting, adaptive,\ndiscounted, and other related regrets. Our analysis also captures and extends\nthe generalized weight sharing technique of Bousquet and Warmuth, and can be\nrefined in several ways, including improvements for small losses and adaptive\ntuning of parameters.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2012 14:39:42 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2012 19:39:42 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Cesa-Bianchi", "Nicol\u00f2", "", "INRIA Paris - Rocquencourt,\n  DMA"], ["Gaillard", "Pierre", "", "INRIA Paris - Rocquencourt,\n  DMA"], ["Lugosi", "Gabor", "", "ICREA"], ["Stoltz", "Gilles", "", "INRIA Paris - Rocquencourt, DMA,\n  GREGH"]]}, {"id": "1202.3451", "submitter": "Fionn Murtagh", "authors": "Fionn Murtagh and Pedro Contreras", "title": "The Future of Search and Discovery in Big Data Analytics: Ultrametric\n  Information Spaces", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider observation data, comprised of n observation vectors with values on\na set of attributes. This gives us n points in attribute space. Having data\nstructured as a tree, implied by having our observations embedded in an\nultrametric topology, offers great advantage for proximity searching. If we\nhave preprocessed data through such an embedding, then an observation's nearest\nneighbor is found in constant computational time, i.e. O(1) time. A further\npowerful approach is discussed in this work: the inducing of a hierarchy, and\nhence a tree, in linear computational time, i.e. O(n) time for n observations.\nIt is with such a basis for proximity search and best match that we can address\nthe burgeoning problems of processing very large, and possibly also very high\ndimensional, data sets.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2012 21:33:44 GMT"}], "update_date": "2012-02-17", "authors_parsed": [["Murtagh", "Fionn", ""], ["Contreras", "Pedro", ""]]}, {"id": "1202.3701", "submitter": "Gowtham Bellala", "authors": "Gowtham Bellala, Jason Stanley, Clayton Scott, Suresh K. Bhavnani", "title": "Active Diagnosis via AUC Maximization: An Efficient Approach for\n  Multiple Fault Identification in Large Scale, Noisy Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-35-42", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of active diagnosis arises in several applications such as\ndisease diagnosis, and fault diagnosis in computer networks, where the goal is\nto rapidly identify the binary states of a set of objects (e.g., faulty or\nworking) by sequentially selecting, and observing, (noisy) responses to binary\nvalued queries. Current algorithms in this area rely on loopy belief\npropagation for active query selection. These algorithms have an exponential\ntime complexity, making them slow and even intractable in large networks. We\npropose a rank-based greedy algorithm that sequentially chooses queries such\nthat the area under the ROC curve of the rank-based output is maximized. The\nAUC criterion allows us to make a simplifying assumption that significantly\nreduces the complexity of active query selection (from exponential to near\nquadratic), with little or no compromise on the performance quality.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Bellala", "Gowtham", ""], ["Stanley", "Jason", ""], ["Scott", "Clayton", ""], ["Bhavnani", "Suresh K.", ""]]}, {"id": "1202.3702", "submitter": "Avleen S. Bijral", "authors": "Avleen S. Bijral, Nathan Ratliff, Nathan Srebro", "title": "Semi-supervised Learning with Density Based Distances", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-43-50", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple, yet effective, approach to Semi-Supervised Learning. Our\napproach is based on estimating density-based distances (DBD) using a shortest\npath calculation on a graph. These Graph-DBD estimates can then be used in any\ndistance-based supervised learning method, such as Nearest Neighbor methods and\nSVMs with RBF kernels. In order to apply the method to very large data sets, we\nalso present a novel algorithm which integrates nearest neighbor computations\ninto the shortest path search and can find exact shortest paths even in\nextremely large dense graphs. Significant runtime improvement over the commonly\nused Laplacian regularization method is then shown on a large scale dataset.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Bijral", "Avleen S.", ""], ["Ratliff", "Nathan", ""], ["Srebro", "Nathan", ""]]}, {"id": "1202.3704", "submitter": "Mithun Chakraborty", "authors": "Mithun Chakraborty, Sanmay Das, Malik Magdon-Ismail", "title": "Near-Optimal Target Learning With Stochastic Binary Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-69-76", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study learning in a noisy bisection model: specifically, Bayesian\nalgorithms to learn a target value V given access only to noisy realizations of\nwhether V is less than or greater than a threshold theta. At step t = 0, 1, 2,\n..., the learner sets threshold theta t and observes a noisy realization of\nsign(V - theta t). After T steps, the goal is to output an estimate V^ which is\nwithin an eta-tolerance of V . This problem has been studied, predominantly in\nenvironments with a fixed error probability q < 1/2 for the noisy realization\nof sign(V - theta t). In practice, it is often the case that q can approach\n1/2, especially as theta -> V, and there is little known when this happens. We\ngive a pseudo-Bayesian algorithm which provably converges to V. When the true\nprior matches our algorithm's Gaussian prior, we show near-optimal expected\nperformance. Our methods extend to the general multiple-threshold setting where\nthe observation noisily indicates which of k >= 2 regions V belongs to.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Chakraborty", "Mithun", ""], ["Das", "Sanmay", ""], ["Magdon-Ismail", "Malik", ""]]}, {"id": "1202.3708", "submitter": "Xi Chen", "authors": "Xi Chen, Qihang Lin, Seyoung Kim, Jaime G. Carbonell, Eric P. Xing", "title": "Smoothing Proximal Gradient Method for General Structured Sparse\n  Learning", "comments": "arXiv admin note: substantial text overlap with arXiv:1005.4717", "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-105-114", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning high dimensional regression models\nregularized by a structured-sparsity-inducing penalty that encodes prior\nstructural information on either input or output sides. We consider two widely\nadopted types of such penalties as our motivating examples: 1) overlapping\ngroup lasso penalty, based on the l1/l2 mixed-norm penalty, and 2) graph-guided\nfusion penalty. For both types of penalties, due to their non-separability,\ndeveloping an efficient optimization method has remained a challenging problem.\nIn this paper, we propose a general optimization approach, called smoothing\nproximal gradient method, which can solve the structured sparse regression\nproblems with a smooth convex loss and a wide spectrum of\nstructured-sparsity-inducing penalties. Our approach is based on a general\nsmoothing technique of Nesterov. It achieves a convergence rate faster than the\nstandard first-order method, subgradient method, and is much more scalable than\nthe most widely used interior-point method. Numerical results are reported to\ndemonstrate the efficiency and scalability of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Chen", "Xi", ""], ["Lin", "Qihang", ""], ["Kim", "Seyoung", ""], ["Carbonell", "Jaime G.", ""], ["Xing", "Eric P.", ""]]}, {"id": "1202.3712", "submitter": "Corinna Cortes", "authors": "Corinna Cortes, Mehryar Mohri, Afshin Rostamizadeh", "title": "Ensembles of Kernel Predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-145-152", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the problem of learning with a finite and possibly large\nset of p base kernels. It presents a theoretical and empirical analysis of an\napproach addressing this problem based on ensembles of kernel predictors. This\nincludes novel theoretical guarantees based on the Rademacher complexity of the\ncorresponding hypothesis sets, the introduction and analysis of a learning\nalgorithm based on these hypothesis sets, and a series of experiments using\nensembles of kernel predictors with several data sets. Both convex combinations\nof kernel-based hypotheses and more general Lq-regularized nonnegative\ncombinations are analyzed. These theoretical, algorithmic, and empirical\nresults are compared with those achieved by using learning kernel techniques,\nwhich can be viewed as another approach for solving the same problem.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Cortes", "Corinna", ""], ["Mohri", "Mehryar", ""], ["Rostamizadeh", "Afshin", ""]]}, {"id": "1202.3714", "submitter": "Kun Deng", "authors": "Kun Deng, Joelle Pineau, Susan A. Murphy", "title": "Active Learning for Developing Personalized Treatment", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-161-168", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The personalization of treatment via bio-markers and other risk categories\nhas drawn increasing interest among clinical scientists. Personalized treatment\nstrategies can be learned using data from clinical trials, but such trials are\nvery costly to run. This paper explores the use of active learning techniques\nto design more efficient trials, addressing issues such as whom to recruit, at\nwhat point in the trial, and which treatment to assign, throughout the duration\nof the trial. We propose a minimax bandit model with two different optimization\ncriteria, and discuss the computational challenges and issues pertaining to\nthis approach. We evaluate our active learning policies using both simulated\ndata, and data modeled after a clinical trial for treating depressed\nindividuals, and contrast our methods with other plausible active learning\npolicies.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Deng", "Kun", ""], ["Pineau", "Joelle", ""], ["Murphy", "Susan A.", ""]]}, {"id": "1202.3716", "submitter": "Narayanan U. Edakunni", "authors": "Narayanan U. Edakunni, Gary Brown, Tim Kovacs", "title": "Boosting as a Product of Experts", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-187-194", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we derive a novel probabilistic model of boosting as a Product\nof Experts. We re-derive the boosting algorithm as a greedy incremental model\nselection procedure which ensures that addition of new experts to the ensemble\ndoes not decrease the likelihood of the data. These learning rules lead to a\ngeneric boosting algorithm - POE- Boost which turns out to be similar to the\nAdaBoost algorithm under certain assumptions on the expert probabilities. The\npaper then extends the POEBoost algorithm to POEBoost.CS which handles\nhypothesis that produce probabilistic predictions. This new algorithm is shown\nto have better generalization performance compared to other state of the art\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Edakunni", "Narayanan U.", ""], ["Brown", "Gary", ""], ["Kovacs", "Tim", ""]]}, {"id": "1202.3717", "submitter": "Mahdi MIlani Fard", "authors": "Mahdi MIlani Fard, Joelle Pineau, Csaba Szepesvari", "title": "PAC-Bayesian Policy Evaluation for Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-195-202", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian priors offer a compact yet general means of incorporating domain\nknowledge into many learning tasks. The correctness of the Bayesian analysis\nand inference, however, largely depends on accuracy and correctness of these\npriors. PAC-Bayesian methods overcome this problem by providing bounds that\nhold regardless of the correctness of the prior distribution. This paper\nintroduces the first PAC-Bayesian bound for the batch reinforcement learning\nproblem with function approximation. We show how this bound can be used to\nperform model-selection in a transfer learning scenario. Our empirical results\nconfirm that PAC-Bayesian policy evaluation is able to leverage prior\ndistributions when they are informative and, unlike standard Bayesian RL\napproaches, ignore them when they are misleading.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Fard", "Mahdi MIlani", ""], ["Pineau", "Joelle", ""], ["Szepesvari", "Csaba", ""]]}, {"id": "1202.3722", "submitter": "Inmar Givoni", "authors": "Inmar Givoni, Clement Chung, Brendan J. Frey", "title": "Hierarchical Affinity Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-238-246", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affinity propagation is an exemplar-based clustering algorithm that finds a\nset of data-points that best exemplify the data, and associates each datapoint\nwith one exemplar. We extend affinity propagation in a principled way to solve\nthe hierarchical clustering problem, which arises in a variety of domains\nincluding biology, sensor networks and decision making in operational research.\nWe derive an inference algorithm that operates by propagating information up\nand down the hierarchy, and is efficient despite the high-order potentials\nrequired for the graphical model formulation. We demonstrate that our method\noutperforms greedy techniques that cluster one layer at a time. We show that on\nan artificial dataset designed to mimic the HIV-strain mutation dynamics, our\nmethod outperforms related methods. For real HIV sequences, where the ground\ntruth is not available, we show our method achieves better results, in terms of\nthe underlying objective function, and show the results correspond meaningfully\nto geographical location and strain subtypes. Finally we report results on\nusing the method for the analysis of mass spectra, showing it performs\nfavorably compared to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Givoni", "Inmar", ""], ["Chung", "Clement", ""], ["Frey", "Brendan J.", ""]]}, {"id": "1202.3725", "submitter": "Quanquan Gu", "authors": "Quanquan Gu, Zhenhui Li, Jiawei Han", "title": "Generalized Fisher Score for Feature Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-266-273", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fisher score is one of the most widely used supervised feature selection\nmethods. However, it selects each feature independently according to their\nscores under the Fisher criterion, which leads to a suboptimal subset of\nfeatures. In this paper, we present a generalized Fisher score to jointly\nselect features. It aims at finding an subset of features, which maximize the\nlower bound of traditional Fisher score. The resulting feature selection\nproblem is a mixed integer programming, which can be reformulated as a\nquadratically constrained linear programming (QCLP). It is solved by cutting\nplane algorithm, in each iteration of which a multiple kernel learning problem\nis solved alternatively by multivariate ridge regression and projected gradient\ndescent. Experiments on benchmark data sets indicate that the proposed method\noutperforms Fisher score as well as many other state-of-the-art feature\nselection methods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Gu", "Quanquan", ""], ["Li", "Zhenhui", ""], ["Han", "Jiawei", ""]]}, {"id": "1202.3726", "submitter": "Andrew Guillory", "authors": "Andrew Guillory, Jeff A. Bilmes", "title": "Active Semi-Supervised Learning using Submodular Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-274-282", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider active, semi-supervised learning in an offline transductive\nsetting. We show that a previously proposed error bound for active learning on\nundirected weighted graphs can be generalized by replacing graph cut with an\narbitrary symmetric submodular function. Arbitrary non-symmetric submodular\nfunctions can be used via symmetrization. Different choices of submodular\nfunctions give different versions of the error bound that are appropriate for\ndifferent kinds of problems. Moreover, the bound is deterministic and holds for\nadversarially chosen labels. We show exactly minimizing this error bound is\nNP-complete. However, we also introduce for any submodular function an\nassociated active semi-supervised learning method that approximately minimizes\nthe corresponding error bound. We show that the error bound is tight in the\nsense that there is no other bound of the same form which is better. Our\ntheoretical results are supported by experiments on real data.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Guillory", "Andrew", ""], ["Bilmes", "Jeff A.", ""]]}, {"id": "1202.3727", "submitter": "Michael Gutmann", "authors": "Michael Gutmann, Jun-ichiro Hirayama", "title": "Bregman divergence as general framework to estimate unnormalized\n  statistical models", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-283-290", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the Bregman divergence provides a rich framework to estimate\nunnormalized statistical models for continuous or discrete random variables,\nthat is, models which do not integrate or sum to one, respectively. We prove\nthat recent estimation methods such as noise-contrastive estimation, ratio\nmatching, and score matching belong to the proposed framework, and explain\ntheir interconnection based on supervised learning. Further, we discuss the\nrole of boosting in unsupervised learning.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Gutmann", "Michael", ""], ["Hirayama", "Jun-ichiro", ""]]}, {"id": "1202.3730", "submitter": "Jouni Hartikainen", "authors": "Jouni Hartikainen, Simo Sarkka", "title": "Sequential Inference for Latent Force Models", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-311-318", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent force models (LFMs) are hybrid models combining mechanistic principles\nwith non-parametric components. In this article, we shall show how LFMs can be\nequivalently formulated and solved using the state variable approach. We shall\nalso show how the Gaussian process prior used in LFMs can be equivalently\nformulated as a linear statespace model driven by a white noise process and how\ninference on the resulting model can be efficiently implemented using Kalman\nfilter and smoother. Then we shall show how the recently proposed switching LFM\ncan be reformulated using the state variable approach, and how we can construct\na probabilistic model for the switches by formulating a similar switching LFM\nas a switching linear dynamic system (SLDS). We illustrate the performance of\nthe proposed methodology in simulated scenarios and apply it to inferring the\nswitching points in GPS data collected from car movement data in urban\nenvironment.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Hartikainen", "Jouni", ""], ["Sarkka", "Simo", ""]]}, {"id": "1202.3731", "submitter": "Uri Heinemann", "authors": "Uri Heinemann, Amir Globerson", "title": "What Cannot be Learned with Bethe Approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-319-326", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of learning the parameters in graphical models when\ninference is intractable. A common strategy in this case is to replace the\npartition function with its Bethe approximation. We show that there exists a\nregime of empirical marginals where such Bethe learning will fail. By failure\nwe mean that the empirical marginals cannot be recovered from the approximated\nmaximum likelihood parameters (i.e., moment matching is not achieved). We\nprovide several conditions on empirical marginals that yield outer and inner\nbounds on the set of Bethe learnable marginals. An interesting implication of\nour results is that there exists a large class of marginals that cannot be\nobtained as stable fixed points of belief propagation. Taken together our\nresults provide a novel approach to analyzing learning with Bethe\napproximations and highlight when it can be expected to work or fail.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Heinemann", "Uri", ""], ["Globerson", "Amir", ""]]}, {"id": "1202.3732", "submitter": "Hoifung Poon", "authors": "Hoifung Poon, Pedro Domingos", "title": "Sum-Product Networks: A New Deep Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-337-346", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key limiting factor in graphical model inference and learning is the\ncomplexity of the partition function. We thus ask the question: what are\ngeneral conditions under which the partition function is tractable? The answer\nleads to a new kind of deep architecture, which we call sum-product networks\n(SPNs). SPNs are directed acyclic graphs with variables as leaves, sums and\nproducts as internal nodes, and weighted edges. We show that if an SPN is\ncomplete and consistent it represents the partition function and all marginals\nof some graphical model, and give semantics to its nodes. Essentially all\ntractable graphical models can be cast as SPNs, but SPNs are also strictly more\ngeneral. We then propose learning algorithms for SPNs, based on backpropagation\nand EM. Experiments show that inference and learning with SPNs can be both\nfaster and more accurate than with standard deep networks. For example, SPNs\nperform image completion better than state-of-the-art deep networks for this\ntask. SPNs also have intriguing potential connections to the architecture of\nthe cortex.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Poon", "Hoifung", ""], ["Domingos", "Pedro", ""]]}, {"id": "1202.3733", "submitter": "Jean Honorio", "authors": "Jean Honorio", "title": "Lipschitz Parametrization of Probabilistic Graphical Models", "comments": null, "journal-ref": "Uncertainty in Artificial Intelligence (UAI), 2011", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the log-likelihood of several probabilistic graphical models is\nLipschitz continuous with respect to the lp-norm of the parameters. We discuss\nseveral implications of Lipschitz parametrization. We present an upper bound of\nthe Kullback-Leibler divergence that allows understanding methods that penalize\nthe lp-norm of differences of parameters as the minimization of that upper\nbound. The expected log-likelihood is lower bounded by the negative lp-norm,\nwhich allows understanding the generalization ability of probabilistic models.\nThe exponential of the negative lp-norm is involved in the lower bound of the\nBayes error rate, which shows that it is reasonable to use parameters as\nfeatures in algorithms that rely on metric spaces (e.g. classification,\ndimensionality reduction, clustering). Our results do not rely on specific\nalgorithms for learning the structure or parameters. We show preliminary\nresults for activity recognition and temporal segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Honorio", "Jean", ""]]}, {"id": "1202.3734", "submitter": "Jonathan Huang", "authors": "Jonathan Huang, Ashish Kapoor, Carlos E. Guestrin", "title": "Efficient Probabilistic Inference with Partial Ranking Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-355-362", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributions over rankings are used to model data in various settings such\nas preference analysis and political elections. The factorial size of the space\nof rankings, however, typically forces one to make structural assumptions, such\nas smoothness, sparsity, or probabilistic independence about these underlying\ndistributions. We approach the modeling problem from the computational\nprinciple that one should make structural assumptions which allow for efficient\ncalculation of typical probabilistic queries. For ranking models, \"typical\"\nqueries predominantly take the form of partial ranking queries (e.g., given a\nuser's top-k favorite movies, what are his preferences over remaining movies?).\nIn this paper, we argue that riffled independence factorizations proposed in\nrecent literature [7, 8] are a natural structural assumption for ranking\ndistributions, allowing for particularly efficient processing of partial\nranking queries.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Huang", "Jonathan", ""], ["Kapoor", "Ashish", ""], ["Guestrin", "Carlos E.", ""]]}, {"id": "1202.3735", "submitter": "Antti Hyttinen", "authors": "Antti Hyttinen, Frederick Eberhardt, Patrik O. Hoyer", "title": "Noisy-OR Models with Latent Confounding", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-363-372", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of experiments in which varying subsets of observed variables are\nsubject to intervention, we consider the problem of identifiability of causal\nmodels exhibiting latent confounding. While identifiability is trivial when\neach experiment intervenes on a large number of variables, the situation is\nmore complicated when only one or a few variables are subject to intervention\nper experiment. For linear causal models with latent variables Hyttinen et al.\n(2010) gave precise conditions for when such data are sufficient to identify\nthe full model. While their result cannot be extended to discrete-valued\nvariables with arbitrary cause-effect relationships, we show that a similar\nresult can be obtained for the class of causal models whose conditional\nprobability distributions are restricted to a `noisy-OR' parameterization. We\nfurther show that identification is preserved under an extension of the model\nthat allows for negative influences, and present learning algorithms that we\ntest for accuracy, scalability and robustness.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Hyttinen", "Antti", ""], ["Eberhardt", "Frederick", ""], ["Hoyer", "Patrik O.", ""]]}, {"id": "1202.3736", "submitter": "Takanori Inazumi", "authors": "Takanori Inazumi, Takashi Washio, Shohei Shimizu, Joe Suzuki, Akihiro\n  Yamamoto, Yoshinobu Kawahara", "title": "Discovering causal structures in binary exclusive-or skew acyclic models", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-373-382", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering causal relations among observed variables in a given data set is\na main topic in studies of statistics and artificial intelligence. Recently,\nsome techniques to discover an identifiable causal structure have been explored\nbased on non-Gaussianity of the observed data distribution. However, most of\nthese are limited to continuous data. In this paper, we present a novel causal\nmodel for binary data and propose a new approach to derive an identifiable\ncausal structure governing the data based on skew Bernoulli distributions of\nexternal noise. Experimental evaluation shows excellent performance for both\nartificial and real world data sets.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Inazumi", "Takanori", ""], ["Washio", "Takashi", ""], ["Shimizu", "Shohei", ""], ["Suzuki", "Joe", ""], ["Yamamoto", "Akihiro", ""], ["Kawahara", "Yoshinobu", ""]]}, {"id": "1202.3737", "submitter": "Dominik Janzing", "authors": "Dominik Janzing, Eleni Sgouritsa, Oliver Stegle, Jonas Peters,\n  Bernhard Schoelkopf", "title": "Detecting low-complexity unobserved causes", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-383-391", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method that infers whether statistical dependences between two\nobserved variables X and Y are due to a \"direct\" causal link or only due to a\nconnecting causal path that contains an unobserved variable of low complexity,\ne.g., a binary variable. This problem is motivated by statistical genetics.\nGiven a genetic marker that is correlated with a phenotype of interest, we want\nto detect whether this marker is causal or it only correlates with a causal\none. Our method is based on the analysis of the location of the conditional\ndistributions P(Y|x) in the simplex of all distributions of Y. We report\nencouraging results on semi-empirical data.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Janzing", "Dominik", ""], ["Sgouritsa", "Eleni", ""], ["Stegle", "Oliver", ""], ["Peters", "Jonas", ""], ["Schoelkopf", "Bernhard", ""]]}, {"id": "1202.3738", "submitter": "Alex Kulesza", "authors": "Alex Kulesza, Ben Taskar", "title": "Learning Determinantal Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-419-427", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal point processes (DPPs), which arise in random matrix theory and\nquantum physics, are natural models for subset selection problems where\ndiversity is preferred. Among many remarkable properties, DPPs offer tractable\nalgorithms for exact inference, including computing marginal probabilities and\nsampling; however, an important open question has been how to learn a DPP from\nlabeled training data. In this paper we propose a natural feature-based\nparameterization of conditional DPPs, and show how it leads to a convex and\nefficient learning formulation. We analyze the relationship between our model\nand binary Markov random fields with repulsive potentials, which are\nqualitatively similar but computationally intractable. Finally, we apply our\napproach to the task of extractive summarization, where the goal is to choose a\nsmall subset of sentences conveying the most important information from a set\nof documents. In this task there is a fundamental tradeoff between sentences\nthat are highly relevant to the collection as a whole, and sentences that are\ndiverse and not repetitive. Our parameterization allows us to naturally balance\nthese two characteristics. We evaluate our system on data from the DUC 2003/04\nmulti-document summarization task, achieving state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Kulesza", "Alex", ""], ["Taskar", "Ben", ""]]}, {"id": "1202.3742", "submitter": "Qiang Liu", "authors": "Qiang Liu, Alexander T. Ihler", "title": "Variational Algorithms for Marginal MAP", "comments": "conference version. full journal version is at arXiv:1302.6584", "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-453-462", "categories": "cs.LG cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marginal MAP problems are notoriously difficult tasks for graphical models.\nWe derive a general variational framework for solving marginal MAP problems, in\nwhich we apply analogues of the Bethe, tree-reweighted, and mean field\napproximations. We then derive a \"mixed\" message passing algorithm and a\nconvergent alternative using CCCP to solve the BP-type approximations.\nTheoretically, we give conditions under which the decoded solution is a global\nor local optimum, and obtain novel upper bounds on solutions. Experimentally we\ndemonstrate that our algorithms outperform related approaches. We also show\nthat EM and variational EM comprise a special case of our framework.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2013-02-28", "authors_parsed": [["Liu", "Qiang", ""], ["Ihler", "Alexander T.", ""]]}, {"id": "1202.3746", "submitter": "Benjamin Marlin", "authors": "Benjamin Marlin, Nando de Freitas", "title": "Asymptotic Efficiency of Deterministic Estimators for Discrete\n  Energy-Based Models: Ratio Matching and Pseudolikelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-497-505", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard maximum likelihood estimation cannot be applied to discrete\nenergy-based models in the general case because the computation of exact model\nprobabilities is intractable. Recent research has seen the proposal of several\nnew estimators designed specifically to overcome this intractability, but\nvirtually nothing is known about their theoretical properties. In this paper,\nwe present a generalized estimator that unifies many of the classical and\nrecently proposed estimators. We use results from the standard asymptotic\ntheory for M-estimators to derive a generic expression for the asymptotic\ncovariance matrix of our generalized estimator. We apply these results to study\nthe relative statistical efficiency of classical pseudolikelihood and the\nrecently-proposed ratio matching estimator.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Marlin", "Benjamin", ""], ["de Freitas", "Nando", ""]]}, {"id": "1202.3747", "submitter": "David Mimno", "authors": "David Mimno", "title": "Reconstructing Pompeian Households", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-506-513", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A database of objects discovered in houses in the Roman city of Pompeii\nprovides a unique view of ordinary life in an ancient city. Experts have used\nthis collection to study the structure of Roman households, exploring the\ndistribution and variability of tasks in architectural spaces, but such\napproaches are necessarily affected by modern cultural assumptions. In this\nstudy we present a data-driven approach to household archeology, treating it as\nan unsupervised labeling problem. This approach scales to large data sets and\nprovides a more objective complement to human interpretation.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Mimno", "David", ""]]}, {"id": "1202.3748", "submitter": "Volodymyr Mnih", "authors": "Volodymyr Mnih, Hugo Larochelle, Geoffrey E. Hinton", "title": "Conditional Restricted Boltzmann Machines for Structured Output\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-514-522", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional Restricted Boltzmann Machines (CRBMs) are rich probabilistic\nmodels that have recently been applied to a wide range of problems, including\ncollaborative filtering, classification, and modeling motion capture data.\nWhile much progress has been made in training non-conditional RBMs, these\nalgorithms are not applicable to conditional models and there has been almost\nno work on training and generating predictions from conditional RBMs for\nstructured output problems. We first argue that standard Contrastive\nDivergence-based learning may not be suitable for training CRBMs. We then\nidentify two distinct types of structured output prediction problems and\npropose an improved learning algorithm for each. The first problem type is one\nwhere the output space has arbitrary structure but the set of likely output\nconfigurations is relatively small, such as in multi-label classification. The\nsecond problem is one where the output space is arbitrarily structured but\nwhere the output space variability is much greater, such as in image denoising\nor pixel labeling. We show that the new learning algorithms can work much\nbetter than Contrastive Divergence on both types of problems.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Mnih", "Volodymyr", ""], ["Larochelle", "Hugo", ""], ["Hinton", "Geoffrey E.", ""]]}, {"id": "1202.3750", "submitter": "Ananda Narayanan B", "authors": "Ananda Narayanan B, Balaraman Ravindran", "title": "Fractional Moments on Bandit Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-531-538", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning addresses the dilemma between exploration to find\nprofitable actions and exploitation to act according to the best observations\nalready made. Bandit problems are one such class of problems in stateless\nenvironments that represent this explore/exploit situation. We propose a\nlearning algorithm for bandit problems based on fractional expectation of\nrewards acquired. The algorithm is theoretically shown to converge on an\neta-optimal arm and achieve O(n) sample complexity. Experimental results show\nthe algorithm incurs substantially lower regrets than parameter-optimized\neta-greedy and SoftMax approaches and other low sample complexity\nstate-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["B", "Ananda Narayanan", ""], ["Ravindran", "Balaraman", ""]]}, {"id": "1202.3752", "submitter": "Nebojsa Jojic", "authors": "Nebojsa Jojic, Alessandro Perina", "title": "Multidimensional counting grids: Inferring word order from disordered\n  bags of words", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-547-556", "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models of bags of words typically assume topic mixing so that the words in a\nsingle bag come from a limited number of topics. We show here that many sets of\nbag of words exhibit a very different pattern of variation than the patterns\nthat are efficiently captured by topic mixing. In many cases, from one bag of\nwords to the next, the words disappear and new ones appear as if the theme\nslowly and smoothly shifted across documents (providing that the documents are\nsomehow ordered). Examples of latent structure that describe such ordering are\neasily imagined. For example, the advancement of the date of the news stories\nis reflected in a smooth change over the theme of the day as certain evolving\nnews stories fall out of favor and new events create new stories. Overlaps\namong the stories of consecutive days can be modeled by using windows over\nlinearly arranged tight distributions over words. We show here that such\nstrategy can be extended to multiple dimensions and cases where the ordering of\ndata is not readily obvious. We demonstrate that this way of modeling\ncovariation in word occurrences outperforms standard topic models in\nclassification and prediction tasks in applications in biology, text modeling\nand computer vision.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Jojic", "Nebojsa", ""], ["Perina", "Alessandro", ""]]}, {"id": "1202.3753", "submitter": "Teppo Niinimaki", "authors": "Teppo Niinimaki, Pekka Parviainen, Mikko Koivisto", "title": "Partial Order MCMC for Structure Discovery in Bayesian Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-557-564", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new Markov chain Monte Carlo method for estimating posterior\nprobabilities of structural features in Bayesian networks. The method draws\nsamples from the posterior distribution of partial orders on the nodes; for\neach sampled partial order, the conditional probabilities of interest are\ncomputed exactly. We give both analytical and empirical results that suggest\nthe superiority of the new method compared to previous methods, which sample\neither directed acyclic graphs or linear orders on the nodes.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Niinimaki", "Teppo", ""], ["Parviainen", "Pekka", ""], ["Koivisto", "Mikko", ""]]}, {"id": "1202.3757", "submitter": "Jonas Peters", "authors": "Jonas Peters, Joris Mooij, Dominik Janzing, Bernhard Schoelkopf", "title": "Identifiability of Causal Graphs using Functional Models", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-589-598", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the following question: Under what assumptions on the\ndata generating process can one infer the causal graph from the joint\ndistribution? The approach taken by conditional independence-based causal\ndiscovery methods is based on two assumptions: the Markov condition and\nfaithfulness. It has been shown that under these assumptions the causal graph\ncan be identified up to Markov equivalence (some arrows remain undirected)\nusing methods like the PC algorithm. In this work we propose an alternative by\ndefining Identifiable Functional Model Classes (IFMOCs). As our main theorem we\nprove that if the data generating process belongs to an IFMOC, one can identify\nthe complete causal graph. To the best of our knowledge this is the first\nidentifiability result of this kind that is not limited to linear functional\nrelationships. We discuss how the IFMOC assumption and the Markov and\nfaithfulness assumptions relate to each other and explain why we believe that\nthe IFMOC assumption can be tested more easily on given data. We further\nprovide a practical algorithm that recovers the causal graph from finitely many\ndata; experiments on simulated data support the theoretical findings.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Peters", "Jonas", ""], ["Mooij", "Joris", ""], ["Janzing", "Dominik", ""], ["Schoelkopf", "Bernhard", ""]]}, {"id": "1202.3758", "submitter": "Barnabas Poczos", "authors": "Barnabas Poczos, Liang Xiong, Jeff Schneider", "title": "Nonparametric Divergence Estimation with Applications to Machine\n  Learning on Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-599-608", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-dimensional embedding, manifold learning, clustering, classification, and\nanomaly detection are among the most important problems in machine learning.\nThe existing methods usually consider the case when each instance has a fixed,\nfinite-dimensional feature representation. Here we consider a different\nsetting. We assume that each instance corresponds to a continuous probability\ndistribution. These distributions are unknown, but we are given some i.i.d.\nsamples from each distribution. Our goal is to estimate the distances between\nthese distributions and use these distances to perform low-dimensional\nembedding, clustering/classification, or anomaly detection for the\ndistributions. We present estimation algorithms, describe how to apply them for\nmachine learning tasks on distributions, and show empirical results on\nsynthetic data, real word images, and astronomical data sets.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Poczos", "Barnabas", ""], ["Xiong", "Liang", ""], ["Schneider", "Jeff", ""]]}, {"id": "1202.3760", "submitter": "Vinayak Rao", "authors": "Vinayak Rao, Yee Whye Teh", "title": "Fast MCMC sampling for Markov jump processes and continuous time\n  Bayesian networks", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-619-626", "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov jump processes and continuous time Bayesian networks are important\nclasses of continuous time dynamical systems. In this paper, we tackle the\nproblem of inferring unobserved paths in these models by introducing a fast\nauxiliary variable Gibbs sampler. Our approach is based on the idea of\nuniformization, and sets up a Markov chain over paths by sampling a finite set\nof virtual jump times and then running a standard hidden Markov model forward\nfiltering-backward sampling algorithm over states at the set of extant and\nvirtual jump times. We demonstrate significant computational benefits over a\nstate-of-the-art Gibbs sampler on a number of continuous time Bayesian\nnetworks.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Rao", "Vinayak", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1202.3761", "submitter": "Nima Reyhani", "authors": "Nima Reyhani, Hideitsu Hino, Ricardo Vigario", "title": "New Probabilistic Bounds on Eigenvalues and Eigenvectors of Random\n  Kernel Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-627-634", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel methods are successful approaches for different machine learning\nproblems. This success is mainly rooted in using feature maps and kernel\nmatrices. Some methods rely on the eigenvalues/eigenvectors of the kernel\nmatrix, while for other methods the spectral information can be used to\nestimate the excess risk. An important question remains on how close the sample\neigenvalues/eigenvectors are to the population values. In this paper, we\nimprove earlier results on concentration bounds for eigenvalues of general\nkernel matrices. For distance and inner product kernel functions, e.g. radial\nbasis functions, we provide new concentration bounds, which are characterized\nby the eigenvalues of the sample covariance matrix. Meanwhile, the obstacles\nfor sharper bounds are accounted for and partially addressed. As a case study,\nwe derive a concentration inequality for sample kernel target-alignment.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Reyhani", "Nima", ""], ["Hino", "Hideitsu", ""], ["Vigario", "Ricardo", ""]]}, {"id": "1202.3763", "submitter": "Ilya Shpitser", "authors": "Ilya Shpitser, Thomas S. Richardson, James M. Robins", "title": "An Efficient Algorithm for Computing Interventional Distributions in\n  Latent Variable Causal Models", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-661-670", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic inference in graphical models is the task of computing marginal\nand conditional densities of interest from a factorized representation of a\njoint probability distribution. Inference algorithms such as variable\nelimination and belief propagation take advantage of constraints embedded in\nthis factorization to compute such densities efficiently. In this paper, we\npropose an algorithm which computes interventional distributions in latent\nvariable causal models represented by acyclic directed mixed graphs(ADMGs). To\ncompute these distributions efficiently, we take advantage of a recursive\nfactorization which generalizes the usual Markov factorization for DAGs and the\nmore recent factorization for ADMGs. Our algorithm can be viewed as a\ngeneralization of variable elimination to the mixed graph case. We show our\nalgorithm is exponential in the mixed graph generalization of treewidth.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Shpitser", "Ilya", ""], ["Richardson", "Thomas S.", ""], ["Robins", "James M.", ""]]}, {"id": "1202.3765", "submitter": "Inma Tur", "authors": "Inma Tur, Robert Castelo", "title": "Learning mixed graphical models from data with p larger than n", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-689-697", "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structure learning of Gaussian graphical models is an extensively studied\nproblem in the classical multivariate setting where the sample size n is larger\nthan the number of random variables p, as well as in the more challenging\nsetting when p>>n. However, analogous approaches for learning the structure of\ngraphical models with mixed discrete and continuous variables when p>>n remain\nlargely unexplored. Here we describe a statistical learning procedure for this\nproblem based on limited-order correlations and assess its performance with\nsynthetic and real data.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Tur", "Inma", ""], ["Castelo", "Robert", ""]]}, {"id": "1202.3766", "submitter": "Maomi Ueno", "authors": "Maomi Ueno", "title": "Robust learning Bayesian networks for prior belief", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-698-707", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent reports have described that learning Bayesian networks are highly\nsensitive to the chosen equivalent sample size (ESS) in the Bayesian Dirichlet\nequivalence uniform (BDeu). This sensitivity often engenders some unstable or\nundesirable results. This paper describes some asymptotic analyses of BDeu to\nexplain the reasons for the sensitivity and its effects. Furthermore, this\npaper presents a proposal for a robust learning score for ESS by eliminating\nthe sensitive factors from the approximation of log-BDeu.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Ueno", "Maomi", ""]]}, {"id": "1202.3769", "submitter": "Feng Yan", "authors": "Feng Yan, Zenglin Xu, Yuan (Alan) Qi", "title": "Sparse matrix-variate Gaussian process blockmodels for network modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-745-752", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We face network data from various sources, such as protein interactions and\nonline social networks. A critical problem is to model network interactions and\nidentify latent groups of network nodes. This problem is challenging due to\nmany reasons. For example, the network nodes are interdependent instead of\nindependent of each other, and the data are known to be very noisy (e.g.,\nmissing edges). To address these challenges, we propose a new relational model\nfor network data, Sparse Matrix-variate Gaussian process Blockmodel (SMGB). Our\nmodel generalizes popular bilinear generative models and captures nonlinear\nnetwork interactions using a matrix-variate Gaussian process with latent\nmembership variables. We also assign sparse prior distributions on the latent\nmembership variables to learn sparse group assignments for individual network\nnodes. To estimate the latent variables efficiently from data, we develop an\nefficient variational expectation maximization method. We compared our\napproaches with several state-of-the-art network models on both synthetic and\nreal-world network datasets. Experimental results demonstrate SMGBs outperform\nthe alternative approaches in terms of discovering latent classes or predicting\nunknown interactions.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Yan", "Feng", "", "Alan"], ["Xu", "Zenglin", "", "Alan"], ["Yuan", "", "", "Alan"], ["Qi", "", ""]]}, {"id": "1202.3770", "submitter": "Jian-Bo Yang", "authors": "Jian-Bo Yang, Ivor W. Tsang", "title": "Hierarchical Maximum Margin Learning for Multi-Class Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-753-760", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to myriads of classes, designing accurate and efficient classifiers\nbecomes very challenging for multi-class classification. Recent research has\nshown that class structure learning can greatly facilitate multi-class\nlearning. In this paper, we propose a novel method to learn the class structure\nfor multi-class classification problems. The class structure is assumed to be a\nbinary hierarchical tree. To learn such a tree, we propose a maximum separating\nmargin method to determine the child nodes of any internal node. The proposed\nmethod ensures that two classgroups represented by any two sibling nodes are\nmost separable. In the experiments, we evaluate the accuracy and efficiency of\nthe proposed method over other multi-class classification methods on real world\nlarge-scale problems. The results show that the proposed method outperforms\nbenchmark methods in terms of accuracy for most datasets and performs\ncomparably with other class structure learning methods in terms of efficiency\nfor all datasets.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Yang", "Jian-Bo", ""], ["Tsang", "Ivor W.", ""]]}, {"id": "1202.3771", "submitter": "Julian Yarkony", "authors": "Julian Yarkony, Ragib Morshed, Alexander T. Ihler, Charless C. Fowlkes", "title": "Tightening MRF Relaxations with Planar Subproblems", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-770-777", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new technique for computing lower-bounds on the minimum energy\nconfiguration of a planar Markov Random Field (MRF). Our method successively\nadds large numbers of constraints and enforces consistency over binary\nprojections of the original problem state space. These constraints are\nrepresented in terms of subproblems in a dual-decomposition framework that is\noptimized using subgradient techniques. The complete set of constraints we\nconsider enforces cycle consistency over the original graph. In practice we\nfind that the method converges quickly on most problems with the addition of a\nfew subproblems and outperforms existing methods for some interesting classes\nof hard potentials.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Yarkony", "Julian", ""], ["Morshed", "Ragib", ""], ["Ihler", "Alexander T.", ""], ["Fowlkes", "Charless C.", ""]]}, {"id": "1202.3772", "submitter": "Yao-Liang Yu", "authors": "Yao-Liang Yu, Dale Schuurmans", "title": "Rank/Norm Regularization with Closed-Form Solutions: Application to\n  Subspace Clustering", "comments": "11 pages, 1 figure, appeared in UAI 2011. One footnote corrected and\n  appendix added", "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-778-785", "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When data is sampled from an unknown subspace, principal component analysis\n(PCA) provides an effective way to estimate the subspace and hence reduce the\ndimension of the data. At the heart of PCA is the Eckart-Young-Mirsky theorem,\nwhich characterizes the best rank k approximation of a matrix. In this paper,\nwe prove a generalization of the Eckart-Young-Mirsky theorem under all\nunitarily invariant norms. Using this result, we obtain closed-form solutions\nfor a set of rank/norm regularized problems, and derive closed-form solutions\nfor a general class of subspace clustering problems (where data is modelled by\nunions of unknown subspaces). From these results we obtain new theoretical\ninsights and promising experimental results.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2012 21:00:59 GMT"}], "update_date": "2012-10-11", "authors_parsed": [["Yu", "Yao-Liang", ""], ["Schuurmans", "Dale", ""]]}, {"id": "1202.3774", "submitter": "Chao Zhang", "authors": "Chao Zhang, Dacheng Tao", "title": "Risk Bounds for Infinitely Divisible Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-796-803", "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the risk bounds for samples independently drawn from\nan infinitely divisible (ID) distribution. In particular, based on a martingale\nmethod, we develop two deviation inequalities for a sequence of random\nvariables of an ID distribution with zero Gaussian component. By applying the\ndeviation inequalities, we obtain the risk bounds based on the covering number\nfor the ID distribution. Finally, we analyze the asymptotic convergence of the\nrisk bound derived from one of the two deviation inequalities and show that the\nconvergence rate of the bound is faster than the result for the generic i.i.d.\nempirical process (Mendelson, 2003).\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Zhang", "Chao", ""], ["Tao", "Dacheng", ""]]}, {"id": "1202.3775", "submitter": "Kun Zhang", "authors": "Kun Zhang, Jonas Peters, Dominik Janzing, Bernhard Schoelkopf", "title": "Kernel-based Conditional Independence Test and Application in Causal\n  Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-804-813", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional independence testing is an important problem, especially in\nBayesian network learning and causal discovery. Due to the curse of\ndimensionality, testing for conditional independence of continuous variables is\nparticularly challenging. We propose a Kernel-based Conditional Independence\ntest (KCI-test), by constructing an appropriate test statistic and deriving its\nasymptotic distribution under the null hypothesis of conditional independence.\nThe proposed method is computationally efficient and easy to implement.\nExperimental results show that it outperforms other methods, especially when\nthe conditioning set is large or the sample size is not very large, in which\ncase other methods encounter difficulties.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Zhang", "Kun", ""], ["Peters", "Jonas", ""], ["Janzing", "Dominik", ""], ["Schoelkopf", "Bernhard", ""]]}, {"id": "1202.3776", "submitter": "Xinhua Zhang", "authors": "Xinhua Zhang, Ankan Saha, S. V.N. Vishwanatan", "title": "Smoothing Multivariate Performance Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-814-821", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Support Vector Method for multivariate performance measures was recently\nintroduced by Joachims (2005). The underlying optimization problem is currently\nsolved using cutting plane methods such as SVM-Perf and BMRM. One can show that\nthese algorithms converge to an eta accurate solution in O(1/Lambda*e)\niterations, where lambda is the trade-off parameter between the regularizer and\nthe loss function. We present a smoothing strategy for multivariate performance\nscores, in particular precision/recall break-even point and ROCArea. When\ncombined with Nesterov's accelerated gradient algorithm our smoothing strategy\nyields an optimization algorithm which converges to an eta accurate solution in\nO(min{1/e,1/sqrt(lambda*e)}) iterations. Furthermore, the cost per iteration of\nour scheme is the same as that of SVM-Perf and BMRM. Empirical evaluation on a\nnumber of publicly available datasets shows that our method converges\nsignificantly faster than cutting plane methods without sacrificing\ngeneralization ability.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Zhang", "Xinhua", ""], ["Saha", "Ankan", ""], ["Vishwanatan", "S. V. N.", ""]]}, {"id": "1202.3778", "submitter": "Jun Zhu", "authors": "Jun Zhu, Eric P. Xing", "title": "Sparse Topical Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-831-838", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present sparse topical coding (STC), a non-probabilistic formulation of\ntopic models for discovering latent representations of large collections of\ndata. Unlike probabilistic topic models, STC relaxes the normalization\nconstraint of admixture proportions and the constraint of defining a normalized\nlikelihood function. Such relaxations make STC amenable to: 1) directly control\nthe sparsity of inferred representations by using sparsity-inducing\nregularizers; 2) be seamlessly integrated with a convex error function (e.g.,\nSVM hinge loss) for supervised learning; and 3) be efficiently learned with a\nsimply structured coordinate descent algorithm. Our results demonstrate the\nadvantages of STC and supervised MedSTC on identifying topical meanings of\nwords and improving classification accuracy and time efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Zhu", "Jun", ""], ["Xing", "Eric P.", ""]]}, {"id": "1202.3779", "submitter": "Jakob Zscheischler", "authors": "Jakob Zscheischler, Dominik Janzing, Kun Zhang", "title": "Testing whether linear equations are causal: A free probability theory\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-839-846", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method that infers whether linear relations between two\nhigh-dimensional variables X and Y are due to a causal influence from X to Y or\nfrom Y to X. The earlier proposed so-called Trace Method is extended to the\nregime where the dimension of the observed variables exceeds the sample size.\nBased on previous work, we postulate conditions that characterize a causal\nrelation between X and Y. Moreover, we describe a statistical test and argue\nthat both causal directions are typically rejected if there is a common cause.\nA full theoretical analysis is presented for the deterministic case but our\napproach seems to be valid for the noisy case, too, for which we additionally\npresent an approach based on a sparsity constraint. The discussed method yields\npromising results for both simulated and real world data.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Zscheischler", "Jakob", ""], ["Janzing", "Dominik", ""], ["Zhang", "Kun", ""]]}, {"id": "1202.3782", "submitter": "Kareem Amin", "authors": "Kareem Amin, Michael Kearns, Umar Syed", "title": "Graphical Models for Bandit Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-1-10", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a rich class of graphical models for multi-armed bandit problems\nthat permit both the state or context space and the action space to be very\nlarge, yet succinctly specify the payoffs for any context-action pair. Our main\nresult is an algorithm for such models whose regret is bounded by the number of\nparameters and whose running time depends only on the treewidth of the graph\nsubstructure induced by the action space.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Amin", "Kareem", ""], ["Kearns", "Michael", ""], ["Syed", "Umar", ""]]}, {"id": "1202.4044", "submitter": "Michael McCoy", "authors": "Gilad Lerman, Michael McCoy, Joel A. Tropp, and Teng Zhang", "title": "Robust computation of linear models by convex relaxation", "comments": "Formerly titled \"Robust computation of linear models, or How to find\n  a needle in a haystack\"", "journal-ref": "Foundations of Computational Mathematics, April 2015, Volume 15,\n  Issue 2, pp 363-410", "doi": "10.1007/s10208-014-9221-0", "report-no": null, "categories": "cs.IT math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a dataset of vector-valued observations that consists of noisy\ninliers, which are explained well by a low-dimensional subspace, along with\nsome number of outliers. This work describes a convex optimization problem,\ncalled REAPER, that can reliably fit a low-dimensional model to this type of\ndata. This approach parameterizes linear subspaces using orthogonal projectors,\nand it uses a relaxation of the set of orthogonal projectors to reach the\nconvex formulation. The paper provides an efficient algorithm for solving the\nREAPER problem, and it documents numerical experiments which confirm that\nREAPER can dependably find linear structure in synthetic and natural data. In\naddition, when the inliers lie near a low-dimensional subspace, there is a\nrigorous theory that describes when REAPER can approximate this subspace.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2012 00:47:22 GMT"}, {"version": "v2", "created": "Mon, 11 Aug 2014 19:19:28 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Lerman", "Gilad", ""], ["McCoy", "Michael", ""], ["Tropp", "Joel A.", ""], ["Zhang", "Teng", ""]]}, {"id": "1202.4050", "submitter": "Nishant Mehta", "authors": "Nishant A. Mehta and Alexander G. Gray", "title": "On the Sample Complexity of Predictive Sparse Coding", "comments": "Sparse Coding Stability Theorem from version 1 has been relaxed\n  considerably using a new notion of coding margin. Old Sparse Coding Stability\n  Theorem still in new version, now as Theorem 2. Presentation of all proofs\n  simplified/improved considerably. Paper reorganized. Empirical analysis\n  showing new coding margin is non-trivial on real datasets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of predictive sparse coding is to learn a representation of examples\nas sparse linear combinations of elements from a dictionary, such that a\nlearned hypothesis linear in the new representation performs well on a\npredictive task. Predictive sparse coding algorithms recently have demonstrated\nimpressive performance on a variety of supervised tasks, but their\ngeneralization properties have not been studied. We establish the first\ngeneralization error bounds for predictive sparse coding, covering two\nsettings: 1) the overcomplete setting, where the number of features k exceeds\nthe original dimensionality d; and 2) the high or infinite-dimensional setting,\nwhere only dimension-free bounds are useful. Both learning bounds intimately\ndepend on stability properties of the learned sparse encoder, as measured on\nthe training sample. Consequently, we first present a fundamental stability\nresult for the LASSO, a result characterizing the stability of the sparse codes\nwith respect to perturbations to the dictionary. In the overcomplete setting,\nwe present an estimation error bound that decays as \\tilde{O}(sqrt(d k/m)) with\nrespect to d and k. In the high or infinite-dimensional setting, we show a\ndimension-free bound that is \\tilde{O}(sqrt(k^2 s / m)) with respect to k and\ns, where s is an upper bound on the number of non-zeros in the sparse code for\nany training data point.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2012 02:28:49 GMT"}, {"version": "v2", "created": "Mon, 8 Oct 2012 00:07:13 GMT"}], "update_date": "2012-10-09", "authors_parsed": [["Mehta", "Nishant A.", ""], ["Gray", "Alexander G.", ""]]}, {"id": "1202.4184", "submitter": "Benjamin Recht", "authors": "Benjamin Recht and Christopher Re", "title": "Beneath the valley of the noncommutative arithmetic-geometric mean\n  inequality: conjectures, case-studies, and consequences", "comments": "25 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized algorithms that base iteration-level decisions on samples from\nsome pool are ubiquitous in machine learning and optimization. Examples include\nstochastic gradient descent and randomized coordinate descent. This paper makes\nprogress at theoretically evaluating the difference in performance between\nsampling with- and without-replacement in such algorithms. Focusing on least\nmeans squares optimization, we formulate a noncommutative arithmetic-geometric\nmean inequality that would prove that the expected convergence rate of\nwithout-replacement sampling is faster than that of with-replacement sampling.\nWe demonstrate that this inequality holds for many classes of random matrices\nand for some pathological examples as well. We provide a deterministic\nworst-case bound on the gap between the discrepancy between the two sampling\nmodels, and explore some of the impediments to proving this inequality in full\ngenerality. We detail the consequences of this inequality for stochastic\ngradient descent and the randomized Kaczmarz algorithm for solving linear\nsystems.\n", "versions": [{"version": "v1", "created": "Sun, 19 Feb 2012 20:58:56 GMT"}], "update_date": "2012-02-21", "authors_parsed": [["Recht", "Benjamin", ""], ["Re", "Christopher", ""]]}, {"id": "1202.4478", "submitter": "Elad Hazan", "authors": "Elad Hazan, Sham Kakade", "title": "(weak) Calibration is Computationally Hard", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the existence of a computationally efficient calibration\nalgorithm, with a low weak calibration rate, would imply the existence of an\nefficient algorithm for computing approximate Nash equilibria - thus implying\nthe unlikely conclusion that every problem in PPAD is solvable in polynomial\ntime.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2012 21:48:09 GMT"}], "update_date": "2012-02-23", "authors_parsed": [["Hazan", "Elad", ""], ["Kakade", "Sham", ""]]}, {"id": "1202.5070", "submitter": "Quentin Berthet", "authors": "Quentin Berthet, Philippe Rigollet", "title": "Optimal detection of sparse principal components in high dimension", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1127 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 4, 1780-1815", "doi": "10.1214/13-AOS1127", "report-no": "IMS-AOS-AOS1127", "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform a finite sample analysis of the detection levels for sparse\nprincipal components of a high-dimensional covariance matrix. Our minimax\noptimal test is based on a sparse eigenvalue statistic. Alas, computing this\ntest is known to be NP-complete in general, and we describe a computationally\nefficient alternative test using convex relaxations. Our relaxation is also\nproved to detect sparse principal components at near optimal detection levels,\nand it performs well on simulated datasets. Moreover, using polynomial time\nreductions from theoretical computer science, we bring significant evidence\nthat our results cannot be improved, thus revealing an inherent trade off\nbetween statistical and computational performance.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2012 00:55:31 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2012 14:19:04 GMT"}, {"version": "v3", "created": "Thu, 19 Sep 2013 07:09:05 GMT"}], "update_date": "2014-01-30", "authors_parsed": [["Berthet", "Quentin", ""], ["Rigollet", "Philippe", ""]]}, {"id": "1202.5130", "submitter": "Yair Goldberg", "authors": "Yair Goldberg and Michael R. Kosorok", "title": "Support Vector Regression for Right Censored Data", "comments": "In this version, we strengthened the theoretical results and\n  corrected a few mistakes", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a unified approach for classification and regression support\nvector machines for data subject to right censoring. We provide finite sample\nbounds on the generalization error of the algorithm, prove risk consistency for\na wide class of probability measures, and study the associated learning rates.\nWe apply the general methodology to estimation of the (truncated) mean, median,\nquantiles, and for classification problems. We present a simulation study that\ndemonstrates the performance of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2012 09:33:17 GMT"}, {"version": "v2", "created": "Sat, 12 Jan 2013 20:43:14 GMT"}], "update_date": "2013-01-15", "authors_parsed": [["Goldberg", "Yair", ""], ["Kosorok", "Michael R.", ""]]}, {"id": "1202.5514", "submitter": "Simplice Dossou-Gb\\'et\\'e", "authors": "Cheikh Ndour (1,2,3), Aliou Diop (1), Simplice Dossou-Gb\\'et\\'e (2)\n  ((1) Universit\\'e Gaston Berger, Saint-Louis, S\\'en\\'egal (2) Universit\\'e de\n  Pau et des Pays de l 'Adour, Pau, France (3) Universit\\'e de Bordeaux,\n  Bordeaux, France)", "title": "Classification approach based on association rules mining for unbalanced\n  data", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the binary classification task when the target class\nhas the lower probability of occurrence. In such situation, it is not possible\nto build a powerful classifier by using standard methods such as logistic\nregression, classification tree, discriminant analysis, etc. To overcome this\nshort-coming of these methods which yield classifiers with low sensibility, we\ntackled the classification problem here through an approach based on the\nassociation rules learning. This approach has the advantage of allowing the\nidentification of the patterns that are well correlated with the target class.\nAssociation rules learning is a well known method in the area of data-mining.\nIt is used when dealing with large database for unsupervised discovery of local\npatterns that expresses hidden relationships between input variables. In\nconsidering association rules from a supervised learning point of view, a\nrelevant set of weak classifiers is obtained from which one derives a\nclassifier that performs well.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2012 17:55:33 GMT"}, {"version": "v2", "created": "Tue, 24 Feb 2015 21:17:54 GMT"}], "update_date": "2015-02-26", "authors_parsed": [["Ndour", "Cheikh", ""], ["Diop", "Aliou", ""], ["Dossou-Gb\u00e9t\u00e9", "Simplice", ""]]}, {"id": "1202.5598", "submitter": "Ali Jalali", "authors": "Ali Jalali and Nathan Srebro", "title": "Clustering using Max-norm Constrained Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We suggest using the max-norm as a convex surrogate constraint for\nclustering. We show how this yields a better exact cluster recovery guarantee\nthan previously suggested nuclear-norm relaxation, and study the effectiveness\nof our method, and other related convex relaxations, compared to other\nclustering approaches.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2012 02:10:20 GMT"}, {"version": "v2", "created": "Sun, 18 Mar 2012 22:09:44 GMT"}, {"version": "v3", "created": "Wed, 11 Apr 2012 20:56:40 GMT"}, {"version": "v4", "created": "Fri, 13 Apr 2012 06:52:44 GMT"}], "update_date": "2012-04-16", "authors_parsed": [["Jalali", "Ali", ""], ["Srebro", "Nathan", ""]]}, {"id": "1202.5695", "submitter": "Hugo Larochelle", "authors": "George E. Dahl, Ryan P. Adams and Hugo Larochelle", "title": "Training Restricted Boltzmann Machines on Word Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The restricted Boltzmann machine (RBM) is a flexible tool for modeling\ncomplex data, however there have been significant computational difficulties in\nusing RBMs to model high-dimensional multinomial observations. In natural\nlanguage processing applications, words are naturally modeled by K-ary discrete\ndistributions, where K is determined by the vocabulary size and can easily be\nin the hundreds of thousands. The conventional approach to training RBMs on\nword observations is limited because it requires sampling the states of K-way\nsoftmax visible units during block Gibbs updates, an operation that takes time\nlinear in K. In this work, we address this issue by employing a more general\nclass of Markov chain Monte Carlo operators on the visible units, yielding\nupdates with computational complexity independent of K. We demonstrate the\nsuccess of our approach by training RBMs on hundreds of millions of word\nn-grams using larger vocabularies than previously feasible and using the\nlearned features to improve performance on chunking and sentiment\nclassification tasks, achieving state-of-the-art results on the latter.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2012 20:23:37 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2012 12:15:40 GMT"}], "update_date": "2012-07-06", "authors_parsed": [["Dahl", "George E.", ""], ["Adams", "Ryan P.", ""], ["Larochelle", "Hugo", ""]]}, {"id": "1202.5918", "submitter": "Matthew Urry Mr", "authors": "Matthew J. Urry and Peter Sollich", "title": "Replica theory for learning curves for Gaussian processes on random\n  graphs", "comments": null, "journal-ref": "M J Urry and P Sollich 2012 J. Phys. A: Math. Theor. 45 425005", "doi": "10.1088/1751-8113/45/42/425005", "report-no": null, "categories": "stat.ML cond-mat.dis-nn cond-mat.stat-mech", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical physics approaches can be used to derive accurate predictions for\nthe performance of inference methods learning from potentially noisy data, as\nquantified by the learning curve defined as the average error versus number of\ntraining examples. We analyse a challenging problem in the area of\nnon-parametric inference where an effectively infinite number of parameters has\nto be learned, specifically Gaussian process regression. When the inputs are\nvertices on a random graph and the outputs noisy function values, we show that\nreplica techniques can be used to obtain exact performance predictions in the\nlimit of large graphs. The covariance of the Gaussian process prior is defined\nby a random walk kernel, the discrete analogue of squared exponential kernels\non continuous spaces. Conventionally this kernel is normalised only globally,\nso that the prior variance can differ between vertices; as a more principled\nalternative we consider local normalisation, where the prior variance is\nuniform.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2012 13:08:33 GMT"}, {"version": "v2", "created": "Tue, 1 May 2012 16:15:57 GMT"}, {"version": "v3", "created": "Tue, 6 Nov 2012 17:36:28 GMT"}], "update_date": "2012-11-07", "authors_parsed": [["Urry", "Matthew J.", ""], ["Sollich", "Peter", ""]]}, {"id": "1202.6001", "submitter": "Hyokun Yun", "authors": "Hyokun Yun and S. V. N. Vishwanathan", "title": "Efficiently Sampling Multiplicative Attribute Graphs Using a\n  Ball-Dropping Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel and efficient sampling algorithm for the Multiplicative\nAttribute Graph Model (MAGM - Kim and Leskovec (2010)}). Our algorithm is\n\\emph{strictly} more efficient than the algorithm proposed by Yun and\nVishwanathan (2012), in the sense that our method extends the \\emph{best} time\ncomplexity guarantee of their algorithm to a larger fraction of parameter\nspace. Both in theory and in empirical evaluation on sparse graphs, our new\nalgorithm outperforms the previous one. To design our algorithm, we first\ndefine a stochastic \\emph{ball-dropping process} (BDP). Although a special case\nof this process was introduced as an efficient approximate sampling algorithm\nfor the Kronecker Product Graph Model (KPGM - Leskovec et al. (2010)}), neither\n\\emph{why} such an approximation works nor \\emph{what} is the actual\ndistribution this process is sampling from has been addressed so far to the\nbest of our knowledge. Our rigorous treatment of the BDP enables us to clarify\nthe rational behind a BDP approximation of KPGM, and design an efficient\nsampling algorithm for the MAGM.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2012 17:17:16 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2012 02:34:47 GMT"}], "update_date": "2012-02-29", "authors_parsed": [["Yun", "Hyokun", ""], ["Vishwanathan", "S. V. N.", ""]]}, {"id": "1202.6078", "submitter": "Avishek Saha", "authors": "Hal Daume III, Jeff M. Phillips, Avishek Saha, Suresh\n  Venkatasubramanian", "title": "Protocols for Learning Classifiers on Distributed Data", "comments": "19 pages, 12 figures, accepted at AISTATS 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning classifiers for labeled data that has\nbeen distributed across several nodes. Our goal is to find a single classifier,\nwith small approximation error, across all datasets while minimizing the\ncommunication between nodes. This setting models real-world communication\nbottlenecks in the processing of massive distributed datasets. We present\nseveral very general sampling-based solutions as well as some two-way protocols\nwhich have a provable exponential speed-up over any one-way protocol. We focus\non core problems for noiseless data distributed across two or more nodes. The\ntechniques we introduce are reminiscent of active learning, but rather than\nactively probing labels, nodes actively communicate with each other, each node\nsimultaneously learning the important data from another node.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2012 21:33:32 GMT"}], "update_date": "2012-03-06", "authors_parsed": [["Daume", "Hal", "III"], ["Phillips", "Jeff M.", ""], ["Saha", "Avishek", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "1202.6228", "submitter": "Emilie Morvant", "authors": "Emilie Morvant (LIF), Sokol Ko\\c{c}o (LIF), Liva Ralaivola (LIF)", "title": "PAC-Bayesian Generalization Bound on Confusion Matrix for Multi-Class\n  Classification", "comments": "Arxiv: http://arxiv.org/abs/1202.6228, Accepted at ICML 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a PAC-Bayes bound for the generalization risk of the\nGibbs classifier in the multi-class classification framework. The novelty of\nour work is the critical use of the confusion matrix of a classifier as an\nerror measure; this puts our contribution in the line of work aiming at dealing\nwith performance measure that are richer than mere scalar criterion such as the\nmisclassification rate. Thanks to very recent and beautiful results on matrix\nconcentration inequalities, we derive two bounds showing that the true\nconfusion risk of the Gibbs classifier is upper-bounded by its empirical risk\nplus a term depending on the number of training examples in each class. To the\nbest of our knowledge, this is the first PAC-Bayes bounds based on confusion\nmatrices.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2012 14:13:01 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2012 09:29:28 GMT"}, {"version": "v3", "created": "Thu, 24 May 2012 06:26:12 GMT"}, {"version": "v4", "created": "Sat, 30 Jun 2012 19:23:14 GMT"}, {"version": "v5", "created": "Wed, 4 Jul 2012 14:57:53 GMT"}, {"version": "v6", "created": "Tue, 22 Oct 2013 08:25:52 GMT"}], "update_date": "2013-10-23", "authors_parsed": [["Morvant", "Emilie", "", "LIF"], ["Ko\u00e7o", "Sokol", "", "LIF"], ["Ralaivola", "Liva", "", "LIF"]]}, {"id": "1202.6294", "submitter": "Nicolas Dobigeon", "authors": "Jos\\'e M. Bioucas-Dias, Antonio Plaza, Nicolas Dobigeon, Mario\n  Parente, Qian Du, Paul Gader and Jocelyn Chanussot", "title": "Hyperspectral Unmixing Overview: Geometrical, Statistical, and Sparse\n  Regression-Based Approaches", "comments": "This work has been accepted for publication in IEEE Journal of\n  Selected Topics in Applied Earth Observations and Remote Sensing", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an math.OC stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imaging spectrometers measure electromagnetic energy scattered in their\ninstantaneous field view in hundreds or thousands of spectral channels with\nhigher spectral resolution than multispectral cameras. Imaging spectrometers\nare therefore often referred to as hyperspectral cameras (HSCs). Higher\nspectral resolution enables material identification via spectroscopic analysis,\nwhich facilitates countless applications that require identifying materials in\nscenarios unsuitable for classical spectroscopic analysis. Due to low spatial\nresolution of HSCs, microscopic material mixing, and multiple scattering,\nspectra measured by HSCs are mixtures of spectra of materials in a scene. Thus,\naccurate estimation requires unmixing. Pixels are assumed to be mixtures of a\nfew materials, called endmembers. Unmixing involves estimating all or some of:\nthe number of endmembers, their spectral signatures, and their abundances at\neach pixel. Unmixing is a challenging, ill-posed inverse problem because of\nmodel inaccuracies, observation noise, environmental conditions, endmember\nvariability, and data set size. Researchers have devised and investigated many\nmodels searching for robust, stable, tractable, and accurate unmixing\nalgorithms. This paper presents an overview of unmixing methods from the time\nof Keshava and Mustard's unmixing tutorial [1] to the present. Mixing models\nare first discussed. Signal-subspace, geometrical, statistical, sparsity-based,\nand spatial-contextual unmixing algorithms are described. Mathematical problems\nand potential solutions are described. Algorithm characteristics are\nillustrated experimentally.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2012 17:30:39 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2012 11:52:08 GMT"}], "update_date": "2012-04-25", "authors_parsed": [["Bioucas-Dias", "Jos\u00e9 M.", ""], ["Plaza", "Antonio", ""], ["Dobigeon", "Nicolas", ""], ["Parente", "Mario", ""], ["Du", "Qian", ""], ["Gader", "Paul", ""], ["Chanussot", "Jocelyn", ""]]}, {"id": "1202.6504", "submitter": "Krikamol Muandet", "authors": "Krikamol Muandet, Kenji Fukumizu, Francesco Dinuzzo, Bernhard\n  Sch\\\"olkopf", "title": "Learning from Distributions via Support Measure Machines", "comments": "Advances in Neural Information Processing Systems 25", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a kernel-based discriminative learning framework on\nprobability measures. Rather than relying on large collections of vectorial\ntraining examples, our framework learns using a collection of probability\ndistributions that have been constructed to meaningfully represent training\ndata. By representing these probability distributions as mean embeddings in the\nreproducing kernel Hilbert space (RKHS), we are able to apply many standard\nkernel-based learning techniques in straightforward fashion. To accomplish\nthis, we construct a generalization of the support vector machine (SVM) called\na support measure machine (SMM). Our analyses of SMMs provides several insights\ninto their relationship to traditional SVMs. Based on such insights, we propose\na flexible SVM (Flex-SVM) that places different kernel functions on each\ntraining example. Experimental results on both synthetic and real-world data\ndemonstrate the effectiveness of our proposed framework.\n", "versions": [{"version": "v1", "created": "Wed, 29 Feb 2012 10:09:26 GMT"}, {"version": "v2", "created": "Sat, 12 Jan 2013 12:43:09 GMT"}], "update_date": "2013-01-15", "authors_parsed": [["Muandet", "Krikamol", ""], ["Fukumizu", "Kenji", ""], ["Dinuzzo", "Francesco", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1202.6548", "submitter": "Davide Albanese", "authors": "Davide Albanese and Roberto Visintainer and Stefano Merler and\n  Samantha Riccadonna and Giuseppe Jurman and Cesare Furlanello", "title": "mlpy: Machine Learning Python", "comments": "Corrected a few typos; rephrased two sentences in the Overview\n  section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  mlpy is a Python Open Source Machine Learning library built on top of\nNumPy/SciPy and the GNU Scientific Libraries. mlpy provides a wide range of\nstate-of-the-art machine learning methods for supervised and unsupervised\nproblems and it is aimed at finding a reasonable compromise among modularity,\nmaintainability, reproducibility, usability and efficiency. mlpy is\nmultiplatform, it works with Python 2 and 3 and it is distributed under GPL3 at\nthe website http://mlpy.fbk.eu.\n", "versions": [{"version": "v1", "created": "Wed, 29 Feb 2012 13:49:10 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2012 13:31:54 GMT"}], "update_date": "2012-03-02", "authors_parsed": [["Albanese", "Davide", ""], ["Visintainer", "Roberto", ""], ["Merler", "Stefano", ""], ["Riccadonna", "Samantha", ""], ["Jurman", "Giuseppe", ""], ["Furlanello", "Cesare", ""]]}, {"id": "1202.6590", "submitter": "Giusi Moffa", "authors": "Jack Kuipers and Giusi Moffa", "title": "Uniform random generation of large acyclic digraphs", "comments": "15 pages, 2 figures. To appear in Statistics and Computing", "journal-ref": null, "doi": "10.1007/s11222-013-9428-y", "report-no": null, "categories": "stat.CO math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directed acyclic graphs are the basic representation of the structure\nunderlying Bayesian networks, which represent multivariate probability\ndistributions. In many practical applications, such as the reverse engineering\nof gene regulatory networks, not only the estimation of model parameters but\nthe reconstruction of the structure itself is of great interest. As well as for\nthe assessment of different structure learning algorithms in simulation\nstudies, a uniform sample from the space of directed acyclic graphs is required\nto evaluate the prevalence of certain structural features. Here we analyse how\nto sample acyclic digraphs uniformly at random through recursive enumeration,\nan approach previously thought too computationally involved. Based on\ncomplexity considerations, we discuss in particular how the enumeration\ndirectly provides an exact method, which avoids the convergence issues of the\nalternative Markov chain methods and is actually computationally much faster.\nThe limiting behaviour of the distribution of acyclic digraphs then allows us\nto sample arbitrarily large graphs. Building on the ideas of recursive\nenumeration based sampling we also introduce a novel hybrid Markov chain with\nmuch faster convergence than current alternatives while still being easy to\nadapt to various restrictions. Finally we discuss how to include such\nrestrictions in the combinatorial enumeration and the new hybrid Markov chain\nmethod for efficient uniform sampling of the corresponding graphs.\n", "versions": [{"version": "v1", "created": "Wed, 29 Feb 2012 16:24:13 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2012 12:50:10 GMT"}, {"version": "v3", "created": "Fri, 3 May 2013 13:51:07 GMT"}, {"version": "v4", "created": "Thu, 14 Nov 2013 20:56:06 GMT"}], "update_date": "2013-11-15", "authors_parsed": [["Kuipers", "Jack", ""], ["Moffa", "Giusi", ""]]}, {"id": "1202.6666", "submitter": "Francois Meyer", "authors": "Francois G. Meyer and Xilin Shen", "title": "Perturbation of the Eigenvectors of the Graph Laplacian: Application to\n  Image Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The original contributions of this paper are twofold: a new understanding of\nthe influence of noise on the eigenvectors of the graph Laplacian of a set of\nimage patches, and an algorithm to estimate a denoised set of patches from a\nnoisy image. The algorithm relies on the following two observations: (1) the\nlow-index eigenvectors of the diffusion, or graph Laplacian, operators are very\nrobust to random perturbations of the weights and random changes in the\nconnections of the patch-graph; and (2) patches extracted from smooth regions\nof the image are organized along smooth low-dimensional structures in the\npatch-set, and therefore can be reconstructed with few eigenvectors.\nExperiments demonstrate that our denoising algorithm outperforms the denoising\ngold-standards.\n", "versions": [{"version": "v1", "created": "Wed, 29 Feb 2012 19:59:53 GMT"}], "update_date": "2012-03-01", "authors_parsed": [["Meyer", "Francois G.", ""], ["Shen", "Xilin", ""]]}]